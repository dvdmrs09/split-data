{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef serialize(self, q):\n        children = []\n        for child in q.children:\n            if isinstance(child, Q):\n                children.append(self.serialize(child))\n            else:\n                children.append(child)\n        serialized = q.__dict__\n        serialized['children'] = children\n        return serialized", "response": "Serialize a Q object into a possibly nested dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_field_values_list(self, d):\n        fields = []\n        children = d.get('children', [])\n        for child in children:\n            if isinstance(child, dict):\n                fields.extend(self.get_field_values_list(child))\n            else:\n                f = {'field': child[0], 'value': child[1]}\n                if self._is_range(child):\n                    f['value_from'] = child[1][0]\n                    f['value_to'] = child[1][1]\n                f['negate'] = d.get('negated', False)\n                fields.append(f)\n\n            # add _OR line\n            if d['connector'] == 'OR' and children[-1] != child:\n                fields.append({'field': '_OR', 'value': 'null'})\n        return fields", "response": "Iterate over a dict and return a list of all the fields that are values for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a string value into a regex pattern string.", "response": "def to_python(self, value):\n        \"\"\"\n        Split a string value by separator (default to \",\") into a\n        list; then, returns a regex pattern string that ORs the values\n        in the resulting list.\n\n        >>> field = VaryingTypeCharField()\n        >>> assert field.to_python('') == ''\n        >>> assert field.to_python('test') == 'test'\n        >>> assert field.to_python('and,me') == '(and|me)'\n        >>> assert field.to_python('and,me;too') == '(and|me;too)'\n        \"\"\"\n        res = super(VaryingTypeCharField, self).to_python(value)\n        split_res = res.split(self._default_separator)\n        if not res or len(split_res) < 2:\n            return res.strip()\n\n        # create a regex string out of the list of choices passed, i.e: (a|b)\n        res = r\"({pattern})\".format(pattern=\"|\".join(\n            map(lambda x: x.strip(), split_res)))\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncleaning the data for a single record.", "response": "def clean(self):\n        \"\"\"\n        >>> import django.forms\n        >>> class MyForm(CleanWhiteSpacesMixin, django.forms.Form):\n        ...     some_field = django.forms.CharField()\n        >>>\n        >>> form = MyForm({'some_field': ' a   weird value  '})\n        >>> assert form.is_valid()\n        >>> assert form.cleaned_data == {'some_field': 'a weird value'}\n        \"\"\"\n        cleaned_data = super(CleanWhiteSpacesMixin, self).clean()\n        for k in self.cleaned_data:\n            if isinstance(self.cleaned_data[k], six.string_types):\n                cleaned_data[k] = re.sub(extra_spaces_pattern, ' ',\n                                         self.cleaned_data[k] or '').strip()\n        return cleaned_data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _build_field_choices(self, fields):\n        return tuple(sorted(\n            [(fquery, capfirst(fname)) for fquery, fname in fields.items()],\n            key=lambda f: f[1].lower())\n        ) + self.FIELD_CHOICES", "response": "Build the field choices tuple for the passed model fields tuple and update initial choices."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build_query_dict(self, formdata=None):\n        if self.is_valid() and formdata is None:\n            formdata = self.cleaned_data\n        key = \"{field}__{operator}\".format(**formdata)\n        if formdata['operator'] == \"isnull\":\n            return {key: None}\n        elif formdata['operator'] == \"istrue\":\n            return {formdata['field']: True}\n        elif formdata['operator'] == \"isfalse\":\n            return {formdata['field']: False}\n        return {key: formdata['value']}", "response": "Build a query dict from the submitted data and return it"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_query_dict(query_data, model):\n        operator = 'iexact'\n        if query_data['field'] == '_OR':\n            query_data['operator'] = operator\n            return query_data\n\n        parts = query_data['field'].split('__')\n        if len(parts) < 2:\n            field = parts[0]\n        else:\n            if parts[-1] in dict(AdvancedFilterQueryForm.OPERATORS).keys():\n                field = '__'.join(parts[:-1])\n                operator = parts[-1]\n            else:\n                field = query_data['field']\n\n        query_data['field'] = field\n        mfield = get_fields_from_path(model, query_data['field'])\n        if not mfield:\n            raise Exception('Field path \"%s\" could not be followed to a field'\n                            ' in model %s', query_data['field'], model)\n        else:\n            mfield = mfield[-1]  # get the field object\n\n        if query_data['value'] is None:\n            query_data['operator'] = \"isnull\"\n        elif query_data['value'] is True:\n            query_data['operator'] = \"istrue\"\n        elif query_data['value'] is False:\n            query_data['operator'] = \"isfalse\"\n        else:\n            if isinstance(mfield, DateField):\n                # this is a date/datetime field\n                query_data['operator'] = \"range\"  # default\n            else:\n                query_data['operator'] = operator  # default\n\n        if isinstance(query_data.get('value'),\n                      list) and query_data['operator'] == 'range':\n            date_from = date_to_string(query_data.get('value_from'))\n            date_to = date_to_string(query_data.get('value_to'))\n            query_data['value'] = ','.join([date_from, date_to])\n\n        return query_data", "response": "Take a list of query field dict and return data for form initialization\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvalidating date range by parsing into 2 datetime objects and validating them both.", "response": "def set_range_value(self, data):\n        \"\"\"\n        Validates date range by parsing into 2 datetime objects and\n        validating them both.\n        \"\"\"\n        dtfrom = data.pop('value_from')\n        dtto = data.pop('value_to')\n        if dtfrom is dtto is None:\n            self.errors['value'] = ['Date range requires values']\n            raise forms.ValidationError([])\n        data['value'] = (dtfrom, dtto)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a Q object from the submitted form", "response": "def make_query(self, *args, **kwargs):\n        \"\"\" Returns a Q object from the submitted form \"\"\"\n        query = Q()  # initial is an empty query\n        query_dict = self._build_query_dict(self.cleaned_data)\n        if 'negate' in self.cleaned_data and self.cleaned_data['negate']:\n            query = query & ~Q(**query_dict)\n        else:\n            query = query & Q(**query_dict)\n        return query"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_fields_from_model(self, model, fields):\n        model_fields = {}\n        for field in fields:\n                if isinstance(field, tuple) and len(field) == 2:\n                    field, verbose_name = field[0], field[1]\n                else:\n                    try:\n                        model_field = get_fields_from_path(model, field)[-1]\n                        verbose_name = model_field.verbose_name\n                    except (FieldDoesNotExist, IndexError, TypeError) as e:\n                        logger.warn(\"AdvancedFilterForm: skip invalid field \"\n                                    \"- %s\", e)\n                        continue\n                model_fields[field] = verbose_name\n        return model_fields", "response": "Get the fields that are available for the given model."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a query for the current set of entries in the database.", "response": "def generate_query(self):\n        \"\"\" Reduces multiple queries into a single usable query \"\"\"\n        query = Q()\n        ORed = []\n        for form in self._non_deleted_forms:\n            if not hasattr(form, 'cleaned_data'):\n                continue\n            if form.cleaned_data['field'] == \"_OR\":\n                ORed.append(query)\n                query = Q()\n            else:\n                query = query & form.make_query()\n        if ORed:\n            if query:  # add last query for OR if any\n                ORed.append(query)\n            query = reduce(operator.or_, ORed)\n        return query"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntaking a finalized query and generates it s form data", "response": "def initialize_form(self, instance, model, data=None, extra=None):\n        \"\"\" Takes a \"finalized\" query and generate it's form data \"\"\"\n        model_fields = self.get_fields_from_model(model, self._filter_fields)\n\n        forms = []\n        if instance:\n            for field_data in instance.list_fields():\n                forms.append(\n                    AdvancedFilterQueryForm._parse_query_dict(\n                        field_data, model))\n\n        formset = AFQFormSetNoExtra if not extra else AFQFormSet\n        self.fields_formset = formset(\n            data=data,\n            initial=forms or None,\n            model_fields=model_fields\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef logout():\n\n    cas_username_session_key = current_app.config['CAS_USERNAME_SESSION_KEY']\n    cas_attributes_session_key = current_app.config['CAS_ATTRIBUTES_SESSION_KEY']\n\n    if cas_username_session_key in flask.session:\n        del flask.session[cas_username_session_key]\n\n    if cas_attributes_session_key in flask.session:\n        del flask.session[cas_attributes_session_key]\n\n    if(current_app.config['CAS_AFTER_LOGOUT'] is not None):\n        redirect_url = create_cas_logout_url(\n            current_app.config['CAS_SERVER'],\n            current_app.config['CAS_LOGOUT_ROUTE'],\n            current_app.config['CAS_AFTER_LOGOUT'])\n    else:\n        redirect_url = create_cas_logout_url(\n            current_app.config['CAS_SERVER'],\n            current_app.config['CAS_LOGOUT_ROUTE'])\n\n    current_app.logger.debug('Redirecting to: {0}'.format(redirect_url))\n    return flask.redirect(redirect_url)", "response": "When the user accesses this route they are logged out."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate(ticket):\n\n    cas_username_session_key = current_app.config['CAS_USERNAME_SESSION_KEY']\n    cas_attributes_session_key = current_app.config['CAS_ATTRIBUTES_SESSION_KEY']\n\n    current_app.logger.debug(\"validating token {0}\".format(ticket))\n\n    cas_validate_url = create_cas_validate_url(\n        current_app.config['CAS_SERVER'],\n        current_app.config['CAS_VALIDATE_ROUTE'],\n        flask.url_for('.login', origin=flask.session.get('CAS_AFTER_LOGIN_SESSION_URL'), _external=True),\n        ticket)\n\n    current_app.logger.debug(\"Making GET request to {0}\".format(\n        cas_validate_url))\n\n    xml_from_dict = {}\n    isValid = False\n\n    try:\n        xmldump = urlopen(cas_validate_url).read().strip().decode('utf8', 'ignore')\n        xml_from_dict = parse(xmldump)\n        isValid = True if \"cas:authenticationSuccess\" in xml_from_dict[\"cas:serviceResponse\"] else False\n    except ValueError:\n        current_app.logger.error(\"CAS returned unexpected result\")\n\n    if isValid:\n        current_app.logger.debug(\"valid\")\n        xml_from_dict = xml_from_dict[\"cas:serviceResponse\"][\"cas:authenticationSuccess\"]\n        username = xml_from_dict[\"cas:user\"]\n        flask.session[cas_username_session_key] = username\n\n        if \"cas:attributes\" in xml_from_dict:\n            attributes = xml_from_dict[\"cas:attributes\"]\n\n            if \"cas:memberOf\" in attributes:\n                attributes[\"cas:memberOf\"] = attributes[\"cas:memberOf\"].lstrip('[').rstrip(']').split(',')\n                for group_number in range(0, len(attributes['cas:memberOf'])):\n                    attributes['cas:memberOf'][group_number] = attributes['cas:memberOf'][group_number].lstrip(' ').rstrip(' ')\n\n            flask.session[cas_attributes_session_key] = attributes\n    else:\n        current_app.logger.debug(\"invalid\")\n\n    return isValid", "response": "Validate a CAS ticket."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_url(base, path=None, *query):\n    url = base\n    # Add the path to the url if it's not None.\n    if path is not None:\n        url = urljoin(url, quote(path))\n    # Remove key/value pairs with None values.\n    query = filter(lambda pair: pair[1] is not None, query)\n    # Add the query string to the url\n    url = urljoin(url, '?{0}'.format(urlencode(list(query))))\n    return url", "response": "Create a url from base path and query."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_cas_login_url(cas_url, cas_route, service, renew=None, gateway=None):\n    return create_url(\n        cas_url,\n        cas_route,\n        ('service', service),\n        ('renew', renew),\n        ('gateway', gateway),\n    )", "response": "Create a CAS login URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a CAS validate URL.", "response": "def create_cas_validate_url(cas_url, cas_route, service, ticket,\n                            renew=None):\n    \"\"\" Create a CAS validate URL.\n\n    Keyword arguments:\n    cas_url -- The url to the CAS (ex. http://sso.pdx.edu)\n    cas_route -- The route where the CAS lives on server (ex. /cas/serviceValidate)\n    service -- (ex.  http://localhost:5000/login)\n    ticket -- (ex. 'ST-58274-x839euFek492ou832Eena7ee-cas')\n    renew -- \"true\" or \"false\"\n\n    Example usage:\n    >>> create_cas_validate_url(\n    ...     'http://sso.pdx.edu',\n    ...     '/cas/serviceValidate',\n    ...     'http://localhost:5000/login',\n    ...     'ST-58274-x839euFek492ou832Eena7ee-cas'\n    ... )\n    'http://sso.pdx.edu/cas/serviceValidate?service=http%3A%2F%2Flocalhost%3A5000%2Flogin&ticket=ST-58274-x839euFek492ou832Eena7ee-cas'\n    \"\"\"\n    return create_url(\n        cas_url,\n        cas_route,\n        ('service', service),\n        ('ticket', ticket),\n        ('renew', renew),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef namespace_to_dict(obj):\n    if isinstance(obj, (argparse.Namespace, optparse.Values)):\n        return vars(obj)\n    return obj", "response": "Convert argparse. Namespace to dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the absolute path to the package containing the named module or None if the path could not be identified.", "response": "def _package_path(name):\n    \"\"\"Returns the path to the package containing the named module or\n    None if the path could not be identified (e.g., if\n    ``name == \"__main__\"``).\n    \"\"\"\n    loader = pkgutil.get_loader(name)\n    if loader is None or name == '__main__':\n        return None\n\n    if hasattr(loader, 'get_filename'):\n        filepath = loader.get_filename(name)\n    else:\n        # Fall back to importing the specified module.\n        __import__(name)\n        filepath = sys.modules[name].__file__\n\n    return os.path.dirname(os.path.abspath(filepath))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of paths taken from the XDG_CONFIG_DIRS environment varibables if they exist otherwise returns an empty list", "response": "def xdg_config_dirs():\n    \"\"\"Returns a list of paths taken from the XDG_CONFIG_DIRS\n    and XDG_CONFIG_HOME environment varibables if they exist\n    \"\"\"\n    paths = []\n    if 'XDG_CONFIG_HOME' in os.environ:\n        paths.append(os.environ['XDG_CONFIG_HOME'])\n    if 'XDG_CONFIG_DIRS' in os.environ:\n        paths.extend(os.environ['XDG_CONFIG_DIRS'].split(':'))\n    else:\n        paths.append('/etc/xdg')\n    paths.append('/etc')\n    return paths"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of platform - specific user - specific configuration directories on the system.", "response": "def config_dirs():\n    \"\"\"Return a platform-specific list of candidates for user\n    configuration directories on the system.\n\n    The candidates are in order of priority, from highest to lowest. The\n    last element is the \"fallback\" location to be used when no\n    higher-priority config file exists.\n    \"\"\"\n    paths = []\n\n    if platform.system() == 'Darwin':\n        paths.append(MAC_DIR)\n        paths.append(UNIX_DIR_FALLBACK)\n        paths.extend(xdg_config_dirs())\n\n    elif platform.system() == 'Windows':\n        paths.append(WINDOWS_DIR_FALLBACK)\n        if WINDOWS_DIR_VAR in os.environ:\n            paths.append(os.environ[WINDOWS_DIR_VAR])\n\n    else:\n        # Assume Unix.\n        paths.append(UNIX_DIR_FALLBACK)\n        paths.extend(xdg_config_dirs())\n\n    # Expand and deduplicate paths.\n    out = []\n    for path in paths:\n        path = os.path.abspath(os.path.expanduser(path))\n        if path not in out:\n            out.append(path)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a YAML document from a file.", "response": "def load_yaml(filename):\n    \"\"\"Read a YAML document from a file. If the file cannot be read or\n    parsed, a ConfigReadError is raised.\n    \"\"\"\n    try:\n        with open(filename, 'rb') as f:\n            return yaml.load(f, Loader=Loader)\n    except (IOError, yaml.error.YAMLError) as exc:\n        raise ConfigReadError(filename, exc)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrestoring the yaml comments in data.", "response": "def restore_yaml_comments(data, default_data):\n    \"\"\"Scan default_data for comments (we include empty lines in our\n    definition of comments) and place them before the same keys in data.\n    Only works with comments that are on one or more own lines, i.e.\n    not next to a yaml mapping.\n    \"\"\"\n    comment_map = dict()\n    default_lines = iter(default_data.splitlines())\n    for line in default_lines:\n        if not line:\n            comment = \"\\n\"\n        elif line.startswith(\"#\"):\n            comment = \"{0}\\n\".format(line)\n        else:\n            continue\n        while True:\n            line = next(default_lines)\n            if line and not line.startswith(\"#\"):\n                break\n            comment += \"{0}\\n\".format(line)\n        key = line.split(':')[0].strip()\n        comment_map[key] = comment\n    out_lines = iter(data.splitlines())\n    out_data = \"\"\n    for line in out_lines:\n        key = line.split(':')[0].strip()\n        if key in comment_map:\n            out_data += comment_map[key]\n        out_data += \"{0}\\n\".format(line)\n    return out_data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a simple shorthand Python value to a Template.", "response": "def as_template(value):\n    \"\"\"Convert a simple \"shorthand\" Python value to a `Template`.\n    \"\"\"\n    if isinstance(value, Template):\n        # If it's already a Template, pass it through.\n        return value\n    elif isinstance(value, abc.Mapping):\n        # Dictionaries work as templates.\n        return MappingTemplate(value)\n    elif value is int:\n        return Integer()\n    elif isinstance(value, int):\n        return Integer(value)\n    elif isinstance(value, type) and issubclass(value, BASESTRING):\n        return String()\n    elif isinstance(value, BASESTRING):\n        return String(value)\n    elif isinstance(value, set):\n        # convert to list to avoid hash related problems\n        return Choice(list(value))\n    elif (SUPPORTS_ENUM and isinstance(value, type)\n            and issubclass(value, enum.Enum)):\n        return Choice(value)\n    elif isinstance(value, list):\n        return OneOf(value)\n    elif value is float:\n        return Number()\n    elif value is None:\n        return Template()\n    elif value is dict:\n        return TypeTemplate(abc.Mapping)\n    elif value is list:\n        return TypeTemplate(abc.Sequence)\n    elif isinstance(value, type):\n        return TypeTemplate(value)\n    else:\n        raise ValueError(u'cannot convert to template: {0!r}'.format(value))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef of(cls, value):\n        if isinstance(value, ConfigSource):\n            return value\n        elif isinstance(value, dict):\n            return ConfigSource(value)\n        else:\n            raise TypeError(u'source value must be a dict')", "response": "Given either a dictionary or a ConfigSource object return either a ConfigSource object or raise a TypeError"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a value source pair for the first object found for this view.", "response": "def first(self):\n        \"\"\"Return a (value, source) pair for the first object found for\n        this view. This amounts to the first element returned by\n        `resolve`. If no values are available, a NotFoundError is\n        raised.\n        \"\"\"\n        pairs = self.resolve()\n        try:\n            return iter_first(pairs)\n        except ValueError:\n            raise NotFoundError(u\"{0} not found\".format(self.name))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\noverlaying parsed command - line arguments generated by a library like argparse or optparse onto this view s value.", "response": "def set_args(self, namespace, dots=False):\n        \"\"\"Overlay parsed command-line arguments, generated by a library\n        like argparse or optparse, onto this view's value.\n\n        :param namespace: Dictionary or Namespace to overlay this config with.\n            Supports nested Dictionaries and Namespaces.\n        :type namespace: dict or Namespace\n        :param dots: If True, any properties on namespace that contain dots (.)\n            will be broken down into child dictionaries.\n            :Example:\n\n            {'foo.bar': 'car'}\n            # Will be turned into\n            {'foo': {'bar': 'car'}}\n        :type dots: bool\n        \"\"\"\n        self.set(self._build_namespace_dict(namespace, dots))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list containing all the keys available as subviews of the current views.", "response": "def keys(self):\n        \"\"\"Returns a list containing all the keys available as subviews\n        of the current views. This enumerates all the keys in *all*\n        dictionaries matching the current view, in contrast to\n        ``view.get(dict).keys()``, which gets all the keys for the\n        *first* dict matching the view. If the object for this view in\n        any source is not a dict, then a ConfigTypeError is raised. The\n        keys are ordered according to how they appear in each source.\n        \"\"\"\n        keys = []\n\n        for dic, _ in self.resolve():\n            try:\n                cur_keys = dic.keys()\n            except AttributeError:\n                raise ConfigTypeError(\n                    u'{0} must be a dict, not {1}'.format(\n                        self.name, type(dic).__name__\n                    )\n                )\n\n            for key in cur_keys:\n                if key not in keys:\n                    keys.append(key)\n\n        return keys"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\niterates over all subviews at this view from all sources and yields all the contents of the list.", "response": "def all_contents(self):\n        \"\"\"Iterates over all subviews from collections at this view from\n        *all* sources. If the object for this view in any source is not\n        iterable, then a ConfigTypeError is raised. This method is\n        intended to be used when the view indicates a list; this method\n        will concatenate the contents of the list from all sources.\n        \"\"\"\n        for collection, _ in self.resolve():\n            try:\n                it = iter(collection)\n            except TypeError:\n                raise ConfigTypeError(\n                    u'{0} must be an iterable, not {1}'.format(\n                        self.name, type(collection).__name__\n                    )\n                )\n            for value in it:\n                yield value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a hierarchy of OrderedDicts containing the data from the current object and all its views.", "response": "def flatten(self, redact=False):\n        \"\"\"Create a hierarchy of OrderedDicts containing the data from\n        this view, recursively reifying all views to get their\n        represented values.\n\n        If `redact` is set, then sensitive values are replaced with\n        the string \"REDACTED\".\n        \"\"\"\n        od = OrderedDict()\n        for key, view in self.items():\n            if redact and view.redact:\n                od[key] = REDACTED_TOMBSTONE\n            else:\n                try:\n                    od[key] = view.flatten(redact=redact)\n                except ConfigTypeError:\n                    od[key] = view.get()\n        return od"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrepresent a list of items in inline style.", "response": "def represent_list(self, data):\n        \"\"\"If a list has less than 4 items, represent it in inline style\n        (i.e. comma separated, within square brackets).\n        \"\"\"\n        node = super(Dumper, self).represent_list(data)\n        length = len(data)\n        if self.default_flow_style is None and length < 4:\n            node.flow_style = True\n        elif self.default_flow_style is None:\n            node.flow_style = False\n        return node"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrepresenting bool as yes or no instead of true or false.", "response": "def represent_bool(self, data):\n        \"\"\"Represent bool as 'yes' or 'no' instead of 'true' or 'false'.\n        \"\"\"\n        if data:\n            value = u'yes'\n        else:\n            value = u'no'\n        return self.represent_scalar('tag:yaml.org,2002:bool', value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding the configuration options from the user s configuration directory.", "response": "def _add_user_source(self):\n        \"\"\"Add the configuration options from the YAML file in the\n        user's configuration directory (given by `config_dir`) if it\n        exists.\n        \"\"\"\n        filename = self.user_config_path()\n        if os.path.isfile(filename):\n            self.add(ConfigSource(load_yaml(filename) or {}, filename))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_default_source(self):\n        if self.modname:\n            if self._package_path:\n                filename = os.path.join(self._package_path, DEFAULT_FILENAME)\n                if os.path.isfile(filename):\n                    self.add(ConfigSource(load_yaml(filename), filename, True))", "response": "Add the package s default configuration settings. This looks\n        for a YAML file located inside the package for the module\n        modname."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding and read the files for this configuration and set them as the sources for this configuration.", "response": "def read(self, user=True, defaults=True):\n        \"\"\"Find and read the files for this configuration and set them\n        as the sources for this configuration. To disable either\n        discovered user configuration files or the in-package defaults,\n        set `user` or `defaults` to `False`.\n        \"\"\"\n        if user:\n            self._add_user_source()\n        if defaults:\n            self._add_default_source()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the path to the user configuration directory.", "response": "def config_dir(self):\n        \"\"\"Get the path to the user configuration directory. The\n        directory is guaranteed to exist as a postcondition (one may be\n        created if none exist).\n\n        If the application's ``...DIR`` environment variable is set, it\n        is used as the configuration directory. Otherwise,\n        platform-specific standard configuration locations are searched\n        for a ``config.yaml`` file. If no configuration file is found, a\n        fallback path is used.\n        \"\"\"\n        # If environment variable is set, use it.\n        if self._env_var in os.environ:\n            appdir = os.environ[self._env_var]\n            appdir = os.path.abspath(os.path.expanduser(appdir))\n            if os.path.isfile(appdir):\n                raise ConfigError(u'{0} must be a directory'.format(\n                    self._env_var\n                ))\n\n        else:\n            # Search platform-specific locations. If no config file is\n            # found, fall back to the first directory in the list.\n            configdirs = config_dirs()\n            for confdir in configdirs:\n                appdir = os.path.join(confdir, self.appname)\n                if os.path.isfile(os.path.join(appdir, CONFIG_FILENAME)):\n                    break\n            else:\n                appdir = os.path.join(configdirs[0], self.appname)\n\n        # Ensure that the directory exists.\n        if not os.path.isdir(appdir):\n            os.makedirs(appdir)\n        return appdir"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_file(self, filename):\n        filename = os.path.abspath(filename)\n        self.set(ConfigSource(load_yaml(filename), filename))", "response": "Parses the file and inserts it into the configuration\n        sources with highest priority."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndumping the configuration object to a YAML string.", "response": "def dump(self, full=True, redact=False):\n        \"\"\"Dump the Configuration object to a YAML file.\n\n        The order of the keys is determined from the default\n        configuration file. All keys not in the default configuration\n        will be appended to the end of the file.\n\n        :param filename:  The file to dump the configuration to, or None\n                          if the YAML string should be returned instead\n        :type filename:   unicode\n        :param full:      Dump settings that don't differ from the defaults\n                          as well\n        :param redact:    Remove sensitive information (views with the `redact`\n                          flag set) from the output\n        \"\"\"\n        if full:\n            out_dict = self.flatten(redact=redact)\n        else:\n            # Exclude defaults when flattening.\n            sources = [s for s in self.sources if not s.default]\n            temp_root = RootView(sources)\n            temp_root.redactions = self.redactions\n            out_dict = temp_root.flatten(redact=redact)\n\n        yaml_out = yaml.dump(out_dict, Dumper=Dumper,\n                             default_flow_style=None, indent=4,\n                             width=1000)\n\n        # Restore comments to the YAML text.\n        default_source = None\n        for source in self.sources:\n            if source.default:\n                default_source = source\n                break\n        if default_source and default_source.filename:\n            with open(default_source.filename, 'rb') as fp:\n                default_data = fp.read()\n            yaml_out = restore_yaml_comments(yaml_out,\n                                             default_data.decode('utf-8'))\n\n        return yaml_out"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves all sources from this configuration.", "response": "def clear(self):\n        \"\"\"Remove all sources from this configuration.\"\"\"\n        super(LazyConfig, self).clear()\n        self._lazy_suffix = []\n        self._lazy_prefix = []"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef value(self, view, template=None):\n        if view.exists():\n            value, _ = view.first()\n            return self.convert(value, view)\n        elif self.default is REQUIRED:\n            # Missing required value. This is an error.\n            raise NotFoundError(u\"{0} not found\".format(view.name))\n        else:\n            # Missing value, but not required.\n            return self.default", "response": "Get the value for a ConfigView."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nraising an exception indicating that a value cannot be accepted.", "response": "def fail(self, message, view, type_error=False):\n        \"\"\"Raise an exception indicating that a value cannot be\n        accepted.\n\n        `type_error` indicates whether the error is due to a type\n        mismatch rather than a malformed value. In this case, a more\n        specific exception is raised.\n        \"\"\"\n        exc_class = ConfigTypeError if type_error else ConfigValueError\n        raise exc_class(\n            u'{0}: {1}'.format(view.name, message)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert(self, value, view):\n        if isinstance(value, int):\n            return value\n        elif isinstance(value, float):\n            return int(value)\n        else:\n            self.fail(u'must be a number', view, True)", "response": "Check that the value is an integer. Floats are rounded."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck that the value is an int or a float.", "response": "def convert(self, value, view):\n        \"\"\"Check that the value is an int or a float.\n        \"\"\"\n        if isinstance(value, NUMERIC_TYPES):\n            return value\n        else:\n            self.fail(\n                u'must be numeric, not {0}'.format(type(value).__name__),\n                view,\n                True\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a dict with the same keys as the template and values validated according to the value types.", "response": "def value(self, view, template=None):\n        \"\"\"Get a dict with the same keys as the template and values\n        validated according to the value types.\n        \"\"\"\n        out = AttrDict()\n        for key, typ in self.subtemplates.items():\n            out[key] = typ.value(view[key], self)\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef value(self, view, template=None):\n        out = []\n        for item in view:\n            out.append(self.subtemplate.value(item, self))\n        return out", "response": "Get a list of items validated against the template."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert(self, value, view):\n        if isinstance(value, BASESTRING):\n            if self.pattern and not self.regex.match(value):\n                self.fail(\n                    u\"must match the pattern {0}\".format(self.pattern),\n                    view\n                )\n            return value\n        else:\n            self.fail(u'must be a string', view, True)", "response": "Check that the value is a string and matches the pattern."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nensure that the value is among the choices and remap if the choices are a mapping.", "response": "def convert(self, value, view):\n        \"\"\"Ensure that the value is among the choices (and remap if the\n        choices are a mapping).\n        \"\"\"\n        if (SUPPORTS_ENUM and isinstance(self.choices, type)\n                and issubclass(self.choices, enum.Enum)):\n            try:\n                return self.choices(value)\n            except ValueError:\n                self.fail(\n                    u'must be one of {0!r}, not {1!r}'.format(\n                        [c.value for c in self.choices], value\n                    ),\n                    view\n                )\n\n        if value not in self.choices:\n            self.fail(\n                u'must be one of {0!r}, not {1!r}'.format(\n                    list(self.choices), value\n                ),\n                view\n            )\n\n        if isinstance(self.choices, abc.Mapping):\n            return self.choices[value]\n        else:\n            return value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nensuring that the value follows at least one template.", "response": "def convert(self, value, view):\n        \"\"\"Ensure that the value follows at least one template.\n        \"\"\"\n        is_mapping = isinstance(self.template, MappingTemplate)\n\n        for candidate in self.allowed:\n            try:\n                if is_mapping:\n                    if isinstance(candidate, Filename) and \\\n                            candidate.relative_to:\n                        next_template = candidate.template_with_relatives(\n                            view,\n                            self.template\n                        )\n\n                        next_template.subtemplates[view.key] = as_template(\n                            candidate\n                        )\n                    else:\n                        next_template = MappingTemplate({view.key: candidate})\n\n                    return view.parent.get(next_template)[view.key]\n                else:\n                    return view.get(candidate)\n            except ConfigTemplateError:\n                raise\n            except ConfigError:\n                pass\n            except ValueError as exc:\n                raise ConfigTemplateError(exc)\n\n        self.fail(\n            u'must be one of {0}, not {1}'.format(\n                repr(self.allowed), repr(value)\n            ),\n            view\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef export_live_eggs(self, env=False):\n        path_eggs = [p for p in sys.path if p.endswith('.egg')]\n\n        command = self.get_finalized_command(\"egg_info\")\n        egg_base = path.abspath(command.egg_base)\n\n        unique_path_eggs = set(path_eggs + [egg_base])\n\n        os.environ['PYTHONPATH'] = ':'.join(unique_path_eggs)", "response": "Adds all of the eggs in the current environment to PYTHONPATH."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the JavaScript runtime that is specified in EXECJS_RUNTIME environment variable.", "response": "def get_from_environment():\n    '''\n        Return the JavaScript runtime that is specified in EXECJS_RUNTIME environment variable.\n        If EXECJS_RUNTIME environment variable is empty or invalid, return None.\n    '''\n    name = os.environ.get(\"EXECJS_RUNTIME\", \"\")\n    if not name:\n        return None\n\n    try:\n        return _find_runtime_by_name(name)\n    except exceptions.RuntimeUnavailableError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef encode_unicode_codepoints(str):\n    codepoint_format = '\\\\u{0:04x}'.format\n\n    def codepoint(m):\n        return codepoint_format(ord(m.group(0)))\n\n    return re.sub('[^\\x00-\\x7f]', codepoint, str)", "response": "r Encode unicode codepoints in a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprotects if s is not text", "response": "def _decode_if_not_text(s):\n    \"\"\"protected\"\"\"\n    if isinstance(s, six.text_type):\n        return s\n    return s.decode(sys.getfilesystemencoding())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _find_executable(prog, pathext=(\"\",)):\n    pathlist = _decode_if_not_text(os.environ.get('PATH', '')).split(os.pathsep)\n\n    for dir in pathlist:\n        for ext in pathext:\n            filename = os.path.join(dir, prog + ext)\n            try:\n                st = os.stat(filename)\n            except os.error:\n                continue\n            if stat.S_ISREG(st.st_mode) and (stat.S_IMODE(st.st_mode) & 0o111):\n                return filename\n    return None", "response": "protected function to find executable file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _which(command):\n    if isinstance(command, str):\n        command = [command]\n    command = list(command)\n    name = command[0]\n    args = command[1:]\n\n    if _is_windows():\n        pathext = _decode_if_not_text(os.environ.get(\"PATHEXT\", \"\"))\n        path = _find_executable(name, pathext.split(os.pathsep))\n    else:\n        path = _find_executable(name)\n\n    if not path:\n        return None\n    return [path] + args", "response": "return the path to the command"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef split_table_cells(self, row):\n\n        row = iter(row)\n        col = 0\n        start_col = col + 1\n        cell = ''\n        first_cell = True\n        while True:\n            char = next(row, None)\n            col += 1\n            if char == '|':\n                if first_cell:\n                    # First cell (content before the first |) is skipped\n                    first_cell = False\n                else:\n                    yield (cell, start_col)\n                cell = ''\n                start_col = col + 1\n            elif char == '\\\\':\n                char = next(row)\n                col += 1\n                if char == 'n':\n                    cell += '\\n'\n                else:\n                    if char not in ['|', '\\\\']:\n                        cell += '\\\\'\n                    cell += char\n            elif char:\n                cell += char\n            else:\n                break", "response": "An iterator returning all the table cells in a row with their positions accounting for escaping."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a Series with the number of days between the given index.", "response": "def day_counts(index):\n    \"\"\"Days between DatetimeIndex values as a :any:`pandas.Series`.\n\n    Parameters\n    ----------\n    index : :any:`pandas.DatetimeIndex`\n        The index for which to get day counts.\n\n    Returns\n    -------\n    day_counts : :any:`pandas.Series`\n        A :any:`pandas.Series` with counts of days between periods. Counts are\n        given on start dates of periods.\n    \"\"\"\n    # dont affect the original data\n    index = index.copy()\n\n    if len(index) == 0:\n        return pd.Series([], index=index)\n\n    timedeltas = (index[1:] - index[:-1]).append(pd.TimedeltaIndex([pd.NaT]))\n    timedelta_days = timedeltas.total_seconds() / (60 * 60 * 24)\n\n    return pd.Series(timedelta_days, index=index)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_baseline_data(\n    data,\n    start=None,\n    end=None,\n    max_days=365,\n    allow_billing_period_overshoot=False,\n    ignore_billing_period_gap_for_day_count=False,\n):\n    \"\"\" Filter down to baseline period data.\n\n    .. note::\n\n        For compliance with CalTRACK, set ``max_days=365`` (section 2.2.1.1).\n\n    Parameters\n    ----------\n    data : :any:`pandas.DataFrame` or :any:`pandas.Series`\n        The data to filter to baseline data. This data will be filtered down\n        to an acceptable baseline period according to the dates passed as\n        `start` and `end`, or the maximum period specified with `max_days`.\n    start : :any:`datetime.datetime`\n        A timezone-aware datetime that represents the earliest allowable start\n        date for the baseline data. The stricter of this or `max_days` is used\n        to determine the earliest allowable baseline period date.\n    end : :any:`datetime.datetime`\n        A timezone-aware datetime that represents the latest allowable end\n        date for the baseline data, i.e., the latest date for which data is\n        available before the intervention begins.\n    max_days : :any:`int`, default 365\n        The maximum length of the period. Ignored if `end` is not set.\n        The stricter of this or `start` is used to determine the earliest\n        allowable baseline period date.\n    allow_billing_period_overshoot : :any:`bool`, default False\n        If True, count `max_days` from the end of the last billing data period\n        that ends before the `end` date, rather than from the exact `end` date.\n        Otherwise use the exact `end` date as the cutoff.\n    ignore_billing_period_gap_for_day_count : :any:`bool`, default False\n        If True, instead of going back `max_days` from either the\n        `end` date or end of the last billing period before that date (depending\n        on the value of the `allow_billing_period_overshoot` setting) and\n        excluding the last period that began before that date, first check to\n        see if excluding or including that period gets closer to a total of\n        `max_days` of data.\n\n        For example, with `max_days=365`, if an exact 365 period would targeted\n        Feb 15, but the billing period went from Jan 20 to Feb 20, exclude that\n        period for a total of ~360 days of data, because that's closer to 365\n        than ~390 days, which would be the total if that period was included.\n        If, on the other hand, if that period started Feb 10 and went to Mar 10,\n        include the period, because ~370 days of data is closer to than ~340.\n\n    Returns\n    -------\n    baseline_data, warnings : :any:`tuple` of (:any:`pandas.DataFrame` or :any:`pandas.Series`, :any:`list` of :any:`eemeter.EEMeterWarning`)\n        Data for only the specified baseline period and any associated warnings.\n    \"\"\"\n    if max_days is not None:\n        if start is not None:\n            raise ValueError(  # pragma: no cover\n                \"If max_days is set, start cannot be set: start={}, max_days={}.\".format(\n                    start, max_days\n                )\n            )\n\n    start_inf = False\n    if start is None:\n        # py datetime min/max are out of range of pd.Timestamp min/max\n        start_target = pytz.UTC.localize(pd.Timestamp.min)\n        start_inf = True\n    else:\n        start_target = start\n\n    end_inf = False\n    if end is None:\n        end_limit = pytz.UTC.localize(pd.Timestamp.max)\n        end_inf = True\n    else:\n        end_limit = end\n\n    # copying prevents setting on slice warnings\n    data_before_end_limit = data[:end_limit].copy()\n\n    if ignore_billing_period_gap_for_day_count:\n        end_limit = data_before_end_limit.index.max()\n\n    if not end_inf and max_days is not None:\n        start_target = end_limit - timedelta(days=max_days)\n\n    if allow_billing_period_overshoot:\n        # adjust start limit to get a selection closest to max_days\n        # also consider ffill for get_loc method - always picks previous\n        try:\n            loc = data_before_end_limit.index.get_loc(start_target, method=\"nearest\")\n        except (KeyError, IndexError):  # pragma: no cover\n            baseline_data = data_before_end_limit\n            start_limit = start_target\n        else:\n            start_limit = data_before_end_limit.index[loc]\n            baseline_data = data_before_end_limit[start_limit:].copy()\n\n    else:\n        # use hard limit for baseline start\n        start_limit = start_target\n        baseline_data = data_before_end_limit[start_limit:].copy()\n\n    if baseline_data.dropna().empty:\n        raise NoBaselineDataError()\n\n    baseline_data.iloc[-1] = np.nan\n\n    data_end = data.index.max()\n    data_start = data.index.min()\n    return (\n        baseline_data,\n        _make_baseline_warnings(\n            end_inf, start_inf, data_start, data_end, start_limit, end_limit\n        ),\n    )", "response": "Filter down to baseline data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_reporting_data(\n    data,\n    start=None,\n    end=None,\n    max_days=365,\n    allow_billing_period_overshoot=False,\n    ignore_billing_period_gap_for_day_count=False,\n):\n    \"\"\" Filter down to reporting period data.\n\n    Parameters\n    ----------\n    data : :any:`pandas.DataFrame` or :any:`pandas.Series`\n        The data to filter to reporting data. This data will be filtered down\n        to an acceptable reporting period according to the dates passed as\n        `start` and `end`, or the maximum period specified with `max_days`.\n    start : :any:`datetime.datetime`\n        A timezone-aware datetime that represents the earliest allowable start\n        date for the reporting data, i.e., the earliest date for which data is\n        available after the intervention begins.\n    end : :any:`datetime.datetime`\n        A timezone-aware datetime that represents the latest allowable end\n        date for the reporting data. The stricter of this or `max_days` is used\n        to determine the latest allowable reporting period date.\n    max_days : :any:`int`, default 365\n        The maximum length of the period. Ignored if `start` is not set.\n        The stricter of this or `end` is used to determine the latest\n        allowable reporting period date.\n    allow_billing_period_overshoot : :any:`bool`, default False\n        If True, count `max_days` from the start of the first billing data period\n        that starts after the `start` date, rather than from the exact `start` date.\n        Otherwise use the exact `start` date as the cutoff.\n    ignore_billing_period_gap_for_day_count : :any:`bool`, default False\n        If True, instead of going forward `max_days` from either the\n        `start` date or the `start` of the first billing period after that date\n        (depending on the value of the `allow_billing_period_overshoot` setting)\n        and excluding the first period that ended after that date, first check\n        to see if excluding or including that period gets closer to a total of\n        `max_days` of data.\n\n        For example, with `max_days=365`, if an exact 365 period would targeted\n        Feb 15, but the billing period went from Jan 20 to Feb 20, include that\n        period for a total of ~370 days of data, because that's closer to 365\n        than ~340 days, which would be the total if that period was excluded.\n        If, on the other hand, if that period started Feb 10 and went to Mar 10,\n        exclude the period, because ~360 days of data is closer to than ~390.\n\n    Returns\n    -------\n    reporting_data, warnings : :any:`tuple` of (:any:`pandas.DataFrame` or :any:`pandas.Series`, :any:`list` of :any:`eemeter.EEMeterWarning`)\n        Data for only the specified reporting period and any associated warnings.\n    \"\"\"\n    if max_days is not None:\n        if end is not None:\n            raise ValueError(  # pragma: no cover\n                \"If max_days is set, end cannot be set: end={}, max_days={}.\".format(\n                    end, max_days\n                )\n            )\n\n    start_inf = False\n    if start is None:\n        # py datetime min/max are out of range of pd.Timestamp min/max\n        start_limit = pytz.UTC.localize(pd.Timestamp.min)\n        start_inf = True\n    else:\n        start_limit = start\n\n    end_inf = False\n    if end is None:\n        end_target = pytz.UTC.localize(pd.Timestamp.max)\n        end_inf = True\n    else:\n        end_target = end\n\n    # copying prevents setting on slice warnings\n    data_after_start_limit = data[start_limit:].copy()\n\n    if ignore_billing_period_gap_for_day_count:\n        start_limit = data_after_start_limit.index.min()\n\n    if not start_inf and max_days is not None:\n        end_target = start_limit + timedelta(days=max_days)\n\n    if allow_billing_period_overshoot:\n        # adjust start limit to get a selection closest to max_days\n        # also consider bfill for get_loc method - always picks next\n        try:\n            loc = data_after_start_limit.index.get_loc(end_target, method=\"nearest\")\n        except (KeyError, IndexError):  # pragma: no cover\n            reporting_data = data_after_start_limit\n            end_limit = end_target\n        else:\n            end_limit = data_after_start_limit.index[loc]\n            reporting_data = data_after_start_limit[:end_limit].copy()\n\n    else:\n        # use hard limit for baseline start\n        end_limit = end_target\n        reporting_data = data_after_start_limit[:end_limit].copy()\n\n    if reporting_data.dropna().empty:\n        raise NoReportingDataError()\n\n    reporting_data.iloc[-1] = np.nan\n\n    data_end = data.index.max()\n    data_start = data.index.min()\n    return (\n        reporting_data,\n        _make_reporting_warnings(\n            end_inf, start_inf, data_start, data_end, start_limit, end_limit\n        ),\n    )", "response": "Filter down to reporting data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_temperature_features(\n    meter_data_index,\n    temperature_data,\n    heating_balance_points=None,\n    cooling_balance_points=None,\n    data_quality=False,\n    temperature_mean=True,\n    degree_day_method=\"daily\",\n    percent_hourly_coverage_per_day=0.5,\n    percent_hourly_coverage_per_billing_period=0.9,\n    use_mean_daily_values=True,\n    tolerance=None,\n    keep_partial_nan_rows=False,\n):\n    \"\"\" Compute temperature features from hourly temperature data using the\n    :any:`pandas.DatetimeIndex` meter data..\n\n    Creates a :any:`pandas.DataFrame` with the same index as the meter data.\n\n    .. note::\n\n        For CalTRACK compliance (2.2.2.3), must set\n        ``percent_hourly_coverage_per_day=0.5``,\n        ``cooling_balance_points=range(30,90,X)``, and\n        ``heating_balance_points=range(30,90,X)``, where\n        X is either 1, 2, or 3. For natural gas meter use data, must\n        set ``fit_cdd=False``.\n\n    .. note::\n\n        For CalTRACK compliance (2.2.3.2), for billing methods, must set\n        ``percent_hourly_coverage_per_billing_period=0.9``.\n\n    .. note::\n\n        For CalTRACK compliance (2.3.3), ``meter_data_index`` and ``temperature_data``\n        must both be timezone-aware and have matching timezones.\n\n    .. note::\n\n        For CalTRACK compliance (3.3.1.1), for billing methods, must set\n        ``use_mean_daily_values=True``.\n\n    .. note::\n\n        For CalTRACK compliance (3.3.1.2), for daily or billing methods,\n        must set ``degree_day_method=daily``.\n\n    Parameters\n    ----------\n    meter_data_index : :any:`pandas.DataFrame`\n        A :any:`pandas.DatetimeIndex` corresponding to the index over which\n        to compute temperature features.\n    temperature_data : :any:`pandas.Series`\n        Series with :any:`pandas.DatetimeIndex` with hourly (``'H'``) frequency\n        and a set of temperature values.\n    cooling_balance_points : :any:`list` of :any:`int` or :any:`float`, optional\n        List of cooling balance points for which to create cooling degree days.\n    heating_balance_points : :any:`list` of :any:`int` or :any:`float`, optional\n        List of heating balance points for which to create heating degree days.\n    data_quality : :any:`bool`, optional\n        If True, compute data quality columns for temperature, i.e.,\n        ``temperature_not_null`` and ``temperature_null``, containing for\n        each meter value\n    temperature_mean : :any:`bool`, optional\n        If True, compute temperature means for each meter period.\n    degree_day_method : :any:`str`, ``'daily'`` or ``'hourly'``\n        The method to use in calculating degree days.\n    percent_hourly_coverage_per_day : :any:`str`, optional\n        Percent hourly temperature coverage per day for heating and cooling\n        degree days to not be dropped.\n    use_mean_daily_values : :any:`bool`, optional\n        If True, meter and degree day values should be mean daily values, not\n        totals. If False, totals will be used instead.\n    tolerance : :any:`pandas.Timedelta`, optional\n        Do not merge more than this amount of temperature data beyond this limit.\n    keep_partial_nan_rows: :any:`bool`, optional\n        If True, keeps data in resultant :any:`pandas.DataFrame` that has\n        missing temperature or meter data. Otherwise, these rows are overwritten\n        entirely with ``numpy.nan`` values.\n\n    Returns\n    -------\n    data : :any:`pandas.DataFrame`\n        A dataset with the specified parameters.\n    \"\"\"\n    if temperature_data.index.freq != \"H\":\n        raise ValueError(\n            \"temperature_data.index must have hourly frequency (freq='H').\"\n            \" Found: {}\".format(temperature_data.index.freq)\n        )\n\n    if not temperature_data.index.tz:\n        raise ValueError(\n            \"temperature_data.index must be timezone-aware. You can set it with\"\n            \" temperature_data.tz_localize(...).\"\n        )\n\n    if meter_data_index.freq is None and meter_data_index.inferred_freq == \"H\":\n        raise ValueError(\n            \"If you have hourly data explicitly set the frequency\"\n            \" of the dataframe by setting\"\n            \"``meter_data_index.freq =\"\n            \" pd.tseries.frequencies.to_offset('H').\"\n        )\n\n    if not meter_data_index.tz:\n        raise ValueError(\n            \"meter_data_index must be timezone-aware. You can set it with\"\n            \" meter_data.tz_localize(...).\"\n        )\n\n    if meter_data_index.duplicated().any():\n        raise ValueError(\"Duplicates found in input meter trace index.\")\n\n    temp_agg_funcs = []\n    temp_agg_column_renames = {}\n\n    if heating_balance_points is None:\n        heating_balance_points = []\n    if cooling_balance_points is None:\n        cooling_balance_points = []\n\n    if meter_data_index.freq is not None:\n        try:\n            freq_timedelta = pd.Timedelta(meter_data_index.freq)\n        except ValueError:  # freq cannot be converted to timedelta\n            freq_timedelta = None\n    else:\n        freq_timedelta = None\n\n    if tolerance is None:\n        tolerance = freq_timedelta\n\n    if not (heating_balance_points == [] and cooling_balance_points == []):\n        if degree_day_method == \"hourly\":\n            pass\n        elif degree_day_method == \"daily\":\n            if meter_data_index.freq == \"H\":\n                raise ValueError(\n                    \"degree_day_method='hourly' must be used with\"\n                    \" hourly meter data. Found: 'daily'\".format(degree_day_method)\n                )\n        else:\n            raise ValueError(\"method not supported: {}\".format(degree_day_method))\n\n    if freq_timedelta == pd.Timedelta(\"1H\"):\n        # special fast route for hourly data.\n        df = temperature_data.to_frame(\"temperature_mean\").reindex(meter_data_index)\n\n        if use_mean_daily_values:\n            n_days = 1\n        else:\n            n_days = 1.0 / 24.0\n\n        df = df.assign(\n            **{\n                \"cdd_{}\".format(bp): np.maximum(df.temperature_mean - bp, 0) * n_days\n                for bp in cooling_balance_points\n            }\n        )\n        df = df.assign(\n            **{\n                \"hdd_{}\".format(bp): np.maximum(bp - df.temperature_mean, 0) * n_days\n                for bp in heating_balance_points\n            }\n        )\n        df = df.assign(\n            n_hours_dropped=df.temperature_mean.isnull().astype(int),\n            n_hours_kept=df.temperature_mean.notnull().astype(int),\n        )\n        # TODO(philngo): bad interface or maybe this is just wrong for some reason?\n        if data_quality:\n            df = df.assign(\n                temperature_null=df.n_hours_dropped,\n                temperature_not_null=df.n_hours_kept,\n            )\n        if not temperature_mean:\n            del df[\"temperature_mean\"]\n    else:\n        # daily/billing route\n        # heating/cooling degree day aggregations. Needed for n_days fields as well.\n        temp_agg_funcs.extend(\n            _degree_day_columns(\n                heating_balance_points=heating_balance_points,\n                cooling_balance_points=cooling_balance_points,\n                degree_day_method=degree_day_method,\n                percent_hourly_coverage_per_day=percent_hourly_coverage_per_day,\n                percent_hourly_coverage_per_billing_period=percent_hourly_coverage_per_billing_period,\n                use_mean_daily_values=use_mean_daily_values,\n            )\n        )\n        temp_agg_column_renames.update(\n            {(\"temp\", \"degree_day_columns\"): \"degree_day_columns\"}\n        )\n\n        if data_quality:\n            temp_agg_funcs.extend(\n                [(\"not_null\", \"count\"), (\"null\", lambda x: x.isnull().sum())]\n            )\n            temp_agg_column_renames.update(\n                {\n                    (\"temp\", \"not_null\"): \"temperature_not_null\",\n                    (\"temp\", \"null\"): \"temperature_null\",\n                }\n            )\n\n        if temperature_mean:\n            temp_agg_funcs.extend([(\"mean\", \"mean\")])\n            temp_agg_column_renames.update({(\"temp\", \"mean\"): \"temperature_mean\"})\n\n        # aggregate temperatures\n        temp_df = temperature_data.to_frame(\"temp\")\n        temp_groups = _matching_groups(meter_data_index, temp_df, tolerance)\n        temp_aggregations = temp_groups.agg({\"temp\": temp_agg_funcs})\n\n        # expand temp aggregations by faking and deleting the `meter_value` column.\n        # I haven't yet figured out a way to avoid this and get the desired\n        # structure and behavior. (philngo)\n        meter_value = pd.DataFrame({\"meter_value\": 0}, index=meter_data_index)\n        df = pd.concat([meter_value, temp_aggregations], axis=1).rename(\n            columns=temp_agg_column_renames\n        )\n        del df[\"meter_value\"]\n\n        if \"degree_day_columns\" in df:\n            if df[\"degree_day_columns\"].dropna().empty:\n                column_defaults = {\n                    column: np.full(df[\"degree_day_columns\"].shape, np.nan)\n                    for column in [\"n_days_dropped\", \"n_days_kept\"]\n                }\n                df = df.drop([\"degree_day_columns\"], axis=1).assign(**column_defaults)\n            else:\n                df = pd.concat(\n                    [\n                        df.drop([\"degree_day_columns\"], axis=1),\n                        df[\"degree_day_columns\"].dropna().apply(pd.Series),\n                    ],\n                    axis=1,\n                )\n\n    if not keep_partial_nan_rows:\n        df = overwrite_partial_rows_with_nan(df)\n\n    # nan last row\n    df = df.iloc[:-1].reindex(df.index)\n    return df", "response": "Compute temperature features from hourly temperature data using the base class."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef metered_savings(\n    baseline_model,\n    reporting_meter_data,\n    temperature_data,\n    with_disaggregated=False,\n    confidence_level=0.90,\n    predict_kwargs=None,\n):\n    \"\"\" Compute metered savings, i.e., savings in which the baseline model\n    is used to calculate the modeled usage in the reporting period. This\n    modeled usage is then compared to the actual usage from the reporting period.\n    Also compute two measures of the uncertainty of the aggregate savings estimate,\n    a fractional savings uncertainty (FSU) error band and an OLS error band. (To convert\n    the FSU error band into FSU, divide by total estimated savings.)\n\n    Parameters\n    ----------\n    baseline_model : :any:`eemeter.CalTRACKUsagePerDayModelResults`\n        Object to use for predicting pre-intervention usage.\n    reporting_meter_data : :any:`pandas.DataFrame`\n        The observed reporting period data (totals). Savings will be computed for the\n        periods supplied in the reporting period data.\n    temperature_data : :any:`pandas.Series`\n        Hourly-frequency timeseries of temperature data during the reporting\n        period.\n    with_disaggregated : :any:`bool`, optional\n        If True, calculate baseline counterfactual disaggregated usage\n        estimates. Savings cannot be disaggregated for metered savings. For\n        that, use :any:`eemeter.modeled_savings`.\n    confidence_level : :any:`float`, optional\n        The two-tailed confidence level used to calculate the t-statistic used\n        in calculation of the error bands.\n\n        Ignored if not computing error bands.\n    predict_kwargs : :any:`dict`, optional\n        Extra kwargs to pass to the baseline_model.predict method.\n\n    Returns\n    -------\n    results : :any:`pandas.DataFrame`\n        DataFrame with metered savings, indexed with\n        ``reporting_meter_data.index``. Will include the following columns:\n\n        - ``counterfactual_usage`` (baseline model projected into reporting period)\n        - ``reporting_observed`` (given by reporting_meter_data)\n        - ``metered_savings``\n\n        If `with_disaggregated` is set to True, the following columns will also\n        be in the results DataFrame:\n\n        - ``counterfactual_base_load``\n        - ``counterfactual_heating_load``\n        - ``counterfactual_cooling_load``\n\n    error_bands : :any:`dict`, optional\n        If baseline_model is an instance of CalTRACKUsagePerDayModelResults,\n        will also return a dictionary of FSU and OLS error bands for the\n        aggregated energy savings over the post period.\n    \"\"\"\n    if predict_kwargs is None:\n        predict_kwargs = {}\n\n    model_type = None  # generic\n    if isinstance(baseline_model, CalTRACKUsagePerDayModelResults):\n        model_type = \"usage_per_day\"\n\n    if model_type == \"usage_per_day\" and with_disaggregated:\n        predict_kwargs[\"with_disaggregated\"] = True\n\n    prediction_index = reporting_meter_data.index\n    model_prediction = baseline_model.predict(\n        prediction_index, temperature_data, **predict_kwargs\n    )\n\n    predicted_baseline_usage = model_prediction.result\n\n    # CalTrack 3.5.1\n    counterfactual_usage = predicted_baseline_usage[\"predicted_usage\"].to_frame(\n        \"counterfactual_usage\"\n    )\n\n    reporting_observed = reporting_meter_data[\"value\"].to_frame(\"reporting_observed\")\n\n    def metered_savings_func(row):\n        return row.counterfactual_usage - row.reporting_observed\n\n    results = reporting_observed.join(counterfactual_usage).assign(\n        metered_savings=metered_savings_func\n    )\n\n    if model_type == \"usage_per_day\" and with_disaggregated:\n        counterfactual_usage_disaggregated = predicted_baseline_usage[\n            [\"base_load\", \"heating_load\", \"cooling_load\"]\n        ].rename(\n            columns={\n                \"base_load\": \"counterfactual_base_load\",\n                \"heating_load\": \"counterfactual_heating_load\",\n                \"cooling_load\": \"counterfactual_cooling_load\",\n            }\n        )\n        results = results.join(counterfactual_usage_disaggregated)\n\n    results = results.dropna().reindex(results.index)  # carry NaNs\n\n    # compute t-statistic associated with n degrees of freedom\n    # and a two-tailed confidence level.\n    error_bands = None\n    if model_type == \"usage_per_day\":  # has totals_metrics\n        error_bands = _compute_error_bands_metered_savings(\n            baseline_model.totals_metrics,\n            results,\n            baseline_model.interval,\n            confidence_level,\n        )\n    return results, error_bands", "response": "Compute metered savings for a single base language."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes modeled savings in which baseline and reporting usage values are based on models.", "response": "def modeled_savings(\n    baseline_model,\n    reporting_model,\n    result_index,\n    temperature_data,\n    with_disaggregated=False,\n    confidence_level=0.90,\n    predict_kwargs=None,\n):\n    \"\"\" Compute modeled savings, i.e., savings in which baseline and reporting\n    usage values are based on models. This is appropriate for annualizing or\n    weather normalizing models.\n\n    Parameters\n    ----------\n    baseline_model : :any:`eemeter.CalTRACKUsagePerDayCandidateModel`\n        Model to use for predicting pre-intervention usage.\n    reporting_model : :any:`eemeter.CalTRACKUsagePerDayCandidateModel`\n        Model to use for predicting post-intervention usage.\n    result_index : :any:`pandas.DatetimeIndex`\n        The dates for which usage should be modeled.\n    temperature_data : :any:`pandas.Series`\n        Hourly-frequency timeseries of temperature data during the modeled\n        period.\n    with_disaggregated : :any:`bool`, optional\n        If True, calculate modeled disaggregated usage estimates and savings.\n    confidence_level : :any:`float`, optional\n        The two-tailed confidence level used to calculate the t-statistic used\n        in calculation of the error bands.\n\n        Ignored if not computing error bands.\n    predict_kwargs : :any:`dict`, optional\n        Extra kwargs to pass to the baseline_model.predict and\n        reporting_model.predict methods.\n\n    Returns\n    -------\n    results : :any:`pandas.DataFrame`\n        DataFrame with modeled savings, indexed with the result_index. Will\n        include the following columns:\n\n        - ``modeled_baseline_usage``\n        - ``modeled_reporting_usage``\n        - ``modeled_savings``\n\n        If `with_disaggregated` is set to True, the following columns will also\n        be in the results DataFrame:\n\n        - ``modeled_baseline_base_load``\n        - ``modeled_baseline_cooling_load``\n        - ``modeled_baseline_heating_load``\n        - ``modeled_reporting_base_load``\n        - ``modeled_reporting_cooling_load``\n        - ``modeled_reporting_heating_load``\n        - ``modeled_base_load_savings``\n        - ``modeled_cooling_load_savings``\n        - ``modeled_heating_load_savings``\n    error_bands : :any:`dict`, optional\n        If baseline_model and reporting_model are instances of\n        CalTRACKUsagePerDayModelResults, will also return a dictionary of\n        FSU and error bands for the aggregated energy savings over the\n        normal year period.\n    \"\"\"\n    prediction_index = result_index\n\n    if predict_kwargs is None:\n        predict_kwargs = {}\n\n    model_type = None  # generic\n    if isinstance(baseline_model, CalTRACKUsagePerDayModelResults):\n        model_type = \"usage_per_day\"\n\n    if model_type == \"usage_per_day\" and with_disaggregated:\n        predict_kwargs[\"with_disaggregated\"] = True\n\n    def _predicted_usage(model):\n        model_prediction = model.predict(\n            prediction_index, temperature_data, **predict_kwargs\n        )\n        predicted_usage = model_prediction.result\n        return predicted_usage\n\n    predicted_baseline_usage = _predicted_usage(baseline_model)\n    predicted_reporting_usage = _predicted_usage(reporting_model)\n    modeled_baseline_usage = predicted_baseline_usage[\"predicted_usage\"].to_frame(\n        \"modeled_baseline_usage\"\n    )\n    modeled_reporting_usage = predicted_reporting_usage[\"predicted_usage\"].to_frame(\n        \"modeled_reporting_usage\"\n    )\n\n    def modeled_savings_func(row):\n        return row.modeled_baseline_usage - row.modeled_reporting_usage\n\n    results = modeled_baseline_usage.join(modeled_reporting_usage).assign(\n        modeled_savings=modeled_savings_func\n    )\n\n    if model_type == \"usage_per_day\" and with_disaggregated:\n        modeled_baseline_usage_disaggregated = predicted_baseline_usage[\n            [\"base_load\", \"heating_load\", \"cooling_load\"]\n        ].rename(\n            columns={\n                \"base_load\": \"modeled_baseline_base_load\",\n                \"heating_load\": \"modeled_baseline_heating_load\",\n                \"cooling_load\": \"modeled_baseline_cooling_load\",\n            }\n        )\n\n        modeled_reporting_usage_disaggregated = predicted_reporting_usage[\n            [\"base_load\", \"heating_load\", \"cooling_load\"]\n        ].rename(\n            columns={\n                \"base_load\": \"modeled_reporting_base_load\",\n                \"heating_load\": \"modeled_reporting_heating_load\",\n                \"cooling_load\": \"modeled_reporting_cooling_load\",\n            }\n        )\n\n        def modeled_base_load_savings_func(row):\n            return row.modeled_baseline_base_load - row.modeled_reporting_base_load\n\n        def modeled_heating_load_savings_func(row):\n            return (\n                row.modeled_baseline_heating_load - row.modeled_reporting_heating_load\n            )\n\n        def modeled_cooling_load_savings_func(row):\n            return (\n                row.modeled_baseline_cooling_load - row.modeled_reporting_cooling_load\n            )\n\n        results = (\n            results.join(modeled_baseline_usage_disaggregated)\n            .join(modeled_reporting_usage_disaggregated)\n            .assign(\n                modeled_base_load_savings=modeled_base_load_savings_func,\n                modeled_heating_load_savings=modeled_heating_load_savings_func,\n                modeled_cooling_load_savings=modeled_cooling_load_savings_func,\n            )\n        )\n\n    results = results.dropna().reindex(results.index)  # carry NaNs\n\n    error_bands = None\n    if model_type == \"usage_per_day\":  # has totals_metrics\n        error_bands = _compute_error_bands_modeled_savings(\n            baseline_model.totals_metrics,\n            reporting_model.totals_metrics,\n            results,\n            baseline_model.interval,\n            reporting_model.interval,\n            confidence_level,\n        )\n    return results, error_bands"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef json(self):\n        data = super(CalTRACKHourlyModel, self).json()\n        data.update(\n            {\n                \"occupancy_lookup\": self.occupancy_lookup.to_json(orient=\"split\"),\n                \"temperature_bins\": self.temperature_bins.to_json(orient=\"split\"),\n            }\n        )\n        return data", "response": "Return a JSON - serializable representation of this result."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_too_few_non_zero_degree_day_warning(\n    model_type, balance_point, degree_day_type, degree_days, minimum_non_zero\n):\n    \"\"\" Return an empty list or a single warning wrapped in a list regarding\n    non-zero degree days for a set of degree days.\n\n    Parameters\n    ----------\n    model_type : :any:`str`\n        Model type (e.g., ``'cdd_hdd'``).\n    balance_point : :any:`float`\n        The balance point in question.\n    degree_day_type : :any:`str`\n        The type of degree days (``'cdd'`` or ``'hdd'``).\n    degree_days : :any:`pandas.Series`\n        A series of degree day values.\n    minimum_non_zero : :any:`int`\n        Minimum allowable number of non-zero degree day values.\n\n    Returns\n    -------\n    warnings : :any:`list` of :any:`eemeter.EEMeterWarning`\n        Empty list or list of single warning.\n    \"\"\"\n    warnings = []\n    n_non_zero = int((degree_days > 0).sum())\n    if n_non_zero < minimum_non_zero:\n        warnings.append(\n            EEMeterWarning(\n                qualified_name=(\n                    \"eemeter.caltrack_daily.{model_type}.too_few_non_zero_{degree_day_type}\".format(\n                        model_type=model_type, degree_day_type=degree_day_type\n                    )\n                ),\n                description=(\n                    \"Number of non-zero daily {degree_day_type} values below accepted minimum.\"\n                    \" Candidate fit not attempted.\".format(\n                        degree_day_type=degree_day_type.upper()\n                    )\n                ),\n                data={\n                    \"n_non_zero_{degree_day_type}\".format(\n                        degree_day_type=degree_day_type\n                    ): n_non_zero,\n                    \"minimum_non_zero_{degree_day_type}\".format(\n                        degree_day_type=degree_day_type\n                    ): minimum_non_zero,\n                    \"{degree_day_type}_balance_point\".format(\n                        degree_day_type=degree_day_type\n                    ): balance_point,\n                },\n            )\n        )\n    return warnings", "response": "Returns an empty list or a single warning wrapped in a list regarding the number of non - zero degree days."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_total_degree_day_too_low_warning(\n    model_type,\n    balance_point,\n    degree_day_type,\n    avg_degree_days,\n    period_days,\n    minimum_total,\n):\n    \"\"\" Return an empty list or a single warning wrapped in a list regarding\n    the total summed degree day values.\n\n    Parameters\n    ----------\n    model_type : :any:`str`\n        Model type (e.g., ``'cdd_hdd'``).\n    balance_point : :any:`float`\n        The balance point in question.\n    degree_day_type : :any:`str`\n        The type of degree days (``'cdd'`` or ``'hdd'``).\n    avg_degree_days : :any:`pandas.Series`\n        A series of degree day values.\n    period_days : :any:`pandas.Series`\n        A series of containing day counts.\n    minimum_total : :any:`float`\n        Minimum allowable total sum of degree day values.\n\n    Returns\n    -------\n    warnings : :any:`list` of :any:`eemeter.EEMeterWarning`\n        Empty list or list of single warning.\n    \"\"\"\n\n    warnings = []\n    total_degree_days = (avg_degree_days * period_days).sum()\n    if total_degree_days < minimum_total:\n        warnings.append(\n            EEMeterWarning(\n                qualified_name=(\n                    \"eemeter.caltrack_daily.{model_type}.total_{degree_day_type}_too_low\".format(\n                        model_type=model_type, degree_day_type=degree_day_type\n                    )\n                ),\n                description=(\n                    \"Total {degree_day_type} below accepted minimum.\"\n                    \" Candidate fit not attempted.\".format(\n                        degree_day_type=degree_day_type.upper()\n                    )\n                ),\n                data={\n                    \"total_{degree_day_type}\".format(\n                        degree_day_type=degree_day_type\n                    ): total_degree_days,\n                    \"total_{degree_day_type}_minimum\".format(\n                        degree_day_type=degree_day_type\n                    ): minimum_total,\n                    \"{degree_day_type}_balance_point\".format(\n                        degree_day_type=degree_day_type\n                    ): balance_point,\n                },\n            )\n        )\n    return warnings", "response": "Returns an empty list or a single warning wrapped in a list regarding the total summed degree day values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_parameter_negative_warning(model_type, model_params, parameter):\n    warnings = []\n    if model_params.get(parameter, 0) < 0:\n        warnings.append(\n            EEMeterWarning(\n                qualified_name=(\n                    \"eemeter.caltrack_daily.{model_type}.{parameter}_negative\".format(\n                        model_type=model_type, parameter=parameter\n                    )\n                ),\n                description=(\n                    \"Model fit {parameter} parameter is negative. Candidate model rejected.\".format(\n                        parameter=parameter\n                    )\n                ),\n                data=model_params,\n            )\n        )\n    return warnings", "response": "Returns an empty list or a single warning indicating whether model parameter is negative."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an empty list or a single warning indicating whether model parameter p - value is too high.", "response": "def get_parameter_p_value_too_high_warning(\n    model_type, model_params, parameter, p_value, maximum_p_value\n):\n    \"\"\" Return an empty list or a single warning wrapped in a list indicating\n    whether model parameter p-value is too high.\n\n    Parameters\n    ----------\n    model_type : :any:`str`\n        Model type (e.g., ``'cdd_hdd'``).\n    model_params : :any:`dict`\n        Parameters as stored in :any:`eemeter.CalTRACKUsagePerDayCandidateModel.model_params`.\n    parameter : :any:`str`\n        The name of the parameter, e.g., ``'intercept'``.\n    p_value : :any:`float`\n        The p-value of the parameter.\n    maximum_p_value : :any:`float`\n        The maximum allowable p-value of the parameter.\n\n    Returns\n    -------\n    warnings : :any:`list` of :any:`eemeter.EEMeterWarning`\n        Empty list or list of single warning.\n    \"\"\"\n    warnings = []\n    if p_value > maximum_p_value:\n        data = {\n            \"{}_p_value\".format(parameter): p_value,\n            \"{}_maximum_p_value\".format(parameter): maximum_p_value,\n        }\n        data.update(model_params)\n        warnings.append(\n            EEMeterWarning(\n                qualified_name=(\n                    \"eemeter.caltrack_daily.{model_type}.{parameter}_p_value_too_high\".format(\n                        model_type=model_type, parameter=parameter\n                    )\n                ),\n                description=(\n                    \"Model fit {parameter} p-value is too high. Candidate model rejected.\".format(\n                        parameter=parameter\n                    )\n                ),\n                data=data,\n            )\n        )\n    return warnings"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_fit_failed_candidate_model(model_type, formula):\n    warnings = [\n        EEMeterWarning(\n            qualified_name=\"eemeter.caltrack_daily.{}.model_results\".format(model_type),\n            description=(\n                \"Error encountered in statsmodels.formula.api.ols method. (Empty data?)\"\n            ),\n            data={\"traceback\": traceback.format_exc()},\n        )\n    ]\n    return CalTRACKUsagePerDayCandidateModel(\n        model_type=model_type, formula=formula, status=\"ERROR\", warnings=warnings\n    )", "response": "Returns a Candidate model that indicates the fitting routine failed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_intercept_only_candidate_models(data, weights_col):\n    model_type = \"intercept_only\"\n    formula = \"meter_value ~ 1\"\n\n    if weights_col is None:\n        weights = 1\n    else:\n        weights = data[weights_col]\n\n    try:\n        model = smf.wls(formula=formula, data=data, weights=weights)\n    except Exception as e:\n        return [get_fit_failed_candidate_model(model_type, formula)]\n\n    result = model.fit()\n\n    # CalTrack 3.3.1.3\n    model_params = {\"intercept\": result.params[\"Intercept\"]}\n\n    model_warnings = []\n\n    # CalTrack 3.4.3.2\n    for parameter in [\"intercept\"]:\n        model_warnings.extend(\n            get_parameter_negative_warning(model_type, model_params, parameter)\n        )\n\n    if len(model_warnings) > 0:\n        status = \"DISQUALIFIED\"\n    else:\n        status = \"QUALIFIED\"\n\n    return [\n        CalTRACKUsagePerDayCandidateModel(\n            model_type=model_type,\n            formula=formula,\n            status=status,\n            warnings=model_warnings,\n            model_params=model_params,\n            model=model,\n            result=result,\n            r_squared_adj=0,\n        )\n    ]", "response": "Returns a list of single intercept - only candidate models."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_single_cdd_only_candidate_model(\n    data,\n    minimum_non_zero_cdd,\n    minimum_total_cdd,\n    beta_cdd_maximum_p_value,\n    weights_col,\n    balance_point,\n):\n    \"\"\" Return a single candidate cdd-only model for a particular balance\n    point.\n\n    Parameters\n    ----------\n    data : :any:`pandas.DataFrame`\n        A DataFrame containing at least the column ``meter_value`` and\n        ``cdd_<balance_point>``\n        DataFrames of this form can be made using the\n        :any:`eemeter.create_caltrack_daily_design_matrix` or\n        :any:`eemeter.create_caltrack_billing_design_matrix` methods.\n    minimum_non_zero_cdd : :any:`int`\n        Minimum allowable number of non-zero cooling degree day values.\n    minimum_total_cdd : :any:`float`\n        Minimum allowable total sum of cooling degree day values.\n    beta_cdd_maximum_p_value : :any:`float`\n        The maximum allowable p-value of the beta cdd parameter.\n    weights_col : :any:`str` or None\n        The name of the column (if any) in ``data`` to use as weights.\n    balance_point : :any:`float`\n        The cooling balance point for this model.\n\n    Returns\n    -------\n    candidate_model : :any:`CalTRACKUsagePerDayCandidateModel`\n        A single cdd-only candidate model, with any associated warnings.\n    \"\"\"\n    model_type = \"cdd_only\"\n    cdd_column = \"cdd_%s\" % balance_point\n    formula = \"meter_value ~ %s\" % cdd_column\n\n    if weights_col is None:\n        weights = 1\n    else:\n        weights = data[weights_col]\n\n    period_days = weights\n\n    degree_day_warnings = []\n    degree_day_warnings.extend(\n        get_total_degree_day_too_low_warning(\n            model_type,\n            balance_point,\n            \"cdd\",\n            data[cdd_column],\n            period_days,\n            minimum_total_cdd,\n        )\n    )\n    degree_day_warnings.extend(\n        get_too_few_non_zero_degree_day_warning(\n            model_type, balance_point, \"cdd\", data[cdd_column], minimum_non_zero_cdd\n        )\n    )\n\n    if len(degree_day_warnings) > 0:\n        return CalTRACKUsagePerDayCandidateModel(\n            model_type=model_type,\n            formula=formula,\n            status=\"NOT ATTEMPTED\",\n            warnings=degree_day_warnings,\n        )\n\n    try:\n        model = smf.wls(formula=formula, data=data, weights=weights)\n    except Exception as e:\n        return get_fit_failed_candidate_model(model_type, formula)\n\n    result = model.fit()\n    r_squared_adj = result.rsquared_adj\n    beta_cdd_p_value = result.pvalues[cdd_column]\n\n    # CalTrack 3.3.1.3\n    model_params = {\n        \"intercept\": result.params[\"Intercept\"],\n        \"beta_cdd\": result.params[cdd_column],\n        \"cooling_balance_point\": balance_point,\n    }\n\n    model_warnings = []\n\n    # CalTrack 3.4.3.2\n    for parameter in [\"intercept\", \"beta_cdd\"]:\n        model_warnings.extend(\n            get_parameter_negative_warning(model_type, model_params, parameter)\n        )\n    model_warnings.extend(\n        get_parameter_p_value_too_high_warning(\n            model_type,\n            model_params,\n            parameter,\n            beta_cdd_p_value,\n            beta_cdd_maximum_p_value,\n        )\n    )\n\n    if len(model_warnings) > 0:\n        status = \"DISQUALIFIED\"\n    else:\n        status = \"QUALIFIED\"\n\n    return CalTRACKUsagePerDayCandidateModel(\n        model_type=model_type,\n        formula=formula,\n        status=status,\n        warnings=model_warnings,\n        model_params=model_params,\n        model=model,\n        result=result,\n        r_squared_adj=r_squared_adj,\n    )", "response": "Returns a single candidate cdd - only model for a particular balance - point."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_cdd_only_candidate_models(\n    data, minimum_non_zero_cdd, minimum_total_cdd, beta_cdd_maximum_p_value, weights_col\n):\n    \"\"\" Return a list of all possible candidate cdd-only models.\n\n    Parameters\n    ----------\n    data : :any:`pandas.DataFrame`\n        A DataFrame containing at least the column ``meter_value`` and 1 to n\n        columns with names of the form ``cdd_<balance_point>``. All columns\n        with names of this form will be used to fit a candidate model.\n        DataFrames of this form can be made using the\n        :any:`eemeter.create_caltrack_daily_design_matrix` or\n        :any:`eemeter.create_caltrack_billing_design_matrix` methods.\n    minimum_non_zero_cdd : :any:`int`\n        Minimum allowable number of non-zero cooling degree day values.\n    minimum_total_cdd : :any:`float`\n        Minimum allowable total sum of cooling degree day values.\n    beta_cdd_maximum_p_value : :any:`float`\n        The maximum allowable p-value of the beta cdd parameter.\n    weights_col : :any:`str` or None\n        The name of the column (if any) in ``data`` to use as weights.\n\n    Returns\n    -------\n    candidate_models : :any:`list` of :any:`CalTRACKUsagePerDayCandidateModel`\n        A list of cdd-only candidate models, with any associated warnings.\n    \"\"\"\n    balance_points = [int(col[4:]) for col in data.columns if col.startswith(\"cdd\")]\n    candidate_models = [\n        get_single_cdd_only_candidate_model(\n            data,\n            minimum_non_zero_cdd,\n            minimum_total_cdd,\n            beta_cdd_maximum_p_value,\n            weights_col,\n            balance_point,\n        )\n        for balance_point in balance_points\n    ]\n    return candidate_models", "response": "Returns a list of all possible candidate cdd - only models."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a single candidate hdd - only model for a particular balance - point.", "response": "def get_single_hdd_only_candidate_model(\n    data,\n    minimum_non_zero_hdd,\n    minimum_total_hdd,\n    beta_hdd_maximum_p_value,\n    weights_col,\n    balance_point,\n):\n    \"\"\" Return a single candidate hdd-only model for a particular balance\n    point.\n\n    Parameters\n    ----------\n    data : :any:`pandas.DataFrame`\n        A DataFrame containing at least the column ``meter_value`` and\n        ``hdd_<balance_point>``\n        DataFrames of this form can be made using the\n        :any:`eemeter.create_caltrack_daily_design_matrix` or\n        :any:`eemeter.create_caltrack_billing_design_matrix` methods.\n    minimum_non_zero_hdd : :any:`int`\n        Minimum allowable number of non-zero heating degree day values.\n    minimum_total_hdd : :any:`float`\n        Minimum allowable total sum of heating degree day values.\n    beta_hdd_maximum_p_value : :any:`float`\n        The maximum allowable p-value of the beta hdd parameter.\n    weights_col : :any:`str` or None\n        The name of the column (if any) in ``data`` to use as weights.\n    balance_point : :any:`float`\n        The heating balance point for this model.\n\n    Returns\n    -------\n    candidate_model : :any:`CalTRACKUsagePerDayCandidateModel`\n        A single hdd-only candidate model, with any associated warnings.\n    \"\"\"\n    model_type = \"hdd_only\"\n    hdd_column = \"hdd_%s\" % balance_point\n    formula = \"meter_value ~ %s\" % hdd_column\n\n    if weights_col is None:\n        weights = 1\n    else:\n        weights = data[weights_col]\n\n    period_days = weights\n\n    degree_day_warnings = []\n    degree_day_warnings.extend(\n        get_total_degree_day_too_low_warning(\n            model_type,\n            balance_point,\n            \"hdd\",\n            data[hdd_column],\n            period_days,\n            minimum_total_hdd,\n        )\n    )\n    degree_day_warnings.extend(\n        get_too_few_non_zero_degree_day_warning(\n            model_type, balance_point, \"hdd\", data[hdd_column], minimum_non_zero_hdd\n        )\n    )\n\n    if len(degree_day_warnings) > 0:\n        return CalTRACKUsagePerDayCandidateModel(\n            model_type=model_type,\n            formula=formula,\n            status=\"NOT ATTEMPTED\",\n            warnings=degree_day_warnings,\n        )\n\n    try:\n        model = smf.wls(formula=formula, data=data, weights=weights)\n    except Exception as e:\n        return get_fit_failed_candidate_model(model_type, formula)\n\n    result = model.fit()\n    r_squared_adj = result.rsquared_adj\n    beta_hdd_p_value = result.pvalues[hdd_column]\n\n    # CalTrack 3.3.1.3\n    model_params = {\n        \"intercept\": result.params[\"Intercept\"],\n        \"beta_hdd\": result.params[hdd_column],\n        \"heating_balance_point\": balance_point,\n    }\n\n    model_warnings = []\n\n    # CalTrack 3.4.3.2\n    for parameter in [\"intercept\", \"beta_hdd\"]:\n        model_warnings.extend(\n            get_parameter_negative_warning(model_type, model_params, parameter)\n        )\n    model_warnings.extend(\n        get_parameter_p_value_too_high_warning(\n            model_type,\n            model_params,\n            parameter,\n            beta_hdd_p_value,\n            beta_hdd_maximum_p_value,\n        )\n    )\n\n    if len(model_warnings) > 0:\n        status = \"DISQUALIFIED\"\n    else:\n        status = \"QUALIFIED\"\n\n    return CalTRACKUsagePerDayCandidateModel(\n        model_type=model_type,\n        formula=formula,\n        status=status,\n        warnings=model_warnings,\n        model_params=model_params,\n        model=model,\n        result=result,\n        r_squared_adj=r_squared_adj,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of candidate models for the given data set and minimum_non_zero_hdd minimum_total_hdd and beta_hdd_maximum_p_value.", "response": "def get_hdd_only_candidate_models(\n    data, minimum_non_zero_hdd, minimum_total_hdd, beta_hdd_maximum_p_value, weights_col\n):\n    \"\"\"\n    Parameters\n    ----------\n    data : :any:`pandas.DataFrame`\n        A DataFrame containing at least the column ``meter_value`` and 1 to n\n        columns with names of the form ``hdd_<balance_point>``. All columns\n        with names of this form will be used to fit a candidate model.\n        DataFrames of this form can be made using the\n        :any:`eemeter.create_caltrack_daily_design_matrix` or\n        :any:`eemeter.create_caltrack_billing_design_matrix` methods.\n    minimum_non_zero_hdd : :any:`int`\n        Minimum allowable number of non-zero heating degree day values.\n    minimum_total_hdd : :any:`float`\n        Minimum allowable total sum of heating degree day values.\n    beta_hdd_maximum_p_value : :any:`float`\n        The maximum allowable p-value of the beta hdd parameter.\n    weights_col : :any:`str` or None\n        The name of the column (if any) in ``data`` to use as weights.\n\n    Returns\n    -------\n    candidate_models : :any:`list` of :any:`CalTRACKUsagePerDayCandidateModel`\n        A list of hdd-only candidate models, with any associated warnings.\n    \"\"\"\n\n    balance_points = [int(col[4:]) for col in data.columns if col.startswith(\"hdd\")]\n\n    candidate_models = [\n        get_single_hdd_only_candidate_model(\n            data,\n            minimum_non_zero_hdd,\n            minimum_total_hdd,\n            beta_hdd_maximum_p_value,\n            weights_col,\n            balance_point,\n        )\n        for balance_point in balance_points\n    ]\n    return candidate_models"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a single candidate cdd_hdd model for a particular selection.", "response": "def get_single_cdd_hdd_candidate_model(\n    data,\n    minimum_non_zero_cdd,\n    minimum_non_zero_hdd,\n    minimum_total_cdd,\n    minimum_total_hdd,\n    beta_cdd_maximum_p_value,\n    beta_hdd_maximum_p_value,\n    weights_col,\n    cooling_balance_point,\n    heating_balance_point,\n):\n    \"\"\" Return and fit a single candidate cdd_hdd model for a particular selection\n    of cooling balance point and heating balance point\n\n    Parameters\n    ----------\n    data : :any:`pandas.DataFrame`\n        A DataFrame containing at least the column ``meter_value`` and\n        ``hdd_<heating_balance_point>`` and ``cdd_<cooling_balance_point>``\n        DataFrames of this form can be made using the\n        :any:`eemeter.create_caltrack_daily_design_matrix` or\n        :any:`eemeter.create_caltrack_billing_design_matrix` methods.\n    minimum_non_zero_cdd : :any:`int`\n        Minimum allowable number of non-zero cooling degree day values.\n    minimum_non_zero_hdd : :any:`int`\n        Minimum allowable number of non-zero heating degree day values.\n    minimum_total_cdd : :any:`float`\n        Minimum allowable total sum of cooling degree day values.\n    minimum_total_hdd : :any:`float`\n        Minimum allowable total sum of heating degree day values.\n    beta_cdd_maximum_p_value : :any:`float`\n        The maximum allowable p-value of the beta cdd parameter.\n    beta_hdd_maximum_p_value : :any:`float`\n        The maximum allowable p-value of the beta hdd parameter.\n    weights_col : :any:`str` or None\n        The name of the column (if any) in ``data`` to use as weights.\n    cooling_balance_point : :any:`float`\n        The cooling balance point for this model.\n    heating_balance_point : :any:`float`\n        The heating balance point for this model.\n\n    Returns\n    -------\n    candidate_model : :any:`CalTRACKUsagePerDayCandidateModel`\n        A single cdd-hdd candidate model, with any associated warnings.\n    \"\"\"\n    model_type = \"cdd_hdd\"\n    cdd_column = \"cdd_%s\" % cooling_balance_point\n    hdd_column = \"hdd_%s\" % heating_balance_point\n    formula = \"meter_value ~ %s + %s\" % (cdd_column, hdd_column)\n    n_days_column = None\n\n    if weights_col is None:\n        weights = 1\n    else:\n        weights = data[weights_col]\n\n    period_days = weights\n\n    degree_day_warnings = []\n    degree_day_warnings.extend(\n        get_total_degree_day_too_low_warning(\n            model_type,\n            cooling_balance_point,\n            \"cdd\",\n            data[cdd_column],\n            period_days,\n            minimum_total_cdd,\n        )\n    )\n    degree_day_warnings.extend(\n        get_too_few_non_zero_degree_day_warning(\n            model_type,\n            cooling_balance_point,\n            \"cdd\",\n            data[cdd_column],\n            minimum_non_zero_cdd,\n        )\n    )\n    degree_day_warnings.extend(\n        get_total_degree_day_too_low_warning(\n            model_type,\n            heating_balance_point,\n            \"hdd\",\n            data[hdd_column],\n            period_days,\n            minimum_total_hdd,\n        )\n    )\n    degree_day_warnings.extend(\n        get_too_few_non_zero_degree_day_warning(\n            model_type,\n            heating_balance_point,\n            \"hdd\",\n            data[hdd_column],\n            minimum_non_zero_hdd,\n        )\n    )\n\n    if len(degree_day_warnings) > 0:\n        return CalTRACKUsagePerDayCandidateModel(\n            model_type, formula, \"NOT ATTEMPTED\", warnings=degree_day_warnings\n        )\n\n    try:\n        model = smf.wls(formula=formula, data=data, weights=weights)\n    except Exception as e:\n        return get_fit_failed_candidate_model(model_type, formula)\n\n    result = model.fit()\n    r_squared_adj = result.rsquared_adj\n    beta_cdd_p_value = result.pvalues[cdd_column]\n    beta_hdd_p_value = result.pvalues[hdd_column]\n\n    # CalTrack 3.3.1.3\n    model_params = {\n        \"intercept\": result.params[\"Intercept\"],\n        \"beta_cdd\": result.params[cdd_column],\n        \"beta_hdd\": result.params[hdd_column],\n        \"cooling_balance_point\": cooling_balance_point,\n        \"heating_balance_point\": heating_balance_point,\n    }\n\n    model_warnings = []\n\n    # CalTrack 3.4.3.2\n    for parameter in [\"intercept\", \"beta_cdd\", \"beta_hdd\"]:\n        model_warnings.extend(\n            get_parameter_negative_warning(model_type, model_params, parameter)\n        )\n    model_warnings.extend(\n        get_parameter_p_value_too_high_warning(\n            model_type,\n            model_params,\n            parameter,\n            beta_cdd_p_value,\n            beta_cdd_maximum_p_value,\n        )\n    )\n    model_warnings.extend(\n        get_parameter_p_value_too_high_warning(\n            model_type,\n            model_params,\n            parameter,\n            beta_hdd_p_value,\n            beta_hdd_maximum_p_value,\n        )\n    )\n\n    if len(model_warnings) > 0:\n        status = \"DISQUALIFIED\"\n    else:\n        status = \"QUALIFIED\"\n\n    return CalTRACKUsagePerDayCandidateModel(\n        model_type=model_type,\n        formula=formula,\n        status=status,\n        warnings=model_warnings,\n        model_params=model_params,\n        model=model,\n        result=result,\n        r_squared_adj=r_squared_adj,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of candidate cdd_hdd models for a particular selection.", "response": "def get_cdd_hdd_candidate_models(\n    data,\n    minimum_non_zero_cdd,\n    minimum_non_zero_hdd,\n    minimum_total_cdd,\n    minimum_total_hdd,\n    beta_cdd_maximum_p_value,\n    beta_hdd_maximum_p_value,\n    weights_col,\n):\n    \"\"\" Return a list of candidate cdd_hdd models for a particular selection\n    of cooling balance point and heating balance point\n\n    Parameters\n    ----------\n    data : :any:`pandas.DataFrame`\n        A DataFrame containing at least the column ``meter_value`` and 1 to n\n        columns each of the form ``hdd_<heating_balance_point>``\n        and ``cdd_<cooling_balance_point>``. DataFrames of this form can be\n        made using the\n        :any:`eemeter.create_caltrack_daily_design_matrix` or\n        :any:`eemeter.create_caltrack_billing_design_matrix` methods.\n    minimum_non_zero_cdd : :any:`int`\n        Minimum allowable number of non-zero cooling degree day values.\n    minimum_non_zero_hdd : :any:`int`\n        Minimum allowable number of non-zero heating degree day values.\n    minimum_total_cdd : :any:`float`\n        Minimum allowable total sum of cooling degree day values.\n    minimum_total_hdd : :any:`float`\n        Minimum allowable total sum of heating degree day values.\n    beta_cdd_maximum_p_value : :any:`float`\n        The maximum allowable p-value of the beta cdd parameter.\n    beta_hdd_maximum_p_value : :any:`float`\n        The maximum allowable p-value of the beta hdd parameter.\n    weights_col : :any:`str` or None\n        The name of the column (if any) in ``data`` to use as weights.\n\n    Returns\n    -------\n    candidate_models : :any:`list` of :any:`CalTRACKUsagePerDayCandidateModel`\n        A list of cdd_hdd candidate models, with any associated warnings.\n    \"\"\"\n\n    cooling_balance_points = [\n        int(col[4:]) for col in data.columns if col.startswith(\"cdd\")\n    ]\n    heating_balance_points = [\n        int(col[4:]) for col in data.columns if col.startswith(\"hdd\")\n    ]\n\n    # CalTrack 3.2.2.1\n    candidate_models = [\n        get_single_cdd_hdd_candidate_model(\n            data,\n            minimum_non_zero_cdd,\n            minimum_non_zero_hdd,\n            minimum_total_cdd,\n            minimum_total_hdd,\n            beta_cdd_maximum_p_value,\n            beta_hdd_maximum_p_value,\n            weights_col,\n            cooling_balance_point,\n            heating_balance_point,\n        )\n        for cooling_balance_point in cooling_balance_points\n        for heating_balance_point in heating_balance_points\n        if heating_balance_point <= cooling_balance_point\n    ]\n    return candidate_models"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nselect and return the best candidate model based on r - squared and qualification.", "response": "def select_best_candidate(candidate_models):\n    \"\"\" Select and return the best candidate model based on r-squared and\n    qualification.\n\n    Parameters\n    ----------\n    candidate_models : :any:`list` of :any:`eemeter.CalTRACKUsagePerDayCandidateModel`\n        Candidate models to select from.\n\n    Returns\n    -------\n    (best_candidate, warnings) : :any:`tuple` of :any:`eemeter.CalTRACKUsagePerDayCandidateModel` or :any:`None` and :any:`list` of `eemeter.EEMeterWarning`\n        Return the candidate model with highest r-squared or None if none meet\n        the requirements, and a list of warnings about this selection (or lack\n        of selection).\n    \"\"\"\n    best_r_squared_adj = -np.inf\n    best_candidate = None\n\n    # CalTrack 3.4.3.3\n    for candidate in candidate_models:\n        if (\n            candidate.status == \"QUALIFIED\"\n            and candidate.r_squared_adj > best_r_squared_adj\n        ):\n            best_candidate = candidate\n            best_r_squared_adj = candidate.r_squared_adj\n\n    if best_candidate is None:\n        warnings = [\n            EEMeterWarning(\n                qualified_name=\"eemeter.caltrack_daily.select_best_candidate.no_candidates\",\n                description=\"No qualified model candidates available.\",\n                data={\n                    \"status_count:{}\".format(status): count\n                    for status, count in Counter(\n                        [c.status for c in candidate_models]\n                    ).items()\n                },\n            )\n        ]\n        return None, warnings\n\n    return best_candidate, []"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fit_caltrack_usage_per_day_model(\n    data,\n    fit_cdd=True,\n    use_billing_presets=False,\n    minimum_non_zero_cdd=10,\n    minimum_non_zero_hdd=10,\n    minimum_total_cdd=20,\n    minimum_total_hdd=20,\n    beta_cdd_maximum_p_value=1,\n    beta_hdd_maximum_p_value=1,\n    weights_col=None,\n    fit_intercept_only=True,\n    fit_cdd_only=True,\n    fit_hdd_only=True,\n    fit_cdd_hdd=True,\n):\n    \"\"\" CalTRACK daily and billing methods using a usage-per-day modeling\n    strategy.\n\n    Parameters\n    ----------\n    data : :any:`pandas.DataFrame`\n        A DataFrame containing at least the column ``meter_value`` and 1 to n\n        columns each of the form ``hdd_<heating_balance_point>``\n        and ``cdd_<cooling_balance_point>``. DataFrames of this form can be\n        made using the :any:`eemeter.create_caltrack_daily_design_matrix` or\n        :any:`eemeter.create_caltrack_billing_design_matrix` methods.\n        Should have a :any:`pandas.DatetimeIndex`.\n    fit_cdd : :any:`bool`, optional\n        If True, fit CDD models unless overridden by ``fit_cdd_only`` or\n        ``fit_cdd_hdd`` flags. Should be set to ``False`` for gas meter data.\n    use_billing_presets : :any:`bool`, optional\n        Use presets appropriate for billing models. Otherwise defaults are\n        appropriate for daily models.\n    minimum_non_zero_cdd : :any:`int`, optional\n        Minimum allowable number of non-zero cooling degree day values.\n    minimum_non_zero_hdd : :any:`int`, optional\n        Minimum allowable number of non-zero heating degree day values.\n    minimum_total_cdd : :any:`float`, optional\n        Minimum allowable total sum of cooling degree day values.\n    minimum_total_hdd : :any:`float`, optional\n        Minimum allowable total sum of heating degree day values.\n    beta_cdd_maximum_p_value : :any:`float`, optional\n        The maximum allowable p-value of the beta cdd parameter. The default\n        value is the most permissive possible (i.e., 1). This is here\n        for backwards compatibility with CalTRACK 1.0 methods.\n    beta_hdd_maximum_p_value : :any:`float`, optional\n        The maximum allowable p-value of the beta hdd parameter. The default\n        value is the most permissive possible (i.e., 1). This is here\n        for backwards compatibility with CalTRACK 1.0 methods.\n    weights_col : :any:`str` or None, optional\n        The name of the column (if any) in ``data`` to use as weights. Weight\n        must be the number of days of data in the period.\n    fit_intercept_only : :any:`bool`, optional\n        If True, fit and consider intercept_only model candidates.\n    fit_cdd_only : :any:`bool`, optional\n        If True, fit and consider cdd_only model candidates. Ignored if\n        ``fit_cdd=False``.\n    fit_hdd_only : :any:`bool`, optional\n        If True, fit and consider hdd_only model candidates.\n    fit_cdd_hdd : :any:`bool`, optional\n        If True, fit and consider cdd_hdd model candidates. Ignored if\n        ``fit_cdd=False``.\n\n    Returns\n    -------\n    model_results : :any:`eemeter.CalTRACKUsagePerDayModelResults`\n        Results of running CalTRACK daily method. See :any:`eemeter.CalTRACKUsagePerDayModelResults`\n        for more details.\n    \"\"\"\n    if use_billing_presets:\n        # CalTrack 3.2.2.2.1\n        minimum_non_zero_cdd = 0\n        minimum_non_zero_hdd = 0\n        # CalTrack 3.2.2.2.2\n        minimum_total_cdd = 20\n        minimum_total_hdd = 20\n        # CalTrack 3.4.2\n        if weights_col is None:\n            raise ValueError(\n                \"If using billing presets, the weights_col argument must be specified.\"\n            )\n        interval = \"billing\"\n    else:\n        interval = \"daily\"\n\n    # cleans data to fully NaN rows that have missing temp or meter data\n    data = overwrite_partial_rows_with_nan(data)\n\n    if data.dropna().empty:\n        return CalTRACKUsagePerDayModelResults(\n            status=\"NO DATA\",\n            method_name=\"caltrack_usage_per_day\",\n            warnings=[\n                EEMeterWarning(\n                    qualified_name=\"eemeter.caltrack_usage_per_day.no_data\",\n                    description=(\"No data available. Cannot fit model.\"),\n                    data={},\n                )\n            ],\n        )\n\n    # collect all candidate results, then validate all at once\n    # CalTrack 3.4.3.1\n    candidates = []\n\n    if fit_intercept_only:\n        candidates.extend(\n            get_intercept_only_candidate_models(data, weights_col=weights_col)\n        )\n\n    if fit_hdd_only:\n        candidates.extend(\n            get_hdd_only_candidate_models(\n                data=data,\n                minimum_non_zero_hdd=minimum_non_zero_hdd,\n                minimum_total_hdd=minimum_total_hdd,\n                beta_hdd_maximum_p_value=beta_hdd_maximum_p_value,\n                weights_col=weights_col,\n            )\n        )\n\n    # cdd models ignored for gas\n    if fit_cdd:\n        if fit_cdd_only:\n            candidates.extend(\n                get_cdd_only_candidate_models(\n                    data=data,\n                    minimum_non_zero_cdd=minimum_non_zero_cdd,\n                    minimum_total_cdd=minimum_total_cdd,\n                    beta_cdd_maximum_p_value=beta_cdd_maximum_p_value,\n                    weights_col=weights_col,\n                )\n            )\n\n        if fit_cdd_hdd:\n            candidates.extend(\n                get_cdd_hdd_candidate_models(\n                    data=data,\n                    minimum_non_zero_cdd=minimum_non_zero_cdd,\n                    minimum_non_zero_hdd=minimum_non_zero_hdd,\n                    minimum_total_cdd=minimum_total_cdd,\n                    minimum_total_hdd=minimum_total_hdd,\n                    beta_cdd_maximum_p_value=beta_cdd_maximum_p_value,\n                    beta_hdd_maximum_p_value=beta_hdd_maximum_p_value,\n                    weights_col=weights_col,\n                )\n            )\n\n    # find best candidate result\n    best_candidate, candidate_warnings = select_best_candidate(candidates)\n\n    warnings = candidate_warnings\n\n    if best_candidate is None:\n        status = \"NO MODEL\"\n        r_squared_adj = None\n    else:\n        status = \"SUCCESS\"\n        r_squared_adj = best_candidate.r_squared_adj\n\n    model_result = CalTRACKUsagePerDayModelResults(\n        status=status,\n        method_name=\"caltrack_usage_per_day\",\n        interval=interval,\n        model=best_candidate,\n        candidates=candidates,\n        r_squared_adj=r_squared_adj,\n        warnings=warnings,\n        settings={\n            \"fit_cdd\": fit_cdd,\n            \"minimum_non_zero_cdd\": minimum_non_zero_cdd,\n            \"minimum_non_zero_hdd\": minimum_non_zero_hdd,\n            \"minimum_total_cdd\": minimum_total_cdd,\n            \"minimum_total_hdd\": minimum_total_hdd,\n            \"beta_cdd_maximum_p_value\": beta_cdd_maximum_p_value,\n            \"beta_hdd_maximum_p_value\": beta_hdd_maximum_p_value,\n        },\n    )\n\n    if best_candidate is not None:\n        if best_candidate.model_type in [\"cdd_hdd\"]:\n            num_parameters = 2\n        elif best_candidate.model_type in [\"hdd_only\", \"cdd_only\"]:\n            num_parameters = 1\n        else:\n            num_parameters = 0\n\n        predicted_avgs = _caltrack_predict_design_matrix(\n            best_candidate.model_type,\n            best_candidate.model_params,\n            data,\n            input_averages=True,\n            output_averages=True,\n        )\n        model_result.avgs_metrics = ModelMetrics(\n            data.meter_value, predicted_avgs, num_parameters\n        )\n\n        predicted_totals = _caltrack_predict_design_matrix(\n            best_candidate.model_type,\n            best_candidate.model_params,\n            data,\n            input_averages=True,\n            output_averages=False,\n        )\n\n        days_per_period = day_counts(data.index)\n        data_totals = data.meter_value * days_per_period\n        model_result.totals_metrics = ModelMetrics(\n            data_totals, predicted_totals, num_parameters\n        )\n\n    return model_result", "response": "Fit a CalTRACK usage - per - day model to the given data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef caltrack_sufficiency_criteria(\n    data_quality,\n    requested_start,\n    requested_end,\n    num_days=365,\n    min_fraction_daily_coverage=0.9,  # TODO: needs to be per year\n    min_fraction_hourly_temperature_coverage_per_period=0.9,\n):\n    \"\"\"CalTRACK daily data sufficiency criteria.\n\n    .. note::\n\n        For CalTRACK compliance, ``min_fraction_daily_coverage`` must be set\n        at ``0.9`` (section 2.2.1.2), and requested_start and requested_end must\n        not be None (section 2.2.4).\n\n\n    Parameters\n    ----------\n    data_quality : :any:`pandas.DataFrame`\n        A DataFrame containing at least the column ``meter_value`` and the two\n        columns ``temperature_null``, containing a count of null hourly\n        temperature values for each meter value, and ``temperature_not_null``,\n        containing a count of not-null hourly temperature values for each\n        meter value. Should have a :any:`pandas.DatetimeIndex`.\n    requested_start : :any:`datetime.datetime`, timezone aware (or :any:`None`)\n        The desired start of the period, if any, especially if this is\n        different from the start of the data. If given, warnings\n        are reported on the basis of this start date instead of data start\n        date. Must be explicitly set to ``None`` in order to use data start date.\n    requested_end : :any:`datetime.datetime`, timezone aware (or :any:`None`)\n        The desired end of the period, if any, especially if this is\n        different from the end of the data. If given, warnings\n        are reported on the basis of this end date instead of data end date.\n        Must be explicitly set to ``None`` in order to use data end date.\n    num_days : :any:`int`, optional\n        Exact number of days allowed in data, including extent given by\n        ``requested_start`` or ``requested_end``, if given.\n    min_fraction_daily_coverage : :any:, optional\n        Minimum fraction of days of data in total data extent for which data\n        must be available.\n    min_fraction_hourly_temperature_coverage_per_period=0.9,\n        Minimum fraction of hours of temperature data coverage in a particular\n        period. Anything below this causes the whole period to be considered\n        considered missing.\n\n    Returns\n    -------\n    data_sufficiency : :any:`eemeter.DataSufficiency`\n        The an object containing sufficiency status and warnings for this data.\n    \"\"\"\n    criteria_name = \"caltrack_sufficiency_criteria\"\n\n    if data_quality.dropna().empty:\n        return DataSufficiency(\n            status=\"NO DATA\",\n            criteria_name=criteria_name,\n            warnings=[\n                EEMeterWarning(\n                    qualified_name=\"eemeter.caltrack_sufficiency_criteria.no_data\",\n                    description=(\"No data available.\"),\n                    data={},\n                )\n            ],\n        )\n\n    data_start = data_quality.index.min().tz_convert(\"UTC\")\n    data_end = data_quality.index.max().tz_convert(\"UTC\")\n    n_days_data = (data_end - data_start).days\n\n    if requested_start is not None:\n        # check for gap at beginning\n        requested_start = requested_start.astimezone(pytz.UTC)\n        n_days_start_gap = (data_start - requested_start).days\n    else:\n        n_days_start_gap = 0\n\n    if requested_end is not None:\n        # check for gap at end\n        requested_end = requested_end.astimezone(pytz.UTC)\n        n_days_end_gap = (requested_end - data_end).days\n    else:\n        n_days_end_gap = 0\n\n    critical_warnings = []\n\n    if n_days_end_gap < 0:\n        # CalTRACK 2.2.4\n        critical_warnings.append(\n            EEMeterWarning(\n                qualified_name=(\n                    \"eemeter.caltrack_sufficiency_criteria\"\n                    \".extra_data_after_requested_end_date\"\n                ),\n                description=(\"Extra data found after requested end date.\"),\n                data={\n                    \"requested_end\": requested_end.isoformat(),\n                    \"data_end\": data_end.isoformat(),\n                },\n            )\n        )\n        n_days_end_gap = 0\n\n    if n_days_start_gap < 0:\n        # CalTRACK 2.2.4\n        critical_warnings.append(\n            EEMeterWarning(\n                qualified_name=(\n                    \"eemeter.caltrack_sufficiency_criteria\"\n                    \".extra_data_before_requested_start_date\"\n                ),\n                description=(\"Extra data found before requested start date.\"),\n                data={\n                    \"requested_start\": requested_start.isoformat(),\n                    \"data_start\": data_start.isoformat(),\n                },\n            )\n        )\n        n_days_start_gap = 0\n\n    n_days_total = n_days_data + n_days_start_gap + n_days_end_gap\n\n    n_negative_meter_values = data_quality.meter_value[\n        data_quality.meter_value < 0\n    ].shape[0]\n\n    if n_negative_meter_values > 0:\n        # CalTrack 2.3.5\n        critical_warnings.append(\n            EEMeterWarning(\n                qualified_name=(\n                    \"eemeter.caltrack_sufficiency_criteria\" \".negative_meter_values\"\n                ),\n                description=(\n                    \"Found negative meter data values, which may indicate presence\"\n                    \" of solar net metering.\"\n                ),\n                data={\"n_negative_meter_values\": n_negative_meter_values},\n            )\n        )\n\n    # TODO(philngo): detect and report unsorted or repeated values.\n\n    # create masks showing which daily or billing periods meet criteria\n    valid_meter_value_rows = data_quality.meter_value.notnull()\n    valid_temperature_rows = (\n        data_quality.temperature_not_null\n        / (data_quality.temperature_not_null + data_quality.temperature_null)\n    ) > min_fraction_hourly_temperature_coverage_per_period\n    valid_rows = valid_meter_value_rows & valid_temperature_rows\n\n    # get number of days per period - for daily this should be a series of ones\n    row_day_counts = day_counts(data_quality.index)\n\n    # apply masks, giving total\n    n_valid_meter_value_days = int((valid_meter_value_rows * row_day_counts).sum())\n    n_valid_temperature_days = int((valid_temperature_rows * row_day_counts).sum())\n    n_valid_days = int((valid_rows * row_day_counts).sum())\n\n    median = data_quality.meter_value.median()\n    upper_quantile = data_quality.meter_value.quantile(.75)\n    lower_quantile = data_quality.meter_value.quantile(.25)\n    iqr = upper_quantile - lower_quantile\n    extreme_value_limit = median + (3 * iqr)\n    n_extreme_values = data_quality.meter_value[\n        data_quality.meter_value > extreme_value_limit\n    ].shape[0]\n    max_value = float(data_quality.meter_value.max())\n\n    if n_days_total > 0:\n        fraction_valid_meter_value_days = n_valid_meter_value_days / float(n_days_total)\n        fraction_valid_temperature_days = n_valid_temperature_days / float(n_days_total)\n        fraction_valid_days = n_valid_days / float(n_days_total)\n    else:\n        # unreachable, I think.\n        fraction_valid_meter_value_days = 0\n        fraction_valid_temperature_days = 0\n        fraction_valid_days = 0\n\n    if n_days_total != num_days:\n        critical_warnings.append(\n            EEMeterWarning(\n                qualified_name=(\n                    \"eemeter.caltrack_sufficiency_criteria\"\n                    \".incorrect_number_of_total_days\"\n                ),\n                description=(\"Total data span does not match the required value.\"),\n                data={\"num_days\": num_days, \"n_days_total\": n_days_total},\n            )\n        )\n\n    if fraction_valid_days < min_fraction_daily_coverage:\n        critical_warnings.append(\n            EEMeterWarning(\n                qualified_name=(\n                    \"eemeter.caltrack_sufficiency_criteria\"\n                    \".too_many_days_with_missing_data\"\n                ),\n                description=(\n                    \"Too many days in data have missing meter data or\"\n                    \" temperature data.\"\n                ),\n                data={\"n_valid_days\": n_valid_days, \"n_days_total\": n_days_total},\n            )\n        )\n\n    if fraction_valid_meter_value_days < min_fraction_daily_coverage:\n        critical_warnings.append(\n            EEMeterWarning(\n                qualified_name=(\n                    \"eemeter.caltrack_sufficiency_criteria\"\n                    \".too_many_days_with_missing_meter_data\"\n                ),\n                description=(\"Too many days in data have missing meter data.\"),\n                data={\n                    \"n_valid_meter_data_days\": n_valid_meter_value_days,\n                    \"n_days_total\": n_days_total,\n                },\n            )\n        )\n\n    if fraction_valid_temperature_days < min_fraction_daily_coverage:\n        critical_warnings.append(\n            EEMeterWarning(\n                qualified_name=(\n                    \"eemeter.caltrack_sufficiency_criteria\"\n                    \".too_many_days_with_missing_temperature_data\"\n                ),\n                description=(\"Too many days in data have missing temperature data.\"),\n                data={\n                    \"n_valid_temperature_data_days\": n_valid_temperature_days,\n                    \"n_days_total\": n_days_total,\n                },\n            )\n        )\n\n    if len(critical_warnings) > 0:\n        status = \"FAIL\"\n    else:\n        status = \"PASS\"\n\n    non_critical_warnings = []\n    if n_extreme_values > 0:\n        # CalTRACK 2.3.6\n        non_critical_warnings.append(\n            EEMeterWarning(\n                qualified_name=(\n                    \"eemeter.caltrack_sufficiency_criteria\" \".extreme_values_detected\"\n                ),\n                description=(\n                    \"Extreme values (greater than (median + (3 * IQR)),\"\n                    \" must be flagged for manual review.\"\n                ),\n                data={\n                    \"n_extreme_values\": n_extreme_values,\n                    \"median\": median,\n                    \"upper_quantile\": upper_quantile,\n                    \"lower_quantile\": lower_quantile,\n                    \"extreme_value_limit\": extreme_value_limit,\n                    \"max_value\": max_value,\n                },\n            )\n        )\n\n    warnings = critical_warnings + non_critical_warnings\n\n    return DataSufficiency(\n        status=status,\n        criteria_name=criteria_name,\n        warnings=warnings,\n        settings={\n            \"num_days\": num_days,\n            \"min_fraction_daily_coverage\": min_fraction_daily_coverage,\n            \"min_fraction_hourly_temperature_coverage_per_period\": min_fraction_hourly_temperature_coverage_per_period,\n        },\n    )", "response": "Return a new catalydar sufficiency criteria for a given set of data items."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot_caltrack_candidate(\n    candidate,\n    best=False,\n    ax=None,\n    title=None,\n    figsize=None,\n    temp_range=None,\n    alpha=None,\n    **kwargs\n):\n    \"\"\" Plot a CalTRACK candidate model.\n\n    Parameters\n    ----------\n    candidate : :any:`eemeter.CalTRACKUsagePerDayCandidateModel`\n        A candidate model with a predict function.\n    best : :any:`bool`, optional\n        Whether this is the best candidate or not.\n    ax : :any:`matplotlib.axes.Axes`, optional\n        Existing axes to plot on.\n    title : :any:`str`, optional\n        Chart title.\n    figsize : :any:`tuple`, optional\n        (width, height) of chart.\n    temp_range : :any:`tuple`, optional\n        (min, max) temperatures to plot model.\n    alpha : :any:`float` between 0 and 1, optional\n        Transparency, 0 fully transparent, 1 fully opaque.\n    **kwargs\n        Keyword arguments for :any:`matplotlib.axes.Axes.plot`\n\n    Returns\n    -------\n    ax : :any:`matplotlib.axes.Axes`\n        Matplotlib axes.\n    \"\"\"\n    try:\n        import matplotlib.pyplot as plt\n    except ImportError:  # pragma: no cover\n        raise ImportError(\"matplotlib is required for plotting.\")\n\n    if figsize is None:\n        figsize = (10, 4)\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    if candidate.status == \"QUALIFIED\":\n        color = \"C2\"\n    elif candidate.status == \"DISQUALIFIED\":\n        color = \"C3\"\n    else:\n        return\n\n    if best:\n        color = \"C1\"\n        alpha = 1\n\n    temp_min, temp_max = (30, 90) if temp_range is None else temp_range\n\n    temps = np.arange(temp_min, temp_max)\n\n    data = {\"n_days\": np.ones(temps.shape)}\n\n    prediction_index = pd.date_range(\n        \"2017-01-01T00:00:00Z\", periods=len(temps), freq=\"D\"\n    )\n\n    temps_hourly = pd.Series(temps, index=prediction_index).resample(\"H\").ffill()\n\n    prediction = candidate.predict(\n        prediction_index, temps_hourly, \"daily\"\n    ).result.predicted_usage\n\n    plot_kwargs = {\"color\": color, \"alpha\": alpha or 0.3}\n    plot_kwargs.update(kwargs)\n\n    ax.plot(temps, prediction, **plot_kwargs)\n\n    if title is not None:\n        ax.set_title(title)\n\n    return ax", "response": "Plots a CalTRACK candidate model."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a JSON - serializable representation of this result.", "response": "def json(self, with_candidates=False):\n        \"\"\" Return a JSON-serializable representation of this result.\n\n        The output of this function can be converted to a serialized string\n        with :any:`json.dumps`.\n        \"\"\"\n\n        def _json_or_none(obj):\n            return None if obj is None else obj.json()\n\n        data = {\n            \"status\": self.status,\n            \"method_name\": self.method_name,\n            \"interval\": self.interval,\n            \"model\": _json_or_none(self.model),\n            \"r_squared_adj\": _noneify(self.r_squared_adj),\n            \"warnings\": [w.json() for w in self.warnings],\n            \"metadata\": self.metadata,\n            \"settings\": self.settings,\n            \"totals_metrics\": _json_or_none(self.totals_metrics),\n            \"avgs_metrics\": _json_or_none(self.avgs_metrics),\n            \"candidates\": None,\n        }\n        if with_candidates:\n            data[\"candidates\"] = [candidate.json() for candidate in self.candidates]\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplotting a model fit.", "response": "def plot(\n        self,\n        ax=None,\n        title=None,\n        figsize=None,\n        with_candidates=False,\n        candidate_alpha=None,\n        temp_range=None,\n    ):\n        \"\"\" Plot a model fit.\n\n        Parameters\n        ----------\n        ax : :any:`matplotlib.axes.Axes`, optional\n            Existing axes to plot on.\n        title : :any:`str`, optional\n            Chart title.\n        figsize : :any:`tuple`, optional\n            (width, height) of chart.\n        with_candidates : :any:`bool`\n            If True, also plot candidate models.\n        candidate_alpha : :any:`float` between 0 and 1\n            Transparency at which to plot candidate models. 0 fully transparent,\n            1 fully opaque.\n\n        Returns\n        -------\n        ax : :any:`matplotlib.axes.Axes`\n            Matplotlib axes.\n        \"\"\"\n        try:\n            import matplotlib.pyplot as plt\n        except ImportError:  # pragma: no cover\n            raise ImportError(\"matplotlib is required for plotting.\")\n\n        if figsize is None:\n            figsize = (10, 4)\n\n        if ax is None:\n            fig, ax = plt.subplots(figsize=figsize)\n\n        if temp_range is None:\n            temp_range = (20, 90)\n\n        if with_candidates:\n            for candidate in self.candidates:\n                candidate.plot(ax=ax, temp_range=temp_range, alpha=candidate_alpha)\n        self.model.plot(ax=ax, best=True, temp_range=temp_range)\n\n        if title is not None:\n            ax.set_title(title)\n\n        return ax"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a JSON - serializable representation of this result.", "response": "def json(self):\n        \"\"\" Return a JSON-serializable representation of this result.\n\n        The output of this function can be converted to a serialized string\n        with :any:`json.dumps`.\n        \"\"\"\n        return {\n            \"model_type\": self.model_type,\n            \"formula\": self.formula,\n            \"status\": self.status,\n            \"model_params\": self.model_params,\n            \"r_squared_adj\": _noneify(self.r_squared_adj),\n            \"warnings\": [w.json() for w in self.warnings],\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef predict(\n        self,\n        prediction_index,\n        temperature_data,\n        with_disaggregated=False,\n        with_design_matrix=False,\n        **kwargs\n    ):\n        \"\"\" Predict\n        \"\"\"\n        return caltrack_usage_per_day_predict(\n            self.model_type,\n            self.model_params,\n            prediction_index,\n            temperature_data,\n            with_disaggregated=with_disaggregated,\n            with_design_matrix=with_design_matrix,\n            **kwargs\n        )", "response": "Predicts the usage per day of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot(\n        self,\n        best=False,\n        ax=None,\n        title=None,\n        figsize=None,\n        temp_range=None,\n        alpha=None,\n        **kwargs\n    ):\n        \"\"\" Plot\n        \"\"\"\n        return plot_caltrack_candidate(\n            self,\n            best=best,\n            ax=ax,\n            title=title,\n            figsize=figsize,\n            temp_range=temp_range,\n            alpha=alpha,\n            **kwargs\n        )", "response": "Plot the calibrated candidate calendartrack."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a JSON - serializable representation of this result.", "response": "def json(self):\n        \"\"\" Return a JSON-serializable representation of this result.\n\n        The output of this function can be converted to a serialized string\n        with :any:`json.dumps`.\n        \"\"\"\n        return {\n            \"status\": self.status,\n            \"criteria_name\": self.criteria_name,\n            \"warnings\": [w.json() for w in self.warnings],\n            \"settings\": self.settings,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_sample(sample):\n    sample_metadata = _load_sample_metadata()\n    metadata = sample_metadata.get(sample)\n    if metadata is None:\n        raise ValueError(\n            \"Sample not found: {}. Try one of these?\\n{}\".format(\n                sample,\n                \"\\n\".join(\n                    [\" - {}\".format(key) for key in sorted(sample_metadata.keys())]\n                ),\n            )\n        )\n\n    freq = metadata.get(\"freq\")\n    if freq not in (\"hourly\", \"daily\"):\n        freq = None\n\n    meter_data_filename = metadata[\"meter_data_filename\"]\n    with resource_stream(\"eemeter.samples\", meter_data_filename) as f:\n        meter_data = meter_data_from_csv(f, gzipped=True, freq=freq)\n\n    temperature_filename = metadata[\"temperature_filename\"]\n    with resource_stream(\"eemeter.samples\", temperature_filename) as f:\n        temperature_data = temperature_data_from_csv(f, gzipped=True, freq=\"hourly\")\n\n    metadata[\"blackout_start_date\"] = pytz.UTC.localize(\n        parse_date(metadata[\"blackout_start_date\"])\n    )\n    metadata[\"blackout_end_date\"] = pytz.UTC.localize(\n        parse_date(metadata[\"blackout_end_date\"])\n    )\n\n    return meter_data, temperature_data, metadata", "response": "Loads meter data temperature and metadata for a specific sample identifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nplots meter and temperature data in dual - axes time series.", "response": "def plot_time_series(meter_data, temperature_data, **kwargs):\n    \"\"\" Plot meter and temperature data in dual-axes time series.\n\n    Parameters\n    ----------\n    meter_data : :any:`pandas.DataFrame`\n        A :any:`pandas.DatetimeIndex`-indexed DataFrame of meter data with the column ``value``.\n    temperature_data : :any:`pandas.Series`\n        A :any:`pandas.DatetimeIndex`-indexed Series of temperature data.\n    **kwargs\n        Arbitrary keyword arguments to pass to\n        :any:`plt.subplots <matplotlib.pyplot.subplots>`\n\n    Returns\n    -------\n    axes : :any:`tuple` of :any:`matplotlib.axes.Axes`\n        Tuple of ``(ax_meter_data, ax_temperature_data)``.\n    \"\"\"\n    # TODO(philngo): include image in docs.\n    try:\n        import matplotlib.pyplot as plt\n    except ImportError:  # pragma: no cover\n        raise ImportError(\"matplotlib is required for plotting.\")\n\n    default_kwargs = {\"figsize\": (16, 4)}\n    default_kwargs.update(kwargs)\n    fig, ax1 = plt.subplots(**default_kwargs)\n\n    ax1.plot(\n        meter_data.index,\n        meter_data.value,\n        color=\"C0\",\n        label=\"Energy Use\",\n        drawstyle=\"steps-post\",\n    )\n    ax1.set_ylabel(\"Energy Use\")\n\n    ax2 = ax1.twinx()\n    ax2.plot(\n        temperature_data.index,\n        temperature_data,\n        color=\"C1\",\n        label=\"Temperature\",\n        alpha=0.8,\n    )\n    ax2.set_ylabel(\"Temperature\")\n\n    fig.legend()\n\n    return ax1, ax2"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplotting the meter and temperature data in energy signature.", "response": "def plot_energy_signature(\n    meter_data,\n    temperature_data,\n    temp_col=None,\n    ax=None,\n    title=None,\n    figsize=None,\n    **kwargs\n):\n    \"\"\" Plot meter and temperature data in energy signature.\n\n    Parameters\n    ----------\n    meter_data : :any:`pandas.DataFrame`\n        A :any:`pandas.DatetimeIndex`-indexed DataFrame of meter data with the column ``value``.\n    temperature_data : :any:`pandas.Series`\n        A :any:`pandas.DatetimeIndex`-indexed Series of temperature data.\n    temp_col : :any:`str`, default ``'temperature_mean'``\n        The name of the temperature column.\n    ax : :any:`matplotlib.axes.Axes`\n        The axis on which to plot.\n    title : :any:`str`, optional\n        Chart title.\n    figsize : :any:`tuple`, optional\n        (width, height) of chart.\n    **kwargs\n        Arbitrary keyword arguments to pass to\n        :any:`matplotlib.axes.Axes.scatter`.\n\n    Returns\n    -------\n    ax : :any:`matplotlib.axes.Axes`\n        Matplotlib axes.\n    \"\"\"\n    try:\n        import matplotlib.pyplot as plt\n    except ImportError:  # pragma: no cover\n        raise ImportError(\"matplotlib is required for plotting.\")\n\n    # format data\n    temperature_mean = compute_temperature_features(meter_data.index, temperature_data)\n    usage_per_day = compute_usage_per_day_feature(meter_data, series_name=\"meter_value\")\n    df = merge_features([usage_per_day, temperature_mean.temperature_mean])\n\n    if figsize is None:\n        figsize = (10, 4)\n\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n\n    if temp_col is None:\n        temp_col = \"temperature_mean\"\n\n    ax.scatter(df[temp_col], df.meter_value, **kwargs)\n    ax.set_xlabel(\"Temperature\")\n    ax.set_ylabel(\"Energy Use per Day\")\n\n    if title is not None:\n        ax.set_title(title)\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a number to a float.", "response": "def _json_safe_float(number):\n    \"\"\"\n    JSON serialization for infinity can be problematic.\n    See https://docs.python.org/2/library/json.html#basic-usage\n    This function returns None if `number` is infinity or negative infinity.\n\n    If the `number` cannot be converted to float, this will raise an exception.\n    \"\"\"\n    if number is None:\n        return None\n\n    if isinstance(number, float):\n        return None if np.isinf(number) or np.isnan(number) else number\n\n    # errors if number is not float compatible\n    return float(number)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef json(self):\n        return {\n            \"observed_length\": _json_safe_float(self.observed_length),\n            \"predicted_length\": _json_safe_float(self.predicted_length),\n            \"merged_length\": _json_safe_float(self.merged_length),\n            \"num_parameters\": _json_safe_float(self.num_parameters),\n            \"observed_mean\": _json_safe_float(self.observed_mean),\n            \"predicted_mean\": _json_safe_float(self.predicted_mean),\n            \"observed_variance\": _json_safe_float(self.observed_variance),\n            \"predicted_variance\": _json_safe_float(self.predicted_variance),\n            \"observed_skew\": _json_safe_float(self.observed_skew),\n            \"predicted_skew\": _json_safe_float(self.predicted_skew),\n            \"observed_kurtosis\": _json_safe_float(self.observed_kurtosis),\n            \"predicted_kurtosis\": _json_safe_float(self.predicted_kurtosis),\n            \"observed_cvstd\": _json_safe_float(self.observed_cvstd),\n            \"predicted_cvstd\": _json_safe_float(self.predicted_cvstd),\n            \"r_squared\": _json_safe_float(self.r_squared),\n            \"r_squared_adj\": _json_safe_float(self.r_squared_adj),\n            \"rmse\": _json_safe_float(self.rmse),\n            \"rmse_adj\": _json_safe_float(self.rmse_adj),\n            \"cvrmse\": _json_safe_float(self.cvrmse),\n            \"cvrmse_adj\": _json_safe_float(self.cvrmse_adj),\n            \"mape\": _json_safe_float(self.mape),\n            \"mape_no_zeros\": _json_safe_float(self.mape_no_zeros),\n            \"num_meter_zeros\": _json_safe_float(self.num_meter_zeros),\n            \"nmae\": _json_safe_float(self.nmae),\n            \"nmbe\": _json_safe_float(self.nmbe),\n            \"autocorr_resid\": _json_safe_float(self.autocorr_resid),\n        }", "response": "Return a JSON - serializable representation of this result."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef json(self):\n\n        data = {\n            \"segment_name\": self.segment_name,\n            \"formula\": self.formula,\n            \"warnings\": [w.json() for w in self.warnings],\n            \"model_params\": self.model_params,\n        }\n        return data", "response": "Return a JSON - serializable representation of this result."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a JSON - serializable representation of this result.", "response": "def json(self):\n        \"\"\" Return a JSON-serializable representation of this result.\n\n        The output of this function can be converted to a serialized string\n        with :any:`json.dumps`.\n        \"\"\"\n\n        def _json_or_none(obj):\n            return None if obj is None else obj.json()\n\n        data = {\n            \"segment_models\": [_json_or_none(m) for m in self.segment_models],\n            \"model_lookup\": {\n                key: _json_or_none(val) for key, val in self.model_lookup.items()\n            },\n            \"prediction_segment_type\": self.prediction_segment_type,\n            \"prediction_segment_name_mapping\": self.prediction_segment_name_mapping,\n            \"prediction_feature_processor\": self.prediction_feature_processor.__name__,\n        }\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef meter_data_from_csv(\n    filepath_or_buffer,\n    tz=None,\n    start_col=\"start\",\n    value_col=\"value\",\n    gzipped=False,\n    freq=None,\n    **kwargs\n):\n    \"\"\" Load meter data from a CSV file.\n\n    Default format::\n\n        start,value\n        2017-01-01T00:00:00+00:00,0.31\n        2017-01-02T00:00:00+00:00,0.4\n        2017-01-03T00:00:00+00:00,0.58\n\n    Parameters\n    ----------\n    filepath_or_buffer : :any:`str` or file-handle\n        File path or object.\n    tz : :any:`str`, optional\n        E.g., ``'UTC'`` or ``'US/Pacific'``\n    start_col : :any:`str`, optional, default ``'start'``\n        Date period start column.\n    value_col : :any:`str`, optional, default ``'value'``\n        Value column, can be in any unit.\n    gzipped : :any:`bool`, optional\n        Whether file is gzipped.\n    freq : :any:`str`, optional\n        If given, apply frequency to data using :any:`pandas.DataFrame.resample`.\n    **kwargs\n        Extra keyword arguments to pass to :any:`pandas.read_csv`, such as\n        ``sep='|'``.\n    \"\"\"\n\n    read_csv_kwargs = {\n        \"usecols\": [start_col, value_col],\n        \"dtype\": {value_col: np.float64},\n        \"parse_dates\": [start_col],\n        \"index_col\": start_col,\n    }\n\n    if gzipped:\n        read_csv_kwargs.update({\"compression\": \"gzip\"})\n\n    # allow passing extra kwargs\n    read_csv_kwargs.update(kwargs)\n\n    df = pd.read_csv(filepath_or_buffer, **read_csv_kwargs).tz_localize(\"UTC\")\n    if tz is not None:\n        df = df.tz_convert(tz)\n\n    if freq == \"hourly\":\n        df = df.resample(\"H\").sum()\n    elif freq == \"daily\":\n        df = df.resample(\"D\").sum()\n\n    return df", "response": "Load meter data from a CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef temperature_data_from_csv(\n    filepath_or_buffer,\n    tz=None,\n    date_col=\"dt\",\n    temp_col=\"tempF\",\n    gzipped=False,\n    freq=None,\n    **kwargs\n):\n    \"\"\" Load temperature data from a CSV file.\n\n    Default format::\n\n        dt,tempF\n        2017-01-01T00:00:00+00:00,21\n        2017-01-01T01:00:00+00:00,22.5\n        2017-01-01T02:00:00+00:00,23.5\n\n    Parameters\n    ----------\n    filepath_or_buffer : :any:`str` or file-handle\n        File path or object.\n    tz : :any:`str`, optional\n        E.g., ``'UTC'`` or ``'US/Pacific'``\n    date_col : :any:`str`, optional, default ``'dt'``\n        Date period start column.\n    temp_col : :any:`str`, optional, default ``'tempF'``\n        Temperature column.\n    gzipped : :any:`bool`, optional\n        Whether file is gzipped.\n    freq : :any:`str`, optional\n        If given, apply frequency to data using :any:`pandas.Series.resample`.\n    **kwargs\n        Extra keyword arguments to pass to :any:`pandas.read_csv`, such as\n        ``sep='|'``.\n    \"\"\"\n    read_csv_kwargs = {\n        \"usecols\": [date_col, temp_col],\n        \"dtype\": {temp_col: np.float64},\n        \"parse_dates\": [date_col],\n        \"index_col\": date_col,\n    }\n\n    if gzipped:\n        read_csv_kwargs.update({\"compression\": \"gzip\"})\n\n    # allow passing extra kwargs\n    read_csv_kwargs.update(kwargs)\n\n    if tz is None:\n        tz = \"UTC\"\n\n    df = pd.read_csv(filepath_or_buffer, **read_csv_kwargs).tz_localize(tz)\n\n    if freq == \"hourly\":\n        df = df.resample(\"H\").sum()\n\n    return df[temp_col]", "response": "Load temperature data from a CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef meter_data_from_json(data, orient=\"list\"):\n    if orient == \"list\":\n        df = pd.DataFrame(data, columns=[\"start\", \"value\"])\n        df[\"start\"] = pd.DatetimeIndex(df.start).tz_localize(\"UTC\")\n        df = df.set_index(\"start\")\n        return df\n    else:\n        raise ValueError(\"orientation not recognized.\")", "response": "Load meter data from json format."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads temperature data from json format.", "response": "def temperature_data_from_json(data, orient=\"list\"):\n    \"\"\" Load temperature data from json. (Must be given in degrees\n    Fahrenheit).\n\n    Default format::\n\n        [\n            ['2017-01-01T00:00:00+00:00', 3.5],\n            ['2017-01-01T01:00:00+00:00', 5.4],\n            ['2017-01-01T02:00:00+00:00', 7.4],\n        ]\n\n    Parameters\n    ----------\n    data : :any:`list`\n        List elements are each a rows of data.\n\n    Returns\n    -------\n    series : :any:`pandas.Series`\n        DataFrame with a single column (``'tempF'``) and a\n        :any:`pandas.DatetimeIndex`.\n    \"\"\"\n    if orient == \"list\":\n        df = pd.DataFrame(data, columns=[\"dt\", \"tempF\"])\n        series = df.tempF\n        series.index = pd.DatetimeIndex(df.dt).tz_localize(\"UTC\")\n        return series\n    else:\n        raise ValueError(\"orientation not recognized.\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef meter_data_to_csv(meter_data, path_or_buf):\n    if meter_data.index.name is None:\n        meter_data.index.name = \"start\"\n    return meter_data.to_csv(path_or_buf, index=True)", "response": "Write meter data to CSV."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef temperature_data_to_csv(temperature_data, path_or_buf):\n    if temperature_data.index.name is None:\n        temperature_data.index.name = \"dt\"\n    if temperature_data.name is None:\n        temperature_data.name = \"temperature\"\n    return temperature_data.to_frame().to_csv(path_or_buf, index=True)", "response": "Write temperature data to CSV."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef notify(self):\n        if flask.has_request_context():\n            emit(_NAME + str(self._uuid))\n        else:\n            sio = flask.current_app.extensions['socketio']\n            sio.emit(_NAME + str(self._uuid))\n        eventlet.sleep()", "response": "Notify the client.\n\n        The function passed to ``App.respond`` will get called."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks that the key is a string or bytestring.", "response": "def validate(key):\n    \"\"\"Check that the key is a string or bytestring.\n\n    That's the only valid type of key.\n    \"\"\"\n    if not isinstance(key, (str, bytes)):\n        raise KeyError('Key must be of type str or bytes, found type {}'.format(type(key)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef json_conversion(obj: Any) -> JSON:\n    try:\n        # numpy isn't an explicit dependency of bowtie\n        # so we can't assume it's available\n        import numpy as np\n        if isinstance(obj, (np.ndarray, np.generic)):\n            return obj.tolist()\n    except ImportError:\n        pass\n\n    try:\n        # pandas isn't an explicit dependency of bowtie\n        # so we can't assume it's available\n        import pandas as pd\n        if isinstance(obj, pd.DatetimeIndex):\n            return [x.isoformat() for x in obj.to_pydatetime()]\n        if isinstance(obj, pd.Index):\n            return obj.tolist()\n        if isinstance(obj, pd.Series):\n            try:\n                return [x.isoformat() for x in obj.dt.to_pydatetime()]\n            except AttributeError:\n                return obj.tolist()\n    except ImportError:\n        pass\n\n    if isinstance(obj, (datetime, time, date)):\n        return obj.isoformat()\n    raise TypeError('Not sure how to serialize {} of type {}'.format(obj, type(obj)))", "response": "Encode additional objects to JSON."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nencodes x into msgpack with additional encoders.", "response": "def pack(x: Any) -> bytes:\n    \"\"\"Encode ``x`` into msgpack with additional encoders.\"\"\"\n    try:\n        return msgpack.packb(x, default=encoders)\n    except TypeError as exc:\n        message = ('Serialization error, check the data passed to a do_ command. '\n                   'Cannot serialize this object:\\n') + str(exc)[16:]\n        raise SerializationError(message)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating an event from a method signature.", "response": "def make_event(event: Callable) -> Callable:\n    \"\"\"Create an event from a method signature.\"\"\"\n    @property  # type: ignore\n    @wraps(event)\n    def actualevent(self):  # pylint: disable=missing-docstring\n        name = event.__name__[3:]\n        try:\n            # the getter post processing function\n            # is preserved with an underscore\n            getter = event(self).__name__\n        except AttributeError:\n            getter = None\n        return Event(name, self._uuid, getter)  # pylint: disable=protected-access\n\n    return actualevent"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a command from a method signature.", "response": "def make_command(command: Callable) -> Callable:\n    \"\"\"Create an command from a method signature.\"\"\"\n    @wraps(command)\n    def actualcommand(self, *args, **kwds):  # pylint: disable=missing-docstring\n        data = command(self, *args, **kwds)\n        name = command.__name__[3:]\n        signal = '{uuid}{sep}{event}'.format(\n            uuid=self._uuid,  # pylint: disable=protected-access\n            sep=SEPARATOR,\n            event=name\n        )\n        if flask.has_request_context():\n            emit(signal, {'data': pack(data)})\n        else:\n            sio = flask.current_app.extensions['socketio']\n            sio.emit(signal, {'data': pack(data)})\n        eventlet.sleep()\n\n    return actualcommand"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating an command from a method signature.", "response": "def make_getter(getter: Callable) -> Callable:\n    \"\"\"Create an command from a method signature.\"\"\"\n    def get(self, timeout=10):  # pylint: disable=missing-docstring\n        name = getter.__name__\n        signal = '{uuid}{sep}{event}'.format(\n            uuid=self._uuid,  # pylint: disable=protected-access\n            sep=SEPARATOR,\n            event=name\n        )\n        event = LightQueue(1)\n        if flask.has_request_context():\n            emit(signal, callback=lambda x: event.put(unpack(x)))\n        else:\n            sio = flask.current_app.extensions['socketio']\n            sio.emit(signal, callback=lambda x: event.put(unpack(x)))\n        data = event.get(timeout=timeout)\n        return getter(self, data)\n\n    # don't want to copy the signature in this case\n    get.__doc__ = getter.__doc__\n\n    return get"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninsert the component tag into the wrapper html.", "response": "def _insert(wrap: str, tag: Optional[str]) -> str:\n        \"\"\"Insert the component tag into the wrapper html.\n\n        This ignores other tags already created like ``{socket}``.\n\n        https://stackoverflow.com/a/11284026/744520\n        \"\"\"\n        if tag is None:\n            raise ValueError('tag cannot be None')\n        formatter = string.Formatter()\n        mapping = FormatDict(component=tag)\n        return formatter.vformat(wrap, (), mapping)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef do_options(self, labels, values):\n        return [dict(label=l, value=v) for l, v in zip(labels, values)]", "response": "Replace the drop down fields."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef do_options(self, labels: Sequence[str],\n                   values: Sequence[Union[str, int]]) -> Sequence[Dict]:\n        \"\"\"Replace the checkbox options.\n\n        Parameters\n        ----------\n        labels : array-like\n            List of strings which will be visible to the user.\n        values : array-like\n            List of values associated with the labels that are hidden from the user.\n\n        Returns\n        -------\n        None\n\n        \"\"\"\n        return [{'label': label, 'value': value} for label, value in zip(labels, values)]", "response": "Replace the checkbox options."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef do_options(self, labels, values):\n        return [{'label': label, 'value': value} for label, value in zip(labels, values)]", "response": "Replace the radio button options."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninvoking the function repeatedly on a timer.", "response": "def run(self):\n        \"\"\"Invoke the function repeatedly on a timer.\"\"\"\n        ret = eventlet.spawn(self.context(self.func))\n        eventlet.sleep(self.seconds)\n        try:\n            ret.wait()\n        except Exception:  # pylint: disable=broad-except\n            traceback.print_exc()\n        self.thread = eventlet.spawn(self.run)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef overlap(self, other: 'Span'):\n        return not (\n            # if one rectangle is left of other\n            other.column_end <= self.column_start\n            or self.column_end <= other.column_start\n            # if one rectangle is above other\n            or other.row_end <= self.row_start\n            or self.row_end <= other.row_start\n        )", "response": "Detect if two spans overlap."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cells(self) -> Generator[Tuple[int, int], None, None]:\n        yield from itertools.product(\n            range(self.row_start, self.row_end),\n            range(self.column_start, self.column_end)\n        )", "response": "Generate cells in span."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pixels(self, value: float) -> 'Size':\n        raise_not_number(value)\n        self.maximum = '{}px'.format(value)\n        return self", "response": "Set the size in pixels."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the minimum size in pixels.", "response": "def min_pixels(self, value: float) -> 'Size':\n        \"\"\"Set the minimum size in pixels.\"\"\"\n        raise_not_number(value)\n        self.minimum = '{}px'.format(value)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the size in ems.", "response": "def ems(self, value: float) -> 'Size':\n        \"\"\"Set the size in ems.\"\"\"\n        raise_not_number(value)\n        self.maximum = '{}em'.format(value)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef min_ems(self, value: float) -> 'Size':\n        raise_not_number(value)\n        self.minimum = '{}em'.format(value)\n        return self", "response": "Set the minimum size in ems."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fraction(self, value: float) -> 'Size':\n        raise_not_number(value)\n        self.maximum = '{}fr'.format(value)\n        return self", "response": "Set the fraction of free space to use."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the percentage of free space to use.", "response": "def percent(self, value: float) -> 'Size':\n        \"\"\"Set the percentage of free space to use.\"\"\"\n        raise_not_number(value)\n        self.maximum = '{}%'.format(value)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef min_percent(self, value: float) -> 'Size':\n        raise_not_number(value)\n        self.minimum = '{}%'.format(value)\n        return self", "response": "Set the minimum percentage of free space to use."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pixels(self, value: int) -> 'Gap':\n        raise_not_number(value)\n        self.gap = '{}px'.format(value)\n        return self", "response": "Set the margin in pixels."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the margin in ems.", "response": "def ems(self, value: int) -> 'Gap':\n        \"\"\"Set the margin in ems.\"\"\"\n        raise_not_number(value)\n        self.gap = '{}em'.format(value)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the margin as a percentage.", "response": "def percent(self, value) -> 'Gap':\n        \"\"\"Set the margin as a percentage.\"\"\"\n        raise_not_number(value)\n        self.gap = '{}%'.format(value)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add(self, component: Union[Component, Sequence[Component]]) -> None:\n        try:\n            self[Span(*self._available_cell())] = component\n        except NoUnusedCellsError:\n            span = list(self._spans.keys())[-1]\n            self._spans[span] += component", "response": "Adds a widget to the grid in the next available cell."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind next available cell.", "response": "def _available_cell(self) -> Tuple[int, int]:\n        \"\"\"Find next available cell first by row then column.\n\n        First, construct a set containing all cells.\n        Then iterate over the spans and remove occupied cells.\n        \"\"\"\n        cells = set(itertools.product(range(len(self.rows)), range(len(self.columns))))\n        for span in self._spans:\n            for cell in span.cells:\n                cells.remove(cell)\n\n        if not cells:\n            raise NoUnusedCellsError('No available cells')\n        return min(cells)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a widget to the sidebar.", "response": "def add_sidebar(self, component: Component) -> None:\n        \"\"\"Add a widget to the sidebar.\n\n        Parameters\n        ----------\n        component : bowtie._Component\n            Add this component to the sidebar, it will be appended to the end.\n\n        \"\"\"\n        if not self.sidebar:\n            raise NoSidebarError('Set `sidebar=True` if you want to use the sidebar.')\n\n        if not isinstance(component, Component):\n            raise ValueError('component must be Component type, found {}'.format(component))\n        # self._track_widget(widget)\n        self._controllers.append(component)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a view to the app.", "response": "def add_route(self, view: View, path: str, exact: bool = True) -> None:\n        \"\"\"Add a view to the app.\n\n        Parameters\n        ----------\n        view : View\n        path : str\n        exact : bool, optional\n\n        \"\"\"\n        if path[0] != '/':\n            path = '/' + path\n        for route in self._routes:\n            assert path != route.path, 'Cannot use the same path twice'\n        self._routes.append(Route(view=view, path=path, exact=exact))\n\n        self.app.add_url_rule(\n            path, path[1:], lambda: render_template('bowtie.html', title=self.title)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef subscribe(self, *events: Union[Event, Pager]) -> Callable:\n        try:\n            first_event = events[0]\n        except IndexError:\n            raise IndexError('Must subscribe to at least one event.')\n        if len(events) != len(set(events)):\n            raise ValueError(\n                'Subscribed to the same event multiple times. All events must be unique.'\n            )\n\n        if len(events) > 1:\n            # check if we are using any non stateful events\n            for event in events:\n                if isinstance(event, Pager):\n                    raise NotStatefulEvent('Pagers must be subscribed by itself.')\n                if event.getter is None:\n                    raise NotStatefulEvent(\n                        f'{event.uuid}.on_{event.name} is not a stateful event. '\n                        'It must be used alone.'\n                    )\n\n        def decorator(func: Callable) -> Callable:\n            \"\"\"Handle three types of events: pages, uploads, and normal events.\"\"\"\n            if isinstance(first_event, Pager):\n                self._pages[first_event] = func\n            elif first_event.name == 'upload':\n                if first_event.uuid in self._uploads:\n                    warnings.warn(\n                        ('Overwriting function \"{func1}\" with function '\n                         '\"{func2}\" for upload object \"{obj}\".').format(\n                             func1=self._uploads[first_event.uuid],\n                             func2=func.__name__,\n                             obj=COMPONENT_REGISTRY[first_event.uuid]\n                         ), Warning)\n                self._uploads[first_event.uuid] = func\n            else:\n                for event in events:\n                    # need to have `events` here to maintain order of arguments\n                    # not sure how to deal with mypy typing errors on events so ignoring\n                    self._subscriptions[event].append((events, func))  # type: ignore\n            return func\n\n        return decorator", "response": "Subscribe to a given event or pager."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef schedule(self, seconds: float):\n        def wrap(func: Callable):\n            self._schedules.append(Scheduler(self.app, seconds, func))\n        return wrap", "response": "Decorator to schedule a function periodically."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _build(self, notebook: Optional[str] = None) -> None:\n        if node_version() < _MIN_NODE_VERSION:\n            raise WebpackError(\n                f'Webpack requires at least version {_MIN_NODE_VERSION} of Node, '\n                f'found version {node_version}.'\n            )\n\n        packages = self._write_templates()\n\n        for filename in ['package.json', 'webpack.prod.js', 'webpack.dev.js']:\n            if not (self._build_dir / filename).is_file():\n                sourcefile = self._package_dir / 'src' / filename\n                shutil.copy(sourcefile, self._build_dir)\n\n        if self._run(['yarn', '--ignore-engines', 'install'], notebook=notebook) > 1:\n            raise YarnError('Error installing node packages')\n\n        if packages:\n            installed = self._installed_packages()\n            new_packages = [x for x in packages if x.split('@')[0] not in installed]\n\n            if new_packages:\n                retval = self._run(\n                    ['yarn', '--ignore-engines', 'add'] + new_packages, notebook=notebook\n                )\n                if retval > 1:\n                    raise YarnError('Error installing node packages')\n                elif retval == 1:\n                    print('Yarn error but trying to continue build')\n        retval = self._run([_WEBPACK, '--config', 'webpack.dev.js'], notebook=notebook)\n        if retval != 0:\n            raise WebpackError('Error building with webpack')", "response": "Compile the Bowtie application."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting installed packages as list from package. json.", "response": "def _installed_packages(self) -> Generator[str, None, None]:\n        \"\"\"Extract installed packages as list from `package.json`.\"\"\"\n        with (self._build_dir / 'package.json').open('r') as f:\n            packages = json.load(f)\n        yield from packages['dependencies'].keys()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates the source directory for the build.", "response": "def _create_jspath(self) -> Path:\n        \"\"\"Create the source directory for the build.\"\"\"\n        src = self._build_dir / 'bowtiejs'\n        os.makedirs(src, exist_ok=True)\n        return src"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun command from terminal and notebook and view output from subprocess.", "response": "def _run(self, command: List[str], notebook: Optional[str] = None) -> int:\n        \"\"\"Run command from terminal and notebook and view output from subprocess.\"\"\"\n        if notebook is None:\n            return Popen(command, cwd=self._build_dir).wait()\n        cmd = Popen(command, cwd=self._build_dir, stdout=PIPE, stderr=STDOUT)\n        while True:\n            line = cmd.stdout.readline()\n            if line == b'' and cmd.poll() is not None:\n                return cmd.poll()\n            print(line.decode('utf-8'), end='')\n        raise Exception()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if a username and password combination is valid.", "response": "def _check_auth(self, username: str, password: str) -> bool:\n        \"\"\"Check if a username/password combination is valid.\"\"\"\n        try:\n            return self.credentials[username] == password\n        except KeyError:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining if a user is allowed to view this route.", "response": "def before_request(self) -> Optional[Response]:\n        \"\"\"Determine if a user is allowed to view this route.\"\"\"\n        auth = request.authorization\n        if not auth or not self._check_auth(auth.username, auth.password):\n            return Response(\n                'Could not verify your access level for that URL.\\n'\n                'You have to login with proper credentials', 401,\n                {'WWW-Authenticate': 'Basic realm=\"Login Required\"'}\n            )\n        session['logged_in'] = auth.username\n        # pylint wants this return statement\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the full path of the jupyter notebook.", "response": "def get_notebook_name() -> str:\n    \"\"\"Return the full path of the jupyter notebook.\n\n    References\n    ----------\n    https://github.com/jupyter/notebook/issues/1000#issuecomment-359875246\n\n    \"\"\"\n    kernel_id = re.search(  # type: ignore\n        'kernel-(.*).json',\n        ipykernel.connect.get_connection_file()\n    ).group(1)\n    servers = list_running_servers()\n    for server in servers:\n        response = requests.get(urljoin(server['url'], 'api/sessions'),\n                                params={'token': server.get('token', '')})\n        for session in json.loads(response.text):\n            if session['kernel']['id'] == kernel_id:\n                relative_path = session['notebook']['path']\n                return pjoin(server['notebook_dir'], relative_path)\n    raise Exception('Noteboook not found.')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_notebook(fullname: str):\n    shell = InteractiveShell.instance()\n    path = fullname\n\n    # load the notebook object\n    with open(path, 'r', encoding='utf-8') as f:\n        notebook = read(f, 4)\n\n    # create the module and add it to sys.modules\n    mod = types.ModuleType(fullname)\n    mod.__file__ = path\n    # mod.__loader__ = self\n    mod.__dict__['get_ipython'] = get_ipython\n    sys.modules[fullname] = mod\n\n    # extra work to ensure that magics that would affect the user_ns\n    # actually affect the notebook module's ns\n    save_user_ns = shell.user_ns\n    shell.user_ns = mod.__dict__\n\n    try:\n        for cell in notebook.cells:\n            if cell.cell_type == 'code':\n                try:\n                    # only run valid python code\n                    ast.parse(cell.source)\n                except SyntaxError:\n                    continue\n                try:\n                    # pylint: disable=exec-used\n                    exec(cell.source, mod.__dict__)\n                except NameError:\n                    print(cell.source)\n                    raise\n    finally:\n        shell.user_ns = save_user_ns\n    return mod", "response": "Import a notebook as a module."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends a message to the log.", "response": "def _message(status, content):\n    \"\"\"Send message interface.\n\n    Parameters\n    ----------\n    status : str\n        The type of message\n    content : str\n\n    \"\"\"\n    event = f'message.{status}'\n    if flask.has_request_context():\n        emit(event, dict(data=pack(content)))\n    else:\n        sio = flask.current_app.extensions['socketio']\n        sio.emit(event, dict(data=pack(content)))\n    eventlet.sleep()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _make_columns(columns: List[Union[int, str]]) -> List[Dict]:\n        return [dict(title=str(c),\n                     dataIndex=str(c),\n                     key=str(c))\n                for c in columns]", "response": "Transform list of columns into AntTable format."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _make_data(data) -> Tuple[List[Dict], List[Dict]]:\n        jsdata = []\n        for idx, row in data.iterrows():\n            row.index = row.index.astype(str)\n            rdict = row.to_dict()\n            rdict.update(dict(key=str(idx)))\n            jsdata.append(rdict)\n\n        return jsdata, Table._make_columns(data.columns)", "response": "Transform table data into JSON."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncommands line interface decorator. Decorate a function for building a Bowtie application and turn it into a command line interface.", "response": "def command(func):\n    \"\"\"Command line interface decorator.\n\n    Decorate a function for building a Bowtie\n    application and turn it into a command line interface.\n    \"\"\"\n    # pylint: disable=protected-access,unused-variable\n    nargs = numargs(func)\n    if nargs > 0:\n        raise WrongNumberOfArguments(\n            f'Decorated function \"{func.__name__}\" should have no arguments, it has {nargs}.'\n        )\n    app = func()\n    if app is None:\n        raise TypeError(\n            'No `App` instance was returned. '\n            'In the function decorated with @command, '\n            'return the `App` instance so it can be built.'\n        )\n    if not isinstance(app, App):\n        raise TypeError(\n            f'Returned value {app} is of type {type(app)}, '\n            'it needs to be a bowtie.App instance.'\n        )\n\n    @click.group(options_metavar='[--help]')\n    def cmd():\n        \"\"\"Bowtie CLI to help build and run your app.\"\"\"\n\n    @cmd.command(add_help_option=False)\n    def build():\n        \"\"\"Write the app, downloads the packages, and bundles it with Webpack.\"\"\"\n        app._build()\n\n    @cmd.command(add_help_option=False)\n    @click.option('--host', '-h', default='0.0.0.0', type=str)\n    @click.option('--port', '-p', default=9991, type=int)\n    def run(host, port):\n        \"\"\"Build the app and serve it.\"\"\"\n        app._build()\n        app._serve(host, port)\n\n    @cmd.command(add_help_option=True)\n    @click.option('--host', '-h', default='0.0.0.0', type=str)\n    @click.option('--port', '-p', default=9991, type=int)\n    def serve(host, port):\n        \"\"\"Serve the Bowtie app.\"\"\"\n        app._serve(host, port)\n\n    @cmd.command(context_settings=dict(ignore_unknown_options=True),\n                 add_help_option=False)\n    @click.argument('extra', nargs=-1, type=click.UNPROCESSED)\n    def dev(extra):\n        \"\"\"Recompile the app for development.\"\"\"\n        line = (_WEBPACK, '--config', 'webpack.dev.js') + extra\n        call(line, cwd=app._build_dir)\n\n    @cmd.command(context_settings=dict(ignore_unknown_options=True),\n                 add_help_option=False)\n    @click.argument('extra', nargs=-1, type=click.UNPROCESSED)\n    def prod(extra):\n        \"\"\"Recompile the app for production.\"\"\"\n        line = (_WEBPACK, '--config', 'webpack.prod.js', '--progress') + extra\n        call(line, cwd=app._build_dir)\n\n    locale = inspect.stack()[1][0].f_locals\n    module = locale.get(\"__name__\")\n\n    if module == \"__main__\":\n        try:\n            arg = sys.argv[1:]\n        except IndexError:\n            arg = ('--help',)\n        # pylint: disable=too-many-function-args\n        sys.exit(cmd(arg))\n\n    return cmd"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef toParallelTargets(suite, targets):\n    targets = filter(lambda x: x != '.', targets)\n    # First, convert the suite to a proto test list - proto tests nicely\n    # parse things like the fully dotted name of the test and the\n    # finest-grained module it belongs to, which simplifies our job.\n    proto_test_list = toProtoTestList(suite)\n    # Extract a list of the modules that all of the discovered tests are in\n    modules = set([x.module for x in proto_test_list])\n    # Get the list of user-specified targets that are NOT modules\n    non_module_targets = []\n    for target in targets:\n        if not list(filter(None, [target in x for x in modules])):\n            non_module_targets.append(target)\n    # Main loop -- iterating through all loaded test methods\n    parallel_targets = []\n    for test in proto_test_list:\n        found = False\n        for target in non_module_targets:\n            # target is a dotted name of either a test case or test method\n            # here test.dotted name is always a dotted name of a method\n            if (target in test.dotted_name):\n                if target not in parallel_targets:\n                    # Explicitly specified targets get their own entry to\n                    # run parallel to everything else\n                    parallel_targets.append(target)\n                found = True\n                break\n        if found:\n            continue\n        # This test does not appear to be part of a specified target, so\n        # its entire module must have been discovered, so just add the\n        # whole module to the list if we haven't already.\n        if test.module not in parallel_targets:\n            parallel_targets.append(test.module)\n\n    return parallel_targets", "response": "Convert a list of targets into a list of targets which should be tested in parallel."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetermines whether or not a given path is a subpackage or not.", "response": "def isPackage(file_path):\n    \"\"\"\n    Determine whether or not a given path is a (sub)package or not.\n    \"\"\"\n    return (os.path.isdir(file_path) and\n            os.path.isfile(os.path.join(file_path, '__init__.py')))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the command line arguments and returns the args object.", "response": "def parseArguments(argv=None):  # pragma: no cover\n    \"\"\"\n    I parse arguments in sys.argv and return the args object.  The parser\n    itself is available as args.parser.\n\n    Adds the following members to args:\n        parser    = the parser object\n        store_opt = the StoreOpt object\n    \"\"\"\n    store_opt = StoreOpt()\n    parser = argparse.ArgumentParser(\n            prog='green',\n            usage='%(prog)s [options] [target [target2 ...]]',\n            add_help=False,\n            description=dedent(\n                \"\"\"\n                Green is a clean, colorful, fast test runner for Python unit tests.\n                \"\"\".rstrip()),\n            epilog=dedent(\n                \"\"\"\n                ENABLING SHELL COMPLETION\n\n                  To enable bash- or zsh-completion, add the line below to the end of your\n                  .bashrc or .zshrc file (or equivalent config file):\n\n                    which green >& /dev/null && source \"$( green --completion-file )\"\n\n                  Warning!  Generating a completion list actually discovers and loads tests\n                  -- this can be very slow if you run it in huge directories!\n\n                SETUP.PY RUNNER\n\n                  To run green as a setup.py command, simply add green to the 'setup_requires'\n                  section in the setup.py file, and specify a target as the 'test_suite'\n                  parameter if you do not want green to load all the tests:\n\n                     setup(\n                         setup_requires = ['green'],\n                         install_requires = 'myproject.tests'\n                     )\n\n                  Then simply run green as any other setup.py command (it accepts the same\n                  parameters as the 'green' executable):\n\n                     python setup.py green\n                     python setup.py green -r   # to run with coverage, etc.\n\n\n                CONFIG FILES\n\n                  For documentation on config files, please see\n                  https://github.com/CleanCut/green#config-files\n                \"\"\".rstrip()),\n            formatter_class=argparse.RawDescriptionHelpFormatter)\n\n    target_args = parser.add_argument_group(\"Target Specification\")\n    target_args.add_argument('targets', action='store', nargs='*',\n        metavar='target',\n        help=(\"\"\"Targets to test.  Any number of targets may be specified.  If\n        blank, then discover all testcases in the current directory tree.  Can\n        be a directory (or package), file (or module), or fully-qualified\n        'dotted name' like proj.tests.test_things.TestStuff.  If a directory\n        (or package) is specified, then we will attempt to discover all tests\n        under the directory (even if the directory is a package and the tests\n        would not be accessible through the package's scope).  In all other\n        cases, only tests accessible from introspection of the object will\n        be loaded.\"\"\"),\n        default=argparse.SUPPRESS)\n\n    concurrency_args = parser.add_argument_group(\"Concurrency Options\")\n    store_opt(\n        concurrency_args.add_argument('-s', '--processes', action='store',\n            type=int, metavar='NUM',\n            help=\"Number of processes to use to run tests.  Note that your \"\n            \"tests need to be written to avoid using the same resources (temp \"\n            \"files, sockets, ports, etc.) for the multi-process mode to work \"\n            \"well (--initializer and --finalizer can help provision \"\n            \"per-process resources). Default is to run the same number of \"\n            \"processes as your machine has logical CPUs. Note that for a \"\n            \"small number of trivial tests, running everything in a single \"\n            \"process may be faster than the overhead of initializing all the \"\n            \"processes.\",\n            default=argparse.SUPPRESS))\n    store_opt(\n        concurrency_args.add_argument('-i', '--initializer', action='store',\n            metavar='DOTTED_FUNCTION',\n            help=\"Python function to run inside of a single worker process \"\n            \"before it starts running tests.  This is the way to provision \"\n            \"external resources that each concurrent worker process needs to \"\n            \"have exclusive access to. Specify the function in dotted \"\n            \"notation in a way that will be importable from the location you \"\n            \"are running green from.\",\n            default=argparse.SUPPRESS))\n    store_opt(\n        concurrency_args.add_argument('-z', '--finalizer', action='store',\n            metavar='DOTTED_FUNCTION',\n            help=\"Same as --initializer, only run at the end of a worker \"\n            \"process's lifetime.  Used to unprovision resources provisioned by \"\n            \"the initializer.\",\n            default=argparse.SUPPRESS))\n\n    format_args = parser.add_argument_group(\"Format Options\")\n    store_opt(format_args.add_argument('-t', '--termcolor', action='store_true',\n        help=\"Force terminal colors on.  Default is to autodetect.\",\n        default=argparse.SUPPRESS))\n    store_opt(\n        format_args.add_argument('-T', '--notermcolor', action='store_true',\n        help=\"Force terminal colors off.  Default is to autodetect.\",\n        default=argparse.SUPPRESS))\n    store_opt(\n        format_args.add_argument('-W', '--disable-windows', action='store_true',\n        help=\"Disable Windows support by turning off Colorama\",\n        default=argparse.SUPPRESS))\n\n    out_args = parser.add_argument_group(\"Output Options\")\n    store_opt(out_args.add_argument('-a', '--allow-stdout', action='store_true',\n        help=(\"Instead of capturing the stdout and stderr and presenting it \"\n        \"in the summary of results, let it come through.\"),\n        default=argparse.SUPPRESS))\n    store_opt(out_args.add_argument('-q', '--quiet-stdout', action='store_true',\n        help=(\"Instead of capturing the stdout and stderr and presenting it \"\n        \"in the summary of results, discard it completly for successful \"\n        \"tests. --allow-stdout option overrides it.\"),\n        default=argparse.SUPPRESS))\n    store_opt(out_args.add_argument('-k', '--no-skip-report',\n        action='store_true', help=(\"Don't print the report of skipped tests \"\n        \"after testing is done.  Skips will still show up in the progress \"\n        \"report and summary count.\"),\n        default=argparse.SUPPRESS))\n    store_opt(out_args.add_argument('-e', '--no-tracebacks',\n        action='store_true', help=(\"Don't print tracebacks for failures and \"\n        \"errors.\"),\n        default=argparse.SUPPRESS))\n    store_opt(out_args.add_argument('-h', '--help', action='store_true',\n        help=\"Show this help message and exit.\",\n        default=argparse.SUPPRESS))\n    store_opt(out_args.add_argument('-V', '--version', action='store_true',\n        help=\"Print the version of Green and Python and exit.\",\n        default=argparse.SUPPRESS))\n    store_opt(out_args.add_argument('-l', '--logging', action='store_true',\n        help=\"Don't configure the root logger to redirect to /dev/null, \"\n        \"enabling internal debugging output, as well as any output test (or \"\n        \"tested) code may be sending via the root logger.\",\n        default=argparse.SUPPRESS))\n    store_opt(out_args.add_argument('-d', '--debug', action='count',\n        help=(\"Enable internal debugging statements.  Implies --logging.  Can \"\n        \"be specified up to three times for more debug output.\"),\n        default=argparse.SUPPRESS))\n    store_opt(out_args.add_argument('-v', '--verbose', action='count',\n        help=(\"Verbose. Can be specified up to three times for more \"\n        \"verbosity. Recommended levels are -v and -vv.\"),\n        default=argparse.SUPPRESS))\n    store_opt(out_args.add_argument('-U', '--disable-unidecode', action='store_true',\n        help=(\"Disable unidecode which converts test output from unicode to\"\n            \"ascii by default on Windows to avoid hard-to-debug crashes.\"),\n        default=argparse.SUPPRESS))\n\n    other_args = parser.add_argument_group(\"Other Options\")\n    store_opt(other_args.add_argument('-f', '--failfast', action='store_true',\n        help=(\"Stop execution at the first test that fails or errors.\"),\n        default=argparse.SUPPRESS))\n    store_opt(other_args.add_argument('-c', '--config', action='store',\n        metavar='FILE', help=\"Use this config file to override any values from \"\n        \"the config file specified by environment variable GREEN_CONFIG, \"\n        \"~/.green, and .green in the current working directory.\",\n        default=argparse.SUPPRESS))\n    store_opt(other_args.add_argument('-p', '--file-pattern', action='store',\n        metavar='PATTERN',\n        help=\"Pattern to match test files. Default is test*.py\",\n        default=argparse.SUPPRESS))\n    store_opt(other_args.add_argument('-n', '--test-pattern', action='store',\n        metavar='PATTERN', help=\"Pattern to match test method names after \"\n        \"'test'.  Default is '*', meaning match methods named 'test*'.\",\n        default=argparse.SUPPRESS))\n    store_opt(other_args.add_argument('-j', '--junit-report',\n                                      action='store',\n                                      metavar=\"FILENAME\",\n                                      help=(\"Generate a JUnit XML report.\"),\n                                      default=argparse.SUPPRESS))\n\n    cov_args = parser.add_argument_group(\n        \"Coverage Options ({})\".format(coverage_version))\n    store_opt(cov_args.add_argument('-r', '--run-coverage', action='store_true',\n        help=(\"Produce coverage output.\"), default=argparse.SUPPRESS))\n    store_opt(cov_args.add_argument('-g', '--cov-config-file',\n        action='store', metavar='FILE', help=(\"Specify a coverage config file. \"\n            \"Implies --run-coverage See the coverage documentation \"\n            \"at https://coverage.readthedocs.io/en/v4.5.x/config.html \"\n            \"for coverage config file syntax. The [run] and [report] sections \"\n            \"are most relevant.\"),\n        default=argparse.SUPPRESS)),\n    store_opt(cov_args.add_argument('-R', '--quiet-coverage', action='store_true',\n        help=(\"Do not print coverage report to stdout (coverage files will \"\n            \"still be created). Implies --run-coverage\"),\n        default=argparse.SUPPRESS))\n    store_opt(cov_args.add_argument('-O', '--clear-omit', action='store_true',\n        help=(\"Green tries really hard to set up a good list of patterns of \"\n            \"files to omit from coverage reports.  If the default list \"\n            \"catches files that you DO want to cover you can specify this \"\n            \"flag to leave the default list empty to start with.  You can \"\n            \"then add patterns back in with --omit-add. The default list is \"\n            \"something like '*/test*,*/mock*,*(temp dir)*,*(python system \"\n            \"packages)*' -- only longer.\"),\n        default=argparse.SUPPRESS))\n    store_opt(cov_args.add_argument('-u', '--include-patterns', action='store',\n        metavar='PATTERN',\n        help=(\"Comma-separated file-patterns to include in coverage.  This \"\n            \"implies that anything that does not match the include pattern is \"\n            \"omitted from coverage reporting.\"),\n        default=argparse.SUPPRESS))\n    store_opt(cov_args.add_argument('-o', '--omit-patterns', action='store',\n        metavar='PATTERN',\n        help=(\"Comma-separated file-patterns to omit from coverage.  For \"\n            \"example, if coverage reported a file mypackage/foo/bar you could \"\n            \"omit it from coverage with 'mypackage*', '*/foo/*', or '*bar'\"),\n        default=argparse.SUPPRESS))\n    store_opt(cov_args.add_argument('-m', '--minimum-coverage', action='store',\n        metavar='X', type=int, help=(\"Integer. A minimum coverage value.  If \"\n            \"not met, then we will print a message and exit with a nonzero \"\n            \"status. Implies --run-coverage\"),\n        default=argparse.SUPPRESS))\n\n    integration_args = parser.add_argument_group(\"Integration Options\")\n    store_opt(integration_args.add_argument('--completion-file',\n        action='store_true', help=(\"Location of the bash- and zsh-completion \"\n            \"file.  To enable bash- or zsh-completion, see ENABLING SHELL \"\n            \"COMPLETION below.\"), default=argparse.SUPPRESS))\n    store_opt(integration_args.add_argument('--completions',\n        action='store_true',\n        help=(\"Output possible completions of the given target.  Used by \"\n        \"bash- and zsh-completion.\"), default=argparse.SUPPRESS))\n    store_opt(integration_args.add_argument('--options', action='store_true',\n        help=\"Output all options.  Used by bash- and zsh-completion.\",\n        default=argparse.SUPPRESS))\n\n    args = parser.parse_args(argv)\n\n    # Add additional members\n    args.parser    = parser\n    args.store_opt = store_opt\n\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getConfig(filepath=None):  # pragma: no cover\n    parser = configparser.ConfigParser()\n\n    filepaths = []\n    # Lowest priority goes first in the list\n    home = os.getenv(\"HOME\")\n    if home:\n        default_filepath = os.path.join(home, \".green\")\n        if os.path.isfile(default_filepath):\n            filepaths.append(default_filepath)\n\n    # Low priority\n    env_filepath = os.getenv(\"GREEN_CONFIG\")\n    if env_filepath and os.path.isfile(env_filepath):\n        filepaths.append(env_filepath)\n\n    # Medium priority\n    for cfg_file in (\"setup.cfg\", \".green\"):\n        cwd_filepath = os.path.join(os.getcwd(), cfg_file)\n        if os.path.isfile(cwd_filepath):\n            filepaths.append(cwd_filepath)\n\n    # High priority\n    if filepath and os.path.isfile(filepath):\n        filepaths.append(filepath)\n\n    if filepaths:\n        global files_loaded\n        files_loaded = filepaths\n        # Python 3 has parser.read_file(iterator) while Python2 has\n        # parser.readfp(obj_with_readline)\n        read_func = getattr(parser, 'read_file', getattr(parser, 'readfp'))\n        for filepath in filepaths:\n            # Users are expected to put a [green] section\n            # only if they use setup.cfg\n            if filepath.endswith('setup.cfg'):\n                with open(filepath) as f:\n                    read_func(f)\n            else:\n                read_func(ConfigFile(filepath))\n\n    return parser", "response": "Get the Green config file settings."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmerging in options from config files into a new argparse. Namespace.", "response": "def mergeConfig(args, testing=False):  # pragma: no cover\n    \"\"\"\n    I take in a namespace created by the ArgumentParser in cmdline.main() and\n    merge in options from configuration files.  The config items only replace\n    argument items that are set to default value.\n\n    Returns: I return a new argparse.Namespace, adding members:\n        shouldExit       = default False\n        exitCode         = default 0\n        include patterns = include-patterns setting converted to list.\n        omit_patterns    = omit-patterns settings converted to list and\n                           extended, taking clear-omit into account.\n        cov              = coverage object default None\n    \"\"\"\n    config = getConfig(getattr(args, 'config', default_args.config))\n    new_args = copy.deepcopy(default_args)  # Default by default!\n\n    for name, default_value in dict(default_args._get_kwargs()).items():\n        # Config options overwrite default options\n        config_getter = None\n        if name in ['termcolor', 'notermcolor', 'allow_stdout', 'quiet_stdout',\n                    'help', 'logging', 'version', 'disable_unidecode', 'failfast',\n                    'run_coverage', 'options', 'completions', 'completion_file',\n                    'clear_omit', 'no_skip_report', 'no_tracebacks',\n                    'disable_windows', 'quiet_coverage', 'junit_report']:\n            config_getter = config.getboolean\n        elif name in ['processes', 'debug', 'verbose', 'minimum_coverage']:\n            config_getter = config.getint\n        elif name in ['file_pattern', 'finalizer', 'initializer',\n                      'cov_config_file', 'include_patterns', 'omit_patterns',\n                      'warnings', 'test_pattern']:\n            config_getter = config.get\n        elif name in ['targets', 'help', 'config']:\n            pass  # Some options only make sense coming on the command-line.\n        elif name in ['store_opt', 'parser']:\n            pass  # These are convenience objects, not actual settings\n        else:\n            raise NotImplementedError(name)\n\n        if config_getter:\n            try:\n                config_value = config_getter('green', name.replace('_', '-'))\n                setattr(new_args, name, config_value)\n            except (configparser.NoSectionError, configparser.NoOptionError):\n                pass\n\n        # Command-line values overwrite defaults and config values when\n        # specified\n        args_value = getattr(args, name, 'unspecified')\n        if args_value != 'unspecified':\n            setattr(new_args, name, args_value)\n\n    new_args.shouldExit = False\n    new_args.exitCode = 0\n    new_args.cov = None\n\n    # Help?\n    if new_args.help:  # pragma: no cover\n        new_args.parser.print_help()\n        new_args.shouldExit = True\n        return new_args\n\n    # Did we just print the version?\n    if new_args.version:\n        from green.version import pretty_version\n        sys.stdout.write(pretty_version()+'\\n')\n        new_args.shouldExit = True\n        return new_args\n\n    # Handle logging options\n    if new_args.debug:\n        logging.basicConfig(\n                level=logging.DEBUG,\n                format=\"%(asctime)s %(levelname)9s %(message)s\",\n                datefmt=\"%Y-%m-%d %H:%M:%S\")\n    elif not new_args.logging:\n        logging.basicConfig(filename=os.devnull)\n\n    # Disable termcolor?\n    if new_args.notermcolor:\n        new_args.termcolor = False\n\n    # Coverage.  We must enable it here because we cannot cover module-level\n    # code after it is imported, and this is the earliest place we can turn on\n    # coverage.\n    omit_patterns = [\n        '*/argparse*',\n        '*/colorama*',\n        '*/django/*',\n        '*/distutils*',        # Gets pulled in on Travis-CI CPython\n        '*/extras*',           # pulled in by testtools\n        '*/linecache2*',       # pulled in by testtools\n        '*/mimeparse*',        # pulled in by testtools\n        '*/mock*',\n        '*/pbr*',              # pulled in by testtools\n        '*/pkg_resources*',    # pulled in by django\n        '*/pypy*',\n        '*/pytz*',             # pulled in by django\n        '*/six*',              # pulled in by testtools\n        '*/termstyle*',\n        '*/test*',\n        '*/traceback2*',       # pulled in by testtools\n        '*/unittest2*',        # pulled in by testtools\n        '*Python.framework*',  # OS X system python\n        '*site-packages*',     # System python for other OS's\n        tempfile.gettempdir() + '*',\n    ]\n    if new_args.clear_omit:\n        omit_patterns = []\n\n    if new_args.omit_patterns:\n        omit_patterns.extend(new_args.omit_patterns.split(','))\n    new_args.omit_patterns = omit_patterns\n\n    if new_args.include_patterns:\n        new_args.include_patterns = new_args.include_patterns.split(',')\n    else:\n        new_args.include_patterns = []\n\n    if new_args.quiet_coverage or (type(new_args.cov_config_file) == str):\n        new_args.run_coverage = True\n\n    if new_args.minimum_coverage != None:\n        new_args.run_coverage = True\n\n    if new_args.run_coverage:\n        if not testing:\n            cov = coverage.coverage(data_file='.coverage', omit=omit_patterns,\n                                    include=new_args.include_patterns,\n                                    config_file = new_args.cov_config_file)\n            cov.start()\n        new_args.cov = cov\n\n    return new_args"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint a debug message to stdout.", "response": "def ddebug(msg, err=None):  # pragma: no cover\n    \"\"\"\n    err can be an instance of sys.exc_info() -- which is the latest traceback\n    info\n    \"\"\"\n    import os\n    if err:\n        err = ''.join(traceback.format_exception(*err))\n    else:\n        err = ''\n    sys.__stdout__.write(\"({}) {} {}\".format(os.getpid(), msg, err)+'\\n')\n    sys.__stdout__.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef poolRunner(target, queue, coverage_number=None, omit_patterns=[], cov_config_file=True):  # pragma: no cover\n    # Each pool worker gets his own temp directory, to avoid having tests that\n    # are used to taking turns using the same temp file name from interfering\n    # with eachother.  So long as the test doesn't use a hard-coded temp\n    # directory, anyway.\n    saved_tempdir = tempfile.tempdir\n    tempfile.tempdir = tempfile.mkdtemp()\n\n    def cleanup():\n        # Restore the state of the temp directory\n        # TODO: Make this not necessary on macOS+Python3 (see #173)\n        if sys.version_info[0] == 2:\n            shutil.rmtree(tempfile.tempdir, ignore_errors=True)\n        tempfile.tempdir = saved_tempdir\n        queue.put(None)\n        # Finish coverage\n        if coverage_number:\n            cov.stop()\n            cov.save()\n\n    # Each pool starts its own coverage, later combined by the main process.\n    if coverage_number:\n        cov = coverage.coverage(\n                data_file='.coverage.{}_{}'.format(\n                    coverage_number, random.randint(0, 10000)),\n                omit=omit_patterns,\n                config_file=cov_config_file)\n        cov._warn_no_data = False\n        cov.start()\n\n    # What to do each time an individual test is started\n    already_sent = set()\n\n    def start_callback(test):\n        # Let the main process know what test we are starting\n        test = proto_test(test)\n        if test not in already_sent:\n            queue.put(test)\n            already_sent.add(test)\n\n    def finalize_callback(test_result):\n        # Let the main process know what happened with the test run\n        queue.put(test_result)\n\n    result = ProtoTestResult(start_callback, finalize_callback)\n    test = None\n    try:\n        loader = GreenTestLoader()\n        test = loader.loadTargets(target)\n    except:\n        err = sys.exc_info()\n        t             = ProtoTest()\n        t.module      = 'green.loader'\n        t.class_name  = 'N/A'\n        t.description = 'Green encountered an error loading the unit test.'\n        t.method_name = 'poolRunner'\n        result.startTest(t)\n        result.addError(t, err)\n        result.stopTest(t)\n        queue.put(result)\n        cleanup()\n        return\n\n    if getattr(test, 'run', False):\n        # Loading was successful, lets do this\n        try:\n            test.run(result)\n            # If your class setUpClass(self) method crashes, the test doesn't\n            # raise an exception, but it does add an entry to errors.  Some\n            # other things add entries to errors as well, but they all call the\n            # finalize callback.\n            if result and (not result.finalize_callback_called) and getattr(result, 'errors', False):\n                queue.put(test)\n                queue.put(result)\n        except:\n            # Some frameworks like testtools record the error AND THEN let it\n            # through to crash things.  So we only need to manufacture another\n            # error if the underlying framework didn't, but either way we don't\n            # want to crash.\n            if result.errors:\n                queue.put(result)\n            else:\n                err = sys.exc_info()\n                result.startTest(test)\n                result.addError(test, err)\n                result.stopTest(test)\n                queue.put(result)\n    else:\n        # loadTargets() returned an object without a run() method, probably\n        # None\n        description = ('Test loader returned an un-runnable object.  Is \"{}\" '\n                       'importable from your current location?  Maybe you '\n                       'forgot an __init__.py in your directory?  Unrunnable '\n                       'object looks like: {} of type {} with dir {}'\n                       .format(target, str(test), type(test), dir(test))\n                       )\n        err = (TypeError, TypeError(description), None)\n        t             = ProtoTest()\n        target_list = target.split('.')\n        t.module      = '.'.join(target_list[:-2]) if len(target_list) > 1 else target\n        t.class_name  = target.split('.')[-2] if len(target_list) > 1 else 'UnknownClass'\n        t.description = description\n        t.method_name = target.split('.')[-1] if len(target_list) > 1 else 'unknown_method'\n        result.startTest(t)\n        result.addError(t, err)\n        result.stopTest(t)\n        queue.put(result)\n\n    cleanup()", "response": "This function is used by the pool worker processes. It runs one unit test in parallel."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _repopulate_pool(self):\n        for i in range(self._processes - len(self._pool)):\n            w = self.Process(target=worker,\n                             args=(self._inqueue, self._outqueue,\n                                   self._initializer,\n                                   self._initargs, self._maxtasksperchild,\n                                   self._wrap_exception,\n                                   self._finalizer,\n                                   self._finalargs)\n                             )\n            self._pool.append(w)\n            w.name = w.name.replace('Process', 'PoolWorker')\n            w.daemon = True\n            w.start()\n            util.debug('added worker')", "response": "Repopulate the pool with the number of processes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef debug(message, level=1):\n    if level <= debug_level:\n        logging.debug(' ' * (level - 1) * 2 + str(message))", "response": "Log a message at the given level."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef formatLine(self, line, indent=0, outcome_char=''):\n        actual_spaces = (indent * self.indent_spaces) - len(outcome_char)\n        return (outcome_char + ' ' * actual_spaces + line)", "response": "Takes a single line and optionally adds an indent and outcome_char character to the beginning of the line."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(suite, stream, args, testing=False):\n    if not issubclass(GreenStream, type(stream)):\n        stream = GreenStream(stream, disable_windows=args.disable_windows,\n                disable_unidecode=args.disable_unidecode)\n    result = GreenTestResult(args, stream)\n\n    # Note: Catching SIGINT isn't supported by Python on windows (python\n    # \"WONTFIX\" issue 18040)\n    installHandler()\n    registerResult(result)\n\n    with warnings.catch_warnings():\n        if args.warnings:  # pragma: no cover\n            # if args.warnings is set, use it to filter all the warnings\n            warnings.simplefilter(args.warnings)\n            # if the filter is 'default' or 'always', special-case the\n            # warnings from the deprecated unittest methods to show them\n            # no more than once per module, because they can be fairly\n            # noisy.  The -Wd and -Wa flags can be used to bypass this\n            # only when args.warnings is None.\n            if args.warnings in ['default', 'always']:\n                warnings.filterwarnings('module',\n                                        category=DeprecationWarning,\n                                        message='Please use assert\\w+ instead.')\n\n        result.startTestRun()\n\n        pool = LoggingDaemonlessPool(processes=args.processes or None,\n                                     initializer=InitializerOrFinalizer(args.initializer),\n                                     finalizer=InitializerOrFinalizer(args.finalizer))\n        manager = multiprocessing.Manager()\n        targets = [(target, manager.Queue())\n                   for target in toParallelTargets(suite, args.targets)]\n        if targets:\n            for index, (target, queue) in enumerate(targets):\n                if args.run_coverage:\n                    coverage_number = index + 1\n                else:\n                    coverage_number = None\n                debug(\"Sending {} to runner {}\".format(target, poolRunner))\n                pool.apply_async(\n                    poolRunner,\n                    (target, queue, coverage_number, args.omit_patterns, args.cov_config_file))\n            pool.close()\n            for target, queue in targets:\n                abort = False\n\n                while True:\n                    msg = queue.get()\n\n                    # Sentinel value, we're done\n                    if not msg:\n                        break\n                    else:\n                        # Result guaranteed after this message, we're\n                        # currently waiting on this test, so print out\n                        # the white 'processing...' version of the output\n                        result.startTest(msg)\n                        proto_test_result = queue.get()\n                        result.addProtoTestResult(proto_test_result)\n\n                    if result.shouldStop:\n                        abort = True\n                        break\n\n                if abort:\n                    break\n\n        pool.close()\n        pool.join()\n\n        result.stopTestRun()\n\n    removeResult(result)\n\n    return result", "response": "Run the given test case or test suite with the specified arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsolves a batch of QPs. This function solves a batch of QPs, each optimizing over `nz` variables and having `nineq` inequality constraints and `neq` equality constraints. The optimization problem for each instance in the batch (dropping indexing from the notation) is of the form \\hat z = argmin_z 1/2 z^T Q z + p^T z subject to Gz <= h Az = b where Q \\in S^{nz,nz}, S^{nz,nz} is the set of all positive semi-definite matrices, p \\in R^{nz} G \\in R^{nineq,nz} h \\in R^{nineq} A \\in R^{neq,nz} b \\in R^{neq} These parameters should all be passed to this function as Variable- or Parameter-wrapped Tensors. (See torch.autograd.Variable and torch.nn.parameter.Parameter) If you want to solve a batch of QPs where `nz`, `nineq` and `neq` are the same, but some of the contents differ across the minibatch, you can pass in tensors in the standard way where the first dimension indicates the batch example. This can be done with some or all of the coefficients. You do not need to add an extra dimension to coefficients that will not change across all of the minibatch examples. This function is able to infer such cases. If you don't want to use any equality or inequality constraints, you can set the appropriate values to: e = Variable(torch.Tensor()) Parameters: Q: A (nBatch, nz, nz) or (nz, nz) Tensor. p: A (nBatch, nz) or (nz) Tensor. G: A (nBatch, nineq, nz) or (nineq, nz) Tensor. h: A (nBatch, nineq) or (nineq) Tensor. A: A (nBatch, neq, nz) or (neq, nz) Tensor. b: A (nBatch, neq) or (neq) Tensor. Returns: \\hat z: a (nBatch, nz) Tensor.", "response": "def forward(self, Q_, p_, G_, h_, A_, b_):\n        \"\"\"Solve a batch of QPs.\n\n        This function solves a batch of QPs, each optimizing over\n        `nz` variables and having `nineq` inequality constraints\n        and `neq` equality constraints.\n        The optimization problem for each instance in the batch\n        (dropping indexing from the notation) is of the form\n\n            \\hat z =   argmin_z 1/2 z^T Q z + p^T z\n                     subject to Gz <= h\n                                Az  = b\n\n        where Q \\in S^{nz,nz},\n              S^{nz,nz} is the set of all positive semi-definite matrices,\n              p \\in R^{nz}\n              G \\in R^{nineq,nz}\n              h \\in R^{nineq}\n              A \\in R^{neq,nz}\n              b \\in R^{neq}\n\n        These parameters should all be passed to this function as\n        Variable- or Parameter-wrapped Tensors.\n        (See torch.autograd.Variable and torch.nn.parameter.Parameter)\n\n        If you want to solve a batch of QPs where `nz`, `nineq` and `neq`\n        are the same, but some of the contents differ across the\n        minibatch, you can pass in tensors in the standard way\n        where the first dimension indicates the batch example.\n        This can be done with some or all of the coefficients.\n\n        You do not need to add an extra dimension to coefficients\n        that will not change across all of the minibatch examples.\n        This function is able to infer such cases.\n\n        If you don't want to use any equality or inequality constraints,\n        you can set the appropriate values to:\n\n            e = Variable(torch.Tensor())\n\n        Parameters:\n          Q:  A (nBatch, nz, nz) or (nz, nz) Tensor.\n          p:  A (nBatch, nz) or (nz) Tensor.\n          G:  A (nBatch, nineq, nz) or (nineq, nz) Tensor.\n          h:  A (nBatch, nineq) or (nineq) Tensor.\n          A:  A (nBatch, neq, nz) or (neq, nz) Tensor.\n          b:  A (nBatch, neq) or (neq) Tensor.\n\n        Returns: \\hat z: a (nBatch, nz) Tensor.\n        \"\"\"\n        nBatch = extract_nBatch(Q_, p_, G_, h_, A_, b_)\n        Q, _ = expandParam(Q_, nBatch, 3)\n        p, _ = expandParam(p_, nBatch, 2)\n        G, _ = expandParam(G_, nBatch, 3)\n        h, _ = expandParam(h_, nBatch, 2)\n        A, _ = expandParam(A_, nBatch, 3)\n        b, _ = expandParam(b_, nBatch, 2)\n\n        if self.check_Q_spd:\n            for i in range(nBatch):\n                e, _ = torch.eig(Q[i])\n                if not torch.all(e[:,0] > 0):\n                    raise RuntimeError('Q is not SPD.')\n\n        _, nineq, nz = G.size()\n        neq = A.size(1) if A.nelement() > 0 else 0\n        assert(neq > 0 or nineq > 0)\n        self.neq, self.nineq, self.nz = neq, nineq, nz\n\n        if self.solver == QPSolvers.PDIPM_BATCHED:\n            self.Q_LU, self.S_LU, self.R = pdipm_b.pre_factor_kkt(Q, G, A)\n            zhats, self.nus, self.lams, self.slacks = pdipm_b.forward(\n                Q, p, G, h, A, b, self.Q_LU, self.S_LU, self.R,\n                self.eps, self.verbose, self.notImprovedLim, self.maxIter)\n        elif self.solver == QPSolvers.CVXPY:\n            vals = torch.Tensor(nBatch).type_as(Q)\n            zhats = torch.Tensor(nBatch, self.nz).type_as(Q)\n            lams = torch.Tensor(nBatch, self.nineq).type_as(Q)\n            nus = torch.Tensor(nBatch, self.neq).type_as(Q) \\\n                if self.neq > 0 else torch.Tensor()\n            slacks = torch.Tensor(nBatch, self.nineq).type_as(Q)\n            for i in range(nBatch):\n                Ai, bi = (A[i], b[i]) if neq > 0 else (None, None)\n                vals[i], zhati, nui, lami, si = solvers.cvxpy.forward_single_np(\n                    *[x.cpu().numpy() if x is not None else None\n                      for x in (Q[i], p[i], G[i], h[i], Ai, bi)])\n                # if zhati[0] is None:\n                #     import IPython, sys; IPython.embed(); sys.exit(-1)\n                zhats[i] = torch.Tensor(zhati)\n                lams[i] = torch.Tensor(lami)\n                slacks[i] = torch.Tensor(si)\n                if neq > 0:\n                    nus[i] = torch.Tensor(nui)\n\n            self.vals = vals\n            self.lams = lams\n            self.nus = nus\n            self.slacks = slacks\n        else:\n            assert False\n\n        self.save_for_backward(zhats, Q_, p_, G_, h_, A_, b_)\n        return zhats"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nforwarding method for KKT to the next state of the slack.", "response": "def forward(Q, p, G, h, A, b, Q_LU, S_LU, R, eps=1e-12, verbose=0, notImprovedLim=3,\n            maxIter=20, solver=KKTSolvers.LU_PARTIAL):\n    \"\"\"\n    Q_LU, S_LU, R = pre_factor_kkt(Q, G, A)\n    \"\"\"\n    nineq, nz, neq, nBatch = get_sizes(G, A)\n\n    # Find initial values\n    if solver == KKTSolvers.LU_FULL:\n        D = torch.eye(nineq).repeat(nBatch, 1, 1).type_as(Q)\n        x, s, z, y = factor_solve_kkt(\n            Q, D, G, A, p,\n            torch.zeros(nBatch, nineq).type_as(Q),\n            -h, -b if b is not None else None)\n    elif solver == KKTSolvers.LU_PARTIAL:\n        d = torch.ones(nBatch, nineq).type_as(Q)\n        factor_kkt(S_LU, R, d)\n        x, s, z, y = solve_kkt(\n            Q_LU, d, G, A, S_LU,\n            p, torch.zeros(nBatch, nineq).type_as(Q),\n            -h, -b if neq > 0 else None)\n    elif solver == KKTSolvers.IR_UNOPT:\n        D = torch.eye(nineq).repeat(nBatch, 1, 1).type_as(Q)\n        x, s, z, y = solve_kkt_ir(\n            Q, D, G, A, p,\n            torch.zeros(nBatch, nineq).type_as(Q),\n            -h, -b if b is not None else None)\n    else:\n        assert False\n\n    # Make all of the slack variables >= 1.\n    M = torch.min(s, 1)[0]\n    M = M.view(M.size(0), 1).repeat(1, nineq)\n    I = M < 0\n    s[I] -= M[I] - 1\n\n    # Make all of the inequality dual variables >= 1.\n    M = torch.min(z, 1)[0]\n    M = M.view(M.size(0), 1).repeat(1, nineq)\n    I = M < 0\n    z[I] -= M[I] - 1\n\n    best = {'resids': None, 'x': None, 'z': None, 's': None, 'y': None}\n    nNotImproved = 0\n\n    for i in range(maxIter):\n        # affine scaling direction\n        rx = (torch.bmm(y.unsqueeze(1), A).squeeze(1) if neq > 0 else 0.) + \\\n            torch.bmm(z.unsqueeze(1), G).squeeze(1) + \\\n            torch.bmm(x.unsqueeze(1), Q.transpose(1, 2)).squeeze(1) + \\\n            p\n        rs = z\n        rz = torch.bmm(x.unsqueeze(1), G.transpose(1, 2)).squeeze(1) + s - h\n        ry = torch.bmm(x.unsqueeze(1), A.transpose(\n            1, 2)).squeeze(1) - b if neq > 0 else 0.0\n        mu = torch.abs((s * z).sum(1).squeeze() / nineq)\n        z_resid = torch.norm(rz, 2, 1).squeeze()\n        y_resid = torch.norm(ry, 2, 1).squeeze() if neq > 0 else 0\n        pri_resid = y_resid + z_resid\n        dual_resid = torch.norm(rx, 2, 1).squeeze()\n        resids = pri_resid + dual_resid + nineq * mu\n\n        d = z / s\n        try:\n            factor_kkt(S_LU, R, d)\n        except:\n            return best['x'], best['y'], best['z'], best['s']\n\n        if verbose == 1:\n            print('iter: {}, pri_resid: {:.5e}, dual_resid: {:.5e}, mu: {:.5e}'.format(\n                i, pri_resid.mean(), dual_resid.mean(), mu.mean()))\n        if best['resids'] is None:\n            best['resids'] = resids\n            best['x'] = x.clone()\n            best['z'] = z.clone()\n            best['s'] = s.clone()\n            best['y'] = y.clone() if y is not None else None\n            nNotImproved = 0\n        else:\n            I = resids < best['resids']\n            if I.sum() > 0:\n                nNotImproved = 0\n            else:\n                nNotImproved += 1\n            I_nz = I.repeat(nz, 1).t()\n            I_nineq = I.repeat(nineq, 1).t()\n            best['resids'][I] = resids[I]\n            best['x'][I_nz] = x[I_nz]\n            best['z'][I_nineq] = z[I_nineq]\n            best['s'][I_nineq] = s[I_nineq]\n            if neq > 0:\n                I_neq = I.repeat(neq, 1).t()\n                best['y'][I_neq] = y[I_neq]\n        if nNotImproved == notImprovedLim or best['resids'].max() < eps or mu.min() > 1e32:\n            if best['resids'].max() > 1. and verbose >= 0:\n                print(INACC_ERR)\n            return best['x'], best['y'], best['z'], best['s']\n\n        if solver == KKTSolvers.LU_FULL:\n            D = bdiag(d)\n            dx_aff, ds_aff, dz_aff, dy_aff = factor_solve_kkt(\n                Q, D, G, A, rx, rs, rz, ry)\n        elif solver == KKTSolvers.LU_PARTIAL:\n            dx_aff, ds_aff, dz_aff, dy_aff = solve_kkt(\n                Q_LU, d, G, A, S_LU, rx, rs, rz, ry)\n        elif solver == KKTSolvers.IR_UNOPT:\n            D = bdiag(d)\n            dx_aff, ds_aff, dz_aff, dy_aff = solve_kkt_ir(\n                Q, D, G, A, rx, rs, rz, ry)\n        else:\n            assert False\n\n        # compute centering directions\n        alpha = torch.min(torch.min(get_step(z, dz_aff),\n                                    get_step(s, ds_aff)),\n                          torch.ones(nBatch).type_as(Q))\n        alpha_nineq = alpha.repeat(nineq, 1).t()\n        t1 = s + alpha_nineq * ds_aff\n        t2 = z + alpha_nineq * dz_aff\n        t3 = torch.sum(t1 * t2, 1).squeeze()\n        t4 = torch.sum(s * z, 1).squeeze()\n        sig = (t3 / t4)**3\n\n        rx = torch.zeros(nBatch, nz).type_as(Q)\n        rs = ((-mu * sig).repeat(nineq, 1).t() + ds_aff * dz_aff) / s\n        rz = torch.zeros(nBatch, nineq).type_as(Q)\n        ry = torch.zeros(nBatch, neq).type_as(Q) if neq > 0 else torch.Tensor()\n\n        if solver == KKTSolvers.LU_FULL:\n            D = bdiag(d)\n            dx_cor, ds_cor, dz_cor, dy_cor = factor_solve_kkt(\n                Q, D, G, A, rx, rs, rz, ry)\n        elif solver == KKTSolvers.LU_PARTIAL:\n            dx_cor, ds_cor, dz_cor, dy_cor = solve_kkt(\n                Q_LU, d, G, A, S_LU, rx, rs, rz, ry)\n        elif solver == KKTSolvers.IR_UNOPT:\n            D = bdiag(d)\n            dx_cor, ds_cor, dz_cor, dy_cor = solve_kkt_ir(\n                Q, D, G, A, rx, rs, rz, ry)\n        else:\n            assert False\n\n        dx = dx_aff + dx_cor\n        ds = ds_aff + ds_cor\n        dz = dz_aff + dz_cor\n        dy = dy_aff + dy_cor if neq > 0 else None\n        alpha = torch.min(0.999 * torch.min(get_step(z, dz),\n                                            get_step(s, ds)),\n                          torch.ones(nBatch).type_as(Q))\n        alpha_nineq = alpha.repeat(nineq, 1).t()\n        alpha_neq = alpha.repeat(neq, 1).t() if neq > 0 else None\n        alpha_nz = alpha.repeat(nz, 1).t()\n\n        x += alpha_nz * dx\n        s += alpha_nineq * ds\n        z += alpha_nineq * dz\n        y = y + alpha_neq * dy if neq > 0 else None\n\n    if best['resids'].max() > 1. and verbose >= 0:\n        print(INACC_ERR)\n    return best['x'], best['y'], best['z'], best['s']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef solve_kkt(Q_LU, d, G, A, S_LU, rx, rs, rz, ry):\n    nineq, nz, neq, nBatch = get_sizes(G, A)\n\n    invQ_rx = rx.btrisolve(*Q_LU)\n    if neq > 0:\n        h = torch.cat((invQ_rx.unsqueeze(1).bmm(A.transpose(1, 2)).squeeze(1) - ry,\n                       invQ_rx.unsqueeze(1).bmm(G.transpose(1, 2)).squeeze(1) + rs / d - rz), 1)\n    else:\n        h = invQ_rx.unsqueeze(1).bmm(G.transpose(1, 2)).squeeze(1) + rs / d - rz\n\n    w = -(h.btrisolve(*S_LU))\n\n    g1 = -rx - w[:, neq:].unsqueeze(1).bmm(G).squeeze(1)\n    if neq > 0:\n        g1 -= w[:, :neq].unsqueeze(1).bmm(A).squeeze(1)\n    g2 = -rs - w[:, neq:]\n\n    dx = g1.btrisolve(*Q_LU)\n    ds = g2 / d\n    dz = w[:, neq:]\n    dy = w[:, :neq] if neq > 0 else None\n\n    return dx, ds, dz, dy", "response": "Solve KKT equations for the affine step"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nperform all one - time factorizations and cache relevant matrix products", "response": "def pre_factor_kkt(Q, G, A):\n    \"\"\" Perform all one-time factorizations and cache relevant matrix products\"\"\"\n    nineq, nz, neq, nBatch = get_sizes(G, A)\n\n    try:\n        Q_LU = btrifact_hack(Q)\n    except:\n        raise RuntimeError(\"\"\"\nqpth Error: Cannot perform LU factorization on Q.\nPlease make sure that your Q matrix is PSD and has\na non-zero diagonal.\n\"\"\")\n\n    # S = [ A Q^{-1} A^T        A Q^{-1} G^T          ]\n    #     [ G Q^{-1} A^T        G Q^{-1} G^T + D^{-1} ]\n    #\n    # We compute a partial LU decomposition of the S matrix\n    # that can be completed once D^{-1} is known.\n    # See the 'Block LU factorization' part of our website\n    # for more details.\n\n    G_invQ_GT = torch.bmm(G, G.transpose(1, 2).btrisolve(*Q_LU))\n    R = G_invQ_GT.clone()\n    S_LU_pivots = torch.IntTensor(range(1, 1 + neq + nineq)).unsqueeze(0) \\\n        .repeat(nBatch, 1).type_as(Q).int()\n    if neq > 0:\n        invQ_AT = A.transpose(1, 2).btrisolve(*Q_LU)\n        A_invQ_AT = torch.bmm(A, invQ_AT)\n        G_invQ_AT = torch.bmm(G, invQ_AT)\n\n        LU_A_invQ_AT = btrifact_hack(A_invQ_AT)\n        P_A_invQ_AT, L_A_invQ_AT, U_A_invQ_AT = torch.btriunpack(*LU_A_invQ_AT)\n        P_A_invQ_AT = P_A_invQ_AT.type_as(A_invQ_AT)\n\n        S_LU_11 = LU_A_invQ_AT[0]\n        U_A_invQ_AT_inv = (P_A_invQ_AT.bmm(L_A_invQ_AT)\n                           ).btrisolve(*LU_A_invQ_AT)\n        S_LU_21 = G_invQ_AT.bmm(U_A_invQ_AT_inv)\n        T = G_invQ_AT.transpose(1, 2).btrisolve(*LU_A_invQ_AT)\n        S_LU_12 = U_A_invQ_AT.bmm(T)\n        S_LU_22 = torch.zeros(nBatch, nineq, nineq).type_as(Q)\n        S_LU_data = torch.cat((torch.cat((S_LU_11, S_LU_12), 2),\n                               torch.cat((S_LU_21, S_LU_22), 2)),\n                              1)\n        S_LU_pivots[:, :neq] = LU_A_invQ_AT[1]\n\n        R -= G_invQ_AT.bmm(T)\n    else:\n        S_LU_data = torch.zeros(nBatch, nineq, nineq).type_as(Q)\n\n    S_LU = [S_LU_data, S_LU_pivots]\n    return Q_LU, S_LU, R"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef factor_kkt(S_LU, R, d):\n    nBatch, nineq = d.size()\n    neq = S_LU[1].size(1) - nineq\n    # TODO: There's probably a better way to add a batched diagonal.\n    global factor_kkt_eye\n    if factor_kkt_eye is None or factor_kkt_eye.size() != d.size():\n        # print('Updating batchedEye size.')\n        factor_kkt_eye = torch.eye(nineq).repeat(\n            nBatch, 1, 1).type_as(R).byte()\n    T = R.clone()\n    T[factor_kkt_eye] += (1. / d).squeeze().view(-1)\n\n    T_LU = btrifact_hack(T)\n\n    global shown_btrifact_warning\n    if shown_btrifact_warning or not T.is_cuda:\n        # TODO: Don't use pivoting in most cases because\n        # torch.btriunpack is inefficient here:\n        oldPivotsPacked = S_LU[1][:, -nineq:] - neq\n        oldPivots, _, _ = torch.btriunpack(\n            T_LU[0], oldPivotsPacked, unpack_data=False)\n        newPivotsPacked = T_LU[1]\n        newPivots, _, _ = torch.btriunpack(\n            T_LU[0], newPivotsPacked, unpack_data=False)\n\n        # Re-pivot the S_LU_21 block.\n        if neq > 0:\n            S_LU_21 = S_LU[0][:, -nineq:, :neq]\n            S_LU[0][:, -nineq:,\n                    :neq] = newPivots.transpose(1, 2).bmm(oldPivots.bmm(S_LU_21))\n\n        # Add the new S_LU_22 block pivots.\n        S_LU[1][:, -nineq:] = newPivotsPacked + neq\n\n    # Add the new S_LU_22 block.\n    S_LU[0][:, -nineq:, -nineq:] = T_LU[0]", "response": "Factor the U22 block that we can only do after we know D."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef forward(inputs_i, Q, G, A, b, h, U_Q, U_S, R, verbose=False):\n    nineq, nz, neq, _ = get_sizes(G, A)\n\n    # find initial values\n    d = torch.ones(nineq).type_as(Q)\n    nb = -b if b is not None else None\n    factor_kkt(U_S, R, d)\n    x, s, z, y = solve_kkt(\n        U_Q, d, G, A, U_S,\n        inputs_i, torch.zeros(nineq).type_as(Q), -h, nb)\n    # x1, s1, z1, y1 = factor_solve_kkt(Q, torch.eye(nineq).type_as(Q), G, A, inputs_i,\n    # torch.zeros(nineq).type_as(Q), -h, nb)\n\n    if torch.min(s) < 0:\n        s -= torch.min(s) - 1\n    if torch.min(z) < 0:\n        z -= torch.min(z) - 1\n\n    prev_resid = None\n    for i in range(20):\n        # affine scaling direction\n        rx = (torch.mv(A.t(), y) if neq > 0 else 0.) + \\\n            torch.mv(G.t(), z) + torch.mv(Q, x) + inputs_i\n        rs = z\n        rz = torch.mv(G, x) + s - h\n        ry = torch.mv(A, x) - b if neq > 0 else torch.Tensor([0.])\n        mu = torch.dot(s, z) / nineq\n        pri_resid = torch.norm(ry) + torch.norm(rz)\n        dual_resid = torch.norm(rx)\n        resid = pri_resid + dual_resid + nineq * mu\n        d = z / s\n        if verbose:\n            print((\"primal_res = {0:.5g}, dual_res = {1:.5g}, \" +\n                   \"gap = {2:.5g}, kappa(d) = {3:.5g}\").format(\n                pri_resid, dual_resid, mu, min(d) / max(d)))\n        # if (pri_resid < 5e-4 and dual_resid < 5e-4 and mu < 4e-4):\n        improved = (prev_resid is None) or (resid < prev_resid + 1e-6)\n        if not improved or resid < 1e-6:\n            return x, y, z\n        prev_resid = resid\n\n        factor_kkt(U_S, R, d)\n        dx_aff, ds_aff, dz_aff, dy_aff = solve_kkt(U_Q, d, G, A, U_S,\n                                                   rx, rs, rz, ry)\n        # D = torch.diag((z/s).cpu()).type_as(Q)\n        # dx_aff1, ds_aff1, dz_aff1, dy_aff1 = factor_solve_kkt(Q, D, G, A, rx, rs, rz, ry)\n\n        # compute centering directions\n        alpha = min(min(get_step(z, dz_aff), get_step(s, ds_aff)), 1.0)\n        sig = (torch.dot(s + alpha * ds_aff, z +\n                         alpha * dz_aff) / (torch.dot(s, z)))**3\n        dx_cor, ds_cor, dz_cor, dy_cor = solve_kkt(\n            U_Q, d, G, A, U_S, torch.zeros(nz).type_as(Q),\n            (-mu * sig * torch.ones(nineq).type_as(Q) + ds_aff * dz_aff) / s,\n            torch.zeros(nineq).type_as(Q), torch.zeros(neq).type_as(Q))\n        # dx_cor, ds_cor, dz_cor, dy_cor = factor_solve_kkt(Q, D, G, A,\n        #     torch.zeros(nz).type_as(Q),\n        #     (-mu*sig*torch.ones(nineq).type_as(Q) + ds_aff*dz_aff)/s,\n        #     torch.zeros(nineq).type_as(Q), torch.zeros(neq).type_as(Q))\n\n        dx = dx_aff + dx_cor\n        ds = ds_aff + ds_cor\n        dz = dz_aff + dz_cor\n        dy = dy_aff + dy_cor if neq > 0 else None\n        alpha = min(1.0, 0.999 * min(get_step(s, ds), get_step(z, dz)))\n        dx_norm = torch.norm(dx)\n        dz_norm = torch.norm(dz)\n        if np.isnan(dx_norm) or dx_norm > 1e5 or dz_norm > 1e5:\n            # Overflow, return early\n            return x, y, z\n\n        x += alpha * dx\n        s += alpha * ds\n        z += alpha * dz\n        y = y + alpha * dy if neq > 0 else None\n\n    return x, y, z", "response": "forward method for the KKT algorithm"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsolve KKT equations for the affine step", "response": "def solve_kkt(U_Q, d, G, A, U_S, rx, rs, rz, ry, dbg=False):\n    \"\"\" Solve KKT equations for the affine step\"\"\"\n    nineq, nz, neq, _ = get_sizes(G, A)\n\n    invQ_rx = torch.potrs(rx.view(-1, 1), U_Q).view(-1)\n    if neq > 0:\n        h = torch.cat([torch.mv(A, invQ_rx) - ry,\n                       torch.mv(G, invQ_rx) + rs / d - rz], 0)\n    else:\n        h = torch.mv(G, invQ_rx) + rs / d - rz\n\n    w = -torch.potrs(h.view(-1, 1), U_S).view(-1)\n\n    g1 = -rx - torch.mv(G.t(), w[neq:])\n    if neq > 0:\n        g1 -= torch.mv(A.t(), w[:neq])\n    g2 = -rs - w[neq:]\n\n    dx = torch.potrs(g1.view(-1, 1), U_Q).view(-1)\n    ds = g2 / d\n    dz = w[neq:]\n    dy = w[:neq] if neq > 0 else None\n\n    # if np.all(np.array([x.norm() for x in [rx, rs, rz, ry]]) != 0):\n    if dbg:\n        import IPython\n        import sys\n        IPython.embed()\n        sys.exit(-1)\n\n    # if rs.norm() > 0: import IPython, sys; IPython.embed(); sys.exit(-1)\n    return dx, ds, dz, dy"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms all one - time factorizations and cache relevant matrix products", "response": "def pre_factor_kkt(Q, G, A):\n    \"\"\" Perform all one-time factorizations and cache relevant matrix products\"\"\"\n    nineq, nz, neq, _ = get_sizes(G, A)\n\n    # S = [ A Q^{-1} A^T        A Q^{-1} G^T           ]\n    #     [ G Q^{-1} A^T        G Q^{-1} G^T + D^{-1} ]\n\n    U_Q = torch.potrf(Q)\n    # partial cholesky of S matrix\n    U_S = torch.zeros(neq + nineq, neq + nineq).type_as(Q)\n\n    G_invQ_GT = torch.mm(G, torch.potrs(G.t(), U_Q))\n    R = G_invQ_GT\n    if neq > 0:\n        invQ_AT = torch.potrs(A.t(), U_Q)\n        A_invQ_AT = torch.mm(A, invQ_AT)\n        G_invQ_AT = torch.mm(G, invQ_AT)\n\n        # TODO: torch.potrf sometimes says the matrix is not PSD but\n        # numpy does? I filed an issue at\n        # https://github.com/pytorch/pytorch/issues/199\n        try:\n            U11 = torch.potrf(A_invQ_AT)\n        except:\n            U11 = torch.Tensor(np.linalg.cholesky(\n                A_invQ_AT.cpu().numpy())).type_as(A_invQ_AT)\n\n        # TODO: torch.trtrs is currently not implemented on the GPU\n        # and we are using gesv as a workaround.\n        U12 = torch.gesv(G_invQ_AT.t(), U11.t())[0]\n        U_S[:neq, :neq] = U11\n        U_S[:neq, neq:] = U12\n        R -= torch.mm(U12.t(), U12)\n\n    return U_Q, U_S, R"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef factor_kkt(U_S, R, d):\n    nineq = R.size(0)\n    U_S[-nineq:, -nineq:] = torch.potrf(R + torch.diag(1 / d.cpu()).type_as(d))", "response": "Factor the U22 block that we can only do after we know D."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef timeout(seconds=None, use_signals=True, timeout_exception=TimeoutError, exception_message=None):\n    def decorate(function):\n\n        if not seconds:\n            return function\n\n        if use_signals:\n            def handler(signum, frame):\n                _raise_exception(timeout_exception, exception_message)\n\n            @wraps(function)\n            def new_function(*args, **kwargs):\n                new_seconds = kwargs.pop('timeout', seconds)\n                if new_seconds:\n                    old = signal.signal(signal.SIGALRM, handler)\n                    signal.setitimer(signal.ITIMER_REAL, new_seconds)\n                try:\n                    return function(*args, **kwargs)\n                finally:\n                    if new_seconds:\n                        signal.setitimer(signal.ITIMER_REAL, 0)\n                        signal.signal(signal.SIGALRM, old)\n            return new_function\n        else:\n            @wraps(function)\n            def new_function(*args, **kwargs):\n                timeout_wrapper = _Timeout(function, timeout_exception, exception_message, seconds)\n                return timeout_wrapper(*args, **kwargs)\n            return new_function\n\n    return decorate", "response": "Decorator that adds a timeout parameter to a function and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _target(queue, function, *args, **kwargs):\n    try:\n        queue.put((True, function(*args, **kwargs)))\n    except:\n        queue.put((False, sys.exc_info()[1]))", "response": "This is a helper function for the Process created in _Timeout. It runs a function with positional arguments and keyword arguments and returns the function s output via a queue."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cancel(self):\n        if self.__process.is_alive():\n            self.__process.terminate()\n\n        _raise_exception(self.__timeout_exception, self.__exception_message)", "response": "Terminate any possible execution of the embedded function."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ready(self):\n        if self.__timeout < time.time():\n            self.cancel()\n        return self.__queue.full() and not self.__queue.empty()", "response": "Read - only property indicating status of value property."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads - only property containing data returned from function.", "response": "def value(self):\n        \"\"\"Read-only property containing data returned from function.\"\"\"\n        if self.ready is True:\n            flag, load = self.__queue.get()\n            if flag:\n                return load\n            raise load"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef big_bounding_box(paths_n_stuff):\n    bbs = []\n    for thing in paths_n_stuff:\n        if is_path_segment(thing) or isinstance(thing, Path):\n            bbs.append(thing.bbox())\n        elif isinstance(thing, complex):\n            bbs.append((thing.real, thing.real, thing.imag, thing.imag))\n        else:\n            try:\n                complexthing = complex(thing)\n                bbs.append((complexthing.real, complexthing.real,\n                            complexthing.imag, complexthing.imag))\n            except ValueError:\n                raise TypeError(\n                    \"paths_n_stuff can only contains Path, CubicBezier, \"\n                    \"QuadraticBezier, Line, and complex objects.\")\n    xmins, xmaxs, ymins, ymaxs = list(zip(*bbs))\n    xmin = min(xmins)\n    xmax = max(xmaxs)\n    ymin = min(ymins)\n    ymax = max(ymaxs)\n    return xmin, xmax, ymin, ymax", "response": "Finds a BB containing a collection of paths Bezier path segments and a sequence of Bezier path segments and and\n    points."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntakes in a list of paths and creates an SVG file containing said paths.", "response": "def disvg(paths=None, colors=None,\n          filename=os_path.join(getcwd(), 'disvg_output.svg'),\n          stroke_widths=None, nodes=None, node_colors=None, node_radii=None,\n          openinbrowser=True, timestamp=False,\n          margin_size=0.1, mindim=600, dimensions=None,\n          viewbox=None, text=None, text_path=None, font_size=None,\n          attributes=None, svg_attributes=None, svgwrite_debug=False, paths2Drawing=False):\n    \"\"\"Takes in a list of paths and creates an SVG file containing said paths.\n    REQUIRED INPUTS:\n        :param paths - a list of paths\n\n    OPTIONAL INPUT:\n        :param colors - specifies the path stroke color.  By default all paths\n        will be black (#000000).  This paramater can be input in a few ways\n        1) a list of strings that will be input into the path elements stroke\n            attribute (so anything that is understood by the svg viewer).\n        2) a string of single character colors -- e.g. setting colors='rrr' is\n            equivalent to setting colors=['red', 'red', 'red'] (see the\n            'color_dict' dictionary above for a list of possibilities).\n        3) a list of rgb 3-tuples -- e.g. colors = [(255, 0, 0), ...].\n\n        :param filename - the desired location/filename of the SVG file\n        created (by default the SVG will be stored in the current working\n        directory and named 'disvg_output.svg').\n\n        :param stroke_widths - a list of stroke_widths to use for paths\n        (default is 0.5% of the SVG's width or length)\n\n        :param nodes - a list of points to draw as filled-in circles\n\n        :param node_colors - a list of colors to use for the nodes (by default\n        nodes will be red)\n\n        :param node_radii - a list of radii to use for the nodes (by default\n        nodes will be radius will be 1 percent of the svg's width/length)\n\n        :param text - string or list of strings to be displayed\n\n        :param text_path - if text is a list, then this should be a list of\n        path (or path segments of the same length.  Note: the path must be\n        long enough to display the text or the text will be cropped by the svg\n        viewer.\n\n        :param font_size - a single float of list of floats.\n\n        :param openinbrowser -  Set to True to automatically open the created\n        SVG in the user's default web browser.\n\n        :param timestamp - if True, then the a timestamp will be appended to\n        the output SVG's filename.  This will fix issues with rapidly opening\n        multiple SVGs in your browser.\n\n        :param margin_size - The min margin (empty area framing the collection\n        of paths) size used for creating the canvas and background of the SVG.\n\n        :param mindim - The minimum dimension (height or width) of the output\n        SVG (default is 600).\n\n        :param dimensions - The (x,y) display dimensions of the output SVG.\n        I.e. this specifies the `width` and `height` SVG attributes. Note that \n        these also can be used to specify units other than pixels. Using this \n        will override the `mindim` parameter.\n\n        :param viewbox - This specifies the coordinated system used in the svg.\n        The SVG `viewBox` attribute works together with the the `height` and \n        `width` attrinutes.  Using these three attributes allows for shifting \n        and scaling of the SVG canvas without changing the any values other \n        than those in `viewBox`, `height`, and `width`.  `viewbox` should be \n        input as a 4-tuple, (min_x, min_y, width, height), or a string \n        \"min_x min_y width height\".  Using this will override the `mindim` \n        parameter.\n\n        :param attributes - a list of dictionaries of attributes for the input\n        paths.  Note: This will override any other conflicting settings.\n\n        :param svg_attributes - a dictionary of attributes for output svg.\n        \n        :param svgwrite_debug - This parameter turns on/off `svgwrite`'s \n        debugging mode.  By default svgwrite_debug=False.  This increases \n        speed and also prevents `svgwrite` from raising of an error when not \n        all `svg_attributes` key-value pairs are understood.\n        \n        :param paths2Drawing - If true, an `svgwrite.Drawing` object is \n        returned and no file is written.  This `Drawing` can later be saved \n        using the `svgwrite.Drawing.save()` method.\n\n    NOTES:\n        * The `svg_attributes` parameter will override any other conflicting \n        settings.\n\n        * Any `extra` parameters that `svgwrite.Drawing()` accepts can be \n        controlled by passing them in through `svg_attributes`.\n\n        * The unit of length here is assumed to be pixels in all variables.\n\n        * If this function is used multiple times in quick succession to\n        display multiple SVGs (all using the default filename), the\n        svgviewer/browser will likely fail to load some of the SVGs in time.\n        To fix this, use the timestamp attribute, or give the files unique\n        names, or use a pause command (e.g. time.sleep(1)) between uses.\n    \"\"\"\n\n\n    _default_relative_node_radius = 5e-3\n    _default_relative_stroke_width = 1e-3\n    _default_path_color = '#000000'  # black\n    _default_node_color = '#ff0000'  # red\n    _default_font_size = 12\n\n\n    # append directory to filename (if not included)\n    if os_path.dirname(filename) == '':\n        filename = os_path.join(getcwd(), filename)\n\n    # append time stamp to filename\n    if timestamp:\n        fbname, fext = os_path.splitext(filename)\n        dirname = os_path.dirname(filename)\n        tstamp = str(time()).replace('.', '')\n        stfilename = os_path.split(fbname)[1] + '_' + tstamp + fext\n        filename = os_path.join(dirname, stfilename)\n\n    # check paths and colors are set\n    if isinstance(paths, Path) or is_path_segment(paths):\n        paths = [paths]\n    if paths:\n        if not colors:\n            colors = [_default_path_color] * len(paths)\n        else:\n            assert len(colors) == len(paths)\n            if isinstance(colors, str):\n                colors = str2colorlist(colors,\n                                       default_color=_default_path_color)\n            elif isinstance(colors, list):\n                for idx, c in enumerate(colors):\n                    if is3tuple(c):\n                        colors[idx] = \"rgb\" + str(c)\n\n    # check nodes and nodes_colors are set (node_radii are set later)\n    if nodes:\n        if not node_colors:\n            node_colors = [_default_node_color] * len(nodes)\n        else:\n            assert len(node_colors) == len(nodes)\n            if isinstance(node_colors, str):\n                node_colors = str2colorlist(node_colors,\n                                            default_color=_default_node_color)\n            elif isinstance(node_colors, list):\n                for idx, c in enumerate(node_colors):\n                    if is3tuple(c):\n                        node_colors[idx] = \"rgb\" + str(c)\n\n    # set up the viewBox and display dimensions of the output SVG\n    # along the way, set stroke_widths and node_radii if not provided\n    assert paths or nodes\n    stuff2bound = []\n    if viewbox:\n        if not isinstance(viewbox, str):\n            viewbox = '%s %s %s %s' % viewbox\n        if dimensions is None:\n            dimensions = viewbox.split(' ')[2:4]\n    elif dimensions:\n        dimensions = tuple(map(str, dimensions))\n        def strip_units(s):\n            return re.search(r'\\d*\\.?\\d*', s.strip()).group()\n        viewbox = '0 0 %s %s' % tuple(map(strip_units, dimensions))\n    else:\n        if paths:\n            stuff2bound += paths\n        if nodes:\n            stuff2bound += nodes\n        if text_path:\n            stuff2bound += text_path\n        xmin, xmax, ymin, ymax = big_bounding_box(stuff2bound)\n        dx = xmax - xmin\n        dy = ymax - ymin\n\n        if dx == 0:\n            dx = 1\n        if dy == 0:\n            dy = 1\n\n        # determine stroke_widths to use (if not provided) and max_stroke_width\n        if paths:\n            if not stroke_widths:\n                sw = max(dx, dy) * _default_relative_stroke_width\n                stroke_widths = [sw]*len(paths)\n                max_stroke_width = sw\n            else:\n                assert len(paths) == len(stroke_widths)\n                max_stroke_width = max(stroke_widths)\n        else:\n            max_stroke_width = 0\n\n        # determine node_radii to use (if not provided) and max_node_diameter\n        if nodes:\n            if not node_radii:\n                r = max(dx, dy) * _default_relative_node_radius\n                node_radii = [r]*len(nodes)\n                max_node_diameter = 2*r\n            else:\n                assert len(nodes) == len(node_radii)\n                max_node_diameter = 2*max(node_radii)\n        else:\n            max_node_diameter = 0\n\n        extra_space_for_style = max(max_stroke_width, max_node_diameter)\n        xmin -= margin_size*dx + extra_space_for_style/2\n        ymin -= margin_size*dy + extra_space_for_style/2\n        dx += 2*margin_size*dx + extra_space_for_style\n        dy += 2*margin_size*dy + extra_space_for_style\n        viewbox = \"%s %s %s %s\" % (xmin, ymin, dx, dy)\n\n        if dx > dy:\n            szx = str(mindim) + 'px'\n            szy = str(int(ceil(mindim * dy / dx))) + 'px'\n        else:\n            szx = str(int(ceil(mindim * dx / dy))) + 'px'\n            szy = str(mindim) + 'px'\n        dimensions = szx, szy\n\n    # Create an SVG file\n    if svg_attributes is not None:\n        dimensions = (svg_attributes.get(\"width\", dimensions[0]),\n                      svg_attributes.get(\"height\", dimensions[1]))\n        debug = svg_attributes.get(\"debug\", svgwrite_debug)\n        dwg = Drawing(filename=filename, size=dimensions, debug=debug,\n                      **svg_attributes)\n    else:\n        dwg = Drawing(filename=filename, size=dimensions, debug=svgwrite_debug,\n                      viewBox=viewbox)\n\n    # add paths\n    if paths:\n        for i, p in enumerate(paths):\n            if isinstance(p, Path):\n                ps = p.d()\n            elif is_path_segment(p):\n                ps = Path(p).d()\n            else:  # assume this path, p, was input as a Path d-string\n                ps = p\n\n            if attributes:\n                good_attribs = {'d': ps}\n                for key in attributes[i]:\n                    val = attributes[i][key]\n                    if key != 'd':\n                        try:\n                            dwg.path(ps, **{key: val})\n                            good_attribs.update({key: val})\n                        except Exception as e:\n                            warn(str(e))\n\n                dwg.add(dwg.path(**good_attribs))\n            else:\n                dwg.add(dwg.path(ps, stroke=colors[i],\n                                 stroke_width=str(stroke_widths[i]),\n                                 fill='none'))\n\n    # add nodes (filled in circles)\n    if nodes:\n        for i_pt, pt in enumerate([(z.real, z.imag) for z in nodes]):\n            dwg.add(dwg.circle(pt, node_radii[i_pt], fill=node_colors[i_pt]))\n\n    # add texts\n    if text:\n        assert isinstance(text, str) or (isinstance(text, list) and\n                                         isinstance(text_path, list) and\n                                         len(text_path) == len(text))\n        if isinstance(text, str):\n            text = [text]\n            if not font_size:\n                font_size = [_default_font_size]\n            if not text_path:\n                pos = complex(xmin + margin_size*dx, ymin + margin_size*dy)\n                text_path = [Line(pos, pos + 1).d()]\n        else:\n            if font_size:\n                if isinstance(font_size, list):\n                    assert len(font_size) == len(text)\n                else:\n                    font_size = [font_size] * len(text)\n            else:\n                font_size = [_default_font_size] * len(text)\n        for idx, s in enumerate(text):\n            p = text_path[idx]\n            if isinstance(p, Path):\n                ps = p.d()\n            elif is_path_segment(p):\n                ps = Path(p).d()\n            else:  # assume this path, p, was input as a Path d-string\n                ps = p\n\n            # paragraph = dwg.add(dwg.g(font_size=font_size[idx]))\n            # paragraph.add(dwg.textPath(ps, s))\n            pathid = 'tp' + str(idx)\n            dwg.defs.add(dwg.path(d=ps, id=pathid))\n            txter = dwg.add(dwg.text('', font_size=font_size[idx]))\n            txter.add(txt.TextPath('#'+pathid, s))\n\n    if paths2Drawing:\n        return dwg\n      \n    # save svg\n    if not os_path.exists(os_path.dirname(filename)):\n        makedirs(os_path.dirname(filename))\n    dwg.save()\n\n    # re-open the svg, make the xml pretty, and save it again\n    xmlstring = md_xml_parse(filename).toprettyxml()\n    with open(filename, 'w') as f:\n        f.write(xmlstring)\n\n    # try to open in web browser\n    if openinbrowser:\n        try:\n            open_in_browser(filename)\n        except:\n            print(\"Failed to open output SVG in browser.  SVG saved to:\")\n            print(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the roots of a polynomial with coefficients given in p.", "response": "def polyroots(p, realroots=False, condition=lambda r: True):\n    \"\"\"\n    Returns the roots of a polynomial with coefficients given in p.\n      p[0] * x**n + p[1] * x**(n-1) + ... + p[n-1]*x + p[n]\n    INPUT:\n    p - Rank-1 array-like object of polynomial coefficients.\n    realroots - a boolean.  If true, only real roots will be returned  and the\n        condition function can be written assuming all roots are real.\n    condition - a boolean-valued function.  Only roots satisfying this will be\n        returned.  If realroots==True, these conditions should assume the roots\n        are real.\n    OUTPUT:\n    A list containing the roots of the polynomial.\n    NOTE:  This uses np.isclose and np.roots\"\"\"\n    roots = np.roots(p)\n    if realroots:\n        roots = [r.real for r in roots if isclose(r.imag, 0)]\n    roots = [r for r in roots if condition(r)]\n\n    duplicates = []\n    for idx, (r1, r2) in enumerate(combinations(roots, 2)):\n        if isclose(r1, r2):\n            duplicates.append(idx)\n    return [r for idx, r in enumerate(roots) if idx not in duplicates]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the limit of the rational function f and g as t approaches t0.", "response": "def rational_limit(f, g, t0):\n    \"\"\"Computes the limit of the rational function (f/g)(t)\n    as t approaches t0.\"\"\"\n    assert isinstance(f, np.poly1d) and isinstance(g, np.poly1d)\n    assert g != np.poly1d([0])\n    if g(t0) != 0:\n        return f(t0)/g(t0)\n    elif f(t0) == 0:\n        return rational_limit(f.deriv(), g.deriv(), t0)\n    else:\n        raise ValueError(\"Limit does not exist.\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a bounding box 4 - tuple to a Path object.", "response": "def bbox2path(xmin, xmax, ymin, ymax):\n    \"\"\"Converts a bounding box 4-tuple to a Path object.\"\"\"\n    b = Line(xmin + 1j*ymin, xmax + 1j*ymin)\n    t = Line(xmin + 1j*ymax, xmax + 1j*ymax)\n    r = Line(xmax + 1j*ymin, xmax + 1j*ymax)\n    l = Line(xmin + 1j*ymin, xmin + 1j*ymax)\n    return Path(b, r, t.reversed(), l.reversed())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef polyline(*points):\n    return Path(*[Line(points[i], points[i+1])\n                  for i in range(len(points) - 1)])", "response": "Converts a list of points to a Path composed of lines connecting those \n    points."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef polygon(*points):\n    return Path(*[Line(points[i], points[(i + 1) % len(points)])\n                  for i in range(len(points))])", "response": "Converts a list of points to a Path composed of lines connecting those \n    points then closes the path by connecting the last point to the first."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bpoints2bezier(bpoints):\n    order = len(bpoints) - 1\n    if order == 3:\n        return CubicBezier(*bpoints)\n    elif order == 2:\n        return QuadraticBezier(*bpoints)\n    elif order == 1:\n        return Line(*bpoints)\n    else:\n        assert len(bpoints) in {2, 3, 4}", "response": "Converts a list of length 2 3 or 4 to a CubicBezier QuadraticBezier or Line object respectively."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a cubic or lower order Polynomial object or a sequence of coefficients or a Line object as appropriate.", "response": "def poly2bez(poly, return_bpoints=False):\n    \"\"\"Converts a cubic or lower order Polynomial object (or a sequence of\n    coefficients) to a CubicBezier, QuadraticBezier, or Line object as\n    appropriate.  If return_bpoints=True then this will instead only return\n    the control points of the corresponding Bezier curve.\n    Note: The inverse operation is available as a method of CubicBezier,\n    QuadraticBezier and Line objects.\"\"\"\n    bpoints = polynomial2bezier(poly)\n    if return_bpoints:\n        return bpoints\n    else:\n        return bpoints2bezier(bpoints)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a Bezier object or tuple of Bezier control points to a tuple of polynomials.", "response": "def bez2poly(bez, numpy_ordering=True, return_poly1d=False):\n    \"\"\"Converts a Bezier object or tuple of Bezier control points to a tuple\n    of coefficients of the expanded polynomial.\n    return_poly1d : returns a numpy.poly1d object.  This makes computations\n    of derivatives/anti-derivatives and many other operations quite quick.\n    numpy_ordering : By default (to accommodate numpy) the coefficients will\n    be output in reverse standard order.\n    Note:  This function is redundant thanks to the .poly() method included\n    with all bezier segment classes.\"\"\"\n    if is_bezier_segment(bez):\n        bez = bez.bpoints()\n    return bezier2polynomial(bez,\n                             numpy_ordering=numpy_ordering,\n                             return_poly1d=return_poly1d)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new object that rotates the curve around the point origin.", "response": "def rotate(curve, degs, origin=None):\n    \"\"\"Returns curve rotated by `degs` degrees (CCW) around the point `origin`\n    (a complex number).  By default origin is either `curve.point(0.5)`, or in\n    the case that curve is an Arc object, `origin` defaults to `curve.center`.\n    \"\"\"\n    def transform(z):\n        return exp(1j*radians(degs))*(z - origin) + origin\n\n    if origin is None:\n        if isinstance(curve, Arc):\n            origin = curve.center\n        else:\n            origin = curve.point(0.5)\n\n    if isinstance(curve, Path):\n        return Path(*[rotate(seg, degs, origin=origin) for seg in curve])\n    elif is_bezier_segment(curve):\n        return bpoints2bezier([transform(bpt) for bpt in curve.bpoints()])\n    elif isinstance(curve, Arc):\n        new_start = transform(curve.start)\n        new_end = transform(curve.end)\n        new_rotation = curve.rotation + degs\n        return Arc(new_start, radius=curve.radius, rotation=new_rotation,\n                   large_arc=curve.large_arc, sweep=curve.sweep, end=new_end)\n    else:\n        raise TypeError(\"Input `curve` should be a Path, Line, \"\n                        \"QuadraticBezier, CubicBezier, or Arc object.\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nshift the curve by the complex quantity z such that translate ( curve z0 ) = z0.", "response": "def translate(curve, z0):\n    \"\"\"Shifts the curve by the complex quantity z such that\n    translate(curve, z0).point(t) = curve.point(t) + z0\"\"\"\n    if isinstance(curve, Path):\n        return Path(*[translate(seg, z0) for seg in curve])\n    elif is_bezier_segment(curve):\n        return bpoints2bezier([bpt + z0 for bpt in curve.bpoints()])\n    elif isinstance(curve, Arc):\n        new_start = curve.start + z0\n        new_end = curve.end + z0\n        return Arc(new_start, radius=curve.radius, rotation=curve.rotation,\n                   large_arc=curve.large_arc, sweep=curve.sweep, end=new_end)\n    else:\n        raise TypeError(\"Input `curve` should be a Path, Line, \"\n                        \"QuadraticBezier, CubicBezier, or Arc object.\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nscale a curve about origin by diagonal matrix of sx and sy.", "response": "def scale(curve, sx, sy=None, origin=0j):\n    \"\"\"Scales `curve`, about `origin`, by diagonal matrix `[[sx,0],[0,sy]]`.\n\n    Notes:\n    ------\n    * If `sy` is not specified, it is assumed to be equal to `sx` and \n    a scalar transformation of `curve` about `origin` will be returned.\n    I.e.\n        scale(curve, sx, origin).point(t) == \n            ((curve.point(t) - origin) * sx) + origin\n    \"\"\"\n\n    if sy is None:\n        isy = 1j*sx\n    else:\n        isy = 1j*sy\n\n    def _scale(z):\n        if sy is None:\n            return sx*z\n        return sx*z.real + isy*z.imag          \n\n    def scale_bezier(bez):\n        p = [_scale(c) for c in bez2poly(bez)]\n        p[-1] += origin - _scale(origin)\n        return poly2bez(p)\n\n    if isinstance(curve, Path):\n        return Path(*[scale(seg, sx, sy, origin) for seg in curve])\n    elif is_bezier_segment(curve):\n        return scale_bezier(curve)\n    elif isinstance(curve, Arc):\n        if sy is None or sy == sx:\n            return Arc(start=sx*(curve.start - origin) + origin,\n                       radius=sx*curve.radius,\n                       rotation=curve.rotation, \n                       large_arc=curve.large_arc, \n                       sweep=curve.sweep, \n                       end=sx*(curve.end - origin) + origin)\n        else:\n            raise Exception(\"\\nFor `Arc` objects, only scale transforms \"\n                            \"with sx==sy are implemented.\\n\")\n    else:\n        raise TypeError(\"Input `curve` should be a Path, Line, \"\n                        \"QuadraticBezier, CubicBezier, or Arc object.\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntransform the curve by the homogeneous transformation matrix tf", "response": "def transform(curve, tf):\n    \"\"\"Transforms the curve by the homogeneous transformation matrix tf\"\"\"\n    def to_point(p):\n        return np.array([[p.real], [p.imag], [1.0]])\n\n    def to_vector(z):\n        return np.array([[z.real], [z.imag], [0.0]])\n\n    def to_complex(v):\n        return v.item(0) + 1j * v.item(1)\n\n    if isinstance(curve, Path):\n        return Path(*[transform(segment, tf) for segment in curve])\n    elif is_bezier_segment(curve):\n        return bpoints2bezier([to_complex(tf.dot(to_point(p)))\n                               for p in curve.bpoints()])\n    elif isinstance(curve, Arc):\n        new_start = to_complex(tf.dot(to_point(curve.start)))\n        new_end = to_complex(tf.dot(to_point(curve.end)))\n        new_radius = to_complex(tf.dot(to_vector(curve.radius)))\n        return Arc(new_start, radius=new_radius, rotation=curve.rotation,\n                   large_arc=curve.large_arc, sweep=curve.sweep, end=new_end)\n    else:\n        raise TypeError(\"Input `curve` should be a Path, Line, \"\n                        \"QuadraticBezier, CubicBezier, or Arc object.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the unit tangent of the segment at t.", "response": "def bezier_unit_tangent(seg, t):\n    \"\"\"Returns the unit tangent of the segment at t.\n\n    Notes\n    -----\n    If you receive a RuntimeWarning, try the following:\n    >>> import numpy\n    >>> old_numpy_error_settings = numpy.seterr(invalid='raise')\n    This can be undone with:\n    >>> numpy.seterr(**old_numpy_error_settings)\n    \"\"\"\n    assert 0 <= t <= 1\n    dseg = seg.derivative(t)\n\n    # Note: dseg might be numpy value, use np.seterr(invalid='raise')\n    try:\n        unit_tangent = dseg/abs(dseg)\n    except (ZeroDivisionError, FloatingPointError):\n        # This may be a removable singularity, if so we just need to compute\n        # the limit.\n        # Note: limit{{dseg / abs(dseg)} = sqrt(limit{dseg**2 / abs(dseg)**2})\n        dseg_poly = seg.poly().deriv()\n        dseg_abs_squared_poly = (real(dseg_poly) ** 2 +\n                                 imag(dseg_poly) ** 2)\n        try:\n            unit_tangent = csqrt(rational_limit(dseg_poly**2,\n                                            dseg_abs_squared_poly, t))\n        except ValueError:\n            bef = seg.poly().deriv()(t - 1e-4)\n            aft = seg.poly().deriv()(t + 1e-4)\n            mes = (\"Unit tangent appears to not be well-defined at \"\n                   \"t = {}, \\n\".format(t) +\n                   \"seg.poly().deriv()(t - 1e-4) = {}\\n\".format(bef) +\n                   \"seg.poly().deriv()(t + 1e-4) = {}\".format(aft))\n            raise ValueError(mes)\n    return unit_tangent"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the curvature of the segment at t", "response": "def segment_curvature(self, t, use_inf=False):\n    \"\"\"returns the curvature of the segment at t.\n\n    Notes\n    -----\n    If you receive a RuntimeWarning, run command\n    >>> old = np.seterr(invalid='raise')\n    This can be undone with\n    >>> np.seterr(**old)\n    \"\"\"\n\n    dz = self.derivative(t)\n    ddz = self.derivative(t, n=2)\n    dx, dy = dz.real, dz.imag\n    ddx, ddy = ddz.real, ddz.imag\n    old_np_seterr = np.seterr(invalid='raise')\n    try:\n        kappa = abs(dx*ddy - dy*ddx)/sqrt(dx*dx + dy*dy)**3\n    except (ZeroDivisionError, FloatingPointError):\n        # tangent vector is zero at t, use polytools to find limit\n        p = self.poly()\n        dp = p.deriv()\n        ddp = dp.deriv()\n        dx, dy = real(dp), imag(dp)\n        ddx, ddy = real(ddp), imag(ddp)\n        f2 = (dx*ddy - dy*ddx)**2\n        g2 = (dx*dx + dy*dy)**3\n        lim2 = rational_limit(f2, g2, t)\n        if lim2 < 0:  # impossible, must be numerical error\n            return 0\n        kappa = sqrt(lim2)\n    finally:\n        np.seterr(**old_np_seterr)\n    return kappa"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bezier_radialrange(seg, origin, return_all_global_extrema=False):\n\n    def _radius(tau):\n        return abs(seg.point(tau) - origin)\n\n    shifted_seg_poly = seg.poly() - origin\n    r_squared = real(shifted_seg_poly) ** 2 + \\\n                imag(shifted_seg_poly) ** 2\n    extremizers = [0, 1] + polyroots01(r_squared.deriv())\n    extrema = [(_radius(t), t) for t in extremizers]\n\n    if return_all_global_extrema:\n        raise NotImplementedError\n    else:\n        seg_global_min = min(extrema, key=itemgetter(0))\n        seg_global_max = max(extrema, key=itemgetter(0))\n        return seg_global_min, seg_global_max", "response": "returns the min and max values which minimize and\n    maximize respectively the distance d = |self. point ( t ) - origin|"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn true if pt is a point enclosed by path.", "response": "def path_encloses_pt(pt, opt, path):\n    \"\"\"returns true if pt is a point enclosed by path (which must be a Path\n    object satisfying path.isclosed==True).  opt is a point you know is\n    NOT enclosed by path.\"\"\"\n    assert path.isclosed()\n    intersections = Path(Line(pt, opt)).intersect(path)\n    if len(intersections) % 2:\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the length of a segment by straight lines", "response": "def segment_length(curve, start, end, start_point, end_point,\n                   error=LENGTH_ERROR, min_depth=LENGTH_MIN_DEPTH, depth=0):\n    \"\"\"Recursively approximates the length by straight lines\"\"\"\n    mid = (start + end)/2\n    mid_point = curve.point(mid)\n    length = abs(end_point - start_point)\n    first_half = abs(mid_point - start_point)\n    second_half = abs(end_point - mid_point)\n\n    length2 = first_half + second_half\n    if (length2 - length > error) or (depth < min_depth):\n        # Calculate the length of each segment:\n        depth += 1\n        return (segment_length(curve, start, mid, start_point, mid_point,\n                               error, min_depth, depth) +\n                segment_length(curve, mid, end, mid_point, end_point,\n                               error, min_depth, depth))\n    # This is accurate enough.\n    return length2"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef inv_arclength(curve, s, s_tol=ILENGTH_S_TOL, maxits=ILENGTH_MAXITS,\n                  error=ILENGTH_ERROR, min_depth=ILENGTH_MIN_DEPTH):\n    \"\"\"INPUT: curve should be a CubicBezier, Line, of Path of CubicBezier\n    and/or Line objects.\n    OUTPUT: Returns a float, t, such that the arc length of curve from 0 to\n    t is approximately s.\n    s_tol - exit when |s(t) - s| < s_tol where\n        s(t) = seg.length(0, t, error, min_depth) and seg is either curve or,\n        if curve is a Path object, then seg is a segment in curve.\n    error - used to compute lengths of cubics and arcs\n    min_depth - used to compute lengths of cubics and arcs\n    Note:  This function is not designed to be efficient, but if it's slower\n    than you need, make sure you have scipy installed.\"\"\"\n\n    curve_length = curve.length(error=error, min_depth=min_depth)\n    assert curve_length > 0\n    if not 0 <= s <= curve_length:\n        raise ValueError(\"s is not in interval [0, curve.length()].\")\n\n    if s == 0:\n        return 0\n    if s == curve_length:\n        return 1\n\n    if isinstance(curve, Path):\n        seg_lengths = [seg.length(error=error, min_depth=min_depth)\n                       for seg in curve]\n        lsum = 0\n        # Find which segment the point we search for is located on\n        for k, len_k in enumerate(seg_lengths):\n            if lsum <= s <= lsum + len_k:\n                t = inv_arclength(curve[k], s - lsum, s_tol=s_tol,\n                                  maxits=maxits, error=error,\n                                  min_depth=min_depth)\n                return curve.t2T(k, t)\n            lsum += len_k\n        return 1\n\n    elif isinstance(curve, Line):\n        return s / curve.length(error=error, min_depth=min_depth)\n\n    elif (isinstance(curve, QuadraticBezier) or\n          isinstance(curve, CubicBezier) or\n          isinstance(curve, Arc)):\n        t_upper = 1\n        t_lower = 0\n        iteration = 0\n        while iteration < maxits:\n            iteration += 1\n            t = (t_lower + t_upper)/2\n            s_t = curve.length(t1=t, error=error, min_depth=min_depth)\n            if abs(s_t - s) < s_tol:\n                return t\n            elif s_t < s:  # t too small\n                t_lower = t\n            else:  # s < s_t, t too big\n                t_upper = t\n            if t_upper == t_lower:\n                warn(\"t is as close as a float can be to the correct value, \"\n                     \"but |s(t) - s| = {} > s_tol\".format(abs(s_t-s)))\n                return t\n        raise Exception(\"Maximum iterations reached with s(t) - s = {}.\"\n                        \"\".format(s_t - s))\n    else:\n        raise TypeError(\"First argument must be a Line, QuadraticBezier, \"\n                        \"CubicBezier, Arc, or Path object.\")", "response": "This function calculates the arc length of a CubicBezier curve and a Line object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a cropped copy of this segment which starts at t0 and ends at t1 and returns a cropped copy of this segment which starts at t0 and ends at t1.", "response": "def crop_bezier(seg, t0, t1):\n    \"\"\"returns a cropped copy of this segment which starts at self.point(t0)\n    and ends at self.point(t1).\"\"\"\n    assert t0 < t1\n    if t0 == 0:\n        cropped_seg = seg.split(t1)[0]\n    elif t1 == 1:\n        cropped_seg = seg.split(t0)[1]\n    else:\n        pt1 = seg.point(t1)\n\n        # trim off the 0 <= t < t0 part\n        trimmed_seg = crop_bezier(seg, t0, 1)\n\n        # find the adjusted t1 (i.e. the t1 such that\n        # trimmed_seg.point(t1) ~= pt))and trim off the t1 < t <= 1 part\n        t1_adj = trimmed_seg.radialrange(pt1)[0][1]\n        cropped_seg = crop_bezier(trimmed_seg, 0, t1_adj)\n    return cropped_seg"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if this segment joins smoothly with previous segment.", "response": "def joins_smoothly_with(self, previous, wrt_parameterization=False):\n        \"\"\"Checks if this segment joins smoothly with previous segment.  By\n        default, this only checks that this segment starts moving (at t=0) in\n        the same direction (and from the same positive) as previous stopped\n        moving (at t=1).  To check if the tangent magnitudes also match, set\n        wrt_parameterization=True.\"\"\"\n        if wrt_parameterization:\n            return self.start == previous.end and np.isclose(\n                self.derivative(0), previous.derivative(1))\n        else:\n            return self.start == previous.end and np.isclose(\n                self.unit_tangent(0), previous.unit_tangent(1))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef point(self, t):\n        distance = self.end - self.start\n        return self.start + distance*t", "response": "returns the coordinates of the Bezier curve evaluated at t"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the length of the line segment between t0 and t1", "response": "def length(self, t0=0, t1=1, error=None, min_depth=None):\n        \"\"\"returns the length of the line segment between t0 and t1.\"\"\"\n        return abs(self.end - self.start)*(t1-t0)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a float t such that self. length is approximately s. ilength", "response": "def ilength(self, s, s_tol=ILENGTH_S_TOL, maxits=ILENGTH_MAXITS,\n                error=ILENGTH_ERROR, min_depth=ILENGTH_MIN_DEPTH):\n        \"\"\"Returns a float, t, such that self.length(0, t) is approximately s.\n        See the inv_arclength() docstring for more details.\"\"\"\n        return inv_arclength(self, s, s_tol=s_tol, maxits=maxits, error=error,\n                             min_depth=min_depth)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the nth derivative of the segment at t", "response": "def derivative(self, t=None, n=1):\n        \"\"\"returns the nth derivative of the segment at t.\"\"\"\n        assert self.end != self.start\n        if n == 1:\n            return self.end - self.start\n        elif n > 1:\n            return 0\n        else:\n            raise ValueError(\"n should be a positive integer.\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unit_tangent(self, t=None):\n        assert self.end != self.start\n        dseg = self.end - self.start\n        return dseg/abs(dseg)", "response": "returns the unit tangent of the segment at t"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef intersect(self, other_seg, tol=None):\n        if isinstance(other_seg, Line):\n            assert other_seg.end != other_seg.start and self.end != self.start\n            assert self != other_seg\n            # Solve the system [p1-p0, q1-q0]*[t1, t2]^T = q0 - p0\n            # where self == Line(p0, p1) and other_seg == Line(q0, q1)\n            a = (self.start.real, self.end.real)\n            b = (self.start.imag, self.end.imag)\n            c = (other_seg.start.real, other_seg.end.real)\n            d = (other_seg.start.imag, other_seg.end.imag)\n            denom = ((a[1] - a[0])*(d[0] - d[1]) -\n                     (b[1] - b[0])*(c[0] - c[1]))\n            if np.isclose(denom, 0):\n                return []\n            t1 = (c[0]*(b[0] - d[1]) -\n                  c[1]*(b[0] - d[0]) -\n                  a[0]*(d[0] - d[1]))/denom\n            t2 = -(a[1]*(b[0] - d[0]) -\n                   a[0]*(b[1] - d[0]) -\n                   c[0]*(b[0] - b[1]))/denom\n            if 0 <= t1 <= 1 and 0 <= t2 <= 1:\n                return [(t1, t2)]\n            return []\n        elif isinstance(other_seg, QuadraticBezier):\n            t2t1s = bezier_by_line_intersections(other_seg, self)\n            return [(t1, t2) for t2, t1 in t2t1s]\n        elif isinstance(other_seg, CubicBezier):\n            t2t1s = bezier_by_line_intersections(other_seg, self)\n            return [(t1, t2) for t2, t1 in t2t1s]\n        elif isinstance(other_seg, Arc):\n            t2t1s = other_seg.intersect(self)\n            return [(t1, t2) for t2, t1 in t2t1s]\n        elif isinstance(other_seg, Path):\n            raise TypeError(\n                \"other_seg must be a path segment, not a Path object, use \"\n                \"Path.intersect().\")\n        else:\n            raise TypeError(\"other_seg must be a path segment.\")", "response": "Finds the intersections of two segments."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the bounding box for the segment in the form xmin xmax ymin ymax", "response": "def bbox(self):\n        \"\"\"returns the bounding box for the segment in the form\n        (xmin, xmax, ymin, ymax).\"\"\"\n        xmin = min(self.start.real, self.end.real)\n        xmax = max(self.start.real, self.end.real)\n        ymin = min(self.start.imag, self.end.imag)\n        ymax = max(self.start.imag, self.end.imag)\n        return xmin, xmax, ymin, ymax"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef point_to_t(self, point):\n\n        # Single-precision floats have only 7 significant figures of\n        # resolution, so test that we're within 6 sig figs.\n        if np.isclose(point, self.start, rtol=0, atol=1e-6):\n            return 0.0\n        elif np.isclose(point, self.end, rtol=0, atol=1e-6):\n            return 1.0\n\n        # Finding the point \"by hand\" here is much faster than calling\n        # radialrange(), see the discussion on PR #40:\n        # https://github.com/mathandy/svgpathtools/pull/40#issuecomment-358134261\n\n        p = self.poly()\n        # p(t) = (p_1 * t) + p_0 = point\n        # t = (point - p_0) / p_1\n        t = (point - p[0]) / p[1]\n        if np.isclose(t.imag, 0) and (t.real >= 0.0) and (t.real <= 1.0):\n            return t.real\n        return None", "response": "Given a point returns its t parameter."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cropped(self, t0, t1):\n        return Line(self.point(t0), self.point(t1))", "response": "returns a cropped copy of this segment which starts at t0 and ends at t1."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef split(self, t):\n        pt = self.point(t)\n        return Line(self.start, pt), Line(pt, self.end)", "response": "returns two segments whose union is this segment and whose join at\n        self. point t."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef radialrange(self, origin, return_all_global_extrema=False):\n        return bezier_radialrange(self, origin,\n                return_all_global_extrema=return_all_global_extrema)", "response": "returns the tuples d_min d_max which minimize\n        and maximize respectively the distance d = |self. point ( t ) - origin|."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nscales transform. See `scale` function for further explanation.", "response": "def scaled(self, sx, sy=None, origin=0j):\n        \"\"\"Scale transform.  See `scale` function for further explanation.\"\"\"\n        return scale(self, sx=sx, sy=sy, origin=origin)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_smooth_from(self, previous, warning_on=True):\n        if warning_on:\n            warn(_is_smooth_from_warning)\n        if isinstance(previous, QuadraticBezier):\n            return (self.start == previous.end and\n                    (self.control - self.start) == (\n                        previous.end - previous.control))\n        else:\n            return self.control == self.start", "response": "Check if this segment is a smooth from another segment."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the coordinates of the Bezier curve evaluated at t", "response": "def point(self, t):\n        \"\"\"returns the coordinates of the Bezier curve evaluated at t.\"\"\"\n        return (1 - t)**2*self.start + 2*(1 - t)*t*self.control + t**2*self.end"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the quadratic as a Polynomial object.", "response": "def poly(self, return_coeffs=False):\n        \"\"\"returns the quadratic as a Polynomial object.\"\"\"\n        p = self.bpoints()\n        coeffs = (p[0] - 2*p[1] + p[2], 2*(p[1] - p[0]), p[0])\n        if return_coeffs:\n            return coeffs\n        else:\n            return np.poly1d(coeffs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef derivative(self, t, n=1):\n        p = self.bpoints()\n        if n == 1:\n            return 2*((p[1] - p[0])*(1 - t) + (p[2] - p[1])*t)\n        elif n == 2:\n            return 2*(p[2] - 2*p[1] + p[0])\n        elif n > 2:\n            return 0\n        else:\n            raise ValueError(\"n should be a positive integer.\")", "response": "returns the nth derivative of the segment at t"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reversed(self):\n        new_quad = QuadraticBezier(self.end, self.control, self.start)\n        if self._length_info['length']:\n            new_quad._length_info = self._length_info\n            new_quad._length_info['bpoints'] = (\n                self.end, self.control, self.start)\n        return new_quad", "response": "returns a copy of the QuadraticBezier object with its orientation\n        reversed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn two segments whose union is this segment and whose join at self. point t.", "response": "def split(self, t):\n        \"\"\"returns two segments, whose union is this segment and which join at\n        self.point(t).\"\"\"\n        bpoints1, bpoints2 = split_bezier(self.bpoints(), t)\n        return QuadraticBezier(*bpoints1), QuadraticBezier(*bpoints2)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if this segment is a smooth from another segment.", "response": "def is_smooth_from(self, previous, warning_on=True):\n        \"\"\"[Warning: The name of this method is somewhat misleading (yet kept\n        for compatibility with scripts created using svg.path 2.0).  This\n        method is meant only for d string creation and should not be used to\n        check for kinks.  To check a segment for differentiability, use the\n        joins_smoothly_with() method instead.]\"\"\"\n        if warning_on:\n            warn(_is_smooth_from_warning)\n        if isinstance(previous, CubicBezier):\n            return (self.start == previous.end and\n                    (self.control1 - self.start) == (\n                        previous.end - previous.control2))\n        else:\n            return self.control1 == self.start"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nevaluate the cubic Bezier curve at t using Horner s rule.", "response": "def point(self, t):\n        \"\"\"Evaluate the cubic Bezier curve at t using Horner's rule.\"\"\"\n        # algebraically equivalent to\n        # P0*(1-t)**3 + 3*P1*t*(1-t)**2 + 3*P2*(1-t)*t**2 + P3*t**3\n        # for (P0, P1, P2, P3) = self.bpoints()\n        return self.start + t*(\n            3*(self.control1 - self.start) + t*(\n                3*(self.start + self.control2) - 6*self.control1 + t*(\n                    -self.start + 3*(self.control1 - self.control2) + self.end\n                )))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the length of the path up to a certain position", "response": "def length(self, t0=0, t1=1, error=LENGTH_ERROR, min_depth=LENGTH_MIN_DEPTH):\n        \"\"\"Calculate the length of the path up to a certain position\"\"\"\n        if t0 == 0 and t1 == 1:\n            if self._length_info['bpoints'] == self.bpoints() \\\n                    and self._length_info['error'] >= error \\\n                    and self._length_info['min_depth'] >= min_depth:\n                return self._length_info['length']\n\n        # using scipy.integrate.quad is quick\n        if _quad_available:\n            s = quad(lambda tau: abs(self.derivative(tau)), t0, t1,\n                            epsabs=error, limit=1000)[0]\n        else:\n            s = segment_length(self, t0, t1, self.point(t0), self.point(t1),\n                               error, min_depth, 0)\n\n        if t0 == 0 and t1 == 1:\n            self._length_info['length'] = s\n            self._length_info['bpoints'] = self.bpoints()\n            self._length_info['error'] = error\n            self._length_info['min_depth'] = min_depth\n            return self._length_info['length']\n        else:\n            return s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the Bezier control points of the segment", "response": "def bpoints(self):\n        \"\"\"returns the Bezier control points of the segment.\"\"\"\n        return self.start, self.control1, self.control2, self.end"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the nth derivative of the segment at t", "response": "def derivative(self, t, n=1):\n        \"\"\"returns the nth derivative of the segment at t.\n        Note: Bezier curves can have points where their derivative vanishes.\n        If you are interested in the tangent direction, use the unit_tangent()\n        method instead.\"\"\"\n        p = self.bpoints()\n        if n == 1:\n            return 3*(p[1] - p[0])*(1 - t)**2 + 6*(p[2] - p[1])*(1 - t)*t + 3*(\n                p[3] - p[2])*t**2\n        elif n == 2:\n            return 6*(\n                (1 - t)*(p[2] - 2*p[1] + p[0]) + t*(p[3] - 2*p[2] + p[1]))\n        elif n == 3:\n            return 6*(p[3] - 3*(p[2] - p[1]) - p[0])\n        elif n > 3:\n            return 0\n        else:\n            raise ValueError(\"n should be a positive integer.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a copy of the CubicBezier object with its orientation reversed.", "response": "def reversed(self):\n        \"\"\"returns a copy of the CubicBezier object with its orientation\n        reversed.\"\"\"\n        new_cub = CubicBezier(self.end, self.control2, self.control1,\n                              self.start)\n        if self._length_info['length']:\n            new_cub._length_info = self._length_info\n            new_cub._length_info['bpoints'] = (\n                self.end, self.control2, self.control1, self.start)\n        return new_cub"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind the intersections of two segments.", "response": "def intersect(self, other_seg, tol=1e-12):\n        \"\"\"Finds the intersections of two segments.\n        returns a list of tuples (t1, t2) such that\n        self.point(t1) == other_seg.point(t2).\n        Note: This will fail if the two segments coincide for more than a\n        finite collection of points.\"\"\"\n        if isinstance(other_seg, Line):\n            return bezier_by_line_intersections(self, other_seg)\n        elif (isinstance(other_seg, QuadraticBezier) or\n              isinstance(other_seg, CubicBezier)):\n            assert self != other_seg\n            longer_length = max(self.length(), other_seg.length())\n            return bezier_intersections(self, other_seg,\n                                        longer_length=longer_length,\n                                        tol=tol, tol_deC=tol)\n        elif isinstance(other_seg, Arc):\n            t2t1s = other_seg.intersect(self)\n            return [(t1, t2) for t2, t1 in t2t1s]\n        elif isinstance(other_seg, Path):\n            raise TypeError(\n                \"other_seg must be a path segment, not a Path object, use \"\n                \"Path.intersect().\")\n        else:\n            raise TypeError(\"other_seg must be a path segment.\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef split(self, t):\n        bpoints1, bpoints2 = split_bezier(self.bpoints(), t)\n        return CubicBezier(*bpoints1), CubicBezier(*bpoints2)", "response": "returns two segments whose union is this segment and whose join at\n        self. point t."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef point_to_t(self, point):\n\n        def in_range(min, max, val):\n            return (min <= val) and (max >= val)\n\n        # Single-precision floats have only 7 significant figures of\n        # resolution, so test that we're within 6 sig figs.\n        if np.isclose(point, self.start, rtol=0.0, atol=1e-6):\n            return 0.0\n        elif np.isclose(point, self.end, rtol=0.0, atol=1e-6):\n            return 1.0\n\n        if self.rotation != 0.0:\n            raise ValueError(\"Arc.point_to_t() only works on non-rotated Arcs.\")\n\n        v = point - self.center\n        distance_from_center = sqrt((v.real * v.real) + (v.imag * v.imag))\n        min_radius = min(self.radius.real, self.radius.imag)\n        max_radius = max(self.radius.real, self.radius.imag)\n        if (distance_from_center < min_radius) and not np.isclose(distance_from_center, min_radius):\n            return None\n        if (distance_from_center > max_radius) and not np.isclose(distance_from_center, max_radius):\n            return None\n\n        # x = center_x + radius_x cos(radians(theta + t delta))\n        # y = center_y + radius_y sin(radians(theta + t delta))\n        #\n        # For x:\n        # cos(radians(theta + t delta)) = (x - center_x) / radius_x\n        # radians(theta + t delta) = acos((x - center_x) / radius_x)\n        # theta + t delta = degrees(acos((x - center_x) / radius_x))\n        # t_x = (degrees(acos((x - center_x) / radius_x)) - theta) / delta\n        #\n        # Similarly for y:\n        # t_y = (degrees(asin((y - center_y) / radius_y)) - theta) / delta\n\n        x = point.real\n        y = point.imag\n\n        #\n        # +Y points down!\n        #\n        # sweep mean clocwise\n        # sweep && (delta > 0)\n        # !sweep && (delta < 0)\n        #\n        # -180 <= theta_1 <= 180\n        #\n        # large_arc && (-360 <= delta <= 360)\n        # !large_arc && (-180 < delta < 180)\n        #\n\n        end_angle = self.theta + self.delta\n        min_angle = min(self.theta, end_angle)\n        max_angle = max(self.theta, end_angle)\n\n        acos_arg = (x - self.center.real) / self.radius.real\n        if acos_arg > 1.0:\n            acos_arg = 1.0\n        elif acos_arg < -1.0:\n            acos_arg = -1.0\n\n        x_angle_0 = degrees(acos(acos_arg))\n        while x_angle_0 < min_angle:\n            x_angle_0 += 360.0\n        while x_angle_0 > max_angle:\n            x_angle_0 -= 360.0\n\n        x_angle_1 = -1.0 * x_angle_0\n        while x_angle_1 < min_angle:\n            x_angle_1 += 360.0\n        while x_angle_1 > max_angle:\n            x_angle_1 -= 360.0\n\n        t_x_0 = (x_angle_0 - self.theta) / self.delta\n        t_x_1 = (x_angle_1 - self.theta) / self.delta\n\n        asin_arg = (y - self.center.imag) / self.radius.imag\n        if asin_arg > 1.0:\n            asin_arg = 1.0\n        elif asin_arg < -1.0:\n            asin_arg = -1.0\n\n        y_angle_0 = degrees(asin(asin_arg))\n        while y_angle_0 < min_angle:\n            y_angle_0 += 360.0\n        while y_angle_0 > max_angle:\n            y_angle_0 -= 360.0\n\n        y_angle_1 = 180 - y_angle_0\n        while y_angle_1 < min_angle:\n            y_angle_1 += 360.0\n        while y_angle_1 > max_angle:\n            y_angle_1 -= 360.0\n\n        t_y_0 = (y_angle_0 - self.theta) / self.delta\n        t_y_1 = (y_angle_1 - self.theta) / self.delta\n\n        t = None\n        if np.isclose(t_x_0, t_y_0):\n            t = (t_x_0 + t_y_0) / 2.0\n        elif np.isclose(t_x_0, t_y_1):\n            t= (t_x_0 + t_y_1) / 2.0\n        elif np.isclose(t_x_1, t_y_0):\n            t = (t_x_1 + t_y_0) / 2.0\n        elif np.isclose(t_x_1, t_y_1):\n            t = (t_x_1 + t_y_1) / 2.0\n        else:\n            # Comparing None and float yields a result in python2,\n            # but throws TypeError in python3.  This fix (suggested by\n            # @CatherineH) explicitly handles and avoids the case where\n            # the None-vs-float comparison would have happened below.\n            return None\n\n        if (t >= 0.0) and (t <= 1.0):\n            return t\n\n        return None", "response": "This function converts a point to its t parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef length(self, t0=0, t1=1, error=LENGTH_ERROR, min_depth=LENGTH_MIN_DEPTH):\n        assert 0 <= t0 <= 1 and 0 <= t1 <= 1\n        if _quad_available:\n            return quad(lambda tau: abs(self.derivative(tau)), t0, t1,\n                        epsabs=error, limit=1000)[0]\n        else:\n            return segment_length(self, t0, t1, self.point(t0), self.point(t1),\n                                  error, min_depth, 0)", "response": "The length of an elliptical large_arc segment."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef derivative(self, t, n=1):\n        angle = radians(self.theta + t*self.delta)\n        phi = radians(self.rotation)\n        rx = self.radius.real\n        ry = self.radius.imag\n        k = (self.delta*2*pi/360)**n  # ((d/dt)angle)**n\n\n        if n % 4 == 0 and n > 0:\n            return rx*cos(phi)*cos(angle) - ry*sin(phi)*sin(angle) + 1j*(\n                rx*sin(phi)*cos(angle) + ry*cos(phi)*sin(angle))\n        elif n % 4 == 1:\n            return k*(-rx*cos(phi)*sin(angle) - ry*sin(phi)*cos(angle) + 1j*(\n                -rx*sin(phi)*sin(angle) + ry*cos(phi)*cos(angle)))\n        elif n % 4 == 2:\n            return k*(-rx*cos(phi)*cos(angle) + ry*sin(phi)*sin(angle) + 1j*(\n                -rx*sin(phi)*cos(angle) - ry*cos(phi)*sin(angle)))\n        elif n % 4 == 3:\n            return k*(rx*cos(phi)*sin(angle) + ry*sin(phi)*cos(angle) + 1j*(\n                rx*sin(phi)*sin(angle) - ry*cos(phi)*cos(angle)))\n        else:\n            raise ValueError(\"n should be a positive integer.\")", "response": "returns the nth derivative of the segment at t"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unit_tangent(self, t):\n        dseg = self.derivative(t)\n        return dseg/abs(dseg)", "response": "returns the unit tangent vector of the segment at t"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reversed(self):\n        return Arc(self.end, self.radius, self.rotation, self.large_arc,\n                   not self.sweep, self.start)", "response": "returns a copy of the Arc object with its orientation reversed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef phase2t(self, psi):\n        def _deg(rads, domain_lower_limit):\n            # Convert rads to degrees in [0, 360) domain\n            degs = degrees(rads % (2*pi))\n\n            # Convert to [domain_lower_limit, domain_lower_limit + 360) domain\n            k = domain_lower_limit // 360\n            degs += k * 360\n            if degs < domain_lower_limit:\n                degs += 360\n            return degs\n\n        if self.delta > 0:\n            degs = _deg(psi, domain_lower_limit=self.theta)\n        else:\n            degs = _deg(psi, domain_lower_limit=self.theta)\n        return (degs - self.theta)/self.delta", "response": "Given a phase - pi < psi < pi returns the t value such that the phase - pi < psi < pi"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind the intersections of two segments.", "response": "def intersect(self, other_seg, tol=1e-12):\n        \"\"\"NOT FULLY IMPLEMENTED.  Finds the intersections of two segments.\n        returns a list of tuples (t1, t2) such that\n        self.point(t1) == other_seg.point(t2).\n        Note: This will fail if the two segments coincide for more than a\n        finite collection of points.\n\n        Note: Arc related intersections are only partially supported, i.e. are\n        only half-heartedly implemented and not well tested.  Please feel free\n        to let me know if you're interested in such a feature -- or even better\n        please submit an implementation if you want to code one.\"\"\"\n\n        # This special case can be easily solved algebraically.\n        if (self.rotation == 0) and isinstance(other_seg, Line):\n            a = self.radius.real\n            b = self.radius.imag\n\n            # Ignore the ellipse's center point (to pretend that it's\n            # centered at the origin), and translate the Line to match.\n            l = Line(start=(other_seg.start-self.center), end=(other_seg.end-self.center))\n\n            # This gives us the translated Line as a parametric equation.\n            # s = p1 t + p0\n            p = l.poly()\n\n            if p[1].real == 0.0:\n                # The `x` value doesn't depend on `t`, the line is vertical.\n                c = p[0].real\n                x_values = [c]\n\n                # Substitute the line `x = c` into the equation for the\n                # (origin-centered) ellipse.\n                #\n                # x^2/a^2 + y^2/b^2 = 1\n                # c^2/a^2 + y^2/b^2 = 1\n                # y^2/b^2 = 1 - c^2/a^2\n                # y^2 = b^2(1 - c^2/a^2)\n                # y = +-b sqrt(1 - c^2/a^2)\n\n                discriminant = 1 - (c * c)/(a * a)\n                if discriminant < 0:\n                    return []\n                elif discriminant == 0:\n                    y_values = [0]\n                else:\n                    val = b * sqrt(discriminant)\n                    y_values = [val, -val]\n\n            else:\n                # This is a non-vertical line.\n                #\n                # Convert the Line's parametric equation to the \"y = mx + c\" format.\n                # x = p1.real t + p0.real\n                # y = p1.imag t + p0.imag\n                #\n                # t = (x - p0.real) / p1.real\n                # t = (y - p0.imag) / p1.imag\n                #\n                # (y - p0.imag) / p1.imag = (x - p0.real) / p1.real\n                # (y - p0.imag) = ((x - p0.real) * p1.imag) / p1.real\n                # y = ((x - p0.real) * p1.imag) / p1.real + p0.imag\n                # y = (x p1.imag - p0.real * p1.imag) / p1.real + p0.imag\n                # y = x p1.imag/p1.real - p0.real p1.imag / p1.real + p0.imag\n                # m = p1.imag/p1.real\n                # c = -m p0.real + p0.imag\n                m = p[1].imag / p[1].real\n                c = (-m * p[0].real) + p[0].imag\n\n                # Substitute the line's y(x) equation into the equation for\n                # the ellipse.  We can pretend the ellipse is centered at the\n                # origin, since we shifted the Line by the ellipse's center.\n                #\n                # x^2/a^2 + y^2/b^2 = 1\n                # x^2/a^2 + (mx+c)^2/b^2 = 1\n                # (b^2 x^2 + a^2 (mx+c)^2)/(a^2 b^2) = 1\n                # b^2 x^2 + a^2 (mx+c)^2 = a^2 b^2\n                # b^2 x^2 + a^2(m^2 x^2 + 2mcx + c^2) = a^2 b^2\n                # b^2 x^2 + a^2 m^2 x^2 + 2a^2 mcx + a^2 c^2 - a^2 b^2 = 0\n                # (a^2 m^2 + b^2)x^2 + 2a^2 mcx + a^2(c^2 - b^2) = 0\n                #\n                # The quadratic forumla tells us:  x = (-B +- sqrt(B^2 - 4AC)) / 2A\n                # Where:\n                #     A = a^2 m^2 + b^2\n                #     B = 2 a^2 mc\n                #     C = a^2(c^2 - b^2)\n                #\n                # The determinant is: B^2 - 4AC\n                #\n                # The solution simplifies to:\n                # x = (-a^2 mc +- a b sqrt(a^2 m^2 + b^2 - c^2)) / (a^2 m^2 + b^2)\n                #\n                # Solving the line for x(y) and substituting *that* into\n                # the equation for the ellipse gives this solution for y:\n                # y = (b^2 c +- abm sqrt(a^2 m^2 + b^2 - c^2)) / (a^2 m^2 + b^2)\n\n                denominator = (a * a * m * m) + (b * b)\n\n                discriminant = denominator - (c * c)\n                if discriminant < 0:\n                    return []\n\n                x_sqrt = a * b * sqrt(discriminant)\n                x1 = (-(a * a * m * c) + x_sqrt) / denominator \n                x2 = (-(a * a * m * c) - x_sqrt) / denominator \n                x_values = [x1]\n                if x1 != x2:\n                    x_values.append(x2)\n\n                y_sqrt = x_sqrt * m\n                y1 = ((b * b * c) + y_sqrt) / denominator\n                y2 = ((b * b * c) - y_sqrt) / denominator\n                y_values = [y1]\n                if y1 != y2:\n                    y_values.append(y2)\n\n            intersections = []\n            for x in x_values:\n                for y in y_values:\n                    p = complex(x, y) + self.center\n                    my_t = self.point_to_t(p)\n                    if my_t == None:\n                        continue\n                    other_t = other_seg.point_to_t(p)\n                    if other_t == None:\n                        continue\n                    intersections.append([my_t, other_t])\n            return intersections\n\n        elif is_bezier_segment(other_seg):\n            u1poly = self.u1transform(other_seg.poly())\n            u1poly_mag2 = real(u1poly)**2 + imag(u1poly)**2\n            t2s = polyroots01(u1poly_mag2 - 1)\n            t1s = [self.phase2t(phase(u1poly(t2))) for t2 in t2s]\n            return list(zip(t1s, t2s))\n        elif isinstance(other_seg, Arc):\n            assert other_seg != self\n            # This could be made explicit to increase efficiency\n            longer_length = max(self.length(), other_seg.length())\n            inters = bezier_intersections(self, other_seg,\n                                          longer_length=longer_length,\n                                          tol=tol, tol_deC=tol)\n\n            # ad hoc fix for redundant solutions\n            if len(inters) > 2:\n                def keyfcn(tpair):\n                    t1, t2 = tpair\n                    return abs(self.point(t1) - other_seg.point(t2))\n                inters.sort(key=keyfcn)\n                for idx in range(1, len(inters)-1):\n                    if (abs(inters[idx][0] - inters[idx + 1][0])\n                            <  abs(inters[idx][0] - inters[0][0])):\n                        return [inters[0], inters[idx]]\n                else:\n                    return [inters[0], inters[-1]]\n            return inters\n        else:\n            raise TypeError(\"other_seg should be a Arc, Line, \"\n                            \"QuadraticBezier, or CubicBezier object.\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bbox(self):\n        # a(t) = radians(self.theta + self.delta*t)\n        #      = (2*pi/360)*(self.theta + self.delta*t)\n        # x'=0: ~~~~~~~~~\n        # -rx*cos(phi)*sin(a(t)) = ry*sin(phi)*cos(a(t))\n        # -(rx/ry)*cot(phi)*tan(a(t)) = 1\n        # a(t) = arctan(-(ry/rx)tan(phi)) + pi*k === atan_x\n        # y'=0: ~~~~~~~~~~\n        # rx*sin(phi)*sin(a(t)) = ry*cos(phi)*cos(a(t))\n        # (rx/ry)*tan(phi)*tan(a(t)) = 1\n        # a(t) = arctan((ry/rx)*cot(phi))\n        # atanres = arctan((ry/rx)*cot(phi)) === atan_y\n        # ~~~~~~~~\n        # (2*pi/360)*(self.theta + self.delta*t) = atanres + pi*k\n        # Therfore, for both x' and y', we have...\n        # t = ((atan_{x/y} + pi*k)*(360/(2*pi)) - self.theta)/self.delta\n        # for all k s.t. 0 < t < 1\n        from math import atan, tan\n\n        if cos(self.phi) == 0:\n            atan_x = pi/2\n            atan_y = 0\n        elif sin(self.phi) == 0:\n            atan_x = 0\n            atan_y = pi/2\n        else:\n            rx, ry = self.radius.real, self.radius.imag\n            atan_x = atan(-(ry/rx)*tan(self.phi))\n            atan_y = atan((ry/rx)/tan(self.phi))\n\n        def angle_inv(ang, k):  # inverse of angle from Arc.derivative()\n            return ((ang + pi*k)*(360/(2*pi)) - self.theta)/self.delta\n\n        xtrema = [self.start.real, self.end.real]\n        ytrema = [self.start.imag, self.end.imag]\n\n        for k in range(-4, 5):\n            tx = angle_inv(atan_x, k)\n            ty = angle_inv(atan_y, k)\n            if 0 <= tx <= 1:\n                xtrema.append(self.point(tx).real)\n            if 0 <= ty <= 1:\n                ytrema.append(self.point(ty).imag)\n        xmin = max(xtrema)\n        return min(xtrema), max(xtrema), min(ytrema), max(ytrema)", "response": "returns the bounding box for the current segment in the form xmin xmax ymin ymax"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a cropped copy of this segment which starts at self. point t0 and ends at self. point t1.", "response": "def cropped(self, t0, t1):\n        \"\"\"returns a cropped copy of this segment which starts at\n        self.point(t0) and ends at self.point(t1).\"\"\"\n        if abs(self.delta*(t1 - t0)) <= 180:\n            new_large_arc = 0\n        else:\n            new_large_arc = 1\n        return Arc(self.point(t0), radius=self.radius, rotation=self.rotation,\n                   large_arc=new_large_arc, sweep=self.sweep,\n                   end=self.point(t1), autoscale_radius=self.autoscale_radius)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reversed(self):\n        newpath = [seg.reversed() for seg in self]\n        newpath.reverse()\n        return Path(*newpath)", "response": "returns a copy of the Path object with its orientation reversed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if a path is continuous with respect to its parameterization.", "response": "def iscontinuous(self):\n        \"\"\"Checks if a path is continuous with respect to its\n        parameterization.\"\"\"\n        return all(self[i].end == self[i+1].start for i in range(len(self) - 1))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef continuous_subpaths(self):\n        subpaths = []\n        subpath_start = 0\n        for i in range(len(self) - 1):\n            if self[i].end != self[(i+1) % len(self)].start:\n                subpaths.append(Path(*self[subpath_start: i+1]))\n                subpath_start = i+1\n        subpaths.append(Path(*self[subpath_start: len(self)]))\n        return subpaths", "response": "Breaks self into its continuous components returning a list of\n        continuous subpaths."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef closed(self, warning_on=CLOSED_WARNING_ON):\n        mes = (\"This attribute is deprecated, consider using isclosed() \"\n               \"method instead.\\n\\nThis attribute is kept for compatibility \"\n               \"with scripts created using svg.path (v2.0). You can prevent \"\n               \"this warning in the future by setting \"\n               \"CLOSED_WARNING_ON=False.\")\n        if warning_on:\n            warn(mes)\n        return self._closed and self._is_closable()", "response": "Deprecated. Use the isclosed method instead."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef d(self, useSandT=False, use_closed_attrib=False):\n\n        if use_closed_attrib:\n            self_closed = self.closed(warning_on=False)\n            if self_closed:\n                segments = self[:-1]\n            else:\n                segments = self[:]\n        else:\n            self_closed = False\n            segments = self[:]\n\n        current_pos = None\n        parts = []\n        previous_segment = None\n        end = self[-1].end\n\n        for segment in segments:\n            seg_start = segment.start\n            # If the start of this segment does not coincide with the end of\n            # the last segment or if this segment is actually the close point\n            # of a closed path, then we should start a new subpath here.\n            if current_pos != seg_start or \\\n                    (self_closed and seg_start == end and use_closed_attrib):\n                parts.append('M {},{}'.format(seg_start.real, seg_start.imag))\n\n            if isinstance(segment, Line):\n                args = segment.end.real, segment.end.imag\n                parts.append('L {},{}'.format(*args))\n            elif isinstance(segment, CubicBezier):\n                if useSandT and segment.is_smooth_from(previous_segment,\n                                                       warning_on=False):\n                    args = (segment.control2.real, segment.control2.imag,\n                            segment.end.real, segment.end.imag)\n                    parts.append('S {},{} {},{}'.format(*args))\n                else:\n                    args = (segment.control1.real, segment.control1.imag,\n                            segment.control2.real, segment.control2.imag,\n                            segment.end.real, segment.end.imag)\n                    parts.append('C {},{} {},{} {},{}'.format(*args))\n            elif isinstance(segment, QuadraticBezier):\n                if useSandT and segment.is_smooth_from(previous_segment,\n                                                       warning_on=False):\n                    args = segment.end.real, segment.end.imag\n                    parts.append('T {},{}'.format(*args))\n                else:\n                    args = (segment.control.real, segment.control.imag,\n                            segment.end.real, segment.end.imag)\n                    parts.append('Q {},{} {},{}'.format(*args))\n\n            elif isinstance(segment, Arc):\n                args = (segment.radius.real, segment.radius.imag,\n                        segment.rotation,int(segment.large_arc),\n                        int(segment.sweep),segment.end.real, segment.end.imag)\n                parts.append('A {},{} {} {:d},{:d} {},{}'.format(*args))\n            current_pos = segment.end\n            previous_segment = segment\n\n        if self_closed:\n            parts.append('Z')\n\n        return ' '.join(parts)", "response": "Returns a string that represents the path object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the segment index seg_idx segment parameter t corresponding to the path parameter T.", "response": "def T2t(self, T):\n        \"\"\"returns the segment index, `seg_idx`, and segment parameter, `t`,\n        corresponding to the path parameter `T`.  In other words, this is the\n        inverse of the `Path.t2T()` method.\"\"\"\n        if T == 1:\n            return len(self)-1, 1\n        if T == 0:\n            return 0, 0\n        self._calc_lengths()\n        # Find which segment self.point(T) falls on:\n        T0 = 0  # the T-value the current segment starts on\n        for seg_idx, seg_length in enumerate(self._lengths):\n            T1 = T0 + seg_length  # the T-value the current segment ends on\n            if T1 >= T:\n                # This is the segment!\n                t = (T - T0)/seg_length\n                return seg_idx, t\n            T0 = T1\n\n        assert 0 <= T <= 1\n        raise BugException"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef t2T(self, seg, t):\n        self._calc_lengths()\n        # Accept an index or a segment for seg\n        if isinstance(seg, int):\n            seg_idx = seg\n        else:\n            try:\n                seg_idx = self.index(seg)\n            except ValueError:\n                assert is_path_segment(seg) or isinstance(seg, int)\n                raise\n\n        segment_start = sum(self._lengths[:seg_idx])\n        segment_end = segment_start + self._lengths[seg_idx]\n        T = (segment_end - segment_start)*t + segment_start\n        return T", "response": "returns the path parameter T which corresponds to the segment\n        parameter t."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the tangent vector of the Path at T n times the tangent vector of the Path at T", "response": "def derivative(self, T, n=1):\n        \"\"\"returns the tangent vector of the Path at T (centered at the origin\n        and expressed as a complex number).\n        Note: Bezier curves can have points where their derivative vanishes.\n        If you are interested in the tangent direction, use unit_tangent()\n        method instead.\"\"\"\n        seg_idx, t = self.T2t(T)\n        seg = self._segments[seg_idx]\n        return seg.derivative(t, n=n)/seg.length()**n"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the unit tangent vector of the Path at T", "response": "def unit_tangent(self, T):\n        \"\"\"returns the unit tangent vector of the Path at T (centered at the\n        origin and expressed as a complex number).  If the tangent vector's\n        magnitude is zero, this method will find the limit of\n        self.derivative(tau)/abs(self.derivative(tau)) as tau approaches T.\"\"\"\n        seg_idx, t = self.T2t(T)\n        return self._segments[seg_idx].unit_tangent(t)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef curvature(self, T):\n        seg_idx, t = self.T2t(T)\n        seg = self[seg_idx]\n        if np.isclose(t, 0) and (seg_idx != 0 or self.end==self.start):\n            previous_seg_in_path = self._segments[\n                (seg_idx - 1) % len(self._segments)]\n            if not seg.joins_smoothly_with(previous_seg_in_path):\n                return float('inf')\n        elif np.isclose(t, 1) and (seg_idx != len(self) - 1 or\n                                   self.end == self.start):\n            next_seg_in_path = self._segments[\n                (seg_idx + 1) % len(self._segments)]\n            if not next_seg_in_path.joins_smoothly_with(seg):\n                return float('inf')\n        dz = self.derivative(T)\n        ddz = self.derivative(T, n=2)\n        dx, dy = dz.real, dz.imag\n        ddx, ddy = ddz.real, ddz.imag\n        return abs(dx*ddy - dy*ddx)/(dx*dx + dy*dy)**1.5", "response": "returns the curvature of this Path object at T and outputs\n        float ( inf if not differentiable at T."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef area(self, chord_length=1e-4):\n\n        def area_without_arcs(path):\n            area_enclosed = 0\n            for seg in path:\n                x = real(seg.poly())\n                dy = imag(seg.poly()).deriv()\n                integrand = x*dy\n                integral = integrand.integ()\n                area_enclosed += integral(1) - integral(0)\n            return area_enclosed\n\n        def seg2lines(seg):\n            \"\"\"Find piecewise-linear approximation of `seg`.\"\"\"\n            num_lines = int(ceil(seg.length() / chord_length))\n            pts = [seg.point(t) for t in np.linspace(0, 1, num_lines+1)]\n            return [Line(pts[i], pts[i+1]) for i in range(num_lines)]\n\n        assert self.isclosed()\n\n        bezier_path_approximation = []\n        for seg in self:\n            if isinstance(seg, Arc):\n                bezier_path_approximation += seg2lines(seg)\n            else:\n                bezier_path_approximation.append(seg)\n        return area_without_arcs(Path(*bezier_path_approximation))", "response": "Find the area of the Path with lines enclosed by the Path with lines approximately chord_length long and returns the area enclosed by the approximated Path."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef intersect(self, other_curve, justonemode=False, tol=1e-12):\n        path1 = self\n        if isinstance(other_curve, Path):\n            path2 = other_curve\n        else:\n            path2 = Path(other_curve)\n        assert path1 != path2\n        intersection_list = []\n        for seg1 in path1:\n            for seg2 in path2:\n                if justonemode and intersection_list:\n                    return intersection_list[0]\n                for t1, t2 in seg1.intersect(seg2, tol=tol):\n                    T1 = path1.t2T(seg1, t1)\n                    T2 = path2.t2T(seg2, t2)\n                    intersection_list.append(((T1, seg1, t1), (T2, seg2, t2)))\n        if justonemode and intersection_list:\n            return intersection_list[0]\n\n        # Note: If the intersection takes place at a joint (point one seg ends\n        # and next begins in path) then intersection_list may contain a\n        # redundant intersection.  This code block checks for and removes said\n        # redundancies.\n        if intersection_list:\n            pts = [seg1.point(_t1)\n                   for _T1, _seg1, _t1 in list(zip(*intersection_list))[0]]\n            indices2remove = []\n            for ind1 in range(len(pts)):\n                for ind2 in range(ind1 + 1, len(pts)):\n                    if abs(pts[ind1] - pts[ind2]) < tol:\n                        # then there's a redundancy. Remove it.\n                        indices2remove.append(ind2)\n            intersection_list = [inter for ind, inter in\n                                 enumerate(intersection_list) if\n                                 ind not in indices2remove]\n        return intersection_list", "response": "returns a list of pairs of pairs ( T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T1 T2 T2"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bbox(self):\n        bbs = [seg.bbox() for seg in self._segments]\n        xmins, xmaxs, ymins, ymaxs = list(zip(*bbs))\n        xmin = min(xmins)\n        xmax = max(xmaxs)\n        ymin = min(ymins)\n        ymax = max(ymaxs)\n        return xmin, xmax, ymin, ymax", "response": "returns a bounding box for the input Path object in the form\n        xmin xmax ymin ymax"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a cropped copy of the path.", "response": "def cropped(self, T0, T1):\n        \"\"\"returns a cropped copy of the path.\"\"\"\n        assert 0 <= T0 <= 1 and 0 <= T1<= 1\n        assert T0 != T1\n        assert not (T0 == 1 and T1 == 0)\n\n        if T0 == 1 and 0 < T1 < 1 and self.isclosed():\n            return self.cropped(0, T1)\n\n        if T1 == 1:\n            seg1 = self[-1]\n            t_seg1 = 1\n            i1 = len(self) - 1\n        else:\n            seg1_idx, t_seg1 = self.T2t(T1)\n            seg1 = self[seg1_idx]\n            if np.isclose(t_seg1, 0):\n                i1 = (self.index(seg1) - 1) % len(self)\n                seg1 = self[i1]\n                t_seg1 = 1\n            else:\n                i1 = self.index(seg1)\n        if T0 == 0:\n            seg0 = self[0]\n            t_seg0 = 0\n            i0 = 0\n        else:\n            seg0_idx, t_seg0 = self.T2t(T0)\n            seg0 = self[seg0_idx]\n            if np.isclose(t_seg0, 1):\n                i0 = (self.index(seg0) + 1) % len(self)\n                seg0 = self[i0]\n                t_seg0 = 0\n            else:\n                i0 = self.index(seg0)\n\n        if T0 < T1 and i0 == i1:\n            new_path = Path(seg0.cropped(t_seg0, t_seg1))\n        else:\n            new_path = Path(seg0.cropped(t_seg0, 1))\n\n            # T1<T0 must cross discontinuity case\n            if T1 < T0:\n                if not self.isclosed():\n                    raise ValueError(\"This path is not closed, thus T0 must \"\n                                     \"be less than T1.\")\n                else:\n                    for i in range(i0 + 1, len(self)):\n                        new_path.append(self[i])\n                    for i in range(0, i1):\n                        new_path.append(self[i])\n\n            # T0<T1 straight-forward case\n            else:\n                for i in range(i0 + 1, i1):\n                    new_path.append(self[i])\n\n            if t_seg1 != 0:\n                new_path.append(seg1.cropped(0, t_seg1))\n        return new_path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef radialrange(self, origin, return_all_global_extrema=False):\n        if return_all_global_extrema:\n            raise NotImplementedError\n        else:\n            global_min = (np.inf, None, None)\n            global_max = (0, None, None)\n            for seg_idx, seg in enumerate(self):\n                seg_global_min, seg_global_max = seg.radialrange(origin)\n                if seg_global_min[0] < global_min[0]:\n                    global_min = seg_global_min + (seg_idx,)\n                if seg_global_max[0] > global_max[0]:\n                    global_max = seg_global_max + (seg_idx,)\n            return global_min, global_max", "response": "returns the tuples d_min t_min idx_min d_max t_max idx_max which minimize and maximize respectively the distance\n        d = |self[idx].point ( t ) - origin| the distance\n        d = |self[idx].point ( t ) - origin| the distance\n        d = |self[idx].point ( t ) - origin| the distance\n            d = |self[idx].point ( t ) - origin|."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of the Bernstein basis polynomials evaluated at t for i = 0... n", "response": "def bernstein(n, t):\n    \"\"\"returns a list of the Bernstein basis polynomials b_{i, n} evaluated at\n    t, for i =0...n\"\"\"\n    t1 = 1-t\n    return [n_choose_k(n, k) * t1**(n-k) * t**k for k in range(n+1)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bezier_point(p, t):\n\n    # begin arc support block ########################\n    try:\n        p.large_arc\n        return p.point(t)\n    except:\n        pass\n    # end arc support block ##########################\n\n    deg = len(p) - 1\n    if deg == 3:\n        return p[0] + t*(\n            3*(p[1] - p[0]) + t*(\n                3*(p[0] + p[2]) - 6*p[1] + t*(\n                    -p[0] + 3*(p[1] - p[2]) + p[3])))\n    elif deg == 2:\n        return p[0] + t*(\n            2*(p[1] - p[0]) + t*(\n                p[0] - 2*p[1] + p[2]))\n    elif deg == 1:\n        return p[0] + t*(p[1] - p[0])\n    elif deg == 0:\n        return p[0]\n    else:\n        bern = bernstein(deg, t)\n        return sum(bern[k]*p[k] for k in range(deg+1))", "response": "Evaluates the Bezier curve given by it s control points p at t."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bezier2polynomial(p, numpy_ordering=True, return_poly1d=False):\n    if len(p) == 4:\n        coeffs = (-p[0] + 3*(p[1] - p[2]) + p[3],\n                  3*(p[0] - 2*p[1] + p[2]),\n                  3*(p[1]-p[0]),\n                  p[0])\n    elif len(p) == 3:\n        coeffs = (p[0] - 2*p[1] + p[2],\n                  2*(p[1] - p[0]),\n                  p[0])\n    elif len(p) == 2:\n        coeffs = (p[1]-p[0],\n                  p[0])\n    elif len(p) == 1:\n        coeffs = p\n    else:\n        # https://en.wikipedia.org/wiki/Bezier_curve#Polynomial_form\n        n = len(p) - 1\n        coeffs = [fac(n)//fac(n-j) * sum(\n            (-1)**(i+j) * p[i] / (fac(i) * fac(j-i)) for i in range(j+1))\n            for j in range(n+1)]\n        coeffs.reverse()\n    if not numpy_ordering:\n        coeffs = coeffs[::-1]  # can't use .reverse() as might be tuple\n    if return_poly1d:\n        return poly1d(coeffs)\n    return coeffs", "response": "Converts a tuple of Bezier control points to a tuple of coefficients\n    of the expanded polynomial."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef polynomial2bezier(poly):\n    if isinstance(poly, poly1d):\n        c = poly.coeffs\n    else:\n        c = poly\n    order = len(c)-1\n    if order == 3:\n        bpoints = (c[3], c[2]/3 + c[3], (c[1] + 2*c[2])/3 + c[3],\n                   c[0] + c[1] + c[2] + c[3])\n    elif order == 2:\n        bpoints = (c[2], c[1]/2 + c[2], c[0] + c[1] + c[2])\n    elif order == 1:\n        bpoints = (c[1], c[0] + c[1])\n    else:\n        raise AssertionError(\"This function is only implemented for linear, \"\n                             \"quadratic, and cubic polynomials.\")\n    return bpoints", "response": "Converts a cubic or lower order Polynomial object or a sequence of\n    coefficients to a CubicBezier QuadraticBezier or Line object as\n    appropriate."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nuse deCasteljau s recursion to split the Bezier curve at t into two ArcGIS Bezier curves of the same order.", "response": "def split_bezier(bpoints, t):\n    \"\"\"Uses deCasteljau's recursion to split the Bezier curve at t into two\n    Bezier curves of the same order.\"\"\"\n    def split_bezier_recursion(bpoints_left_, bpoints_right_, bpoints_, t_):\n        if len(bpoints_) == 1:\n            bpoints_left_.append(bpoints_[0])\n            bpoints_right_.append(bpoints_[0])\n        else:\n            new_points = [None]*(len(bpoints_) - 1)\n            bpoints_left_.append(bpoints_[0])\n            bpoints_right_.append(bpoints_[-1])\n            for i in range(len(bpoints_) - 1):\n                new_points[i] = (1 - t_)*bpoints_[i] + t_*bpoints_[i + 1]\n            bpoints_left_, bpoints_right_ = split_bezier_recursion(\n                bpoints_left_, bpoints_right_, new_points, t_)\n        return bpoints_left_, bpoints_right_\n\n    bpoints_left = []\n    bpoints_right = []\n    bpoints_left, bpoints_right = \\\n        split_bezier_recursion(bpoints_left, bpoints_right, bpoints, t)\n    bpoints_right.reverse()\n    return bpoints_left, bpoints_right"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the minimum and maximum for any real cubic bezier", "response": "def bezier_real_minmax(p):\n    \"\"\"returns the minimum and maximum for any real cubic bezier\"\"\"\n    local_extremizers = [0, 1]\n    if len(p) == 4:  # cubic case\n        a = [p.real for p in p]\n        denom = a[0] - 3*a[1] + 3*a[2] - a[3]\n        if denom != 0:\n            delta = a[1]**2 - (a[0] + a[1])*a[2] + a[2]**2 + (a[0] - a[1])*a[3]\n            if delta >= 0:  # otherwise no local extrema\n                sqdelta = sqrt(delta)\n                tau = a[0] - 2*a[1] + a[2]\n                r1 = (tau + sqdelta)/denom\n                r2 = (tau - sqdelta)/denom\n                if 0 < r1 < 1:\n                    local_extremizers.append(r1)\n                if 0 < r2 < 1:\n                    local_extremizers.append(r2)\n            local_extrema = [bezier_point(a, t) for t in local_extremizers]\n            return min(local_extrema), max(local_extrema)\n\n    # find reverse standard coefficients of the derivative\n    dcoeffs = bezier2polynomial(a, return_poly1d=True).deriv().coeffs\n\n    # find real roots, r, such that 0 <= r <= 1\n    local_extremizers += polyroots01(dcoeffs)\n    local_extrema = [bezier_point(a, t) for t in local_extremizers]\n    return min(local_extrema), max(local_extrema)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the bounding box for the given bezier object", "response": "def bezier_bounding_box(bez):\n    \"\"\"returns the bounding box for the segment in the form\n    (xmin, xmax, ymin, ymax).\n    Warning: For the non-cubic case this is not particularly efficient.\"\"\"\n\n    # begin arc support block ########################\n    try:\n        bla = bez.large_arc\n        return bez.bbox()  # added to support Arc objects\n    except:\n        pass\n    # end arc support block ##########################\n\n    if len(bez) == 4:\n        xmin, xmax = bezier_real_minmax([p.real for p in bez])\n        ymin, ymax = bezier_real_minmax([p.imag for p in bez])\n        return xmin, xmax, ymin, ymax\n    poly = bezier2polynomial(bez, return_poly1d=True)\n    x = real(poly)\n    y = imag(poly)\n    dx = x.deriv()\n    dy = y.deriv()\n    x_extremizers = [0, 1] + polyroots(dx, realroots=True,\n                                    condition=lambda r: 0 < r < 1)\n    y_extremizers = [0, 1] + polyroots(dy, realroots=True,\n                                    condition=lambda r: 0 < r < 1)\n    x_extrema = [x(t) for t in x_extremizers]\n    y_extrema = [y(t) for t in y_extremizers]\n    return min(x_extrema), max(x_extrema), min(y_extrema), max(y_extrema)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the width of the intersection of two intervals a b c d", "response": "def interval_intersection_width(a, b, c, d):\n    \"\"\"returns the width of the intersection of intervals [a,b] and [c,d]\n    (thinking of these as intervals on the real number line)\"\"\"\n    return max(0, min(b, d) - max(a, c))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef boxes_intersect(box1, box2):\n    xmin1, xmax1, ymin1, ymax1 = box1\n    xmin2, xmax2, ymin2, ymax2 = box2\n    if interval_intersection_width(xmin1, xmax1, xmin2, xmax2) and \\\n            interval_intersection_width(ymin1, ymax1, ymin2, ymax2):\n        return True\n    else:\n        return False", "response": "Determines if two rectangles each input as a tuple\n            xmin xmax ymin ymax intersect."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bezier_intersections(bez1, bez2, longer_length, tol=1e-8, tol_deC=1e-8):\n    maxits = int(ceil(1-log(tol_deC/longer_length)/log(2)))\n    pair_list = [BPair(bez1, bez2, 0.5, 0.5)]\n    intersection_list = []\n    k = 0\n    approx_point_set = ApproxSolutionSet(tol)\n    while pair_list and k < maxits:\n        new_pairs = []\n        delta = 0.5**(k + 2)\n        for pair in pair_list:\n            bbox1 = bezier_bounding_box(pair.bez1)\n            bbox2 = bezier_bounding_box(pair.bez2)\n            if boxes_intersect(bbox1, bbox2):\n                if box_area(*bbox1) < tol_deC and box_area(*bbox2) < tol_deC:\n                    point = bezier_point(bez1, pair.t1)\n                    if point not in approx_point_set:\n                        approx_point_set.append(point)\n                        # this is the point in the middle of the pair\n                        intersection_list.append((pair.t1, pair.t2))\n\n                    # this prevents the output of redundant intersection points\n                    for otherPair in pair_list:\n                        if pair.bez1 == otherPair.bez1 or \\\n                                pair.bez2 == otherPair.bez2 or \\\n                                pair.bez1 == otherPair.bez2 or \\\n                                pair.bez2 == otherPair.bez1:\n                            pair_list.remove(otherPair)\n                else:\n                    (c11, c12) = halve_bezier(pair.bez1)\n                    (t11, t12) = (pair.t1 - delta, pair.t1 + delta)\n                    (c21, c22) = halve_bezier(pair.bez2)\n                    (t21, t22) = (pair.t2 - delta, pair.t2 + delta)\n                    new_pairs += [BPair(c11, c21, t11, t21),\n                                  BPair(c11, c22, t11, t22),\n                                  BPair(c12, c21, t12, t21),\n                                  BPair(c12, c22, t12, t22)]\n        pair_list = new_pairs\n        k += 1\n    if k >= maxits:\n        raise Exception(\"bezier_intersections has reached maximum \"\n                        \"iterations without terminating... \"\n                        \"either there's a problem/bug or you can fix by \"\n                        \"raising the max iterations or lowering tol_deC\")\n    return intersection_list", "response": "This function checks for intersections between two Bezier curves."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new node of the bezier tree that is a subset of the line.", "response": "def bezier_by_line_intersections(bezier, line):\n    \"\"\"Returns tuples (t1,t2) such that bezier.point(t1) ~= line.point(t2).\"\"\"\n    # The method here is to translate (shift) then rotate the complex plane so\n    # that line starts at the origin and proceeds along the positive real axis.\n    # After this transformation, the intersection points are the real roots of\n    # the imaginary component of the bezier for which the real component is\n    # between 0 and abs(line[1]-line[0])].\n    assert len(line[:]) == 2\n    assert line[0] != line[1]\n    if not any(p != bezier[0] for p in bezier):\n        raise ValueError(\"bezier is nodal, use \"\n                         \"bezier_by_line_intersection(bezier[0], line) \"\n                         \"instead for a bool to be returned.\")\n\n    # First let's shift the complex plane so that line starts at the origin\n    shifted_bezier = [z - line[0] for z in bezier]\n    shifted_line_end = line[1] - line[0]\n    line_length = abs(shifted_line_end)\n\n    # Now let's rotate the complex plane so that line falls on the x-axis\n    rotation_matrix = line_length/shifted_line_end\n    transformed_bezier = [rotation_matrix*z for z in shifted_bezier]\n\n    # Now all intersections should be roots of the imaginary component of\n    # the transformed bezier\n    transformed_bezier_imag = [p.imag for p in transformed_bezier]\n    coeffs_y = bezier2polynomial(transformed_bezier_imag)\n    roots_y = list(polyroots01(coeffs_y))  # returns real roots 0 <= r <= 1\n\n    transformed_bezier_real = [p.real for p in transformed_bezier]\n    intersection_list = []\n    for bez_t in set(roots_y):\n        xval = bezier_point(transformed_bezier_real, bez_t)\n        if 0 <= xval <= line_length:\n            line_t = xval/line_length\n            intersection_list.append((bez_t, line_t))\n    return intersection_list"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nflattening all paths inside a group into a single tree of base coordinates.", "response": "def flatten_all_paths(group, group_filter=lambda x: True,\n                      path_filter=lambda x: True, path_conversions=CONVERSIONS,\n                      group_search_xpath=SVG_GROUP_TAG):\n    \"\"\"Returns the paths inside a group (recursively), expressing the\n    paths in the base coordinates.\n\n    Note that if the group being passed in is nested inside some parent\n    group(s), we cannot take the parent group(s) into account, because\n    xml.etree.Element has no pointer to its parent. You should use\n    Document.flatten_group(group) to flatten a specific nested group into\n    the root coordinates.\n\n    Args:\n        group is an Element\n        path_conversions (dict):\n            A dictionary to convert from an SVG element to a path data\n            string. Any element tags that are not included in this\n            dictionary will be ignored (including the `path` tag). To\n            only convert explicit path elements, pass in\n            `path_conversions=CONVERT_ONLY_PATHS`.\n    \"\"\"\n    if not isinstance(group, Element):\n        raise TypeError('Must provide an xml.etree.Element object. '\n                        'Instead you provided {0}'.format(type(group)))\n\n    # Stop right away if the group_selector rejects this group\n    if not group_filter(group):\n        return []\n\n    # To handle the transforms efficiently, we'll traverse the tree of\n    # groups depth-first using a stack of tuples.\n    # The first entry in the tuple is a group element and the second\n    # entry is its transform. As we pop each entry in the stack, we\n    # will add all its child group elements to the stack.\n    StackElement = collections.namedtuple('StackElement',\n                                          ['group', 'transform'])\n\n    def new_stack_element(element, last_tf):\n        return StackElement(element, last_tf.dot(\n            parse_transform(element.get('transform'))))\n\n    def get_relevant_children(parent, last_tf):\n        children = []\n        for elem in filter(group_filter,\n                           parent.iterfind(group_search_xpath, SVG_NAMESPACE)):\n            children.append(new_stack_element(elem, last_tf))\n        return children\n\n    stack = [new_stack_element(group, np.identity(3))]\n\n    FlattenedPath = collections.namedtuple('FlattenedPath',\n                                           ['path', 'element', 'transform'])\n    paths = []\n\n    while stack:\n        top = stack.pop()\n\n        # For each element type that we know how to convert into path\n        # data, parse the element after confirming that the path_filter\n        # accepts it.\n        for key, converter in path_conversions.items():\n            for path_elem in filter(path_filter, top.group.iterfind(\n                    'svg:'+key, SVG_NAMESPACE)):\n                path_tf = top.transform.dot(\n                    parse_transform(path_elem.get('transform')))\n                path = transform(parse_path(converter(path_elem)), path_tf)\n                paths.append(FlattenedPath(path, path_elem, path_tf))\n\n        stack.extend(get_relevant_children(top.group, top.transform))\n\n    return paths"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef flatten_group(group_to_flatten, root, recursive=True,\n                  group_filter=lambda x: True, path_filter=lambda x: True,\n                  path_conversions=CONVERSIONS,\n                  group_search_xpath=SVG_GROUP_TAG):\n    \"\"\"Flatten all the paths in a specific group.\n\n    The paths will be flattened into the 'root' frame. Note that root\n    needs to be an ancestor of the group that is being flattened.\n    Otherwise, no paths will be returned.\"\"\"\n\n    if not any(group_to_flatten is descendant for descendant in root.iter()):\n        warnings.warn('The requested group_to_flatten is not a '\n                      'descendant of root')\n        # We will shortcut here, because it is impossible for any paths\n        # to be returned anyhow.\n        return []\n\n    # We create a set of the unique IDs of each element that we wish to\n    # flatten, if those elements are groups. Any groups outside of this\n    # set will be skipped while we flatten the paths.\n    desired_groups = set()\n    if recursive:\n        for group in group_to_flatten.iter():\n            desired_groups.add(id(group))\n    else:\n        desired_groups.add(id(group_to_flatten))\n\n    def desired_group_filter(x):\n        return (id(x) in desired_groups) and group_filter(x)\n\n    return flatten_all_paths(root, desired_group_filter, path_filter,\n                             path_conversions, group_search_xpath)", "response": "Flatten all the paths in a specific group into a single group."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nforward the tree of this document into the more general Hierarchy flatten_all_paths function and return the result.", "response": "def flatten_all_paths(self, group_filter=lambda x: True,\n                          path_filter=lambda x: True,\n                          path_conversions=CONVERSIONS):\n        \"\"\"Forward the tree of this document into the more general\n        flatten_all_paths function and return the result.\"\"\"\n        return flatten_all_paths(self.tree.getroot(), group_filter,\n                                 path_filter, path_conversions)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a new path to the SVG.", "response": "def add_path(self, path, attribs=None, group=None):\n        \"\"\"Add a new path to the SVG.\"\"\"\n\n        # If not given a parent, assume that the path does not have a group\n        if group is None:\n            group = self.tree.getroot()\n\n        # If given a list of strings (one or more), assume it represents\n        # a sequence of nested group names\n        elif all(isinstance(elem, str) for elem in group):\n            group = self.get_or_add_group(group)\n\n        elif not isinstance(group, Element):\n            raise TypeError(\n                'Must provide a list of strings or an xml.etree.Element '\n                'object. Instead you provided {0}'.format(group))\n\n        else:\n            # Make sure that the group belongs to this Document object\n            if not self.contains_group(group):\n                warnings.warn('The requested group does not belong to '\n                              'this Document')\n\n        # TODO: It might be better to use duck-typing here with a try-except\n        if isinstance(path, Path):\n            path_svg = path.d()\n        elif is_path_segment(path):\n            path_svg = Path(path).d()\n        elif isinstance(path, str):\n            # Assume this is a valid d-string.\n            # TODO: Should we sanity check the input string?\n            path_svg = path\n        else:\n            raise TypeError(\n                'Must provide a Path, a path segment type, or a valid '\n                'SVG path d-string. Instead you provided {0}'.format(path))\n\n        if attribs is None:\n            attribs = {}\n        else:\n            attribs = attribs.copy()\n\n        attribs['d'] = path_svg\n\n        return SubElement(group, 'path', attribs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_or_add_group(self, nested_names, name_attr='id'):\n        group = self.tree.getroot()\n        # Drill down through the names until we find the desired group\n        while len(nested_names):\n            prev_group = group\n            next_name = nested_names.pop(0)\n            for elem in group.iterfind(SVG_GROUP_TAG, SVG_NAMESPACE):\n                if elem.get(name_attr) == next_name:\n                    group = elem\n                    break\n\n            if prev_group is group:\n                # The group we're looking for does not exist, so let's\n                # create the group structure\n                nested_names.insert(0, next_name)\n\n                while nested_names:\n                    next_name = nested_names.pop(0)\n                    group = self.add_group({'id': next_name}, group)\n                # Now nested_names will be empty, so the topmost\n                # while-loop will end\n        return group", "response": "Get a group from the tree or add a new one with the given name structure."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding an empty group element to the SVG.", "response": "def add_group(self, group_attribs=None, parent=None):\n        \"\"\"Add an empty group element to the SVG.\"\"\"\n        if parent is None:\n            parent = self.tree.getroot()\n        elif not self.contains_group(parent):\n            warnings.warn('The requested group {0} does not belong to '\n                          'this Document'.format(parent))\n\n        if group_attribs is None:\n            group_attribs = {}\n        else:\n            group_attribs = group_attribs.copy()\n\n        return SubElement(parent, '{{{0}}}g'.format(\n            SVG_NAMESPACE['svg']), group_attribs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndisplay the doc using the OS s default application.", "response": "def display(self, filename=None):\n        \"\"\"Displays/opens the doc using the OS's default application.\"\"\"\n\n        if filename is None:\n            filename = self.original_filename\n\n        # write to a (by default temporary) file\n        with open(filename, 'w') as output_svg:\n            output_svg.write(etree.tostring(self.tree.getroot()))\n\n        open_in_browser(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef points_in_each_seg(path, tvals):\n    A = np.array([[-1,  3, -3,  1], # transforms cubic bez to standard poly\n                  [ 3, -6,  3,  0],\n                  [-3,  3,  0,  0],\n                  [ 1,  0,  0,  0]])\n    B = [seg.bpoints() for seg in path]\n    return np.dot(B, np.dot(A, np.power(tvals, [[3],[2],[1],[0]])))", "response": "Compute seg. point for each seg in path and each t in tvals."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a valid SVG transformation string into a 3x3 matrix.", "response": "def parse_transform(transform_str):\n    \"\"\"Converts a valid SVG transformation string into a 3x3 matrix.\n    If the string is empty or null, this returns a 3x3 identity matrix\"\"\"\n    if not transform_str:\n        return np.identity(3)\n    elif not isinstance(transform_str, str):\n        raise TypeError('Must provide a string to parse')\n\n    total_transform = np.identity(3)\n    transform_substrs = transform_str.split(')')[:-1]  # Skip the last element, because it should be empty\n    for substr in transform_substrs:\n        total_transform = total_transform.dot(_parse_transform_substr(substr))\n\n    return total_transform"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns indices of segments that start on a non - differentiable joint.", "response": "def kinks(path, tol=1e-8):\n    \"\"\"returns indices of segments that start on a non-differentiable joint.\"\"\"\n    kink_list = []\n    for idx in range(len(path)):\n        if idx == 0 and not path.isclosed():\n            continue\n        try:\n            u = path[(idx - 1) % len(path)].unit_tangent(1)\n            v = path[idx].unit_tangent(0)\n            u_dot_v = u.real*v.real + u.imag*v.imag\n            flag = False\n        except ValueError:\n            flag = True\n\n        if flag or abs(u_dot_v - 1) > tol:\n            kink_list.append(idx)\n    return kink_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nseeing Andy's notes on Smoothing Bezier Paths for an explanation of the method. Input: two segments seg0, seg1 such that seg0.end==seg1.start, and jointsize, a positive number Output: seg0_trimmed, elbow, seg1_trimmed, where elbow is a cubic bezier object that smoothly connects seg0_trimmed and seg1_trimmed.", "response": "def smoothed_joint(seg0, seg1, maxjointsize=3, tightness=1.99):\n    \"\"\" See Andy's notes on\n    Smoothing Bezier Paths for an explanation of the method.\n    Input: two segments seg0, seg1 such that seg0.end==seg1.start, and\n    jointsize, a positive number\n\n    Output: seg0_trimmed, elbow, seg1_trimmed, where elbow is a cubic bezier\n        object that smoothly connects seg0_trimmed and seg1_trimmed.\n\n    \"\"\"\n    assert seg0.end == seg1.start\n    assert 0 < maxjointsize\n    assert 0 < tightness < 2\n#    sgn = lambda x:x/abs(x)\n    q = seg0.end\n\n    try: v = seg0.unit_tangent(1)\n    except: v = seg0.unit_tangent(1 - 1e-4)\n    try: w = seg1.unit_tangent(0)\n    except: w = seg1.unit_tangent(1e-4)\n\n    max_a = maxjointsize / 2\n    a = min(max_a, min(seg1.length(), seg0.length()) / 20)\n    if isinstance(seg0, Line) and isinstance(seg1, Line):\n        '''\n        Note: Letting\n            c(t) = elbow.point(t), v= the unit tangent of seg0 at 1, w = the\n            unit tangent vector of seg1 at 0,\n            Q = seg0.point(1) = seg1.point(0), and a,b>0 some constants.\n            The elbow will be the unique CubicBezier, c, such that\n            c(0)= Q-av, c(1)=Q+aw, c'(0) = bv, and c'(1) = bw\n            where a and b are derived above/below from tightness and\n            maxjointsize.\n        '''\n#        det = v.imag*w.real-v.real*w.imag\n        # Note:\n        # If det is negative, the curvature of elbow is negative for all\n        # real t if and only if b/a > 6\n        # If det is positive, the curvature of elbow is negative for all\n        # real t if and only if b/a < 2\n\n#        if det < 0:\n#            b = (6+tightness)*a\n#        elif det > 0:\n#            b = (2-tightness)*a\n#        else:\n#            raise Exception(\"seg0 and seg1 are parallel lines.\")\n        b = (2 - tightness)*a\n        elbow = CubicBezier(q - a*v, q - (a - b/3)*v, q + (a - b/3)*w, q + a*w)\n        seg0_trimmed = Line(seg0.start, elbow.start)\n        seg1_trimmed = Line(elbow.end, seg1.end)\n        return seg0_trimmed, [elbow], seg1_trimmed\n    elif isinstance(seg0, Line):\n        '''\n        Note: Letting\n            c(t) = elbow.point(t), v= the unit tangent of seg0 at 1,\n            w = the unit tangent vector of seg1 at 0,\n            Q = seg0.point(1) = seg1.point(0), and a,b>0 some constants.\n            The elbow will be the unique CubicBezier, c, such that\n            c(0)= Q-av, c(1)=Q, c'(0) = bv, and c'(1) = bw\n            where a and b are derived above/below from tightness and\n            maxjointsize.\n        '''\n#        det = v.imag*w.real-v.real*w.imag\n        # Note: If g has the same sign as det, then the curvature of elbow is\n        # negative for all real t if and only if b/a < 4\n        b = (4 - tightness)*a\n#        g = sgn(det)*b\n        elbow = CubicBezier(q - a*v, q + (b/3 - a)*v, q - b/3*w, q)\n        seg0_trimmed = Line(seg0.start, elbow.start)\n        return seg0_trimmed, [elbow], seg1\n    elif isinstance(seg1, Line):\n        args = (seg1.reversed(), seg0.reversed(), maxjointsize, tightness)\n        rseg1_trimmed, relbow, rseg0 = smoothed_joint(*args)\n        elbow = relbow[0].reversed()\n        return seg0, [elbow], rseg1_trimmed.reversed()\n    else:\n        # find a point on each seg that is about a/2 away from joint.  Make\n        # line between them.\n        t0 = seg0.ilength(seg0.length() - a/2)\n        t1 = seg1.ilength(a/2)\n        seg0_trimmed = seg0.cropped(0, t0)\n        seg1_trimmed = seg1.cropped(t1, 1)\n        seg0_line = Line(seg0_trimmed.end, q)\n        seg1_line = Line(q, seg1_trimmed.start)\n\n        args = (seg0_trimmed, seg0_line, maxjointsize, tightness)\n        dummy, elbow0, seg0_line_trimmed = smoothed_joint(*args)\n\n        args = (seg1_line, seg1_trimmed, maxjointsize, tightness)\n        seg1_line_trimmed, elbow1, dummy = smoothed_joint(*args)\n\n        args = (seg0_line_trimmed, seg1_line_trimmed, maxjointsize, tightness)\n        seg0_line_trimmed, elbowq, seg1_line_trimmed = smoothed_joint(*args)\n\n        elbow = elbow0 + [seg0_line_trimmed] + elbowq + [seg1_line_trimmed] + elbow1\n        return seg0_trimmed, elbow, seg1_trimmed"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef smoothed_path(path, maxjointsize=3, tightness=1.99, ignore_unfixable_kinks=False):\n    if len(path) == 1:\n        return path\n\n    assert path.iscontinuous()\n\n    sharp_kinks = []\n    new_path = [path[0]]\n    for idx in range(len(path)):\n        if idx == len(path)-1:\n            if not path.isclosed():\n                continue\n            else:\n                seg1 = new_path[0]\n        else:\n            seg1 = path[idx + 1]\n        seg0 = new_path[-1]\n\n        try:\n            unit_tangent0 = seg0.unit_tangent(1)\n            unit_tangent1 = seg1.unit_tangent(0)\n            flag = False\n        except ValueError:\n            flag = True  # unit tangent not well-defined\n\n        if not flag and isclose(unit_tangent0, unit_tangent1):  # joint is already smooth\n            if idx != len(path)-1:\n                new_path.append(seg1)\n            continue\n        else:\n            kink_idx = (idx + 1) % len(path)  # kink at start of this seg\n            if not flag and isclose(-unit_tangent0, unit_tangent1):\n                # joint is sharp 180 deg (must be fixed manually)\n                new_path.append(seg1)\n                sharp_kinks.append(kink_idx)\n            else:  # joint is not smooth, let's  smooth it.\n                args = (seg0, seg1, maxjointsize, tightness)\n                new_seg0, elbow_segs, new_seg1 = smoothed_joint(*args)\n                new_path[-1] = new_seg0\n                new_path += elbow_segs\n                if idx == len(path) - 1:\n                    new_path[0] = new_seg1\n                else:\n                    new_path.append(new_seg1)\n\n    # If unfixable kinks were found, let the user know\n    if sharp_kinks and not ignore_unfixable_kinks:\n        _report_unfixable_kinks(path, sharp_kinks)\n\n    return Path(*new_path)", "response": "returns a path with no non - differentiable joints."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef display(self, filename=None):\n        if filename is None:\n            filename = 'display_temp.svg'\n        self.save(filename)\n        open_in_browser(filename)", "response": "Displays the doc using the OS s default application."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hex2rgb(value):\n    value = value.lstrip('#')\n    lv = len(value)\n    return tuple(int(value[i:i+lv//3], 16) for i in range(0, lv, lv//3))", "response": "Converts a hexadeximal color string to an RGB 3 - tuple"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef isclose(a, b, rtol=1e-5, atol=1e-8):\n    return abs(a - b) < (atol + rtol * abs(b))", "response": "This is essentially np. isclose but slightly faster."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef open_in_browser(file_location):\n\n    # If just the name of the file was given, check if it's in the Current\n    # Working Directory.\n    if not os.path.isfile(file_location):\n        file_location = os.path.join(os.getcwd(), file_location)\n    if not os.path.isfile(file_location):\n        raise IOError(\"\\n\\nFile not found.\")\n\n    #  For some reason OSX requires this adjustment (tested on 10.10.4)\n    if sys.platform == \"darwin\":\n        file_location = \"file:///\"+file_location\n\n    new = 2  # open in a new tab, if possible\n    webbrowser.get().open(file_location, new=new)", "response": "Attempt to open file located at file_location in the default web\n    browser."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ellipse2pathd(ellipse):\n\n    cx = ellipse.get('cx', 0)\n    cy = ellipse.get('cy', 0)\n    rx = ellipse.get('rx', None)\n    ry = ellipse.get('ry', None)\n    r = ellipse.get('r', None)\n\n    if r is not None:\n        rx = ry = float(r)\n    else:\n        rx = float(rx)\n        ry = float(ry)\n\n    cx = float(cx)\n    cy = float(cy)\n\n    d = ''\n    d += 'M' + str(cx - rx) + ',' + str(cy)\n    d += 'a' + str(rx) + ',' + str(ry) + ' 0 1,0 ' + str(2 * rx) + ',0'\n    d += 'a' + str(rx) + ',' + str(ry) + ' 0 1,0 ' + str(-2 * rx) + ',0'\n\n    return d", "response": "converts the parameters from an ellipse or a circle to a string for a \n    Path object d - attribute"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef polyline2pathd(polyline_d, is_polygon=False):\n    points = COORD_PAIR_TMPLT.findall(polyline_d)\n    closed = (float(points[0][0]) == float(points[-1][0]) and\n              float(points[0][1]) == float(points[-1][1]))\n\n    # The `parse_path` call ignores redundant 'z' (closure) commands\n    # e.g. `parse_path('M0 0L100 100Z') == parse_path('M0 0L100 100L0 0Z')`\n    # This check ensures that an n-point polygon is converted to an n-Line path.\n    if is_polygon and closed:\n        points.append(points[0])\n\n    d = 'M' + 'L'.join('{0} {1}'.format(x,y) for x,y in points)\n    if is_polygon or closed:\n        d += 'z'\n    return d", "response": "converts the string from a polyline points - attribute to a Path object d - attribute"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts an SVG - rect element to a Path d - string.", "response": "def rect2pathd(rect):\n    \"\"\"Converts an SVG-rect element to a Path d-string.\n    \n    The rectangle will start at the (x,y) coordinate specified by the \n    rectangle object and proceed counter-clockwise.\"\"\"\n    x0, y0 = float(rect.get('x', 0)), float(rect.get('y', 0))\n    w, h = float(rect.get('width', 0)), float(rect.get('height', 0))\n    x1, y1 = x0 + w, y0\n    x2, y2 = x0 + w, y0 + h\n    x3, y3 = x0, y0 + h\n\n    d = (\"M{} {} L {} {} L {} {} L {} {} z\"\n         \"\".format(x0, y0, x1, y1, x2, y2, x3, y3))\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts an SVG into a list of Path objects and attribute dictionaries. Converts an SVG file into a list of Path objects and a list of dictionaries containing their attributes. This currently supports SVG Path, Line, Polyline, Polygon, Circle, and Ellipse elements. Args: svg_file_location (string): the location of the svg file return_svg_attributes (bool): Set to True and a dictionary of svg-attributes will be extracted and returned. See also the `svg2paths2()` function. convert_circles_to_paths: Set to False to exclude SVG-Circle elements (converted to Paths). By default circles are included as paths of two `Arc` objects. convert_ellipses_to_paths (bool): Set to False to exclude SVG-Ellipse elements (converted to Paths). By default ellipses are included as paths of two `Arc` objects. convert_lines_to_paths (bool): Set to False to exclude SVG-Line elements (converted to Paths) convert_polylines_to_paths (bool): Set to False to exclude SVG-Polyline elements (converted to Paths) convert_polygons_to_paths (bool): Set to False to exclude SVG-Polygon elements (converted to Paths) convert_rectangles_to_paths (bool): Set to False to exclude SVG-Rect elements (converted to Paths). Returns: list: The list of Path objects. list: The list of corresponding path attribute dictionaries. dict (optional): A dictionary of svg-attributes (see `svg2paths2()`).", "response": "def svg2paths(svg_file_location,\n              return_svg_attributes=False,\n              convert_circles_to_paths=True,\n              convert_ellipses_to_paths=True,\n              convert_lines_to_paths=True,\n              convert_polylines_to_paths=True,\n              convert_polygons_to_paths=True,\n              convert_rectangles_to_paths=True):\n    \"\"\"Converts an SVG into a list of Path objects and attribute dictionaries. \n\n    Converts an SVG file into a list of Path objects and a list of\n    dictionaries containing their attributes.  This currently supports\n    SVG Path, Line, Polyline, Polygon, Circle, and Ellipse elements.\n\n    Args:\n        svg_file_location (string): the location of the svg file\n        return_svg_attributes (bool): Set to True and a dictionary of\n            svg-attributes will be extracted and returned.  See also the \n            `svg2paths2()` function.\n        convert_circles_to_paths: Set to False to exclude SVG-Circle\n            elements (converted to Paths).  By default circles are included as \n            paths of two `Arc` objects.\n        convert_ellipses_to_paths (bool): Set to False to exclude SVG-Ellipse\n            elements (converted to Paths).  By default ellipses are included as \n            paths of two `Arc` objects.\n        convert_lines_to_paths (bool): Set to False to exclude SVG-Line elements\n            (converted to Paths)\n        convert_polylines_to_paths (bool): Set to False to exclude SVG-Polyline\n            elements (converted to Paths)\n        convert_polygons_to_paths (bool): Set to False to exclude SVG-Polygon\n            elements (converted to Paths)\n        convert_rectangles_to_paths (bool): Set to False to exclude SVG-Rect\n            elements (converted to Paths).\n\n    Returns: \n        list: The list of Path objects.\n        list: The list of corresponding path attribute dictionaries.\n        dict (optional): A dictionary of svg-attributes (see `svg2paths2()`).\n    \"\"\"\n    if os_path.dirname(svg_file_location) == '':\n        svg_file_location = os_path.join(getcwd(), svg_file_location)\n\n    doc = parse(svg_file_location)\n\n    def dom2dict(element):\n        \"\"\"Converts DOM elements to dictionaries of attributes.\"\"\"\n        keys = list(element.attributes.keys())\n        values = [val.value for val in list(element.attributes.values())]\n        return dict(list(zip(keys, values)))\n\n    # Use minidom to extract path strings from input SVG\n    paths = [dom2dict(el) for el in doc.getElementsByTagName('path')]\n    d_strings = [el['d'] for el in paths]\n    attribute_dictionary_list = paths\n\n    # Use minidom to extract polyline strings from input SVG, convert to\n    # path strings, add to list\n    if convert_polylines_to_paths:\n        plins = [dom2dict(el) for el in doc.getElementsByTagName('polyline')]\n        d_strings += [polyline2pathd(pl['points']) for pl in plins]\n        attribute_dictionary_list += plins\n\n    # Use minidom to extract polygon strings from input SVG, convert to\n    # path strings, add to list\n    if convert_polygons_to_paths:\n        pgons = [dom2dict(el) for el in doc.getElementsByTagName('polygon')]\n        d_strings += [polygon2pathd(pg['points']) for pg in pgons]\n        attribute_dictionary_list += pgons\n\n    if convert_lines_to_paths:\n        lines = [dom2dict(el) for el in doc.getElementsByTagName('line')]\n        d_strings += [('M' + l['x1'] + ' ' + l['y1'] +\n                       'L' + l['x2'] + ' ' + l['y2']) for l in lines]\n        attribute_dictionary_list += lines\n\n    if convert_ellipses_to_paths:\n        ellipses = [dom2dict(el) for el in doc.getElementsByTagName('ellipse')]\n        d_strings += [ellipse2pathd(e) for e in ellipses]\n        attribute_dictionary_list += ellipses\n\n    if convert_circles_to_paths:\n        circles = [dom2dict(el) for el in doc.getElementsByTagName('circle')]\n        d_strings += [ellipse2pathd(c) for c in circles]\n        attribute_dictionary_list += circles\n\n    if convert_rectangles_to_paths:\n        rectangles = [dom2dict(el) for el in doc.getElementsByTagName('rect')]\n        d_strings += [rect2pathd(r) for r in rectangles]\n        attribute_dictionary_list += rectangles\n\n    if return_svg_attributes:\n        svg_attributes = dom2dict(doc.getElementsByTagName('svg')[0])\n        doc.unlink()\n        path_list = [parse_path(d) for d in d_strings]\n        return path_list, attribute_dictionary_list, svg_attributes\n    else:\n        doc.unlink()\n        path_list = [parse_path(d) for d in d_strings]\n        return path_list, attribute_dictionary_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef svg2paths2(svg_file_location,\n               return_svg_attributes=True,\n               convert_circles_to_paths=True,\n               convert_ellipses_to_paths=True,\n               convert_lines_to_paths=True,\n               convert_polylines_to_paths=True,\n               convert_polygons_to_paths=True,\n               convert_rectangles_to_paths=True):\n    \"\"\"Convenience function; identical to svg2paths() except that\n    return_svg_attributes=True by default.  See svg2paths() docstring for more\n    info.\"\"\"\n    return svg2paths(svg_file_location=svg_file_location,\n                     return_svg_attributes=return_svg_attributes,\n                     convert_circles_to_paths=convert_circles_to_paths,\n                     convert_ellipses_to_paths=convert_ellipses_to_paths,\n                     convert_lines_to_paths=convert_lines_to_paths,\n                     convert_polylines_to_paths=convert_polylines_to_paths,\n                     convert_polygons_to_paths=convert_polygons_to_paths,\n                     convert_rectangles_to_paths=convert_rectangles_to_paths)", "response": "Convenience function to convert an SVG file to a list of paths."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _reinit_daq_daemons(sender, instance, **kwargs):\n    if type(instance) is ModbusDevice:\n        post_save.send_robust(sender=Device, instance=instance.modbus_device)\n    elif type(instance) is ModbusVariable:\n        post_save.send_robust(sender=Variable, instance=instance.modbus_variable)\n    elif type(instance) is ExtendedModbusVariable:\n        post_save.send_robust(sender=Variable, instance=Variable.objects.get(pk=instance.pk))\n    elif type(instance) is ExtendedModbusDevice:\n        post_save.send_robust(sender=Device, instance=Device.objects.get(pk=instance.pk))", "response": "update the daq daemon configuration when changes be applied in the models\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread values from the device", "response": "def read_data(self, variable_instance):\n        \"\"\"\n        read values from the device\n        \"\"\"\n        if self.inst is None:\n            return\n        if variable_instance.visavariable.device_property.upper() == 'PRESENT_VALUE':\n            return self.parse_value(self.inst.query('?U6P0'))\n        elif variable_instance.visavariable.device_property.upper() == 'PRESENT_VALUE_MANUAL_C_FREQ':\n            freq = VariableProperty.objects.get_property(variable=variable_instance, name='VISA:FREQ')\n            if freq is None:\n                freq = 500\n            return self.parse_value(self.inst.query('?MAM1SR9HT3ST2SM%dE'%freq))\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert dtype to matlab class string", "response": "def dtype_to_matlab_class(dtype):\n    \"\"\"\n    convert dtype to matlab class string\n    \"\"\"\n    if dtype.str in ['<f8']:\n        return 'double'\n    elif dtype.str in ['<f4']:\n        return 'single'\n    elif dtype.str in ['<i8']:\n        return 'int64'\n    elif dtype.str in ['<u8']:\n        return 'uint64'\n    elif dtype.str in ['<i4']:\n        return 'int32'\n    elif dtype.str in ['<u4']:\n        return 'uint32'\n    elif dtype.str in ['<i2']:\n        return 'int16'\n    elif dtype.str in ['<u2']:\n        return 'uint16'\n    elif dtype.str in ['|i1']:\n        return 'int8'\n    elif dtype.str in ['|u1']:\n        return 'uint8'"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nestablishes a connection to the Instrument", "response": "def connect(self):\n        \"\"\"\n        establish a connection to the Instrument\n        \"\"\"\n        if not driver_ok:\n            logger.error(\"Visa driver NOT ok\")\n            return False\n        visa_backend = '@py'  # use PyVISA-py as backend\n        if hasattr(settings, 'VISA_BACKEND'):\n            visa_backend = settings.VISA_BACKEND\n\n        try:\n            self.rm = visa.ResourceManager(visa_backend)\n        except:\n            logger.error(\"Visa ResourceManager cannot load resources : %s\" %self)\n            return False\n        try:\n            resource_prefix = self._device.visadevice.resource_name.split('::')[0]\n            extras = {}\n            if hasattr(settings, 'VISA_DEVICE_SETTINGS'):\n                if resource_prefix in settings.VISA_DEVICE_SETTINGS:\n                    extras = settings.VISA_DEVICE_SETTINGS[resource_prefix]\n            logger.debug('VISA_DEVICE_SETTINGS for %s: %r'%(resource_prefix,extras))\n            self.inst = self.rm.open_resource(self._device.visadevice.resource_name, **extras)\n        except:\n            logger.error(\"Visa ResourceManager cannot open resource : %s\" %self._device.visadevice.resource_name)\n            return False\n        logger.debug('connected visa device')\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef decode_bcd(values):\n\n    bin_str_out = ''\n    if isinstance(values, integer_types):\n        bin_str_out = bin(values)[2:].zfill(16)\n        bin_str_out = bin_str_out[::-1]\n    else:\n        for value in values:\n            bin_str = bin(value)[2:].zfill(16)\n            bin_str = bin_str[::-1]\n            bin_str_out = bin_str + bin_str_out\n\n    dec_num = 0\n    for i in range(len(bin_str_out) / 4):\n        bcd_num = int(bin_str_out[(i * 4):(i + 1) * 4][::-1], 2)\n        if bcd_num > 9:\n            dec_num = -dec_num\n        else:\n            dec_num = dec_num + (bcd_num * pow(10, i))\n    return dec_num", "response": "decode bcd as int to dec\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds a gap in the list of modbus registers", "response": "def find_gap(l, value):\n    \"\"\"\n    try to find a address gap in the list of modbus registers\n    \"\"\"\n    for index in range(len(l)):\n        if l[index] == value:\n            return None\n        if l[index] > value:\n            return index"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _connect(self):\n        framer = None\n        if self._framer == 0:  # Socket Framer\n            framer = ModbusSocketFramer\n        elif self._framer == 1:  # RTU Framer\n            framer = ModbusRtuFramer\n        elif self._framer == 2:  # ASCII Framer\n            framer = ModbusAsciiFramer\n        elif self._framer == 3:  # Binary Framer\n            framer = ModbusBinaryFramer\n\n        if self._protocol == 0:  # TCP\n            if self._framer is None:  # No Framer\n                self.slave = ModbusTcpClient(self._address, int(self._port))\n            else:\n                self.slave = ModbusTcpClient(self._address, int(self._port), framer=framer)\n        elif self._protocol == 1:  # UDP\n            if self._framer is None:  # No Framer\n                self.slave = ModbusUdpClient(self._address, int(self._port))\n            else:\n                self.slave = ModbusUdpClient(self._address, int(self._port), framer=framer)\n        elif self._protocol in (2, 3, 4):  # serial\n            method_list = {2: 'ascii', 3: 'rtu', 4: 'binary'}\n            self.slave = ModbusSerialClient(\n                method=method_list[self._protocol],\n                port=self._port,\n                stopbits=self._stopbits,\n                bytesize=self._bytesize,\n                parity=self._parity,\n                baudrate=self._baudrate,\n                timeout=self._timeout)\n        else:\n            raise NotImplementedError(\"Protocol not supported\")\n        status = self.slave.connect()\n        return status", "response": "Connects to the modbus slave."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_data(self, variable_id, value, task):\n        if variable_id not in self.variables:\n            return False\n\n        if not self.variables[variable_id].writeable:\n            return False\n\n        if self.variables[variable_id].modbusvariable.function_code_read == 3:\n            # write register\n            if 0 <= self.variables[variable_id].modbusvariable.address <= 65535:\n\n                if self._connect():\n                    if self.variables[variable_id].get_bits_by_class() / 16 == 1:\n                        # just write the value to one register\n                        self.slave.write_register(self.variables[variable_id].modbusvariable.address, int(value),\n                                                  unit=self._unit_id)\n                    else:\n                        # encode it first\n                        self.slave.write_registers(self.variables[variable_id].modbusvariable.address,\n                                                   list(self.variables[variable_id].encode_value(value)),\n                                                   unit=self._unit_id)\n                    self._disconnect()\n                    return True\n                else:\n                    logger.info(\"device with id: %d is now accessible\" % self.device.pk)\n                    return False\n            else:\n                logger.error('Modbus Address %d out of range' % self.variables[variable_id].modbusvariable.address)\n                return False\n        elif self.variables[variable_id].modbusvariable.function_code_read == 1:\n            # write coil\n            if 0 <= self.variables[variable_id].modbusvariable.address <= 65535:\n                if self._connect():\n                    self.slave.write_coil(self.variables[variable_id].modbusvariable.address, bool(value),\n                                          unit=self._unit_id)\n                    self._disconnect()\n                    return True\n                else:\n                    logger.info(\"device with id: %d is now accessible\" % self.device.pk)\n                    return False\n            else:\n                logger.error('Modbus Address %d out of range' % self.variables[variable_id].modbusvariable.address)\n        else:\n            logger.error('wrong type of function code %d' %\n                         self.variables[variable_id].modbusvariable.function_code_read)\n            return False", "response": "write value to modbus register or coil"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nkills a dedicated process", "response": "def kill_process(modeladmin, request, queryset):\n    \"\"\"\n    restarts a dedicated process\n    :return:\n    \"\"\"\n    for process in queryset:\n        process.stop(signum=signal.SIGKILL)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef lookups(self, request, model_admin):\n        qs = model_admin.get_queryset(request)\n        qs.filter(id__range=(1, 99))\n        for item in qs:\n            dp = DeviceProtocol.objects.filter(pk=item.id).first()\n            if dp:\n                yield (dp.pk, dp.app_name)", "response": "Returns a list of tuples. The first element in each tuple is the coded value for the option that will appear\n        in the URL query. The second element is the human - readable name for the option that will appear\n        in the right sidebar."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the filtered queryset based on the value of the record set.", "response": "def queryset(self, request, queryset):\n        \"\"\"\n        Returns the filtered queryset based on the value\n        provided in the query string and retrievable via\n        `self.value()`.\n        \"\"\"\n        if self.value() is not None:\n            if self.value() > 0:\n                return queryset.filter(parent_process_id=self.value())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _reinit_daq_daemons(sender, instance, **kwargs):\n    if type(instance) is OneWireDevice:\n        post_save.send_robust(sender=Device, instance=instance.onewire_device)\n    elif type(instance) is OneWireVariable:\n        post_save.send_robust(sender=Variable, instance=instance.onewire_variable)\n    elif type(instance) is ExtendedOneWireVariable:\n        post_save.send_robust(sender=Variable, instance=Variable.objects.get(pk=instance.pk))\n    elif type(instance) is ExtendedOneWireDevice:\n        post_save.send_robust(sender=Device, instance=Device.objects.get(pk=instance.pk))", "response": "update the daq daemon configuration when changes be applied in the models\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_data(self,variable_id, value, task):\n        '''\n        write value to the instrument/device\n        '''\n        output = []\n        if not driver_visa_ok:\n            logger.info(\"Visa-device-write data-visa NOT ok\")\n            return output\n        for item in self.variables.values():\n            if not (item.visavariable.variable_type == 0 and item.id == variable_id):\n                # skip all config values\n                continue\n            start=time()\n            # read_value = self._h.write_data(item.visavariable.device_property, value)\n            read_value = self._h.write_data(variable_id, value, task)\n            end=time()\n            duration=float(end - start)\n            logger.info((\"%s - %s - %s - %s - %s - %s\") %(item.device.__str__(), item.__str__(), item.visavariable.device_property, value, read_value, duration))\n            if read_value is not None and item.update_value(read_value, time()):\n                output.append(item.create_recorded_data_element())\n            else:\n                logger.info(\"Visa-Output not ok : %s\" % output)\n        return output", "response": "write value to the instrument / device"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef request_data(self):\n        output = []\n        if not driver_visa_ok:\n            logger.info('Request Data Visa Driver Not Ok')\n            return output\n        \n        for item in self.variables.values():\n            if not item.visavariable.variable_type == 1:\n                # skip all config values\n                continue\n            \n            value = self._h.read_data(item)\n            \n            if value is not None and item.update_value(value, time()):\n                output.append(item.create_recorded_data_element())\n\n        return output", "response": "Request data from the instrument and device"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef loop(self):\n        today = date.today()\n        # only start new jobs after change the day changed\n        if self._current_day != gmtime().tm_yday:\n            self._current_day = gmtime().tm_yday\n            for job in ScheduledExportTask.objects.filter(active=1):  # get all active jobs\n\n                add_task = False\n                if job.export_period == 1:  # daily\n                    start_time = '%s %02d:00:00' % ((today - timedelta(1)).strftime('%d-%b-%Y'),\n                                                    job.day_time)  # \"%d-%b-%Y %H:%M:%S\"\n                    start_time = mktime(datetime.strptime(start_time, \"%d-%b-%Y %H:%M:%S\").timetuple())\n                    filename_suffix = 'daily_export_%d_%s' % (job.pk, job.label)\n                    add_task = True\n                elif job.export_period == 2 and gmtime().tm_yday % 2 == 0:  # on even days (2,4,...)\n                    start_time = '%s %02d:00:00' % ((today - timedelta(2)).strftime('%d-%b-%Y'),\n                                                    job.day_time)  # \"%d-%b-%Y %H:%M:%S\"\n                    start_time = mktime(datetime.strptime(start_time, \"%d-%b-%Y %H:%M:%S\").timetuple())\n                    filename_suffix = 'two_day_export_%d_%s' % (job.pk, job.label)\n                    add_task = True\n                elif job.export_period == 7 and gmtime().tm_wday == 0:  # on every monday\n                    start_time = '%s %02d:00:00' % ((today - timedelta(7)).strftime('%d-%b-%Y'),\n                                                    job.day_time)  # \"%d-%b-%Y %H:%M:%S\"\n                    start_time = mktime(datetime.strptime(start_time, \"%d-%b-%Y %H:%M:%S\").timetuple())\n                    filename_suffix = 'weekly_export_%d_%s' % (job.pk, job.label)\n                    add_task = True\n                elif job.export_period == 14 and gmtime().tm_yday % 14 == 0:  # on every second monday\n                    start_time = '%s %02d:00:00' % ((today - timedelta(14)).strftime('%d-%b-%Y'),\n                                                    job.day_time)  # \"%d-%b-%Y %H:%M:%S\"\n                    start_time = mktime(datetime.strptime(start_time, \"%d-%b-%Y %H:%M:%S\").timetuple())\n                    filename_suffix = 'two_week_export_%d_%s' % (job.pk, job.label)\n                    add_task = True\n                elif job.export_period == 30 and gmtime().tm_yday % 30 == 0:  # on every 30 days\n                    start_time = '%s %02d:00:00' % ((today - timedelta(30)).strftime('%d-%b-%Y'),\n                                                    job.day_time)  # \"%d-%b-%Y %H:%M:%S\"\n                    start_time = mktime(datetime.strptime(start_time, \"%d-%b-%Y %H:%M:%S\").timetuple())\n                    filename_suffix = '30_day_export_%d_%s' % (job.pk, job.label)\n                    add_task = True\n\n                if job.day_time == 0:\n                    end_time = '%s %02d:59:59' % ((today - timedelta(1)).strftime('%d-%b-%Y'),\n                                                  23)  # \"%d-%b-%Y %H:%M:%S\"\n                else:\n                    end_time = '%s %02d:59:59' % (today.strftime('%d-%b-%Y'), job.day_time - 1)  # \"%d-%b-%Y %H:%M:%S\"\n                end_time = mktime(datetime.strptime(end_time, \"%d-%b-%Y %H:%M:%S\").timetuple())\n                # create ExportTask\n                if add_task:\n                    et = ExportTask(\n                        label=filename_suffix,\n                        datetime_max=datetime.fromtimestamp(end_time, UTC),\n                        datetime_min=datetime.fromtimestamp(start_time, UTC),\n                        filename_suffix=filename_suffix,\n                        mean_value_period=job.mean_value_period,\n                        file_format=job.file_format,\n                        datetime_start=datetime.fromtimestamp(end_time + 60, UTC)\n                        )\n                    et.save()\n\n                    et.variables.add(*job.variables.all())\n\n        # check running tasks and start the next Export Task\n        running_jobs = ExportTask.objects.filter(busy=True, failed=False)\n        if running_jobs:\n            for job in running_jobs:\n                if time() - job.start() < 30:\n                    # only check Task when it is running longer then 30s\n                    continue\n\n                if job.backgroundprocess is None:\n                    # if the job has no backgroundprocess assosiated mark as failed\n                    job.failed = True\n                    job.save()\n                    continue\n\n                if now() - timedelta(hours=1) > job.backgroundprocess.last_update:\n                    # if the Background Process has been updated in the past 60s wait\n                    continue\n\n                if job.backgroundprocess.pid == 0:\n                    # if the job has no valid pid mark as failed\n                    job.failed = True\n                    job.save()\n                    continue\n\n        else:\n            # start the next Export Task\n            job = ExportTask.objects.filter(\n                done=False,\n                busy=False,\n                failed=False,\n                datetime_start__lte=datetime.now(UTC)).first()  # get all jobs\n            if job:\n                bp = BackgroundProcess(label='pyscada.export.ExportProcess-%d' % job.pk,\n                                       message='waiting..',\n                                       enabled=True,\n                                       parent_process_id=self.parent_process_id,\n                                       process_class='pyscada.export.worker.ExportProcess',\n                                       process_class_kwargs=json.dumps({\"job_id\": job.pk}))\n                bp.save()\n                if job.datetime_start is None:\n                    job.datetime_start = datetime.now(UTC)\n                job.busy = True\n                job.save()\n\n        # delete all done jobs older the 60 days\n        for job in ExportTask.objects.filter(done=True, busy=False,\n                                             datetime_start__gte=datetime.fromtimestamp(time() + 60 * 24 * 60 * 60,\n                                                                                        UTC)):\n            job.delete()\n        # delete all failed jobs older the 60 days\n        for job in ExportTask.objects.filter(failed=True,\n                                             datetime_start__gte=datetime.fromtimestamp(time() + 60 * 24 * 60 * 60,\n                                                                                        UTC)):\n            job.delete()\n\n        return 1, None", "response": "This function will loop over all scheduled jobs and add tasks to the scheduled jobs list."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef demonize(self):\n        # check if a process is already running\n        if access(self.pid_file_name, F_OK):\n            # read the pid file\n            pid = self.read_pid()\n\n            try:\n                kill(pid, 0)  # check if process is running\n                self.stderr.write(\"process is already running\\n\")\n                return False\n            except OSError as e:\n                if e.errno == errno.ESRCH:\n                    # process is dead\n                    self.delete_pid(force_del=True)\n                else:\n                    self.stderr.write(\"demonize failed, something went wrong: %d (%s)\\n\" % (e.errno, e.strerror))\n                    return False\n\n        try:\n            pid = fork()\n            if pid > 0:\n                # Exit from the first parent\n                timeout = time() + 60\n                while self.read_pid() is None:\n                    self.stderr.write(\"waiting for pid..\\n\")\n                    sleep(0.5)\n                    if time() > timeout:\n                        break\n                self.stderr.write(\"pid is %d\\n\" % self.read_pid())\n                sys.exit(0)\n        except OSError as e:\n            self.stderr.write(\"demonize failed in 1. Fork: %d (%s)\\n\" % (e.errno, e.strerror))\n            sys.exit(1)\n\n        # Decouple from parent environment\n        # os.chdir(\"/\")\n        setsid()\n        umask(0)\n\n        # Do the Second fork\n        try:\n            pid = fork()\n            if pid > 0:\n                # Exit from the second parent\n                sys.exit(0)\n        except OSError as e:\n            self.stderr.write(\"demonize failed in 2. Fork: %d (%s)\\n\" % (e.errno, e.strerror))\n            sys.exit(1)\n\n        # Redirect standard file descriptors\n        # sys.stdout.flush()\n        # sys.stderr.flush()\n        # si = file(self.stdin, 'r')\n        # so = file(self.stdout, 'a+')\n        # se = file(self.stderr, 'a+',\n        # os.dup2(si.fileno(), sys.stdin.fileno())\n        # os.dup2(so.fileno(), sys.stdout.fileno())\n        # os.dup2(se.fileno(), sys.stderr.fileno())\n\n        # Write the PID file\n        #atexit.register(self.delete_pid)\n        self.write_pid()\n        return True", "response": "Do the double fork magic"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting the pid file", "response": "def delete_pid(self, force_del=False):\n        \"\"\"\n        delete the pid file\n        \"\"\"\n        pid = self.read_pid()\n        if pid != getpid() or force_del:\n            logger.debug('process %d tried to delete pid' % getpid())\n            return False\n\n        if access(self.pid_file_name, F_OK):\n            try:\n                remove(self.pid_file_name)  # remove the old pid file\n                logger.debug('delete pid (%d)' % getpid())\n            except:\n                logger.debug(\"can't delete pid file\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self):\n        try:\n            master_process = BackgroundProcess.objects.filter(pk=self.process_id).first()\n            if master_process:\n                master_process.last_update = now()\n                master_process.message = 'init child processes'\n                master_process.save()\n            else:\n                self.delete_pid(force_del=True)\n                self.stderr.write(\"no such process in BackgroundProcesses\")\n                sys.exit(0)\n\n            self.manage_processes()\n            while True:\n                # handle signals\n                sig = self.SIG_QUEUE.pop(0) if len(self.SIG_QUEUE) else None\n\n                # check the DB connection\n                check_db_connection()\n\n                # update the P\n                BackgroundProcess.objects.filter(pk=self.process_id).update(\n                    last_update=now(),\n                    message='running..')\n                if sig is None:\n                    self.manage_processes()\n                elif sig not in self.SIGNALS:\n                    logger.error('%s, unhandled signal %d' % (self.label, sig))\n                    continue\n                elif sig == signal.SIGTERM:\n                    logger.debug('%s, termination signal' % self.label)\n                    raise StopIteration\n                elif sig == signal.SIGHUP:\n                    # todo handle sighup\n                    pass\n                elif sig == signal.SIGUSR1:\n                    # restart all child processes\n                    logger.debug('PID %d, processed SIGUSR1 (%d) signal' % (self.pid, sig))\n                    self.restart()\n                elif sig == signal.SIGUSR2:\n                    # write the process status to stdout\n                    self.status()\n                    pass\n                sleep(5)\n        except StopIteration:\n            self.stop()\n            self.delete_pid()\n            sys.exit(0)\n        except SystemExit:\n            raise\n        except:\n            logger.error('%s(%d), unhandled exception\\n%s' % (self.label, getpid(), traceback.format_exc()))", "response": "main loop for the main loop"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_chld(self, sig, frame):\n        try:\n            while True:\n                wpid, status = waitpid(-1, WNOHANG)\n                if not wpid:\n                    break\n                    # self.stdout.write('%d,%d\\n' % (wpid, status))\n        except:\n            pass", "response": "handle_chld - Handles SIGCHLD handling\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef restart(self):\n        BackgroundProcess.objects.filter(pk=self.process_id).update(\n            last_update=now(),\n            message='restarting..')\n        timeout = time() + 60  # wait max 60 seconds\n        self.kill_processes(signal.SIGTERM)\n        while self.PROCESSES and time() < timeout:\n            sleep(0.1)\n        self.kill_processes(signal.SIGKILL)\n        self.manage_processes()\n        logger.debug('BD %d: restarted'%self.process_id)", "response": "restart all child processes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstop the scheduler and stop all processes", "response": "def stop(self, sig=signal.SIGTERM):\n        \"\"\"\n        stop the scheduler and stop all processes\n        \"\"\"\n\n        if self.pid is None:\n            self.pid = self.read_pid()\n        if self.pid is None:\n            sp = BackgroundProcess.objects.filter(pk=1).first()\n            if sp:\n                self.pid = sp.pid\n        if self.pid is None or self.pid == 0:\n            logger.error(\"can't determine process id exiting.\")\n            return False\n        if self.pid != getpid():\n            # calling from outside the daemon instance\n            logger.debug('send sigterm to daemon')\n            try:\n                kill(self.pid, sig)\n                return True\n            except OSError as e:\n                if e.errno == errno.ESRCH:\n                    return False\n                else:\n                    return False\n\n        logger.debug('start termination of the daemon')\n        BackgroundProcess.objects.filter(pk=self.process_id).update(\n            last_update=now(),\n            message='stopping..')\n\n        timeout = time() + 60  # wait max 60 seconds\n        self.kill_processes(signal.SIGTERM)\n        while self.PROCESSES and time() < timeout:\n            self.kill_processes(signal.SIGTERM)\n            sleep(1)\n        self.kill_processes(signal.SIGKILL)\n        BackgroundProcess.objects.filter(pk=self.process_id).update(\n            last_update=now(),\n            message='stopped')\n        logger.debug('termination of the daemon done')\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef spawn_process(self, process=None):\n        if process is None:\n            return False\n        # start new child process\n        pid = fork()\n        if pid != 0:\n            # parent process\n            process.pid = pid\n            self.PROCESSES[process.process_id] = process\n            connections.close_all()\n            return True\n        # child process\n        process.pid = getpid()\n        # connection.connection.close()\n        # connection.connection = None\n        process.pre_init_process()\n        process.init_process()\n        process.run()\n        sys.exit(0)", "response": "Spawn a new process."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef status(self):\n\n        if self.pid is None:\n            self.pid = self.read_pid()\n        if self.pid is None:\n            sp = BackgroundProcess.objects.filter(pk=1).first()\n            if sp:\n                self.pid = sp.pid\n        if self.pid is None or self.pid == 0:\n            self.stderr.write(\"%s: can't determine process id exiting.\\n\" % datetime.now().isoformat(' '))\n            return False\n        if self.pid != getpid():\n            # calling from outside the daemon instance\n            try:\n                kill(self.pid, signal.SIGUSR2)\n                return True\n            except OSError as e:\n                if e.errno == errno.ESRCH:\n                    return False\n                else:\n                    return False\n\n        process_list = []\n        for process in BackgroundProcess.objects.filter(parent_process__pk=self.process_id):\n            process_list.append(process)\n            process_list += list(process.backgroundprocess_set.filter())\n        for process in process_list:\n            logger.debug('%s, parrent process_id %d' % (self.label, process.parent_process.pk))\n            logger.debug('%s, process_id %d' % (self.label, self.process_id))", "response": "check the status of the current daemon instance"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pre_init_process(self):\n        db.connections.close_all()\n        # update process info\n        BackgroundProcess.objects.filter(pk=self.process_id).update(\n            pid=self.pid,\n            last_update=now(),\n            running_since=now(),\n            done=False,\n            failed=False,\n            message='init process..',\n        )\n\n        [signal.signal(s, signal.SIG_DFL) for s in self.SIGNALS]  # reset\n        [signal.signal(s, self.signal) for s in self.SIGNALS]", "response": "update the recordset with the current process info"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef init_process(self):\n\n        for item in Device.objects.filter(protocol__daq_daemon=1, active=1, id__in=self.device_ids):\n            try:\n                tmp_device = item.get_device_instance()\n                if tmp_device is not None:\n                    self.devices[item.pk] = tmp_device\n                    self.dt_set = min(self.dt_set, item.polling_interval)\n                    self.dt_query_data = min(self.dt_query_data, item.polling_interval)\n            except:\n                var = traceback.format_exc()\n                logger.error(\"exception while initialisation of DAQ Process for Device %d %s %s\" % (\n                    item.pk, linesep, var))\n\n        return True", "response": "init a standard daq process for multiple devices"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the daq daemon configuration when changes be applied in the models", "response": "def _reinit_daq_daemons(sender, instance, **kwargs):\n    \"\"\"\n    update the daq daemon configuration when changes be applied in the models\n    \"\"\"\n    if type(instance) is SMbusDevice:\n        post_save.send_robust(sender=Device, instance=instance.smbus_device)\n    elif type(instance) is SMbusVariable:\n        post_save.send_robust(sender=Variable, instance=instance.smbus_variable)\n    elif type(instance) is ExtendedSMbusVariable:\n        post_save.send_robust(sender=Variable, instance=Variable.objects.get(pk=instance.pk))\n    elif type(instance) is ExtendedSMBusDevice:\n        post_save.send_robust(sender=Device, instance=Device.objects.get(pk=instance.pk))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading values from the device", "response": "def read_data(self,variable_instance):\n        \"\"\"\n        read values from the device\n        \"\"\"\n        if self.inst is None:\n            return\n        if variable_instance.visavariable.device_property.upper() == 'PRESENT_VALUE':\n            return self.parse_value(self.inst.query('?U6P0'))\n        elif variable_instance.visavariable.device_property.upper() == 'PRESENT_VALUE_DCV':\n            return self.parse_value(self.inst.query('?U6P0F1T3'))\n        elif variable_instance.visavariable.device_property.upper() == 'PRESENT_VALUE_ACV':\n            return self.parse_value(self.inst.query('?U6P0F2T3'))\n        elif variable_instance.visavariable.device_property.upper() == 'PRESENT_VALUE_DCV+ACV':\n            return self.parse_value(self.inst.query('?U6P0F3T3'))\n        elif variable_instance.visavariable.device_property.upper() == 'PRESENT_VALUE_2W_OHM':\n            return self.parse_value(self.inst.query('?U6P0F4T3'))\n        elif variable_instance.visavariable.device_property.upper() == 'PRESENT_VALUE_4W_OHM':\n            return self.parse_value(self.inst.query('?U6P0F5T3'))\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_data(self,variable_id, value, task):\n        variable = self._variables[variable_id]\n        if task.property_name != '':\n            # write the freq property to VariableProperty use that for later read\n            vp = VariableProperty.objects.update_or_create_property(variable=variable, name=task.property_name.upper(),\n                                                        value=value, value_class='FLOAT64')\n            return True\n        if variable.visavariable.variable_type == 0:  # configuration\n            # only write to configuration variables\n            pass\n        else:\n            return False", "response": "write values to the device"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the daq daemon configuration when changes be applied in the models", "response": "def _reinit_daq_daemons(sender, instance, **kwargs):\n    \"\"\"\n    update the daq daemon configuration when changes be applied in the models\n    \"\"\"\n    if type(instance) is SystemStatVariable:\n        post_save.send_robust(sender=Variable, instance=instance.system_stat_variable)\n    elif type(instance) is ExtendedSystemStatVariable:\n        post_save.send_robust(sender=Variable, instance=Variable.objects.get(pk=instance.pk))\n    elif type(instance) is ExtendedSystemStatDevice:\n        post_save.send_robust(sender=Device, instance=Device.objects.get(pk=instance.pk))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading values from the device", "response": "def read_data(self, variable_instance):\n        \"\"\"\n        read values from the device\n        \"\"\"\n        if self.inst is None:\n            return\n        if variable_instance.visavariable.device_property.upper() == 'vrms_chan1':\n            return self.parse_value(self.inst.query(':MEAS:ITEM? VRMS,CHAN1'))\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes the widget content instance when a WidgetContentModel is deleted", "response": "def _delete_widget_content(sender, instance, **kwargs):\n    \"\"\"\n    delete the widget content instance when a WidgetContentModel is deleted\n    \"\"\"\n    if not issubclass(sender, WidgetContentModel):\n        return\n\n    # delete WidgetContent Entry\n    wcs = WidgetContent.objects.filter(\n        content_pk=instance.pk,\n        content_model=('%s' % instance.__class__).replace(\"<class '\", '').replace(\"'>\", ''))\n    for wc in wcs:\n        logger.debug('delete wc %r'%wc)\n        wc.delete()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_widget_content(sender, instance, created=False, **kwargs):\n    if not issubclass(sender, WidgetContentModel):\n        return\n\n    # create a WidgetContent Entry\n    if created:\n        instance.create_widget_content_entry()\n        return", "response": "Create a WidgetContent instance when a WidgetContentModel is deleted"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the daq daemon configuration when changes be applied in the models", "response": "def _reinit_daq_daemons(sender, instance, **kwargs):\n    \"\"\"\n    update the daq daemon configuration when changes be applied in the models\n    \"\"\"\n    if type(instance) is Device:\n        try:\n            bp = BackgroundProcess.objects.get(pk=instance.protocol_id)\n        except:\n            return False\n        bp.restart()\n    elif type(instance) is Variable:\n        try:\n            bp = BackgroundProcess.objects.get(pk=instance.device.protocol_id)\n        except:\n            return False\n        bp.restart()\n    elif type(instance) is Scaling:\n        for bp_pk in list(instance.variable_set.all().values_list('device__protocol_id').distinct()):\n            try:\n                bp = BackgroundProcess.objects.get(pk=bp_pk)\n            except:\n                return False\n            bp.restart()\n    else:\n        logger.debug('post_save from %s' % type(instance))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_values_in_time_range(self, time_min=None, time_max=None, query_first_value=False, time_in_ms=False,\n                                 key_is_variable_name=False, add_timestamp_field=False, add_fake_data=False,\n                                 add_latest_value=True, blow_up=False, use_date_saved=False,\n                                 use_recorded_data_old=False, add_date_saved_max_field=False, **kwargs):\n        #logger.debug('%r' % [time_min, time_max])\n        if time_min is None:\n            time_min = 0\n        else:\n            db_time_min = RecordedData.objects.first()\n            if use_recorded_data_old:\n                pass  # todo\n            if db_time_min:\n                db_time_min = db_time_min.timestamp\n            else:\n                return None\n            time_min = max(db_time_min, time_min)\n        if time_max is None:\n            time_max = time.time()\n        else:\n            time_max = min(time_max, time.time())\n\n        #logger.debug('%r' % [time_min, time_max])\n        date_saved_max = time_min\n        values = {}\n        var_filter = True\n        if 'variable' in kwargs:\n            variables = Variable.objects.filter(pk=kwargs['variable'].pk)\n        elif 'variable_id' in kwargs:\n            variables = Variable.objects.filter(pk=kwargs['variable_id'])\n        elif 'variable_pk__in' in kwargs:\n            # return all values for the given variables\n            variables = Variable.objects.filter(pk__in=kwargs['variable_pk__in'])\n        elif 'variable_id__in' in kwargs:\n            # return all values for the given variables\n            variables = Variable.objects.filter(pk__in=kwargs['variable_id__in'])\n        elif 'variable__in' in kwargs:\n            # return all values for the given variables\n            variables = kwargs['variable__in']\n        else:\n            variables = Variable.objects.all()\n            var_filter = False\n\n        # export in seconds or millis\n        if time_in_ms:\n            f_time_scale = 1000\n        else:\n            f_time_scale = 1\n\n        variable_ids = variables.values_list('pk', flat=True)\n        # only filter by variable wenn less the 70% of all variables are queried\n        if len(variable_ids) > float(Variable.objects.count()) * 0.7:\n            var_filter = False\n\n        tmp_time_max = 0  # get the most recent time value\n        tmp_time_min = time.time()  #\n\n        time_slice = 60 * max(60, min(24 * 60, -3 * len(variable_ids) + 1440))\n        query_time_min = time_min\n        query_time_max = min(time_min + time_slice, time_max)\n        #logger.debug('%r'%[time_min,time_max,query_time_min,query_time_max,time_slice])\n\n        while query_time_min < time_max:\n            if use_date_saved:\n                if var_filter:\n                    tmp = list(super(RecordedDataValueManager, self).get_queryset().filter(\n                        date_saved__range=(timestamp_to_datetime(query_time_min),\n                                           timestamp_to_datetime(min(query_time_max, time_max))),\n                        variable__in=variables\n                    ).values_list('variable_id', 'pk', 'value_float64',\n                                  'value_int64', 'value_int32', 'value_int16',\n                                  'value_boolean', 'date_saved'))\n                else:\n                    tmp = list(super(RecordedDataValueManager, self).get_queryset().filter(\n                        date_saved__range=(timestamp_to_datetime(query_time_min),\n                                           timestamp_to_datetime(min(query_time_max, time_max)))\n                    ).values_list('variable_id', 'pk', 'value_float64',\n                                  'value_int64', 'value_int32', 'value_int16', 'value_boolean', 'date_saved'))\n            else:\n                if var_filter:\n                    tmp = list(super(RecordedDataValueManager, self).get_queryset().filter(\n                        id__range=(query_time_min * 2097152 * 1000, min(query_time_max * 2097152 * 1000 + 2097151,\n                                                                        time_max * 2097152 * 1000 + 2097151)),\n                        variable__in=variables\n                    ).values_list('variable_id', 'pk', 'value_float64',\n                                  'value_int64', 'value_int32', 'value_int16',\n                                  'value_boolean', 'date_saved'))\n                else:\n                    tmp = list(super(RecordedDataValueManager, self).get_queryset().filter(\n                        id__range=(query_time_min * 2097152 * 1000, min(query_time_max * 2097152 * 1000 + 2097151,\n                                                                        time_max * 2097152 * 1000 + 2097151))\n                    ).values_list('variable_id', 'pk', 'value_float64',\n                                  'value_int64', 'value_int32', 'value_int16', 'value_boolean', 'date_saved'))\n\n            for item in tmp:\n                if item[0] not in variable_ids:\n                    continue\n                if not item[0] in values:\n                    values[item[0]] = []\n                tmp_time = float(item[1] - item[0]) / (2097152.0 * 1000)  # calc the timestamp in seconds\n                tmp_time_max = max(tmp_time, tmp_time_max)\n                tmp_time_min = min(tmp_time, tmp_time_min)\n                tmp_time = tmp_time * f_time_scale\n                date_saved_max = max(date_saved_max, time.mktime(item[7].utctimetuple())+item[7].microsecond/1e6)\n                if item[2] is not None:  # float64\n                    values[item[0]].append([tmp_time, item[2]])  # time, value\n                elif item[3] is not None:  # int64\n                    values[item[0]].append([tmp_time, item[3]])  # time, value\n                elif item[4] is not None:  # int32\n                    values[item[0]].append([tmp_time, item[4]])  # time, value\n                elif item[5] is not None:  # int16\n                    values[item[0]].append([tmp_time, item[5]])  # time, value\n                elif item[6] is not None:  # boolean\n                    values[item[0]].append([tmp_time, item[6]])  # time, value\n                else:\n                    values[item[0]].append([tmp_time, 0])  # time, value\n\n            del tmp\n            query_time_min = query_time_max  # + 1\n            query_time_max = query_time_min + time_slice\n\n        update_first_value_list = []\n        timestamp_max = tmp_time_max\n        for key, item in values.items():\n            if item[-1][0] < time_max * f_time_scale:\n                if (time_max * f_time_scale) - item[-1][0] < 3610 and add_latest_value:\n                    # append last value\n                    item.append([time_max * f_time_scale, item[-1][1]])\n\n            if query_first_value and item[0][0] > time_min * f_time_scale:\n                update_first_value_list.append(key)\n\n        if query_first_value:\n            for vid in variable_ids:\n                if vid not in values.keys():\n                    update_first_value_list.append(vid)\n\n        if len(update_first_value_list) > 0:  # TODO add n times the recording interval to the range (3600 + n)\n            if use_date_saved:\n                tmp = list(super(RecordedDataValueManager, self).get_queryset().filter(\n                    use_date_saved__range=(timestamp_to_datetime(time_min - 3660), timestamp_to_datetime(time_min)),\n                    variable_id__in=update_first_value_list\n                ).values_list('variable_id', 'pk', 'value_float64',\n                              'value_int64', 'value_int32', 'value_int16',\n                              'value_boolean'))\n            else:\n                tmp = list(super(RecordedDataValueManager, self).get_queryset().filter(\n                    id__range=((time_min - 3660) * 2097152 * 1000, time_min * 2097152 * 1000),\n                    variable_id__in=update_first_value_list\n                ).values_list('variable_id', 'pk', 'value_float64',\n                              'value_int64', 'value_int32', 'value_int16',\n                              'value_boolean'))\n\n            first_values = {}\n            for item in tmp:\n                tmp_timestamp = float(item[1] - item[0]) / (2097152.0 * 1000)\n\n                if not item[0] in first_values:\n                    first_values[item[0]] = [tmp_timestamp, 0]\n\n                if tmp_timestamp >= first_values[item[0]][0]:\n                    if item[2] is not None:  # float64\n                        first_values[item[0]][1] = item[2]  # time, value\n                    elif item[3] is not None:  # int64\n                        first_values[item[0]][1] = item[3]  # time, value\n                    elif item[4] is not None:  # int32\n                        first_values[item[0]][1] = item[4]  # time, value\n                    elif item[5] is not None:  # int16\n                        first_values[item[0]][1] = item[5]  # time, value\n                    elif item[6] is not None:  # boolean\n                        first_values[item[0]][1] = item[6]  # time, value\n\n            for key in update_first_value_list:\n                if key in first_values:\n                    if key not in values:\n                        values[key] = []\n                    values[key].insert(0, [time_min * f_time_scale, first_values[key][1]])\n\n        '''\n        add a data point before the next change of state\n        '''\n        if add_fake_data:\n            for key in values:\n                i = 1\n                while i < len(values[key]):\n                    if values[key][i][0] - values[key][i - 1][0] > 1.0 and values[key][i][1] != values[key][i - 1][1]:\n                        values[key].insert(i, [values[key][i][0], values[key][i - 1][1]])\n                        i += 2\n                    else:\n                        i += 1\n        '''\n        blow up data\n        '''\n\n        if blow_up:\n            if 'mean_value_period' in kwargs:\n                mean_value_period = kwargs['mean_value_period']\n            else:\n                mean_value_period = 5.0\n            if 'no_mean_value' in kwargs:\n                no_mean_value = kwargs['no_mean_value']\n            else:\n                no_mean_value = True\n            timevalues = np.arange(np.ceil((time_min) / mean_value_period) * mean_value_period*f_time_scale,\n                                   np.floor((time_max) / mean_value_period) * mean_value_period*f_time_scale, mean_value_period*f_time_scale)\n\n            for key in values:\n                values[key] = blow_up_data(values[key], timevalues, mean_value_period*f_time_scale, no_mean_value)\n            values['timevalues'] = timevalues\n\n        '''\n        change output tuple key from pk to variable name\n        '''\n        if key_is_variable_name:\n            for item in variables:\n                if item.pk in values:\n                    values[item.name] = values.pop(item.pk)\n        '''\n        add the timestamp of the most recent value\n        '''\n        if add_timestamp_field:\n            if timestamp_max == 0:\n                timestamp_max = time_min\n            values['timestamp'] = timestamp_max * f_time_scale\n\n        if add_date_saved_max_field:\n            values['date_saved_max'] = date_saved_max * f_time_scale\n        return values", "response": "Get all values in a given time range."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a new message to the log file", "response": "def add(message, level=0, user=None, message_short=None,log_file_name='%s/pyscada_daemon.log' % settings.BASE_DIR,):\n    \"\"\"\n    add a new massage/error notice to the log\n    <0 - Debug\n    1 - Emergency\n    2 - Critical\n    3 - Errors\n    4 - Alerts\n    5 - Warnings\n    6 - Notification (webnotice)\n    7 - Information (webinfo)\n    8 - Notification (notice)\n    9 - Information (info)\n\n    \"\"\"\n    #if not access(path.dirname(self.log_file_name), W_OK):\n    #    self.stderr.write(\"logfile path is not writeable\\n\")\n    #    sys.exit(0)\n    #if access(self.log_file_name, F_OK) and not access(self.log_file_name, W_OK):\n    #    self.stderr.write(\"logfile is not writeable\\n\")\n    #    sys.exit(0)\n\n    if message_short is None:\n        message_len = len(message)\n        if message_len > 35:\n            message_short = message[0:31] + '...'\n        else:\n            message_short = message\n\n    #log_ob = Log(message=message, level=level, message_short=message_short, timestamp=time())\n    #if user:\n    #    log_ob.user = user\n    #log_ob.save()\n    stdout = open(log_file_name, \"a+\")\n    stdout.write(\"%s (%s,%d):%s\\n\" % (datetime.now().isoformat(' '),'none',level,message))\n    stdout.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the daq daemon configuration when changes be applied in the models", "response": "def _reinit_daq_daemons(sender, instance, **kwargs):\n    \"\"\"\n    update the daq daemon configuration when changes be applied in the models\n    \"\"\"\n    if type(instance) is VISADevice:\n        post_save.send_robust(sender=Device, instance=instance.visa_device)\n    elif type(instance) is VISAVariable:\n        post_save.send_robust(sender=Variable, instance=instance.visa_variable)\n    elif type(instance) is VISADeviceHandler:\n        # todo\n        pass\n    elif type(instance) is ExtendedVISAVariable:\n        post_save.send_robust(sender=Variable, instance=Variable.objects.get(pk=instance.pk))\n    elif type(instance) is ExtendedVISADevice:\n        post_save.send_robust(sender=Device, instance=Device.objects.get(pk=instance.pk))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef query_apsupsd_status():\n    import subprocess\n    output = {}\n    try:\n        apc_status = subprocess.check_output(\"/sbin/apcaccess\")\n    except:\n        return None\n    output['timestamp'] = time()\n    for line in apc_status.split('\\n'):\n        (key, spl, val) = line.partition(': ')\n        key = key.rstrip().upper()\n        val = val.strip()\n        val = val.split(' ')[0]\n        if key == 'STATUS':\n            output[key] = True if val.upper() == 'ONLINE' else False\n        elif key in ['LINEV', 'BATTV', 'BCHARGE', 'TIMELEFT', 'LOADPCT']:\n            output[key] = float(val)\n\n    return output", "response": "Query the APCAccess status of the available APCs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrequest the data from the APCUPSD.", "response": "def request_data(self):\n        \"\"\"\n        (0,'cpu_percent'),\n        (1,'virtual_memory_total'),\n        (2,'virtual_memory_available'),\n        (3,'virtual_memory_percent'),\n        (4,'virtual_memory_used'),\n        (5,'virtual_memory_free'),\n        (6,'virtual_memory_active'),\n        (7,'virtual_memory_inactive'),\n        (8,'virtual_memory_buffers'),\n        (9,'virtual_memory_cached'),\n        (10,'swap_memory_total'),\n        (11,'swap_memory_used'),\n        (12,'swap_memory_free'),\n        (13,'swap_memory_percent'),\n        (14,'swap_memory_sin'),\n        (15,'swap_memory_sout'),\n        (17,'disk_usage_systemdisk_percent'),\n        (18,'disk_usage_disk_percent'),\n        ### APCUPSD Status\n        (100, 'STATUS'), # True/False\n        (101, 'LINEV'), # Volts\n        (102, 'BATTV'), # Volts\n        (103, 'BCHARGE'), # %\n        (104, 'TIMELEFT'), # Minutes\n        (105, 'LOADPCT'), #\n\n\n\n        \"\"\"\n        if not driver_ok:\n            return None\n\n        output = []\n        apcupsd_status_is_queried = False\n        for item in self.variables:\n            timestamp = time()\n            value = None\n            if item.systemstatvariable.information == 0:\n                # cpu_percent\n                if hasattr(psutil, 'cpu_percent'):\n                    value = psutil.cpu_percent()\n                    timestamp = time()\n            elif item.systemstatvariable.information == 1:\n                # virtual_memory_total\n                if hasattr(psutil, 'virtual_memory'):\n                    value = psutil.virtual_memory().total\n                    timestamp = time()\n            elif item.systemstatvariable.information == 2:\n                # virtual_memory_available\n                if hasattr(psutil, 'virtual_memory'):\n                    value = psutil.virtual_memory().available\n                    timestamp = time()\n            elif item.systemstatvariable.information == 3:\n                # virtual_memory_percent\n                if hasattr(psutil, 'virtual_memory'):\n                    value = psutil.virtual_memory().percent\n                    timestamp = time()\n            elif item.systemstatvariable.information == 4:\n                # virtual_memory_used\n                if hasattr(psutil, 'virtual_memory'):\n                    value = psutil.virtual_memory().used\n                    timestamp = time()\n            elif item.systemstatvariable.information == 5:\n                # virtual_memory_free\n                if hasattr(psutil, 'virtual_memory'):\n                    value = psutil.virtual_memory().free\n                    timestamp = time()\n            elif item.systemstatvariable.information == 6:\n                # virtual_memory_active\n                if hasattr(psutil, 'virtual_memory'):\n                    value = psutil.virtual_memory().active\n                    timestamp = time()\n            elif item.systemstatvariable.information == 7:\n                # virtual_memory_inactive\n                if hasattr(psutil, 'virtual_memory'):\n                    value = psutil.virtual_memory().inactive\n                    timestamp = time()\n            elif item.systemstatvariable.information == 8:\n                # virtual_memory_buffers\n                if hasattr(psutil, 'virtual_memory'):\n                    value = psutil.virtual_memory().buffers\n                    timestamp = time()\n            elif item.systemstatvariable.information == 9:\n                # virtual_memory_cached\n                if hasattr(psutil, 'virtual_memory'):\n                    value = psutil.virtual_memory().cached\n                    timestamp = time()\n            elif item.systemstatvariable.information == 10:\n                # swap_memory_total\n                if hasattr(psutil, 'swap_memory'):\n                    value = psutil.swap_memory().total\n                    timestamp = time()\n            elif item.systemstatvariable.information == 11:\n                # swap_memory_used\n                if hasattr(psutil, 'swap_memory'):\n                    value = psutil.swap_memory().used\n                    timestamp = time()\n            elif item.systemstatvariable.information == 12:\n                # swap_memory_free\n                if hasattr(psutil, 'swap_memory'):\n                    value = psutil.swap_memory().free\n                    timestamp = time()\n            elif item.systemstatvariable.information == 13:\n                # swap_memory_percent\n                if hasattr(psutil, 'swap_memory'):\n                    value = psutil.swap_memory().percent\n                    timestamp = time()\n            elif item.systemstatvariable.information == 14:\n                # swap_memory_sin\n                if hasattr(psutil, 'swap_memory'):\n                    value = psutil.swap_memory().sin\n                    timestamp = time()\n            elif item.systemstatvariable.information == 15:\n                # swap_memory_sout\n                if hasattr(psutil, 'swap_memory'):\n                    value = psutil.swap_memory().sout\n                    timestamp = time()\n            elif item.systemstatvariable.information == 17:\n                # disk_usage_systemdisk_percent\n                if hasattr(psutil, 'disk_usage'):\n                    value = psutil.disk_usage('/').percent\n                    timestamp = time()\n            elif item.systemstatvariable.information == 18:\n                # disk_usage_disk_percent\n                if hasattr(psutil, 'disk_usage'):\n                    value = psutil.disk_usage(item.systemstatvariable.parameter).percent\n                    timestamp = time()\n            elif 100 <= item.systemstatvariable.information <= 105:\n                # APCUPSD Status\n                apcupsd_status = None\n                if not apcupsd_status_is_queried:\n                    apcupsd_status = query_apsupsd_status()\n                    apcupsd_status_is_queried = True\n                if apcupsd_status is not None:\n                    if item.systemstatvariable.information == 100:\n                        if 'STATUS' in apcupsd_status:\n                            value = apcupsd_status['STATUS']\n                            timestamp = apcupsd_status['timestamp']\n                    elif item.systemstatvariable.information == 101:\n                        if 'LINEV' in apcupsd_status:\n                            value = apcupsd_status['LINEV']\n                            timestamp = apcupsd_status['timestamp']\n                    elif item.systemstatvariable.information == 102:\n                        if 'BATTV' in apcupsd_status:\n                            value = apcupsd_status['BATTV']\n                            timestamp = apcupsd_status['timestamp']\n                    elif item.systemstatvariable.information == 103:\n                        if 'BCHARGE' in apcupsd_status:\n                            value = apcupsd_status['BCHARGE']\n                            timestamp = apcupsd_status['timestamp']\n                    elif item.systemstatvariable.information == 104:\n                        if 'TIMELEFT' in apcupsd_status:\n                            value = apcupsd_status['TIMELEFT']\n                            timestamp = apcupsd_status['timestamp']\n                    elif item.systemstatvariable.information == 105:\n                        if 'LOADPCT' in apcupsd_status:\n                            value = apcupsd_status['LOADPCT']\n                            timestamp = apcupsd_status['timestamp']\n            else:\n                value = None\n            # update variable\n            if value is not None and item.update_value(value, timestamp):\n                output.append(item.create_recorded_data_element())\n\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads values from the device", "response": "def read_data(self, device_property):\n        \"\"\"\n        read values from the device\n        \"\"\"\n        if self.inst is None:\n            logger.error(\"Visa-MDO3014-read data-Self.inst : None\")\n            return None\n        if device_property == 'present_value':\n            return self.parse_value(self.inst.query(':READ?'))\n        else:\n            value = self.inst.query(device_property)\n            logger.info(\"Visa-MDO3014-read data-property : %s - value : %s\" %(device_property, value))\n            return self.parse_value(value)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_data(self, variable_id, value):\n        i = 0\n        j = 0\n        while i < 10:\n            try:\n                self.inst.query('*IDN?')\n                logger.info(\"Visa-MDO3014-Write- variable_id : %s et value : %s\" %(variable_id, value))\n                i = 12\n                j = 1\n            except:\n                self.connect()\n                time.sleep(1)\n                i += 1\n                logger.error(\"MDO3014 connect error i : %s\" % i)\n        if j == 0:\n            logger.error(\"MDO3014-Instrument not connected\")\n            return None\n        if variable_id == 'init_BODE':\n            self.inst.read_termination = '\\n'\n            # Reset and init Osc\n            Vepp = RecordedData.objects.last_element(variable__name='BODE_Vpp')\n            # self.inst.write('*CLS')\n            self.inst.write('*RST;:SEL:CH1 1;:SEL:CH2 1;:HORIZONTAL:POSITION 0;:CH1:YUN \"V\";:CH1:SCALE '\n                            + str(1.2*float(Vepp.value())/(2*4))+';:CH2:YUN \"V\";:CH2:BANdwidth 10000000;:'\n                                                                 'CH1:BANdwidth 10000000;:TRIG:A:TYP EDGE;'\n                                                                 ':TRIG:A:EDGE:COUPLING DC;:TRIG:A:EDGE:SOU CH1;'\n                                                                 ':TRIG:A:EDGE:SLO FALL;:TRIG:A:MODE NORM')\n            return True\n        elif variable_id == 'find_phase':\n            # N = int(RecordedData.objects.last_element(variable__name='BODE_n').value())\n            # if N == 1:\n            #     time.sleep(1)\n            i = 0\n            F = RecordedData.objects.last_element(variable__name='BODE_F')\n            while F is None and i<10:\n                F = RecordedData.objects.last_element(variable__name='BODE_F')\n                i += 1\n            Vsff = value\n            Vsmax = float(Vsff)*float(math.sqrt(2))\n            CMD = str(':HORIZONTAL:SCALE '+str(round(float(float(4)/(float(10)*float(F.value()))), 6))+';:CH2:SCALE '\n                      + str(1.4*float(Vsmax)/4)+';:TRIG:A:LEV:CH1 '+str(0.8*float(Vsmax)) +\n                      ';:MEASUrement:IMMed:SOUrce1 CH1;:MEASUrement:IMMed:SOUrce2 CH2;:MEASUREMENT:IMMED:TYPE PHASE')\n            self.inst.write(CMD)\n            time.sleep(20/F.value())\n            CMD = str(':MEASUREMENT:IMMED:VALUE?')\n            Phase = []\n            for k in range(1, 4):\n                # TODO : put a try to prevent if the read is not a float\n                Phase.append(float(self.inst.query(CMD)))\n                time.sleep(0.1)\n            MeanPhase = np.mean(Phase)\n            StDevPhase = np.std(Phase)\n            logger.info(\"Phase : %s - Mean : %s - StDev : %s\" %(Phase, MeanPhase, StDevPhase))\n            i=0\n            while (float(MeanPhase) > 100000 or StDevPhase > 3) and i<10:\n                logger.info(\"Calculate Phase again\")\n                Phase = []\n                for k in range(1,4):\n                    Phase.append(float(self.inst.query(CMD)))\n                    time.sleep(0.1)\n                MeanPhase = np.mean(Phase)\n                StDevPhase = np.std(Phase)\n                logger.info(\"Phase : %s - Mean : %s - StDev : %s\" %(Phase,MeanPhase,StDevPhase))\n                i += 1\n                time.sleep((1+i)/10)\n            cwt = DeviceWriteTask(variable__name='Find_Gain_Osc', value=1, start=time.time())\n            cwt.save()\n#            self.write_data(\"find_gain\", 1)\n            return self.parse_value(MeanPhase)\n        elif variable_id == 'find_gain':\n            CMD = str(':MEASUrement:IMMed:SOUrce1 CH1;:MEASUREMENT:IMMED:TYPE PK2PK;:MEASUREMENT:IMMED:VALUE?')\n            P2P1 = self.inst.query(CMD)\n            try:\n                float(P2P1)\n                i = 12\n            except:\n                i = 0\n                logger.error(\"i : %s\" % i)\n            while i < 10:\n                try:\n                    P2P1 = self.inst.query('MEASUREMENT:IMMED:VALUE?')\n                    logger.info(\"P2P1 : %s\" % P2P2)\n                    float(P2P1)\n                    i =12\n                except:\n                    i += 1\n                    logger.error(\"P2P1 - i : %s\" % i)\n\n            CMD = str(':MEASUrement:IMMed:SOUrce1 CH2;:MEASUREMENT:IMMED:TYPE PK2PK;:MEASUREMENT:IMMED:VALUE?')\n            P2P2 = self.inst.query(CMD)\n            try:\n                float(P2P2)\n                i = 12\n            except:\n                i = 0\n                logger.error(\"i : %s\" %(i))\n            while i < 10:\n                try:\n                    P2P2 = self.inst.query('MEASUREMENT:IMMED:VALUE?')\n                    logger.info(\"P2P2 : %s\" % P2P2)\n                    float(P2P2)\n                    i =12\n                except:\n                    i += 1\n                    logger.error(\"P2P2 - i : %s\" % i)\n\n            try:\n                float(P2P2)\n                float(P2P1)\n            except:\n                return None\n            G = 20*log10(float(P2P2)/float(P2P1))\n            # self.inst.write('SELect:CH1 0')\n            # self.inst.write('SELect:CH2 0')\n            N = RecordedData.objects.last_element(variable__name='BODE_n').id\n            N_new = N.value() + 1\n            F = RecordedData.objects.last_element(variable__name='BODE_F').id\n            Fmin = RecordedData.objects.last_element(variable__name='BODE_Fmin').id\n            Fmax = RecordedData.objects.last_element(variable__name='BODE_Fmax').id\n            nb_points = RecordedData.objects.last_element(variable__name='BODE_nb_points').id\n            # Variation de F en log\n            k=pow(Fmax.value()/Fmin.value(),(N_new-11)/(nb_points.value()-1))\n            F_new = Fmin.value()*pow(Fmax.value()/Fmin.value(),(N_new-1)/(nb_points.value()-1))\n            F_new = int(F_new)\n#            logger.info(\"N_new : %s - k : %s - F : %s\" %(N_new, k, F_new))\n            # Variation F lin\u00e9aire\n            # F_new = F.value() + (Fmax.value() - Fmin.value())/nb_points.value()\n            if F_new > Fmax.value():\n                logger.info(\"BODE termin\u00e9\")\n            else:\n                cwt = DeviceWriteTask(variable_id=Variable.objects.get(name='BODE_F').id, value=F_new,\n                                      start=time.time())\n                cwt.save()\n                cwt = DeviceWriteTask(variable_id=Variable.objects.get(name='BODE_n').id, value=N_new,\n                                      start=time.time())\n                cwt.save()\n                cwt = DeviceWriteTask(variable_id=Variable.objects.get(name='Set_Freq_GBF').id, value=F_new,\n                                      start=time.time())\n                cwt.save()\n            return self.parse_value(G)\n        return self.parse_value(self.inst.query(str(variable_id)+' '+str(value)))", "response": "write the value to the device"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_cache_data(request):\n    if 'init' in request.POST:\n        init = bool(float(request.POST['init']))\n    else:\n        init = False\n    active_variables = []\n    if 'variables[]' in request.POST:\n        active_variables = request.POST.getlist('variables[]')\n    \"\"\"\n    else:\n        active_variables = list(\n            GroupDisplayPermission.objects.filter(hmi_group__in=request.user.groups.iterator()).values_list(\n                'charts__variables', flat=True))\n        active_variables += list(\n            GroupDisplayPermission.objects.filter(hmi_group__in=request.user.groups.iterator()).values_list(\n                'xy_charts__variables', flat=True))\n        active_variables += list(\n            GroupDisplayPermission.objects.filter(hmi_group__in=request.user.groups.iterator()).values_list(\n                'control_items__variable', flat=True))\n        active_variables += list(\n            GroupDisplayPermission.objects.filter(hmi_group__in=request.user.groups.iterator()).values_list(\n                'custom_html_panels__variables', flat=True))\n        active_variables = list(set(active_variables))\n    \"\"\"\n\n    active_variable_properties = []\n    if 'variable_properties[]' in request.POST:\n        active_variable_properties = request.POST.getlist('variable_properties[]')\n\n    timestamp_from = time.time()\n    if 'timestamp_from' in request.POST:\n        timestamp_from = float(request.POST['timestamp_from']) / 1000.0\n\n    timestamp_to = time.time()\n\n    if 'timestamp_to' in request.POST:\n        timestamp_to = min(timestamp_to, float(request.POST['timestamp_to']) / 1000.0)\n\n    if timestamp_to == 0:\n        timestamp_to = time.time()\n\n    if timestamp_from == 0:\n        timestamp_from == time.time() - 60\n\n    if timestamp_to - timestamp_from > 120 * 60:\n        timestamp_from = timestamp_to - 120 * 60\n\n    #if not init:\n        #timestamp_to = min(timestamp_from + 30, timestamp_to)\n\n    if len(active_variables) > 0:\n        data = RecordedData.objects.db_data(\n            variable_ids=active_variables,\n            time_min=timestamp_from,\n            time_max=timestamp_to,\n            time_in_ms=True,\n            query_first_value=init)\n    else:\n        data = None\n\n    if data is None:\n        data = {}\n\n    data['variable_properties'] = {}\n\n    for item in VariableProperty.objects.filter(pk__in=active_variable_properties):\n        data['variable_properties'][item.pk] = item.value()\n\n    data[\"server_time\"] = time.time() * 1000\n    return HttpResponse(json.dumps(data), content_type='application/json')", "response": "Get the cache data from the POST data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading values from the device", "response": "def read_data(self,variable_instance):\n        \"\"\"\n        read values from the device\n        \"\"\"\n        if self.inst is None:\n             return\n        vp_func = variable_instance.variableproperty_set.filter(name=':FUNC').first()\n        measure_function = ''\n        if vp_func:\n            if vp_func.value():\n                measure_function = ':FUNC \"%s\";'%vp_func.value()\n\n        trig_delay = 0.1\n        if variable_instance.visavariable.device_property.upper() == 'PRESENT_VALUE':\n            return self.parse_value(self.inst.query(':FETCH?'))\n        m = re.search('(PRESENT_VALUE_CH)([0-9]*)', variable_instance.visavariable.device_property.upper())\n        if m:\n            return self.parse_value(\n                        self.inst.query(':route:close (@%s);%s:TRIG:DEL %1.3f;:fetch?'%(m.group(2),measure_function,trig_delay)))\n        return self.parse_value(self.inst.query(variable_instance.visavariable.device_property.upper()))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_data(self, variable_id, value, task):\n        variable = self._variables[variable_id]\n        if task.property_name != '':\n            # write the property to VariableProperty use that for later read\n            vp = VariableProperty.objects.update_or_create_property(variable=variable,\n                                                                    name='VISA:%s' % task.property_name.upper(),\n                                                                    value=value, value_class='FLOAT64')\n            return True\n        return False\n        i = 0\n        j = 0\n        while i < 10:\n            try:\n                self.inst.read_termination = '\\n'\n                self.inst.query('*IDN?')\n                i = 12\n                j = 1\n            except:\n                self.connect()\n                time.sleep(1)\n                i += 1\n                logger.error(\"Keithley connect error i : %s\" %i)\n        if j == 0:\n            logger.error(\"Keithley-Instrument not connected\")\n            return False\n\n        # if variable_id == 'present_value':\n        if task.variable.visavariable.device_property.upper() == 'PRESENT_VALUE':\n            i = 0\n            while i < 10:\n                Vseff = \"\"\n                try:\n                    Vseff = self.parse_value(self.inst.query(':READ?'))\n                except:\n                    Vseff = \"\"\n                if Vseff is None or Vseff is \"\":\n                    i += 1\n                    logger.error(\"Keithley - Error Read - i : %s\" %i)\n                    self.inst.write('*CLS')\n                else:\n                    i = 12\n                    # Call Phase Osc\n                    # cwt = DeviceWriteTask(variable_id=Variable.objects.get(name='Find_Phase_Osc').id, value=Vseff, start=time.time())\n                    # cwt.save()\n                    logger.info(\"Variable %s - task.property_name : %s - value %s\" %(variable, task.property_name.upper(), value))\n                    vp = VariableProperty.objects.update_or_create_property(variable=variable,\n                                                                   name='VISA:%s' % task.property_name.upper(),\n                                                                   value=value, value_class='FLOAT64')\n                    #vp = VariableProperty.objects.update_or_create_property(variable=variable,\n                    #                                               name='VISA:%s' % task.property_name.upper())\n            return Vseff\n\n        if variable_instance.visavariable.device_property.upper() == 'SET_AC_RANGE_RES':\n        # if variable_id == 'set_ac_range_res':\n            CMD = str('*RST;:FUNC \"VOLTage:AC\";:VOLTage:AC:RANGe:AUTO 1;:VOLTage:AC:RESolution MIN;:TRIG:DEL MIN')\n            self.inst.write(CMD)\n            return True\n        else:\n            logger.error(\"Keithley - variable_id : %s\" %variable_id)\n            return self.parse_value(self.inst.query(str(variable_id)+' '+str(value)))", "response": "write data to the device"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ups_pico(smbus_device,info):\n    if info == 'ad1':\n        data = smbus_device.read_word_data(0x69, 0x05)\n        data = format(data,\"02x\")\n        return (float(data) / 100) # \u00b0C\n    \n    if info == 'ad2':\n        data = smbus_device.read_word_data(0x69, 0x07)\n        data = format(data,\"02x\")\n        return (float(data) / 100) # \u00b0C\n    \n    if info == 'rpi_level':\n        data = smbus_device.read_word_data(0x69, 0x03)\n        data = format(data,\"02x\")\n        return (float(data) / 100) # Volt\n    \n    if info == 'bat_level':\n        data = smbus_device.read_word_data(0x69, 0x01)\n        data = format(data,\"02x\")\n        return (float(data) / 100) # Volt\n    \n    if info == 'pwr_mode':\n        data = smbus_device.read_word_data(0x69, 0x00)\n        return data & ~(1 << 7) # 1 RPi, 2 Bat\n    \n    if info == 'sot23_temp':\n        data = smbus_device.read_byte_data(0x69, 0x0C)\n        data = format(data,\"02x\")\n        return data\n\n    if info == 'to92_temp':\n        data = smbus_device.read_byte_data(0x69, 0x0D)\n        data = format(data,\"02x\")\n        return data\n\n    return None", "response": "Query the data via smbus via UPS Pico"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlimiting Pages to those that belong to the request s user.", "response": "def get_queryset(self, request):\n        \"\"\"Limit Pages to those that belong to the request's user.\"\"\"\n        qs = super(VISADeviceAdmin, self).get_queryset(request)\n        return qs.filter(protocol_id=PROTOCOL_ID)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlimiting Pages to those that belong to the request s user.", "response": "def get_queryset(self, request):\n        \"\"\"Limit Pages to those that belong to the request's user.\"\"\"\n        qs = super(VISAVariableAdmin, self).get_queryset(request)\n        return qs.filter(device__protocol_id=PROTOCOL_ID)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef export_recordeddata_to_file(time_min=None, time_max=None, filename=None, active_vars=None, file_extension=None,\n                                append_to_file=False, no_mean_value=False, mean_value_period=5.0,\n                                backgroundprocess_id=None, export_task_id=None, **kwargs):\n    \"\"\"\n    read all data\n    \"\"\"\n    if backgroundprocess_id is not None:\n        tp = BackgroundProcess.objects.get(id=backgroundprocess_id)\n        tp.message = 'init'\n        tp.last_update = now()\n        tp.save()\n    else:\n        tp = None\n\n    if isinstance(time_max, string_types):\n        # convert date strings\n        time_max = mktime(datetime.strptime(time_max, \"%d-%b-%Y %H:%M:%S\").timetuple())\n    if isinstance(time_min, string_types):\n        # convert date strings\n        time_min = mktime(datetime.strptime(time_min, \"%d-%b-%Y %H:%M:%S\").timetuple())\n\n    # add default time_min\n    if time_max is None:\n        time_max = time()  # now\n    if time_min is None:\n        time_min = time() - 24 * 60 * 60  # last 24 hours\n\n    # add default extension if no extension is given\n    if file_extension is None and filename is None:\n        file_extension = '.h5'\n    elif filename is not None:\n        file_extension = '.' + filename.split('.')[-1]\n    # validate file type\n    if file_extension not in ['.h5', '.mat', '.csv']:\n        if tp is not None:\n            tp.last_update = now()\n            tp.message = 'failed wrong file type'\n            tp.failed = 1\n            tp.save()\n        return\n\n    # \n    if filename is None:\n        if hasattr(settings, 'PYSCADA_EXPORT'):\n            if 'output_folder' in settings.PYSCADA_EXPORT:\n                backup_file_path = os.path.expanduser(settings.PYSCADA_EXPORT['output_folder'])\n            else:\n                backup_file_path = os.path.expanduser('~/measurement_data_dumps')\n        else:\n            backup_file_path = os.path.expanduser('~/measurement_data_dumps')\n\n        # add filename prefix\n        backup_file_name = 'measurement_data'\n        if hasattr(settings, 'PYSCADA_EXPORT'):\n            if 'file_prefix' in settings.PYSCADA_EXPORT:\n                backup_file_name = settings.PYSCADA_EXPORT['file_prefix'] + backup_file_name\n        # create output dir if not existing\n        if not os.path.exists(backup_file_path):\n            os.mkdir(backup_file_path)\n\n        # validate time values\n        db_time_min = RecordedData.objects.first()  # todo add RecordedDataOld\n        if not db_time_min:\n            if tp is not None:\n                tp.last_update = now()\n                tp.message = 'no data to export'\n                tp.failed = 1\n                tp.save()\n            return\n        time_min = max(db_time_min.time_value(), time_min)\n\n        db_time_max = RecordedData.objects.last()  # todo add RecordedDataOld\n        if not db_time_max:\n            if tp is not None:\n                tp.last_update = now()\n                tp.message = 'no data to export'\n                tp.failed = 1\n                tp.save()\n            return\n        time_max = min(db_time_max.time_value(), time_max)\n\n        # filename  and suffix\n        cdstr_from = datetime.fromtimestamp(time_min).strftime(\"%Y_%m_%d_%H%M\")\n        cdstr_to = datetime.fromtimestamp(time_max).strftime(\"%Y_%m_%d_%H%M\")\n\n        if 'filename_suffix' in kwargs:\n            filename = os.path.join(backup_file_path,\n                                    backup_file_name + '_' + cdstr_from + '_' + cdstr_to + '_' + kwargs[\n                                        'filename_suffix'])\n        else:\n            filename = os.path.join(backup_file_path, backup_file_name + '_' + cdstr_from + '_' + cdstr_to)\n\n    # check if file exists\n    if os.path.exists(filename + file_extension) and not append_to_file:\n        count = 0\n        filename_old = filename\n        while os.path.exists(filename + file_extension):\n            filename = filename_old + '_%03.0f' % count\n            count += 1\n\n    # append the extension \n    filename = filename + file_extension\n\n    # add Filename to ExportTask\n    if export_task_id is not None:\n        job = ExportTask.objects.filter(pk=export_task_id).first()\n        if job:\n            job.filename = filename\n            job.save()\n\n    # \n    if active_vars is None:\n        active_vars = Variable.objects.filter(active=1, device__active=1)\n    else:\n        if type(active_vars) is str:\n            if active_vars == 'all':\n                active_vars = Variable.objects.all()\n            else:\n                active_vars = Variable.objects.filter(active=1, device__active=1)\n        else:\n            active_vars = Variable.objects.filter(pk__in=active_vars, active=1, device__active=1)\n\n    if mean_value_period == 0:\n        no_mean_value = True\n        mean_value_period = 5.0  # todo get from DB, default is 5 seconds\n\n    # calculate time vector\n\n    timevalues = arange(math.ceil(time_min / mean_value_period) * mean_value_period,\n                        math.floor(time_max / mean_value_period) * mean_value_period, mean_value_period)\n\n    # get Meta from Settings\n    if hasattr(settings, 'PYSCADA_META'):\n        if 'description' in settings.PYSCADA_META:\n            description = settings.PYSCADA_META['description']\n        else:\n            description = 'None'\n        if 'name' in settings.PYSCADA_META:\n            name = settings.PYSCADA_META['name']\n        else:\n            name = 'None'\n    else:\n        description = 'None'\n        name = 'None'\n\n    if file_extension in ['.h5', '.mat']:\n        bf = MatCompatibleH5(filename, version='1.1', description=description, name=name,\n                             creation_date=strftime('%d-%b-%Y %H:%M:%S'))\n        out_timevalues = [unix_time_stamp_to_matlab_datenum(element) for element in timevalues]\n    elif file_extension in ['.csv']:\n        bf = ExcelCompatibleCSV(filename, version='1.1', description=description, name=name,\n                                creation_date=strftime('%d-%b-%Y %H:%M:%S'))\n        out_timevalues = [unix_time_stamp_to_excel_datenum(element) for element in timevalues]\n    else:\n        return\n\n    # less than 24\n    # read everything\n    bf.write_data('time', float64(out_timevalues),\n                  id=0,\n                  description=\"global time vector\",\n                  value_class=validate_value_class('FLOAT64'),\n                  unit=\"Days since 0000-1-1 00:00:00\",\n                  color='#000000',\n                  short_name='time',\n                  chart_line_thickness=3\n                  )\n\n    for var_idx in range(0, active_vars.count(), 10):\n        if tp is not None:\n            tp.last_update = now()\n            tp.message = 'reading values from database (%d)' % var_idx\n            tp.save()\n        # query data\n        var_slice = active_vars[var_idx:var_idx + 10]\n        data = RecordedData.objects.get_values_in_time_range(\n            variable_id__in=list(var_slice.values_list('pk', flat=True)),\n            time_min=time_min,\n            time_max=time_max,\n            query_first_value=True)\n\n        for var in var_slice:\n            # write background task info\n            if tp is not None:\n                tp.last_update = now()\n                tp.message = 'writing values for %s (%d) to file' % (var.name, var.pk)\n                tp.save()\n            # check if variable is scalled \n            if var.scaling is None or var.value_class.upper() in ['BOOL', 'BOOLEAN']:\n                value_class = var.value_class\n            else:\n                value_class = 'FLOAT64'\n            # read unit \n            if hasattr(var.unit, 'udunit'):\n                udunit = var.unit.udunit\n            else:\n                udunit = 'None'\n\n            if var.pk not in data:\n                # write dummy data\n                bf.write_data(var.name, _cast_value([0] * len(timevalues), validate_value_class(value_class)),\n                              id=var.pk,\n                              description=var.description,\n                              value_class=validate_value_class(value_class),\n                              unit=udunit,\n                              color=var.chart_line_color_code(),\n                              short_name=var.short_name,\n                              chart_line_thickness=var.chart_line_thickness)\n                continue\n\n            out_data = np.zeros(len(timevalues))\n            # i                            # time data index\n            ii = 0  # source data index\n            # calculate mean values\n            last_value = None\n            max_ii = len(data[var.pk]) - 1\n            for i in range(len(timevalues)):  # iter over time values\n\n                if ii >= max_ii + 1:\n                    # if not more data in data source break\n                    if last_value is not None:\n                        out_data[i] = last_value\n                        continue\n                # init mean value vars\n                tmp = 0.0  # sum\n                tmp_i = 0.0  # count\n\n                if data[var.pk][ii][0] < timevalues[i]:\n                    # skip elements that are befor current time step\n                    while data[var.pk][ii][0] < timevalues[i] and ii < max_ii:\n                        last_value = data[var.pk][ii][1]\n                        ii += 1\n\n                if ii >= max_ii:\n                    if last_value is not None:\n                        out_data[i] = last_value\n                        continue\n                # calc mean value\n                if timevalues[i] <= data[var.pk][ii][0] < timevalues[i] + mean_value_period:\n                    # there is data in time range\n                    while timevalues[i] <= data[var.pk][ii][0] < timevalues[i] + mean_value_period and ii < max_ii:\n                        # calculate mean value\n                        if no_mean_value:\n                            tmp = data[var.pk][ii][1]\n                            tmp_i = 1\n                        else:\n                            tmp += data[var.pk][ii][1]\n                            tmp_i += 1\n\n                        last_value = data[var.pk][ii][1]\n                        ii += 1\n                    # calc and store mean value\n                    if tmp_i > 0:\n                        out_data[i] = tmp / tmp_i\n                    else:\n                        out_data[i] = data[var.pk][ii][1]\n                        last_value = data[var.pk][ii][1]\n                else:\n                    # there is no data in time range, keep last value, not mean value\n                    if last_value is not None:\n                        out_data[i] = last_value\n\n            # write data\n            bf.write_data(var.name, _cast_value(out_data, validate_value_class(value_class)),\n                          id=var.pk,\n                          description=var.description,\n                          value_class=validate_value_class(value_class),\n                          unit=udunit,\n                          color=var.chart_line_color_code(),\n                          short_name=var.short_name,\n                          chart_line_thickness=var.chart_line_thickness)\n\n    bf.close_file()\n    if tp is not None:\n        tp.last_update = now()\n        tp.message = 'done'\n        tp.done = True\n        tp.save()", "response": "export all data to a file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _cast_value(value, _type):\n    if _type.upper() == 'FLOAT64':\n        return float64(value)\n    elif _type.upper() == 'FLOAT32':\n        return float32(value)\n    elif _type.upper() == 'INT32':\n        return int32(value)\n    elif _type.upper() == 'UINT16':\n        return uint16(value)\n    elif _type.upper() == 'INT16':\n        return int16(value)\n    elif _type.upper() == 'BOOLEAN':\n        return uint8(value)\n    else:\n        return float64(value)", "response": "cast value to _type\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_data(self, variable_id, value):\n        i = 0\n        j = 0\n        while i < 10:\n            try:\n                self.inst.query('*IDN?')\n                # logger.info(\"Visa-AFG1022-Write-variable_id : %s et value : %s\" %(variable_id, value))\n                i = 12\n                j = 1\n            except:\n                self.connect()\n                time.sleep(1)\n                i += 1\n                logger.error(\"AFG1022 connect error i : %s\" %i)\n        if j == 0:\n            logger.error(\"AFG1022-Instrument not connected\")\n            return None\n        if variable_id == 'init_BODE':\n\n            # N\n            try:\n                N = int(RecordedData.objects.last_element(variable_id=Variable.objects.get(name='BODE_n').id).value())\n            except:\n                N = 0\n                logger.error('AFG1022 cannot load N')\n            if N == 0:\n                # Set N to 1\n                cwt = DeviceWriteTask(variable_id=Variable.objects.get(name='BODE_n').id, value=1, start=time.time())\n                cwt.save()\n                # ReCall init GBF\n                cwt = DeviceWriteTask(variable_id=Variable.objects.get(name='Init_BODE_GBF').id, value=1, start=time.time())\n                cwt.save()\n                return None\n            elif N == 1:\n                self.inst.read_termination = '\\n'\n                # R\u00e9cup de Ve\n                Vepp = RecordedData.objects.last_element(variable_id=Variable.objects.get(name='BODE_Vpp').id).value()\n                # Fmin\n                Fmin = RecordedData.objects.last_element(variable_id=Variable.objects.get(name='BODE_Fmin').id).value()\n                # Call Range MM\n                cwt = DeviceWriteTask(variable_id=Variable.objects.get(name='Set_AC_Range_and_Resolution_and_Measure_MM').id, value=Vepp, start=time.time())\n                cwt.save()\n                # Call Init Osc\n                cwt = DeviceWriteTask(variable_id=Variable.objects.get(name='Init_BODE_Osc').id, value=1, start=time.time())\n                cwt.save()\n                # Reset GBF\n                CMD = str('*RST;OUTPut1:STATe ON;OUTP1:IMP MAX;SOUR1:AM:STAT OFF;SOUR1:FUNC:SHAP SIN;SOUR1:VOLT:LEV:IMM:AMPL '+str(Vepp)+'Vpp')\n                self.inst.write(CMD)\n#                self.inst.write('*CLS')\n                # Set F value\n                cwt = DeviceWriteTask(variable_id=Variable.objects.get(name='BODE_F').id, value=Fmin, start=time.time())\n                cwt.save()\n                # Call  Set Freq GBF\n                # cwt = DeviceWriteTask(variable_id=Variable.objects.get(name='Set_Freq_GBF').id, value=Fmin, start=time.time())\n                # cwt.save()\n                self.write_data(\"set_freq\", Fmin)\n                return True\n            else:\n                cwt = DeviceWriteTask(variable_id=Variable.objects.get(name='Init_BODE_GBF').id, value=1, start=time.time())\n                cwt.save()\n                logger.info(\"Init GBF - N : %s\" %N)\n                return False\n            return None\n        elif variable_id == 'set_freq':\n            # Define Freq\n            self.inst.write('SOUR1:FREQ:FIX '+str(value))\n            # Call Read MM\n            cwt = DeviceWriteTask(variable_id=Variable.objects.get(name='Read_MM_acual_value').id, value=1, start=time.time())\n            cwt.save()\n            return self.parse_value(value)\n        elif variable_id == 'set_tension':\n            # Define tension\n            self.inst.write('SOUR1:VOLT:LEV:IMM:AMPL '+str(value)+'Vpp')\n            # F = Fmin\n            F = RecordedData.objects.last_element(variable_id=Variable.objects.get(name='BODE_Fmin').id).value()\n            # Set F value\n            cwt = DeviceWriteTask(variable_id=Variable.objects.get(name='BODE_F').id, value=F, start=time.time())\n            cwt.save()\n            # Call  Set Freq GBF\n            cwt = DeviceWriteTask(variable_id=Variable.objects.get(name='Set_Freq_GBF').id, value=F, start=time.time())\n            cwt.save()\n            return self.parse_value(value)\n        elif variable_id == 'return_value':\n            return self.parse_value(value)\n        elif variable_id == 'reboot':\n            import os\n            os.system('sudo reboot')\n            return 1\n        else:\n            return self.parse_value(self.inst.query(str(variable_id)+' '+str(value)))\n        return None", "response": "Writes the value to the device"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nloop through all the mails and send them", "response": "def loop(self):\n        \"\"\"\n        check for mails and send them\n        \"\"\"\n        for mail in Mail.objects.filter(done=False, send_fail_count__lt=3):\n            # send all emails that are not already send or failed to send less\n            # then three times\n            mail.send_mail()\n\n        for mail in Mail.objects.filter(done=True, timestamp__lt=time() - 60 * 60 * 24 * 7):\n            # delete all done emails older then one week\n            mail.delete()\n        return 1, None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nnormalizing input list. Parameters ---------- l: list The list to be normalized Returns ------- The normalized list or numpy array Raises ------ ValueError, if the list sums to zero", "response": "def normalize(l):\n    \"\"\"\n    Normalizes input list.\n\n    Parameters\n    ----------\n    l: list\n        The list to be normalized\n\n    Returns\n    -------\n    The normalized list or numpy array\n\n    Raises\n    ------\n    ValueError, if the list sums to zero\n    \"\"\"\n\n    s = float(sum(l))\n    if s == 0:\n        raise ValueError(\"Cannot normalize list with sum 0\")\n    return [x / s for x in l]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef permute_point(p, permutation=None):\n    if not permutation:\n        return p\n    return [p[int(permutation[i])] for i in range(len(p))]", "response": "Permutes the point according to the permutation keyword argument."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprojects a point in a 3 - tuple to planar simplex.", "response": "def project_point(p, permutation=None):\n    \"\"\"\n    Maps (x,y,z) coordinates to planar simplex.\n\n    Parameters\n    ----------\n    p: 3-tuple\n        The point to be projected p = (x, y, z)\n    permutation: string, None, equivalent to \"012\"\n        The order of the coordinates, counterclockwise from the origin\n    \"\"\"\n    permuted = permute_point(p, permutation=permutation)\n    a = permuted[0]\n    b = permuted[1]\n    x = a + b/2.\n    y = SQRT3OVER2 * b\n    return numpy.array([x, y])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef project_sequence(s, permutation=None):\n\n    xs, ys = unzip([project_point(p, permutation=permutation) for p in s])\n    return xs, ys", "response": "Projects a point or sequence of points using project_point to lists xs ys for plotting with Matplotlib."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a 3 - tuple in data coordinates into simplex coordinates for plotting.", "response": "def convert_coordinates(q, conversion, axisorder):\n    \"\"\"\n    Convert a 3-tuple in data coordinates into to simplex data\n    coordinates for plotting.\n\n    Parameters\n    ----------\n    q: 3-tuple\n       the point to be plotted in data coordinates\n\n    conversion: dict\n        keys = ['b','l','r']\n        values = lambda function giving the conversion\n    axisorder: String giving the order of the axes for the coordinate tuple\n        e.g. 'blr' for bottom, left, right coordinates.\n\n    Returns\n    -------\n    p: 3-tuple\n       The point converted to simplex coordinates.\n    \"\"\"\n    p = []\n    for k in range(3):\n        p.append(conversion[axisorder[k]](q[k]))\n\n    return tuple(p)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the conversion equations for each axis.", "response": "def get_conversion(scale, limits):\n    \"\"\"\n    Get the conversion equations for each axis.\n\n    limits: dict of min and max values for the axes in the order blr.\n    \"\"\"\n    fb = float(scale) / float(limits['b'][1] - limits['b'][0])\n    fl = float(scale) / float(limits['l'][1] - limits['l'][0])\n    fr = float(scale) / float(limits['r'][1] - limits['r'][0])\n\n    conversion = {\"b\": lambda x: (x - limits['b'][0]) * fb,\n                  \"l\": lambda x: (x - limits['l'][0]) * fl,\n                  \"r\": lambda x: (x - limits['r'][0]) * fr}\n\n    return conversion"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts 3 - tuples in data coordinates and return a list of 3 - tuples converted to simplex coordinates.", "response": "def convert_coordinates_sequence(qs, scale, limits, axisorder):\n    \"\"\"\n    Take a sequence of 3-tuples in data coordinates and convert them\n    to simplex coordinates for plotting. This is needed for custom\n    plots where the scale of the simplex axes is set within limits rather\n    than being defined by the scale parameter.\n\n    Parameters\n    ----------\n    qs, sequence of 3-tuples\n       The points to be plotted in data coordinates.\n\n    scale: int\n        The scale parameter for the plot.\n    limits: dict\n        keys = ['b','l','r']\n        values = min,max data values for this axis.\n    axisorder: String giving the order of the axes for the coordinate tuple\n        e.g. 'blr' for bottom, left, right coordinates.\n\n    Returns\n    -------\n    s, list of 3-tuples\n       the points converted to simplex coordinates\n    \"\"\"\n    conversion = get_conversion(scale, limits)\n    \n    return [convert_coordinates(q, conversion, axisorder) for q in qs]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking sure the drawing surface is large enough to display projected content. Parameters ---------- ax: Matplotlib AxesSubplot, None The subplot to draw on. scale: float, 1.0 Simplex scale size.", "response": "def resize_drawing_canvas(ax, scale=1.):\n    \"\"\"\n    Makes sure the drawing surface is large enough to display projected\n    content.\n\n    Parameters\n    ----------\n    ax: Matplotlib AxesSubplot, None\n        The subplot to draw on.\n    scale: float, 1.0\n        Simplex scale size.\n    \"\"\"\n    ax.set_ylim((-0.10 * scale, .90 * scale))\n    ax.set_xlim((-0.05 * scale, 1.05 * scale))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clear_matplotlib_ticks(ax=None, axis=\"both\"):\n    if not ax:\n        return\n    if axis.lower() in [\"both\", \"x\", \"horizontal\"]:\n        ax.set_xticks([], [])\n    if axis.lower() in [\"both\", \"y\", \"vertical\"]:\n        ax.set_yticks([], [])", "response": "Clears the matplotlib ticks for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nplotting a single trajectory with changing color simlar to plot.", "response": "def plot_colored_trajectory(points, cmap=None, ax=None, permutation=None,\n                            **kwargs):\n    \"\"\"\n    Plots trajectories with changing color, simlar to `plot`. Trajectory points\n    are tuples (x,y,z) satisfying x + y + z = scale (not checked). The tuples are\n    projected and plotted as a curve.\n\n    Parameters\n    ----------\n    points: List of 3-tuples\n        The list of tuples to be plotted as a connected curve.\n    ax: Matplotlib AxesSubplot, None\n        The subplot to draw on.\n    cmap: String or matplotlib.colors.Colormap, None\n        The name of the Matplotlib colormap to use.\n    kwargs:\n        Any kwargs to pass through to matplotlib.\n    \"\"\"\n    if not ax:\n        fig, ax = pyplot.subplots()\n    cmap = get_cmap(cmap)\n    xs, ys = project_sequence(points, permutation=permutation)\n\n    # We want to color each segment independently...which is annoying.\n    segments = []\n    for i in range(len(xs) - 1):\n        cur_line = []\n        x_before = xs[i]\n        y_before = ys[i]\n        x_after = xs[i+1]\n        y_after = ys[i+1]\n\n        cur_line.append([x_before, y_before])\n        cur_line.append([x_after, y_after])\n        segments.append(cur_line)\n    segments = np.array(segments)\n\n    line_segments = matplotlib.collections.LineCollection(segments, cmap=cmap, **kwargs)\n    line_segments.set_array(np.arange(len(segments)))\n    ax.add_collection(line_segments)\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef scatter(points, ax=None, permutation=None, colorbar=False, colormap=None,\n            vmin=0, vmax=1, scientific=False, cbarlabel=None, cb_kwargs=None,\n            **kwargs):\n    \"\"\"\n    Plots trajectory points where each point satisfies x + y + z = scale.\n    First argument is a list or numpy array of tuples of length 3.\n\n    Parameters\n    ----------\n    points: List of 3-tuples\n        The list of tuples to be scatter-plotted.\n    ax: Matplotlib AxesSubplot, None\n        The subplot to draw on.\n    colorbar: bool, False\n        Show colorbar.\n    colormap: String or matplotlib.colors.Colormap, None\n        The name of the Matplotlib colormap to use.\n    vmin: int, 0\n        Minimum value for colorbar.\n    vmax: int, 1\n        Maximum value for colorbar.\n    cb_kwargs: dict\n        Any additional kwargs to pass to colorbar\n    kwargs:\n        Any kwargs to pass through to matplotlib.\n    \"\"\"\n    if not ax:\n        fig, ax = pyplot.subplots()\n    xs, ys = project_sequence(points, permutation=permutation)\n    ax.scatter(xs, ys, vmin=vmin, vmax=vmax, **kwargs)\n\n    if colorbar and (colormap != None):\n        if cb_kwargs != None:\n            colorbar_hack(ax, vmin, vmax, colormap, scientific=scientific,\n                          cbarlabel=cbarlabel, **cb_kwargs)\n        else:\n            colorbar_hack(ax, vmin, vmax, colormap, scientific=scientific,\n                          cbarlabel=cbarlabel)\n\n    return ax", "response": "Plots a scatter plot of the trajectory points."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _en_to_enth(energy,concs,A,B,C):\n\n    enth = abs(energy - concs[0]*A -concs[1]*B -concs[2]*C)\n    return enth", "response": "Converts an energy to an enthalpy."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _energy_to_enthalpy(energy):\n    \n    pureA = [energy[0][0],energy[0][1]]\n    pureB = [energy[1][0],energy[1][1]]\n    pureC = [energy[2][0],energy[2][1]]\n\n    enthalpy = []\n    for en in energy:\n        c = en[2]\n        conc = [float(i)/sum(c) for i in c]\n\n        CE = _en_to_enth(en[0],conc,pureA[0],pureB[0],pureC[0])\n        VASP = _en_to_enth(en[1],conc,pureA[1],pureB[1],pureC[1])\n\n        enthalpy.append([CE,VASP,c])\n\n    return enthalpy", "response": "Converts the energy array to the formation enthalpy."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _find_error(vals):\n\n    err_vals = []\n    for en in vals:\n        c = en[2]\n        conc = [float(i)/sum(c) for i in c]\n\n        err = abs(en[0]-en[1])\n\n        err_vals.append([conc,err])\n\n    return err_vals", "response": "Find the errors in the energy values."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the data in fname into a list where each list entry contains [ predicted energy calculated energy list of concentrations.", "response": "def _read_data(fname):\n    \"\"\"Reads data from file.\n\n    Reads the data in 'fname' into a list where each list entry contains \n    [energy predicted, energy calculated, list of concentrations].\n\n    Parameters\n    ----------\n    fname : str\n        The name and path to the data file.\n    \n    Returns\n    -------\n    energy : list of lists of floats\n       A list of the energies and the concentrations.\n    \"\"\"\n    \n    energy = []\n    with open(fname,'r') as f:\n        for line in f:\n            CE = abs(float(line.strip().split()[0]))\n            VASP = abs(float(line.strip().split()[1]))\n            conc = [i for i in line.strip().split()[2:]]\n\n            conc_f = []\n            for c in conc:\n                if '[' in c and ']' in c:\n                    conc_f.append(int(c[1:-1]))\n                elif '[' in c:\n                    conc_f.append(int(c[1:-1]))\n                elif ']' in c or ',' in c:\n                    conc_f.append(int(c[:-1]))\n                else:\n                    conc_f.append(int(c))\n            energy.append([CE,VASP,conc_f])\n    return energy"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef conc_err_plot(fname):\n\n    energies = _read_data(fname)\n    enthalpy = _energy_to_enthalpy(energies)\n    this_errors = _find_error(enthalpy)\n\n    points = []\n    colors = []\n    for er in this_errors:\n        concs = er[0]\n        points.append((concs[0]*100,concs[1]*100,concs[2]*100))\n        colors.append(er[1])\n    \n    scale = 100\n    figure, tax = ternary.figure(scale=scale)\n    tax.boundary(linewidth = 1.0)\n    tax.set_title(\"Errors in Convex Hull Predictions.\",fontsize=20)\n    tax.gridlines(multiple=10,color=\"blue\")\n    tax.scatter(points,vmax=max(colors),colormap=plt.cm.viridis,colorbar=True,c=colors,cmap=plt.cm.viridis)\n\n    tax.show()", "response": "Plots the error in the CE data within a ternary concentration diagram."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef figure(ax=None, scale=None, permutation=None):\n\n    ternary_ax = TernaryAxesSubplot(ax=ax, scale=scale, permutation=permutation)\n    return ternary_ax.get_figure(), ternary_ax", "response": "Wraps a Matplotlib AxesSubplot or generates a new one."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _connect_callbacks(self):\n        figure = self.get_figure()\n        callback = partial(mpl_redraw_callback, tax=self)\n        event_names = ('resize_event', 'draw_event')\n        for event_name in event_names:\n            figure.canvas.mpl_connect(event_name, callback)", "response": "Connect resize matplotlib callbacks."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the title on the underlying matplotlib AxesSubplot.", "response": "def set_title(self, title, **kwargs):\n        \"\"\"Sets the title on the underlying matplotlib AxesSubplot.\"\"\"\n        ax = self.get_axes()\n        ax.set_title(title, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the label on the left axis.", "response": "def left_axis_label(self, label, position=None,  rotation=60, offset=0.08,\n                        **kwargs):\n        \"\"\"\n        Sets the label on the left axis.\n\n        Parameters\n        ----------\n        label: String\n            The axis label\n        position: 3-Tuple of floats, None\n            The position of the text label\n        rotation: float, 60\n            The angle of rotation of the label\n        offset: float,\n            Used to compute the distance of the label from the axis\n        kwargs:\n            Any kwargs to pass through to matplotlib.\n        \"\"\"\n\n        if not position:\n            position = (-offset, 3./5, 2./5)\n        self._labels[\"left\"] = (label, position, rotation, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the label on the right axis.", "response": "def right_axis_label(self, label, position=None, rotation=-60, offset=0.08,\n                         **kwargs):\n\n        \"\"\"\n        Sets the label on the right axis.\n\n        Parameters\n        ----------\n        label: String\n            The axis label\n        position: 3-Tuple of floats, None\n            The position of the text label\n        rotation: float, -60\n            The angle of rotation of the label\n        offset: float,\n            Used to compute the distance of the label from the axis\n        kwargs:\n            Any kwargs to pass through to matplotlib.\n        \"\"\"\n\n        if not position:\n            position = (2. / 5 + offset, 3. / 5, 0)\n        self._labels[\"right\"] = (label, position, rotation, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the label on the bottom axis.", "response": "def bottom_axis_label(self, label, position=None, rotation=0, offset=0.02,\n                          **kwargs):\n        \"\"\"\n        Sets the label on the bottom axis.\n\n        Parameters\n        ----------\n        label: String\n            The axis label\n        position: 3-Tuple of floats, None\n            The position of the text label\n        rotation: float, 0\n            The angle of rotation of the label\n        offset: float,\n            Used to compute the distance of the label from the axis\n        kwargs:\n            Any kwargs to pass through to matplotlib.\n        \"\"\"\n\n        if not position:\n            position = (0.5, -offset / 2., 0.5)\n        self._labels[\"bottom\"] = (label, position, rotation, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the label on the right corner of the log2 message.", "response": "def right_corner_label(self, label, position=None, rotation=0, offset=0.08,\n                           **kwargs):\n        \"\"\"\n        Sets the label on the right corner (complements left axis).\n\n        Parameters\n        ----------\n        label: String\n            The axis label\n        position: 3-Tuple of floats, None\n            The position of the text label\n        rotation: float, 0\n            The angle of rotation of the label\n        offset: float,\n            Used to compute the distance of the label from the axis\n        kwargs:\n            Any kwargs to pass through to matplotlib.\n        \"\"\"\n\n        if not position:\n            position = (1, offset / 2, 0)\n        self._corner_labels[\"right\"] = (label, position, rotation, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the label on the left corner.", "response": "def left_corner_label(self, label, position=None, rotation=0, offset=0.08,\n                          **kwargs):\n        \"\"\"\n        Sets the label on the left corner (complements right axis.)\n\n        Parameters\n        ----------\n        label: string\n            The axis label\n        position: 3-Tuple of floats, None\n            The position of the text label\n        rotation: float, 0\n            The angle of rotation of the label\n        offset: float,\n            Used to compute the distance of the label from the axis\n        kwargs:\n            Any kwargs to pass through to matplotlib.\n        \"\"\"\n\n        if not position:\n            position = (-offset / 2, offset / 2, 0)\n        self._corner_labels[\"left\"] = (label, position, rotation, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the label on the top corner of the logarithmic logarithm.", "response": "def top_corner_label(self, label, position=None, rotation=0, offset=0.2,\n                         **kwargs):\n        \"\"\"\n        Sets the label on the bottom axis.\n\n        Parameters\n        ----------\n        label: String\n            The axis label\n        position: 3-Tuple of floats, None\n            The position of the text label\n        rotation: float, 0\n            The angle of rotation of the label\n        offset: float,\n            Used to compute the distance of the label from the axis\n        kwargs:\n            Any kwargs to pass through to matplotlib.\n        \"\"\"\n\n        if not position:\n            position = (-offset / 2, 1 + offset, 0)\n        self._corner_labels[\"top\"] = (label, position, rotation, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclearing the default matplotlib ticks.", "response": "def clear_matplotlib_ticks(self, axis=\"both\"):\n        \"\"\"Clears the default matplotlib ticks.\"\"\"\n        ax = self.get_axes()\n        plotting.clear_matplotlib_ticks(ax=ax, axis=axis)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the ticks for all three axes and store them in self. _ticks under the key b l r for bottom and left and right axes.", "response": "def get_ticks_from_axis_limits(self, multiple=1):\n        \"\"\"\n        Taking self._axis_limits and self._boundary_scale get the scaled\n        ticks for all three axes and store them in self._ticks under the\n        keys 'b' for bottom, 'l' for left and 'r' for right axes.\n        \"\"\"\n        for k in ['b', 'l', 'r']:\n            self._ticks[k] = numpy.linspace(\n                self._axis_limits[k][0],\n                self._axis_limits[k][1],\n                self._boundary_scale / float(multiple) + 1\n            ).tolist()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_custom_ticks(self, locations=None, clockwise=False, multiple=1,\n                         axes_colors=None, tick_formats=None, **kwargs):\n        \"\"\"\n        Having called get_ticks_from_axis_limits, set the custom ticks on the\n        plot.\n        \"\"\"\n        for k in ['b', 'l', 'r']:\n            self.ticks(ticks=self._ticks[k], locations=locations,\n                       axis=k, clockwise=clockwise, multiple=multiple,\n                       axes_colors=axes_colors, tick_formats=tick_formats,\n                       **kwargs)", "response": "Set the custom ticks on the base class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nredrawing axis labels typically after draw or resize events.", "response": "def _redraw_labels(self):\n        \"\"\"Redraw axis labels, typically after draw or resize events.\"\"\"\n        ax = self.get_axes()\n        # Remove any previous labels\n        for mpl_object in self._to_remove:\n            mpl_object.remove()\n        self._to_remove = []\n        # Redraw the labels with the appropriate angles\n        label_data = list(self._labels.values())\n        label_data.extend(self._corner_labels.values())\n        for (label, position, rotation, kwargs) in label_data:\n            transform = ax.transAxes\n            x, y = project_point(position)\n            # Calculate the new angle.\n            position = numpy.array([x, y])\n            new_rotation = ax.transData.transform_angles(\n                numpy.array((rotation,)), position.reshape((1, 2)))[0]\n            text = ax.text(x, y, label, rotation=new_rotation,\n                           transform=transform, horizontalalignment=\"center\",\n                           **kwargs)\n            text.set_rotation_mode(\"anchor\")\n            self._to_remove.append(text)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting data coordinates to simplex coordinates for plotting .", "response": "def convert_coordinates(self, points, axisorder='blr'):\n        \"\"\"\n        Convert data coordinates to simplex coordinates for plotting\n        in the case that axis limits have been applied.\n        \"\"\"\n        return convert_coordinates_sequence(points,self._boundary_scale,\n                                            self._axis_limits, axisorder)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_cmap(cmap=None):\n\n    if isinstance(cmap, matplotlib.colors.Colormap):\n        return cmap\n    if isinstance(cmap, str):\n        cmap_name = cmap\n    else:\n        cmap_name = DEFAULT_COLOR_MAP_NAME\n    return plt.get_cmap(cmap_name)", "response": "Loads a matplotlib colormap if specified or supplies the default colormap."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef colormapper(value, lower=0, upper=1, cmap=None):\n\n    cmap = get_cmap(cmap)\n    if upper - lower == 0:\n        rgba = cmap(0)\n    else:\n        rgba = cmap((value - lower) / float(upper - lower))\n    hex_ = rgb2hex(rgba)\n    return hex_", "response": "Maps values to colors by normalizing within [ a b )."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef colorbar_hack(ax, vmin, vmax, cmap, scientific=False, cbarlabel=None,\n                  **kwargs):\n    \"\"\"\n    Colorbar hack to insert colorbar on ternary plot. \n    \n    Called by heatmap, not intended for direct usage.\n    \n    Parameters\n    ----------\n    vmin: float\n        Minimum value to portray in colorbar\n    vmax: float\n        Maximum value to portray in colorbar\n    cmap: Matplotlib colormap\n        Matplotlib colormap to use\n\n    \"\"\"\n    # http://stackoverflow.com/questions/8342549/matplotlib-add-colorbar-to-a-sequence-of-line-plots\n    norm = plt.Normalize(vmin=vmin, vmax=vmax)\n    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n    sm._A = []\n    cb = plt.colorbar(sm, ax=ax, **kwargs)\n    if cbarlabel is not None:\n        cb.set_label(cbarlabel)\n    if scientific:\n        cb.locator = matplotlib.ticker.LinearLocator(numticks=7)\n        cb.formatter = matplotlib.ticker.ScalarFormatter()\n        cb.formatter.set_powerlimits((0, 0))\n        cb.update_ticks()", "response": "Colorbar hack to insert colorbar on ternary plot."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the average value of the three vertices of a triangle in the simplex triangulation where two of the vertices are on the lower horizontal.", "response": "def blend_value(data, i, j, k, keys=None):\n    \"\"\"Computes the average value of the three vertices of a triangle in the\n    simplex triangulation, where two of the vertices are on the lower\n    horizontal.\"\"\"\n\n    key_size = len(list(data.keys())[0])\n    if not keys:\n        keys = triangle_coordinates(i, j, k)\n    # Reduce key from (i, j, k) to (i, j) if necessary\n    keys = [tuple(key[:key_size]) for key in keys]\n\n    # Sum over the values of the points to blend\n    try:\n        s = sum(data[key] for key in keys)\n        value = s / 3.\n    except KeyError:\n        value = None\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the average value of the three vertices of a triangle in the upper .", "response": "def alt_blend_value(data, i, j, k):\n    \"\"\"Computes the average value of the three vertices of a triangle in the\n    simplex triangulation, where two of the vertices are on the upper\n    horizontal.\"\"\"\n\n    keys = alt_triangle_coordinates(i, j, k)\n    return blend_value(data, i, j, k, keys=keys)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef triangle_coordinates(i, j, k):\n\n    return [(i, j, k), (i + 1, j, k - 1), (i, j + 1, k - 1)]", "response": "Computes the coordinates of the constituent triangles of a simplex."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a dictionary of the necessary additive vectors to generate the hexagon points for the hexagonal heatmap.", "response": "def generate_hexagon_deltas():\n    \"\"\"\n    Generates a dictionary of the necessary additive vectors to generate the\n    hexagon points for the hexagonal heatmap.\n    \"\"\"\n\n    zero = numpy.array([0, 0, 0])\n    alpha = numpy.array([-1./3, 2./3, 0])\n    deltaup = numpy.array([1./3, 1./3, 0])\n    deltadown = numpy.array([2./3, -1./3, 0])\n    i_vec = numpy.array([0, 1./2, -1./2])\n    i_vec_down = numpy.array([1./2, -1./2, 0])\n    deltaX_vec = numpy.array([1./2, 0, -1./2])\n\n    d = dict()\n    # Corner Points\n    d[\"100\"] = [zero, -deltaX_vec, -deltadown, -i_vec_down]\n    d[\"010\"] = [zero, i_vec_down, -alpha, -i_vec]\n    d[\"001\"] = [zero, i_vec, deltaup, deltaX_vec]\n    # On the Edges\n    d[\"011\"] = [i_vec, deltaup, deltadown, -alpha, -i_vec]\n    d[\"101\"] = [-deltaX_vec, -deltadown, alpha, deltaup, deltaX_vec]\n    d[\"110\"] = [i_vec_down, -alpha, -deltaup, -deltadown, -i_vec_down]\n    # Interior point\n    d[\"111\"] = [alpha, deltaup, deltadown, -alpha, -deltaup, -deltadown]\n\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the coordinates of the constituent hexagons of a hexagonal heatmap.", "response": "def hexagon_coordinates(i, j, k):\n    \"\"\"\n    Computes coordinates of the constituent hexagons of a hexagonal heatmap.\n\n    Parameters\n    ----------\n    i, j, k: enumeration of the desired hexagon\n\n    Returns\n    -------\n    A numpy array of coordinates of the hexagon (unprojected)\n    \"\"\"\n\n    signature = \"\"\n    for x in [i, j, k]:\n        if x == 0:\n            signature += \"0\"\n        else:\n            signature += \"1\"\n    deltas = hexagon_deltas[signature]\n    center = numpy.array([i, j, k])\n    return numpy.array([center + x for x in deltas])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef polygon_generator(data, scale, style, permutation=None):\n\n    # We'll project the coordinates inside this function to prevent\n    # passing around permutation more than necessary\n    project = functools.partial(project_point, permutation=permutation)\n\n    if isinstance(data, dict):\n        data_gen = data.items()\n    else:\n        # Only works with style == 'h'\n        data_gen = data\n\n    for key, value in data_gen:\n        if value is None:\n            continue\n        i = key[0]\n        j = key[1]\n        k = scale - i - j\n        if style == 'h':\n            vertices = hexagon_coordinates(i, j, k)\n            yield (map(project, vertices), value)\n        elif style == 'd':\n            # Upright triangles\n            if (i <= scale) and (j <= scale) and (k >= 0):\n                vertices = triangle_coordinates(i, j, k)\n                yield (map(project, vertices), value)\n            # Upside-down triangles\n            if (i < scale) and (j < scale) and (k >= 1):\n                vertices = alt_triangle_coordinates(i, j, k)\n                value = blend_value(data, i, j, k)\n                yield (map(project, vertices), value)\n        elif style == 't':\n            # Upright triangles\n            if (i < scale) and (j < scale) and (k > 0):\n                vertices = triangle_coordinates(i, j, k)\n                value = blend_value(data, i, j, k)\n                yield (map(project, vertices), value)\n            # If not on the boundary add the upside-down triangle\n            if (i < scale) and (j < scale) and (k > 1):\n                vertices = alt_triangle_coordinates(i, j, k)\n                value = alt_blend_value(data, i, j, k)\n                yield (map(project, vertices), value)", "response": "Generator for the vertices of the polygon to be colored and its color. Called by heatmap."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef heatmap(data, scale, vmin=None, vmax=None, cmap=None, ax=None,\n            scientific=False, style='triangular', colorbar=True,\n            permutation=None, use_rgba=False, cbarlabel=None, cb_kwargs=None):\n    \"\"\"\n    Plots heatmap of given color values.\n\n    Parameters\n    ----------\n    data: dictionary\n        A dictionary mapping the i, j polygon to the heatmap color, where\n        i + j + k = scale.\n    scale: Integer\n        The scale used to partition the simplex.\n    vmin: float, None\n        The minimum color value, used to normalize colors. Computed if absent.\n    vmax: float, None\n        The maximum color value, used to normalize colors. Computed if absent.\n    cmap: String or matplotlib.colors.Colormap, None\n        The name of the Matplotlib colormap to use.\n    ax: Matplotlib AxesSubplot, None\n        The subplot to draw on.\n    scientific: Bool, False\n        Whether to use scientific notation for colorbar numbers.\n    style: String, \"triangular\"\n        The style of the heatmap, \"triangular\", \"dual-triangular\" or \"hexagonal\"\n    colorbar: bool, True\n        Show colorbar.\n    permutation: string, None\n        A permutation of the coordinates\n    use_rgba: bool, False\n        Use rgba color values\n    cbarlabel: string, None\n        Text label for the colorbar\n    cb_kwargs: dict\n        dict of kwargs to pass to colorbar\n\n    Returns\n    -------\n    ax: The matplotlib axis\n    \"\"\"\n\n    if not ax:\n        fig, ax = pyplot.subplots()\n    # If use_rgba, make the RGBA values numpy arrays so that they can\n    # be averaged.\n    if use_rgba:\n        for k, v in data.items():\n            data[k] = numpy.array(v)\n    else:\n        cmap = get_cmap(cmap)\n        if vmin is None:\n            vmin = min(data.values())\n        if vmax is None:\n            vmax = max(data.values())\n    style = style.lower()[0]\n    if style not in [\"t\", \"h\", 'd']:\n        raise ValueError(\"Heatmap style must be 'triangular', 'dual-triangular', or 'hexagonal'\")\n\n    vertices_values = polygon_generator(data, scale, style,\n                                        permutation=permutation)\n\n    # Draw the polygons and color them\n    for vertices, value in vertices_values:\n        if value is None:\n            continue\n        if not use_rgba:\n            color = colormapper(value, vmin, vmax, cmap=cmap)\n        else:\n            color = value  # rgba tuple (r,g,b,a) all in [0,1]\n        # Matplotlib wants a list of xs and a list of ys\n        xs, ys = unzip(vertices)\n        ax.fill(xs, ys, facecolor=color, edgecolor=color)\n\n    if not cb_kwargs:\n        cb_kwargs = dict()\n    if colorbar:\n        colorbar_hack(ax, vmin, vmax, cmap, scientific=scientific,\n                      cbarlabel=cbarlabel, **cb_kwargs)\n    return ax", "response": "Plots a simplex of the given color values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfunction to compute the function on the simplex and plots a heatmap.", "response": "def heatmapf(func, scale=10, boundary=True, cmap=None, ax=None,\n             scientific=False, style='triangular', colorbar=True,\n             permutation=None, vmin=None, vmax=None, cbarlabel=None,\n             cb_kwargs=None):\n    \"\"\"\n    Computes func on heatmap partition coordinates and plots heatmap. In other\n    words, computes the function on lattice points of the simplex (normalized\n    points) and creates a heatmap from the values.\n\n    Parameters\n    ----------\n    func: Function\n        A function of 3-tuples to be heatmapped\n    scale: Integer\n        The scale used to partition the simplex\n    boundary: Bool, True\n        Include the boundary points or not\n    cmap: String, None\n        The name of the Matplotlib colormap to use\n    ax: Matplotlib axis object, None\n        The axis to draw the colormap on\n    style: String, \"triangular\"\n        The style of the heatmap, \"triangular\", \"dual-triangular\" or \"hexagonal\"\n    scientific: Bool, False\n        Whether to use scientific notation for colorbar numbers.\n    colorbar: bool, True\n        Show colorbar.\n    permutation: string, None\n        A permutation of the coordinates\n    vmin: float\n        The minimum color value, used to normalize colors.\n    vmax: float\n        The maximum color value, used to normalize colors.\n    cb_kwargs: dict\n        dict of kwargs to pass to colorbar\n\n    Returns\n    -------\n    ax, The matplotlib axis\n    \"\"\"\n\n    # Apply the function to a simplex partition\n    data = dict()\n    for i, j, k in simplex_iterator(scale=scale, boundary=boundary):\n        data[(i, j)] = func(normalize([i, j, k]))\n    # Pass everything to the heatmapper\n    ax = heatmap(data, scale, cmap=cmap, ax=ax, style=style,\n                 scientific=scientific, colorbar=colorbar,\n                 permutation=permutation, vmin=vmin, vmax=vmax, \n                 cbarlabel=cbarlabel, cb_kwargs=cb_kwargs)\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef svg_polygon(coordinates, color):\n\n    coord_str = []\n    for c in coordinates:\n        coord_str.append(\",\".join(map(str, c)))\n    coord_str = \" \".join(coord_str)\n    polygon = '<polygon points=\"%s\" style=\"fill:%s;stroke:%s;stroke-width:0\"/>\\n' % (coord_str, color, color)\n    return polygon", "response": "Create an svg polygon for the stationary heatmap."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef svg_heatmap(data, scale, filename, vmax=None, vmin=None, style='h',\n                permutation=None, cmap=None):\n    \"\"\"\n    Create a heatmap in SVG format. Intended for use with very large datasets,\n    which would require large amounts of RAM using matplotlib. You can convert\n    the image to another format with e.g. ImageMagick:\n\n    convert -density 1200 -resize -rotate 180 1000x1000 your.svg your.png\n\n    Parameters\n    ----------\n\n    data: dictionary or k, v generator\n        A dictionary mapping the i, j polygon to the heatmap color, where\n        i + j + k = scale. If using a generator, style must be 'h'.\n    scale: Integer\n        The scale used to partition the simplex.\n    filename: string\n        The filename to write the SVG data to.\n    vmin: float\n        The minimum color value, used to normalize colors.\n    vmax: float\n        The maximum color value, used to normalize colors.\n    cmap: String or matplotlib.colors.Colormap, None\n        The name of the Matplotlib colormap to use.\n    style: String, \"h\"\n        The style of the heatmap, \"triangular\", \"dual-triangular\" or \"hexagonal\"\n    permutation: string, None\n        A permutation of the coordinates\n    \"\"\"\n\n    style = style.lower()[0]\n    if style not in [\"t\", \"h\", 'd']:\n        raise ValueError(\"Heatmap style must be 'triangular', 'dual-triangular', or 'hexagonal'\")\n\n    if not isinstance(data, dict):\n        if not style == 'h':\n            raise ValueError(\"Data can only be given as a generator for hexagonal style heatmaps because of blending for adjacent polygons.\")\n        elif vmax is None or vmin is None:\n            raise ValueError(\"vmax and vmin must be supplied for data given as a generator.\")\n\n    cmap = get_cmap(cmap)\n\n    if not vmin:\n        vmin = min(data.values())\n    if not vmax:\n        vmax = max(data.values())\n\n    height = scale * numpy.sqrt(3) / 2 + 2\n\n    output_file = open(filename, 'w')\n    output_file.write('<svg height=\"%s\" width=\"%s\">\\n' % (height, scale))\n\n    vertices_values = polygon_generator(data, scale, style,\n                                        permutation=permutation)\n\n    # Draw the polygons and color them\n    for vertices, value in vertices_values:\n        color = colormapper(value, vmin, vmax, cmap=cmap)\n        output_file.write(svg_polygon(vertices, color))\n\n    output_file.write('</svg>\\n')", "response": "Create a heatmap in SVG format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef line(ax, p1, p2, permutation=None, **kwargs):\n\n    pp1 = project_point(p1, permutation=permutation)\n    pp2 = project_point(p2, permutation=permutation)\n    ax.add_line(Line2D((pp1[0], pp2[0]), (pp1[1], pp2[1]), **kwargs))", "response": "Draws a line on ax from p1 to p2."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndraws the i - th horizontal line parallel to the lower axis.", "response": "def horizontal_line(ax, scale, i, **kwargs):\n    \"\"\"\n    Draws the i-th horizontal line parallel to the lower axis.\n\n    Parameters\n    ----------\n    ax: Matplotlib AxesSubplot\n        The subplot to draw on.\n    scale: float, 1.0\n        Simplex scale size.\n    i: float\n        The index of the line to draw\n    kwargs: Dictionary\n        Any kwargs to pass through to Matplotlib.\n    \"\"\"\n\n    p1 = (0, i, scale - i)\n    p2 = (scale - i, i, 0)\n    line(ax, p1, p2, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef left_parallel_line(ax, scale, i,  **kwargs):\n\n    p1 = (i, scale - i, 0)\n    p2 = (i, 0, scale - i)\n    line(ax, p1, p2, **kwargs)", "response": "Draw the i - th line parallel to the left axis."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndrawing the i - th line parallel to the right axis.", "response": "def right_parallel_line(ax, scale, i, **kwargs):\n    \"\"\"\n    Draws the i-th line parallel to the right axis.\n\n    Parameters\n    ----------\n    ax: Matplotlib AxesSubplot\n        The subplot to draw on.\n    scale: float\n        Simplex scale size.\n    i: float\n        The index of the line to draw\n    kwargs: Dictionary\n        Any kwargs to pass through to Matplotlib.\n    \"\"\"\n\n    p1 = (0, scale - i, i)\n    p2 = (scale - i, 0, i)\n    line(ax, p1, p2, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef boundary(ax, scale, axes_colors=None, **kwargs):\n\n    # Set default color as black.\n    if axes_colors is None:\n        axes_colors = dict()\n    for _axis in ['l', 'r', 'b']:\n        if _axis not in axes_colors.keys():\n            axes_colors[_axis] = 'black'\n\n    horizontal_line(ax, scale, 0, color=axes_colors['b'], **kwargs)\n    left_parallel_line(ax, scale, 0, color=axes_colors['l'], **kwargs)\n    right_parallel_line(ax, scale, 0, color=axes_colors['r'], **kwargs)\n    return ax", "response": "Plots the boundary of the simplex."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive two dicts merge them into a new dict as a shallow copy.", "response": "def merge_dicts(base, updates):\n    \"\"\"\n    Given two dicts, merge them into a new dict as a shallow copy.\n\n    Parameters\n    ----------\n    base: dict\n        The base dictionary.\n    updates: dict\n        Secondary dictionary whose values override the base.\n    \"\"\"\n    if not base:\n        base = dict()\n    if not updates:\n        updates = dict()\n    z = base.copy()\n    z.update(updates)\n    return z"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gridlines(ax, scale, multiple=None, horizontal_kwargs=None,\n              left_kwargs=None, right_kwargs=None, **kwargs):\n    \"\"\"\n    Plots grid lines excluding boundary.\n\n    Parameters\n    ----------\n    ax: Matplotlib AxesSubplot, None\n        The subplot to draw on.\n    scale: float\n        Simplex scale size.\n    multiple: float, None\n        Specifies which inner gridelines to draw. For example, if scale=30 and\n        multiple=6, only 5 inner gridlines will be drawn.\n    horizontal_kwargs: dict, None\n        Any kwargs to pass through to matplotlib for horizontal gridlines\n    left_kwargs: dict, None\n        Any kwargs to pass through to matplotlib for left parallel gridlines\n    right_kwargs: dict, None\n        Any kwargs to pass through to matplotlib for right parallel gridlines\n    kwargs:\n        Any kwargs to pass through to matplotlib, if not using\n        horizontal_kwargs, left_kwargs, or right_kwargs\n    \"\"\"\n\n    if 'linewidth' not in kwargs:\n        kwargs[\"linewidth\"] = 0.5\n    if 'linestyle' not in kwargs:\n        kwargs[\"linestyle\"] = ':'\n    horizontal_kwargs = merge_dicts(kwargs, horizontal_kwargs)\n    left_kwargs = merge_dicts(kwargs, left_kwargs)\n    right_kwargs = merge_dicts(kwargs, right_kwargs)\n    if not multiple:\n        multiple = 1.\n    ## Draw grid-lines\n    # Parallel to horizontal axis\n    for i in arange(0, scale, multiple):\n        horizontal_line(ax, scale, i, **horizontal_kwargs)\n    # Parallel to left and right axes\n    for i in arange(0, scale + multiple, multiple):\n        left_parallel_line(ax, scale, i, **left_kwargs)\n        right_parallel_line(ax, scale, i, **right_kwargs)\n    return ax", "response": "Plots the inner gridlines of the current node."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplot the ticks for the current log - level level.", "response": "def ticks(ax, scale, ticks=None, locations=None, multiple=1, axis='b',\n          offset=0.01, clockwise=False, axes_colors=None, fontsize=10,\n          tick_formats=None, **kwargs):\n    \"\"\"\n    Sets tick marks and labels.\n\n    Parameters\n    ----------\n    ax: Matplotlib AxesSubplot, None\n        The subplot to draw on.\n    scale: float, 1.0\n        Simplex scale size.\n    ticks: list of strings, None\n        The tick labels\n    locations: list of points, None\n        The locations of the ticks\n    multiple: float, None\n        Specifies which ticks gridelines to draw. For example, if scale=30 and\n        multiple=6, only 5 ticks will be drawn.\n    axis: str, 'b'\n        The axis or axes to draw the ticks for. `axis` must be a substring of\n        'lrb' (as sets)\n    offset: float, 0.01\n        controls the length of the ticks\n    clockwise: bool, False\n        Draw ticks marks clockwise or counterclockwise\n    axes_colors: Dict, None\n        Option to color ticks differently for each axis, 'l', 'r', 'b'\n        e.g. {'l': 'g', 'r':'b', 'b': 'y'}\n    tick_formats: None, Dict, Str\n        If None, all axes will be labelled with ints. If Dict, the keys are\n        'b', 'l' and 'r' and the values are format strings e.g. \"%.3f\" for\n        a float with 3 decimal places or \"%.3e\" for scientific format with\n        3 decimal places or \"%d\" for ints. If tick_formats is a string, it\n        is assumed that this is a format string to be applied to all axes.\n    kwargs:\n        Any kwargs to pass through to matplotlib.\n\n    \"\"\"\n\n    axis = axis.lower()\n    valid_axis_chars = set(['l', 'r', 'b'])\n    axis_chars = set(axis)\n    if not axis_chars.issubset(valid_axis_chars):\n        raise ValueError(\"axis must be some combination of 'l', 'r', and 'b'\")\n\n    if ticks and not locations:\n        num_ticks = len(ticks)\n        if num_ticks != 0:\n            multiple = scale / (num_ticks - 1)\n            locations = arange(0, scale + multiple, multiple)\n\n    if not ticks:\n        locations = arange(0, scale + multiple, multiple)\n        ticks = locations\n\n    tick_formats = normalize_tick_formats(tick_formats)\n\n    # Default color: black\n    if axes_colors is None:\n        axes_colors = dict()\n    for _axis in valid_axis_chars:\n        if _axis not in axes_colors:\n            axes_colors[_axis] = 'black'\n\n    offset *= scale\n\n    if 'r' in axis:\n        for index, i in enumerate(locations):\n            loc1 = (scale - i, i, 0)\n            if clockwise:\n                # Right parallel\n                loc2 = (scale - i, i + offset, 0)\n                text_location = (scale - i, i + 2 * offset, 0)\n                tick = ticks[-(index+1)]\n            else:\n                # Horizontal\n                loc2 = (scale - i + offset, i, 0)\n                text_location = (scale - i + 2.6 * offset, i - 0.5 * offset, 0)\n                tick = ticks[index]\n            line(ax, loc1, loc2, color=axes_colors['r'], **kwargs)\n            x, y = project_point(text_location)\n            s = tick_formats['r'] % tick\n            ax.text(x, y, s, horizontalalignment=\"center\",\n                    color=axes_colors['r'], fontsize=fontsize)\n\n    if 'l' in axis:\n        for index, i in enumerate(locations):\n            loc1 = (0, i, 0)\n            if clockwise:\n                # Horizontal\n                loc2 = (-offset, i, 0)\n                text_location = (-2 * offset, i - 0.5 * offset, 0)\n                tick = ticks[index]\n            else:\n                # Right parallel\n                loc2 = (-offset, i + offset, 0)\n                text_location = (-2 * offset, i + 1.5 * offset, 0)\n                tick = ticks[-(index+1)]\n            line(ax, loc1, loc2, color=axes_colors['l'], **kwargs)\n            x, y = project_point(text_location)\n            s = tick_formats['l'] % tick\n            ax.text(x, y, s, horizontalalignment=\"center\",\n                    color=axes_colors['l'], fontsize=fontsize)\n\n    if 'b' in axis:\n        for index, i in enumerate(locations):\n            loc1 = (i, 0, 0)\n            if clockwise:\n                # Right parallel\n                loc2 = (i + offset, -offset, 0)\n                text_location = (i + 3 * offset, -3.5 * offset, 0)\n                tick = ticks[-(index+1)]\n            else:\n                # Left parallel\n                loc2 = (i, -offset, 0)\n                text_location = (i + 0.5 * offset, -3.5 * offset, 0)\n                tick = ticks[index]\n            line(ax, loc1, loc2, color=axes_colors['b'], **kwargs)\n            x, y = project_point(text_location)\n            s = tick_formats['b'] % tick\n            ax.text(x, y, s, horizontalalignment=\"center\",\n                    color=axes_colors['b'], fontsize=fontsize)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_next_candidates(prev_candidates, length):\n    # Solve the items.\n    item_set = set()\n    for candidate in prev_candidates:\n        for item in candidate:\n            item_set.add(item)\n    items = sorted(item_set)\n\n    # Create the temporary candidates. These will be filtered below.\n    tmp_next_candidates = (frozenset(x) for x in combinations(items, length))\n\n    # Return all the candidates if the length of the next candidates is 2\n    # because their subsets are the same as items.\n    if length < 3:\n        return list(tmp_next_candidates)\n\n    # Filter candidates that all of their subsets are\n    # in the previous candidates.\n    next_candidates = [\n        candidate for candidate in tmp_next_candidates\n        if all(\n            True if frozenset(x) in prev_candidates else False\n            for x in combinations(candidate, length - 1))\n    ]\n    return next_candidates", "response": "Creates the next candidates."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a generator of support records with given transactions.", "response": "def gen_support_records(transaction_manager, min_support, **kwargs):\n    \"\"\"\n    Returns a generator of support records with given transactions.\n\n    Arguments:\n        transaction_manager -- Transactions as a TransactionManager instance.\n        min_support -- A minimum support (float).\n\n    Keyword arguments:\n        max_length -- The maximum length of relations (integer).\n    \"\"\"\n    # Parse arguments.\n    max_length = kwargs.get('max_length')\n\n    # For testing.\n    _create_next_candidates = kwargs.get(\n        '_create_next_candidates', create_next_candidates)\n\n    # Process.\n    candidates = transaction_manager.initial_candidates()\n    length = 1\n    while candidates:\n        relations = set()\n        for relation_candidate in candidates:\n            support = transaction_manager.calc_support(relation_candidate)\n            if support < min_support:\n                continue\n            candidate_set = frozenset(relation_candidate)\n            relations.add(candidate_set)\n            yield SupportRecord(candidate_set, support)\n        length += 1\n        if max_length and length > max_length:\n            break\n        candidates = _create_next_candidates(relations, length)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gen_ordered_statistics(transaction_manager, record):\n    items = record.items\n    for combination_set in combinations(sorted(items), len(items) - 1):\n        items_base = frozenset(combination_set)\n        items_add = frozenset(items.difference(items_base))\n        confidence = (\n            record.support / transaction_manager.calc_support(items_base))\n        lift = confidence / transaction_manager.calc_support(items_add)\n        yield OrderedStatistic(\n            frozenset(items_base), frozenset(items_add), confidence, lift)", "response": "Returns a generator of OrderedStatistic instances."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfiltering the ordered_statistics by the minimum confidence and lift of relations.", "response": "def filter_ordered_statistics(ordered_statistics, **kwargs):\n    \"\"\"\n    Filter OrderedStatistic objects.\n\n    Arguments:\n        ordered_statistics -- A OrderedStatistic iterable object.\n\n    Keyword arguments:\n        min_confidence -- The minimum confidence of relations (float).\n        min_lift -- The minimum lift of relations (float).\n    \"\"\"\n    min_confidence = kwargs.get('min_confidence', 0.0)\n    min_lift = kwargs.get('min_lift', 0.0)\n\n    for ordered_statistic in ordered_statistics:\n        if ordered_statistic.confidence < min_confidence:\n            continue\n        if ordered_statistic.lift < min_lift:\n            continue\n        yield ordered_statistic"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef apriori(transactions, **kwargs):\n    # Parse the arguments.\n    min_support = kwargs.get('min_support', 0.1)\n    min_confidence = kwargs.get('min_confidence', 0.0)\n    min_lift = kwargs.get('min_lift', 0.0)\n    max_length = kwargs.get('max_length', None)\n\n    # Check arguments.\n    if min_support <= 0:\n        raise ValueError('minimum support must be > 0')\n\n    # For testing.\n    _gen_support_records = kwargs.get(\n        '_gen_support_records', gen_support_records)\n    _gen_ordered_statistics = kwargs.get(\n        '_gen_ordered_statistics', gen_ordered_statistics)\n    _filter_ordered_statistics = kwargs.get(\n        '_filter_ordered_statistics', filter_ordered_statistics)\n\n    # Calculate supports.\n    transaction_manager = TransactionManager.create(transactions)\n    support_records = _gen_support_records(\n        transaction_manager, min_support, max_length=max_length)\n\n    # Calculate ordered stats.\n    for support_record in support_records:\n        ordered_statistics = list(\n            _filter_ordered_statistics(\n                _gen_ordered_statistics(transaction_manager, support_record),\n                min_confidence=min_confidence,\n                min_lift=min_lift,\n            )\n        )\n        if not ordered_statistics:\n            continue\n        yield RelationRecord(\n            support_record.items, support_record.support, ordered_statistics)", "response": "Executes Apriori algorithm and returns a generator of RelationRecords."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_args(argv):\n    output_funcs = {\n        'json': dump_as_json,\n        'tsv': dump_as_two_item_tsv,\n    }\n    default_output_func_key = 'json'\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '-v', '--version', action='version',\n        version='%(prog)s {0}'.format(__version__))\n    parser.add_argument(\n        'input', metavar='inpath', nargs='*',\n        help='Input transaction file (default: stdin).',\n        type=argparse.FileType('r'), default=[sys.stdin])\n    parser.add_argument(\n        '-o', '--output', metavar='outpath',\n        help='Output file (default: stdout).',\n        type=argparse.FileType('w'), default=sys.stdout)\n    parser.add_argument(\n        '-l', '--max-length', metavar='int',\n        help='Max length of relations (default: infinite).',\n        type=int, default=None)\n    parser.add_argument(\n        '-s', '--min-support', metavar='float',\n        help='Minimum support ratio (must be > 0, default: 0.1).',\n        type=float, default=0.1)\n    parser.add_argument(\n        '-c', '--min-confidence', metavar='float',\n        help='Minimum confidence (default: 0.5).',\n        type=float, default=0.5)\n    parser.add_argument(\n        '-t', '--min-lift', metavar='float',\n        help='Minimum lift (default: 0.0).',\n        type=float, default=0.0)\n    parser.add_argument(\n        '-d', '--delimiter', metavar='str',\n        help='Delimiter for items of transactions (default: tab).',\n        type=str, default='\\t')\n    parser.add_argument(\n        '-f', '--out-format', metavar='str',\n        help='Output format ({0}; default: {1}).'.format(\n            ', '.join(output_funcs.keys()), default_output_func_key),\n        type=str, choices=output_funcs.keys(), default=default_output_func_key)\n    args = parser.parse_args(argv)\n\n    args.output_func = output_funcs[args.out_format]\n    return args", "response": "Parse commandline arguments.\n\n    Arguments:\n        argv -- An argument list without the program name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload transactions and returns a generator for transactions.", "response": "def load_transactions(input_file, **kwargs):\n    \"\"\"\n    Load transactions and returns a generator for transactions.\n\n    Arguments:\n        input_file -- An input file.\n\n    Keyword arguments:\n        delimiter -- The delimiter of the transaction.\n    \"\"\"\n    delimiter = kwargs.get('delimiter', '\\t')\n    for transaction in csv.reader(input_file, delimiter=delimiter):\n        yield transaction if transaction else ['']"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndump a relation record as a json value.", "response": "def dump_as_json(record, output_file):\n    \"\"\"\n    Dump an relation record as a json value.\n\n    Arguments:\n        record -- A RelationRecord instance to dump.\n        output_file -- A file to output.\n    \"\"\"\n    def default_func(value):\n        \"\"\"\n        Default conversion for JSON value.\n        \"\"\"\n        if isinstance(value, frozenset):\n            return sorted(value)\n        raise TypeError(repr(value) + \" is not JSON serializable\")\n\n    converted_record = record._replace(\n        ordered_statistics=[x._asdict() for x in record.ordered_statistics])\n    json.dump(\n        converted_record._asdict(), output_file,\n        default=default_func, ensure_ascii=False)\n    output_file.write(os.linesep)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndumps a relation record as TSV only for 2 item relations.", "response": "def dump_as_two_item_tsv(record, output_file):\n    \"\"\"\n    Dump a relation record as TSV only for 2 item relations.\n\n    Arguments:\n        record -- A RelationRecord instance to dump.\n        output_file -- A file to output.\n    \"\"\"\n    for ordered_stats in record.ordered_statistics:\n        if len(ordered_stats.items_base) != 1:\n            continue\n        if len(ordered_stats.items_add) != 1:\n            continue\n        output_file.write('{0}\\t{1}\\t{2:.8f}\\t{3:.8f}\\t{4:.8f}{5}'.format(\n            list(ordered_stats.items_base)[0], list(ordered_stats.items_add)[0],\n            record.support, ordered_stats.confidence, ordered_stats.lift,\n            os.linesep))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main(**kwargs):\n    # For tests.\n    _parse_args = kwargs.get('_parse_args', parse_args)\n    _load_transactions = kwargs.get('_load_transactions', load_transactions)\n    _apriori = kwargs.get('_apriori', apriori)\n\n    args = _parse_args(sys.argv[1:])\n    transactions = _load_transactions(\n        chain(*args.input), delimiter=args.delimiter)\n    result = _apriori(\n        transactions,\n        max_length=args.max_length,\n        min_support=args.min_support,\n        min_confidence=args.min_confidence)\n    for record in result:\n        args.output_func(record, args.output)", "response": "Main entry point for the Apriori command line interface."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_transaction(self, transaction):\n        for item in transaction:\n            if item not in self.__transaction_index_map:\n                self.__items.append(item)\n                self.__transaction_index_map[item] = set()\n            self.__transaction_index_map[item].add(self.__num_transaction)\n        self.__num_transaction += 1", "response": "Adds a transaction to the list."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the support for the items.", "response": "def calc_support(self, items):\n        \"\"\"\n        Returns a support for items.\n\n        Arguments:\n            items -- Items as an iterable object (eg. ['A', 'B']).\n        \"\"\"\n        # Empty items is supported by all transactions.\n        if not items:\n            return 1.0\n\n        # Empty transactions supports no items.\n        if not self.num_transaction:\n            return 0.0\n\n        # Create the transaction index intersection.\n        sum_indexes = None\n        for item in items:\n            indexes = self.__transaction_index_map.get(item)\n            if indexes is None:\n                # No support for any set that contains a not existing item.\n                return 0.0\n\n            if sum_indexes is None:\n                # Assign the indexes on the first time.\n                sum_indexes = indexes\n            else:\n                # Calculate the intersection on not the first time.\n                sum_indexes = sum_indexes.intersection(indexes)\n\n        # Calculate and return the support.\n        return float(len(sum_indexes)) / self.__num_transaction"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndisplay the DirectPayment for entering payment information.", "response": "def render_payment_form(self):\n        \"\"\"Display the DirectPayment for entering payment information.\"\"\"\n        self.context[self.form_context_name] = self.payment_form_cls()\n        return TemplateResponse(self.request, self.payment_template, self.context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntries to validate and then process the DirectPayment form.", "response": "def validate_payment_form(self):\n        \"\"\"Try to validate and then process the DirectPayment form.\"\"\"\n        warn_untested()\n        form = self.payment_form_cls(self.request.POST)\n        if form.is_valid():\n            success = form.process(self.request, self.item)\n            if success:\n                return HttpResponseRedirect(self.success_url)\n            else:\n                self.context['errors'] = self.errors['processing']\n\n        self.context[self.form_context_name] = form\n        self.context.setdefault(\"errors\", self.errors['form'])\n        return TemplateResponse(self.request, self.payment_template, self.context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef redirect_to_express(self):\n        wpp = PayPalWPP(self.request)\n        try:\n            nvp_obj = wpp.setExpressCheckout(self.item)\n        except PayPalFailure:\n            warn_untested()\n            self.context['errors'] = self.errors['paypal']\n            return self.render_payment_form()\n        else:\n            return HttpResponseRedirect(express_endpoint_for_token(nvp_obj.token))", "response": "Redirect the request to PayPal using the the\n            data returned from setExpressCheckout."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nseconds step of ExpressCheckout. Display an order confirmation form which contains hidden fields with the token and PayerID from PayPal.", "response": "def render_confirm_form(self):\n        \"\"\"\n        Second step of ExpressCheckout. Display an order confirmation form which\n        contains hidden fields with the token / PayerID from PayPal.\n        \"\"\"\n        warn_untested()\n        initial = dict(token=self.request.GET['token'], PayerID=self.request.GET['PayerID'])\n        self.context[self.form_context_name] = self.confirm_form_cls(initial=initial)\n        return TemplateResponse(self.request, self.confirm_template, self.context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nthirds and final step of ExpressCheckout. Request has pressed the confirmation but and we can send the final confirmation to PayPal using the data from the POST'ed form.", "response": "def validate_confirm_form(self):\n        \"\"\"\n        Third and final step of ExpressCheckout. Request has pressed the confirmation but\n        and we can send the final confirmation to PayPal using the data from the POST'ed form.\n        \"\"\"\n        wpp = PayPalWPP(self.request)\n        pp_data = dict(token=self.request.POST['token'], payerid=self.request.POST['PayerID'])\n        self.item.update(pp_data)\n\n        # @@@ This check and call could be moved into PayPalWPP.\n        try:\n            if self.is_recurring():\n                warn_untested()\n                nvp = wpp.createRecurringPaymentsProfile(self.item)\n            else:\n                nvp = wpp.doExpressCheckoutPayment(self.item)\n            self.handle_nvp(nvp)\n        except PayPalFailure:\n            self.context['errors'] = self.errors['processing']\n            return self.render_payment_form()\n        else:\n            return HttpResponseRedirect(self.success_url)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize a PayPalNVP instance from a HttpRequest.", "response": "def init(self, request, paypal_request, paypal_response):\n        \"\"\"Initialize a PayPalNVP instance from a HttpRequest.\"\"\"\n        if request is not None:\n            from paypal.pro.helpers import strip_ip_port\n            self.ipaddress = strip_ip_port(request.META.get('REMOTE_ADDR', ''))\n            if (hasattr(request, \"user\") and request.user.is_authenticated):\n                self.user = request.user\n        else:\n            self.ipaddress = ''\n\n        # No storing credit card info.\n        query_data = dict((k, v) for k, v in paypal_request.items() if k not in self.RESTRICTED_FIELDS)\n        self.query = urlencode(query_data)\n        self.response = urlencode(paypal_response)\n\n        # Was there a flag on the play?\n        ack = paypal_response.get('ack', False)\n        if ack != \"Success\":\n            if ack == \"SuccessWithWarning\":\n                warn_untested()\n                self.flag_info = paypal_response.get('l_longmessage0', '')\n            else:\n                self.set_flag(paypal_response.get('l_longmessage0', ''), paypal_response.get('l_errorcode', ''))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_flag(self, info, code=None):\n        self.flag = True\n        self.flag_info += info\n        if code is not None:\n            self.flag_code = code", "response": "Flag this instance for investigation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process(self, request, item):\n        warn_untested()\n        from paypal.pro.helpers import PayPalWPP\n\n        wpp = PayPalWPP(request)\n\n        # Change the model information into a dict that PayPal can understand.\n        params = model_to_dict(self, exclude=self.ADMIN_FIELDS)\n        params['acct'] = self.acct\n        params['creditcardtype'] = self.creditcardtype\n        params['expdate'] = self.expdate\n        params['cvv2'] = self.cvv2\n        params.update(item)\n\n        # Create recurring payment:\n        if 'billingperiod' in params:\n            return wpp.createRecurringPaymentsProfile(params, direct=True)\n        # Create single payment:\n        else:\n            return wpp.doDirectPayment(params)", "response": "Do a direct payment."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef posted_data_dict(self):\n        if not self.query:\n            return None\n        from django.http import QueryDict\n        roughdecode = dict(item.split('=', 1) for item in self.query.split('&'))\n        encoding = roughdecode.get('charset', None)\n        if encoding is None:\n            encoding = DEFAULT_ENCODING\n        query = self.query.encode('ascii')\n        data = QueryDict(query, encoding=encoding)\n        return data.dict()", "response": "Returns a dictionary of all the data that PayPal posted to us as a correctly parsed dictionary of values."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nverifies an IPN and PDT.", "response": "def verify(self):\n        \"\"\"\n        Verifies an IPN and a PDT.\n        Checks for obvious signs of weirdness in the payment and flags appropriately.\n        \"\"\"\n        self.response = self._postback().decode('ascii')\n        self.clear_flag()\n        self._verify_postback()\n        if not self.flag:\n            if self.is_transaction():\n                if self.payment_status not in self.PAYMENT_STATUS_CHOICES:\n                    self.set_flag(\"Invalid payment_status. (%s)\" % self.payment_status)\n                if duplicate_txn_id(self):\n                    self.set_flag(\"Duplicate txn_id. (%s)\" % self.txn_id)\n\n        self.save()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef verify_secret(self, form_instance, secret):\n        warn_untested()\n        if not check_secret(form_instance, secret):\n            self.set_flag(\"Invalid secret. (%s)\") % secret\n        self.save()", "response": "Verifies an IPN payment over SSL using EWP."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef initialize(self, request):\n        if request.method == 'GET':\n            # PDT only - this data is currently unused\n            self.query = request.META.get('QUERY_STRING', '')\n        elif request.method == 'POST':\n            # The following works if paypal sends an ASCII bytestring, which it does.\n            self.query = request.body.decode('ascii')\n        self.ipaddress = request.META.get('REMOTE_ADDR', '')", "response": "Store the data we ll need to make the postback from the request object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _encrypt(self):\n        from M2Crypto import BIO, SMIME, X509\n\n        # Iterate through the fields and pull out the ones that have a value.\n        plaintext = 'cert_id=%s\\n' % self.cert_id\n        for name, field in self.fields.items():\n            value = None\n            if name in self.initial:\n                value = self.initial[name]\n            elif field.initial is not None:\n                value = field.initial\n            if value is not None:\n                plaintext += u'%s=%s\\n' % (name, value)\n        plaintext = plaintext.encode('utf-8')\n\n        # Begin crypto weirdness.\n        s = SMIME.SMIME()\n        s.load_key_bio(BIO.openfile(self.private_cert), BIO.openfile(self.public_cert))\n        p7 = s.sign(BIO.MemoryBuffer(plaintext), flags=SMIME.PKCS7_BINARY)\n        x509 = X509.load_cert_bio(BIO.openfile(self.paypal_cert))\n        sk = X509.X509_Stack()\n        sk.push(x509)\n        s.set_x509_stack(sk)\n        s.set_cipher(SMIME.Cipher('des_ede3_cbc'))\n        tmp = BIO.MemoryBuffer()\n        p7.write_der(tmp)\n        p7 = s.encrypt(tmp, flags=SMIME.PKCS7_BINARY)\n        out = BIO.MemoryBuffer()\n        p7.write(out)\n        return out.read().decode()", "response": "Use your key thing to encrypt things."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a time suitable for PayPal time fields.", "response": "def paypal_time(time_obj=None):\n    \"\"\"Returns a time suitable for PayPal time fields.\"\"\"\n    warn_untested()\n    if time_obj is None:\n        time_obj = time.gmtime()\n    return time.strftime(PayPalNVP.TIMESTAMP_FORMAT, time_obj)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef paypaltime2datetime(s):\n    naive = datetime.datetime.strptime(s, PayPalNVP.TIMESTAMP_FORMAT)\n    if not settings.USE_TZ:\n        return naive\n    else:\n        # TIMESTAMP_FORMAT is UTC\n        return timezone.make_aware(naive, timezone.utc)", "response": "Convert a PayPal time string to a DateTime."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the PayPal Express Checkout endpoint for a token.", "response": "def express_endpoint_for_token(token, commit=False):\n    \"\"\"\n    Returns the PayPal Express Checkout endpoint for a token.\n    Pass 'commit=True' if you will not prompt for confirmation when the user\n    returns to your site.\n    \"\"\"\n    pp_params = dict(token=token)\n    if commit:\n        pp_params['useraction'] = 'commit'\n    return express_endpoint() % urlencode(pp_params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstripping the port from an IPv4 or IPv6 address returns a unicode object.", "response": "def strip_ip_port(ip_address):\n    \"\"\"\n    Strips the port from an IPv4 or IPv6 address, returns a unicode object.\n    \"\"\"\n\n    # IPv4 with or without port\n    if '.' in ip_address:\n        cleaned_ip = ip_address.split(':')[0]\n\n    # IPv6 with port\n    elif ']:' in ip_address:\n        # Remove the port following last ':', and then strip first and last chars for [].\n        cleaned_ip = ip_address.rpartition(':')[0][1:-1]\n\n    # IPv6 without port\n    else:\n        cleaned_ip = ip_address\n\n    return cleaned_ip"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalling PayPal DoDirectPayment method.", "response": "def doDirectPayment(self, params):\n        \"\"\"Call PayPal DoDirectPayment method.\"\"\"\n        defaults = {\"method\": \"DoDirectPayment\", \"paymentaction\": \"Sale\"}\n        required = [\"creditcardtype\",\n                    \"acct\",\n                    \"expdate\",\n                    \"cvv2\",\n                    \"ipaddress\",\n                    \"firstname\",\n                    \"lastname\",\n                    \"street\",\n                    \"city\",\n                    \"state\",\n                    \"countrycode\",\n                    \"zip\",\n                    \"amt\",\n                    ]\n        nvp_obj = self._fetch(params, required, defaults)\n        if nvp_obj.flag:\n            raise PayPalFailure(nvp_obj.flag_info, nvp=nvp_obj)\n        # @@@ Could check cvv2match / avscode are both 'X' or '0'\n        # qd = django.http.QueryDict(nvp_obj.response)\n        # if qd.get('cvv2match') not in ['X', '0']:\n        #   nvp_obj.set_flag(\"Invalid cvv2match: %s\" % qd.get('cvv2match')\n        # if qd.get('avscode') not in ['X', '0']:\n        #   nvp_obj.set_flag(\"Invalid avscode: %s\" % qd.get('avscode')\n        return nvp_obj"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setExpressCheckout(self, params):\n        if self._is_recurring(params):\n            params = self._recurring_setExpressCheckout_adapter(params)\n\n        defaults = {\"method\": \"SetExpressCheckout\", \"noshipping\": 1}\n        required = [\"returnurl\", \"cancelurl\", \"paymentrequest_0_amt\"]\n        nvp_obj = self._fetch(params, required, defaults)\n        if nvp_obj.flag:\n            raise PayPalFailure(nvp_obj.flag_info, nvp=nvp_obj)\n        return nvp_obj", "response": "Sets up an Express Checkout transaction."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a recurring payment profile.", "response": "def createRecurringPaymentsProfile(self, params, direct=False):\n        \"\"\"\n        Set direct to True to indicate that this is being called as a directPayment.\n        Returns True PayPal successfully creates the profile otherwise False.\n        \"\"\"\n        defaults = {\"method\": \"CreateRecurringPaymentsProfile\"}\n        required = [\"profilestartdate\", \"billingperiod\", \"billingfrequency\", \"amt\"]\n\n        # Direct payments require CC data\n        if direct:\n            required + [\"creditcardtype\", \"acct\", \"expdate\", \"firstname\", \"lastname\"]\n        else:\n            required + [\"token\", \"payerid\"]\n\n        nvp_obj = self._fetch(params, required, defaults)\n\n        # Flag if profile_type != ActiveProfile\n        if nvp_obj.flag:\n            raise PayPalFailure(nvp_obj.flag_info, nvp=nvp_obj)\n        return nvp_obj"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a NVP object for the given profileid and action.", "response": "def manangeRecurringPaymentsProfileStatus(self, params, fail_silently=False):\n        \"\"\"\n        Requires `profileid` and `action` params.\n        Action must be either \"Cancel\", \"Suspend\", or \"Reactivate\".\n        \"\"\"\n        defaults = {\"method\": \"ManageRecurringPaymentsProfileStatus\"}\n        required = [\"profileid\", \"action\"]\n\n        nvp_obj = self._fetch(params, required, defaults)\n\n        # TODO: This fail silently check should be using the error code, but its not easy to access\n        flag_info_test_string = 'Invalid profile status for cancel action; profile should be active or suspended'\n        if nvp_obj.flag and not (fail_silently and nvp_obj.flag_info == flag_info_test_string):\n            raise PayPalFailure(nvp_obj.flag_info, nvp=nvp_obj)\n        return nvp_obj"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _fetch(self, params, required, defaults):\n        defaults.update(params)\n        pp_params = self._check_and_update_params(required, defaults)\n        pp_string = self.signature + urlencode(pp_params)\n        response = self._request(pp_string)\n        response_params = self._parse_response(response)\n\n        log.debug('PayPal Request:\\n%s\\n', pprint.pformat(defaults))\n        log.debug('PayPal Response:\\n%s\\n', pprint.pformat(response_params))\n\n        # Gather all NVP parameters to pass to a new instance.\n        nvp_params = {}\n        tmpd = defaults.copy()\n        tmpd.update(response_params)\n        for k, v in tmpd.items():\n            if k in self.NVP_FIELDS:\n                nvp_params[str(k)] = v\n\n        # PayPal timestamp has to be formatted.\n        if 'timestamp' in nvp_params:\n            nvp_params['timestamp'] = paypaltime2datetime(nvp_params['timestamp'])\n\n        nvp_obj = PayPalNVP(**nvp_params)\n        nvp_obj.init(self.request, params, response_params)\n        nvp_obj.save()\n        return nvp_obj", "response": "Make the NVP request and store the response."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _request(self, data):\n        return requests.post(self.endpoint, data=data.encode(\"ascii\")).content", "response": "Moved out to make testing easier."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _check_and_update_params(self, required, params):\n        for r in required:\n            if r not in params:\n                raise PayPalError(\"Missing required param: %s\" % r)\n\n        # Upper case all the parameters for PayPal.\n        return (dict((k.upper(), v) for k, v in params.items()))", "response": "Check and update the required parameters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nturning the PayPal response into a dict", "response": "def _parse_response(self, response):\n        \"\"\"Turn the PayPal response into a dict\"\"\"\n        q = QueryDict(response, encoding='UTF-8').dict()\n        return {k.lower(): v for k, v in q.items()}"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms PayPal PDT Postback validation.", "response": "def _postback(self):\n        \"\"\"\n        Perform PayPal PDT Postback validation.\n        Sends the transaction ID and business token to PayPal which responses with\n        SUCCESS or FAILED.\n\n        \"\"\"\n        return requests.post(self.get_endpoint(),\n                             data=dict(cmd=\"_notify-synch\", at=IDENTITY_TOKEN, tx=self.tx)).content"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if a record with this transaction id exists and its payment_status has not changed.", "response": "def duplicate_txn_id(ipn_obj):\n    \"\"\"\n    Returns True if a record with this transaction id exists and its\n    payment_status has not changed.\n    This function has been completely changed from its previous implementation\n    where it used to specifically only check for a Pending->Completed\n    transition.\n\n    \"\"\"\n\n    # get latest similar transaction(s)\n    similars = (ipn_obj.__class__._default_manager\n                .filter(txn_id=ipn_obj.txn_id)\n                .exclude(id=ipn_obj.id)\n                .exclude(flag=True)\n                .order_by('-created_at')[:1])\n\n    if len(similars) > 0:\n        # we have a similar transaction, has the payment_status changed?\n        return similars[0].payment_status == ipn_obj.payment_status\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a secret for use in an EWP form or an IPN verification based on the secret_fields.", "response": "def make_secret(form_instance, secret_fields=None):\n    \"\"\"\n    Returns a secret for use in a EWP form or an IPN verification based on a\n    selection of variables in params. Should only be used with SSL.\n\n    \"\"\"\n    warn_untested()\n    # @@@ Moved here as temporary fix to avoid dependancy on auth.models.\n    # @@@ amount is mc_gross on the IPN - where should mapping logic go?\n    # @@@ amount / mc_gross is not nessecarily returned as it was sent - how to use it? 10.00 vs. 10.0\n    # @@@ the secret should be based on the invoice or custom fields as well - otherwise its always the same.\n\n    # Build the secret with fields availible in both PaymentForm and the IPN. Order matters.\n    if secret_fields is None:\n        secret_fields = ['business', 'item_name']\n\n    data = \"\"\n    for name in secret_fields:\n        if hasattr(form_instance, 'cleaned_data'):\n            if name in form_instance.cleaned_data:\n                data += unicode(form_instance.cleaned_data[name])\n        else:\n            # Initial data passed into the constructor overrides defaults.\n            if name in form_instance.initial:\n                data += unicode(form_instance.initial[name])\n            elif name in form_instance.fields and form_instance.fields[name].initial is not None:\n                data += unicode(form_instance.fields[name].initial)\n\n    secret = get_sha1_hexdigest(settings.SECRET_KEY, data)\n    return secret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_pdt(request):\n\n    pdt_obj = None\n    txn_id = request.GET.get('tx')\n    failed = False\n    if txn_id is not None:\n        # If an existing transaction with the id tx exists: use it\n        try:\n            pdt_obj = PayPalPDT.objects.get(txn_id=txn_id)\n        except PayPalPDT.DoesNotExist:\n            # This is a new transaction so we continue processing PDT request\n            pass\n\n        if pdt_obj is None:\n            form = PayPalPDTForm(request.GET)\n            if form.is_valid():\n                try:\n                    pdt_obj = form.save(commit=False)\n                except Exception as e:\n                    warn_untested()\n                    error = repr(e)\n                    failed = True\n            else:\n                warn_untested()\n                error = form.errors\n                failed = True\n\n            if failed:\n                warn_untested()\n                pdt_obj = PayPalPDT()\n                pdt_obj.set_flag(\"Invalid form. %s\" % error)\n\n            pdt_obj.initialize(request)\n\n            if not failed:\n                # The PDT object gets saved during verify\n                pdt_obj.verify()\n    else:\n        pass  # we ignore any PDT requests that don't have a transaction id\n\n    return (pdt_obj, failed)", "response": "This function processes the PDT request and returns the object that is returned by PayPalPDT."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntrue if there is at least one digit in number.", "response": "def is_number(self):\n        \"\"\"True if there is at least one digit in number.\"\"\"\n        self.number = re.sub(r'[^\\d]', '', self.number)\n        return self.number.isdigit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_mod10(self):\n        double = 0\n        total = 0\n        for i in range(len(self.number) - 1, -1, -1):\n            for c in str((double + 1) * int(self.number[i])):\n                total = total + int(c)\n            double = (double + 1) % 2\n        return (total % 10) == 0", "response": "Returns True if number is valid according to mod10."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_type(self):\n        for card, pattern in CARDS.items():\n            if pattern.match(self.number):\n                return card\n        return None", "response": "Return the type of the item if it matches one of the cards."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef verify(self):\n        if self.is_number() and not self.is_test() and self.is_mod10():\n            return self.get_type()\n        return None", "response": "Returns the type if valid else None."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nraising a ValidationError if the card is not valid and stashes card type.", "response": "def clean(self, value):\n        \"\"\"Raises a ValidationError if the card is not valid and stashes card type.\"\"\"\n        if value:\n            value = value.replace('-', '').replace(' ', '')\n            self.card_type = verify_credit_card(value)\n            if self.card_type is None:\n                raise forms.ValidationError(\"Invalid credit card number.\")\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the object for the current IPN.", "response": "def ipn(request):\n    \"\"\"\n    PayPal IPN endpoint (notify_url).\n    Used by both PayPal Payments Pro and Payments Standard to confirm transactions.\n    http://tinyurl.com/d9vu9d\n\n    PayPal IPN Simulator:\n    https://developer.paypal.com/cgi-bin/devscr?cmd=_ipn-link-session\n    \"\"\"\n    # TODO: Clean up code so that we don't need to set None here and have a lot\n    #       of if checks just to determine if flag is set.\n    flag = None\n    ipn_obj = None\n\n    # Avoid the RawPostDataException. See original issue for details:\n    # https://github.com/spookylukey/django-paypal/issues/79\n    if not request.META.get('CONTENT_TYPE', '').startswith(\n            'application/x-www-form-urlencoded'):\n        raise AssertionError(CONTENT_TYPE_ERROR)\n\n    logger.debug(\"PayPal incoming POST data: %s\", request.body)\n\n    # Clean up the data as PayPal sends some weird values such as \"N/A\"\n    # Also, need to cope with custom encoding, which is stored in the body (!).\n    # Assuming the tolerant parsing of QueryDict and an ASCII-like encoding,\n    # such as windows-1252, latin1 or UTF8, the following will work:\n    encoding = request.POST.get('charset', None)\n\n    encoding_missing = encoding is None\n    if encoding_missing:\n        encoding = DEFAULT_ENCODING\n\n    try:\n        data = QueryDict(request.body, encoding=encoding).copy()\n    except LookupError:\n        warn_untested()\n        data = None\n        flag = \"Invalid form - invalid charset\"\n\n    if data is not None:\n        if hasattr(PayPalIPN._meta, 'get_fields'):\n            date_fields = [f.attname for f in PayPalIPN._meta.get_fields() if f.__class__.__name__ == 'DateTimeField']\n        else:\n            date_fields = [f.attname for f, m in PayPalIPN._meta.get_fields_with_model()\n                           if f.__class__.__name__ == 'DateTimeField']\n\n        for date_field in date_fields:\n            if data.get(date_field) == 'N/A':\n                del data[date_field]\n\n        form = PayPalIPNForm(data)\n        if form.is_valid():\n            try:\n                # When commit = False, object is returned without saving to DB.\n                ipn_obj = form.save(commit=False)\n            except Exception as e:\n                flag = \"Exception while processing. (%s)\" % e\n        else:\n            formatted_form_errors = [\"{0}: {1}\".format(k, \", \".join(v)) for k, v in form.errors.items()]\n            flag = \"Invalid form. ({0})\".format(\", \".join(formatted_form_errors))\n\n    if ipn_obj is None:\n        ipn_obj = PayPalIPN()\n\n    # Set query params and sender's IP address\n    ipn_obj.initialize(request)\n\n    if flag is not None:\n        # We save errors in the flag field\n        ipn_obj.set_flag(flag)\n    else:\n        # Secrets should only be used over SSL.\n        if request.is_secure() and 'secret' in request.GET:\n            warn_untested()\n            ipn_obj.verify_secret(form, request.GET['secret'])\n        else:\n            ipn_obj.verify()\n\n    ipn_obj.save()\n    ipn_obj.send_signals()\n\n    if encoding_missing:\n        # Wait until we have an ID to log warning\n        logger.warning(\"No charset passed with PayPalIPN: %s. Guessing %s\", ipn_obj.id, encoding)\n\n    return HttpResponse(\"OKAY\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing a PayPal direct payment.", "response": "def process(self, request, item):\n        \"\"\"Process a PayPal direct payment.\"\"\"\n        warn_untested()\n        from paypal.pro.helpers import PayPalWPP\n\n        wpp = PayPalWPP(request)\n        params = self.cleaned_data\n        params['creditcardtype'] = self.fields['acct'].card_type\n        params['expdate'] = self.cleaned_data['expdate'].strftime(\"%m%Y\")\n        params['ipaddress'] = request.META.get(\"REMOTE_ADDR\", \"\")\n        params.update(item)\n\n        try:\n            # Create single payment:\n            if 'billingperiod' not in params:\n                wpp.doDirectPayment(params)\n            # Create recurring payment:\n            else:\n                wpp.createRecurringPaymentsProfile(params, direct=True)\n        except PayPalFailure:\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nperforming PayPal Postback validation.", "response": "def _postback(self):\n        \"\"\"Perform PayPal Postback validation.\"\"\"\n        return requests.post(self.get_endpoint(), data=b\"cmd=_notify-validate&\" + self.query.encode(\"ascii\")).content"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending signals to the user that a txn was successful.", "response": "def send_signals(self):\n        \"\"\"Shout for the world to hear whether a txn was successful.\"\"\"\n        if self.flag:\n            invalid_ipn_received.send(sender=self)\n            return\n        else:\n            valid_ipn_received.send(sender=self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gen_columns_from_children(root):\n    if isinstance(root, (Delete, BinaryExpression, BooleanClauseList)):\n        for child in root.get_children():\n            yc = gen_columns_from_children(child)\n            for it in yc:\n                yield it\n    elif isinstance(root, sa.Column):\n        yield root", "response": "Generates the columns that are being used in child elements of the delete query\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef visit_delete_stmt(element, compiler, **kwargs):\n\n    # Set empty strings for the default where clause and using clause\n    whereclause = ''\n    usingclause = ''\n\n    # determine if the delete query needs a ``USING`` injected\n    # by inspecting the whereclause's children & their children...\n    # first, the where clause text is buit, if applicable\n    # then, the using clause text is built, if applicable\n    # note:\n    #   the tables in the using clause are sorted in the order in\n    #   which they first appear in the where clause.\n    delete_stmt_table = compiler.process(element.table, asfrom=True, **kwargs)\n    whereclause_tuple = element.get_children()\n    if whereclause_tuple:\n        usingclause_tables = []\n        whereclause = ' WHERE {clause}'.format(\n            clause=compiler.process(*whereclause_tuple, **kwargs)\n        )\n\n        whereclause_columns = gen_columns_from_children(element)\n        for col in whereclause_columns:\n            table = compiler.process(col.table, asfrom=True, **kwargs)\n            if table != delete_stmt_table and table not in usingclause_tables:\n                usingclause_tables.append(table)\n        if usingclause_tables:\n            usingclause = ' USING {clause}'.format(\n                clause=', '.join(usingclause_tables)\n            )\n\n    return 'DELETE FROM {table}{using}{where}'.format(\n        table=delete_stmt_table,\n        using=usingclause,\n        where=whereclause)", "response": "A basic delete statement that is a SqlAlchemy table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the name without double quotes removed.", "response": "def unquoted(self):\n        \"\"\"\n        Return *key* with one level of double quotes removed.\n\n        Redshift stores some identifiers without quotes in internal tables,\n        even though the name must be quoted elsewhere.\n        In particular, this happens for tables named as a keyword.\n        \"\"\"\n        key = str(self)\n        if key.startswith('\"') and key.endswith('\"'):\n            return key[1:-1]\n        return key"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_columns(self, connection, table_name, schema=None, **kw):\n        cols = self._get_redshift_columns(connection, table_name, schema, **kw)\n        if not self._domains:\n            self._domains = self._load_domains(connection)\n        domains = self._domains\n        columns = []\n        for col in cols:\n            column_info = self._get_column_info(\n                name=col.name, format_type=col.format_type,\n                default=col.default, notnull=col.notnull, domains=domains,\n                enums=[], schema=col.schema, encode=col.encode)\n            columns.append(column_info)\n        return columns", "response": "Returns information about columns in table_name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_pk_constraint(self, connection, table_name, schema=None, **kw):\n        constraints = self._get_redshift_constraints(connection, table_name,\n                                                     schema, **kw)\n        pk_constraints = [c for c in constraints if c.contype == 'p']\n        if not pk_constraints:\n            return {'constrained_columns': [], 'name': ''}\n        pk_constraint = pk_constraints[0]\n        m = PRIMARY_KEY_RE.match(pk_constraint.condef)\n        colstring = m.group('columns')\n        constrained_columns = SQL_IDENTIFIER_RE.findall(colstring)\n        return {\n            'constrained_columns': constrained_columns,\n            'name': pk_constraint.conname,\n        }", "response": "Returns information about the primary key constraint on table_name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_foreign_keys(self, connection, table_name, schema=None, **kw):\n        constraints = self._get_redshift_constraints(connection, table_name,\n                                                     schema, **kw)\n        fk_constraints = [c for c in constraints if c.contype == 'f']\n        uniques = defaultdict(lambda: defaultdict(dict))\n        for con in fk_constraints:\n            uniques[con.conname][\"key\"] = con.conkey\n            uniques[con.conname][\"condef\"] = con.condef\n        fkeys = []\n        for conname, attrs in uniques.items():\n            m = FOREIGN_KEY_RE.match(attrs['condef'])\n            colstring = m.group('referred_columns')\n            referred_columns = SQL_IDENTIFIER_RE.findall(colstring)\n            referred_table = m.group('referred_table')\n            referred_schema = m.group('referred_schema')\n            colstring = m.group('columns')\n            constrained_columns = SQL_IDENTIFIER_RE.findall(colstring)\n            fkey_d = {\n                'name': conname,\n                'constrained_columns': constrained_columns,\n                'referred_schema': referred_schema,\n                'referred_table': referred_table,\n                'referred_columns': referred_columns,\n            }\n            fkeys.append(fkey_d)\n        return fkeys", "response": "Returns information about foreign keys in table_name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of table names for schema.", "response": "def get_table_names(self, connection, schema=None, **kw):\n        \"\"\"\n        Return a list of table names for `schema`.\n\n        Overrides interface\n        :meth:`~sqlalchemy.engine.interfaces.Dialect.get_table_names`.\n        \"\"\"\n        return self._get_table_or_view_names('r', connection, schema, **kw)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a list of view names for schema.", "response": "def get_view_names(self, connection, schema=None, **kw):\n        \"\"\"\n        Return a list of view names for `schema`.\n\n        Overrides interface\n        :meth:`~sqlalchemy.engine.interfaces.Dialect.get_view_names`.\n        \"\"\"\n        return self._get_table_or_view_names('v', connection, schema, **kw)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the view definition. Given a : class :. Connection a string view_name and an optional string schema return the view definition.", "response": "def get_view_definition(self, connection, view_name, schema=None, **kw):\n        \"\"\"Return view definition.\n        Given a :class:`.Connection`, a string `view_name`,\n        and an optional string `schema`, return the view definition.\n\n        Overrides interface\n        :meth:`~sqlalchemy.engine.interfaces.Dialect.get_view_definition`.\n        \"\"\"\n        view = self._get_redshift_relation(connection, view_name, schema, **kw)\n        return sa.text(view.view_definition)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_unique_constraints(self, connection, table_name,\n                               schema=None, **kw):\n        \"\"\"\n        Return information about unique constraints in `table_name`.\n\n        Overrides interface\n        :meth:`~sqlalchemy.engine.interfaces.Dialect.get_unique_constraints`.\n        \"\"\"\n        constraints = self._get_redshift_constraints(connection,\n                                                     table_name, schema, **kw)\n        constraints = [c for c in constraints if c.contype == 'u']\n        uniques = defaultdict(lambda: defaultdict(dict))\n        for con in constraints:\n            uniques[con.conname][\"key\"] = con.conkey\n            uniques[con.conname][\"cols\"][con.attnum] = con.attname\n\n        return [\n            {'name': None,\n             'column_names': [uc[\"cols\"][i] for i in uc[\"key\"]]}\n            for name, uc in uniques.items()\n        ]", "response": "Returns information about unique constraints in table_name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dictionary of options specified when the table of the given name was created.", "response": "def get_table_options(self, connection, table_name, schema, **kw):\n        \"\"\"\n        Return a dictionary of options specified when the table of the\n        given name was created.\n\n        Overrides interface\n        :meth:`~sqlalchemy.engine.Inspector.get_table_options`.\n        \"\"\"\n        def keyfunc(column):\n            num = int(column.sortkey)\n            # If sortkey is interleaved, column numbers alternate\n            # negative values, so take abs.\n            return abs(num)\n        table = self._get_redshift_relation(connection, table_name,\n                                            schema, **kw)\n        columns = self._get_redshift_columns(connection, table_name,\n                                             schema, **kw)\n        sortkey_cols = sorted([col for col in columns if col.sortkey],\n                              key=keyfunc)\n        interleaved = any([int(col.sortkey) < 0 for col in sortkey_cols])\n        sortkey = [col.name for col in sortkey_cols]\n        interleaved_sortkey = None\n        if interleaved:\n            interleaved_sortkey = sortkey\n            sortkey = None\n        distkeys = [col.name for col in columns if col.distkey]\n        distkey = distkeys[0] if distkeys else None\n        return {\n            'redshift_diststyle': table.diststyle,\n            'redshift_distkey': distkey,\n            'redshift_sortkey': sortkey,\n            'redshift_interleaved_sortkey': interleaved_sortkey,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_connect_args(self, *args, **kwargs):\n        default_args = {\n            'sslmode': 'verify-full',\n            'sslrootcert': pkg_resources.resource_filename(\n                __name__,\n                'redshift-ca-bundle.crt'\n            ),\n        }\n        cargs, cparams = super(RedshiftDialect, self).create_connect_args(\n            *args, **kwargs\n        )\n        default_args.update(cparams)\n        return cargs, default_args", "response": "Build DB - API compatible connection arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef visit_unload_from_select(element, compiler, **kw):\n\n    template = \"\"\"\n       UNLOAD (:select) TO :unload_location\n       CREDENTIALS :credentials\n       {manifest}\n       {header}\n       {delimiter}\n       {encrypted}\n       {fixed_width}\n       {gzip}\n       {add_quotes}\n       {null}\n       {escape}\n       {allow_overwrite}\n       {parallel}\n       {region}\n       {max_file_size}\n    \"\"\"\n    el = element\n\n    qs = template.format(\n        manifest='MANIFEST' if el.manifest else '',\n        header='HEADER' if el.header else '',\n        delimiter=(\n            'DELIMITER AS :delimiter' if el.delimiter is not None else ''\n        ),\n        encrypted='ENCRYPTED' if el.encrypted else '',\n        fixed_width='FIXEDWIDTH AS :fixed_width' if el.fixed_width else '',\n        gzip='GZIP' if el.gzip else '',\n        add_quotes='ADDQUOTES' if el.add_quotes else '',\n        escape='ESCAPE' if el.escape else '',\n        null='NULL AS :null_as' if el.null is not None else '',\n        allow_overwrite='ALLOWOVERWRITE' if el.allow_overwrite else '',\n        parallel='PARALLEL OFF' if not el.parallel else '',\n        region='REGION :region' if el.region is not None else '',\n        max_file_size=(\n            'MAXFILESIZE :max_file_size MB'\n            if el.max_file_size is not None else ''\n        ),\n    )\n\n    query = sa.text(qs)\n\n    if el.delimiter is not None:\n        query = query.bindparams(sa.bindparam(\n            'delimiter', value=element.delimiter, type_=sa.String,\n        ))\n\n    if el.fixed_width:\n        query = query.bindparams(sa.bindparam(\n            'fixed_width',\n            value=_process_fixed_width(el.fixed_width),\n            type_=sa.String,\n        ))\n\n    if el.null is not None:\n        query = query.bindparams(sa.bindparam(\n            'null_as', value=el.null, type_=sa.String\n        ))\n\n    if el.region is not None:\n        query = query.bindparams(sa.bindparam(\n            'region', value=el.region, type_=sa.String\n        ))\n\n    if el.max_file_size is not None:\n        max_file_size_mib = float(el.max_file_size) / 1024 / 1024\n        query = query.bindparams(sa.bindparam(\n            'max_file_size', value=max_file_size_mib, type_=sa.Float\n        ))\n\n    return compiler.process(\n        query.bindparams(\n            sa.bindparam('credentials', value=el.credentials, type_=sa.String),\n            sa.bindparam(\n                'unload_location', value=el.unload_location, type_=sa.String,\n            ),\n            sa.bindparam(\n                'select',\n                value=compiler.process(\n                    el.select,\n                    literal_binds=True,\n                ),\n                type_=sa.String,\n            ),\n        ),\n        **kw\n    )", "response": "Returns the actual sql query for the UnloadFromSelect class."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the actual sql query for the CopyCommand class.", "response": "def visit_copy_command(element, compiler, **kw):\n    \"\"\"\n    Returns the actual sql query for the CopyCommand class.\n    \"\"\"\n    qs = \"\"\"COPY {table}{columns} FROM :data_location\n        WITH CREDENTIALS AS :credentials\n        {format}\n        {parameters}\"\"\"\n    parameters = []\n    bindparams = [\n        sa.bindparam(\n            'data_location',\n            value=element.data_location,\n            type_=sa.String,\n        ),\n        sa.bindparam(\n            'credentials',\n            value=element.credentials,\n            type_=sa.String,\n        ),\n    ]\n\n    if element.format == Format.csv:\n        format_ = 'FORMAT AS CSV'\n        if element.quote is not None:\n            format_ += ' QUOTE AS :quote_character'\n            bindparams.append(sa.bindparam(\n                'quote_character',\n                value=element.quote,\n                type_=sa.String,\n            ))\n    elif element.format == Format.json:\n        format_ = 'FORMAT AS JSON AS :json_option'\n        bindparams.append(sa.bindparam(\n            'json_option',\n            value=element.path_file,\n            type_=sa.String,\n        ))\n    elif element.format == Format.avro:\n        format_ = 'FORMAT AS AVRO AS :avro_option'\n        bindparams.append(sa.bindparam(\n            'avro_option',\n            value=element.path_file,\n            type_=sa.String,\n        ))\n    elif element.format == Format.orc:\n        format_ = 'FORMAT AS ORC'\n    elif element.format == Format.parquet:\n        format_ = 'FORMAT AS PARQUET'\n    elif element.format == Format.fixed_width and element.fixed_width is None:\n        raise sa_exc.CompileError(\n            \"'fixed_width' argument required for format 'FIXEDWIDTH'.\")\n    else:\n        format_ = ''\n\n    if element.delimiter is not None:\n        parameters.append('DELIMITER AS :delimiter_char')\n        bindparams.append(sa.bindparam(\n            'delimiter_char',\n            value=element.delimiter,\n            type_=sa.String,\n        ))\n\n    if element.fixed_width is not None:\n        parameters.append('FIXEDWIDTH AS :fixedwidth_spec')\n        bindparams.append(sa.bindparam(\n            'fixedwidth_spec',\n            value=_process_fixed_width(element.fixed_width),\n            type_=sa.String,\n        ))\n\n    if element.compression is not None:\n        parameters.append(Compression(element.compression).value)\n\n    if element.manifest:\n        parameters.append('MANIFEST')\n\n    if element.accept_any_date:\n        parameters.append('ACCEPTANYDATE')\n\n    if element.accept_inv_chars is not None:\n        parameters.append('ACCEPTINVCHARS AS :replacement_char')\n        bindparams.append(sa.bindparam(\n            'replacement_char',\n            value=element.accept_inv_chars,\n            type_=sa.String\n        ))\n\n    if element.blanks_as_null:\n        parameters.append('BLANKSASNULL')\n\n    if element.date_format is not None:\n        parameters.append('DATEFORMAT AS :dateformat_string')\n        bindparams.append(sa.bindparam(\n            'dateformat_string',\n            value=element.date_format,\n            type_=sa.String,\n        ))\n\n    if element.empty_as_null:\n        parameters.append('EMPTYASNULL')\n\n    if element.encoding is not None:\n        parameters.append('ENCODING AS ' + Encoding(element.encoding).value)\n\n    if element.escape:\n        parameters.append('ESCAPE')\n\n    if element.explicit_ids:\n        parameters.append('EXPLICIT_IDS')\n\n    if element.fill_record:\n        parameters.append('FILLRECORD')\n\n    if element.ignore_blank_lines:\n        parameters.append('IGNOREBLANKLINES')\n\n    if element.ignore_header is not None:\n        parameters.append('IGNOREHEADER AS :number_rows')\n        bindparams.append(sa.bindparam(\n            'number_rows',\n            value=element.ignore_header,\n            type_=sa.Integer,\n        ))\n\n    if element.dangerous_null_delimiter is not None:\n        parameters.append(\"NULL AS '%s'\" % element.dangerous_null_delimiter)\n\n    if element.remove_quotes:\n        parameters.append('REMOVEQUOTES')\n\n    if element.roundec:\n        parameters.append('ROUNDEC')\n\n    if element.time_format is not None:\n        parameters.append('TIMEFORMAT AS :timeformat_string')\n        bindparams.append(sa.bindparam(\n            'timeformat_string',\n            value=element.time_format,\n            type_=sa.String,\n        ))\n\n    if element.trim_blanks:\n        parameters.append('TRIMBLANKS')\n\n    if element.truncate_columns:\n        parameters.append('TRUNCATECOLUMNS')\n\n    if element.comp_rows:\n        parameters.append('COMPROWS :numrows')\n        bindparams.append(sa.bindparam(\n            'numrows',\n            value=element.comp_rows,\n            type_=sa.Integer,\n        ))\n\n    if element.comp_update:\n        parameters.append('COMPUPDATE ON')\n    elif element.comp_update is not None:\n        parameters.append('COMPUPDATE OFF')\n\n    if element.max_error is not None:\n        parameters.append('MAXERROR AS :error_count')\n        bindparams.append(sa.bindparam(\n            'error_count',\n            value=element.max_error,\n            type_=sa.Integer,\n        ))\n\n    if element.no_load:\n        parameters.append('NOLOAD')\n\n    if element.stat_update:\n        parameters.append('STATUPDATE ON')\n    elif element.stat_update is not None:\n        parameters.append('STATUPDATE OFF')\n\n    if element.region is not None:\n        parameters.append('REGION :region')\n        bindparams.append(sa.bindparam(\n            'region',\n            value=element.region,\n            type_=sa.String\n        ))\n\n    columns = ' (%s)' % ', '.join(\n        compiler.preparer.format_column(column) for column in element.columns\n    ) if element.columns else ''\n\n    qs = qs.format(\n        table=compiler.preparer.format_table(element.table),\n        columns=columns,\n        format=format_,\n        parameters='\\n'.join(parameters)\n    )\n\n    return compiler.process(sa.text(qs).bindparams(*bindparams), **kw)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef visit_create_library_command(element, compiler, **kw):\n    query = \"\"\"\n        CREATE {or_replace} LIBRARY {name}\n        LANGUAGE pythonplu\n        FROM :location\n        WITH CREDENTIALS AS :credentials\n        {region}\n    \"\"\"\n    bindparams = [\n        sa.bindparam(\n            'location',\n            value=element.location,\n            type_=sa.String,\n        ),\n        sa.bindparam(\n            'credentials',\n            value=element.credentials,\n            type_=sa.String,\n        ),\n    ]\n\n    if element.region is not None:\n        bindparams.append(sa.bindparam(\n            'region',\n            value=element.region,\n            type_=sa.String,\n        ))\n\n    quoted_lib_name = compiler.preparer.quote_identifier(element.library_name)\n    query = query.format(name=quoted_lib_name,\n                         or_replace='OR REPLACE' if element.replace else '',\n                         region='REGION :region' if element.region else '')\n    return compiler.process(sa.text(query).bindparams(*bindparams), **kw)", "response": "Returns the actual sql query for the CreateLibraryCommand class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread the pacfile and evaluates it in the javascript engine created by . init.", "response": "def parse_pac_file(pacfile):\n  \"\"\"\n  Reads the pacfile and evaluates it in the Javascript engine created by\n  init().\n  \"\"\"\n  try:\n    with open(pacfile) as f:\n      pac_script = f.read()\n      _pacparser.parse_pac_string(pac_script)\n  except IOError:\n    raise IOError('Could not read the pacfile: {}'.format(pacfile))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind the proxy string for the given url and host.", "response": "def find_proxy(url, host=None):\n  \"\"\"\n  Finds proxy string for the given url and host. If host is not\n  defined, it's extracted from the url.\n  \"\"\"\n  if host is None:\n    m = _URL_REGEX.match(url)\n    if not m:\n      raise URLError(url)\n    if len(m.groups()) is 1:\n      host = m.groups()[0]\n    else:\n      raise URLError(url)\n  return _pacparser.find_proxy(url, host)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef just_find_proxy(pacfile, url, host=None):\n  if not os.path.isfile(pacfile):\n    raise IOError('Pac file does not exist: {}'.format(pacfile))\n  init()\n  parse_pac(pacfile)\n  proxy = find_proxy(url,host)\n  cleanup()\n  return proxy", "response": "This function is a wrapper around init parse_pac find_proxy and cleanup."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef split_data(iterable, pred):\n    yes, no = [], []\n    for d in iterable:\n        if pred(d):\n            yes.append(d)\n        else:\n            no.append(d)\n    return yes, no", "response": "Split data from iterable into two lists."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nyield a list of all possible domain variants.", "response": "def _domain_variants(domain):\n    \"\"\"\n    >>> list(_domain_variants(\"foo.bar.example.com\"))\n    ['foo.bar.example.com', 'bar.example.com', 'example.com']\n    >>> list(_domain_variants(\"example.com\"))\n    ['example.com']\n    >>> list(_domain_variants(\"localhost\"))\n    ['localhost']\n    \"\"\"\n    parts = domain.split('.')\n    if len(parts) == 1:\n        yield parts[0]\n    else:\n        for i in range(len(parts), 1, -1):\n            yield \".\".join(parts[-i:])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a compiled regex combined from a list of regexes.", "response": "def _combined_regex(regexes, flags=re.IGNORECASE, use_re2=False, max_mem=None):\n    \"\"\"\n    Return a compiled regex combined (using OR) from a list of ``regexes``.\n    If there is nothing to combine, None is returned.\n\n    re2 library (https://github.com/axiak/pyre2) often can match and compile\n    large regexes much faster than stdlib re module (10x is not uncommon),\n    but there are some gotchas:\n\n    * in case of \"DFA out of memory\" errors use ``max_mem`` argument\n      to increase the amount of memory re2 is allowed to use.\n    \"\"\"\n    joined_regexes = \"|\".join(r for r in regexes if r)\n    if not joined_regexes:\n        return None\n\n    if use_re2:\n        import re2\n        return re2.compile(joined_regexes, flags=flags, max_mem=max_mem)\n    return re.compile(joined_regexes, flags=flags)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef match_url(self, url, options=None):\n        options = options or {}\n        for optname in self.options:\n            if optname == 'match-case':  # TODO\n                continue\n\n            if optname not in options:\n                raise ValueError(\"Rule requires option %s\" % optname)\n\n            if optname == 'domain':\n                if not self._domain_matches(options['domain']):\n                    return False\n                continue\n\n            if options[optname] != self.options[optname]:\n                return False\n\n        return self._url_matches(url)", "response": "Return True if this rule matches the URL."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef matching_supported(self, options=None):\n        if self.is_comment:\n            return False\n\n        if self.is_html_rule:  # HTML rules are not supported yet\n            return False\n\n        options = options or {}\n        keys = set(options.keys())\n        if not keys.issuperset(self._options_keys):\n            # some of the required options are not given\n            return False\n\n        return True", "response": "Return whether this rule can return meaningful result False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rule_to_regex(cls, rule):\n        if not rule:\n            return rule\n\n        # Check if the rule isn't already regexp\n        if rule.startswith('/') and rule.endswith('/'):\n            if len(rule) > 1:\n                rule = rule[1:-1]\n            else:\n                raise AdblockParsingError('Invalid rule')\n            return rule\n\n        # escape special regex characters\n        rule = re.sub(r\"([.$+?{}()\\[\\]\\\\])\", r\"\\\\\\1\", rule)\n\n        # XXX: the resulting regex must use non-capturing groups (?:\n        # for performance reasons; also, there is a limit on number\n        # of capturing groups, no using them would prevent building\n        # a single regex out of several rules.\n\n        # Separator character ^ matches anything but a letter, a digit, or\n        # one of the following: _ - . %. The end of the address is also\n        # accepted as separator.\n        rule = rule.replace(\"^\", \"(?:[^\\w\\d_\\-.%]|$)\")\n\n        # * symbol\n        rule = rule.replace(\"*\", \".*\")\n\n        # | in the end means the end of the address\n        if rule[-1] == '|':\n            rule = rule[:-1] + '$'\n\n        # || in the beginning means beginning of the domain name\n        if rule[:2] == '||':\n            # XXX: it is better to use urlparse for such things,\n            # but urlparse doesn't give us a single regex.\n            # Regex is based on http://tools.ietf.org/html/rfc3986#appendix-B\n            if len(rule) > 2:\n                #          |            | complete part     |\n                #          |  scheme    | of the domain     |\n                rule = r\"^(?:[^:/?#]+:)?(?://(?:[^/?#]*\\.)?)?\" + rule[2:]\n\n        elif rule[0] == '|':\n            # | in the beginning means start of the address\n            rule = '^' + rule[1:]\n\n        # other | symbols should be escaped\n        # we have \"|$\" in our regexp - do not touch it\n        rule = re.sub(\"(\\|)[^$]\", r\"\\|\", rule)\n\n        return rule", "response": "Convert an AdBlock rule to a regular expression."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _matches(self, url, options,\n                 general_re, domain_required_rules, rules_with_options):\n        \"\"\"\n        Return if ``url``/``options`` are matched by rules defined by\n        ``general_re``, ``domain_required_rules`` and ``rules_with_options``.\n\n        ``general_re`` is a compiled regex for rules without options.\n\n        ``domain_required_rules`` is a {domain: [rules_which_require_it]}\n        mapping.\n\n         ``rules_with_options`` is a list of AdblockRule instances that\n        don't require any domain, but have other options.\n        \"\"\"\n        if general_re and general_re.search(url):\n            return True\n\n        rules = []\n        if 'domain' in options and domain_required_rules:\n            src_domain = options['domain']\n            for domain in _domain_variants(src_domain):\n                if domain in domain_required_rules:\n                    rules.extend(domain_required_rules[domain])\n\n        rules.extend(rules_with_options)\n\n        if self.skip_unsupported_rules:\n            rules = [rule for rule in rules if rule.matching_supported(options)]\n\n        return any(rule.match_url(url, options) for rule in rules)", "response": "Return True if url and options match Adblock rules."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef seconds_to_str_fromatter(_format):\n    # check directives are correct \n    \n    if _format == \"%S\":\n        def _fromatter(seconds):\n            return \"{:.2f}\".format(seconds)\n    \n    elif _format == \"%I\":\n        def _fromatter(seconds):\n            return \"{0}\".format(int(seconds * 1000))\n    \n    else:\n        _format = _format.replace(\"%h\", \"{hrs:02d}\")\n        _format = _format.replace(\"%m\", \"{mins:02d}\")\n        _format = _format.replace(\"%s\", \"{secs:02d}\")\n        _format = _format.replace(\"%i\", \"{millis:03d}\")\n        \n        try:\n            i = _format.index(\"%\")\n            raise TimeFormatError(\"Unknow time format directive '{0}'\".format(_format[i:i+2]))\n        except ValueError:\n            pass\n        \n        def _fromatter(seconds):\n            millis = int(seconds * 1000)\n            hrs, millis = divmod(millis, 3600000)\n            mins, millis = divmod(millis, 60000)\n            secs, millis = divmod(millis, 1000)\n            return _format.format(hrs=hrs, mins=mins, secs=secs, millis=millis)\n    \n    return _fromatter", "response": "Convert a number of seconds into a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the mode of the tokenization.", "response": "def set_mode(self, mode):\n        \"\"\"\n        :Parameters:\n\n            `mode` : *(int)*\n                New mode, must be one of:\n\n\n            - `StreamTokenizer.STRICT_MIN_LENGTH`\n\n            - `StreamTokenizer.DROP_TRAILING_SILENCE`\n\n            - `StreamTokenizer.STRICT_MIN_LENGTH | StreamTokenizer.DROP_TRAILING_SILENCE`\n\n            - `0`\n\n        See `StreamTokenizer.__init__` for more information about the mode.\n        \"\"\"\n\n        if not mode in [self.STRICT_MIN_LENGTH, self.DROP_TRAILING_SILENCE,\n                        self.STRICT_MIN_LENGTH | self.DROP_TRAILING_SILENCE, 0]:\n\n            raise ValueError(\"Wrong value for mode\")\n\n        self._mode = mode\n        self._strict_min_length = (mode & self.STRICT_MIN_LENGTH) != 0\n        self._drop_tailing_silence = (mode & self.DROP_TRAILING_SILENCE) != 0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tokenize(self, data_source, callback=None):\n\n        self._reinitialize()\n\n        if callback is not None:\n            self._deliver = callback\n\n        while True:\n            frame = data_source.read()\n            if frame is None:\n                break\n            self._current_frame += 1\n            self._process(frame)\n\n        self._post_process()\n\n        if callback is None:\n            _ret = self._tokens\n            self._tokens = None\n            return _ret", "response": "Read data from data_source and process the frames in the order they make up valid tokens."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read(self):\n\n        if self._current >= len(self._data):\n            return None\n        self._current += 1\n        return self._data[self._current - 1]", "response": "Reads one character from the buffer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_data(self, data):\n\n        if not isinstance(data, basestring):\n            raise ValueError(\"data must an instance of basestring\")\n        self._data = data\n        self._current = 0", "response": "Set a new data buffer."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an ADS object for the given microphone.", "response": "def ads(**kwargs):\n        \"\"\"\n        Create an return an :class:`ADSFactory.AudioDataSource`. The type and behavior of the object is the result\n        of the supplied parameters.\n\n        :Parameters:\n\n        *No parameters* :  \n           read audio data from the available built-in microphone with the default parameters.\n           The returned :class:`ADSFactory.AudioDataSource` encapsulate an :class:`io.PyAudioSource` object and hence \n           it accepts the next four parameters are passed to use instead of their default values.\n\n        `sampling_rate`, `sr` : *(int)*\n            number of samples per second. Default = 16000.\n\n        `sample_width`, `sw` : *(int)*\n            number of bytes per sample (must be in (1, 2, 4)). Default = 2\n\n        `channels`, `ch` : *(int)*\n            number of audio channels. Default = 1 (only this value is currently accepted)  \n\n        `frames_per_buffer`, `fpb` : *(int)*\n            number of samples of PyAudio buffer. Default = 1024.\n\n        `audio_source`, `asrc` : an `AudioSource` object\n            read data from this audio source\n\n        `filename`, `fn` : *(string)*\n            build an `io.AudioSource` object using this file (currently only wave format is supported)\n\n        `data_buffer`, `db` : *(string)*\n            build an `io.BufferAudioSource` using data in `data_buffer`. If this keyword is used,\n            `sampling_rate`, `sample_width` and `channels` are passed to `io.BufferAudioSource`\n            constructor and used instead of default values.\n\n        `max_time`, `mt` : *(float)*\n            maximum time (in seconds) to read. Default behavior: read until there is no more data\n            available. \n\n        `record`, `rec` : *(bool)*\n            save all read data in cache. Provide a navigable object which boasts a `rewind` method.\n            Default = False.\n\n        `block_dur`, `bd` : *(float)*\n            processing block duration in seconds. This represents the quantity of audio data to return \n            each time the :func:`read` method is invoked. If `block_dur` is 0.025 (i.e. 25 ms) and the sampling\n            rate is 8000 and the sample width is 2 bytes, :func:`read` returns a buffer of 0.025 * 8000 * 2 = 400\n            bytes at most. This parameter will be looked for (and used if available) before `block_size`.\n            If neither parameter is given, `block_dur` will be set to 0.01 second (i.e. 10 ms)\n\n        `hop_dur`, `hd` : *(float)*\n            quantity of data to skip from current processing window. if `hop_dur` is supplied then there\n            will be an overlap of `block_dur` - `hop_dur` between two adjacent blocks. This\n            parameter will be looked for (and used if available) before `hop_size`. If neither parameter\n            is given, `hop_dur` will be set to `block_dur` which means that there will be no overlap\n            between two consecutively read blocks.\n\n        `block_size`, `bs` : *(int)*\n            number of samples to read each time the `read` method is called. Default: a block size\n            that represents a window of 10ms, so for a sampling rate of 16000, the default `block_size`\n            is 160 samples, for a rate of 44100, `block_size` = 441 samples, etc.\n\n        `hop_size`, `hs` : *(int)*\n            determines the number of overlapping samples between two adjacent read windows. For a\n            `hop_size` of value *N*, the overlap is `block_size` - *N*. Default : `hop_size` = `block_size`,\n            means that there is no overlap.\n\n        :Returns:\n\n        An AudioDataSource object that has the desired features.\n\n        :Exampels:\n\n        1. **Create an AudioDataSource that reads data from the microphone (requires Pyaudio) with default audio parameters:**\n\n        .. code:: python\n\n            from auditok import ADSFactory\n            ads = ADSFactory.ads()\n            ads.get_sampling_rate()\n            16000\n            ads.get_sample_width()\n            2\n            ads.get_channels()\n            1\n\n        2. **Create an AudioDataSource that reads data from the microphone with a sampling rate of 48KHz:**\n\n        .. code:: python\n\n            from auditok import ADSFactory\n            ads = ADSFactory.ads(sr=48000)\n            ads.get_sampling_rate()\n            48000\n\n        3. **Create an AudioDataSource that reads data from a wave file:**\n\n        .. code:: python\n\n            import auditok\n            from auditok import ADSFactory\n            ads = ADSFactory.ads(fn=auditok.dataset.was_der_mensch_saet_mono_44100_lead_trail_silence)\n            ads.get_sampling_rate()\n            44100\n            ads.get_sample_width()\n            2\n            ads.get_channels()\n            1\n\n        4. **Define size of read blocks as 20 ms**\n\n        .. code:: python\n\n            import auditok\n            from auditok import ADSFactory\n            '''\n            we know samling rate for previous file is 44100 samples/second\n            so 10 ms are equivalent to 441 samples and 20 ms to 882\n            '''\n            block_size = 882\n            ads = ADSFactory.ads(bs = 882, fn=auditok.dataset.was_der_mensch_saet_mono_44100_lead_trail_silence)\n            ads.open()\n            # read one block\n            data = ads.read()\n            ads.close()\n            len(data)\n            1764\n            assert len(data) ==  ads.get_sample_width() * block_size\n\n        5. **Define block size as a duration (use block_dur or bd):**\n\n        .. code:: python\n\n            import auditok\n            from auditok import ADSFactory\n            dur = 0.25 # second\n            ads = ADSFactory.ads(bd = dur, fn=auditok.dataset.was_der_mensch_saet_mono_44100_lead_trail_silence)\n            '''\n            we know samling rate for previous file is 44100 samples/second\n            for a block duration of 250 ms, block size should be 0.25 * 44100 = 11025\n            '''\n            ads.get_block_size()\n            11025\n            assert ads.get_block_size() ==  int(0.25 * 44100)\n            ads.open()\n            # read one block\n            data = ads.read()\n            ads.close()\n            len(data)\n            22050\n            assert len(data) ==  ads.get_sample_width() * ads.get_block_size()\n\n        6. **Read overlapping blocks (one of hope_size, hs, hop_dur or hd > 0):**\n\n        For better readability we'd better use :class:`auditok.io.BufferAudioSource` with a string buffer:\n\n        .. code:: python\n\n            import auditok\n            from auditok import ADSFactory\n            '''\n            we supply a data beffer instead of a file (keyword 'bata_buffer' or 'db')\n            sr : sampling rate = 16 samples/sec\n            sw : sample width = 1 byte\n            ch : channels = 1\n            '''\n            buffer = \"abcdefghijklmnop\" # 16 bytes = 1 second of data\n            bd = 0.250 # block duration = 250 ms = 4 bytes\n            hd = 0.125 # hop duration = 125 ms = 2 bytes \n            ads = ADSFactory.ads(db = \"abcdefghijklmnop\", bd = bd, hd = hd, sr = 16, sw = 1, ch = 1)\n            ads.open()\n            ads.read()\n            'abcd'\n            ads.read()\n            'cdef'\n            ads.read()\n            'efgh'\n            ads.read()\n            'ghij'\n            data = ads.read()\n            assert data == 'ijkl'\n\n        7. **Limit amount of read data (use max_time or mt):**\n\n        .. code:: python\n\n            '''\n            We know audio file is larger than 2.25 seconds\n            We want to read up to 2.25 seconds of audio data\n            '''\n            ads = ADSFactory.ads(mt = 2.25, fn=auditok.dataset.was_der_mensch_saet_mono_44100_lead_trail_silence)\n            ads.open()\n            data = []\n            while True:\n                d = ads.read()\n                if d is None:\n                    break\n                data.append(d)\n\n            ads.close()\n            data = b''.join(data)\n            assert len(data) == int(ads.get_sampling_rate() * 2.25 * ads.get_sample_width() * ads.get_channels())\n        \"\"\"\n\n        # copy user's dicionary (shallow copy)\n        kwargs = kwargs.copy()\n\n        # check and normalize keyword arguments\n        ADSFactory._check_normalize_args(kwargs)\n\n        block_dur = kwargs.pop(\"bd\")\n        hop_dur = kwargs.pop(\"hd\")\n        block_size = kwargs.pop(\"bs\")\n        hop_size = kwargs.pop(\"hs\")\n        max_time = kwargs.pop(\"mt\")\n        audio_source = kwargs.pop(\"asrc\")\n        filename = kwargs.pop(\"fn\")\n        data_buffer = kwargs.pop(\"db\")\n        record = kwargs.pop(\"rec\")\n\n        # Case 1: an audio source is supplied\n        if audio_source is not None:\n            if (filename, data_buffer) != (None, None):\n                raise Warning(\"You should provide one of 'audio_source', 'filename' or 'data_buffer'\\\n                 keyword parameters. 'audio_source' will be used\")\n\n        # Case 2: a file name is supplied\n        elif filename is not None:\n            if data_buffer is not None:\n                raise Warning(\"You should provide one of 'filename' or 'data_buffer'\\\n                 keyword parameters. 'filename' will be used\")\n            audio_source = from_file(filename)\n\n        # Case 3: a data_buffer is supplied\n        elif data_buffer is not None:\n            audio_source = BufferAudioSource(data_buffer=data_buffer, **kwargs)\n\n        # Case 4: try to access native audio input\n        else:\n            audio_source = PyAudioSource(**kwargs)\n\n        if block_dur is not None:\n            if block_size is not None:\n                raise DuplicateArgument(\"Either 'block_dur' or 'block_size' can be specified, not both\")\n            else:\n                block_size = int(audio_source.get_sampling_rate() * block_dur)\n        elif block_size is None:\n            # Set default block_size to 10 ms\n            block_size = int(audio_source.get_sampling_rate() / 100)\n\n        # Instantiate base AudioDataSource\n        ads = ADSFactory.AudioDataSource(audio_source=audio_source, block_size=block_size)\n\n        # Limit data to be read\n        if max_time is not None:\n            ads = ADSFactory.LimiterADS(ads=ads, max_time=max_time)\n\n        # Record, rewind and reuse data\n        if record:\n            ads = ADSFactory.RecorderADS(ads=ads)\n\n        # Read overlapping blocks of data\n        if hop_dur is not None:\n            if hop_size is not None:\n                raise DuplicateArgument(\"Either 'hop_dur' or 'hop_size' can be specified, not both\")\n            else:\n                hop_size = int(audio_source.get_sampling_rate() * hop_dur)\n\n        if hop_size is not None:\n            if hop_size <= 0 or hop_size > block_size:\n                raise ValueError(\"hop_size must be > 0 and <= block_size\")\n            if hop_size < block_size:\n                ads = ADSFactory.OverlapADS(ads=ads, hop_size=hop_size)\n\n        return ads"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if data is valid.", "response": "def is_valid(self, data):\n        \"\"\"\n        Check if data is valid. Audio data will be converted into an array (of\n        signed values) of which the log energy is computed. Log energy is computed\n        as follows:\n\n        .. code:: python\n\n            arr = AudioEnergyValidator._convert(signal, sample_width)\n            energy = float(numpy.dot(arr, arr)) / len(arr)\n            log_energy = 10. * numpy.log10(energy)\n\n\n        :Parameters:\n\n        `data` : either a *string* or a *Bytes* buffer\n            `data` is converted into a numerical array using the `sample_width`\n            given in the constructor.\n\n        :Returns:\n\n        True if `log_energy` >= `energy_threshold`, False otherwise.\n        \"\"\"\n\n        signal = AudioEnergyValidator._convert(data, self.sample_width)\n        return AudioEnergyValidator._signal_log_energy(signal) >= self._energy_threshold"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_data(self, data_buffer):\n        if len(data_buffer) % (self.sample_width * self.channels) != 0:\n            raise ValueError(\"length of data_buffer must be a multiple of (sample_width * channels)\")\n        self._buffer = data_buffer\n        self._index = 0\n        self._left = 0 if self._buffer is None else len(self._buffer)", "response": "Set the data for this audio stream."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nappends data to this audio stream.", "response": "def append_data(self, data_buffer):\n        \"\"\" Append data to this audio stream\n\n        :Parameters:\n\n            `data_buffer` : str, basestring, Bytes\n                a buffer with a length multiple of (sample_width * channels)\n        \"\"\"\n\n        if len(data_buffer) % (self.sample_width * self.channels) != 0:\n            raise ValueError(\"length of data_buffer must be a multiple of (sample_width * channels)\")\n\n        self._buffer += data_buffer\n        self._left += len(data_buffer)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef user_post_save(sender, **kwargs):\n\n    # Disable post_save during manage.py loaddata\n    if kwargs.get(\"raw\", False):\n        return False\n\n    user, created = kwargs[\"instance\"], kwargs[\"created\"]\n    disabled = getattr(user, \"_disable_account_creation\", not settings.ACCOUNT_CREATE_ON_SAVE)\n    if created and not disabled:\n        Account.create(user=user)", "response": "Called when a User object is saved."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_form_kwargs(self):\n        kwargs = {\"user\": self.request.user, \"initial\": self.get_initial()}\n        if self.request.method in [\"POST\", \"PUT\"]:\n            kwargs.update({\n                \"data\": self.request.POST,\n                \"files\": self.request.FILES,\n            })\n        return kwargs", "response": "Returns the keyword arguments for instantiating the form."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the password is expired and system is using password expiration", "response": "def check_password_expired(user):\n    \"\"\"\n    Return True if password is expired and system is using\n    password expiration, False otherwise.\n    \"\"\"\n    if not settings.ACCOUNT_PASSWORD_USE_HISTORY:\n        return False\n\n    if hasattr(user, \"password_expiry\"):\n        # user-specific value\n        expiry = user.password_expiry.expiry\n    else:\n        # use global value\n        expiry = settings.ACCOUNT_PASSWORD_EXPIRY\n\n    if expiry == 0:  # zero indicates no expiration\n        return False\n\n    try:\n        # get latest password info\n        latest = user.password_history.latest(\"timestamp\")\n    except PasswordHistory.DoesNotExist:\n        return False\n\n    now = datetime.datetime.now(tz=pytz.UTC)\n    expiration = latest.timestamp + datetime.timedelta(seconds=expiry)\n\n    if expiration < now:\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef login_required(func=None, redirect_field_name=REDIRECT_FIELD_NAME, login_url=None):\n    def decorator(view_func):\n        @functools.wraps(view_func, assigned=available_attrs(view_func))\n        def _wrapped_view(request, *args, **kwargs):\n            if is_authenticated(request.user):\n                return view_func(request, *args, **kwargs)\n            return handle_redirect_to_login(\n                request,\n                redirect_field_name=redirect_field_name,\n                login_url=login_url\n            )\n        return _wrapped_view\n    if func:\n        return decorator(func)\n    return decorator", "response": "Decorator for views that checks that the user is logged in redirecting to the log in page if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef do_user_display(parser, token):\n    bits = token.split_contents()\n    if len(bits) == 2:\n        user = bits[1]\n        as_var = None\n    elif len(bits) == 4:\n        user = bits[1]\n        as_var = bits[3]\n    else:\n        raise template.TemplateSyntaxError(\"'{0}' takes either two or four arguments\".format(bits[0]))\n    return UserDisplayNode(user, as_var)", "response": "Displays the user in the tree."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef urlnext(parser, token):\n    bits = token.split_contents()\n    if len(bits) < 2:\n        raise template.TemplateSyntaxError(\n            \"'%s' takes at least one argument\"\n            \" (path to a view)\" % bits[0]\n        )\n    viewname = parser.compile_filter(bits[1])\n    args = []\n    kwargs = {}\n    asvar = None\n    bits = bits[2:]\n    if len(bits) >= 2 and bits[-2] == \"as\":\n        asvar = bits[-1]\n        bits = bits[:-2]\n\n    if len(bits):\n        for bit in bits:\n            match = kwarg_re.match(bit)\n            if not match:\n                raise template.TemplateSyntaxError(\"Malformed arguments to url tag\")\n            name, value = match.groups()\n            if name:\n                kwargs[name] = parser.compile_filter(value)\n            else:\n                args.append(parser.compile_filter(value))\n\n    return URLNextNode(viewname, args, kwargs, asvar)", "response": "URLNextNode - returns a URLNextNode for the next url in the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds next to url", "response": "def add_next(self, url, context):\n        \"\"\"\n        With both `redirect_field_name` and `redirect_field_value` available in\n        the context, add on a querystring to handle \"next\" redirecting.\n        \"\"\"\n        if all([key in context for key in [\"redirect_field_name\", \"redirect_field_value\"]]):\n            if context[\"redirect_field_value\"]:\n                url += \"?\" + urlencode({\n                    context[\"redirect_field_name\"]: context[\"redirect_field_value\"],\n                })\n        return url"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets an object from the cache.", "response": "def get(self, item):\n        \"\"\"Helper method to avoid calling getattr\n        \"\"\"\n        if item in self:  # noqa\n            item = getattr(self, item)\n            return item()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts the set of presets from the application s configurations.", "response": "def extract_presets(app_config):\n        \"\"\"\n        Pull the application's configurations for Sanic JWT\n        \"\"\"\n        return {\n            x.lower()[10:]: app_config.get(x)\n            for x in filter(lambda x: x.startswith(\"SANIC_JWT\"), app_config)\n        }"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef initialize(*args, **kwargs):\n    if len(args) > 1:\n        kwargs.update({\"authenticate\": args[1]})\n    return Initialize(args[0], **kwargs)", "response": "Functional approach to initializing Sanic JWT."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks for deprecated configuration keys", "response": "def __check_deprecated(self):\n        \"\"\"\n        Checks for deprecated configuration keys\n        \"\"\"\n        # Depracation notices\n        if \"SANIC_JWT_HANDLER_PAYLOAD_SCOPES\" in self.app.config:\n            raise exceptions.InvalidConfiguration(\n                \"SANIC_JWT_HANDLER_PAYLOAD_SCOPES has been deprecated. \"\n                \"Instead, pass your handler method (not an import path) as \"\n                \"initialize(add_scopes_to_payload=my_scope_extender)\"\n            )\n\n        if \"SANIC_JWT_PAYLOAD_HANDLER\" in self.app.config:\n            raise exceptions.InvalidConfiguration(\n                \"SANIC_JWT_PAYLOAD_HANDLER has been deprecated. \"\n                \"Instead, you will need to subclass Authentication. \"\n            )\n\n        if \"SANIC_JWT_HANDLER_PAYLOAD_EXTEND\" in self.app.config:\n            raise exceptions.InvalidConfiguration(\n                \"SANIC_JWT_HANDLER_PAYLOAD_EXTEND has been deprecated. \"\n                \"Instead, you will need to subclass Authentication. \"\n                \"Check out the documentation for more information.\"\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __add_endpoints(self):\n        for mapping in endpoint_mappings:\n            if all(map(self.config.get, mapping.keys)):\n                self.__add_single_endpoint(\n                    mapping.cls, mapping.endpoint, mapping.is_protected\n                )\n\n        self.bp.exception(exceptions.SanicJWTException)(\n            self.responses.exception_response\n        )\n\n        if not self.instance_is_blueprint:\n            url_prefix = self._get_url_prefix()\n            self.instance.blueprint(self.bp, url_prefix=url_prefix)", "response": "Initialize the Sanic JWT Blueprint and add to the instance initialized\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __add_class_views(self):\n        config = self.config\n        if \"class_views\" in self.kwargs:\n            class_views = self.kwargs.pop(\"class_views\")\n\n            for route, view in class_views:\n                if issubclass(view, endpoints.BaseEndpoint) and isinstance(\n                    route, str\n                ):\n                    self.bp.add_route(\n                        view.as_view(\n                            self.responses,\n                            config=self.config,\n                            instance=self.instance,\n                        ),\n                        route,\n                        strict_slashes=config.strict_slashes(),\n                    )\n                else:\n                    raise exceptions.InvalidClassViewsFormat()", "response": "Add any custom class views on the Sanic JWT Blueprint\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if any of the default classes are overwitten and if they are still valid.", "response": "def __check_classes(self):\n        \"\"\"\n        Check if any of the default classes (`Authentication`, `Configuration`\n        and / or `Responses`) have been overwitten and if they're still valid\n        \"\"\"\n        # msg took from BaseAuthentication\n        msg = (\n            \"Sanic JWT was not initialized properly. It did not received \"\n            \"an instance of {}\"\n        )\n        if not issubclass(self.authentication_class, Authentication):\n            raise exceptions.InitializationFailure(\n                message=msg.format(\"Authentication\")\n            )\n\n        if not issubclass(self.configuration_class, Configuration):\n            raise exceptions.InitializationFailure(\n                message=msg.format(\"Configuration\")\n            )\n\n        if not issubclass(self.responses_class, Responses):\n            raise exceptions.InitializationFailure(\n                message=msg.format(\"Responses\")\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __initialize_instance(self):\n        config = self.config\n\n        # Initialize instance of the Authentication class\n        self.instance.auth = self.authentication_class(self.app, config=config)\n\n        init_handlers = (\n            handlers if config.auth_mode() else auth_mode_agnostic_handlers\n        )\n\n        for handler in init_handlers:\n            if handler.keys is None:\n                self.__check_method_in_auth(handler.name, handler.exception)\n            else:\n                if all(map(config.get, handler.keys)):\n                    self.__check_method_in_auth(\n                        handler.name, handler.exception\n                    )\n\n        for handler in init_handlers:\n            if handler.name in self.kwargs:\n                method = self.kwargs.pop(handler.name)\n                setattr(self.instance.auth, handler.name, method)", "response": "Initialize the instance of the Authentication class and the authentication class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload the configuration settings for the instance", "response": "def __load_configuration(self):\n        \"\"\"\n        Configure settings for the instance in the following order:\n\n        1. Sanic JWT's defaults\n        2. Custom Configuration class\n        3. Key word arguments passed to Initialize\n        \"\"\"\n        handler_to_enable = filter(lambda h: h.keys is not None, handlers)\n        for handler in handler_to_enable:\n            if handler.name in self.kwargs:\n                for k in handler.keys:\n                    self.kwargs.update({k: True})\n\n        self.config = self.configuration_class(self.app.config, **self.kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a user_id from a user object.", "response": "async def _get_user_id(self, user, *, asdict=False):\n        \"\"\"\n        Get a user_id from a user object. If `asdict` is True, will return\n        it as a dict with `config.user_id` as key. The `asdict` keyword\n        defaults to `False`.\n        \"\"\"\n        uid = self.config.user_id()\n        if isinstance(user, dict):\n            user_id = user.get(uid)\n        elif hasattr(user, \"to_dict\"):\n            _to_dict = await utils.call(user.to_dict)\n            user_id = _to_dict.get(uid)\n        else:\n            raise exceptions.InvalidRetrieveUserObject()\n\n        if asdict:\n            return {uid: user_id}\n\n        return user_id"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd standard claims into the payload for the user.", "response": "async def add_claims(self, payload, user, *args, **kwargs):\n        \"\"\"\n        Injects standard claims into the payload for: exp, iss, iat, nbf, aud.\n        And, custom claims, if they exist\n        \"\"\"\n        delta = timedelta(seconds=self.config.expiration_delta())\n        exp = datetime.utcnow() + delta\n        additional = {\"exp\": exp}\n\n        for option in [\"iss\", \"iat\", \"nbf\", \"aud\"]:\n            setting = \"claim_{}\".format(option.lower())\n            if setting in self.config:  # noqa\n                attr = self.config.get(setting)\n                if attr:\n                    self.claims.append(option)\n                    method_name = \"build_claim_{}\".format(option)\n                    method = getattr(utils, method_name)\n                    additional.update({option: method(attr, self.config)})\n\n        payload.update(additional)\n\n        if self._custom_claims:\n            custom_claims = {}\n            for claim in self._custom_claims:\n                custom_claims[claim.get_key()] = await utils.call(\n                    claim.setup, payload, user\n                )\n            payload.update(custom_claims)\n\n        return payload"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_authentication(self, request, request_args, request_kwargs):\n        try:\n            is_valid, status, reasons = self._verify(\n                request,\n                request_args=request_args,\n                request_kwargs=request_kwargs,\n            )\n        except Exception as e:\n            logger.debug(e.args)\n            if self.config.debug():\n                raise e\n\n            args = e.args if isinstance(e, SanicJWTException) else []\n\n            raise exceptions.Unauthorized(*args)\n\n        return is_valid, status, reasons", "response": "Checks if a request contains a valid JWT and authenticated JWT."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _decode(self, token, verify=True):\n        secret = self._get_secret()\n        algorithm = self._get_algorithm()\n        kwargs = {}\n\n        for claim in self.claims:\n            if claim != \"exp\":\n                setting = \"claim_{}\".format(claim.lower())\n                if setting in self.config:  # noqa\n                    value = self.config.get(setting)\n                    kwargs.update({claim_label[claim]: value})\n\n        kwargs[\"leeway\"] = int(self.config.leeway())\n        if \"claim_aud\" in self.config:  # noqa\n            kwargs[\"audience\"] = self.config.claim_aud()\n        if \"claim_iss\" in self.config:  # noqa\n            kwargs[\"issuer\"] = self.config.claim_iss()\n\n        decoded = jwt.decode(\n            token,\n            secret,\n            algorithms=[algorithm],\n            verify=verify,\n            options={\"verify_exp\": self.config.verify_exp()},\n            **kwargs\n        )\n        return decoded", "response": "Take a JWT and return a decoded payload. Optionally will verify\n            the claims on the token."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def _get_payload(self, user):\n        payload = await utils.call(self.build_payload, user)\n\n        if (\n            not isinstance(payload, dict)\n            or self.config.user_id() not in payload\n        ):\n            raise exceptions.InvalidPayload\n\n        payload = await utils.call(self.add_claims, payload, user)\n\n        extend_payload_args = inspect.getfullargspec(self.extend_payload)\n        args = [payload]\n        if \"user\" in extend_payload_args.args:\n            args.append(user)\n        payload = await utils.call(self.extend_payload, *args)\n\n        if self.config.scopes_enabled():\n            scopes = await utils.call(self.add_scopes_to_payload, user)\n            if not isinstance(scopes, (tuple, list)):\n                scopes = [scopes]\n            payload[self.config.scopes_name()] = scopes\n\n        claims = self.claims + [x.get_key() for x in self._custom_claims]\n        missing = [x for x in claims if x not in payload]\n        if missing:\n            logger.debug(\"\")\n            raise exceptions.MissingRegisteredClaim(missing=missing)\n\n        return payload", "response": "Given a user object create a payload and extend it as configured."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting the token from the request cookies.", "response": "def _get_token_from_cookies(self, request, refresh_token):\n        \"\"\"\n        Extract the token if present inside the request cookies.\n        \"\"\"\n        if refresh_token:\n            cookie_token_name_key = \"cookie_refresh_token_name\"\n        else:\n            cookie_token_name_key = \"cookie_access_token_name\"\n        cookie_token_name = getattr(self.config, cookie_token_name_key)\n        return request.cookies.get(cookie_token_name(), None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts the token from the request headers.", "response": "def _get_token_from_headers(self, request, refresh_token):\n        \"\"\"\n        Extract the token if present inside the headers of a request.\n        \"\"\"\n        header = request.headers.get(self.config.authorization_header(), None)\n\n        if header is None:\n            return None\n\n        else:\n            header_prefix_key = \"authorization_header_prefix\"\n            header_prefix = getattr(self.config, header_prefix_key)\n            if header_prefix():\n                try:\n                    prefix, token = header.split(\" \")\n                    if prefix != header_prefix():\n                        raise Exception\n\n                except Exception:\n                    raise exceptions.InvalidAuthorizationHeader()\n\n            else:\n                token = header\n\n            if refresh_token:\n                token = request.json.get(self.config.refresh_token_name())\n\n            return token"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_token_from_query_string(self, request, refresh_token):\n        if refresh_token:\n            query_string_token_name_key = \"query_string_refresh_token_name\"\n        else:\n            query_string_token_name_key = \"query_string_access_token_name\"\n        query_string_token_name = getattr(\n            self.config, query_string_token_name_key\n        )\n        return request.args.get(query_string_token_name(), None)", "response": "Extract the token from the request args."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_token(self, request, refresh_token=False):\n        if self.config.cookie_set():\n            token = self._get_token_from_cookies(request, refresh_token)\n            if token:\n                return token\n\n            else:\n                if self.config.cookie_strict():\n                    raise exceptions.MissingAuthorizationCookie()\n\n        if self.config.query_string_set():\n            token = self._get_token_from_query_string(request, refresh_token)\n            if token:\n                return token\n\n            else:\n                if self.config.query_string_strict():\n                    raise exceptions.MissingAuthorizationQueryArg()\n\n        token = self._get_token_from_headers(request, refresh_token)\n\n        if token:\n            return token\n\n        raise exceptions.MissingAuthorizationHeader()", "response": "Extract a token from a request object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _verify(\n        self,\n        request,\n        return_payload=False,\n        verify=True,\n        raise_missing=False,\n        request_args=None,\n        request_kwargs=None,\n        *args,\n        **kwargs\n    ):\n        \"\"\"\n        Verify that a request object is authenticated.\n        \"\"\"\n        try:\n            token = self._get_token(request)\n            is_valid = True\n            reason = None\n        except (\n            exceptions.MissingAuthorizationCookie,\n            exceptions.MissingAuthorizationQueryArg,\n            exceptions.MissingAuthorizationHeader,\n        ) as e:\n            token = None\n            is_valid = False\n            reason = list(e.args)\n            status = e.status_code if self.config.debug() else 401\n\n            if raise_missing:\n                if not self.config.debug():\n                    e.status_code = 401\n                raise e\n\n        if token:\n            try:\n                payload = self._decode(token, verify=verify)\n\n                if verify:\n                    if self._extra_verifications:\n                        self._verify_extras(payload)\n                    if self._custom_claims:\n                        self._verify_custom_claims(payload)\n            except (\n                jwt.exceptions.ExpiredSignatureError,\n                jwt.exceptions.InvalidIssuerError,\n                jwt.exceptions.ImmatureSignatureError,\n                jwt.exceptions.InvalidIssuedAtError,\n                jwt.exceptions.InvalidAudienceError,\n                InvalidVerificationError,\n                InvalidCustomClaimError,\n            ) as e:\n                # Make sure that the reasons all end with '.' for consistency\n                reason = [\n                    x if x.endswith(\".\") else \"{}.\".format(x)\n                    for x in list(e.args)\n                ]\n                payload = None\n                status = 401\n                is_valid = False\n            except jwt.exceptions.DecodeError as e:\n                self._reasons = e.args\n                # Make sure that the reasons all end with '.' for consistency\n                reason = (\n                    [\n                        x if x.endswith(\".\") else \"{}.\".format(x)\n                        for x in list(e.args)\n                    ]\n                    if self.config.debug()\n                    else \"Auth required.\"\n                )\n                logger.debug(e.args)\n                is_valid = False\n                payload = None\n                status = 400 if self.config.debug() else 401\n        else:\n            payload = None\n\n        if return_payload:\n            return payload\n\n        status = 200 if is_valid else status\n\n        return is_valid, status, reason", "response": "Verify that a request object is authenticated."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract_payload(self, request, verify=True, *args, **kwargs):\n        payload = self._verify(\n            request, return_payload=True, verify=verify, *args, **kwargs\n        )\n        return payload", "response": "Extract a payload from a request object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting the scopes from a request object.", "response": "def extract_scopes(self, request):\n        \"\"\"\n        Extract scopes from a request object.\n        \"\"\"\n        payload = self.extract_payload(request)\n        if not payload:\n            return None\n\n        scopes_attribute = self.config.scopes_name()\n        return payload.get(scopes_attribute, None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_user_id(self, request):\n        payload = self.extract_payload(request)\n        user_id_attribute = self.config.user_id()\n        return payload.get(user_id_attribute, None)", "response": "Extract a user id from a request object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def generate_access_token(self, user):\n        payload = await self._get_payload(user)\n        secret = self._get_secret(True)\n        algorithm = self._get_algorithm()\n\n        return jwt.encode(payload, secret, algorithm=algorithm).decode(\"utf-8\")", "response": "Generate an access token for a given user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def generate_refresh_token(self, request, user):\n        refresh_token = await utils.call(self.config.generate_refresh_token())\n        user_id = await self._get_user_id(user)\n        await utils.call(\n            self.store_refresh_token,\n            user_id=user_id,\n            refresh_token=refresh_token,\n            request=request,\n        )\n        return refresh_token", "response": "Generate a new refresh token for a given user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsplitting array into two parts.", "response": "def tsplit(df, shape):\n    \"\"\"Split array into two parts.\"\"\"\n    if isinstance(df, (pd.DataFrame, pd.Series)):\n        return df.iloc[0:shape], df.iloc[shape:]\n    else:\n        return df[0:shape], df[shape:]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef concat(x, y, axis=0):\n    if all([isinstance(df, (pd.DataFrame, pd.Series)) for df in [x, y]]):\n        return pd.concat([x, y], axis=axis)\n    else:\n        if axis == 0:\n            return np.concatenate([x, y])\n        else:\n            return np.column_stack([x, y])", "response": "Concatenate a sequence of pandas or numpy objects into one entity."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a single model.", "response": "def add(self, model):\n        \"\"\"Adds a single model.\n\n        Parameters\n        ----------\n        model : `Estimator`\n        \"\"\"\n        if isinstance(model, (Regressor, Classifier)):\n            self.models.append(model)\n        else:\n            raise ValueError('Unrecognized estimator.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stack(self, k=5, stratify=False, shuffle=True, seed=100, full_test=True, add_diff=False):\n        result_train = []\n        result_test = []\n        y = None\n\n        for model in self.models:\n            result = model.stack(k=k, stratify=stratify, shuffle=shuffle, seed=seed, full_test=full_test)\n            train_df = pd.DataFrame(result.X_train, columns=generate_columns(result.X_train, model.name))\n            test_df = pd.DataFrame(result.X_test, columns=generate_columns(result.X_test, model.name))\n\n            result_train.append(train_df)\n            result_test.append(test_df)\n            if y is None:\n                y = result.y_train\n        result_train = pd.concat(result_train, axis=1)\n        result_test = pd.concat(result_test, axis=1)\n\n        if add_diff:\n            result_train = feature_combiner(result_train)\n            result_test = feature_combiner(result_test)\n\n        ds = Dataset(X_train=result_train, y_train=y, X_test=result_test)\n\n        return ds", "response": "Stacks the model on the train and test data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef blend(self, proportion=0.2, stratify=False, seed=100, indices=None, add_diff=False):\n        result_train = []\n        result_test = []\n        y = None\n\n        for model in self.models:\n            result = model.blend(proportion=proportion, stratify=stratify, seed=seed, indices=indices)\n            train_df = pd.DataFrame(result.X_train, columns=generate_columns(result.X_train, model.name))\n            test_df = pd.DataFrame(result.X_test, columns=generate_columns(result.X_test, model.name))\n\n            result_train.append(train_df)\n            result_test.append(test_df)\n            if y is None:\n                y = result.y_train\n        result_train = pd.concat(result_train, axis=1, ignore_index=True)\n        result_test = pd.concat(result_test, axis=1, ignore_index=True)\n\n        if add_diff:\n            result_train = feature_combiner(result_train)\n            result_test = feature_combiner(result_test)\n\n        return Dataset(X_train=result_train, y_train=y, X_test=result_test)", "response": "Blends the sequence of models into one dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds optimal weights for weighted average of models.", "response": "def find_weights(self, scorer, test_size=0.2, method='SLSQP'):\n        \"\"\"Finds optimal weights for weighted average of models.\n\n        Parameters\n        ----------\n        scorer : function\n            Scikit-learn like metric.\n        test_size : float, default 0.2\n        method : str\n            Type of solver.  Should be one of:\n\n            - 'Nelder-Mead'\n            - 'Powell'\n            - 'CG'\n            - 'BFGS'\n            - 'Newton-CG'\n            - 'L-BFGS-B'\n            - 'TNC'\n            - 'COBYLA'\n            - 'SLSQP'\n            - 'dogleg'\n            - 'trust-ncg'\n\n        Returns\n        -------\n        list\n        \"\"\"\n        p = Optimizer(self.models, test_size=test_size, scorer=scorer)\n        return p.minimize(method)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef weight(self, weights):\n        return self.apply(lambda x: np.average(x, axis=0, weights=weights))", "response": "Applies weighted mean to models.\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef onehot_features(train, test, features, full=False, sparse=False, dummy_na=True):\n\n    features = [f for f in features if f in train.columns]\n    for column in features:\n        if full:\n            categories = pd.concat([train[column], test[column]]).dropna().unique()\n        else:\n            categories = train[column].dropna().unique()\n\n        train[column] = train[column].astype('category', categories=categories)\n        test[column] = test[column].astype('category', categories=categories)\n\n    train = pd.get_dummies(train, columns=features, dummy_na=dummy_na, sparse=sparse)\n    test = pd.get_dummies(test, columns=features, dummy_na=dummy_na, sparse=sparse)\n\n    # d_cols = train.columns[(train == 0).all()]\n    # train.drop(d_cols, 1, inplace=True)\n    # test.drop(d_cols, 1, inplace=True)\n\n    return train, test", "response": "Encode categorical features using a one - hot scheme."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef factorize(train, test, features, na_value=-9999, full=False, sort=True):\n\n    for column in features:\n        if full:\n            vs = pd.concat([train[column], test[column]])\n            labels, indexer = pd.factorize(vs, sort=sort)\n        else:\n            labels, indexer = pd.factorize(train[column], sort=sort)\n\n        train[column] = indexer.get_indexer(train[column])\n        test[column] = indexer.get_indexer(test[column])\n\n        if na_value != -1:\n            train[column] = train[column].replace(-1, na_value)\n            test[column] = test[column].replace(-1, na_value)\n\n    return train, test", "response": "Factorize categorical features.\n\n    Parameters\n    ----------\n    train : pd.DataFrame\n    test : pd.DataFrame\n    features : list\n           Column names in the DataFrame to be encoded.\n    na_value : int, default -9999\n    full : bool, default False\n        Whether use all columns from train/test or only from train.\n    sort : bool, default True\n        Sort by values.\n\n    Returns\n    -------\n    train : pd.DataFrame\n    test : pd.DataFrame"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate weight of evidence.", "response": "def woe(df, feature_name, target_name):\n    \"\"\"Calculate weight of evidence.\n\n    Parameters\n    ----------\n    df: Dataframe\n    feature_name: str\n        Column name to encode.\n    target_name: str\n        Target column name.\n\n    Returns\n    -------\n    Series\n\n    \"\"\"\n\n    def group_woe(group):\n        event = float(group.sum())\n        non_event = group.shape[0] - event\n\n        rel_event = event / event_total\n        rel_non_event = non_event / non_event_total\n\n        return np.log(rel_non_event / rel_event) * 100\n\n    if df[target_name].nunique() > 2:\n        raise ValueError('Target column should be binary (1/0).')\n\n    event_total = float(df[df[target_name] == 1.0].shape[0])\n    non_event_total = float(df.shape[0] - event_total)\n\n    woe_vals = df.groupby(feature_name)[target_name].transform(group_woe)\n    return woe_vals"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mean_target(df, feature_name, target_name, C=None):\n\n    def group_mean(group):\n        group_size = float(group.shape[0])\n        if C is None:\n            return (group.mean() * group_size + global_mean) / group_size\n        else:\n            return (group.mean() * group_size + global_mean * C) / (group_size + C)\n\n    global_mean = df[target_name].mean()\n    return df.groupby(feature_name)[target_name].transform(group_mean)", "response": "Mean target.\n    Original idea: Stanislav Semenov\n\n    Parameters\n    ----------\n    C : float, default None\n        Regularization coefficient. The higher, the more conservative result.\n        The optimal value lies between 10 and 50 depending on the data.\n    feature_name : str\n    target_name : str\n    df: DataFrame\n\n    Returns\n    -------\n    Series"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting xgboost model into categorical features.", "response": "def xgb_to_features(model, X_train, X_test):\n    \"\"\"Converts xgboost model into categorical features.\n    Reference:\n    \"Practical Lessons from Predicting Clicks on Ads at Facebook\"\n    https://research.fb.com/publications/practical-lessons-from-predicting-clicks-on-ads-at-facebook/\n    \"\"\"\n    import xgboost as xgb\n    f_train = model.predict(xgb.DMatrix(X_train), pred_leaf=True)\n    f_test = model.predict(xgb.DMatrix(X_test), pred_leaf=True)\n    enc = OneHotEncoder()\n    enc.fit(f_train)\n    return enc.transform(f_train), enc.transform(f_test)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef split(self, test_size=0.1, stratify=False, inplace=False, seed=33, indices=None):\n\n        if not self.loaded:\n            self.load()\n\n        if stratify:\n            stratify = self.y_train\n        else:\n            stratify = None\n\n        if indices is None:\n            X_train, X_test, y_train, y_test = train_test_split(self.X_train, self._y_train,\n                                                                test_size=test_size,\n                                                                random_state=seed,\n                                                                stratify=stratify, )\n        else:\n            X_train, y_train = idx(self.X_train, indices[0]), self.y_train[indices[0]]\n            X_test, y_test = idx(self.X_train, indices[1]), self.y_train[indices[1]]\n\n        if inplace:\n            self._X_train, self._X_test, self._y_train, self._y_test = X_train, X_test, y_train, y_test\n\n        return X_train, y_train, X_test, y_test", "response": "Splits the train set into two parts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn md5 hash for current dataset.", "response": "def hash(self):\n        \"\"\"Return md5 hash for current dataset.\"\"\"\n        if self._hash is None:\n            m = hashlib.new('md5')\n            if self._preprocessor is None:\n                # generate hash from numpy array\n                m.update(numpy_buffer(self._X_train))\n                m.update(numpy_buffer(self._y_train))\n                if self._X_test is not None:\n                    m.update(numpy_buffer(self._X_test))\n                if self._y_test is not None:\n                    m.update(numpy_buffer(self._y_test))\n            elif callable(self._preprocessor):\n                # generate hash from user defined object (source code)\n                m.update(inspect.getsource(self._preprocessor).encode('utf-8'))\n\n            self._hash = m.hexdigest()\n\n        return self._hash"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef merge(self, ds, inplace=False, axis=1):\n        if not isinstance(ds, Dataset):\n            raise ValueError('Expected `Dataset`, got %s.' % ds)\n\n        X_train = concat(ds.X_train, self.X_train, axis=axis)\n        y_train = concat(ds.y_train, self.y_train, axis=axis)\n\n        if ds.X_test is not None:\n            X_test = concat(ds.X_test, self.X_test, axis=axis)\n        else:\n            X_test = None\n\n        if ds.y_test is not None:\n            y_test = concat(ds.y_test, self.y_test, axis=axis)\n        else:\n            y_test = None\n\n        if inplace:\n            self._X_train = X_train\n            self._y_train = y_train\n\n            if X_test is not None:\n                self._X_test = X_test\n\n            if y_test is not None:\n                self._y_test = y_test\n\n            return None\n\n        return Dataset(X_train, y_train, X_test, y_test)", "response": "Merge two sets of data sets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting Dataset to scipy s Compressed Sparse Column matrix.", "response": "def to_csc(self):\n        \"\"\"Convert Dataset to scipy's Compressed Sparse Column matrix.\"\"\"\n        self._X_train = csc_matrix(self._X_train)\n        self._X_test = csc_matrix(self._X_test)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert Dataset to scipy s Compressed Sparse Row matrix.", "response": "def to_csr(self):\n        \"\"\"Convert Dataset to scipy's Compressed Sparse Row matrix.\"\"\"\n        self._X_train = csr_matrix(self._X_train)\n        self._X_test = csr_matrix(self._X_test)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_dense(self):\n        if hasattr(self._X_train, 'todense'):\n            self._X_train = self._X_train.todense()\n            self._X_test = self._X_test.todense()", "response": "Convert sparse Dataset to dense matrix."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _dhash(self, params):\n        m = hashlib.new('md5')\n        m.update(self.hash.encode('utf-8'))\n        for key in sorted(params.keys()):\n            h_string = ('%s-%s' % (key, params[key])).encode('utf-8')\n            m.update(h_string)\n        return m.hexdigest()", "response": "Generate hash of the dictionary object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate(self, scorer=None, k=1, test_size=0.1, stratify=False, shuffle=True, seed=100, indices=None):\n        if self.use_cache:\n            pdict = {'k': k, 'stratify': stratify, 'shuffle': shuffle, 'seed': seed, 'test_size': test_size}\n\n            if indices is not None:\n                pdict['train_index'] = np_hash(indices[0])\n                pdict['test_index'] = np_hash(indices[1])\n\n            dhash = self._dhash(pdict)\n            c = Cache(dhash, prefix='v')\n            if c.available:\n                logger.info('Loading %s\\'s validation results from cache.' % self._name)\n            elif (self.dataset.X_train is None) and (self.dataset.y_train is None):\n                self.dataset.load()\n\n        scores = []\n        y_true = []\n        y_pred = []\n\n        if k == 1:\n            X_train, y_train, X_test, y_test = self.dataset.split(test_size=test_size, stratify=stratify,\n                                                                  seed=seed, indices=indices)\n            if self.use_cache and c.available:\n                prediction = c.retrieve('0')\n            else:\n                prediction = self._predict(X_train, y_train, X_test, y_test)\n                if self.use_cache:\n                    c.store('0', prediction)\n            if scorer is not None:\n                scores.append(scorer(y_test, prediction))\n            y_true.append(y_test)\n            y_pred.append(prediction)\n\n        else:\n            for i, fold in enumerate(self.dataset.kfold(k, stratify=stratify, seed=seed, shuffle=shuffle)):\n                X_train, y_train, X_test, y_test, train_index, test_index = fold\n                if self.use_cache and c.available:\n                    prediction = c.retrieve(str(i))\n                else:\n                    prediction = None\n\n                if prediction is None:\n                    logger.info('Calculating %s\\'s fold #%s' % (self._name, i + 1))\n                    prediction = self._predict(X_train, y_train, X_test, y_test)\n                    if self.use_cache:\n                        c.store(str(i), prediction)\n                if scorer is not None:\n                    scores.append(scorer(y_test, prediction))\n                y_true.append(y_test)\n                y_pred.append(prediction)\n\n        if scorer is not None:\n            report_score(scores, scorer)\n\n        return y_true, y_pred", "response": "Evaluate score by cross - validation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stack(self, k=5, stratify=False, shuffle=True, seed=100, full_test=True):\n        train = None\n        test = []\n\n        if self.use_cache:\n            pdict = {'k': k, 'stratify': stratify, 'shuffle': shuffle, 'seed': seed, 'full_test': full_test}\n            dhash = self._dhash(pdict)\n            c = Cache(dhash, prefix='s')\n            if c.available:\n                logger.info('Loading %s\\'s stack results from cache.' % self._name)\n                train = c.retrieve('train')\n                test = c.retrieve('test')\n                y_train = c.retrieve('y_train')\n                return Dataset(X_train=train, y_train=y_train, X_test=test)\n            elif not self.dataset.loaded:\n                self.dataset.load()\n\n        for i, fold in enumerate(self.dataset.kfold(k, stratify=stratify, seed=seed, shuffle=shuffle)):\n            X_train, y_train, X_test, y_test, train_index, test_index = fold\n            logger.info('Calculating %s\\'s fold #%s' % (self._name, i + 1))\n            if full_test:\n                prediction = reshape_1d(self._predict(X_train, y_train, X_test, y_test))\n            else:\n                xt_shape = X_test.shape[0]\n                x_t = concat(X_test, self.dataset.X_test)\n                prediction_concat = reshape_1d(self._predict(X_train, y_train, x_t))\n                prediction, prediction_test = tsplit(prediction_concat, xt_shape)\n                test.append(prediction_test)\n\n            if train is None:\n                train = np.zeros((self.dataset.X_train.shape[0], prediction.shape[1]))\n\n            train[test_index] = prediction\n\n        if full_test:\n            logger.info('Calculating %s\\'s test data' % self._name)\n            test = self._predict(self.dataset.X_train, self.dataset.y_train, self.dataset.X_test)\n        else:\n            test = np.mean(test, axis=0)\n\n        test = reshape_1d(test)\n\n        if self.use_cache:\n            c.store('train', train)\n            c.store('test', test)\n            c.store('y_train', self.dataset.y_train)\n\n        return Dataset(X_train=train, y_train=self.dataset.y_train, X_test=test)", "response": "Stack a single model."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef blend(self, proportion=0.2, stratify=False, seed=100, indices=None):\n\n        if self.use_cache:\n            pdict = {'proportion': proportion, 'stratify': stratify, 'seed': seed, 'indices': indices}\n\n            if indices is not None:\n                pdict['train_index'] = np_hash(indices[0])\n                pdict['test_index'] = np_hash(indices[1])\n\n            dhash = self._dhash(pdict)\n            c = Cache(dhash, prefix='b')\n            if c.available:\n                logger.info('Loading %s\\'s blend results from cache.' % self._name)\n                train = c.retrieve('train')\n                test = c.retrieve('test')\n                y_train = c.retrieve('y_train')\n                return Dataset(X_train=train, y_train=y_train, X_test=test)\n            elif not self.dataset.loaded:\n                self.dataset.load()\n\n        X_train, y_train, X_test, y_test = self.dataset.split(test_size=proportion, stratify=stratify,\n                                                              seed=seed, indices=indices)\n\n        xt_shape = X_test.shape[0]\n        x_t = concat(X_test, self.dataset.X_test)\n        prediction_concat = reshape_1d(self._predict(X_train, y_train, x_t))\n        new_train, new_test = tsplit(prediction_concat, xt_shape)\n        if self.use_cache:\n            c.store('train', new_train)\n            c.store('test', new_test)\n            c.store('y_train', y_test)\n        return Dataset(new_train, y_test, new_test)", "response": "Blends a single model into a single dataset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a buffer from c_contiguous numpy ndarray.", "response": "def numpy_buffer(ndarray):\n    \"\"\"Creates a buffer from c_contiguous numpy ndarray.\"\"\"\n    # Credits to: https://github.com/joblib/joblib/blob/04b001861e1dd03a857b7b419c336de64e05714c/joblib/hashing.py\n\n    if isinstance(ndarray, (pd.Series, pd.DataFrame)):\n        ndarray = ndarray.values\n\n    if ndarray.flags.c_contiguous:\n        obj_c_contiguous = ndarray\n    elif ndarray.flags.f_contiguous:\n        obj_c_contiguous = ndarray.T\n    else:\n        obj_c_contiguous = ndarray.flatten()\n\n    obj_c_contiguous = obj_c_contiguous.view(np.uint8)\n\n    if hasattr(np, 'getbuffer'):\n        return np.getbuffer(obj_c_contiguous)\n    else:\n        return memoryview(obj_c_contiguous)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef store(self, key, data):\n        if not os.path.exists(self._hash_dir):\n            os.makedirs(self._hash_dir)\n\n        if isinstance(data, pd.DataFrame):\n            columns = data.columns.tolist()\n            np.save(os.path.join(self._hash_dir, key), data.values)\n            json.dump(columns, open(os.path.join(self._hash_dir, '%s.json' % key), 'w'))\n        else:\n            np.save(os.path.join(self._hash_dir, key), data)", "response": "Takes an array and stores it in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef retrieve(self, key):\n        column_file = os.path.join(self._hash_dir, '%s.json' % key)\n        cache_file = os.path.join(self._hash_dir, '%s.npy' % key)\n\n        if os.path.exists(cache_file):\n            data = np.load(cache_file)\n            if os.path.exists(column_file):\n                with open(column_file, 'r') as json_file:\n                    columns = json.load(json_file)\n                data = pd.DataFrame(data, columns=columns)\n        else:\n            return None\n\n        return data", "response": "Retrieves a cached array if possible."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a tuple of integers representing the Python version number of the current Python version.", "response": "def _pep425_version():\n    \"\"\"\n    :return:\n        A tuple of integers representing the Python version number\n    \"\"\"\n\n    if hasattr(sys, 'pypy_version_info'):\n        return (sys.version_info[0], sys.pypy_version_info.major,\n                sys.pypy_version_info.minor)\n    else:\n        return (sys.version_info[0], sys.version_info[1])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the unicode string of the system abi.", "response": "def _pep425_get_abi():\n    \"\"\"\n    :return:\n        A unicode string of the system abi. Will be something like: \"cp27m\",\n        \"cp33m\", etc.\n    \"\"\"\n\n    try:\n        soabi = sysconfig.get_config_var('SOABI')\n        if soabi:\n            if soabi.startswith('cpython-'):\n                return 'cp%s' % soabi.split('-')[1]\n            return soabi.replace('.', '_').replace('-', '_')\n    except (IOError, NameError):\n        pass\n\n    impl = _pep425_implementation()\n    suffix = ''\n    if impl == 'cp':\n        suffix += 'm'\n    if sys.maxunicode == 0x10ffff and sys.version_info < (3, 3):\n        suffix += 'u'\n    return '%s%s%s' % (impl, ''.join(map(str_cls, _pep425_version())), suffix)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _pep425tags():\n\n    tags = []\n\n    versions = []\n    version_info = _pep425_version()\n    major = version_info[:-1]\n    for minor in range(version_info[-1], -1, -1):\n        versions.append(''.join(map(str, major + (minor,))))\n\n    impl = _pep425_implementation()\n\n    abis = []\n    abi = _pep425_get_abi()\n    if abi:\n        abis.append(abi)\n    abi3 = _pep425_implementation() == 'cp' and sys.version_info >= (3,)\n    if abi3:\n        abis.append('abi3')\n    abis.append('none')\n\n    if sys.platform == 'darwin':\n        plat_ver = platform.mac_ver()\n        ver_parts = plat_ver[0].split('.')\n        minor = int(ver_parts[1])\n        arch = plat_ver[2]\n        if sys.maxsize == 2147483647:\n            arch = 'i386'\n        arches = []\n        while minor > 5:\n            arches.append('macosx_10_%s_%s' % (minor, arch))\n            arches.append('macosx_10_%s_intel' % (minor,))\n            arches.append('macosx_10_%s_universal' % (minor,))\n            minor -= 1\n    else:\n        if sys.platform == 'win32':\n            if 'amd64' in sys.version.lower():\n                arches = ['win_amd64']\n            arches = [sys.platform]\n        elif hasattr(os, 'uname'):\n            (plat, _, _, _, machine) = os.uname()\n            plat = plat.lower().replace('/', '')\n            machine.replace(' ', '_').replace('/', '_')\n            if plat == 'linux' and sys.maxsize == 2147483647:\n                machine = 'i686'\n            arch = '%s_%s' % (plat, machine)\n            if _pep425_supports_manylinux():\n                arches = [arch.replace('linux', 'manylinux1'), arch]\n            else:\n                arches = [arch]\n\n    for abi in abis:\n        for arch in arches:\n            tags.append(('%s%s' % (impl, versions[0]), abi, arch))\n\n    if abi3:\n        for version in versions[1:]:\n            for arch in arches:\n                tags.append(('%s%s' % (impl, version), 'abi3', arch))\n\n    for arch in arches:\n        tags.append(('py%s' % (versions[0][0]), 'none', arch))\n\n    tags.append(('%s%s' % (impl, versions[0]), 'none', 'any'))\n    tags.append(('%s%s' % (impl, versions[0][0]), 'none', 'any'))\n\n    for i, version in enumerate(versions):\n        tags.append(('py%s' % (version,), 'none', 'any'))\n        if i == 0:\n            tags.append(('py%s' % (version[0]), 'none', 'any'))\n\n    tags.append(('py2.py3', 'none', 'any'))\n\n    return tags", "response": "Return a list of 3 - element tuples with unicode strings or None."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the X and Y coordinates for this EC point as native Python 2 - element tuple containing integers X and Y.", "response": "def to_coords(self):\n        \"\"\"\n        Returns the X and Y coordinates for this EC point, as native Python\n        integers\n\n        :return:\n            A 2-element tuple containing integers (X, Y)\n        \"\"\"\n\n        data = self.native\n        first_byte = data[0:1]\n\n        # Uncompressed\n        if first_byte == b'\\x04':\n            remaining = data[1:]\n            field_len = len(remaining) // 2\n            x = int_from_bytes(remaining[0:field_len])\n            y = int_from_bytes(remaining[field_len:])\n            return (x, y)\n\n        if first_byte not in set([b'\\x02', b'\\x03']):\n            raise ValueError(unwrap(\n                '''\n                Invalid EC public key - first byte is incorrect\n                '''\n            ))\n\n        raise ValueError(unwrap(\n            '''\n            Compressed representations of EC public keys are not supported due\n            to patent US6252960\n            '''\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wrap(cls, private_key, algorithm):\n\n        if not isinstance(private_key, byte_cls) and not isinstance(private_key, Asn1Value):\n            raise TypeError(unwrap(\n                '''\n                private_key must be a byte string or Asn1Value, not %s\n                ''',\n                type_name(private_key)\n            ))\n\n        if algorithm == 'rsa':\n            if not isinstance(private_key, RSAPrivateKey):\n                private_key = RSAPrivateKey.load(private_key)\n            params = Null()\n        elif algorithm == 'dsa':\n            if not isinstance(private_key, DSAPrivateKey):\n                private_key = DSAPrivateKey.load(private_key)\n            params = DSAParams()\n            params['p'] = private_key['p']\n            params['q'] = private_key['q']\n            params['g'] = private_key['g']\n            public_key = private_key['public_key']\n            private_key = private_key['private_key']\n        elif algorithm == 'ec':\n            if not isinstance(private_key, ECPrivateKey):\n                private_key = ECPrivateKey.load(private_key)\n            else:\n                private_key = private_key.copy()\n            params = private_key['parameters']\n            del private_key['parameters']\n        else:\n            raise ValueError(unwrap(\n                '''\n                algorithm must be one of \"rsa\", \"dsa\", \"ec\", not %s\n                ''',\n                repr(algorithm)\n            ))\n\n        private_key_algo = PrivateKeyAlgorithm()\n        private_key_algo['algorithm'] = PrivateKeyAlgorithmId(algorithm)\n        private_key_algo['parameters'] = params\n\n        container = cls()\n        container._algorithm = algorithm\n        container['version'] = Integer(0)\n        container['private_key_algorithm'] = private_key_algo\n        container['private_key'] = private_key\n\n        # Here we save the DSA public key if possible since it is not contained\n        # within the PKCS#8 structure for a DSA key\n        if algorithm == 'dsa':\n            container._public_key = public_key\n\n        return container", "response": "Wraps a private key in a PrivateKeyInfo structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_public_key(self):\n\n        if self.algorithm == 'dsa':\n            params = self['private_key_algorithm']['parameters']\n            return Integer(pow(\n                params['g'].native,\n                self['private_key'].parsed.native,\n                params['p'].native\n            ))\n\n        if self.algorithm == 'rsa':\n            key = self['private_key'].parsed\n            return RSAPublicKey({\n                'modulus': key['modulus'],\n                'public_exponent': key['public_exponent'],\n            })\n\n        if self.algorithm == 'ec':\n            curve_type, details = self.curve\n\n            if curve_type == 'implicit_ca':\n                raise ValueError(unwrap(\n                    '''\n                    Unable to compute public key for EC key using Implicit CA\n                    parameters\n                    '''\n                ))\n\n            if curve_type == 'specified':\n                if details['field_id']['field_type'] == 'characteristic_two_field':\n                    raise ValueError(unwrap(\n                        '''\n                        Unable to compute public key for EC key over a\n                        characteristic two field\n                        '''\n                    ))\n\n                curve = PrimeCurve(\n                    details['field_id']['parameters'],\n                    int_from_bytes(details['curve']['a']),\n                    int_from_bytes(details['curve']['b'])\n                )\n                base_x, base_y = self['private_key_algorithm']['parameters'].chosen['base'].to_coords()\n                base_point = PrimePoint(curve, base_x, base_y)\n\n            elif curve_type == 'named':\n                if details not in ('secp192r1', 'secp224r1', 'secp256r1', 'secp384r1', 'secp521r1'):\n                    raise ValueError(unwrap(\n                        '''\n                        Unable to compute public key for EC named curve %s,\n                        parameters not currently included\n                        ''',\n                        details\n                    ))\n\n                base_point = {\n                    'secp192r1': SECP192R1_BASE_POINT,\n                    'secp224r1': SECP224R1_BASE_POINT,\n                    'secp256r1': SECP256R1_BASE_POINT,\n                    'secp384r1': SECP384R1_BASE_POINT,\n                    'secp521r1': SECP521R1_BASE_POINT,\n                }[details]\n\n            public_point = base_point * self['private_key'].parsed['private_key'].native\n            return ECPointBitString.from_coords(public_point.x, public_point.y)", "response": "Computes the public key corresponding to the current private key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unwrap(self):\n\n        if self.algorithm == 'rsa':\n            return self['private_key'].parsed\n\n        if self.algorithm == 'dsa':\n            params = self['private_key_algorithm']['parameters']\n            return DSAPrivateKey({\n                'version': 0,\n                'p': params['p'],\n                'q': params['q'],\n                'g': params['g'],\n                'public_key': self.public_key,\n                'private_key': self['private_key'].parsed,\n            })\n\n        if self.algorithm == 'ec':\n            output = self['private_key'].parsed\n            output['parameters'] = self['private_key_algorithm']['parameters']\n            output['public_key'] = self.public_key\n            return output", "response": "Unwraps the private key into an RSAPrivateKey DSAPrivateKey or ECPrivateKey object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef algorithm(self):\n\n        if self._algorithm is None:\n            self._algorithm = self['private_key_algorithm']['algorithm'].native\n        return self._algorithm", "response": "Returns the private key algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bit_size(self):\n\n        if self._bit_size is None:\n            if self.algorithm == 'rsa':\n                prime = self['private_key'].parsed['modulus'].native\n            elif self.algorithm == 'dsa':\n                prime = self['private_key_algorithm']['parameters']['p'].native\n            elif self.algorithm == 'ec':\n                prime = self['private_key'].parsed['private_key'].native\n            self._bit_size = int(math.ceil(math.log(prime, 2)))\n            modulus = self._bit_size % 8\n            if modulus != 0:\n                self._bit_size += 8 - modulus\n        return self._bit_size", "response": "Returns the bit size of the private key as an integer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the public key of the user.", "response": "def public_key(self):\n        \"\"\"\n        :return:\n            If an RSA key, an RSAPublicKey object. If a DSA key, an Integer\n            object. If an EC key, an ECPointBitString object.\n        \"\"\"\n\n        if self._public_key is None:\n            if self.algorithm == 'ec':\n                key = self['private_key'].parsed\n                if key['public_key']:\n                    self._public_key = key['public_key'].untag()\n                else:\n                    self._public_key = self._compute_public_key()\n            else:\n                self._public_key = self._compute_public_key()\n\n        return self._public_key"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a fingerprint that can be compared with a public key to see if the two form a pair.", "response": "def fingerprint(self):\n        \"\"\"\n        Creates a fingerprint that can be compared with a public key to see if\n        the two form a pair.\n\n        This fingerprint is not compatible with fingerprints generated by any\n        other software.\n\n        :return:\n            A byte string that is a sha256 hash of selected components (based\n            on the key type)\n        \"\"\"\n\n        if self._fingerprint is None:\n            params = self['private_key_algorithm']['parameters']\n            key = self['private_key'].parsed\n\n            if self.algorithm == 'rsa':\n                to_hash = '%d:%d' % (\n                    key['modulus'].native,\n                    key['public_exponent'].native,\n                )\n\n            elif self.algorithm == 'dsa':\n                public_key = self.public_key\n                to_hash = '%d:%d:%d:%d' % (\n                    params['p'].native,\n                    params['q'].native,\n                    params['g'].native,\n                    public_key.native,\n                )\n\n            elif self.algorithm == 'ec':\n                public_key = key['public_key'].native\n                if public_key is None:\n                    public_key = self.public_key.native\n\n                if params.name == 'named':\n                    to_hash = '%s:' % params.chosen.native\n                    to_hash = to_hash.encode('utf-8')\n                    to_hash += public_key\n\n                elif params.name == 'implicit_ca':\n                    to_hash = public_key\n\n                elif params.name == 'specified':\n                    to_hash = '%s:' % params.chosen['field_id']['parameters'].native\n                    to_hash = to_hash.encode('utf-8')\n                    to_hash += b':' + params.chosen['curve']['a'].native\n                    to_hash += b':' + params.chosen['curve']['b'].native\n                    to_hash += public_key\n\n            if isinstance(to_hash, str_cls):\n                to_hash = to_hash.encode('utf-8')\n\n            self._fingerprint = hashlib.sha256(to_hash).digest()\n\n        return self._fingerprint"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwrapping a public key in a PublicKeyInfo structure.", "response": "def wrap(cls, public_key, algorithm):\n        \"\"\"\n        Wraps a public key in a PublicKeyInfo structure\n\n        :param public_key:\n            A byte string or Asn1Value object of the public key\n\n        :param algorithm:\n            A unicode string of \"rsa\"\n\n        :return:\n            A PublicKeyInfo object\n        \"\"\"\n\n        if not isinstance(public_key, byte_cls) and not isinstance(public_key, Asn1Value):\n            raise TypeError(unwrap(\n                '''\n                public_key must be a byte string or Asn1Value, not %s\n                ''',\n                type_name(public_key)\n            ))\n\n        if algorithm != 'rsa':\n            raise ValueError(unwrap(\n                '''\n                algorithm must \"rsa\", not %s\n                ''',\n                repr(algorithm)\n            ))\n\n        algo = PublicKeyAlgorithm()\n        algo['algorithm'] = PublicKeyAlgorithmId(algorithm)\n        algo['parameters'] = Null()\n\n        container = cls()\n        container['algorithm'] = algo\n        if isinstance(public_key, Asn1Value):\n            public_key = public_key.untag().dump()\n        container['public_key'] = ParsableOctetBitString(public_key)\n\n        return container"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unwrap(self):\n\n        if self.algorithm == 'rsa':\n            return self['public_key'].parsed\n\n        key_type = self.algorithm.upper()\n        a_an = 'an' if key_type == 'EC' else 'a'\n        raise ValueError(unwrap(\n            '''\n            Only RSA public keys may be unwrapped - this key is %s %s public\n            key\n            ''',\n            a_an,\n            key_type\n        ))", "response": "Unwraps an RSA public key into an RSAPublicKey object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef curve(self):\n\n        if self.algorithm != 'ec':\n            raise ValueError(unwrap(\n                '''\n                Only EC keys have a curve, this key is %s\n                ''',\n                self.algorithm.upper()\n            ))\n\n        params = self['algorithm']['parameters']\n        chosen = params.chosen\n\n        if params.name == 'implicit_ca':\n            value = None\n        else:\n            value = chosen.native\n\n        return (params.name, value)", "response": "Returns information about the curve used for an EC key."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the name of the family of hash algorithms used to generate a key in the DSA file.", "response": "def hash_algo(self):\n        \"\"\"\n        Returns the name of the family of hash algorithms used to generate a\n        DSA key\n\n        :raises:\n            ValueError - when the key is not a DSA key\n\n        :return:\n            A unicode string of \"sha1\" or \"sha2\" or None if no parameters are\n            present\n        \"\"\"\n\n        if self.algorithm != 'dsa':\n            raise ValueError(unwrap(\n                '''\n                Only DSA keys are generated using a hash algorithm, this key is\n                %s\n                ''',\n                self.algorithm.upper()\n            ))\n\n        parameters = self['algorithm']['parameters']\n        if parameters.native is None:\n            return None\n\n        byte_len = math.log(parameters['q'].native, 2) / 8\n\n        return 'sha1' if byte_len <= 20 else 'sha2'"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the SHA1 hash of the DER - encoded bytes of this public key info.", "response": "def sha1(self):\n        \"\"\"\n        :return:\n            The SHA1 hash of the DER-encoded bytes of this public key info\n        \"\"\"\n\n        if self._sha1 is None:\n            self._sha1 = hashlib.sha1(byte_cls(self['public_key'])).digest()\n        return self._sha1"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sha256(self):\n\n        if self._sha256 is None:\n            self._sha256 = hashlib.sha256(byte_cls(self['public_key'])).digest()\n        return self._sha256", "response": "Returns the SHA - 256 hash of the DER - encoded bytes of this public key info."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a sdist. tar. gz and a bdist_wheel. whl and uploads them to pypi", "response": "def run():\n    \"\"\"\n    Creates a sdist .tar.gz and a bdist_wheel --univeral .whl and uploads\n    them to pypi\n\n    :return:\n        A bool - if the packaging and upload process was successful\n    \"\"\"\n\n    setup_file = os.path.join(package_root, 'setup.py')\n\n    git_wc_proc = subprocess.Popen(\n        ['git', 'status', '--porcelain', '-uno'],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        cwd=package_root\n    )\n    git_wc_status, _ = git_wc_proc.communicate()\n\n    if len(git_wc_status) > 0:\n        print(git_wc_status.decode('utf-8').rstrip(), file=sys.stderr)\n        print('Unable to perform release since working copy is not clean', file=sys.stderr)\n        return False\n\n    git_tag_proc = subprocess.Popen(\n        ['git', 'tag', '-l', '--contains', 'HEAD'],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        cwd=package_root\n    )\n    tag, tag_error = git_tag_proc.communicate()\n\n    if len(tag_error) > 0:\n        print(tag_error.decode('utf-8').rstrip(), file=sys.stderr)\n        print('Error looking for current git tag', file=sys.stderr)\n        return False\n\n    if len(tag) == 0:\n        print('No git tag found on HEAD', file=sys.stderr)\n        return False\n\n    tag = tag.decode('ascii').strip()\n\n    setuptools.sandbox.run_setup(\n        setup_file,\n        ['sdist', 'bdist_wheel', '--universal']\n    )\n\n    twine.cli.dispatch(['upload', 'dist/%s-%s*' % (package_name, tag)])\n\n    setuptools.sandbox.run_setup(\n        setup_file,\n        ['clean']\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns the tests while measuring coverage data", "response": "def run(ci=False):\n    \"\"\"\n    Runs the tests while measuring coverage\n\n    :param ci:\n        If coverage is being run in a CI environment - this triggers trying to\n        run the tests for the rest of modularcrypto and uploading coverage data\n\n    :return:\n        A bool - if the tests ran successfully\n    \"\"\"\n\n    xml_report_path = os.path.join(package_root, 'coverage.xml')\n    if os.path.exists(xml_report_path):\n        os.unlink(xml_report_path)\n\n    cov = coverage.Coverage(include='%s/*.py' % package_name)\n    cov.start()\n\n    from .tests import run as run_tests\n    result = run_tests()\n    print()\n\n    if ci:\n        suite = unittest.TestSuite()\n        loader = unittest.TestLoader()\n        for other_package in other_packages:\n            for test_class in _load_package_tests(other_package):\n                suite.addTest(loader.loadTestsFromTestCase(test_class))\n\n        if suite.countTestCases() > 0:\n            print('Running tests from other modularcrypto packages')\n            sys.stdout.flush()\n            runner_result = unittest.TextTestRunner(stream=sys.stdout, verbosity=1).run(suite)\n            result = runner_result.wasSuccessful() and result\n            print()\n            sys.stdout.flush()\n\n    cov.stop()\n    cov.save()\n\n    cov.report(show_missing=False)\n    print()\n    sys.stdout.flush()\n    if ci:\n        cov.xml_report()\n\n    if ci and result and os.path.exists(xml_report_path):\n        _codecov_submit()\n        print()\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _git_command(params, cwd):\n\n    proc = subprocess.Popen(\n        ['git'] + params,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        cwd=cwd\n    )\n    stdout, stderr = proc.communicate()\n    code = proc.wait()\n    if code != 0:\n        e = OSError('git exit code was non-zero')\n        e.stdout = stdout\n        raise e\n    return stdout.decode('utf-8').strip()", "response": "Executes a git command and returns the output"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a basic VAR = value data file into a dict of name = value data", "response": "def _parse_env_var_file(data):\n    \"\"\"\n    Parses a basic VAR=\"value data\" file contents into a dict\n\n    :param data:\n        A unicode string of the file data\n\n    :return:\n        A dict of parsed name/value data\n    \"\"\"\n\n    output = {}\n    for line in data.splitlines():\n        line = line.strip()\n        if not line or '=' not in line:\n            continue\n        parts = line.split('=')\n        if len(parts) != 2:\n            continue\n        name = parts[0]\n        value = parts[1]\n        if len(value) > 1:\n            if value[0] == '\"' and value[-1] == '\"':\n                value = value[1:-1]\n        output[name] = value\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _platform_name():\n\n    if sys.platform == 'darwin':\n        version = _plat.mac_ver()[0]\n        _plat_ver_info = tuple(map(int, version.split('.')))\n        if _plat_ver_info < (10, 12):\n            name = 'OS X'\n        else:\n            name = 'macOS'\n        return '%s %s' % (name, version)\n\n    elif sys.platform == 'win32':\n        _win_ver = sys.getwindowsversion()\n        _plat_ver_info = (_win_ver[0], _win_ver[1])\n        return 'Windows %s' % _plat.win32_ver()[0]\n\n    elif sys.platform in ['linux', 'linux2']:\n        if os.path.exists('/etc/os-release'):\n            with open('/etc/os-release', 'r', encoding='utf-8') as f:\n                pairs = _parse_env_var_file(f.read())\n                if 'NAME' in pairs and 'VERSION_ID' in pairs:\n                    return '%s %s' % (pairs['NAME'], pairs['VERSION_ID'])\n                    version = pairs['VERSION_ID']\n                elif 'PRETTY_NAME' in pairs:\n                    return pairs['PRETTY_NAME']\n                elif 'NAME' in pairs:\n                    return pairs['NAME']\n                else:\n                    raise ValueError('No suitable version info found in /etc/os-release')\n        elif os.path.exists('/etc/lsb-release'):\n            with open('/etc/lsb-release', 'r', encoding='utf-8') as f:\n                pairs = _parse_env_var_file(f.read())\n                if 'DISTRIB_DESCRIPTION' in pairs:\n                    return pairs['DISTRIB_DESCRIPTION']\n                else:\n                    raise ValueError('No suitable version info found in /etc/lsb-release')\n        else:\n            return 'Linux'\n\n    else:\n        return '%s %s' % (_plat.system(), _plat.release())", "response": "Returns the name of the current operating system and version of the current operating system."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist all of the files in a directory and returns a list of all of the files that are not ignored by gitignore with root.", "response": "def _list_files(root):\n    \"\"\"\n    Lists all of the files in a directory, taking into account any .gitignore\n    file that is present\n\n    :param root:\n        A unicode filesystem path\n\n    :return:\n        A list of unicode strings, containing paths of all files not ignored\n        by .gitignore with root, using relative paths\n    \"\"\"\n\n    dir_patterns, file_patterns = _gitignore(root)\n    paths = []\n    prefix = os.path.abspath(root) + os.sep\n    for base, dirs, files in os.walk(root):\n        for d in dirs:\n            for dir_pattern in dir_patterns:\n                if fnmatch(d, dir_pattern):\n                    dirs.remove(d)\n                    break\n        for f in files:\n            skip = False\n            for file_pattern in file_patterns:\n                if fnmatch(f, file_pattern):\n                    skip = True\n                    break\n            if skip:\n                continue\n            full_path = os.path.join(base, f)\n            if full_path[:len(prefix)] == prefix:\n                full_path = full_path[len(prefix):]\n            paths.append(full_path)\n    return sorted(paths)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _gitignore(root):\n\n    gitignore_path = os.path.join(root, '.gitignore')\n\n    dir_patterns = ['.git']\n    file_patterns = []\n\n    if not os.path.exists(gitignore_path):\n        return (dir_patterns, file_patterns)\n\n    with open(gitignore_path, 'r', encoding='utf-8') as f:\n        for line in f.readlines():\n            line = line.strip()\n            if not line:\n                continue\n            if line.startswith('#'):\n                continue\n            if '**' in line:\n                raise NotImplementedError('gitignore ** wildcards are not implemented')\n            if line.startswith('!'):\n                raise NotImplementedError('gitignore pattern negation is not implemented')\n            if line.startswith('/'):\n                raise NotImplementedError('gitignore anchored patterns are not implemented')\n            if line.startswith('\\\\#'):\n                line = '#' + line[2:]\n            if line.startswith('\\\\!'):\n                line = '!' + line[2:]\n            if line.endswith('/'):\n                dir_patterns.append(line[:-1])\n            else:\n                file_patterns.append(line)\n\n    return (dir_patterns, file_patterns)", "response": "Parses a gitignore file and returns patterns to match dirs and files."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _do_request(method, url, headers, data=None, query_params=None, timeout=20):\n\n    if query_params:\n        url += '?' + urlencode(query_params).replace('+', '%20')\n\n    if isinstance(data, dict):\n        data_bytes = {}\n        for key in data:\n            data_bytes[key.encode('utf-8')] = data[key].encode('utf-8')\n        data = urlencode(data_bytes)\n        headers['Content-Type'] = 'application/x-www-form-urlencoded'\n    if isinstance(data, str_cls):\n        raise TypeError('data must be a byte string')\n\n    try:\n        tempfd, tempf_path = tempfile.mkstemp('-coverage')\n        os.write(tempfd, data or b'')\n        os.close(tempfd)\n\n        if sys.platform == 'win32':\n            powershell_exe = os.path.join('system32\\\\WindowsPowerShell\\\\v1.0\\\\powershell.exe')\n            code = \"[System.Net.ServicePointManager]::SecurityProtocol = [System.Net.SecurityProtocolType]::Tls12;\"\n            code += \"$wc = New-Object Net.WebClient;\"\n            for key in headers:\n                code += \"$wc.Headers.add('%s','%s');\" % (key, headers[key])\n            code += \"$out = $wc.UploadFile('%s', '%s', '%s');\" % (url, method, tempf_path)\n            code += \"[System.Text.Encoding]::GetEncoding('ISO-8859-1').GetString($wc.ResponseHeaders.ToByteArray())\"\n\n            # To properly obtain bytes, we use BitConverter to get hex dash\n            # encoding (e.g. AE-09-3F) and they decode in python\n            code += \" + [System.BitConverter]::ToString($out);\"\n            stdout, stderr = _execute([powershell_exe, '-Command', code], os.getcwd())\n            if stdout[-2:] == b'\\r\\n' and b'\\r\\n\\r\\n' in stdout:\n                # An extra trailing crlf is added at the end by powershell\n                stdout = stdout[0:-2]\n                parts = stdout.split(b'\\r\\n\\r\\n', 1)\n                if len(parts) == 2:\n                    stdout = parts[0] + b'\\r\\n\\r\\n' + codecs.decode(parts[1].replace(b'-', b''), 'hex_codec')\n\n        else:\n            args = [\n                'curl',\n                '--request',\n                method,\n                '--location',\n                '--silent',\n                '--show-error',\n                '--include',\n                # Prevent curl from asking for an HTTP \"100 Continue\" response\n                '--header', 'Expect:'\n            ]\n            for key in headers:\n                args.append('--header')\n                args.append(\"%s: %s\" % (key, headers[key]))\n            args.append('--data-binary')\n            args.append('@%s' % tempf_path)\n            args.append(url)\n            stdout, stderr = _execute(args, os.getcwd())\n    finally:\n        if tempf_path and os.path.exists(tempf_path):\n            os.remove(tempf_path)\n\n    if len(stderr) > 0:\n        raise URLError(\"Error %sing %s:\\n%s\" % (method, url, stderr))\n\n    parts = stdout.split(b'\\r\\n\\r\\n', 1)\n    if len(parts) != 2:\n        raise URLError(\"Error %sing %s, response data malformed:\\n%s\" % (method, url, stdout))\n    header_block, body = parts\n\n    content_type_header = None\n    content_len_header = None\n    for hline in header_block.decode('iso-8859-1').splitlines():\n        hline_parts = hline.split(':', 1)\n        if len(hline_parts) != 2:\n            continue\n        name, val = hline_parts\n        name = name.strip().lower()\n        val = val.strip()\n        if name == 'content-type':\n            content_type_header = val\n        if name == 'content-length':\n            content_len_header = val\n\n    if content_type_header is None and content_len_header != '0':\n        raise URLError(\"Error %sing %s, no content-type header:\\n%s\" % (method, url, stdout))\n\n    if content_type_header is None:\n        content_type = 'text/plain'\n        encoding = 'utf-8'\n    else:\n        content_type, params = cgi.parse_header(content_type_header)\n        encoding = params.get('charset')\n\n    return (content_type, encoding, body)", "response": "This function performs a HTTP request to the specified URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _execute(params, cwd):\n\n    proc = subprocess.Popen(\n        params,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        cwd=cwd\n    )\n    stdout, stderr = proc.communicate()\n    code = proc.wait()\n    if code != 0:\n        e = OSError('subprocess exit code for %r was %d: %s' % (params, code, stderr))\n        e.stdout = stdout\n        e.stderr = stderr\n        raise e\n    return (stdout, stderr)", "response": "Executes a subprocess and returns stdout and stderr"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run():\n\n    deps_dir = os.path.join(build_root, 'modularcrypto-deps')\n    if os.path.exists(deps_dir):\n        shutil.rmtree(deps_dir, ignore_errors=True)\n    os.mkdir(deps_dir)\n\n    try:\n        print(\"Staging ci dependencies\")\n        _stage_requirements(deps_dir, os.path.join(package_root, 'requires', 'ci'))\n\n        print(\"Checking out modularcrypto packages for coverage\")\n        for other_package in other_packages:\n            pkg_url = 'https://github.com/wbond/%s.git' % other_package\n            pkg_dir = os.path.join(build_root, other_package)\n            if os.path.exists(pkg_dir):\n                print(\"%s is already present\" % other_package)\n                continue\n            print(\"Cloning %s\" % pkg_url)\n            _execute(['git', 'clone', pkg_url], build_root)\n        print()\n\n    except (Exception):\n        if os.path.exists(deps_dir):\n            shutil.rmtree(deps_dir, ignore_errors=True)\n        raise\n\n    return True", "response": "Installs required development dependencies. Uses git to checkout modularcrypto repos for more accurate coverage data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndownload a URL to a directory", "response": "def _download(url, dest):\n    \"\"\"\n    Downloads a URL to a directory\n\n    :param url:\n        The URL to download\n\n    :param dest:\n        The path to the directory to save the file in\n\n    :return:\n        The filesystem path to the saved file\n    \"\"\"\n\n    print('Downloading %s' % url)\n    filename = os.path.basename(url)\n    dest_path = os.path.join(dest, filename)\n\n    if sys.platform == 'win32':\n        powershell_exe = os.path.join('system32\\\\WindowsPowerShell\\\\v1.0\\\\powershell.exe')\n        code = \"[System.Net.ServicePointManager]::SecurityProtocol = [System.Net.SecurityProtocolType]::Tls12;\"\n        code += \"(New-Object Net.WebClient).DownloadFile('%s', '%s');\" % (url, dest_path)\n        _execute([powershell_exe, '-Command', code], dest)\n\n    else:\n        _execute(['curl', '-L', '--silent', '--show-error', '-O', url], dest)\n\n    return dest_path"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nopens an archive object", "response": "def _open_archive(path):\n    \"\"\"\n    :param path:\n        A unicode string of the filesystem path to the archive\n\n    :return:\n        An archive object\n    \"\"\"\n\n    if path.endswith('.zip'):\n        return zipfile.ZipFile(path, 'r')\n    return tarfile.open(path, 'r')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _list_archive_members(archive):\n\n    if isinstance(archive, zipfile.ZipFile):\n        return archive.infolist()\n    return archive.getmembers()", "response": "Returns a list of info objects from an archive"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _archive_single_dir(archive):\n\n    common_root = None\n    for info in _list_archive_members(archive):\n        fn = _info_name(info)\n        if fn in set(['.', '/']):\n            continue\n        sep = None\n        if '/' in fn:\n            sep = '/'\n        elif '\\\\' in fn:\n            sep = '\\\\'\n        if sep is None:\n            root_dir = fn\n        else:\n            root_dir, _ = fn.split(sep, 1)\n        if common_root is None:\n            common_root = root_dir\n        else:\n            if common_root != root_dir:\n                return None\n    return common_root", "response": "Check if all members of the archive are in a single top - level directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _info_name(info):\n\n    if isinstance(info, zipfile.ZipInfo):\n        return info.filename.replace('\\\\', '/')\n    return info.name.replace('\\\\', '/')", "response": "Returns a normalized file path for an archive info object"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract the contents of an archive info object into a byte string.", "response": "def _extract_info(archive, info):\n    \"\"\"\n    Extracts the contents of an archive info object\n\n    ;param archive:\n        An archive from _open_archive()\n\n    :param info:\n        An info object from _list_archive_members()\n\n    :return:\n        None, or a byte string of the file contents\n    \"\"\"\n\n    if isinstance(archive, zipfile.ZipFile):\n        fn = info.filename\n        is_dir = fn.endswith('/') or fn.endswith('\\\\')\n        out = archive.read(info)\n        if is_dir and out == b'':\n            return None\n        return out\n\n    info_file = archive.extractfile(info)\n    if info_file:\n        return info_file.read()\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract a. whl or. zip or. tar. bz2 or. tar. bz2 into a package path to be extracted.", "response": "def _extract_package(deps_dir, pkg_path):\n    \"\"\"\n    Extract a .whl, .zip, .tar.gz or .tar.bz2 into a package path to\n    use when running CI tasks\n\n    :param deps_dir:\n        A unicode string of the directory the package should be extracted to\n\n    :param pkg_path:\n        A unicode string of the path to the archive\n    \"\"\"\n\n    if pkg_path.endswith('.exe'):\n        try:\n            zf = None\n            zf = zipfile.ZipFile(pkg_path, 'r')\n            # Exes have a PLATLIB folder containing everything we want\n            for zi in zf.infolist():\n                if not zi.filename.startswith('PLATLIB'):\n                    continue\n                data = _extract_info(zf, zi)\n                if data is not None:\n                    dst_path = os.path.join(deps_dir, zi.filename[8:])\n                    dst_dir = os.path.dirname(dst_path)\n                    if not os.path.exists(dst_dir):\n                        os.makedirs(dst_dir)\n                    with open(dst_path, 'wb') as f:\n                        f.write(data)\n        finally:\n            if zf:\n                zf.close()\n        return\n\n    if pkg_path.endswith('.whl'):\n        try:\n            zf = None\n            zf = zipfile.ZipFile(pkg_path, 'r')\n            # Wheels contain exactly what we need and nothing else\n            zf.extractall(deps_dir)\n        finally:\n            if zf:\n                zf.close()\n        return\n\n    # Source archives may contain a bunch of other things.\n    # The following code works for the packages coverage and\n    # configparser, which are the two we currently require that\n    # do not provide wheels\n\n    try:\n        ar = None\n        ar = _open_archive(pkg_path)\n\n        pkg_name = None\n        base_path = _archive_single_dir(ar) or ''\n        if len(base_path):\n            if '-' in base_path:\n                pkg_name, _ = base_path.split('-', 1)\n            base_path += '/'\n\n        base_pkg_path = None\n        if pkg_name is not None:\n            base_pkg_path = base_path + pkg_name + '/'\n        src_path = base_path + 'src/'\n\n        members = []\n        for info in _list_archive_members(ar):\n            fn = _info_name(info)\n            if base_pkg_path is not None and fn.startswith(base_pkg_path):\n                dst_path = fn[len(base_pkg_path) - len(pkg_name) - 1:]\n                members.append((info, dst_path))\n                continue\n            if fn.startswith(src_path):\n                members.append((info, fn[len(src_path):]))\n                continue\n\n        for info, path in members:\n            info_data = _extract_info(ar, info)\n            # Dirs won't return a file\n            if info_data is not None:\n                dst_path = os.path.join(deps_dir, path)\n                dst_dir = os.path.dirname(dst_path)\n                if not os.path.exists(dst_dir):\n                    os.makedirs(dst_dir)\n                with open(dst_path, 'wb') as f:\n                    f.write(info_data)\n    finally:\n        if ar:\n            ar.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninstalling requirements without using Python to download", "response": "def _stage_requirements(deps_dir, path):\n    \"\"\"\n    Installs requirements without using Python to download, since\n    different services are limiting to TLS 1.2, and older version of\n    Python do not support that\n\n    :param deps_dir:\n        A unicode path to a temporary diretory to use for downloads\n\n    :param path:\n        A unicoe filesystem path to a requirements file\n    \"\"\"\n\n    valid_tags = _pep425tags()\n\n    exe_suffix = None\n    if sys.platform == 'win32' and _pep425_implementation() == 'cp':\n        win_arch = 'win32' if sys.maxsize == 2147483647 else 'win-amd64'\n        version_info = sys.version_info\n        exe_suffix = '.%s-py%d.%d.exe' % (win_arch, version_info[0], version_info[1])\n\n    packages = _parse_requires(path)\n    for p in packages:\n        pkg = p['pkg']\n        if p['type'] == 'url':\n            if pkg.endswith('.zip') or pkg.endswith('.tar.gz') or pkg.endswith('.tar.bz2') or pkg.endswith('.whl'):\n                url = pkg\n            else:\n                raise Exception('Unable to install package from URL that is not an archive')\n        else:\n            pypi_json_url = 'https://pypi.org/pypi/%s/json' % pkg\n            json_dest = _download(pypi_json_url, deps_dir)\n            with open(json_dest, 'rb') as f:\n                pkg_info = json.loads(f.read().decode('utf-8'))\n            if os.path.exists(json_dest):\n                os.remove(json_dest)\n\n            latest = pkg_info['info']['version']\n            if p['type'] == '>=':\n                if _tuple_from_ver(p['ver']) > _tuple_from_ver(latest):\n                    raise Exception('Unable to find version %s of %s, newest is %s' % (p['ver'], pkg, latest))\n                version = latest\n            elif p['type'] == '==':\n                if p['ver'] not in pkg_info['releases']:\n                    raise Exception('Unable to find version %s of %s' % (p['ver'], pkg))\n                version = p['ver']\n            else:\n                version = latest\n\n            wheels = {}\n            whl = None\n            tar_bz2 = None\n            tar_gz = None\n            exe = None\n            for download in pkg_info['releases'][version]:\n                if exe_suffix and download['url'].endswith(exe_suffix):\n                    exe = download['url']\n                if download['url'].endswith('.whl'):\n                    parts = os.path.basename(download['url']).split('-')\n                    tag_impl = parts[-3]\n                    tag_abi = parts[-2]\n                    tag_arch = parts[-1].split('.')[0]\n                    wheels[(tag_impl, tag_abi, tag_arch)] = download['url']\n                if download['url'].endswith('.tar.bz2'):\n                    tar_bz2 = download['url']\n                if download['url'].endswith('.tar.gz'):\n                    tar_gz = download['url']\n\n            # Find the most-specific wheel possible\n            for tag in valid_tags:\n                if tag in wheels:\n                    whl = wheels[tag]\n                    break\n\n            if exe_suffix and exe:\n                url = exe\n            elif whl:\n                url = whl\n            elif tar_bz2:\n                url = tar_bz2\n            elif tar_gz:\n                url = tar_gz\n            else:\n                raise Exception('Unable to find suitable download for %s' % pkg)\n\n        local_path = _download(url, deps_dir)\n\n        _extract_package(deps_dir, local_path)\n\n        os.remove(local_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_requires(path):\n\n    python_version = '.'.join(map(str_cls, sys.version_info[0:2]))\n    sys_platform = sys.platform\n\n    packages = []\n\n    with open(path, 'rb') as f:\n        contents = f.read().decode('utf-8')\n\n    for line in re.split(r'\\r?\\n', contents):\n        line = line.strip()\n        if not len(line):\n            continue\n        if re.match(r'^\\s*#', line):\n            continue\n        if ';' in line:\n            package, cond = line.split(';', 1)\n            package = package.strip()\n            cond = cond.strip()\n            cond = cond.replace('sys_platform', repr(sys_platform))\n            cond = cond.replace('python_version', repr(python_version))\n            if not eval(cond):\n                continue\n        else:\n            package = line.strip()\n\n        if re.match(r'^\\s*-r\\s*', package):\n            sub_req_file = re.sub(r'^\\s*-r\\s*', '', package)\n            sub_req_file = os.path.abspath(os.path.join(os.path.dirname(path), sub_req_file))\n            packages.extend(_parse_requires(sub_req_file))\n            continue\n\n        if re.match(r'https?://', package):\n            packages.append({'type': 'url', 'pkg': package})\n            continue\n\n        if '>=' in package:\n            parts = package.split('>=')\n            package = parts[0].strip()\n            ver = parts[1].strip()\n            packages.append({'type': '>=', 'pkg': package, 'ver': ver})\n            continue\n\n        if '==' in package:\n            parts = package.split('==')\n            package = parts[0].strip()\n            ver = parts[1].strip()\n            packages.append({'type': '==', 'pkg': package, 'ver': ver})\n            continue\n\n        if re.search(r'[^ a-zA-Z0-9\\-]', package):\n            raise Exception('Unsupported requirements format version constraint: %s' % package)\n\n        packages.append({'type': 'any', 'pkg': package})\n\n    return packages", "response": "Parses pip requirements files and returns a list of dicts containing the keys type url pkg ver"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef detect(byte_string):\n\n    if not isinstance(byte_string, byte_cls):\n        raise TypeError(unwrap(\n            '''\n            byte_string must be a byte string, not %s\n            ''',\n            _type_name(byte_string)\n        ))\n\n    return byte_string.find(b'-----BEGIN') != -1 or byte_string.find(b'---- BEGIN') != -1", "response": "Detects if a byte string seems to contain a PEM - encoded block of a node."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a PEM - encoded byte string into one or more DER - encoded byte strings.", "response": "def _unarmor(pem_bytes):\n    \"\"\"\n    Convert a PEM-encoded byte string into one or more DER-encoded byte strings\n\n    :param pem_bytes:\n        A byte string of the PEM-encoded data\n\n    :raises:\n        ValueError - when the pem_bytes do not appear to be PEM-encoded bytes\n\n    :return:\n        A generator of 3-element tuples in the format: (object_type, headers,\n        der_bytes). The object_type is a unicode string of what is between\n        \"-----BEGIN \" and \"-----\". Examples include: \"CERTIFICATE\",\n        \"PUBLIC KEY\", \"PRIVATE KEY\". The headers is a dict containing any lines\n        in the form \"Name: Value\" that are right after the begin line.\n    \"\"\"\n\n    if not isinstance(pem_bytes, byte_cls):\n        raise TypeError(unwrap(\n            '''\n            pem_bytes must be a byte string, not %s\n            ''',\n            _type_name(pem_bytes)\n        ))\n\n    # Valid states include: \"trash\", \"headers\", \"body\"\n    state = 'trash'\n    headers = {}\n    base64_data = b''\n    object_type = None\n\n    found_start = False\n    found_end = False\n\n    for line in pem_bytes.splitlines(False):\n        if line == b'':\n            continue\n\n        if state == \"trash\":\n            # Look for a starting line since some CA cert bundle show the cert\n            # into in a parsed format above each PEM block\n            type_name_match = re.match(b'^(?:---- |-----)BEGIN ([A-Z0-9 ]+)(?: ----|-----)', line)\n            if not type_name_match:\n                continue\n            object_type = type_name_match.group(1).decode('ascii')\n\n            found_start = True\n            state = 'headers'\n            continue\n\n        if state == 'headers':\n            if line.find(b':') == -1:\n                state = 'body'\n            else:\n                decoded_line = line.decode('ascii')\n                name, value = decoded_line.split(':', 1)\n                headers[name] = value.strip()\n                continue\n\n        if state == 'body':\n            if line[0:5] in (b'-----', b'---- '):\n                der_bytes = base64.b64decode(base64_data)\n\n                yield (object_type, headers, der_bytes)\n\n                state = 'trash'\n                headers = {}\n                base64_data = b''\n                object_type = None\n                found_end = True\n                continue\n\n            base64_data += line\n\n    if not found_start or not found_end:\n        raise ValueError(unwrap(\n            '''\n            pem_bytes does not appear to contain PEM-encoded data - no\n            BEGIN/END combination found\n            '''\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a PEM - encoded byte string into a 3 - element dict.", "response": "def unarmor(pem_bytes, multiple=False):\n    \"\"\"\n    Convert a PEM-encoded byte string into a DER-encoded byte string\n\n    :param pem_bytes:\n        A byte string of the PEM-encoded data\n\n    :param multiple:\n        If True, function will return a generator\n\n    :raises:\n        ValueError - when the pem_bytes do not appear to be PEM-encoded bytes\n\n    :return:\n        A 3-element tuple (object_name, headers, der_bytes). The object_name is\n        a unicode string of what is between \"-----BEGIN \" and \"-----\". Examples\n        include: \"CERTIFICATE\", \"PUBLIC KEY\", \"PRIVATE KEY\". The headers is a\n        dict containing any lines in the form \"Name: Value\" that are right\n        after the begin line.\n    \"\"\"\n\n    generator = _unarmor(pem_bytes)\n\n    if not multiple:\n        return next(generator)\n\n    return generator"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the signature algorithm of the current locale.", "response": "def signature_algo(self):\n        \"\"\"\n        :return:\n            A unicode string of \"rsassa_pkcs1v15\", \"rsassa_pss\", \"dsa\" or\n            \"ecdsa\"\n        \"\"\"\n\n        algorithm = self['algorithm'].native\n\n        algo_map = {\n            'md2_rsa': 'rsassa_pkcs1v15',\n            'md5_rsa': 'rsassa_pkcs1v15',\n            'sha1_rsa': 'rsassa_pkcs1v15',\n            'sha224_rsa': 'rsassa_pkcs1v15',\n            'sha256_rsa': 'rsassa_pkcs1v15',\n            'sha384_rsa': 'rsassa_pkcs1v15',\n            'sha512_rsa': 'rsassa_pkcs1v15',\n            'rsassa_pkcs1v15': 'rsassa_pkcs1v15',\n            'rsassa_pss': 'rsassa_pss',\n            'sha1_dsa': 'dsa',\n            'sha224_dsa': 'dsa',\n            'sha256_dsa': 'dsa',\n            'dsa': 'dsa',\n            'sha1_ecdsa': 'ecdsa',\n            'sha224_ecdsa': 'ecdsa',\n            'sha256_ecdsa': 'ecdsa',\n            'sha384_ecdsa': 'ecdsa',\n            'sha512_ecdsa': 'ecdsa',\n            'sha3_224_ecdsa': 'ecdsa',\n            'sha3_256_ecdsa': 'ecdsa',\n            'sha3_384_ecdsa': 'ecdsa',\n            'sha3_512_ecdsa': 'ecdsa',\n            'ecdsa': 'ecdsa',\n        }\n        if algorithm in algo_map:\n            return algo_map[algorithm]\n\n        raise ValueError(unwrap(\n            '''\n            Signature algorithm not known for %s\n            ''',\n            algorithm\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the hash algorithm of the current locale.", "response": "def hash_algo(self):\n        \"\"\"\n        :return:\n            A unicode string of \"md2\", \"md5\", \"sha1\", \"sha224\", \"sha256\",\n            \"sha384\", \"sha512\", \"sha512_224\", \"sha512_256\"\n        \"\"\"\n\n        algorithm = self['algorithm'].native\n\n        algo_map = {\n            'md2_rsa': 'md2',\n            'md5_rsa': 'md5',\n            'sha1_rsa': 'sha1',\n            'sha224_rsa': 'sha224',\n            'sha256_rsa': 'sha256',\n            'sha384_rsa': 'sha384',\n            'sha512_rsa': 'sha512',\n            'sha1_dsa': 'sha1',\n            'sha224_dsa': 'sha224',\n            'sha256_dsa': 'sha256',\n            'sha1_ecdsa': 'sha1',\n            'sha224_ecdsa': 'sha224',\n            'sha256_ecdsa': 'sha256',\n            'sha384_ecdsa': 'sha384',\n            'sha512_ecdsa': 'sha512',\n        }\n        if algorithm in algo_map:\n            return algo_map[algorithm]\n\n        if algorithm == 'rsassa_pss':\n            return self['parameters']['hash_algorithm']['algorithm'].native\n\n        raise ValueError(unwrap(\n            '''\n            Hash algorithm not known for %s\n            ''',\n            algorithm\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread a DSASignature object from a byte string encoding accordint to IEEE P1363", "response": "def from_p1363(cls, data):\n        \"\"\"\n        Reads a signature from a byte string encoding accordint to IEEE P1363,\n        which is used by Microsoft's BCryptSignHash() function.\n\n        :param data:\n            A byte string from BCryptSignHash()\n\n        :return:\n            A DSASignature object\n        \"\"\"\n\n        r = int_from_bytes(data[0:len(data) // 2])\n        s = int_from_bytes(data[len(data) // 2:])\n        return cls({'r': r, 's': s})"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndumping a signature to a byte string compatible with Microsoft s BCryptVerifySignature function.", "response": "def to_p1363(self):\n        \"\"\"\n        Dumps a signature to a byte string compatible with Microsoft's\n        BCryptVerifySignature() function.\n\n        :return:\n            A byte string compatible with BCryptVerifySignature()\n        \"\"\"\n\n        r_bytes = int_to_bytes(self['r'].native)\n        s_bytes = int_to_bytes(self['s'].native)\n\n        int_byte_length = max(len(r_bytes), len(s_bytes))\n        r_bytes = fill_width(r_bytes, int_byte_length)\n        s_bytes = fill_width(s_bytes, int_byte_length)\n\n        return r_bytes + s_bytes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the name of the key derivation function to use.", "response": "def kdf(self):\n        \"\"\"\n        Returns the name of the key derivation function to use.\n\n        :return:\n            A unicode from of one of the following: \"pbkdf1\", \"pbkdf2\",\n            \"pkcs12_kdf\"\n        \"\"\"\n\n        encryption_algo = self['algorithm'].native\n\n        if encryption_algo == 'pbes2':\n            return self['parameters']['key_derivation_func']['algorithm'].native\n\n        if encryption_algo.find('.') == -1:\n            if encryption_algo.find('_') != -1:\n                encryption_algo, _ = encryption_algo.split('_', 1)\n\n                if encryption_algo == 'pbes1':\n                    return 'pbkdf1'\n\n                if encryption_algo == 'pkcs12':\n                    return 'pkcs12_kdf'\n\n            raise ValueError(unwrap(\n                '''\n                Encryption algorithm \"%s\" does not have a registered key\n                derivation function\n                ''',\n                encryption_algo\n            ))\n\n        raise ValueError(unwrap(\n            '''\n            Unrecognized encryption algorithm \"%s\", can not determine key\n            derivation function\n            ''',\n            encryption_algo\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the HMAC algorithm to use with the KDF.", "response": "def kdf_hmac(self):\n        \"\"\"\n        Returns the HMAC algorithm to use with the KDF.\n\n        :return:\n            A unicode string of one of the following: \"md2\", \"md5\", \"sha1\",\n            \"sha224\", \"sha256\", \"sha384\", \"sha512\"\n        \"\"\"\n\n        encryption_algo = self['algorithm'].native\n\n        if encryption_algo == 'pbes2':\n            return self['parameters']['key_derivation_func']['parameters']['prf']['algorithm'].native\n\n        if encryption_algo.find('.') == -1:\n            if encryption_algo.find('_') != -1:\n                _, hmac_algo, _ = encryption_algo.split('_', 2)\n                return hmac_algo\n\n            raise ValueError(unwrap(\n                '''\n                Encryption algorithm \"%s\" does not have a registered key\n                derivation function\n                ''',\n                encryption_algo\n            ))\n\n        raise ValueError(unwrap(\n            '''\n            Unrecognized encryption algorithm \"%s\", can not determine key\n            derivation hmac algorithm\n            ''',\n            encryption_algo\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef kdf_salt(self):\n\n        encryption_algo = self['algorithm'].native\n\n        if encryption_algo == 'pbes2':\n            salt = self['parameters']['key_derivation_func']['parameters']['salt']\n\n            if salt.name == 'other_source':\n                raise ValueError(unwrap(\n                    '''\n                    Can not determine key derivation salt - the\n                    reserved-for-future-use other source salt choice was\n                    specified in the PBKDF2 params structure\n                    '''\n                ))\n\n            return salt.native\n\n        if encryption_algo.find('.') == -1:\n            if encryption_algo.find('_') != -1:\n                return self['parameters']['salt'].native\n\n            raise ValueError(unwrap(\n                '''\n                Encryption algorithm \"%s\" does not have a registered key\n                derivation function\n                ''',\n                encryption_algo\n            ))\n\n        raise ValueError(unwrap(\n            '''\n            Unrecognized encryption algorithm \"%s\", can not determine key\n            derivation salt\n            ''',\n            encryption_algo\n        ))", "response": "Returns the byte string to use as the salt for the KDF."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the number of iterations that should be run via the KDF.", "response": "def kdf_iterations(self):\n        \"\"\"\n        Returns the number of iterations that should be run via the KDF.\n\n        :return:\n            An integer\n        \"\"\"\n\n        encryption_algo = self['algorithm'].native\n\n        if encryption_algo == 'pbes2':\n            return self['parameters']['key_derivation_func']['parameters']['iteration_count'].native\n\n        if encryption_algo.find('.') == -1:\n            if encryption_algo.find('_') != -1:\n                return self['parameters']['iterations'].native\n\n            raise ValueError(unwrap(\n                '''\n                Encryption algorithm \"%s\" does not have a registered key\n                derivation function\n                ''',\n                encryption_algo\n            ))\n\n        raise ValueError(unwrap(\n            '''\n            Unrecognized encryption algorithm \"%s\", can not determine key\n            derivation iterations\n            ''',\n            encryption_algo\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the length of the RC5 key in bytes.", "response": "def key_length(self):\n        \"\"\"\n        Returns the key length to pass to the cipher/kdf. The PKCS#5 spec does\n        not specify a way to store the RC5 key length, however this tends not\n        to be a problem since OpenSSL does not support RC5 in PKCS#8 and OS X\n        does not provide an RC5 cipher for use in the Security Transforms\n        library.\n\n        :raises:\n            ValueError - when the key length can not be determined\n\n        :return:\n            An integer representing the length in bytes\n        \"\"\"\n\n        encryption_algo = self['algorithm'].native\n\n        if encryption_algo[0:3] == 'aes':\n            return {\n                'aes128_': 16,\n                'aes192_': 24,\n                'aes256_': 32,\n            }[encryption_algo[0:7]]\n\n        cipher_lengths = {\n            'des': 8,\n            'tripledes_3key': 24,\n        }\n\n        if encryption_algo in cipher_lengths:\n            return cipher_lengths[encryption_algo]\n\n        if encryption_algo == 'rc2':\n            rc2_params = self['parameters'].parsed['encryption_scheme']['parameters'].parsed\n            rc2_parameter_version = rc2_params['rc2_parameter_version'].native\n\n            # See page 24 of\n            # http://www.emc.com/collateral/white-papers/h11302-pkcs5v2-1-password-based-cryptography-standard-wp.pdf\n            encoded_key_bits_map = {\n                160: 5,   # 40-bit\n                120: 8,   # 64-bit\n                58: 16,   # 128-bit\n            }\n\n            if rc2_parameter_version in encoded_key_bits_map:\n                return encoded_key_bits_map[rc2_parameter_version]\n\n            if rc2_parameter_version >= 256:\n                return rc2_parameter_version\n\n            if rc2_parameter_version is None:\n                return 4  # 32-bit default\n\n            raise ValueError(unwrap(\n                '''\n                Invalid RC2 parameter version found in EncryptionAlgorithm\n                parameters\n                '''\n            ))\n\n        if encryption_algo == 'pbes2':\n            key_length = self['parameters']['key_derivation_func']['parameters']['key_length'].native\n            if key_length is not None:\n                return key_length\n\n            # If the KDF params don't specify the key size, we can infer it from\n            # the encryption scheme for all schemes except for RC5. However, in\n            # practical terms, neither OpenSSL or OS X support RC5 for PKCS#8\n            # so it is unlikely to be an issue that is run into.\n\n            return self['parameters']['encryption_scheme'].key_length\n\n        if encryption_algo.find('.') == -1:\n            return {\n                'pbes1_md2_des': 8,\n                'pbes1_md5_des': 8,\n                'pbes1_md2_rc2': 8,\n                'pbes1_md5_rc2': 8,\n                'pbes1_sha1_des': 8,\n                'pbes1_sha1_rc2': 8,\n                'pkcs12_sha1_rc4_128': 16,\n                'pkcs12_sha1_rc4_40': 5,\n                'pkcs12_sha1_tripledes_3key': 24,\n                'pkcs12_sha1_tripledes_2key': 16,\n                'pkcs12_sha1_rc2_128': 16,\n                'pkcs12_sha1_rc2_40': 5,\n            }[encryption_algo]\n\n        raise ValueError(unwrap(\n            '''\n            Unrecognized encryption algorithm \"%s\"\n            ''',\n            encryption_algo\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the name of the encryption mode to use.", "response": "def encryption_mode(self):\n        \"\"\"\n        Returns the name of the encryption mode to use.\n\n        :return:\n            A unicode string from one of the following: \"cbc\", \"ecb\", \"ofb\",\n            \"cfb\", \"wrap\", \"gcm\", \"ccm\", \"wrap_pad\"\n        \"\"\"\n\n        encryption_algo = self['algorithm'].native\n\n        if encryption_algo[0:7] in set(['aes128_', 'aes192_', 'aes256_']):\n            return encryption_algo[7:]\n\n        if encryption_algo[0:6] == 'pbes1_':\n            return 'cbc'\n\n        if encryption_algo[0:7] == 'pkcs12_':\n            return 'cbc'\n\n        if encryption_algo in set(['des', 'tripledes_3key', 'rc2', 'rc5']):\n            return 'cbc'\n\n        if encryption_algo == 'pbes2':\n            return self['parameters']['encryption_scheme'].encryption_mode\n\n        raise ValueError(unwrap(\n            '''\n            Unrecognized encryption algorithm \"%s\"\n            ''',\n            encryption_algo\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encryption_cipher(self):\n\n        encryption_algo = self['algorithm'].native\n\n        if encryption_algo[0:7] in set(['aes128_', 'aes192_', 'aes256_']):\n            return 'aes'\n\n        if encryption_algo in set(['des', 'rc2', 'rc5']):\n            return encryption_algo\n\n        if encryption_algo == 'tripledes_3key':\n            return 'tripledes'\n\n        if encryption_algo == 'pbes2':\n            return self['parameters']['encryption_scheme'].encryption_cipher\n\n        if encryption_algo.find('.') == -1:\n            return {\n                'pbes1_md2_des': 'des',\n                'pbes1_md5_des': 'des',\n                'pbes1_md2_rc2': 'rc2',\n                'pbes1_md5_rc2': 'rc2',\n                'pbes1_sha1_des': 'des',\n                'pbes1_sha1_rc2': 'rc2',\n                'pkcs12_sha1_rc4_128': 'rc4',\n                'pkcs12_sha1_rc4_40': 'rc4',\n                'pkcs12_sha1_tripledes_3key': 'tripledes',\n                'pkcs12_sha1_tripledes_2key': 'tripledes',\n                'pkcs12_sha1_rc2_128': 'rc2',\n                'pkcs12_sha1_rc2_40': 'rc2',\n            }[encryption_algo]\n\n        raise ValueError(unwrap(\n            '''\n            Unrecognized encryption algorithm \"%s\"\n            ''',\n            encryption_algo\n        ))", "response": "Returns the name of the symmetric encryption cipher for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef encryption_block_size(self):\n\n        encryption_algo = self['algorithm'].native\n\n        if encryption_algo[0:7] in set(['aes128_', 'aes192_', 'aes256_']):\n            return 16\n\n        cipher_map = {\n            'des': 8,\n            'tripledes_3key': 8,\n            'rc2': 8,\n        }\n        if encryption_algo in cipher_map:\n            return cipher_map[encryption_algo]\n\n        if encryption_algo == 'rc5':\n            return self['parameters'].parsed['block_size_in_bits'].native / 8\n\n        if encryption_algo == 'pbes2':\n            return self['parameters']['encryption_scheme'].encryption_block_size\n\n        if encryption_algo.find('.') == -1:\n            return {\n                'pbes1_md2_des': 8,\n                'pbes1_md5_des': 8,\n                'pbes1_md2_rc2': 8,\n                'pbes1_md5_rc2': 8,\n                'pbes1_sha1_des': 8,\n                'pbes1_sha1_rc2': 8,\n                'pkcs12_sha1_rc4_128': 0,\n                'pkcs12_sha1_rc4_40': 0,\n                'pkcs12_sha1_tripledes_3key': 8,\n                'pkcs12_sha1_tripledes_2key': 8,\n                'pkcs12_sha1_rc2_128': 8,\n                'pkcs12_sha1_rc2_40': 8,\n            }[encryption_algo]\n\n        raise ValueError(unwrap(\n            '''\n            Unrecognized encryption algorithm \"%s\"\n            ''',\n            encryption_algo\n        ))", "response": "Returns the block size of the encryption cipher in bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef encryption_iv(self):\n\n        encryption_algo = self['algorithm'].native\n\n        if encryption_algo in set(['rc2', 'rc5']):\n            return self['parameters'].parsed['iv'].native\n\n        # For DES/Triple DES and AES the IV is the entirety of the parameters\n        octet_string_iv_oids = set([\n            'des',\n            'tripledes_3key',\n            'aes128_cbc',\n            'aes192_cbc',\n            'aes256_cbc',\n            'aes128_ofb',\n            'aes192_ofb',\n            'aes256_ofb',\n        ])\n        if encryption_algo in octet_string_iv_oids:\n            return self['parameters'].native\n\n        if encryption_algo == 'pbes2':\n            return self['parameters']['encryption_scheme'].encryption_iv\n\n        # All of the PBES1 algos use their KDF to create the IV. For the pbkdf1,\n        # the KDF is told to generate a key that is an extra 8 bytes long, and\n        # that is used for the IV. For the PKCS#12 KDF, it is called with an id\n        # of 2 to generate the IV. In either case, we can't return the IV\n        # without knowing the user's password.\n        if encryption_algo.find('.') == -1:\n            return None\n\n        raise ValueError(unwrap(\n            '''\n            Unrecognized encryption algorithm \"%s\"\n            ''',\n            encryption_algo\n        ))", "response": "Returns the initialization vector for the specified encryption algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set(self, value):\n\n        if not isinstance(value, str_cls):\n            raise TypeError(unwrap(\n                '''\n                %s value must be a unicode string, not %s\n                ''',\n                type_name(self),\n                type_name(value)\n            ))\n\n        if value.startswith('.'):\n            encoded_value = b'.' + value[1:].encode(self._encoding)\n        else:\n            encoded_value = value.encode(self._encoding)\n\n        self._unicode = value\n        self.contents = encoded_value\n        self._header = None\n        if self._trailer != b'':\n            self._trailer = b''", "response": "Sets the value of the DNS name\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the value of the string .", "response": "def set(self, value):\n        \"\"\"\n        Sets the value of the string\n\n        :param value:\n            A unicode string\n        \"\"\"\n\n        if not isinstance(value, str_cls):\n            raise TypeError(unwrap(\n                '''\n                %s value must be a unicode string, not %s\n                ''',\n                type_name(self),\n                type_name(value)\n            ))\n\n        self._unicode = value\n        self.contents = iri_to_uri(value)\n        self._header = None\n        if self._trailer != b'':\n            self._trailer = b''"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the value of the string .", "response": "def set(self, value):\n        \"\"\"\n        Sets the value of the string\n\n        :param value:\n            A unicode string\n        \"\"\"\n\n        if not isinstance(value, str_cls):\n            raise TypeError(unwrap(\n                '''\n                %s value must be a unicode string, not %s\n                ''',\n                type_name(self),\n                type_name(value)\n            ))\n\n        if value.find('@') != -1:\n            mailbox, hostname = value.rsplit('@', 1)\n            encoded_value = mailbox.encode('ascii') + b'@' + hostname.encode('idna')\n        else:\n            encoded_value = value.encode('ascii')\n\n        self._normalized = True\n        self._unicode = value\n        self.contents = encoded_value\n        self._header = None\n        if self._trailer != b'':\n            self._trailer = b''"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set(self, value):\n\n        if not isinstance(value, str_cls):\n            raise TypeError(unwrap(\n                '''\n                %s value must be a unicode string, not %s\n                ''',\n                type_name(self),\n                type_name(value)\n            ))\n\n        original_value = value\n\n        has_cidr = value.find('/') != -1\n        cidr = 0\n        if has_cidr:\n            parts = value.split('/', 1)\n            value = parts[0]\n            cidr = int(parts[1])\n            if cidr < 0:\n                raise ValueError(unwrap(\n                    '''\n                    %s value contains a CIDR range less than 0\n                    ''',\n                    type_name(self)\n                ))\n\n        if value.find(':') != -1:\n            family = socket.AF_INET6\n            if cidr > 128:\n                raise ValueError(unwrap(\n                    '''\n                    %s value contains a CIDR range bigger than 128, the maximum\n                    value for an IPv6 address\n                    ''',\n                    type_name(self)\n                ))\n            cidr_size = 128\n        else:\n            family = socket.AF_INET\n            if cidr > 32:\n                raise ValueError(unwrap(\n                    '''\n                    %s value contains a CIDR range bigger than 32, the maximum\n                    value for an IPv4 address\n                    ''',\n                    type_name(self)\n                ))\n            cidr_size = 32\n\n        cidr_bytes = b''\n        if has_cidr:\n            cidr_mask = '1' * cidr\n            cidr_mask += '0' * (cidr_size - len(cidr_mask))\n            cidr_bytes = int_to_bytes(int(cidr_mask, 2))\n            cidr_bytes = (b'\\x00' * ((cidr_size // 8) - len(cidr_bytes))) + cidr_bytes\n\n        self._native = original_value\n        self.contents = inet_pton(family, value) + cidr_bytes\n        self._bytes = self.contents\n        self._header = None\n        if self._trailer != b'':\n            self._trailer = b''", "response": "Sets the value of the object object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef native(self):\n\n        if self.contents is None:\n            return None\n\n        if self._native is None:\n            byte_string = self.__bytes__()\n            byte_len = len(byte_string)\n            cidr_int = None\n            if byte_len in set([32, 16]):\n                value = inet_ntop(socket.AF_INET6, byte_string[0:16])\n                if byte_len > 16:\n                    cidr_int = int_from_bytes(byte_string[16:])\n            elif byte_len in set([8, 4]):\n                value = inet_ntop(socket.AF_INET, byte_string[0:4])\n                if byte_len > 4:\n                    cidr_int = int_from_bytes(byte_string[4:])\n            if cidr_int is not None:\n                cidr_bits = '{0:b}'.format(cidr_int)\n                cidr = len(cidr_bits.rstrip('0'))\n                value = value + '/' + str_cls(cidr)\n            self._native = value\n        return self._native", "response": "Returns the native Python datatype representation of this value."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an ordering value for a particular attribute key.", "response": "def preferred_ordinal(cls, attr_name):\n        \"\"\"\n        Returns an ordering value for a particular attribute key.\n\n        Unrecognized attributes and OIDs will be sorted lexically at the end.\n\n        :return:\n            An orderable value.\n\n        \"\"\"\n\n        attr_name = cls.map(attr_name)\n        if attr_name in cls.preferred_order:\n            ordinal = cls.preferred_order.index(attr_name)\n        else:\n            ordinal = len(cls.preferred_order)\n\n        return (ordinal, attr_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prepped_value(self):\n\n        if self._prepped is None:\n            self._prepped = self._ldap_string_prep(self['value'].native)\n        return self._prepped", "response": "Returns the value after being processed by the internationalized string\n            preparation as specified by RFC 5280\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprepares a LDAP string for comparison.", "response": "def _ldap_string_prep(self, string):\n        \"\"\"\n        Implements the internationalized string preparation algorithm from\n        RFC 4518. https://tools.ietf.org/html/rfc4518#section-2\n\n        :param string:\n            A unicode string to prepare\n\n        :return:\n            A prepared unicode string, ready for comparison\n        \"\"\"\n\n        # Map step\n        string = re.sub('[\\u00ad\\u1806\\u034f\\u180b-\\u180d\\ufe0f-\\uff00\\ufffc]+', '', string)\n        string = re.sub('[\\u0009\\u000a\\u000b\\u000c\\u000d\\u0085]', ' ', string)\n        if sys.maxunicode == 0xffff:\n            # Some installs of Python 2.7 don't support 8-digit unicode escape\n            # ranges, so we have to break them into pieces\n            # Original was: \\U0001D173-\\U0001D17A and \\U000E0020-\\U000E007F\n            string = re.sub('\\ud834[\\udd73-\\udd7a]|\\udb40[\\udc20-\\udc7f]|\\U000e0001', '', string)\n        else:\n            string = re.sub('[\\U0001D173-\\U0001D17A\\U000E0020-\\U000E007F\\U000e0001]', '', string)\n        string = re.sub(\n            '[\\u0000-\\u0008\\u000e-\\u001f\\u007f-\\u0084\\u0086-\\u009f\\u06dd\\u070f\\u180e\\u200c-\\u200f'\n            '\\u202a-\\u202e\\u2060-\\u2063\\u206a-\\u206f\\ufeff\\ufff9-\\ufffb]+',\n            '',\n            string\n        )\n        string = string.replace('\\u200b', '')\n        string = re.sub('[\\u00a0\\u1680\\u2000-\\u200a\\u2028-\\u2029\\u202f\\u205f\\u3000]', ' ', string)\n\n        string = ''.join(map(stringprep.map_table_b2, string))\n\n        # Normalize step\n        string = unicodedata.normalize('NFKC', string)\n\n        # Prohibit step\n        for char in string:\n            if stringprep.in_table_a1(char):\n                raise ValueError(unwrap(\n                    '''\n                    X.509 Name objects may not contain unassigned code points\n                    '''\n                ))\n\n            if stringprep.in_table_c8(char):\n                raise ValueError(unwrap(\n                    '''\n                    X.509 Name objects may not contain change display or\n                    zzzzdeprecated characters\n                    '''\n                ))\n\n            if stringprep.in_table_c3(char):\n                raise ValueError(unwrap(\n                    '''\n                    X.509 Name objects may not contain private use characters\n                    '''\n                ))\n\n            if stringprep.in_table_c4(char):\n                raise ValueError(unwrap(\n                    '''\n                    X.509 Name objects may not contain non-character code points\n                    '''\n                ))\n\n            if stringprep.in_table_c5(char):\n                raise ValueError(unwrap(\n                    '''\n                    X.509 Name objects may not contain surrogate code points\n                    '''\n                ))\n\n            if char == '\\ufffd':\n                raise ValueError(unwrap(\n                    '''\n                    X.509 Name objects may not contain the replacement character\n                    '''\n                ))\n\n        # Check bidirectional step - here we ensure that we are not mixing\n        # left-to-right and right-to-left text in the string\n        has_r_and_al_cat = False\n        has_l_cat = False\n        for char in string:\n            if stringprep.in_table_d1(char):\n                has_r_and_al_cat = True\n            elif stringprep.in_table_d2(char):\n                has_l_cat = True\n\n        if has_r_and_al_cat:\n            first_is_r_and_al = stringprep.in_table_d1(string[0])\n            last_is_r_and_al = stringprep.in_table_d1(string[-1])\n\n            if has_l_cat or not first_is_r_and_al or not last_is_r_and_al:\n                raise ValueError(unwrap(\n                    '''\n                    X.509 Name object contains a malformed bidirectional\n                    sequence\n                    '''\n                ))\n\n        # Insignificant space handling step\n        string = ' ' + re.sub(' +', '  ', string).strip() + ' '\n\n        return string"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a unicode string that can be used as a dict key or in a set", "response": "def hashable(self):\n        \"\"\"\n        :return:\n            A unicode string that can be used as a dict key or in a set\n        \"\"\"\n\n        output = []\n        values = self._get_values(self)\n        for key in sorted(values.keys()):\n            output.append('%s: %s' % (key, values[key]))\n        # Unit separator is used here since the normalization process for\n        # values moves any such character, and the keys are all dotted integers\n        # or under_score_words\n        return '\\x1F'.join(output)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dict of prepped values contained in an RDN object", "response": "def _get_values(self, rdn):\n        \"\"\"\n        Returns a dict of prepped values contained in an RDN\n\n        :param rdn:\n            A RelativeDistinguishedName object\n\n        :return:\n            A dict object with unicode strings of NameTypeAndValue value field\n            values that have been prepped for comparison\n        \"\"\"\n\n        output = {}\n        [output.update([(ntv['type'].native, ntv.prepped_value)]) for ntv in rdn]\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds a Name object from a dictionary of unicode string keys and values.", "response": "def build(cls, name_dict, use_printable=False):\n        \"\"\"\n        Creates a Name object from a dict of unicode string keys and values.\n        The keys should be from NameType._map, or a dotted-integer OID unicode\n        string.\n\n        :param name_dict:\n            A dict of name information, e.g. {\"common_name\": \"Will Bond\",\n            \"country_name\": \"US\", \"organization\": \"Codex Non Sufficit LC\"}\n\n        :param use_printable:\n            A bool - if PrintableString should be used for encoding instead of\n            UTF8String. This is for backwards compatibility with old software.\n\n        :return:\n            An x509.Name object\n        \"\"\"\n\n        rdns = []\n        if not use_printable:\n            encoding_name = 'utf8_string'\n            encoding_class = UTF8String\n        else:\n            encoding_name = 'printable_string'\n            encoding_class = PrintableString\n\n        # Sort the attributes according to NameType.preferred_order\n        name_dict = OrderedDict(\n            sorted(\n                name_dict.items(),\n                key=lambda item: NameType.preferred_ordinal(item[0])\n            )\n        )\n\n        for attribute_name, attribute_value in name_dict.items():\n            attribute_name = NameType.map(attribute_name)\n            if attribute_name == 'email_address':\n                value = EmailAddress(attribute_value)\n            elif attribute_name == 'domain_component':\n                value = DNSName(attribute_value)\n            elif attribute_name in set(['dn_qualifier', 'country_name', 'serial_number']):\n                value = DirectoryString(\n                    name='printable_string',\n                    value=PrintableString(attribute_value)\n                )\n            else:\n                value = DirectoryString(\n                    name=encoding_name,\n                    value=encoding_class(attribute_value)\n                )\n\n            rdns.append(RelativeDistinguishedName([\n                NameTypeAndValue({\n                    'type': attribute_name,\n                    'value': value\n                })\n            ]))\n\n        return cls(name='', value=RDNSequence(rdns))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef human_friendly(self):\n\n        if self._human_friendly is None:\n            data = OrderedDict()\n            last_field = None\n            for rdn in self.chosen:\n                for type_val in rdn:\n                    field_name = type_val['type'].human_friendly\n                    last_field = field_name\n                    if field_name in data:\n                        data[field_name] = [data[field_name]]\n                        data[field_name].append(type_val['value'])\n                    else:\n                        data[field_name] = type_val['value']\n            to_join = []\n            keys = data.keys()\n            if last_field == 'Country':\n                keys = reversed(list(keys))\n            for key in keys:\n                value = data[key]\n                native_value = self._recursive_humanize(value)\n                to_join.append('%s: %s' % (key, native_value))\n\n            has_comma = False\n            for element in to_join:\n                if element.find(',') != -1:\n                    has_comma = True\n                    break\n\n            separator = ', ' if not has_comma else '; '\n            self._human_friendly = separator.join(to_join[::-1])\n\n        return self._human_friendly", "response": "Returns a human - friendly unicode string containing the parts of the name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sha1(self):\n\n        if self._sha1 is None:\n            self._sha1 = hashlib.sha1(self.dump()).digest()\n        return self._sha1", "response": "Returns the SHA1 hash of the DER - encoded bytes of this name."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the SHA - 256 hash of the DER - encoded bytes of this name", "response": "def sha256(self):\n        \"\"\"\n        :return:\n            The SHA-256 hash of the DER-encoded bytes of this name\n        \"\"\"\n\n        if self._sha256 is None:\n            self._sha256 = hashlib.sha256(self.dump()).digest()\n        return self._sha256"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef url(self):\n\n        if self._url is False:\n            self._url = None\n            name = self['distribution_point']\n            if name.name != 'full_name':\n                raise ValueError(unwrap(\n                    '''\n                    CRL distribution points that are relative to the issuer are\n                    not supported\n                    '''\n                ))\n\n            for general_name in name.chosen:\n                if general_name.name == 'uniform_resource_identifier':\n                    url = general_name.native\n                    if url.lower().startswith(('http://', 'https://', 'ldap://', 'ldaps://')):\n                        self._url = url\n                        break\n\n        return self._url", "response": "Returns the URL of the CRL distribution point."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef issuer_serial(self):\n\n        if self._issuer_serial is None:\n            self._issuer_serial = self.issuer.sha256 + b':' + str_cls(self.serial_number).encode('ascii')\n        return self._issuer_serial", "response": "Returns the issuer serial number as a byte string"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef authority_issuer_serial(self):\n\n        if self._authority_issuer_serial is False:\n            akiv = self.authority_key_identifier_value\n            if akiv and akiv['authority_cert_issuer'].native:\n                issuer = self.authority_key_identifier_value['authority_cert_issuer'][0].chosen\n                # We untag the element since it is tagged via being a choice from GeneralName\n                issuer = issuer.untag()\n                authority_serial = self.authority_key_identifier_value['authority_cert_serial_number'].native\n                self._authority_issuer_serial = issuer.sha256 + b':' + str_cls(authority_serial).encode('ascii')\n            else:\n                self._authority_issuer_serial = None\n        return self._authority_issuer_serial", "response": "Returns the authority issuer serial number of the isser from the\n            authority key identifier extension concatenated with the ascii\n            character \":\" concatenated with the serial number from the\n            authority key identifier extension as an ascii string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef crl_distribution_points(self):\n\n        if self._crl_distribution_points is None:\n            self._crl_distribution_points = self._get_http_crl_distribution_points(self.crl_distribution_points_value)\n        return self._crl_distribution_points", "response": "Returns complete CRL URLs - does not include delta CRLs\n        Returns zero or more DistributionPoint objects"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delta_crl_distribution_points(self):\n\n        if self._delta_crl_distribution_points is None:\n            self._delta_crl_distribution_points = self._get_http_crl_distribution_points(self.freshest_crl_value)\n        return self._delta_crl_distribution_points", "response": "Returns a list of zero or more DistributionPoint objects that are not complete CRLs\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_http_crl_distribution_points(self, crl_distribution_points):\n\n        output = []\n\n        if crl_distribution_points is None:\n            return []\n\n        for distribution_point in crl_distribution_points:\n            distribution_point_name = distribution_point['distribution_point']\n            if distribution_point_name is VOID:\n                continue\n            # RFC 5280 indicates conforming CA should not use the relative form\n            if distribution_point_name.name == 'name_relative_to_crl_issuer':\n                continue\n            # This library is currently only concerned with HTTP-based CRLs\n            for general_name in distribution_point_name.chosen:\n                if general_name.name == 'uniform_resource_identifier':\n                    output.append(distribution_point)\n\n        return output", "response": "Returns a list of DistributionPoints objects for non - relative HTTP CRLs and CRLs that are referenced by the certificate."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ocsp_urls(self):\n\n        if not self.authority_information_access_value:\n            return []\n\n        output = []\n        for entry in self.authority_information_access_value:\n            if entry['access_method'].native == 'ocsp':\n                location = entry['access_location']\n                if location.name != 'uniform_resource_identifier':\n                    continue\n                url = location.native\n                if url.lower().startswith(('http://', 'https://', 'ldap://', 'ldaps://')):\n                    output.append(url)\n        return output", "response": "Returns a list of zero or more unicode strings of the OCSP URLs for this certificate."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of unicode strings of valid domain names for the certificate.", "response": "def valid_domains(self):\n        \"\"\"\n        :return:\n            A list of unicode strings of valid domain names for the certificate.\n            Wildcard certificates will have a domain in the form: *.example.com\n        \"\"\"\n\n        if self._valid_domains is None:\n            self._valid_domains = []\n\n            # For the subject alt name extension, we can look at the name of\n            # the choice selected since it distinguishes between domain names,\n            # email addresses, IPs, etc\n            if self.subject_alt_name_value:\n                for general_name in self.subject_alt_name_value:\n                    if general_name.name == 'dns_name' and general_name.native not in self._valid_domains:\n                        self._valid_domains.append(general_name.native)\n\n            # If there was no subject alt name extension, and the common name\n            # in the subject looks like a domain, that is considered the valid\n            # list. This is done because according to\n            # https://tools.ietf.org/html/rfc6125#section-6.4.4, the common\n            # name should not be used if the subject alt name is present.\n            else:\n                pattern = re.compile('^(\\\\*\\\\.)?(?:[a-zA-Z0-9](?:[a-zA-Z0-9\\\\-]*[a-zA-Z0-9])?\\\\.)+[a-zA-Z]{2,}$')\n                for rdn in self.subject.chosen:\n                    for name_type_value in rdn:\n                        if name_type_value['type'].native == 'common_name':\n                            value = name_type_value['value'].native\n                            if pattern.match(value):\n                                self._valid_domains.append(value)\n\n        return self._valid_domains"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef valid_ips(self):\n\n        if self._valid_ips is None:\n            self._valid_ips = []\n\n            if self.subject_alt_name_value:\n                for general_name in self.subject_alt_name_value:\n                    if general_name.name == 'ip_address':\n                        self._valid_ips.append(general_name.native)\n\n        return self._valid_ips", "response": "Returns a list of unicode strings of valid IP addresses for the certificate."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef self_issued(self):\n\n        if self._self_issued is None:\n            self._self_issued = self.subject == self.issuer\n        return self._self_issued", "response": "A property that returns True if the certificate is self - issued."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef self_signed(self):\n\n        if self._self_signed is None:\n            self._self_signed = 'no'\n            if self.self_issued:\n                if self.key_identifier:\n                    if not self.authority_key_identifier:\n                        self._self_signed = 'maybe'\n                    elif self.authority_key_identifier == self.key_identifier:\n                        self._self_signed = 'maybe'\n                else:\n                    self._self_signed = 'maybe'\n        return self._self_signed", "response": "Returns the self - signed certificate."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_valid_domain_ip(self, domain_ip):\n\n        if not isinstance(domain_ip, str_cls):\n            raise TypeError(unwrap(\n                '''\n                domain_ip must be a unicode string, not %s\n                ''',\n                type_name(domain_ip)\n            ))\n\n        encoded_domain_ip = domain_ip.encode('idna').decode('ascii').lower()\n\n        is_ipv6 = encoded_domain_ip.find(':') != -1\n        is_ipv4 = not is_ipv6 and re.match('^\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+$', encoded_domain_ip)\n        is_domain = not is_ipv6 and not is_ipv4\n\n        # Handle domain name checks\n        if is_domain:\n            if not self.valid_domains:\n                return False\n\n            domain_labels = encoded_domain_ip.split('.')\n\n            for valid_domain in self.valid_domains:\n                encoded_valid_domain = valid_domain.encode('idna').decode('ascii').lower()\n                valid_domain_labels = encoded_valid_domain.split('.')\n\n                # The domain must be equal in label length to match\n                if len(valid_domain_labels) != len(domain_labels):\n                    continue\n\n                if valid_domain_labels == domain_labels:\n                    return True\n\n                is_wildcard = self._is_wildcard_domain(encoded_valid_domain)\n                if is_wildcard and self._is_wildcard_match(domain_labels, valid_domain_labels):\n                    return True\n\n            return False\n\n        # Handle IP address checks\n        if not self.valid_ips:\n            return False\n\n        family = socket.AF_INET if is_ipv4 else socket.AF_INET6\n        normalized_ip = inet_pton(family, encoded_domain_ip)\n\n        for valid_ip in self.valid_ips:\n            valid_family = socket.AF_INET if valid_ip.find('.') != -1 else socket.AF_INET6\n            normalized_valid_ip = inet_pton(valid_family, valid_ip)\n\n            if normalized_valid_ip == normalized_ip:\n                return True\n\n        return False", "response": "Checks if a domain name or IP address is valid according to the certificate."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if a domain name is a valid wildcard according to the IDN.", "response": "def _is_wildcard_domain(self, domain):\n        \"\"\"\n        Checks if a domain is a valid wildcard according to\n        https://tools.ietf.org/html/rfc6125#section-6.4.3\n\n        :param domain:\n            A unicode string of the domain name, where any U-labels from an IDN\n            have been converted to A-labels\n\n        :return:\n            A boolean - if the domain is a valid wildcard domain\n        \"\"\"\n\n        # The * character must be present for a wildcard match, and if there is\n        # most than one, it is an invalid wildcard specification\n        if domain.count('*') != 1:\n            return False\n\n        labels = domain.lower().split('.')\n\n        if not labels:\n            return False\n\n        # Wildcards may only appear in the left-most label\n        if labels[0].find('*') == -1:\n            return False\n\n        # Wildcards may not be embedded in an A-label from an IDN\n        if labels[0][0:4] == 'xn--':\n            return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _is_wildcard_match(self, domain_labels, valid_domain_labels):\n\n        first_domain_label = domain_labels[0]\n        other_domain_labels = domain_labels[1:]\n\n        wildcard_label = valid_domain_labels[0]\n        other_valid_domain_labels = valid_domain_labels[1:]\n\n        # The wildcard is only allowed in the first label, so if\n        # The subsequent labels are not equal, there is no match\n        if other_domain_labels != other_valid_domain_labels:\n            return False\n\n        if wildcard_label == '*':\n            return True\n\n        wildcard_regex = re.compile('^' + wildcard_label.replace('*', '.*') + '$')\n        if wildcard_regex.match(first_domain_label):\n            return True\n\n        return False", "response": "Determines if the labels in a domain name match the labels in a wildcard domain name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef contains(self, point):\n\n        y2 = point.y * point.y\n        x3 = point.x * point.x * point.x\n        return (y2 - (x3 + self.a * point.x + self.b)) % self.p == 0", "response": "Returns True if the point is on this curve False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef double(self):\n\n        # X9.62 B.3:\n\n        p = self.curve.p\n        a = self.curve.a\n\n        l_ = ((3 * self.x * self.x + a) * inverse_mod(2 * self.y, p)) % p\n\n        x3 = (l_ * l_ - 2 * self.x) % p\n        y3 = (l_ * (self.x - x3) - self.y) % p\n\n        return PrimePoint(self.curve, x3, y3)", "response": "Return a PrimePoint object that is twice this point\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun flake8 lint :return: A bool - if flake8 did not find any errors", "response": "def run():\n    \"\"\"\n    Runs flake8 lint\n\n    :return:\n        A bool - if flake8 did not find any errors\n    \"\"\"\n\n    print('Running flake8 %s' % flake8.__version__)\n\n    flake8_style = get_style_guide(config_file=os.path.join(package_root, 'tox.ini'))\n\n    paths = []\n    for _dir in [package_name, 'dev', 'tests']:\n        for root, _, filenames in os.walk(_dir):\n            for filename in filenames:\n                if not filename.endswith('.py'):\n                    continue\n                paths.append(os.path.join(root, filename))\n    report = flake8_style.check_files(paths)\n    success = report.total_errors == 0\n    if success:\n        print('OK')\n    return success"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns the linter and tests", "response": "def run():\n    \"\"\"\n    Runs the linter and tests\n\n    :return:\n        A bool - if the linter and tests ran successfully\n    \"\"\"\n\n    print('Python ' + sys.version.replace('\\n', ''))\n\n    try:\n        oscrypto_tests_module_info = imp.find_module('tests', [os.path.join(build_root, 'oscrypto')])\n        oscrypto_tests = imp.load_module('oscrypto.tests', *oscrypto_tests_module_info)\n        oscrypto = oscrypto_tests.local_oscrypto()\n        print('\\noscrypto backend: %s' % oscrypto.backend())\n    except (ImportError):\n        pass\n\n    if run_lint:\n        print('')\n        lint_result = run_lint()\n    else:\n        lint_result = True\n\n    if run_coverage:\n        print('\\nRunning tests (via coverage.py)')\n        sys.stdout.flush()\n        tests_result = run_coverage(ci=True)\n    else:\n        print('\\nRunning tests')\n        sys.stdout.flush()\n        tests_result = run_tests()\n    sys.stdout.flush()\n\n    return lint_result and tests_result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef inet_ntop(address_family, packed_ip):\n\n    if address_family not in set([socket.AF_INET, socket.AF_INET6]):\n        raise ValueError(unwrap(\n            '''\n            address_family must be socket.AF_INET (%s) or socket.AF_INET6 (%s),\n            not %s\n            ''',\n            repr(socket.AF_INET),\n            repr(socket.AF_INET6),\n            repr(address_family)\n        ))\n\n    if not isinstance(packed_ip, byte_cls):\n        raise TypeError(unwrap(\n            '''\n            packed_ip must be a byte string, not %s\n            ''',\n            type_name(packed_ip)\n        ))\n\n    required_len = 4 if address_family == socket.AF_INET else 16\n    if len(packed_ip) != required_len:\n        raise ValueError(unwrap(\n            '''\n            packed_ip must be %d bytes long - is %d\n            ''',\n            required_len,\n            len(packed_ip)\n        ))\n\n    if address_family == socket.AF_INET:\n        return '%d.%d.%d.%d' % tuple(bytes_to_list(packed_ip))\n\n    octets = struct.unpack(b'!HHHHHHHH', packed_ip)\n\n    runs_of_zero = {}\n    longest_run = 0\n    zero_index = None\n    for i, octet in enumerate(octets + (-1,)):\n        if octet != 0:\n            if zero_index is not None:\n                length = i - zero_index\n                if length not in runs_of_zero:\n                    runs_of_zero[length] = zero_index\n                longest_run = max(longest_run, length)\n                zero_index = None\n        elif zero_index is None:\n            zero_index = i\n\n    hexed = [hex(o)[2:] for o in octets]\n\n    if longest_run < 2:\n        return ':'.join(hexed)\n\n    zero_start = runs_of_zero[longest_run]\n    zero_end = zero_start + longest_run\n\n    return ':'.join(hexed[:zero_start]) + '::' + ':'.join(hexed[zero_end:])", "response": "This function returns the network form of an IP address."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwindow compatibility shim for socket.inet_ntop(). :param address_family: socket.AF_INET for IPv4 or socket.AF_INET6 for IPv6 :param ip_string: A unicode string of an IP address :return: A byte string of the network form of the IP address", "response": "def inet_pton(address_family, ip_string):\n    \"\"\"\n    Windows compatibility shim for socket.inet_ntop().\n\n    :param address_family:\n        socket.AF_INET for IPv4 or socket.AF_INET6 for IPv6\n\n    :param ip_string:\n        A unicode string of an IP address\n\n    :return:\n        A byte string of the network form of the IP address\n    \"\"\"\n\n    if address_family not in set([socket.AF_INET, socket.AF_INET6]):\n        raise ValueError(unwrap(\n            '''\n            address_family must be socket.AF_INET (%s) or socket.AF_INET6 (%s),\n            not %s\n            ''',\n            repr(socket.AF_INET),\n            repr(socket.AF_INET6),\n            repr(address_family)\n        ))\n\n    if not isinstance(ip_string, str_cls):\n        raise TypeError(unwrap(\n            '''\n            ip_string must be a unicode string, not %s\n            ''',\n            type_name(ip_string)\n        ))\n\n    if address_family == socket.AF_INET:\n        octets = ip_string.split('.')\n        error = len(octets) != 4\n        if not error:\n            ints = []\n            for o in octets:\n                o = int(o)\n                if o > 255 or o < 0:\n                    error = True\n                    break\n                ints.append(o)\n\n        if error:\n            raise ValueError(unwrap(\n                '''\n                ip_string must be a dotted string with four integers in the\n                range of 0 to 255, got %s\n                ''',\n                repr(ip_string)\n            ))\n\n        return struct.pack(b'!BBBB', *ints)\n\n    error = False\n    omitted = ip_string.count('::')\n    if omitted > 1:\n        error = True\n    elif omitted == 0:\n        octets = ip_string.split(':')\n        error = len(octets) != 8\n    else:\n        begin, end = ip_string.split('::')\n        begin_octets = begin.split(':')\n        end_octets = end.split(':')\n        missing = 8 - len(begin_octets) - len(end_octets)\n        octets = begin_octets + (['0'] * missing) + end_octets\n\n    if not error:\n        ints = []\n        for o in octets:\n            o = int(o, 16)\n            if o > 65535 or o < 0:\n                error = True\n                break\n            ints.append(o)\n\n        return struct.pack(b'!HHHHHHHH', *ints)\n\n    raise ValueError(unwrap(\n        '''\n        ip_string must be a valid ipv6 string, got %s\n        ''',\n        repr(ip_string)\n    ))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _format(self, format):\n\n        format = format.replace('%Y', '0000')\n        # Year 0 is 1BC and a leap year. Leap years repeat themselves\n        # every 28 years. Because of adjustments and the proleptic gregorian\n        # calendar, the simplest way to format is to substitute year 2000.\n        temp = date(2000, self.month, self.day)\n        if '%c' in format:\n            c_out = temp.strftime('%c')\n            # Handle full years\n            c_out = c_out.replace('2000', '0000')\n            c_out = c_out.replace('%', '%%')\n            format = format.replace('%c', c_out)\n        if '%x' in format:\n            x_out = temp.strftime('%x')\n            # Handle formats such as 08/16/2000 or 16.08.2000\n            x_out = x_out.replace('2000', '0000')\n            x_out = x_out.replace('%', '%%')\n            format = format.replace('%x', x_out)\n        return temp.strftime(format)", "response": "Returns a unicode string of the formatted date in the specified format."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nformat the date using strftime", "response": "def strftime(self, format):\n        \"\"\"\n        Formats the date using strftime()\n\n        :param format:\n            The strftime() format string\n\n        :return:\n            The formatted date as a unicode string in Python 3 and a byte\n            string in Python 2\n        \"\"\"\n\n        output = self._format(format)\n        if py2:\n            return output.encode('utf-8')\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new datetime. date object with the specified components replaced.", "response": "def replace(self, year=None, month=None, day=None):\n        \"\"\"\n        Returns a new datetime.date or asn1crypto.util.extended_date\n        object with the specified components replaced\n\n        :return:\n            A datetime.date or asn1crypto.util.extended_date object\n        \"\"\"\n\n        if year is None:\n            year = self.year\n        if month is None:\n            month = self.month\n        if day is None:\n            day = self.day\n\n        if year > 0:\n            cls = date\n        else:\n            cls = extended_date\n\n        return cls(\n            year,\n            month,\n            day\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef utcoffset(self):\n\n        if self.tzinfo is None:\n            return None\n        return self.tzinfo.utcoffset(self.replace(year=2000))", "response": "Returns a datetime. timedelta object of the offset from UTC"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dst(self):\n\n        if self.tzinfo is None:\n            return None\n        return self.tzinfo.dst(self.replace(year=2000))", "response": "Returns a datetime. timedelta object of the daylight savings time offset of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the name of the timezone in the current locale.", "response": "def tzname(self):\n        \"\"\"\n        :return:\n            None or the name of the timezone as a unicode string in Python 3\n            and a byte string in Python 2\n        \"\"\"\n\n        if self.tzinfo is None:\n            return None\n        return self.tzinfo.tzname(self.replace(year=2000))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _format(self, format):\n\n        format = format.replace('%Y', '0000')\n        # Year 0 is 1BC and a leap year. Leap years repeat themselves\n        # every 28 years. Because of adjustments and the proleptic gregorian\n        # calendar, the simplest way to format is to substitute year 2000.\n        temp = datetime(\n            2000,\n            self.month,\n            self.day,\n            self.hour,\n            self.minute,\n            self.second,\n            self.microsecond,\n            self.tzinfo\n        )\n        if '%c' in format:\n            c_out = temp.strftime('%c')\n            # Handle full years\n            c_out = c_out.replace('2000', '0000')\n            c_out = c_out.replace('%', '%%')\n            format = format.replace('%c', c_out)\n        if '%x' in format:\n            x_out = temp.strftime('%x')\n            # Handle formats such as 08/16/2000 or 16.08.2000\n            x_out = x_out.replace('2000', '0000')\n            x_out = x_out.replace('%', '%%')\n            format = format.replace('%x', x_out)\n        return temp.strftime(format)", "response": "Returns a unicode string of the formatted datetime in the specified format."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef isoformat(self, sep='T'):\n\n        if self.microsecond == 0:\n            return self.strftime('0000-%%m-%%d%s%%H:%%M:%%S' % sep)\n        return self.strftime('0000-%%m-%%d%s%%H:%%M:%%S.%%f' % sep)", "response": "Formats the datetime as a unicode string in Python 3 and a byte\n            string in Python 2"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef replace(self, year=None, month=None, day=None, hour=None, minute=None,\n                second=None, microsecond=None, tzinfo=None):\n        \"\"\"\n        Returns a new datetime.datetime or asn1crypto.util.extended_datetime\n        object with the specified components replaced\n\n        :return:\n            A datetime.datetime or asn1crypto.util.extended_datetime object\n        \"\"\"\n\n        if year is None:\n            year = self.year\n        if month is None:\n            month = self.month\n        if day is None:\n            day = self.day\n        if hour is None:\n            hour = self.hour\n        if minute is None:\n            minute = self.minute\n        if second is None:\n            second = self.second\n        if microsecond is None:\n            microsecond = self.microsecond\n        if tzinfo is None:\n            tzinfo = self.tzinfo\n\n        if year > 0:\n            cls = datetime\n        else:\n            cls = extended_datetime\n\n        return cls(\n            year,\n            month,\n            day,\n            hour,\n            minute,\n            second,\n            microsecond,\n            tzinfo\n        )", "response": "Returns a new datetime. datetime object with the specified components replaced."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the issuer name of the certificate.", "response": "def issuer_name(self):\n        \"\"\"\n        :return:\n            None, or an asn1crypto.x509.Name object for the issuer of the cert\n        \"\"\"\n\n        if self._issuer_name is False:\n            self._issuer_name = None\n            if self.certificate_issuer_value:\n                for general_name in self.certificate_issuer_value:\n                    if general_name.name == 'directory_name':\n                        self._issuer_name = general_name.chosen\n                        break\n        return self._issuer_name"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef issuer_cert_urls(self):\n\n        if self._issuer_cert_urls is None:\n            self._issuer_cert_urls = []\n            if self.authority_information_access_value:\n                for entry in self.authority_information_access_value:\n                    if entry['access_method'].native == 'ca_issuers':\n                        location = entry['access_location']\n                        if location.name != 'uniform_resource_identifier':\n                            continue\n                        url = location.native\n                        if url.lower()[0:7] == 'http://':\n                            self._issuer_cert_urls.append(url)\n        return self._issuer_cert_urls", "response": "Returns a list of unicode strings that are URLs that should contain either a single X.509 certificate or a DER - encoded CMS\n            message containing multiple certificates."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of zero or more DistributionPoint objects that are not yet complete.", "response": "def delta_crl_distribution_points(self):\n        \"\"\"\n        Returns delta CRL URLs - only applies to complete CRLs\n\n        :return:\n            A list of zero or more DistributionPoint objects\n        \"\"\"\n\n        if self._delta_crl_distribution_points is None:\n            self._delta_crl_distribution_points = []\n\n            if self.freshest_crl_value is not None:\n                for distribution_point in self.freshest_crl_value:\n                    distribution_point_name = distribution_point['distribution_point']\n                    # RFC 5280 indicates conforming CA should not use the relative form\n                    if distribution_point_name.name == 'name_relative_to_crl_issuer':\n                        continue\n                    # This library is currently only concerned with HTTP-based CRLs\n                    for general_name in distribution_point_name.chosen:\n                        if general_name.name == 'uniform_resource_identifier':\n                            self._delta_crl_distribution_points.append(distribution_point)\n\n        return self._delta_crl_distribution_points"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _set_extensions(self):\n\n        self._critical_extensions = set()\n\n        for extension in self['single_extensions']:\n            name = extension['extn_id'].native\n            attribute_name = '_%s_value' % name\n            if hasattr(self, attribute_name):\n                setattr(self, attribute_name, extension['extn_value'].parsed)\n            if extension['critical'].native:\n                self._critical_extensions.add(name)\n\n        self._processed_extensions = True", "response": "Sets common named extensions to private attributes and creates a list of critical extensions\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprinting out basic information about an Asn1Value object.", "response": "def _basic_debug(prefix, self):\n    \"\"\"\n    Prints out basic information about an Asn1Value object. Extracted for reuse\n    among different classes that customize the debug information.\n\n    :param prefix:\n        A unicode string of spaces to prefix output line with\n\n    :param self:\n        The object to print the debugging information about\n    \"\"\"\n\n    print('%s%s Object #%s' % (prefix, type_name(self), id(self)))\n    if self._header:\n        print('%s  Header: 0x%s' % (prefix, binascii.hexlify(self._header or b'').decode('utf-8')))\n\n    has_header = self.method is not None and self.class_ is not None and self.tag is not None\n    if has_header:\n        method_name = METHOD_NUM_TO_NAME_MAP.get(self.method)\n        class_name = CLASS_NUM_TO_NAME_MAP.get(self.class_)\n\n    if self.explicit is not None:\n        for class_, tag in self.explicit:\n            print(\n                '%s    %s tag %s (explicitly tagged)' %\n                (\n                    prefix,\n                    CLASS_NUM_TO_NAME_MAP.get(class_),\n                    tag\n                )\n            )\n        if has_header:\n            print('%s      %s %s %s' % (prefix, method_name, class_name, self.tag))\n\n    elif self.implicit:\n        if has_header:\n            print('%s    %s %s tag %s (implicitly tagged)' % (prefix, method_name, class_name, self.tag))\n\n    elif has_header:\n        print('%s    %s %s tag %s' % (prefix, method_name, class_name, self.tag))\n\n    print('%s  Data: 0x%s' % (prefix, binascii.hexlify(self.contents or b'').decode('utf-8')))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _tag_type_to_explicit_implicit(params):\n\n    if 'tag_type' in params:\n        if params['tag_type'] == 'explicit':\n            params['explicit'] = (params.get('class', 2), params['tag'])\n        elif params['tag_type'] == 'implicit':\n            params['implicit'] = (params.get('class', 2), params['tag'])\n        del params['tag_type']\n        del params['tag']\n        if 'class' in params:\n            del params['class']", "response": "Converts old - style tag_type and tag params to explicit and implicit."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _fix_tagging(value, params):\n\n    _tag_type_to_explicit_implicit(params)\n\n    retag = False\n    if 'implicit' not in params:\n        if value.implicit is not False:\n            retag = True\n    else:\n        if isinstance(params['implicit'], tuple):\n            class_, tag = params['implicit']\n        else:\n            tag = params['implicit']\n            class_ = 'context'\n        if value.implicit is False:\n            retag = True\n        elif value.class_ != CLASS_NAME_TO_NUM_MAP[class_] or value.tag != tag:\n            retag = True\n\n    if params.get('explicit') != value.explicit:\n        retag = True\n\n    if retag:\n        return value.retag(params)\n    return value", "response": "Checks if a value is properly tagged based on the spec and re - untags as necessary."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds a 2 - element integer tuple used to identify fields by grabbing the class_ and tag from an Asn1Value class and the params dict being passed to it .", "response": "def _build_id_tuple(params, spec):\n    \"\"\"\n    Builds a 2-element tuple used to identify fields by grabbing the class_\n    and tag from an Asn1Value class and the params dict being passed to it\n\n    :param params:\n        A dict of params to pass to spec\n\n    :param spec:\n        An Asn1Value class\n\n    :return:\n        A 2-element integer tuple in the form (class_, tag)\n    \"\"\"\n\n    # Handle situations where the spec is not known at setup time\n    if spec is None:\n        return (None, None)\n\n    required_class = spec.class_\n    required_tag = spec.tag\n\n    _tag_type_to_explicit_implicit(params)\n\n    if 'explicit' in params:\n        if isinstance(params['explicit'], tuple):\n            required_class, required_tag = params['explicit']\n        else:\n            required_class = 2\n            required_tag = params['explicit']\n    elif 'implicit' in params:\n        if isinstance(params['implicit'], tuple):\n            required_class, required_tag = params['implicit']\n        else:\n            required_class = 2\n            required_tag = params['implicit']\n    if required_class is not None and not isinstance(required_class, int_types):\n        required_class = CLASS_NAME_TO_NUM_MAP[required_class]\n\n    required_class = params.get('class_', required_class)\n    required_tag = params.get('tag', required_tag)\n\n    return (required_class, required_tag)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds an Asn1Value object generically based on the parameters of the ASN. 1 object.", "response": "def _build(class_, method, tag, header, contents, trailer, spec=None, spec_params=None, nested_spec=None):\n    \"\"\"\n    Builds an Asn1Value object generically, or using a spec with optional params\n\n    :param class_:\n        An integer representing the ASN.1 class\n\n    :param method:\n        An integer representing the ASN.1 method\n\n    :param tag:\n        An integer representing the ASN.1 tag\n\n    :param header:\n        A byte string of the ASN.1 header (class, method, tag, length)\n\n    :param contents:\n        A byte string of the ASN.1 value\n\n    :param trailer:\n        A byte string of any ASN.1 trailer (only used by indefinite length encodings)\n\n    :param spec:\n        A class derived from Asn1Value that defines what class_ and tag the\n        value should have, and the semantics of the encoded value. The\n        return value will be of this type. If omitted, the encoded value\n        will be decoded using the standard universal tag based on the\n        encoded tag number.\n\n    :param spec_params:\n        A dict of params to pass to the spec object\n\n    :param nested_spec:\n        For certain Asn1Value classes (such as OctetString and BitString), the\n        contents can be further parsed and interpreted as another Asn1Value.\n        This parameter controls the spec for that sub-parsing.\n\n    :return:\n        An object of the type spec, or if not specified, a child of Asn1Value\n    \"\"\"\n\n    if spec_params is not None:\n        _tag_type_to_explicit_implicit(spec_params)\n\n    if header is None:\n        return VOID\n\n    header_set = False\n\n    # If an explicit specification was passed in, make sure it matches\n    if spec is not None:\n        # If there is explicit tagging and contents, we have to split\n        # the header and trailer off before we do the parsing\n        no_explicit = spec_params and 'no_explicit' in spec_params\n        if not no_explicit and (spec.explicit or (spec_params and 'explicit' in spec_params)):\n            if spec_params:\n                value = spec(**spec_params)\n            else:\n                value = spec()\n            original_explicit = value.explicit\n            explicit_info = reversed(original_explicit)\n            parsed_class = class_\n            parsed_method = method\n            parsed_tag = tag\n            to_parse = contents\n            explicit_header = header\n            explicit_trailer = trailer or b''\n            for expected_class, expected_tag in explicit_info:\n                if parsed_class != expected_class:\n                    raise ValueError(unwrap(\n                        '''\n                        Error parsing %s - explicitly-tagged class should have been\n                        %s, but %s was found\n                        ''',\n                        type_name(value),\n                        CLASS_NUM_TO_NAME_MAP.get(expected_class),\n                        CLASS_NUM_TO_NAME_MAP.get(parsed_class, parsed_class)\n                    ))\n                if parsed_method != 1:\n                    raise ValueError(unwrap(\n                        '''\n                        Error parsing %s - explicitly-tagged method should have\n                        been %s, but %s was found\n                        ''',\n                        type_name(value),\n                        METHOD_NUM_TO_NAME_MAP.get(1),\n                        METHOD_NUM_TO_NAME_MAP.get(parsed_method, parsed_method)\n                    ))\n                if parsed_tag != expected_tag:\n                    raise ValueError(unwrap(\n                        '''\n                        Error parsing %s - explicitly-tagged tag should have been\n                        %s, but %s was found\n                        ''',\n                        type_name(value),\n                        expected_tag,\n                        parsed_tag\n                    ))\n                info, _ = _parse(to_parse, len(to_parse))\n                parsed_class, parsed_method, parsed_tag, parsed_header, to_parse, parsed_trailer = info\n\n                if not isinstance(value, Choice):\n                    explicit_header += parsed_header\n                    explicit_trailer = parsed_trailer + explicit_trailer\n\n            value = _build(*info, spec=spec, spec_params={'no_explicit': True})\n            value._header = explicit_header\n            value._trailer = explicit_trailer\n            value.explicit = original_explicit\n            header_set = True\n        else:\n            if spec_params:\n                value = spec(contents=contents, **spec_params)\n            else:\n                value = spec(contents=contents)\n\n            if spec is Any:\n                pass\n\n            elif isinstance(value, Choice):\n                value.validate(class_, tag, contents)\n                try:\n                    # Force parsing the Choice now\n                    value.contents = header + value.contents\n                    header = b''\n                    value.parse()\n                except (ValueError, TypeError) as e:\n                    args = e.args[1:]\n                    e.args = (e.args[0] + '\\n    while parsing %s' % type_name(value),) + args\n                    raise e\n\n            else:\n                if class_ != value.class_:\n                    raise ValueError(unwrap(\n                        '''\n                        Error parsing %s - class should have been %s, but %s was\n                        found\n                        ''',\n                        type_name(value),\n                        CLASS_NUM_TO_NAME_MAP.get(value.class_),\n                        CLASS_NUM_TO_NAME_MAP.get(class_, class_)\n                    ))\n                if method != value.method:\n                    # Allow parsing a primitive method as constructed if the value\n                    # is indefinite length. This is to allow parsing BER.\n                    ber_indef = method == 1 and value.method == 0 and trailer == b'\\x00\\x00'\n                    if not ber_indef or not isinstance(value, Constructable):\n                        raise ValueError(unwrap(\n                            '''\n                            Error parsing %s - method should have been %s, but %s was found\n                            ''',\n                            type_name(value),\n                            METHOD_NUM_TO_NAME_MAP.get(value.method),\n                            METHOD_NUM_TO_NAME_MAP.get(method, method)\n                        ))\n                    else:\n                        value.method = method\n                        value._indefinite = True\n                if tag != value.tag and tag != value._bad_tag:\n                    raise ValueError(unwrap(\n                        '''\n                        Error parsing %s - tag should have been %s, but %s was found\n                        ''',\n                        type_name(value),\n                        value.tag,\n                        tag\n                    ))\n\n    # For explicitly tagged, un-speced parsings, we use a generic container\n    # since we will be parsing the contents and discarding the outer object\n    # anyway a little further on\n    elif spec_params and 'explicit' in spec_params:\n        original_value = Asn1Value(contents=contents, **spec_params)\n        original_explicit = original_value.explicit\n\n        to_parse = contents\n        explicit_header = header\n        explicit_trailer = trailer or b''\n        for expected_class, expected_tag in reversed(original_explicit):\n            info, _ = _parse(to_parse, len(to_parse))\n            _, _, _, parsed_header, to_parse, parsed_trailer = info\n            explicit_header += parsed_header\n            explicit_trailer = parsed_trailer + explicit_trailer\n        value = _build(*info, spec=spec, spec_params={'no_explicit': True})\n        value._header = header + value._header\n        value._trailer += trailer or b''\n        value.explicit = original_explicit\n        header_set = True\n\n    # If no spec was specified, allow anything and just process what\n    # is in the input data\n    else:\n        if tag not in _UNIVERSAL_SPECS:\n            raise ValueError(unwrap(\n                '''\n                Unknown element - %s class, %s method, tag %s\n                ''',\n                CLASS_NUM_TO_NAME_MAP.get(class_),\n                METHOD_NUM_TO_NAME_MAP.get(method),\n                tag\n            ))\n\n        spec = _UNIVERSAL_SPECS[tag]\n\n        value = spec(contents=contents, class_=class_)\n        ber_indef = method == 1 and value.method == 0 and trailer == b'\\x00\\x00'\n        if ber_indef and isinstance(value, Constructable):\n            value._indefinite = True\n        value.method = method\n\n    if not header_set:\n        value._header = header\n        value._trailer = trailer or b''\n\n    # Destroy any default value that our contents have overwritten\n    value._native = None\n\n    if nested_spec:\n        try:\n            value.parse(nested_spec)\n        except (ValueError, TypeError) as e:\n            args = e.args[1:]\n            e.args = (e.args[0] + '\\n    while parsing %s' % type_name(value),) + args\n            raise e\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a byte string generically or using a spec with optional params", "response": "def _parse_build(encoded_data, pointer=0, spec=None, spec_params=None, strict=False):\n    \"\"\"\n    Parses a byte string generically, or using a spec with optional params\n\n    :param encoded_data:\n        A byte string that contains BER-encoded data\n\n    :param pointer:\n        The index in the byte string to parse from\n\n    :param spec:\n        A class derived from Asn1Value that defines what class_ and tag the\n        value should have, and the semantics of the encoded value. The\n        return value will be of this type. If omitted, the encoded value\n        will be decoded using the standard universal tag based on the\n        encoded tag number.\n\n    :param spec_params:\n        A dict of params to pass to the spec object\n\n    :param strict:\n        A boolean indicating if trailing data should be forbidden - if so, a\n        ValueError will be raised when trailing data exists\n\n    :return:\n        A 2-element tuple:\n         - 0: An object of the type spec, or if not specified, a child of Asn1Value\n         - 1: An integer indicating how many bytes were consumed\n    \"\"\"\n\n    encoded_len = len(encoded_data)\n    info, new_pointer = _parse(encoded_data, encoded_len, pointer)\n    if strict and new_pointer != pointer + encoded_len:\n        extra_bytes = pointer + encoded_len - new_pointer\n        raise ValueError('Extra data - %d bytes of trailing data were provided' % extra_bytes)\n    return (_build(*info, spec=spec, spec_params=spec_params), new_pointer)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconstruct a new copy of the current object preserving any tagging", "response": "def _new_instance(self):\n        \"\"\"\n        Constructs a new copy of the current object, preserving any tagging\n\n        :return:\n            An Asn1Value object\n        \"\"\"\n\n        new_obj = self.__class__()\n        new_obj.class_ = self.class_\n        new_obj.tag = self.tag\n        new_obj.implicit = self.implicit\n        new_obj.explicit = self.explicit\n        return new_obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef retag(self, tagging, tag=None):\n\n        # This is required to preserve the old API\n        if not isinstance(tagging, dict):\n            tagging = {tagging: tag}\n        new_obj = self.__class__(explicit=tagging.get('explicit'), implicit=tagging.get('implicit'))\n        new_obj._copy(self, copy.deepcopy)\n        return new_obj", "response": "Returns a copy of the object with the specified tagging applied."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef untag(self):\n\n        new_obj = self.__class__()\n        new_obj._copy(self, copy.deepcopy)\n        return new_obj", "response": "Returns a copy of the object removing any special tagging from it\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _copy(self, other, copy_func):\n\n        if self.__class__ != other.__class__:\n            raise TypeError(unwrap(\n                '''\n                Can not copy values from %s object to %s object\n                ''',\n                type_name(other),\n                type_name(self)\n            ))\n\n        self.contents = other.contents\n        self._native = copy_func(other._native)", "response": "Copies the contents of another Asn1Value object to itself."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef debug(self, nest_level=1):\n\n        prefix = '  ' * nest_level\n\n        # This interacts with Any and moves the tag, implicit, explicit, _header,\n        # contents, _footer to the parsed value so duplicate data isn't present\n        has_parsed = hasattr(self, 'parsed')\n\n        _basic_debug(prefix, self)\n        if has_parsed:\n            self.parsed.debug(nest_level + 2)\n        elif hasattr(self, 'chosen'):\n            self.chosen.debug(nest_level + 2)\n        else:\n            if _PY2 and isinstance(self.native, byte_cls):\n                print('%s    Native: b%s' % (prefix, repr(self.native)))\n            else:\n                print('%s    Native: %s' % (prefix, self.native))", "response": "Prints the binary data and parsed data in a tree structure structure."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dump(self, force=False):\n\n        contents = self.contents\n\n        if self._header is None or force:\n            if isinstance(self, Constructable) and self._indefinite:\n                self.method = 0\n\n            header = _dump_header(self.class_, self.method, self.tag, self.contents)\n\n            if self.explicit is not None:\n                for class_, tag in self.explicit:\n                    header = _dump_header(class_, 1, tag, header + self.contents) + header\n\n            self._header = header\n            self._trailer = b''\n\n        return self._header + contents", "response": "Dumps the value of the object into a byte string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _setup(self):\n\n        cls = self.__class__\n        if cls._map is None or cls._reverse_map is not None:\n            return\n        cls._reverse_map = {}\n        for key, value in cls._map.items():\n            cls._reverse_map[value] = key", "response": "Generates _reverse_map from _map\n        Generates _reverse_map from _map\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting the current object into an object of a different class.", "response": "def cast(self, other_class):\n        \"\"\"\n        Converts the current object into an object of a different class. The\n        new class must use the ASN.1 encoding for the value.\n\n        :param other_class:\n            The class to instantiate the new object from\n\n        :return:\n            An instance of the type other_class\n        \"\"\"\n\n        if other_class.tag != self.__class__.tag:\n            raise TypeError(unwrap(\n                '''\n                Can not covert a value from %s object to %s object since they\n                use different tags: %d versus %d\n                ''',\n                type_name(other_class),\n                type_name(self),\n                other_class.tag,\n                self.__class__.tag\n            ))\n\n        new_obj = other_class()\n        new_obj.class_ = self.class_\n        new_obj.implicit = self.implicit\n        new_obj.explicit = self.explicit\n        new_obj._header = self._header\n        new_obj.contents = self.contents\n        new_obj._trailer = self._trailer\n        if isinstance(self, Constructable):\n            new_obj.method = self.method\n            new_obj._indefinite = self._indefinite\n        return new_obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the concatenation of the native values of the contained chunks.", "response": "def _merge_chunks(self):\n        \"\"\"\n        :return:\n            A concatenation of the native values of the contained chunks\n        \"\"\"\n\n        if not self._indefinite:\n            return self._as_chunk()\n\n        pointer = self._chunks_offset\n        contents_len = len(self.contents)\n        output = None\n\n        while pointer < contents_len:\n            # We pass the current class as the spec so content semantics are preserved\n            sub_value, pointer = _parse_build(self.contents, pointer, spec=self.__class__)\n            if output is None:\n                output = sub_value._merge_chunks()\n            else:\n                output += sub_value._merge_chunks()\n\n        if output is None:\n            return self._as_chunk()\n\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncopies the contents of another Constructable object to itself.", "response": "def _copy(self, other, copy_func):\n        \"\"\"\n        Copies the contents of another Constructable object to itself\n\n        :param object:\n            Another instance of the same class\n\n        :param copy_func:\n            An reference of copy.copy() or copy.deepcopy() to use when copying\n            lists, dicts and objects\n        \"\"\"\n\n        super(Constructable, self)._copy(other, copy_func)\n        self.method = other.method\n        self._indefinite = other._indefinite"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse(self, spec=None, spec_params=None):\n\n        if self._parsed is None or self._parsed[1:3] != (spec, spec_params):\n            try:\n                passed_params = spec_params or {}\n                _tag_type_to_explicit_implicit(passed_params)\n                if self.explicit is not None:\n                    if 'explicit' in passed_params:\n                        passed_params['explicit'] = self.explicit + passed_params['explicit']\n                    else:\n                        passed_params['explicit'] = self.explicit\n                contents = self._header + self.contents + self._trailer\n                parsed_value, _ = _parse_build(\n                    contents,\n                    spec=spec,\n                    spec_params=passed_params\n                )\n                self._parsed = (parsed_value, spec, spec_params)\n\n                # Once we've parsed the Any value, clear any attributes from this object\n                # since they are now duplicate\n                self.tag = None\n                self.explicit = None\n                self.implicit = False\n                self._header = b''\n                self.contents = contents\n                self._trailer = b''\n\n            except (ValueError, TypeError) as e:\n                args = e.args[1:]\n                e.args = (e.args[0] + '\\n    while parsing %s' % type_name(self),) + args\n                raise e\n        return self._parsed[0]", "response": "Parses the contents of a Asn1Value object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncopies the contents of another Any object to itself", "response": "def _copy(self, other, copy_func):\n        \"\"\"\n        Copies the contents of another Any object to itself\n\n        :param object:\n            Another instance of the same class\n\n        :param copy_func:\n            An reference of copy.copy() or copy.deepcopy() to use when copying\n            lists, dicts and objects\n        \"\"\"\n\n        super(Any, self)._copy(other, copy_func)\n        self._parsed = copy_func(other._parsed)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dump(self, force=False):\n\n        if self._parsed is None:\n            self.parse()\n\n        return self._parsed[0].dump(force=force)", "response": "Returns the value of the key as a byte string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload a BER or DER - encoded byte string using the current class as the spec .", "response": "def load(cls, encoded_data, strict=False, **kwargs):\n        \"\"\"\n        Loads a BER/DER-encoded byte string using the current class as the spec\n\n        :param encoded_data:\n            A byte string of BER or DER encoded data\n\n        :param strict:\n            A boolean indicating if trailing data should be forbidden - if so, a\n            ValueError will be raised when trailing data exists\n\n        :return:\n            A instance of the current class\n        \"\"\"\n\n        if not isinstance(encoded_data, byte_cls):\n            raise TypeError('encoded_data must be a byte string, not %s' % type_name(encoded_data))\n\n        value, _ = _parse_build(encoded_data, spec=cls, spec_params=kwargs, strict=strict)\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _setup(self):\n\n        cls = self.__class__\n        cls._id_map = {}\n        cls._name_map = {}\n        for index, info in enumerate(cls._alternatives):\n            if len(info) < 3:\n                info = info + ({},)\n                cls._alternatives[index] = info\n            id_ = _build_id_tuple(info[2], info[1])\n            cls._id_map[id_] = index\n            cls._name_map[info[0]] = index", "response": "Generates _id_map from _alternatives to allow validating contents\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef name(self):\n        if not self._name:\n            self._name = self._alternatives[self._choice][0]\n        return self._name", "response": "Returns the unicode string of the field name of the chosen alternative."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the detected alternative term and returns the parsed version of the alternative term.", "response": "def parse(self):\n        \"\"\"\n        Parses the detected alternative\n\n        :return:\n            An Asn1Value object of the chosen alternative\n        \"\"\"\n\n        if self._parsed is not None:\n            return self._parsed\n\n        try:\n            _, spec, params = self._alternatives[self._choice]\n            self._parsed, _ = _parse_build(self._contents, spec=spec, spec_params=params)\n        except (ValueError, TypeError) as e:\n            args = e.args[1:]\n            e.args = (e.args[0] + '\\n    while parsing %s' % type_name(self),) + args\n            raise e"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvalidate that the class and tag specified exist as an alternative.", "response": "def validate(self, class_, tag, contents):\n        \"\"\"\n        Ensures that the class and tag specified exist as an alternative\n\n        :param class_:\n            The integer class_ from the encoded value header\n\n        :param tag:\n            The integer tag from the encoded value header\n\n        :param contents:\n            A byte string of the contents of the value - used when the object\n            is explicitly tagged\n\n        :raises:\n            ValueError - when value is not a valid alternative\n        \"\"\"\n\n        id_ = (class_, tag)\n\n        if self.explicit is not None:\n            if self.explicit[-1] != id_:\n                raise ValueError(unwrap(\n                    '''\n                    %s was explicitly tagged, but the value provided does not\n                    match the class and tag\n                    ''',\n                    type_name(self)\n                ))\n\n            ((class_, _, tag, _, _, _), _) = _parse(contents, len(contents))\n            id_ = (class_, tag)\n\n        if id_ in self._id_map:\n            self._choice = self._id_map[id_]\n            return\n\n        # This means the Choice was implicitly tagged\n        if self.class_ is not None and self.tag is not None:\n            if len(self._alternatives) > 1:\n                raise ValueError(unwrap(\n                    '''\n                    %s was implicitly tagged, but more than one alternative\n                    exists\n                    ''',\n                    type_name(self)\n                ))\n            if id_ == (self.class_, self.tag):\n                self._choice = 0\n                return\n\n        asn1 = self._format_class_tag(class_, tag)\n        asn1s = [self._format_class_tag(pair[0], pair[1]) for pair in self._id_map]\n\n        raise ValueError(unwrap(\n            '''\n            Value %s did not match the class and tag of any of the alternatives\n            in %s: %s\n            ''',\n            asn1,\n            type_name(self),\n            ', '.join(asn1s)\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _copy(self, other, copy_func):\n\n        super(Choice, self)._copy(other, copy_func)\n        self._choice = other._choice\n        self._name = other._name\n        self._parsed = copy_func(other._parsed)", "response": "Copies the contents of another Choice object to itself"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dump(self, force=False):\n\n        self._contents = self.chosen.dump(force=force)\n        if self._header is None or force:\n            self._header = b''\n            if self.explicit is not None:\n                for class_, tag in self.explicit:\n                    self._header = _dump_header(class_, 1, tag, self._header + self._contents) + self._header\n        return self._header + self._contents", "response": "Dumps the value of the object into a byte string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load(cls, encoded_data, strict=False):\n\n        return cls(contents=encoded_data, strict=strict)", "response": "Loads a BER or DER - encoded byte string using the current class as the spec\n            class."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncopies the contents of another Concat object to itself.", "response": "def _copy(self, other, copy_func):\n        \"\"\"\n        Copies the contents of another Concat object to itself\n\n        :param object:\n            Another instance of the same class\n\n        :param copy_func:\n            An reference of copy.copy() or copy.deepcopy() to use when copying\n            lists, dicts and objects\n        \"\"\"\n\n        if self.__class__ != other.__class__:\n            raise TypeError(unwrap(\n                '''\n                Can not copy values from %s object to %s object\n                ''',\n                type_name(other),\n                type_name(self)\n            ))\n\n        self._children = copy_func(other._children)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef debug(self, nest_level=1):\n\n        prefix = '  ' * nest_level\n        print('%s%s Object #%s' % (prefix, type_name(self), id(self)))\n        print('%s  Children:' % (prefix,))\n        for child in self._children:\n            child.debug(nest_level + 2)", "response": "Show the binary data and parsed data in a tree structure"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dump(self, force=False):\n\n        contents = b''\n        for child in self._children:\n            contents += child.dump(force=force)\n        return contents", "response": "Serializes the value using DER - encoded values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set(self, value):\n\n        if not isinstance(value, byte_cls):\n            raise TypeError(unwrap(\n                '''\n                %s value must be a byte string, not %s\n                ''',\n                type_name(self),\n                type_name(value)\n            ))\n\n        self._native = value\n        self.contents = value\n        self._header = None\n        if self._trailer != b'':\n            self._trailer = b''", "response": "Sets the value of the object\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dump(self, force=False):\n\n        if force:\n            native = self.native\n            self.contents = None\n            self.set(native)\n\n        return Asn1Value.dump(self)", "response": "Dumps the value using DER"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set(self, value):\n\n        if not isinstance(value, str_cls):\n            raise TypeError(unwrap(\n                '''\n                %s value must be a unicode string, not %s\n                ''',\n                type_name(self),\n                type_name(value)\n            ))\n\n        self._unicode = value\n        self.contents = value.encode(self._encoding)\n        self._header = None\n        if self._indefinite:\n            self._indefinite = False\n            self.method = 0\n        if self._trailer != b'':\n            self._trailer = b''", "response": "Sets the value of the string\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _copy(self, other, copy_func):\n\n        super(AbstractString, self)._copy(other, copy_func)\n        self._unicode = other._unicode", "response": "Copies the contents of another AbstractString object to itself"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set(self, value):\n\n        self._native = bool(value)\n        self.contents = b'\\x00' if not value else b'\\xff'\n        self._header = None\n        if self._trailer != b'':\n            self._trailer = b''", "response": "Sets the value of the object\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef native(self):\n\n        if self.contents is None:\n            return None\n\n        if self._native is None:\n            self._native = self.__bool__()\n        return self._native", "response": "Returns the native Python datatype representation of this value."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set(self, value):\n\n        if isinstance(value, str_cls):\n            if self._map is None:\n                raise ValueError(unwrap(\n                    '''\n                    %s value is a unicode string, but no _map provided\n                    ''',\n                    type_name(self)\n                ))\n\n            if value not in self._reverse_map:\n                raise ValueError(unwrap(\n                    '''\n                    %s value, %s, is not present in the _map\n                    ''',\n                    type_name(self),\n                    value\n                ))\n\n            value = self._reverse_map[value]\n\n        elif not isinstance(value, int_types):\n            raise TypeError(unwrap(\n                '''\n                %s value must be an integer or unicode string when a name_map\n                is provided, not %s\n                ''',\n                type_name(self),\n                type_name(value)\n            ))\n\n        self._native = self._map[value] if self._map and value in self._map else value\n\n        self.contents = int_to_bytes(value, signed=True)\n        self._header = None\n        if self._trailer != b'':\n            self._trailer = b''", "response": "Sets the value of the object to the specified value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the native Python datatype representation of this value.", "response": "def native(self):\n        \"\"\"\n        The native Python datatype representation of this value\n\n        :return:\n            An integer or None\n        \"\"\"\n\n        if self.contents is None:\n            return None\n\n        if self._native is None:\n            self._native = self.__int__()\n            if self._map is not None and self._native in self._map:\n                self._native = self._map[self._native]\n        return self._native"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _setup(self):\n\n        ValueMap._setup(self)\n\n        cls = self.__class__\n        if cls._map is not None:\n            cls._size = max(self._map.keys()) + 1", "response": "Generates _reverse_map from _map\n        Generates _reverse_map from _map\n        Generates _size from _map\n        Generates _reverse_map from _map\n        Generates _reverse_map from _map\n        Generates _reverse_map from _map\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the value of the object.", "response": "def set(self, value):\n        \"\"\"\n        Sets the value of the object\n\n        :param value:\n            An integer or a tuple of integers 0 and 1\n\n        :raises:\n            ValueError - when an invalid value is passed\n        \"\"\"\n\n        if isinstance(value, set):\n            if self._map is None:\n                raise ValueError(unwrap(\n                    '''\n                    %s._map has not been defined\n                    ''',\n                    type_name(self)\n                ))\n\n            bits = [0] * self._size\n            self._native = value\n            for index in range(0, self._size):\n                key = self._map.get(index)\n                if key is None:\n                    continue\n                if key in value:\n                    bits[index] = 1\n\n            value = ''.join(map(str_cls, bits))\n\n        elif value.__class__ == tuple:\n            if self._map is None:\n                self._native = value\n            else:\n                self._native = set()\n                for index, bit in enumerate(value):\n                    if bit:\n                        name = self._map.get(index, index)\n                        self._native.add(name)\n            value = ''.join(map(str_cls, value))\n\n        else:\n            raise TypeError(unwrap(\n                '''\n                %s value must be a tuple of ones and zeros or a set of unicode\n                strings, not %s\n                ''',\n                type_name(self),\n                type_name(value)\n            ))\n\n        self._chunk = None\n\n        if self._map is not None:\n            if len(value) > self._size:\n                raise ValueError(unwrap(\n                    '''\n                    %s value must be at most %s bits long, specified was %s long\n                    ''',\n                    type_name(self),\n                    self._size,\n                    len(value)\n                ))\n            # A NamedBitList must have trailing zero bit truncated. See\n            # https://www.itu.int/ITU-T/studygroups/com17/languages/X.690-0207.pdf\n            # section 11.2,\n            # https://tools.ietf.org/html/rfc5280#page-134 and\n            # https://www.ietf.org/mail-archive/web/pkix/current/msg10443.html\n            value = value.rstrip('0')\n        size = len(value)\n\n        size_mod = size % 8\n        extra_bits = 0\n        if size_mod != 0:\n            extra_bits = 8 - size_mod\n            value += '0' * extra_bits\n\n        size_in_bytes = int(math.ceil(size / 8))\n\n        if extra_bits:\n            extra_bits_byte = int_to_bytes(extra_bits)\n        else:\n            extra_bits_byte = b'\\x00'\n\n        if value == '':\n            value_bytes = b''\n        else:\n            value_bytes = int_to_bytes(int(value, 2))\n        if len(value_bytes) != size_in_bytes:\n            value_bytes = (b'\\x00' * (size_in_bytes - len(value_bytes))) + value_bytes\n\n        self.contents = extra_bits_byte + value_bytes\n        self._header = None\n        if self._indefinite:\n            self._indefinite = False\n            self.method = 0\n        if self._trailer != b'':\n            self._trailer = b''"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _as_chunk(self):\n\n        extra_bits = int_from_bytes(self.contents[0:1])\n        byte_len = len(self.contents[1:])\n        bit_string = '' if byte_len == 0 else '{0:b}'.format(int_from_bytes(self.contents[1:]))\n        bit_len = len(bit_string)\n\n        # Left-pad the bit string to a byte multiple to ensure we didn't\n        # lose any zero bits on the left\n        mod_bit_len = bit_len % 8\n        if mod_bit_len != 0:\n            bit_string = ('0' * (8 - mod_bit_len)) + bit_string\n            bit_len = len(bit_string)\n\n        if bit_len // 8 < byte_len:\n            missing_bytes = byte_len - (bit_len // 8)\n            bit_string = ('0' * (8 * missing_bytes)) + bit_string\n\n        # Trim off the extra bits on the right used to fill the last byte\n        if extra_bits > 0:\n            bit_string = bit_string[0:0 - extra_bits]\n\n        return tuple(map(int, tuple(bit_string)))", "response": "Returns a tuple of integers representing the bit string of the entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the native Python datatype representation of this value.", "response": "def native(self):\n        \"\"\"\n        The native Python datatype representation of this value\n\n        :return:\n            If a _map is set, a set of names, or if no _map is set, a tuple of\n            integers 1 and 0. None if no value.\n        \"\"\"\n\n        # For BitString we default the value to be all zeros\n        if self.contents is None:\n            if self._map is None:\n                self.set(())\n            else:\n                self.set(set())\n\n        if self._native is None:\n            bits = self._merge_chunks()\n            if self._map:\n                self._native = set()\n                for index, bit in enumerate(bits):\n                    if bit:\n                        name = self._map.get(index, index)\n                        self._native.add(name)\n            else:\n                self._native = bits\n        return self._native"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncopying the contents of another OctetBitString object to itself.", "response": "def _copy(self, other, copy_func):\n        \"\"\"\n        Copies the contents of another OctetBitString object to itself\n\n        :param object:\n            Another instance of the same class\n\n        :param copy_func:\n            An reference of copy.copy() or copy.deepcopy() to use when copying\n            lists, dicts and objects\n        \"\"\"\n\n        super(OctetBitString, self)._copy(other, copy_func)\n        self._bytes = other._bytes"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the contents of the entry as a string of bits - 1s and 0s.", "response": "def _as_chunk(self):\n        \"\"\"\n        Allows reconstructing indefinite length values\n\n        :return:\n            A unicode string of bits - 1s and 0s\n        \"\"\"\n\n        extra_bits = int_from_bytes(self.contents[0:1])\n        bit_string = '{0:b}'.format(int_from_bytes(self.contents[1:]))\n\n        # Ensure we have leading zeros since these chunks may be concatenated together\n        mod_bit_len = len(bit_string) % 8\n        if mod_bit_len != 0:\n            bit_string = ('0' * (8 - mod_bit_len)) + bit_string\n\n        if extra_bits > 0:\n            return bit_string[0:0 - extra_bits]\n\n        return bit_string"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef native(self):\n\n        if self.contents is None:\n            return None\n\n        if self._native is None:\n            extra_bits = int_from_bytes(self.contents[0:1])\n            # Fast path\n            if not self._indefinite and extra_bits == 0:\n                self._native = int_from_bytes(self.contents[1:])\n            else:\n                if self._indefinite and extra_bits > 0:\n                    raise ValueError('Constructed bit string has extra bits on indefinite container')\n                self._native = int(self._merge_chunks(), 2)\n        return self._native", "response": "Returns the native Python datatype representation of this value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set(self, value):\n\n        if not isinstance(value, byte_cls):\n            raise TypeError(unwrap(\n                '''\n                %s value must be a byte string, not %s\n                ''',\n                type_name(self),\n                type_name(value)\n            ))\n\n        self._bytes = value\n        self.contents = value\n        self._header = None\n        if self._indefinite:\n            self._indefinite = False\n            self.method = 0\n        if self._trailer != b'':\n            self._trailer = b''", "response": "Sets the value of the object\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncopying the contents of another OctetString object to itself", "response": "def _copy(self, other, copy_func):\n        \"\"\"\n        Copies the contents of another OctetString object to itself\n\n        :param object:\n            Another instance of the same class\n\n        :param copy_func:\n            An reference of copy.copy() or copy.deepcopy() to use when copying\n            lists, dicts and objects\n        \"\"\"\n\n        super(OctetString, self)._copy(other, copy_func)\n        self._bytes = other._bytes"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set(self, value):\n\n        if not isinstance(value, int_types):\n            raise TypeError(unwrap(\n                '''\n                %s value must be an integer, not %s\n                ''',\n                type_name(self),\n                type_name(value)\n            ))\n\n        self._native = value\n        self.contents = int_to_bytes(value, signed=False)\n        self._header = None\n        if self._indefinite:\n            self._indefinite = False\n            self.method = 0\n        if self._trailer != b'':\n            self._trailer = b''", "response": "Sets the value of the object\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef native(self):\n\n        if self.contents is None:\n            return None\n\n        if self._native is None:\n            self._native = int_from_bytes(self._merge_chunks())\n        return self._native", "response": "Returns the native Python datatype representation of this value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the contents generically or using a spec with optional params", "response": "def parse(self, spec=None, spec_params=None):\n        \"\"\"\n        Parses the contents generically, or using a spec with optional params\n\n        :param spec:\n            A class derived from Asn1Value that defines what class_ and tag the\n            value should have, and the semantics of the encoded value. The\n            return value will be of this type. If omitted, the encoded value\n            will be decoded using the standard universal tag based on the\n            encoded tag number.\n\n        :param spec_params:\n            A dict of params to pass to the spec object\n\n        :return:\n            An object of the type spec, or if not present, a child of Asn1Value\n        \"\"\"\n\n        if self._parsed is None or self._parsed[1:3] != (spec, spec_params):\n            parsed_value, _ = _parse_build(self.__bytes__(), spec=spec, spec_params=spec_params)\n            self._parsed = (parsed_value, spec, spec_params)\n        return self._parsed[0]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncopying the contents of another ParsableOctetString object to itself.", "response": "def _copy(self, other, copy_func):\n        \"\"\"\n        Copies the contents of another ParsableOctetString object to itself\n\n        :param object:\n            Another instance of the same class\n\n        :param copy_func:\n            An reference of copy.copy() or copy.deepcopy() to use when copying\n            lists, dicts and objects\n        \"\"\"\n\n        super(ParsableOctetString, self)._copy(other, copy_func)\n        self._bytes = other._bytes\n        self._parsed = copy_func(other._parsed)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the native Python datatype representation of this value.", "response": "def native(self):\n        \"\"\"\n        The native Python datatype representation of this value\n\n        :return:\n            A byte string or None\n        \"\"\"\n\n        if self.contents is None:\n            return None\n\n        if self._parsed is not None:\n            return self._parsed[0].native\n        else:\n            return self.__bytes__()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndump the value using DER - encoded DER format.", "response": "def dump(self, force=False):\n        \"\"\"\n        Encodes the value using DER\n\n        :param force:\n            If the encoded contents already exist, clear them and regenerate\n            to ensure they are in DER format instead of BER format\n\n        :return:\n            A byte string of the DER-encoded value\n        \"\"\"\n\n        if force:\n            if self._parsed is not None:\n                native = self.parsed.dump(force=force)\n            else:\n                native = self.native\n            self.contents = None\n            self.set(native)\n\n        return Asn1Value.dump(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef map(cls, value):\n\n        if cls._map is None:\n            raise ValueError(unwrap(\n                '''\n                %s._map has not been defined\n                ''',\n                type_name(cls)\n            ))\n\n        if not isinstance(value, str_cls):\n            raise TypeError(unwrap(\n                '''\n                value must be a unicode string, not %s\n                ''',\n                type_name(value)\n            ))\n\n        return cls._map.get(value, value)", "response": "Converts a dotted unicode string OID into a mapped unicode string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unmap(cls, value):\n\n        if cls not in _SETUP_CLASSES:\n            cls()._setup()\n            _SETUP_CLASSES[cls] = True\n\n        if cls._map is None:\n            raise ValueError(unwrap(\n                '''\n                %s._map has not been defined\n                ''',\n                type_name(cls)\n            ))\n\n        if not isinstance(value, str_cls):\n            raise TypeError(unwrap(\n                '''\n                value must be a unicode string, not %s\n                ''',\n                type_name(value)\n            ))\n\n        if value in cls._reverse_map:\n            return cls._reverse_map[value]\n\n        if not _OID_RE.match(value):\n            raise ValueError(unwrap(\n                '''\n                %s._map does not contain an entry for \"%s\"\n                ''',\n                type_name(cls),\n                value\n            ))\n\n        return value", "response": "Converts a unicode string value into a dotted unicode string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the value of the object object.", "response": "def set(self, value):\n        \"\"\"\n        Sets the value of the object\n\n        :param value:\n            A unicode string. May be a dotted integer string, or if _map is\n            provided, one of the mapped values.\n\n        :raises:\n            ValueError - when an invalid value is passed\n        \"\"\"\n\n        if not isinstance(value, str_cls):\n            raise TypeError(unwrap(\n                '''\n                %s value must be a unicode string, not %s\n                ''',\n                type_name(self),\n                type_name(value)\n            ))\n\n        self._native = value\n\n        if self._map is not None:\n            if value in self._reverse_map:\n                value = self._reverse_map[value]\n\n        self.contents = b''\n        first = None\n        for index, part in enumerate(value.split('.')):\n            part = int(part)\n\n            # The first two parts are merged into a single byte\n            if index == 0:\n                first = part\n                continue\n            elif index == 1:\n                part = (first * 40) + part\n\n            encoded_part = chr_cls(0x7F & part)\n            part = part >> 7\n            while part > 0:\n                encoded_part = chr_cls(0x80 | (0x7F & part)) + encoded_part\n                part = part >> 7\n            self.contents += encoded_part\n\n        self._header = None\n        if self._trailer != b'':\n            self._trailer = b''"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dotted(self):\n\n        if self._dotted is None:\n            output = []\n\n            part = 0\n            for byte in self.contents:\n                if _PY2:\n                    byte = ord(byte)\n                part = part * 128\n                part += byte & 127\n                # Last byte in subidentifier has the eighth bit set to 0\n                if byte & 0x80 == 0:\n                    if len(output) == 0:\n                        output.append(str_cls(part // 40))\n                        output.append(str_cls(part % 40))\n                    else:\n                        output.append(str_cls(part))\n                    part = 0\n\n            self._dotted = '.'.join(output)\n        return self._dotted", "response": "Returns the object identifier in dotted notation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef native(self):\n\n        if self.contents is None:\n            return None\n\n        if self._native is None:\n            self._native = self.dotted\n        if self._map is not None and self._native in self._map:\n            self._native = self._map[self._native]\n        return self._native", "response": "Returns the native Python datatype representation of this value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set(self, value):\n\n        if not isinstance(value, int_types) and not isinstance(value, str_cls):\n            raise TypeError(unwrap(\n                '''\n                %s value must be an integer or a unicode string, not %s\n                ''',\n                type_name(self),\n                type_name(value)\n            ))\n\n        if isinstance(value, str_cls):\n            if value not in self._reverse_map:\n                raise ValueError(unwrap(\n                    '''\n                    %s value \"%s\" is not a valid value\n                    ''',\n                    type_name(self),\n                    value\n                ))\n\n            value = self._reverse_map[value]\n\n        elif value not in self._map:\n            raise ValueError(unwrap(\n                '''\n                %s value %s is not a valid value\n                ''',\n                type_name(self),\n                value\n            ))\n\n        Integer.set(self, value)", "response": "Sets the value of the object\n           "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef native(self):\n\n        if self.contents is None:\n            return None\n\n        if self._native is None:\n            self._native = self._map[self.__int__()]\n        return self._native", "response": "Returns the native Python datatype representation of this value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the DER - encoded contents of the sequence as a byte string.", "response": "def contents(self):\n        \"\"\"\n        :return:\n            A byte string of the DER-encoded contents of the sequence\n        \"\"\"\n\n        if self.children is None:\n            return self._contents\n\n        if self._is_mutated():\n            self._set_contents()\n\n        return self._contents"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _is_mutated(self):\n\n        mutated = self._mutated\n        if self.children is not None:\n            for child in self.children:\n                if isinstance(child, Sequence) or isinstance(child, SequenceOf):\n                    mutated = mutated or child._is_mutated()\n\n        return mutated", "response": "A boolean - if the sequence or any children have been mutated"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _lazy_child(self, index):\n\n        child = self.children[index]\n        if child.__class__ == tuple:\n            child = self.children[index] = _build(*child)\n        return child", "response": "Builds a child object if the child has only been parsed into a tuple so far\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _set_contents(self, force=False):\n\n        if self.children is None:\n            self._parse_children()\n\n        contents = BytesIO()\n        for index, info in enumerate(self._fields):\n            child = self.children[index]\n            if child is None:\n                child_dump = b''\n            elif child.__class__ == tuple:\n                if force:\n                    child_dump = self._lazy_child(index).dump(force=force)\n                else:\n                    child_dump = child[3] + child[4] + child[5]\n            else:\n                child_dump = child.dump(force=force)\n            # Skip values that are the same as the default\n            if info[2] and 'default' in info[2]:\n                default_value = info[1](**info[2])\n                if default_value.dump() == child_dump:\n                    continue\n            contents.write(child_dump)\n        self._contents = contents.getvalue()\n\n        self._header = None\n        if self._trailer != b'':\n            self._trailer = b''", "response": "Updates the. contents attribute of the value with the encoded value of\n            all of the child objects that are not None."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating _field_map _field_ids and _oid_nums for use in parsing", "response": "def _setup(self):\n        \"\"\"\n        Generates _field_map, _field_ids and _oid_nums for use in parsing\n        \"\"\"\n\n        cls = self.__class__\n        cls._field_map = {}\n        cls._field_ids = []\n        cls._precomputed_specs = []\n        for index, field in enumerate(cls._fields):\n            if len(field) < 3:\n                field = field + ({},)\n                cls._fields[index] = field\n            cls._field_map[field[0]] = index\n            cls._field_ids.append(_build_id_tuple(field[2], field[1]))\n\n        if cls._oid_pair is not None:\n            cls._oid_nums = (cls._field_map[cls._oid_pair[0]], cls._field_map[cls._oid_pair[1]])\n\n        for index, field in enumerate(cls._fields):\n            has_callback = cls._spec_callbacks is not None and field[0] in cls._spec_callbacks\n            is_mapped_oid = cls._oid_nums is not None and cls._oid_nums[1] == index\n            if has_callback or is_mapped_oid:\n                cls._precomputed_specs.append(None)\n            else:\n                cls._precomputed_specs.append((field[0], field[1], field[1], field[2], None))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndetermine what value for a specific entry in the internal state of the object.", "response": "def _determine_spec(self, index):\n        \"\"\"\n        Determine how a value for a field should be constructed\n\n        :param index:\n            The field number\n\n        :return:\n            A tuple containing the following elements:\n             - unicode string of the field name\n             - Asn1Value class of the field spec\n             - Asn1Value class of the value spec\n             - None or dict of params to pass to the field spec\n             - None or Asn1Value class indicating the value spec was derived from an OID or a spec callback\n        \"\"\"\n\n        name, field_spec, field_params = self._fields[index]\n        value_spec = field_spec\n        spec_override = None\n\n        if self._spec_callbacks is not None and name in self._spec_callbacks:\n            callback = self._spec_callbacks[name]\n            spec_override = callback(self)\n            if spec_override:\n                # Allow a spec callback to specify both the base spec and\n                # the override, for situations such as OctetString and parse_as\n                if spec_override.__class__ == tuple and len(spec_override) == 2:\n                    field_spec, value_spec = spec_override\n                    if value_spec is None:\n                        value_spec = field_spec\n                        spec_override = None\n                # When no field spec is specified, use a single return value as that\n                elif field_spec is None:\n                    field_spec = spec_override\n                    value_spec = field_spec\n                    spec_override = None\n                else:\n                    value_spec = spec_override\n\n        elif self._oid_nums is not None and self._oid_nums[1] == index:\n            oid = self._lazy_child(self._oid_nums[0]).native\n            if oid in self._oid_specs:\n                spec_override = self._oid_specs[oid]\n                value_spec = spec_override\n\n        return (name, field_spec, value_spec, field_params, spec_override)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _make_value(self, field_name, field_spec, value_spec, field_params, value):\n\n        if value is None and 'optional' in field_params:\n            return VOID\n\n        specs_different = field_spec != value_spec\n        is_any = issubclass(field_spec, Any)\n\n        if issubclass(value_spec, Choice):\n            is_asn1value = isinstance(value, Asn1Value)\n            is_tuple = isinstance(value, tuple) and len(value) == 2\n            is_dict = isinstance(value, dict) and len(value) == 1\n            if not is_asn1value and not is_tuple and not is_dict:\n                raise ValueError(unwrap(\n                    '''\n                    Can not set a native python value to %s, which has the\n                    choice type of %s - value must be an instance of Asn1Value\n                    ''',\n                    field_name,\n                    type_name(value_spec)\n                ))\n            if is_tuple or is_dict:\n                value = value_spec(value)\n            if not isinstance(value, value_spec):\n                wrapper = value_spec()\n                wrapper.validate(value.class_, value.tag, value.contents)\n                wrapper._parsed = value\n                new_value = wrapper\n            else:\n                new_value = value\n\n        elif isinstance(value, field_spec):\n            new_value = value\n            if specs_different:\n                new_value.parse(value_spec)\n\n        elif (not specs_different or is_any) and not isinstance(value, value_spec):\n            if (not is_any or specs_different) and isinstance(value, Asn1Value):\n                raise TypeError(unwrap(\n                    '''\n                    %s value must be %s, not %s\n                    ''',\n                    field_name,\n                    type_name(value_spec),\n                    type_name(value)\n                ))\n            new_value = value_spec(value, **field_params)\n\n        else:\n            if isinstance(value, value_spec):\n                new_value = value\n            else:\n                if isinstance(value, Asn1Value):\n                    raise TypeError(unwrap(\n                        '''\n                        %s value must be %s, not %s\n                        ''',\n                        field_name,\n                        type_name(value_spec),\n                        type_name(value)\n                    ))\n                new_value = value_spec(value)\n\n            # For when the field is OctetString or OctetBitString with embedded\n            # values we need to wrap the value in the field spec to get the\n            # appropriate encoded value.\n            if specs_different and not is_any:\n                wrapper = field_spec(value=new_value.dump(), **field_params)\n                wrapper._parsed = (new_value, new_value.__class__, None)\n                new_value = wrapper\n\n        new_value = _fix_tagging(new_value, field_params)\n\n        return new_value", "response": "Creates an appropriate Asn1Value object from the value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_children(self, recurse=False):\n\n        cls = self.__class__\n        if self._contents is None:\n            if self._fields:\n                self.children = [VOID] * len(self._fields)\n                for index, (_, _, params) in enumerate(self._fields):\n                    if 'default' in params:\n                        if cls._precomputed_specs[index]:\n                            field_name, field_spec, value_spec, field_params, _ = cls._precomputed_specs[index]\n                        else:\n                            field_name, field_spec, value_spec, field_params, _ = self._determine_spec(index)\n                        self.children[index] = self._make_value(field_name, field_spec, value_spec, field_params, None)\n            return\n\n        try:\n            self.children = []\n            contents_length = len(self._contents)\n            child_pointer = 0\n            field = 0\n            field_len = len(self._fields)\n            parts = None\n            again = child_pointer < contents_length\n            while again:\n                if parts is None:\n                    parts, child_pointer = _parse(self._contents, contents_length, pointer=child_pointer)\n                again = child_pointer < contents_length\n\n                if field < field_len:\n                    _, field_spec, value_spec, field_params, spec_override = (\n                        cls._precomputed_specs[field] or self._determine_spec(field))\n\n                    # If the next value is optional or default, allow it to be absent\n                    if field_params and ('optional' in field_params or 'default' in field_params):\n                        if self._field_ids[field] != (parts[0], parts[2]) and field_spec != Any:\n\n                            # See if the value is a valid choice before assuming\n                            # that we have a missing optional or default value\n                            choice_match = False\n                            if issubclass(field_spec, Choice):\n                                try:\n                                    tester = field_spec(**field_params)\n                                    tester.validate(parts[0], parts[2], parts[4])\n                                    choice_match = True\n                                except (ValueError):\n                                    pass\n\n                            if not choice_match:\n                                if 'optional' in field_params:\n                                    self.children.append(VOID)\n                                else:\n                                    self.children.append(field_spec(**field_params))\n                                field += 1\n                                again = True\n                                continue\n\n                    if field_spec is None or (spec_override and issubclass(field_spec, Any)):\n                        field_spec = value_spec\n                        spec_override = None\n\n                    if spec_override:\n                        child = parts + (field_spec, field_params, value_spec)\n                    else:\n                        child = parts + (field_spec, field_params)\n\n                # Handle situations where an optional or defaulted field definition is incorrect\n                elif field_len > 0 and field + 1 <= field_len:\n                    missed_fields = []\n                    prev_field = field - 1\n                    while prev_field >= 0:\n                        prev_field_info = self._fields[prev_field]\n                        if len(prev_field_info) < 3:\n                            break\n                        if 'optional' in prev_field_info[2] or 'default' in prev_field_info[2]:\n                            missed_fields.append(prev_field_info[0])\n                        prev_field -= 1\n                    plural = 's' if len(missed_fields) > 1 else ''\n                    missed_field_names = ', '.join(missed_fields)\n                    raise ValueError(unwrap(\n                        '''\n                        Data for field %s (%s class, %s method, tag %s) does\n                        not match the field definition%s of %s\n                        ''',\n                        field + 1,\n                        CLASS_NUM_TO_NAME_MAP.get(parts[0]),\n                        METHOD_NUM_TO_NAME_MAP.get(parts[1]),\n                        parts[2],\n                        plural,\n                        missed_field_names\n                    ))\n\n                else:\n                    child = parts\n\n                if recurse:\n                    child = _build(*child)\n                    if isinstance(child, (Sequence, SequenceOf)):\n                        child._parse_children(recurse=True)\n\n                self.children.append(child)\n                field += 1\n                parts = None\n\n            index = len(self.children)\n            while index < field_len:\n                name, field_spec, field_params = self._fields[index]\n                if 'default' in field_params:\n                    self.children.append(field_spec(**field_params))\n                elif 'optional' in field_params:\n                    self.children.append(VOID)\n                else:\n                    raise ValueError(unwrap(\n                        '''\n                        Field \"%s\" is missing from structure\n                        ''',\n                        name\n                    ))\n                index += 1\n\n        except (ValueError, TypeError) as e:\n            self.children = None\n            args = e.args[1:]\n            e.args = (e.args[0] + '\\n    while parsing %s' % type_name(self),) + args\n            raise e", "response": "Parses the contents and generates Asn1Value objects based on the _fields and _precomputed_specs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef spec(self, field_name):\n\n        if not isinstance(field_name, str_cls):\n            raise TypeError(unwrap(\n                '''\n                field_name must be a unicode string, not %s\n                ''',\n                type_name(field_name)\n            ))\n\n        if self._fields is None:\n            raise ValueError(unwrap(\n                '''\n                Unable to retrieve spec for field %s in the class %s because\n                _fields has not been set\n                ''',\n                repr(field_name),\n                type_name(self)\n            ))\n\n        index = self._field_map[field_name]\n        info = self._determine_spec(index)\n\n        return info[2]", "response": "Determines the spec for the specified object identifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the native Python datatype representation of this value.", "response": "def native(self):\n        \"\"\"\n        The native Python datatype representation of this value\n\n        :return:\n            An OrderedDict or None. If an OrderedDict, all child values are\n            recursively converted to native representation also.\n        \"\"\"\n\n        if self.contents is None:\n            return None\n\n        if self._native is None:\n            if self.children is None:\n                self._parse_children(recurse=True)\n            try:\n                self._native = OrderedDict()\n                for index, child in enumerate(self.children):\n                    if child.__class__ == tuple:\n                        child = _build(*child)\n                        self.children[index] = child\n                    try:\n                        name = self._fields[index][0]\n                    except (IndexError):\n                        name = str_cls(index)\n                    self._native[name] = child.native\n            except (ValueError, TypeError) as e:\n                self._native = None\n                args = e.args[1:]\n                e.args = (e.args[0] + '\\n    while parsing %s' % type_name(self),) + args\n                raise e\n        return self._native"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _copy(self, other, copy_func):\n\n        super(Sequence, self)._copy(other, copy_func)\n        if self.children is not None:\n            self.children = []\n            for child in other.children:\n                if child.__class__ == tuple:\n                    self.children.append(child)\n                else:\n                    self.children.append(child.copy())", "response": "Copies the contents of another Sequence object to itself"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef debug(self, nest_level=1):\n\n        if self.children is None:\n            self._parse_children()\n\n        prefix = '  ' * nest_level\n        _basic_debug(prefix, self)\n        for field_name in self:\n            child = self._lazy_child(self._field_map[field_name])\n            if child is not VOID:\n                print('%s    Field \"%s\"' % (prefix, field_name))\n                child.debug(nest_level + 3)", "response": "Show the binary data and parsed data in a tree structure."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dump(self, force=False):\n\n        if force:\n            self._set_contents(force=force)\n\n        if self._fields and self.children is not None:\n            for index, (field_name, _, params) in enumerate(self._fields):\n                if self.children[index] is not VOID:\n                    continue\n                if 'default' in params or 'optional' in params:\n                    continue\n                raise ValueError(unwrap(\n                    '''\n                    Field \"%s\" is missing from structure\n                    ''',\n                    field_name\n                ))\n\n        return Asn1Value.dump(self)", "response": "Dumps the value of this object into a DER - encoded value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _make_value(self, value):\n\n        if isinstance(value, self._child_spec):\n            new_value = value\n\n        elif issubclass(self._child_spec, Any):\n            if isinstance(value, Asn1Value):\n                new_value = value\n            else:\n                raise ValueError(unwrap(\n                    '''\n                    Can not set a native python value to %s where the\n                    _child_spec is Any - value must be an instance of Asn1Value\n                    ''',\n                    type_name(self)\n                ))\n\n        elif issubclass(self._child_spec, Choice):\n            if not isinstance(value, Asn1Value):\n                raise ValueError(unwrap(\n                    '''\n                    Can not set a native python value to %s where the\n                    _child_spec is the choice type %s - value must be an\n                    instance of Asn1Value\n                    ''',\n                    type_name(self),\n                    self._child_spec.__name__\n                ))\n            if not isinstance(value, self._child_spec):\n                wrapper = self._child_spec()\n                wrapper.validate(value.class_, value.tag, value.contents)\n                wrapper._parsed = value\n                value = wrapper\n            new_value = value\n\n        else:\n            return self._child_spec(value=value)\n\n        params = {}\n        if self._child_spec.explicit:\n            params['explicit'] = self._child_spec.explicit\n        if self._child_spec.implicit:\n            params['implicit'] = (self._child_spec.class_, self._child_spec.tag)\n        return _fix_tagging(new_value, params)", "response": "Construct a _child_spec value from a native Python data type or an appropriate Asn1Value object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef append(self, value):\n\n        # We inline this checks to prevent method invocation each time\n        if self.children is None:\n            self._parse_children()\n\n        self.children.append(self._make_value(value))\n\n        if self._native is not None:\n            self._native.append(self.children[-1].native)\n\n        self._mutated = True", "response": "Allows adding a child to the end of the sequence\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _set_contents(self, force=False):\n\n        if self.children is None:\n            self._parse_children()\n\n        contents = BytesIO()\n        for child in self:\n            contents.write(child.dump(force=force))\n        self._contents = contents.getvalue()\n        self._header = None\n        if self._trailer != b'':\n            self._trailer = b''", "response": "Sets the contents of the object to the contents of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_children(self, recurse=False):\n\n        try:\n            self.children = []\n            if self._contents is None:\n                return\n            contents_length = len(self._contents)\n            child_pointer = 0\n            while child_pointer < contents_length:\n                parts, child_pointer = _parse(self._contents, contents_length, pointer=child_pointer)\n                if self._child_spec:\n                    child = parts + (self._child_spec,)\n                else:\n                    child = parts\n                if recurse:\n                    child = _build(*child)\n                    if isinstance(child, (Sequence, SequenceOf)):\n                        child._parse_children(recurse=True)\n                self.children.append(child)\n        except (ValueError, TypeError) as e:\n            self.children = None\n            args = e.args[1:]\n            e.args = (e.args[0] + '\\n    while parsing %s' % type_name(self),) + args\n            raise e", "response": "Parses the contents and generates Asn1Value objects based on the _child_spec."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef native(self):\n\n        if self.contents is None:\n            return None\n\n        if self._native is None:\n            if self.children is None:\n                self._parse_children(recurse=True)\n            try:\n                self._native = [child.native for child in self]\n            except (ValueError, TypeError) as e:\n                args = e.args[1:]\n                e.args = (e.args[0] + '\\n    while parsing %s' % type_name(self),) + args\n                raise e\n        return self._native", "response": "Returns the native Python datatype representation of this value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef debug(self, nest_level=1):\n\n        if self.children is None:\n            self._parse_children()\n\n        prefix = '  ' * nest_level\n        _basic_debug(prefix, self)\n        for child in self:\n            child.debug(nest_level + 1)", "response": "Show the binary data and parsed data in a tree structure."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndump the value using DER", "response": "def dump(self, force=False):\n        \"\"\"\n        Encodes the value using DER\n\n        :param force:\n            If the encoded contents already exist, clear them and regenerate\n            to ensure they are in DER format instead of BER format\n\n        :return:\n            A byte string of the DER-encoded value\n        \"\"\"\n\n        if force:\n            self._set_contents(force=force)\n\n        return Asn1Value.dump(self)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_children(self, recurse=False):\n\n        cls = self.__class__\n        if self._contents is None:\n            if self._fields:\n                self.children = [VOID] * len(self._fields)\n                for index, (_, _, params) in enumerate(self._fields):\n                    if 'default' in params:\n                        if cls._precomputed_specs[index]:\n                            field_name, field_spec, value_spec, field_params, _ = cls._precomputed_specs[index]\n                        else:\n                            field_name, field_spec, value_spec, field_params, _ = self._determine_spec(index)\n                        self.children[index] = self._make_value(field_name, field_spec, value_spec, field_params, None)\n            return\n\n        try:\n            child_map = {}\n            contents_length = len(self.contents)\n            child_pointer = 0\n            seen_field = 0\n            while child_pointer < contents_length:\n                parts, child_pointer = _parse(self.contents, contents_length, pointer=child_pointer)\n\n                id_ = (parts[0], parts[2])\n\n                field = self._field_ids.get(id_)\n                if field is None:\n                    raise ValueError(unwrap(\n                        '''\n                        Data for field %s (%s class, %s method, tag %s) does\n                        not match any of the field definitions\n                        ''',\n                        seen_field,\n                        CLASS_NUM_TO_NAME_MAP.get(parts[0]),\n                        METHOD_NUM_TO_NAME_MAP.get(parts[1]),\n                        parts[2],\n                    ))\n\n                _, field_spec, value_spec, field_params, spec_override = (\n                    cls._precomputed_specs[field] or self._determine_spec(field))\n\n                if field_spec is None or (spec_override and issubclass(field_spec, Any)):\n                    field_spec = value_spec\n                    spec_override = None\n\n                if spec_override:\n                    child = parts + (field_spec, field_params, value_spec)\n                else:\n                    child = parts + (field_spec, field_params)\n\n                if recurse:\n                    child = _build(*child)\n                    if isinstance(child, (Sequence, SequenceOf)):\n                        child._parse_children(recurse=True)\n\n                child_map[field] = child\n                seen_field += 1\n\n            total_fields = len(self._fields)\n\n            for index in range(0, total_fields):\n                if index in child_map:\n                    continue\n\n                name, field_spec, value_spec, field_params, spec_override = (\n                    cls._precomputed_specs[index] or self._determine_spec(index))\n\n                if field_spec is None or (spec_override and issubclass(field_spec, Any)):\n                    field_spec = value_spec\n                    spec_override = None\n\n                missing = False\n\n                if not field_params:\n                    missing = True\n                elif 'optional' not in field_params and 'default' not in field_params:\n                    missing = True\n                elif 'optional' in field_params:\n                    child_map[index] = VOID\n                elif 'default' in field_params:\n                    child_map[index] = field_spec(**field_params)\n\n                if missing:\n                    raise ValueError(unwrap(\n                        '''\n                        Missing required field \"%s\" from %s\n                        ''',\n                        name,\n                        type_name(self)\n                    ))\n\n            self.children = []\n            for index in range(0, total_fields):\n                self.children.append(child_map[index])\n\n        except (ValueError, TypeError) as e:\n            args = e.args[1:]\n            e.args = (e.args[0] + '\\n    while parsing %s' % type_name(self),) + args\n            raise e", "response": "Parses the contents and generates Asn1Value objects based on the _fields and _precomputed_specs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _set_contents(self, force=False):\n\n        if self.children is None:\n            self._parse_children()\n\n        child_tag_encodings = []\n        for index, child in enumerate(self.children):\n            child_encoding = child.dump(force=force)\n\n            # Skip encoding defaulted children\n            name, spec, field_params = self._fields[index]\n            if 'default' in field_params:\n                if spec(**field_params).dump() == child_encoding:\n                    continue\n\n            child_tag_encodings.append((child.tag, child_encoding))\n        child_tag_encodings.sort(key=lambda ct: ct[0])\n\n        self._contents = b''.join([ct[1] for ct in child_tag_encodings])\n        self._header = None\n        if self._trailer != b'':\n            self._trailer = b''", "response": "Sets the contents of the object to the contents of the object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _set_contents(self, force=False):\n\n        if self.children is None:\n            self._parse_children()\n\n        child_encodings = []\n        for child in self:\n            child_encodings.append(child.dump(force=force))\n\n        self._contents = b''.join(sorted(child_encodings))\n        self._header = None\n        if self._trailer != b'':\n            self._trailer = b''", "response": "Sets the contents of the object to the contents of the children."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the native Python datatype representation of this value.", "response": "def native(self):\n        \"\"\"\n        The native Python datatype representation of this value\n\n        :return:\n            A datetime.datetime object in the UTC timezone or None\n        \"\"\"\n\n        if self.contents is None:\n            return None\n\n        if self._native is None:\n            string = str_cls(self)\n            has_timezone = re.search('[-\\\\+]', string)\n\n            # We don't know what timezone it is in, or it is UTC because of a Z\n            # suffix, so we just assume UTC\n            if not has_timezone:\n                string = string.rstrip('Z')\n                date = self._date_by_len(string)\n                self._native = date.replace(tzinfo=timezone.utc)\n\n            else:\n                # Python 2 doesn't support the %z format code, so we have to manually\n                # process the timezone offset.\n                date = self._date_by_len(string[0:-5])\n\n                hours = int(string[-4:-2])\n                minutes = int(string[-2:])\n                delta = timedelta(hours=abs(hours), minutes=minutes)\n                if hours < 0:\n                    date -= delta\n                else:\n                    date += delta\n\n                self._native = date.replace(tzinfo=timezone.utc)\n\n        return self._native"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set(self, value):\n\n        if isinstance(value, datetime):\n            value = value.strftime('%y%m%d%H%M%SZ')\n            if _PY2:\n                value = value.decode('ascii')\n\n        AbstractString.set(self, value)\n        # Set it to None and let the class take care of converting the next\n        # time that .native is called\n        self._native = None", "response": "Sets the value of the object\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _date_by_len(self, string):\n\n        strlen = len(string)\n\n        year_num = int(string[0:2])\n        if year_num < 50:\n            prefix = '20'\n        else:\n            prefix = '19'\n\n        if strlen == 10:\n            return datetime.strptime(prefix + string, '%Y%m%d%H%M')\n\n        if strlen == 12:\n            return datetime.strptime(prefix + string, '%Y%m%d%H%M%S')\n\n        return string", "response": "Parses a date from a string based on its length"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _date_by_len(self, string):\n\n        strlen = len(string)\n\n        date_format = None\n        if strlen == 10:\n            date_format = '%Y%m%d%H'\n        elif strlen == 12:\n            date_format = '%Y%m%d%H%M'\n        elif strlen == 14:\n            date_format = '%Y%m%d%H%M%S'\n        elif strlen == 18:\n            date_format = '%Y%m%d%H%M%S.%f'\n\n        if date_format:\n            if len(string) >= 4 and string[0:4] == '0000':\n                # Year 2000 shares a calendar with year 0, and is supported natively\n                t = datetime.strptime('2000' + string[4:], date_format)\n                return extended_datetime(\n                    0,\n                    t.month,\n                    t.day,\n                    t.hour,\n                    t.minute,\n                    t.second,\n                    t.microsecond,\n                    t.tzinfo\n                )\n            return datetime.strptime(string, date_format)\n\n        return string", "response": "Parses a date from a string based on its length"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate that the class and tag specified exist as an alternative.", "response": "def validate(self, class_, tag, contents):\n        \"\"\"\n        Ensures that the class and tag specified exist as an alternative. This\n        custom version fixes parsing broken encodings there a V2 attribute\n        # certificate is encoded as a V1\n\n        :param class_:\n            The integer class_ from the encoded value header\n\n        :param tag:\n            The integer tag from the encoded value header\n\n        :param contents:\n            A byte string of the contents of the value - used when the object\n            is explicitly tagged\n\n        :raises:\n            ValueError - when value is not a valid alternative\n        \"\"\"\n\n        super(CertificateChoices, self).validate(class_, tag, contents)\n        if self._choice == 2:\n            if AttCertVersion.load(Sequence.load(contents)[0].dump()).native == 'v2':\n                self._choice = 3"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef type_name(value):\n\n    if inspect.isclass(value):\n        cls = value\n    else:\n        cls = value.__class__\n    if cls.__module__ in set(['builtins', '__builtin__']):\n        return cls.__name__\n    return '%s.%s' % (cls.__module__, cls.__name__)", "response": "Returns a user - readable name for the type of an object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fill_width(bytes_, width):\n\n    while len(bytes_) < width:\n        bytes_ = b'\\x00' + bytes_\n    return bytes_", "response": "Ensure a byte string representing a positive integer is a specific width."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a byte string of the ASN. 1 DER - encoded value.", "response": "def emit(class_, method, tag, contents):\n    \"\"\"\n    Constructs a byte string of an ASN.1 DER-encoded value\n\n    This is typically not useful. Instead, use one of the standard classes from\n    asn1crypto.core, or construct a new class with specific fields, and call the\n    .dump() method.\n\n    :param class_:\n        An integer ASN.1 class value: 0 (universal), 1 (application),\n        2 (context), 3 (private)\n\n    :param method:\n        An integer ASN.1 method value: 0 (primitive), 1 (constructed)\n\n    :param tag:\n        An integer ASN.1 tag value\n\n    :param contents:\n        A byte string of the encoded byte contents\n\n    :return:\n        A byte string of the ASN.1 DER value (header and contents)\n    \"\"\"\n\n    if not isinstance(class_, int):\n        raise TypeError('class_ must be an integer, not %s' % type_name(class_))\n\n    if class_ < 0 or class_ > 3:\n        raise ValueError('class_ must be one of 0, 1, 2 or 3, not %s' % class_)\n\n    if not isinstance(method, int):\n        raise TypeError('method must be an integer, not %s' % type_name(method))\n\n    if method < 0 or method > 1:\n        raise ValueError('method must be 0 or 1, not %s' % method)\n\n    if not isinstance(tag, int):\n        raise TypeError('tag must be an integer, not %s' % type_name(tag))\n\n    if tag < 0:\n        raise ValueError('tag must be greater than zero, not %s' % tag)\n\n    if not isinstance(contents, byte_cls):\n        raise TypeError('contents must be a byte string, not %s' % type_name(contents))\n\n    return _dump_header(class_, method, tag, contents) + contents"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(contents, strict=False):\n\n    if not isinstance(contents, byte_cls):\n        raise TypeError('contents must be a byte string, not %s' % type_name(contents))\n\n    contents_len = len(contents)\n    info, consumed = _parse(contents, contents_len)\n    if strict and consumed != contents_len:\n        raise ValueError('Extra data - %d bytes of trailing data were provided' % (contents_len - consumed))\n    return info", "response": "Parses a byte string of ASN. 1 BER - encoded data into a 6 - element tuple."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a byte string of ASN. 1 BER - encoded data and returns the number of bytes consumed by the ASN. 1 value.", "response": "def peek(contents):\n    \"\"\"\n    Parses a byte string of ASN.1 BER/DER-encoded data to find the length\n\n    This is typically used to look into an encoded value to see how long the\n    next chunk of ASN.1-encoded data is. Primarily it is useful when a\n    value is a concatenation of multiple values.\n\n    :param contents:\n        A byte string of BER/DER-encoded data\n\n    :raises:\n        ValueError - when the contents do not contain an ASN.1 header or are truncated in some way\n        TypeError - when contents is not a byte string\n\n    :return:\n        An integer with the number of bytes occupied by the ASN.1 value\n    \"\"\"\n\n    if not isinstance(contents, byte_cls):\n        raise TypeError('contents must be a byte string, not %s' % type_name(contents))\n\n    info, consumed = _parse(contents, len(contents))\n    return consumed"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a byte string into a 2 - element tuple.", "response": "def _parse(encoded_data, data_len, pointer=0, lengths_only=False):\n    \"\"\"\n    Parses a byte string into component parts\n\n    :param encoded_data:\n        A byte string that contains BER-encoded data\n\n    :param data_len:\n        The integer length of the encoded data\n\n    :param pointer:\n        The index in the byte string to parse from\n\n    :param lengths_only:\n        A boolean to cause the call to return a 2-element tuple of the integer\n        number of bytes in the header and the integer number of bytes in the\n        contents. Internal use only.\n\n    :return:\n        A 2-element tuple:\n         - 0: A tuple of (class_, method, tag, header, content, trailer)\n         - 1: An integer indicating how many bytes were consumed\n    \"\"\"\n\n    if data_len < pointer + 2:\n        raise ValueError(_INSUFFICIENT_DATA_MESSAGE % (2, data_len - pointer))\n\n    start = pointer\n    first_octet = ord(encoded_data[pointer]) if _PY2 else encoded_data[pointer]\n    pointer += 1\n\n    tag = first_octet & 31\n    # Base 128 length using 8th bit as continuation indicator\n    if tag == 31:\n        tag = 0\n        while True:\n            num = ord(encoded_data[pointer]) if _PY2 else encoded_data[pointer]\n            pointer += 1\n            tag *= 128\n            tag += num & 127\n            if num >> 7 == 0:\n                break\n\n    length_octet = ord(encoded_data[pointer]) if _PY2 else encoded_data[pointer]\n    pointer += 1\n\n    if length_octet >> 7 == 0:\n        if lengths_only:\n            return (pointer, pointer + (length_octet & 127))\n        contents_end = pointer + (length_octet & 127)\n\n    else:\n        length_octets = length_octet & 127\n        if length_octets:\n            pointer += length_octets\n            contents_end = pointer + int_from_bytes(encoded_data[pointer - length_octets:pointer], signed=False)\n            if lengths_only:\n                return (pointer, contents_end)\n\n        else:\n            # To properly parse indefinite length values, we need to scan forward\n            # parsing headers until we find a value with a length of zero. If we\n            # just scanned looking for \\x00\\x00, nested indefinite length values\n            # would not work.\n            contents_end = pointer\n            # Unfortunately we need to understand the contents of the data to\n            # properly scan forward, which bleeds some representation info into\n            # the parser. This condition handles the unused bits byte in\n            # constructed bit strings.\n            if tag == 3:\n                contents_end += 1\n            while contents_end < data_len:\n                sub_header_end, contents_end = _parse(encoded_data, data_len, contents_end, lengths_only=True)\n                if contents_end == sub_header_end and encoded_data[contents_end - 2:contents_end] == b'\\x00\\x00':\n                    break\n            if lengths_only:\n                return (pointer, contents_end)\n            if contents_end > data_len:\n                raise ValueError(_INSUFFICIENT_DATA_MESSAGE % (contents_end, data_len))\n            return (\n                (\n                    first_octet >> 6,\n                    (first_octet >> 5) & 1,\n                    tag,\n                    encoded_data[start:pointer],\n                    encoded_data[pointer:contents_end - 2],\n                    b'\\x00\\x00'\n                ),\n                contents_end\n            )\n\n    if contents_end > data_len:\n        raise ValueError(_INSUFFICIENT_DATA_MESSAGE % (contents_end, data_len))\n    return (\n        (\n            first_octet >> 6,\n            (first_octet >> 5) & 1,\n            tag,\n            encoded_data[start:pointer],\n            encoded_data[pointer:contents_end],\n            b''\n        ),\n        contents_end\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndumping the ASN. 1 object into a byte string.", "response": "def _dump_header(class_, method, tag, contents):\n    \"\"\"\n    Constructs the header bytes for an ASN.1 object\n\n    :param class_:\n        An integer ASN.1 class value: 0 (universal), 1 (application),\n        2 (context), 3 (private)\n\n    :param method:\n        An integer ASN.1 method value: 0 (primitive), 1 (constructed)\n\n    :param tag:\n        An integer ASN.1 tag value\n\n    :param contents:\n        A byte string of the encoded byte contents\n\n    :return:\n        A byte string of the ASN.1 DER header\n    \"\"\"\n\n    header = b''\n\n    id_num = 0\n    id_num |= class_ << 6\n    id_num |= method << 5\n\n    if tag >= 31:\n        header += chr_cls(id_num | 31)\n        while tag > 0:\n            continuation_bit = 0x80 if tag > 0x7F else 0\n            header += chr_cls(continuation_bit | (tag & 0x7F))\n            tag = tag >> 7\n    else:\n        header += chr_cls(id_num | tag)\n\n    length = len(contents)\n    if length <= 127:\n        header += chr_cls(length)\n    else:\n        length_bytes = int_to_bytes(length)\n        header += chr_cls(0x80 | len(length_bytes))\n        header += length_bytes\n\n    return header"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nencodes a unicode IRI into an ASCII byte string URI.", "response": "def iri_to_uri(value, normalize=False):\n    \"\"\"\n    Encodes a unicode IRI into an ASCII byte string URI\n\n    :param value:\n        A unicode string of an IRI\n\n    :param normalize:\n        A bool that controls URI normalization\n\n    :return:\n        A byte string of the ASCII-encoded URI\n    \"\"\"\n\n    if not isinstance(value, str_cls):\n        raise TypeError(unwrap(\n            '''\n            value must be a unicode string, not %s\n            ''',\n            type_name(value)\n        ))\n\n    scheme = None\n    # Python 2.6 doesn't split properly is the URL doesn't start with http:// or https://\n    if sys.version_info < (2, 7) and not value.startswith('http://') and not value.startswith('https://'):\n        real_prefix = None\n        prefix_match = re.match('^[^:]*://', value)\n        if prefix_match:\n            real_prefix = prefix_match.group(0)\n            value = 'http://' + value[len(real_prefix):]\n        parsed = urlsplit(value)\n        if real_prefix:\n            value = real_prefix + value[7:]\n            scheme = _urlquote(real_prefix[:-3])\n    else:\n        parsed = urlsplit(value)\n\n    if scheme is None:\n        scheme = _urlquote(parsed.scheme)\n    hostname = parsed.hostname\n    if hostname is not None:\n        hostname = hostname.encode('idna')\n    # RFC 3986 allows userinfo to contain sub-delims\n    username = _urlquote(parsed.username, safe='!$&\\'()*+,;=')\n    password = _urlquote(parsed.password, safe='!$&\\'()*+,;=')\n    port = parsed.port\n    if port is not None:\n        port = str_cls(port).encode('ascii')\n\n    netloc = b''\n    if username is not None:\n        netloc += username\n        if password:\n            netloc += b':' + password\n        netloc += b'@'\n    if hostname is not None:\n        netloc += hostname\n    if port is not None:\n        default_http = scheme == b'http' and port == b'80'\n        default_https = scheme == b'https' and port == b'443'\n        if not normalize or (not default_http and not default_https):\n            netloc += b':' + port\n\n    # RFC 3986 allows a path to contain sub-delims, plus \"@\" and \":\"\n    path = _urlquote(parsed.path, safe='/!$&\\'()*+,;=@:')\n    # RFC 3986 allows the query to contain sub-delims, plus \"@\", \":\" , \"/\" and \"?\"\n    query = _urlquote(parsed.query, safe='/?!$&\\'()*+,;=@:')\n    # RFC 3986 allows the fragment to contain sub-delims, plus \"@\", \":\" , \"/\" and \"?\"\n    fragment = _urlquote(parsed.fragment, safe='/?!$&\\'()*+,;=@:')\n\n    if normalize and query is None and fragment is None and path == b'/':\n        path = None\n\n    # Python 2.7 compat\n    if path is None:\n        path = ''\n\n    output = urlunsplit((scheme, netloc, path, query, fragment))\n    if isinstance(output, str_cls):\n        output = output.encode('latin1')\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef uri_to_iri(value):\n\n    if not isinstance(value, byte_cls):\n        raise TypeError(unwrap(\n            '''\n            value must be a byte string, not %s\n            ''',\n            type_name(value)\n        ))\n\n    parsed = urlsplit(value)\n\n    scheme = parsed.scheme\n    if scheme is not None:\n        scheme = scheme.decode('ascii')\n\n    username = _urlunquote(parsed.username, remap=[':', '@'])\n    password = _urlunquote(parsed.password, remap=[':', '@'])\n    hostname = parsed.hostname\n    if hostname:\n        hostname = hostname.decode('idna')\n    port = parsed.port\n    if port and not isinstance(port, int_types):\n        port = port.decode('ascii')\n\n    netloc = ''\n    if username is not None:\n        netloc += username\n        if password:\n            netloc += ':' + password\n        netloc += '@'\n    if hostname is not None:\n        netloc += hostname\n    if port is not None:\n        netloc += ':' + str_cls(port)\n\n    path = _urlunquote(parsed.path, remap=['/'], preserve=True)\n    query = _urlunquote(parsed.query, remap=['&', '='], preserve=True)\n    fragment = _urlunquote(parsed.fragment)\n\n    return urlunsplit((scheme, netloc, path, query, fragment))", "response": "Converts an ASCII URI byte string into a unicode IRI."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndecoding UTF - 8 errors into an IRI.", "response": "def _iri_utf8_errors_handler(exc):\n    \"\"\"\n    Error handler for decoding UTF-8 parts of a URI into an IRI. Leaves byte\n    sequences encoded in %XX format, but as part of a unicode string.\n\n    :param exc:\n        The UnicodeDecodeError exception\n\n    :return:\n        A 2-element tuple of (replacement unicode string, integer index to\n        resume at)\n    \"\"\"\n\n    bytes_as_ints = bytes_to_list(exc.object[exc.start:exc.end])\n    replacements = ['%%%02x' % num for num in bytes_as_ints]\n    return (''.join(replacements), exc.end)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _urlquote(string, safe=''):\n\n    if string is None or string == '':\n        return None\n\n    # Anything already hex quoted is pulled out of the URL and unquoted if\n    # possible\n    escapes = []\n    if re.search('%[0-9a-fA-F]{2}', string):\n        # Try to unquote any percent values, restoring them if they are not\n        # valid UTF-8. Also, requote any safe chars since encoded versions of\n        # those are functionally different than the unquoted ones.\n        def _try_unescape(match):\n            byte_string = unquote_to_bytes(match.group(0))\n            unicode_string = byte_string.decode('utf-8', 'iriutf8')\n            for safe_char in list(safe):\n                unicode_string = unicode_string.replace(safe_char, '%%%02x' % ord(safe_char))\n            return unicode_string\n        string = re.sub('(?:%[0-9a-fA-F]{2})+', _try_unescape, string)\n\n        # Once we have the minimal set of hex quoted values, removed them from\n        # the string so that they are not double quoted\n        def _extract_escape(match):\n            escapes.append(match.group(0).encode('ascii'))\n            return '\\x00'\n        string = re.sub('%[0-9a-fA-F]{2}', _extract_escape, string)\n\n    output = urlquote(string.encode('utf-8'), safe=safe.encode('utf-8'))\n    if not isinstance(output, byte_cls):\n        output = output.encode('ascii')\n\n    # Restore the existing quoted values that we extracted\n    if len(escapes) > 0:\n        def _return_escape(_):\n            return escapes.pop(0)\n        output = re.sub(b'%00', _return_escape, output)\n\n    return output", "response": "Quotes a unicode string for use in a URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef teletex_search_function(name):\n\n    if name != 'teletex':\n        return None\n\n    return codecs.CodecInfo(\n        name='teletex',\n        encode=TeletexCodec().encode,\n        decode=TeletexCodec().decode,\n        incrementalencoder=TeletexIncrementalEncoder,\n        incrementaldecoder=TeletexIncrementalDecoder,\n        streamreader=TeletexStreamReader,\n        streamwriter=TeletexStreamWriter,\n    )", "response": "Search function for teletex codec that is passed to codecs. register"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef SqueezeNet(include_top=True, weights='imagenet',\n               input_tensor=None, input_shape=None,\n               pooling=None,\n               classes=1000):\n    \"\"\"Instantiates the SqueezeNet architecture.\n    \"\"\"\n        \n    if weights not in {'imagenet', None}:\n        raise ValueError('The `weights` argument should be either '\n                         '`None` (random initialization) or `imagenet` '\n                         '(pre-training on ImageNet).')\n\n    if weights == 'imagenet' and classes != 1000:\n        raise ValueError('If using `weights` as imagenet with `include_top`'\n                         ' as true, `classes` should be 1000')\n\n\n    input_shape = _obtain_input_shape(input_shape,\n                                      default_size=227,\n                                      min_size=48,\n                                      data_format=K.image_data_format(),\n                                      require_flatten=include_top)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n\n\n    x = Convolution2D(64, (3, 3), strides=(2, 2), padding='valid', name='conv1')(img_input)\n    x = Activation('relu', name='relu_conv1')(x)\n    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)\n\n    x = fire_module(x, fire_id=2, squeeze=16, expand=64)\n    x = fire_module(x, fire_id=3, squeeze=16, expand=64)\n    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool3')(x)\n\n    x = fire_module(x, fire_id=4, squeeze=32, expand=128)\n    x = fire_module(x, fire_id=5, squeeze=32, expand=128)\n    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool5')(x)\n\n    x = fire_module(x, fire_id=6, squeeze=48, expand=192)\n    x = fire_module(x, fire_id=7, squeeze=48, expand=192)\n    x = fire_module(x, fire_id=8, squeeze=64, expand=256)\n    x = fire_module(x, fire_id=9, squeeze=64, expand=256)\n    \n    if include_top:\n        # It's not obvious where to cut the network... \n        # Could do the 8th or 9th layer... some work recommends cutting earlier layers.\n    \n        x = Dropout(0.5, name='drop9')(x)\n\n        x = Convolution2D(classes, (1, 1), padding='valid', name='conv10')(x)\n        x = Activation('relu', name='relu_conv10')(x)\n        x = GlobalAveragePooling2D()(x)\n        x = Activation('softmax', name='loss')(x)\n    else:\n        if pooling == 'avg':\n            x = GlobalAveragePooling2D()(x)\n        elif pooling=='max':\n            x = GlobalMaxPooling2D()(x)\n        elif pooling==None:\n            pass\n        else:\n            raise ValueError(\"Unknown argument for 'pooling'=\" + pooling)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n\n    model = Model(inputs, x, name='squeezenet')\n\n    # load weights\n    if weights == 'imagenet':\n        if include_top:\n            weights_path = get_file('squeezenet_weights_tf_dim_ordering_tf_kernels.h5',\n                                    WEIGHTS_PATH,\n                                    cache_subdir='models')\n        else:\n            weights_path = get_file('squeezenet_weights_tf_dim_ordering_tf_kernels_notop.h5',\n                                    WEIGHTS_PATH_NO_TOP,\n                                    cache_subdir='models')\n            \n        model.load_weights(weights_path)\n        if K.backend() == 'theano':\n            layer_utils.convert_all_kernels_in_model(model)\n\n        if K.image_data_format() == 'channels_first':\n\n            if K.backend() == 'tensorflow':\n                warnings.warn('You are using the TensorFlow backend, yet you '\n                              'are using the Theano '\n                              'image data format convention '\n                              '(`image_data_format=\"channels_first\"`). '\n                              'For best performance, set '\n                              '`image_data_format=\"channels_last\"` in '\n                              'your Keras config '\n                              'at ~/.keras/keras.json.')\n    return model", "response": "Constructs the SqueezeNet architecture."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef init_args():\n    parser = argparse.ArgumentParser(\n        description='draw basic graphs on terminal')\n    parser.add_argument(\n        'filename',\n        nargs='?',\n        default=\"-\",\n        help='data file name (comma or space separated). Defaults to stdin.')\n    parser.add_argument(\n        '--title',\n        help='Title of graph'\n    )\n    parser.add_argument(\n        '--width',\n        type=int,\n        default=50,\n        help='width of graph in characters default:50'\n    )\n    parser.add_argument(\n        '--format',\n        default='{:<5.2f}',\n        help='format specifier to use.'\n    )\n    parser.add_argument(\n        '--suffix',\n        default='',\n        help='string to add as a suffix to all data points.'\n    )\n    parser.add_argument(\n        '--no-labels',\n        action='store_true',\n        help='Do not print the label column'\n    )\n    parser.add_argument(\n        '--color',\n        nargs='*',\n        choices=AVAILABLE_COLORS,\n        help='Graph bar color( s )'\n    )\n    parser.add_argument(\n        '--vertical',\n        action='store_true',\n        help='Vertical graph'\n    )\n    parser.add_argument(\n        '--stacked',\n        action='store_true',\n        help='Stacked bar graph'\n    )\n    parser.add_argument(\n        '--different-scale',\n        action='store_true',\n        help='Categories have different scales.'\n    )\n    parser.add_argument(\n        '--calendar',\n        action='store_true',\n        help='Calendar Heatmap chart'\n    )\n    parser.add_argument(\n        '--start-dt',\n        help='Start date for Calendar chart'\n    )\n    parser.add_argument(\n        '--custom-tick',\n        default='',\n        help='Custom tick mark, emoji approved'\n    )\n    parser.add_argument(\n        '--delim',\n        default='',\n        help='Custom delimiter, default , or space'\n    )\n    parser.add_argument(\n        '--verbose',\n        action='store_true',\n        help='Verbose output, helpful for debugging'\n    )\n    parser.add_argument(\n        '--version',\n        action='store_true',\n        help='Display version and exit'\n    )\n    if len(sys.argv) == 1:\n        if sys.stdin.isatty():\n            parser.print_usage()\n            sys.exit(2)\n\n    args = vars(parser.parse_args())\n\n    if args['custom_tick'] != '':\n        global TICK, SM_TICK\n        TICK = args['custom_tick']\n        SM_TICK = ''\n\n    if args['delim'] != '':\n        global DELIM\n        DELIM = args['delim']\n\n    return args", "response": "Parse and return the arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the maximum length for the labels.", "response": "def find_max_label_length(labels):\n    \"\"\"Return the maximum length for the labels.\"\"\"\n    length = 0\n    for i in range(len(labels)):\n        if len(labels[i]) > length:\n            length = len(labels[i])\n\n    return length"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nnormalize the data and return it.", "response": "def normalize(data, width):\n    \"\"\"Normalize the data and return it.\"\"\"\n    min_dat = find_min(data)\n    # We offset by the minimum if there's a negative.\n    off_data = []\n    if min_dat < 0:\n        min_dat = abs(min_dat)\n        for dat in data:\n            off_data.append([_d + min_dat for _d in dat])\n    else:\n        off_data = data\n    min_dat = find_min(off_data)\n    max_dat = find_max(off_data)\n\n    if max_dat < width:\n        # Don't need to normalize if the max value\n        # is less than the width we allow.\n        return off_data\n\n    # max_dat / width is the value for a single tick. norm_factor is the\n    # inverse of this value\n    # If you divide a number to the value of single tick, you will find how\n    # many ticks it does contain basically.\n    norm_factor = width / float(max_dat)\n    normal_dat = []\n    for dat in off_data:\n        normal_dat.append([_v * norm_factor for _v in dat])\n\n    return normal_dat"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npreparing the horizontal graph. Each row is printed through the print_row function.", "response": "def horiz_rows(labels, data, normal_dat, args, colors):\n    \"\"\"Prepare the horizontal graph.\n       Each row is printed through the print_row function.\"\"\"\n    val_min = find_min(data)\n\n    for i in range(len(labels)):\n        if args['no_labels']:\n            # Hide the labels.\n            label = ''\n        else:\n            label = \"{:<{x}}: \".format(labels[i],\n                                       x=find_max_label_length(labels))\n\n        values = data[i]\n        num_blocks = normal_dat[i]\n\n        for j in range(len(values)):\n            # In Multiple series graph 1st category has label at the beginning,\n            # whereas the rest categories have only spaces.\n            if j > 0:\n                len_label = len(label)\n                label = ' ' * len_label\n            tail = ' {}{}'.format(args['format'].format(values[j]),\n                                  args['suffix'])\n            if colors:\n                color = colors[j]\n            else:\n                color = None\n\n            if not args['vertical']:\n                print(label, end=\"\")\n\n            yield(values[j], int(num_blocks[j]), val_min, color)\n\n            if not args['vertical']:\n                print(tail)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stacked_graph(labels, data, normal_data, len_categories, args, colors):\n    val_min = find_min(data)\n\n    for i in range(len(labels)):\n        if args['no_labels']:\n            # Hide the labels.\n            label = ''\n        else:\n            label = \"{:<{x}}: \".format(labels[i],\n                                       x=find_max_label_length(labels))\n\n        print(label, end=\"\")\n        values = data[i]\n        num_blocks = normal_data[i]\n\n        for j in range(len(values)):\n            print_row(values[j], int(num_blocks[j]), val_min, colors[j])\n\n        tail = ' {}{}'.format(args['format'].format(sum(values)),\n                              args['suffix'])\n        print(tail)", "response": "Prepare the horizontal stacked graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef vertically(value, num_blocks, val_min, color, args):\n    global maxi, value_list\n\n    value_list.append(str(value))\n\n    # In case the number of blocks at the end of the normalization is less\n    # than the default number, use the maxi variable to escape.\n    if maxi < num_blocks:\n        maxi = num_blocks\n\n    if num_blocks > 0:\n        vertical_list.append((TICK * num_blocks))\n    else:\n        vertical_list.append(SM_TICK)\n\n    # Zip_longest method in order to turn them vertically.\n    for row in zip_longest(*vertical_list, fillvalue='  '):\n        zipped_list.append(row)\n\n    counter, result_list = 0, []\n\n    # Combined with the maxi variable, escapes the appending method at\n    # the correct point or the default one (width).\n    for i in reversed(zipped_list):\n        result_list.append(i)\n        counter += 1\n\n        if maxi == args['width']:\n            if counter == (args['width']):\n                break\n        else:\n            if counter == maxi:\n                break\n\n    # Return a list of rows which will be used to print the result vertically.\n    return result_list", "response": "This function is used to turn the entire graph vertically."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprinting the whole vertical graph.", "response": "def print_vertical(vertical_rows, labels, color, args):\n    \"\"\"Print the whole vertical graph.\"\"\"\n    if color:\n        sys.stdout.write(f'\\033[{color}m') # Start to write colorized.\n\n    for row in vertical_rows:\n        print(*row)\n\n    sys.stdout.write('\\033[0m') # End of printing colored\n\n    print(\"-\" * len(row) + \"Values\" + \"-\" * len(row))\n    # Print Values\n    for value in zip_longest(*value_list, fillvalue=' '):\n        print(\"  \".join(value))\n\n    if args['no_labels'] == False:\n        print(\"-\" * len(row) + \"Labels\" + \"-\" * len(row))\n        # Print Labels\n        for label in zip_longest(*labels, fillvalue=''):\n            print(\"  \".join(label))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles the normalization of data and printing of the graph.", "response": "def chart(colors, data, args, labels):\n    \"\"\"Handle the normalization of data and the printing of the graph.\"\"\"\n    len_categories = len(data[0])\n    if len_categories > 1:\n        # Stacked graph\n        if args['stacked']:\n            normal_dat = normalize(data, args['width'])\n            stacked_graph(labels, data, normal_dat, len_categories,\n                          args, colors)\n            return\n\n        if not colors:\n            colors = [None] * len_categories\n\n        # Multiple series graph with different scales\n        # Normalization per category\n        if args['different_scale']:\n            for i in range(len_categories):\n                cat_data = []\n                for dat in data:\n                    cat_data.append([dat[i]])\n\n                # Normalize data, handle negatives.\n                normal_cat_data = normalize(cat_data, args['width'])\n\n                # Generate data for a row.\n                for row in horiz_rows(labels, cat_data, normal_cat_data,\n                                      args, [colors[i]]):\n                    # Print the row\n                    if not args['vertical']:\n                        print_row(*row)\n                    else:\n                        vertic = vertically(*row, args=args)\n\n                # Vertical graph\n                if args['vertical']:\n                    print_vertical(vertic, labels, colors[i], args)\n\n                print()\n                value_list.clear(), zipped_list.clear(), vertical_list.clear()\n                return\n\n    # One category/Multiple series graph with same scale\n    # All-together normalization\n    if not args['stacked']:\n        normal_dat = normalize(data, args['width'])\n        for row in horiz_rows(labels, data, normal_dat, args, colors):\n            if not args['vertical']:\n                print_row(*row)\n            else:\n                vertic = vertically(*row, args=args)\n\n        if args['vertical'] and len_categories == 1:\n            if colors:\n                color = colors[0]\n            else:\n                color = None\n\n            print_vertical(vertic, labels, color, args)\n\n        print()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_data(labels, data, args):\n    len_categories = len(data[0])\n\n    # Check that there are data for all labels.\n    if len(labels) != len(data):\n        print(\">> Error: Label and data array sizes don't match\")\n        sys.exit(1)\n\n    # Check that there are data for all categories per label.\n    for dat in data:\n        if len(dat) != len_categories:\n            print(\">> Error: There are missing values\")\n            sys.exit(1)\n\n    colors = []\n\n    # If user inserts colors, they should be as many as the categories.\n    if args['color'] is not None:\n        if len(args['color']) != len_categories:\n            print(\">> Error: Color and category array sizes don't match\")\n            sys.exit(1)\n\n        for color in args['color']:\n            colors.append(AVAILABLE_COLORS.get(color))\n\n    # Vertical graph for multiple series of same scale is not supported yet.\n    if args['vertical'] and len_categories > 1 and not args['different_scale']:\n        print(\">> Error: Vertical graph for multiple series of same \"\n              \"scale is not supported yet.\")\n        sys.exit(1)\n\n    # If user hasn't inserted colors, pick the first n colors\n    # from the dict (n = number of categories).\n    if args['stacked'] and not colors:\n        colors = [v for v in list(AVAILABLE_COLORS.values())[:len_categories]]\n\n    return colors", "response": "Check that all data were inserted correctly. Return the colors."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef print_categories(categories, colors):\n    for i in range(len(categories)):\n        if colors:\n            sys.stdout.write(f'\\033[{colors[i]}m') # Start to write colorized.\n\n        sys.stdout.write(TICK + ' ' + categories[i] + '  ')\n        if colors:\n            sys.stdout.write('\\033[0m') # Back to original.\n\n    print('\\n\\n')", "response": "Print a tick and the category s name for each category above\n       the graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_data(args):\n    filename = args['filename']\n    stdin = filename == '-'\n\n    if args['verbose']:\n        print(f'>> Reading data from {( \"stdin\" if stdin else filename )}')\n\n    print('')\n    if args['title']:\n        print('# ' + args['title'] + '\\n')\n\n    categories, labels, data, colors = ([] for i in range(4))\n\n    f = sys.stdin if stdin else open(filename, \"r\")\n    for line in f:\n        line = line.strip()\n        if line:\n            if not line.startswith('#'):\n                if line.find(DELIM) > 0:\n                    cols = line.split(DELIM)\n                else:\n                    cols = line.split()\n\n                # Line contains categories.\n                if line.startswith('@'):\n                    cols[0] = cols[0].replace(\"@ \", \"\")\n                    categories = cols\n\n                # Line contains label and values.\n                else:\n                    labels.append(cols[0].strip())\n                    data_points = []\n                    for i in range(1, len(cols)):\n                        data_points.append(float(cols[i].strip()))\n\n                    data.append(data_points)\n    f.close()\n\n    # Check that all data are valid. (i.e. There are no missing values.)\n    colors = check_data(labels, data, args)\n    if categories:\n        # Print categories' names above the graph.\n        print_categories(categories, colors)\n\n    return categories, labels, data, colors", "response": "Read data from a file or stdin and returns it."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calendar_heatmap(data, labels, args):\n    if args['color']:\n        colornum = AVAILABLE_COLORS.get(args['color'][0])\n    else:\n        colornum = AVAILABLE_COLORS.get('blue')\n\n    dt_dict = {}\n    for i in range(len(labels)):\n        dt_dict[labels[i]] = data[i][0]\n\n    # get max value\n    max_val = float(max(data)[0])\n\n    tick_1 = \"\u2591\"\n    tick_2 = \"\u2592\"\n    tick_3 = \"\u2593\"\n    tick_4 = \"\u2588\"\n\n    if args['custom_tick']:\n        tick_1 = tick_2 = tick_3 = tick_4 = args['custom_tick']\n\n    # check if start day set, otherwise use one year ago\n    if args['start_dt']:\n        start_dt = datetime.strptime(args['start_dt'], '%Y-%m-%d')\n    else:\n        start = datetime.now()\n        start_dt = datetime(year=start.year-1, month=start.month,\n                            day=start.day)\n\n    # modify start date to be a Monday, subtract weekday() from day\n    start_dt = start_dt - timedelta(start_dt.weekday())\n\n    # TODO: legend doesn't line up properly for all start dates/data\n    # top legend for months\n    sys.stdout.write(\"     \")\n    for month in range(13):\n        month_dt = datetime(year=start_dt.year, month=start_dt.month, day=1) +\\\n                   timedelta(days=month*31)\n        sys.stdout.write(month_dt.strftime(\"%b\") + \" \")\n        if args['custom_tick']: #assume custom tick is emoji which is one wider\n            sys.stdout.write(\" \")\n\n    sys.stdout.write('\\n')\n\n    for day in range(7):\n        sys.stdout.write(DAYS[day] + ': ')\n        for week in range(53):\n            day_ = start_dt + timedelta(days=day + week*7)\n            day_str = day_.strftime(\"%Y-%m-%d\")\n\n            if day_str in dt_dict:\n                if dt_dict[day_str] > max_val * 0.75:\n                    tick = tick_4\n                elif dt_dict[day_str] > max_val * 0.50:\n                    tick = tick_3\n                elif dt_dict[day_str] > max_val * 0.25:\n                    tick = tick_2\n                else:\n                    tick = tick_1\n            else:\n                tick = ' '\n\n            if colornum:\n                sys.stdout.write(f'\\033[{colornum}m')\n\n            sys.stdout.write(tick)\n            if colornum:\n                sys.stdout.write('\\033[0m')\n\n        sys.stdout.write('\\n')", "response": "Print a calendar heatmap."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fuse_get_context():\n    'Returns a (uid, gid, pid) tuple'\n\n    ctxp = _libfuse.fuse_get_context()\n    ctx = ctxp.contents\n    return ctx.uid, ctx.gid, ctx.pid", "response": "Returns a ( uid gid pid ) tuple"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef symlink(self, source, target):\n        'creates a symlink `target -> source` (e.g. ln -s source target)'\n\n        return self.operations('symlink', target.decode(self.encoding),\n                                          source.decode(self.encoding))", "response": "creates a symlink target -> source ( e. g. ln - s source target"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef link(self, source, target):\n        'creates a hard link `target -> source` (e.g. ln source target)'\n\n        return self.operations('link', target.decode(self.encoding),\n                                       source.decode(self.encoding))", "response": "creates a hard link target -> source ( e. g. ln source target"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getattr(self, path, fh=None):\n        '''\n        Returns a dictionary with keys identical to the stat C structure of\n        stat(2).\n\n        st_atime, st_mtime and st_ctime should be floats.\n\n        NOTE: There is an incompatibility between Linux and Mac OS X\n        concerning st_nlink of directories. Mac OS X counts all files inside\n        the directory, while Linux counts only the subdirectories.\n        '''\n\n        if path != '/':\n            raise FuseOSError(errno.ENOENT)\n        return dict(st_mode=(S_IFDIR | 0o755), st_nlink=2)", "response": "Returns a dictionary with keys identical to the stat C structure of\n        stat ( 2 )."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lookup(self, req, parent, name):\n        self.reply_err(req, errno.ENOENT)", "response": "Look up a directory entry by name and get its attributes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getattr(self, req, ino, fi):\n        if ino == 1:\n            attr = {'st_ino': 1, 'st_mode': S_IFDIR | 0o755, 'st_nlink': 2}\n            self.reply_attr(req, attr, 1.0)\n        else:\n            self.reply_err(req, errno.ENOENT)", "response": "Get file attributes of a specific object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setattr(self, req, ino, attr, to_set, fi):\n        self.reply_err(req, errno.EROFS)", "response": "Set the attribute of a file in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mknod(self, req, parent, name, mode, rdev):\n        self.reply_err(req, errno.EROFS)", "response": "Create a new node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mkdir(self, req, parent, name, mode):\n        self.reply_err(req, errno.EROFS)", "response": "Create a directory in the local system."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unlink(self, req, parent, name):\n        self.reply_err(req, errno.EROFS)", "response": "Remove a file from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rmdir(self, req, parent, name):\n        self.reply_err(req, errno.EROFS)", "response": "Remove a directory\n\n        Valid replies:\n            reply_err"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef symlink(self, req, link, parent, name):\n        self.reply_err(req, errno.EROFS)", "response": "Create a symbolic link in the local system."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrename a file in a node.", "response": "def rename(self, req, parent, name, newparent, newname):\n        \"\"\"Rename a file\n\n        Valid replies:\n            reply_err\n        \"\"\"\n        self.reply_err(req, errno.EROFS)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a hard link in a new entry in a new parent.", "response": "def link(self, req, ino, newparent, newname):\n        \"\"\"Create a hard link\n\n        Valid replies:\n            reply_entry\n            reply_err\n        \"\"\"\n        self.reply_err(req, errno.EROFS)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading data from a set of items.", "response": "def read(self, req, ino, size, off, fi):\n        \"\"\"Read data\n\n        Valid replies:\n            reply_buf\n            reply_err\n        \"\"\"\n        self.reply_err(req, errno.EIO)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites data from a set of items in the cache.", "response": "def write(self, req, ino, buf, off, fi):\n        \"\"\"Write data\n\n        Valid replies:\n            reply_write\n            reply_err\n        \"\"\"\n        self.reply_err(req, errno.EROFS)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads directory in the cache.", "response": "def readdir(self, req, ino, size, off, fi):\n        \"\"\"Read directory\n\n        Valid replies:\n            reply_readdir\n            reply_err\n        \"\"\"\n        if ino == 1:\n            attr = {'st_ino': 1, 'st_mode': S_IFDIR}\n            entries = [('.', attr), ('..', attr)]\n            self.reply_readdir(req, size, off, entries)\n        else:\n            self.reply_err(req, errno.ENOENT)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset an extended attribute of the current object.", "response": "def setxattr(self, req, ino, name, value, flags):\n        \"\"\" Set an extended attribute\n\n        Valid replies:\n\t        reply_err\n        \"\"\"\n        self.reply_err(req, errno.ENOSYS)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getxattr(self, req, ino, name, size):\n        self.reply_err(req, errno.ENOSYS)", "response": "Set an extended attribute on the entry."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef listxattr(self, req, ino, size):\n        self.reply_err(req, errno.ENOSYS)", "response": "List extended attribute names."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves an extended attribute from the cache.", "response": "def removexattr(self, req, ino, name):\n        \"\"\"Remove an extended attribute\n\n        Valid replies:\n\t        reply_err\n        \"\"\"\n        self.reply_err(req, errno.ENOSYS)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking file access permissions", "response": "def access(self, req, ino, mask):\n        \"\"\"Check file access permissions\n\n        Valid replies:\n\t        reply_err\n        \"\"\"\n        self.reply_err(req, errno.ENOSYS)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create(self, req, parent, name, mode, fi):\n        self.reply_err(req, errno.ENOSYS)", "response": "Create and open a file in the local cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _prepare_c_jk_m(readout_povm, pauli_basis, channel_ops):\n    channel_transfer_matrices = [pauli_basis.transfer_matrix(qt.to_super(ek)) for ek in channel_ops]\n\n    # This bit could be more efficient but does not run super long and is thus preserved for\n    #  readability.\n    pi_jr = csr_matrix(\n        [pauli_basis.project_op(n_j).toarray().ravel()\n         for n_j in readout_povm.ops])\n\n    # Dict used for constructing our sparse matrix, keys are tuples (row_index, col_index), values\n    # are the non-zero elements of the final matrix.\n    c_jk_m_elms = {}\n\n    # This explicitly exploits the sparsity of all operators involved\n    for k in range(len(channel_ops)):\n        pi_jr__rk_rm = (pi_jr * channel_transfer_matrices[k]).tocoo()\n        for (j, m, val) in ut.izip(pi_jr__rk_rm.row, pi_jr__rk_rm.col, pi_jr__rk_rm.data):\n            # The multi-index (j,k) is enumerated in column-major ordering (like Fortran arrays)\n            c_jk_m_elms[(j + k * readout_povm.pi_basis.dim, m)] = val.real\n\n    # create sparse matrix from COO-format (see scipy.sparse docs)\n    _keys, _values = ut.izip(*c_jk_m_elms.items())\n    _rows, _cols = ut.izip(*_keys)\n    c_jk_m = coo_matrix((list(_values), (list(_rows), list(_cols))),\n                        shape=(readout_povm.pi_basis.dim * len(channel_ops),\n                               pauli_basis.dim)).tocsr()\n    return c_jk_m", "response": "Prepare the coefficient matrix for state tomography."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nyielding tomographic sequences that prepare a state with Quil program state_prep and then append tomographic rotations on the specified qubits.", "response": "def state_tomography_programs(state_prep, qubits=None,\n                              rotation_generator=tomography.default_rotations):\n    \"\"\"\n    Yield tomographic sequences that prepare a state with Quil program `state_prep` and then append\n    tomographic rotations on the specified `qubits`. If `qubits is None`, it assumes all qubits in\n    the program should be tomographically rotated.\n\n    :param Program state_prep: The program to prepare the state to be tomographed.\n    :param list|NoneType qubits: A list of Qubits or Numbers, to perform the tomography on. If\n    `None`, performs it on all in state_prep.\n    :param generator rotation_generator: A generator that yields tomography rotations to perform.\n    :return: Program for state tomography.\n    :rtype: Program\n    \"\"\"\n    if qubits is None:\n        qubits = state_prep.get_qubits()\n    for tomography_program in rotation_generator(*qubits):\n        state_tomography_program = Program(Pragma(\"PRESERVE_BLOCK\"))\n        state_tomography_program.inst(state_prep)\n        state_tomography_program.inst(tomography_program)\n        state_tomography_program.inst(Pragma(\"END_PRESERVE_BLOCK\"))\n        yield state_tomography_program"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_state_tomography(preparation_program, nsamples, cxn, qubits=None, use_run=False):\n    return tomography._do_tomography(preparation_program, nsamples, cxn, qubits,\n                                     tomography.MAX_QUBITS_STATE_TOMO,\n                                     StateTomography, state_tomography_programs,\n                                     DEFAULT_STATE_TOMO_SETTINGS, use_run=use_run)", "response": "Method to perform both a QPU and QVM state tomography and return the resulting StateTomography object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nestimates a density matrix from a set of single shot histograms obtained by measuring bitstrings in the Z - eigenbasis after application of given channel operators.", "response": "def estimate_from_ssr(histograms, readout_povm, channel_ops, settings):\n        \"\"\"\n        Estimate a density matrix from single shot histograms obtained by measuring bitstrings in\n        the Z-eigenbasis after application of given channel operators.\n\n        :param numpy.ndarray histograms: The single shot histograms, `shape=(n_channels, dim)`.\n        :param DiagognalPOVM readout_povm: The POVM corresponding to the readout plus classifier.\n        :param list channel_ops: The tomography measurement channels as `qutip.Qobj`'s.\n        :param TomographySettings settings: The solver and estimation settings.\n        :return: The generated StateTomography object.\n        :rtype: StateTomography\n        \"\"\"\n        nqc = len(channel_ops[0].dims[0])\n        pauli_basis = grove.tomography.operator_utils.PAULI_BASIS ** nqc\n        pi_basis = readout_povm.pi_basis\n\n        if not histograms.shape[1] == pi_basis.dim:  # pragma no coverage\n            raise ValueError(\"Currently tomography is only implemented for two-level systems.\")\n\n        # prepare the log-likelihood function parameters, see documentation\n        n_kj = np.asarray(histograms)\n        c_jk_m = _prepare_c_jk_m(readout_povm, pauli_basis, channel_ops)\n        rho_m = cvxpy.Variable(pauli_basis.dim)\n\n        p_jk = c_jk_m * rho_m\n        obj = -n_kj.ravel() * cvxpy.log(p_jk)\n\n        p_jk_mat = cvxpy.reshape(p_jk, pi_basis.dim, len(channel_ops))  # cvxpy has col-major order\n\n        # Default constraints:\n        # MLE must describe valid probability distribution\n        # i.e., for each k, p_jk must sum to one and be element-wise non-negative:\n        # 1. \\sum_j p_jk == 1  for all k\n        # 2. p_jk >= 0         for all j, k\n        # where p_jk = \\sum_m c_jk_m rho_m\n        constraints = [\n            p_jk >= 0,\n            np.matrix(np.ones((1, pi_basis.dim))) * p_jk_mat == 1,\n        ]\n\n        rho_m_real_imag = sum((rm * o_ut.to_realimag(Pm)\n                               for (rm, Pm) in ut.izip(rho_m, pauli_basis.ops)), 0)\n\n        if POSITIVE in settings.constraints:\n            if tomography._SDP_SOLVER.is_functional():\n                constraints.append(rho_m_real_imag >> 0)\n            else:  # pragma no coverage\n                _log.warning(\"No convex solver capable of semi-definite problems installed.\\n\"\n                             \"Dropping the positivity constraint on the density matrix.\")\n\n        if UNIT_TRACE in settings.constraints:\n            # this assumes that the first element of the Pauli basis is always proportional to\n            # the identity\n            constraints.append(rho_m[0, 0] == 1. / pauli_basis.ops[0].tr().real)\n\n        prob = cvxpy.Problem(cvxpy.Minimize(obj), constraints)\n\n        _log.info(\"Starting convex solver\")\n        prob.solve(solver=tomography.SOLVER, **settings.solver_kwargs)\n        if prob.status != cvxpy.OPTIMAL:  # pragma no coverage\n            _log.warning(\"Problem did not converge to optimal solution. \"\n                         \"Solver settings: {}\".format(settings.solver_kwargs))\n\n        return StateTomography(np.array(rho_m.value).ravel(), pauli_basis, settings)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_state_histogram(self, ax):\n        title = \"Estimated state\"\n        nqc = int(round(np.log2(self.rho_est.data.shape[0])))\n        labels = ut.basis_labels(nqc)\n        return ut.state_histogram(self.rho_est, ax, title)", "response": "Visualize the complex matrix elements of the estimated state."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot(self):\n        width = 10\n        # The pleasing golden ratio.\n        height = width / 1.618\n        f = plt.figure(figsize=(width, height))\n        ax = f.add_subplot(111, projection=\"3d\")\n\n        self.plot_state_histogram(ax)\n        return f", "response": "Visualize the state.\n\n        :return: The generated figure.\n        :rtype: matplotlib.Figure"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_constant(self, qc: QuantumComputer, bitstring_map: Dict[str, str]) -> bool:\n        self._init_attr(bitstring_map)\n\n        prog = Program()\n        dj_ro = prog.declare('ro', 'BIT', len(self.computational_qubits))\n        prog += self.deutsch_jozsa_circuit\n        prog += [MEASURE(qubit, ro) for qubit, ro in zip(self.computational_qubits, dj_ro)]\n        executable = qc.compile(prog)\n        returned_bitstring = qc.run(executable)\n        # We are only running a single shot, so we are only interested in the first element.\n        bitstring = np.array(returned_bitstring, dtype=int)\n        constant = all([bit == 0 for bit in bitstring])\n        return constant", "response": "Computes whether the bitstring_map represents a constant function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _init_attr(self, bitstring_map: Dict[str, str]):\n        self.bit_map = bitstring_map\n        self.n_qubits = len(list(bitstring_map.keys())[0])\n        # We use one extra qubit for making the oracle,\n        # and one for storing the answer of the oracle.\n        self.n_ancillas = 2\n        self._qubits = list(range(self.n_qubits + self.n_ancillas))\n        self.computational_qubits = self._qubits[:self.n_qubits]\n        self.ancillas = self._qubits[self.n_qubits:]\n        self.unitary_matrix = self.unitary_function(bitstring_map)\n        self.deutsch_jozsa_circuit = self._construct_deutsch_jozsa_circuit()", "response": "Acts instead of __init__ method to instantiate the necessary Deutsch - Jozsa state."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _construct_deutsch_jozsa_circuit(self):\n        dj_prog = Program()\n\n        # Put the first ancilla qubit (query qubit) into minus state\n        dj_prog.inst(X(self.ancillas[0]), H(self.ancillas[0]))\n\n        # Apply Hadamard, Oracle, and Hadamard again\n        dj_prog.inst([H(qubit) for qubit in self.computational_qubits])\n\n        # Build the oracle\n        oracle_prog = Program()\n        oracle_prog.defgate(ORACLE_GATE_NAME, self.unitary_matrix)\n\n        scratch_bit = self.ancillas[1]\n        qubits_for_funct = [scratch_bit] + self.computational_qubits\n        oracle_prog.inst(tuple([ORACLE_GATE_NAME] + qubits_for_funct))\n        dj_prog += oracle_prog\n\n        # Here the oracle does not leave the computational qubits unchanged, so we use a CNOT to\n        # to move the result to the query qubit, and then we uncompute with the dagger.\n        dj_prog.inst(CNOT(self._qubits[0], self.ancillas[0]))\n        dj_prog += oracle_prog.dagger()\n        dj_prog.inst([H(qubit) for qubit in self.computational_qubits])\n        return dj_prog", "response": "Constructs the Deutsch - Jozsa circuit."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unitary_function(mappings: Dict[str, str]) -> np.ndarray:\n        num_qubits = int(np.log2(len(mappings)))\n        bitsum = sum([int(bit) for bit in mappings.values()])\n\n        # Only zeros were entered\n        if bitsum == 0:\n            return np.kron(SWAP_MATRIX, np.identity(2 ** (num_qubits - 1)))\n\n        # Half of the entries were 0, half 1\n        elif bitsum == 2 ** (num_qubits - 1):\n            unitary_funct = np.zeros(shape=(2 ** num_qubits, 2 ** num_qubits))\n            index_lists = [list(range(2 ** (num_qubits - 1))),\n                           list(range(2 ** (num_qubits - 1), 2 ** num_qubits))]\n            for j in range(2 ** num_qubits):\n                bitstring = np.binary_repr(j, num_qubits)\n                value = int(mappings[bitstring])\n                mappings.pop(bitstring)\n                i = index_lists[value].pop()\n                unitary_funct[i, j] = 1\n            return np.kron(np.identity(2), unitary_funct)\n\n        # Only ones were entered\n        elif bitsum == 2 ** num_qubits:\n            x_gate = np.array([[0, 1], [1, 0]])\n            return np.kron(SWAP_MATRIX, np.identity(2 ** (num_qubits - 1))).dot(\n                np.kron(x_gate, np.identity(2 ** num_qubits)))\n        else:\n            raise ValueError(\"f(x) must be constant or balanced\")", "response": "Creates a unitary transformation that maps each state to the values specified in mappings."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an oracle that selects the ith element of the computational basis of the given qubits.", "response": "def basis_selector_oracle(qubits: List[int], bitstring: str) -> Program:\n    \"\"\"\n    Defines an oracle that selects the ith element of the computational basis.\n\n    Flips the sign of the state :math:`\\\\vert x\\\\rangle>`\n    if and only if x==bitstring and does nothing otherwise.\n\n    :param qubits: The qubits the oracle is called on. The qubits are assumed to be ordered from\n     most significant qubit to least significant qubit.\n    :param bitstring: The desired bitstring, given as a string of ones and zeros. e.g. \"101\"\n    :return: A program representing this oracle.\n    \"\"\"\n    if len(qubits) != len(bitstring):\n        raise ValueError(\"The bitstring should be the same length as the number of qubits.\")\n    oracle_prog = Program()\n\n    # In the case of one qubit, we just want to flip the phase of state relative to the other.\n    if len(bitstring) == 1:\n        oracle_prog.inst(Z(qubits[0]))\n        return oracle_prog\n    else:\n        bitflip_prog = Program()\n        for i, qubit in enumerate(qubits):\n            if bitstring[i] == '0':\n                bitflip_prog.inst(X(qubit))\n        oracle_prog += bitflip_prog\n        controls = qubits[:-1]\n        target = qubits[-1]\n        operation = np.array([[1, 0], [0, -1]])\n        gate_name = 'Z'\n        n_qubit_controlled_z = (ControlledProgramBuilder()\n                                .with_controls(controls)\n                                .with_target(target)\n                                .with_operation(operation)\n                                .with_gate_name(gate_name)\n                                .build())\n        oracle_prog += n_qubit_controlled_z\n        oracle_prog += bitflip_prog\n    return oracle_prog"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_diagonal_povm(pi_basis, confusion_rate_matrix):\n\n    confusion_rate_matrix = np.asarray(confusion_rate_matrix)\n    if not np.allclose(confusion_rate_matrix.sum(axis=0), np.ones(confusion_rate_matrix.shape[1])):\n        raise CRMUnnormalizedError(\"Unnormalized confusion matrix:\\n{}\".format(\n            confusion_rate_matrix))\n    if not (confusion_rate_matrix >= 0).all() or not (confusion_rate_matrix <= 1).all():\n        raise CRMValueError(\"Confusion matrix must have values in [0, 1]:\"\n                            \"\\n{}\".format(confusion_rate_matrix))\n\n    ops = [sum((pi_j * pjk for (pi_j, pjk) in izip(pi_basis.ops, pjs)), 0)\n           for pjs in confusion_rate_matrix]\n    return DiagonalPOVM(pi_basis=pi_basis, confusion_rate_matrix=confusion_rate_matrix, ops=ops)", "response": "Creates a DiagonalPOVM from a given operator basis and confusion_rate_matrix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if matrix or operator is hermitian.", "response": "def is_hermitian(operator):\n    \"\"\"\n    Check if matrix or operator is hermitian.\n\n    :param (numpy.ndarray|qutip.Qobj) operator: The operator or matrix to be tested.\n    :return: True if the operator is hermitian.\n    :rtype: bool\n    \"\"\"\n    if isinstance(operator, qt.Qobj):\n        return (operator.dag() - operator).norm(FROBENIUS) / operator.norm(FROBENIUS) < EPS\n    if isinstance(operator, np.ndarray):\n        return np.linalg.norm(operator.T.conj() - operator) / np.linalg.norm(operator) < EPS\n    return spnorm(operator.H - operator) / spnorm(operator) < EPS"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_projector(operator):\n    # verify that P^dag=P and P^2-P=0 holds up to relative numerical accuracy EPS.\n    return (is_hermitian(operator) and (operator * operator - operator).norm(FROBENIUS)\n            / operator.norm(FROBENIUS) < EPS)", "response": "Check if the operator is a projector."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the Choi matrix for a quantum process from its Pauli Transfer Matrix.", "response": "def choi_matrix(pauli_tm, basis):\n    \"\"\"\n    Compute the Choi matrix for a quantum process from its Pauli Transfer Matrix.\n\n    This agrees with the definition in\n    `Chow et al. <https://doi.org/10.1103/PhysRevLett.109.060501>`_\n    except for a different overall normalization.\n    Our normalization agrees with that of qutip.\n\n    :param numpy.ndarray pauli_tm: The Pauli Transfer Matrix as 2d-array.\n    :param OperatorBasis basis:  The operator basis, typically products of normalized Paulis.\n    :return: The Choi matrix as qutip.Qobj.\n    :rtype: qutip.Qobj\n    \"\"\"\n\n    if not basis.is_orthonormal():  # pragma no coverage\n        raise ValueError(\"Need an orthonormal operator basis.\")\n    if not all((is_hermitian(op) for op in basis.ops)):  # pragma no coverage\n        raise ValueError(\"Need an operator basis of hermitian operators.\")\n\n    sbasis = basis.super_basis()\n    D = basis.dim\n    choi = sum((pauli_tm[jj, kk] * sbasis.ops[jj + kk * D] for jj in range(D) for kk in range(D)))\n    choi.superrep = CHOI\n    return choi"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a complex hermitian matrix to a real valued doubled up representation i. e. for", "response": "def to_realimag(z):\n    \"\"\"\n    Convert a complex hermitian matrix to a real valued doubled up representation, i.e., for\n    ``Z = Z_r + 1j * Z_i`` return ``R(Z)``::\n\n        R(Z) = [ Z_r   Z_i]\n               [-Z_i   Z_r]\n\n    A complex hermitian matrix ``Z`` with elementwise real and imaginary parts\n    ``Z = Z_r + 1j * Z_i`` can be\n    isomorphically represented in doubled up form as::\n\n        R(Z) = [ Z_r   Z_i]\n               [-Z_i   Z_r]\n\n        R(X)*R(Y) = [ (X_r*Y_r-X_i*Y_i)    (X_r*Y_i + X_i*Y_r)]\n                    [-(X_r*Y_i + X_i*Y_r)  (X_r*Y_r-X_i*Y_i)  ]\n\n                  = R(X*Y).\n\n    In particular, ``Z`` is complex positive (semi-)definite iff ``R(Z)`` is real positive\n    (semi-)definite.\n\n    :param (qutip.Qobj|scipy.sparse.base.spmatrix) z:  The operator representation matrix.\n    :returns: R(Z) the doubled up representation.\n    :rtype: scipy.sparse.csr_matrix\n    \"\"\"\n    if isinstance(z, qt.Qobj):\n        z = z.data\n    if not is_hermitian(z):  # pragma no coverage\n        raise ValueError(\"Need a hermitian matrix z\")\n    return spvstack([sphstack([z.real, z.imag]), sphstack([z.imag.T, z.real])]).tocsr().real"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef metric(self):\n        if self._metric is None:\n            _log.debug(\"Computing and caching operator basis metric\")\n            self._metric = np.matrix([[(j.dag() * k).tr() for k in self.ops] for j in self.ops])\n        return self._metric", "response": "Compute a matrix of Hilbert - Schmidt inner products for the basis operators and update self. _metric and return the value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine if the basis vectors of this OperatorBasis are orthonormal.", "response": "def is_orthonormal(self):\n        \"\"\"\n        Compute a matrix of Hilbert-Schmidt inner products for the basis operators, and see if they\n        are orthonormal. If they are return True, else, False.\n\n        :return: True if the basis vectors represented by this OperatorBasis are orthonormal, False\n         otherwise.\n        :rtype: bool\n        \"\"\"\n        if self._is_orthonormal is None:\n            _log.debug(\"Testing and caching if operator basis is orthonormal\")\n            self._is_orthonormal = np.allclose(self.metric(), np.eye(self.dim))\n        return self._is_orthonormal"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if all basis operators are hermitian.", "response": "def all_hermitian(self):\n        \"\"\"\n        Check if all basis operators are hermitian.\n        \"\"\"\n        if self._all_hermitian is None:\n            _log.debug(\"Testing and caching if all basis operator are hermitian\")\n            self._all_hermitian = all((is_hermitian(op) for op in self.ops))\n        return self._all_hermitian"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the tensor product with another basis.", "response": "def product(self, *bases):\n        \"\"\"\n        Compute the tensor product with another basis.\n\n        :param bases: One or more additional bases to form the product with.\n        :return (OperatorBasis): The tensor product basis as an OperatorBasis object.\n        \"\"\"\n        if len(bases) > 1:\n            basis_rest = bases[0].product(*bases[1:])\n        else:\n            assert len(bases) == 1\n            basis_rest = bases[0]\n\n        labels_ops = [(b1l + b2l, qt.tensor(b1, b2)) for (b1l, b1), (b2l, b2) in\n                      itertools.product(self, basis_rest)]\n\n        return OperatorBasis(labels_ops)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate the superoperator basis in which the Choi matrix can be represented.", "response": "def super_basis(self):\n        \"\"\"\n        Generate the superoperator basis in which the Choi matrix can be represented.\n\n        The follows the definition in\n        `Chow et al. <https://doi.org/10.1103/PhysRevLett.109.060501>`_\n\n\n        :return (OperatorBasis): The super basis as an OperatorBasis object.\n        \"\"\"\n\n        labels_ops = [(bnl + \"^T (x) \" + bml, qt.sprepost(bm, bn)) for (bnl, bn), (bml, bm) in\n                      itertools.product(self, self)]\n        return OperatorBasis(labels_ops)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef project_op(self, op):\n        if not self.is_orthonormal():  # pragma no coverage\n            raise ValueError(\"project_op only implemented for orthonormal operator bases\")\n        return self.basis_transform.H * qt.operator_to_vector(op).data", "response": "Project an operator onto the basis set."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the transfer matrix for the current class.", "response": "def transfer_matrix(self, superoperator):\n        \"\"\"\n        Compute the transfer matrix :math:`R_{jk} = \\tr[P_j sop(P_k)]`.\n\n        :param qutip.Qobj superoperator: The superoperator to transform.\n        :return: The transfer matrix in sparse form.\n        :rtype: scipy.sparse.csr_matrix\n        \"\"\"\n        if not self.is_orthonormal():  # pragma no coverage\n            raise ValueError(\"transfer_matrix() only implemented for orthonormal operator bases.\")\n        return self.basis_transform.H * superoperator.data * self.basis_transform"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreconstructs a super operator from a transfer matrix representation. This inverts self. transfer_matrix(...).", "response": "def super_from_tm(self, transfer_matrix):\n        \"\"\"\n        Reconstruct a super operator from a transfer matrix representation.\n        This inverts `self.transfer_matrix(...)`.\n\n        :param (numpy.ndarray) transfer_matrix: A process in transfer matrix form.\n        :return: A qutip super operator.\n        :rtype: qutip.Qobj.\n        \"\"\"\n        if not self.is_orthonormal():  # pragma no coverage\n            raise ValueError(\"super_from_tm() only implemented for orthonormal operator bases\")\n\n        data = self.basis_transform * transfer_matrix * self.basis_transform.H\n        sop = qt.Qobj(data, dims=[self.ops[0].dims, self.ops[0].dims])\n        sop.superrep = \"super\"\n        return sop"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_unitary(matrix: np.ndarray) -> bool:\n    rows, cols = matrix.shape\n    if rows != cols:\n        return False\n    return np.allclose(np.eye(rows), matrix.dot(matrix.T.conj()))", "response": "A helper function that checks if a matrix is unitary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bitwise_xor(bs0: str, bs1: str) -> str:\n    if len(bs0) != len(bs1):\n        raise ValueError(\"Bit strings are not of equal length\")\n    n_bits = len(bs0)\n    return PADDED_BINARY_BIT_STRING.format(xor(int(bs0, 2), int(bs1, 2)), n_bits)", "response": "A helper to calculate the bitwise XOR between two bit strings"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef isclose(a, b, rel_tol=1e-10, abs_tol=0.0):\n    return abs(a-b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)", "response": "Compares two parameter values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating number partition driver and cost functions for a binary partition.", "response": "def numpart_qaoa(asset_list, A=1.0, minimizer_kwargs=None, steps=1):\n    \"\"\"\n    generate number partition driver and cost functions\n\n    :param asset_list: list to binary partition\n    :param A: (float) optional constant for level separation. Default=1.\n    :param minimizer_kwargs: Arguments for the QAOA minimizer\n    :param steps: (int) number of steps approximating the solution.\n    \"\"\"\n    cost_operators = []\n    ref_operators = []\n    for ii in range(len(asset_list)):\n        for jj in range(ii + 1, len(asset_list)):\n            cost_operators.append(PauliSum([PauliTerm(\"Z\", ii, 2*asset_list[ii]) *\n                                            PauliTerm(\"Z\", jj, A*asset_list[jj])]))\n        ref_operators.append(PauliSum([PauliTerm(\"X\", ii, -1.0)]))\n\n    cost_operators.append(PauliSum([PauliTerm(\"I\", 0, len(asset_list))]))\n\n    if minimizer_kwargs is None:\n        minimizer_kwargs = {'method': 'Nelder-Mead',\n                            'options': {'ftol': 1.0e-2,\n                                        'xtol': 1.0e-2,\n                                        'disp': True}}\n\n    qc = get_qc(f\"{len(asset_list)}q-qvm\")\n\n    qaoa_inst = QAOA(qc, list(range(len(asset_list))), steps=steps, cost_ham=cost_operators,\n                     ref_ham=ref_operators, store_basis=True,\n                     minimizer=minimize, minimizer_kwargs=minimizer_kwargs,\n                     vqe_options={'disp': print})\n\n    return qaoa_inst"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gradient_program(f_h: float, precision: int) -> Program:\n\n    # encode oracle values into phase\n    phase_factor = np.exp(1.0j * 2 * np.pi * abs(f_h))\n    U = np.array([[phase_factor, 0],\n                  [0, phase_factor]])\n    p_gradient = phase_estimation(U, precision)\n\n    return p_gradient", "response": "Compute the gradient program for a given oracle output at perturbation h."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nestimating the gradient using function evaluation at perturbation h.", "response": "def estimate_gradient(f_h: float, precision: int,\n                      gradient_max: int = 1,\n                      n_measurements: int = 50,\n                      qc: QuantumComputer = None) -> float:\n    \"\"\"\n    Estimate the gradient using function evaluation at perturbation, h.\n\n    :param f_h: Oracle output at perturbation h.\n    :param precision: Bit precision of gradient.\n    :param gradient_max: OOM estimate of largest gradient value.\n    :param n_measurements: Number of times to measure system.\n    :param qc: The QuantumComputer object.\n    :return: Decimal estimate of gradient.\n    \"\"\"\n\n    # scale f_h by range of values gradient can take on\n    f_h *= 1. / gradient_max\n\n    # generate gradient program\n    perturbation_sign = np.sign(f_h)\n    p_gradient = gradient_program(f_h, precision)\n\n    # run gradient program\n    if qc is None:\n        qc = get_qc(f\"{len(p_gradient.get_qubits())}q-qvm\")\n\n    p_gradient.wrap_in_numshots_loop(n_measurements)\n    executable = qc.compiler.native_quil_to_executable(p_gradient)\n    measurements = qc.run(executable)\n\n    # summarize measurements\n    bf_estimate = perturbation_sign * measurements_to_bf(measurements)\n    bf_explicit = '{0:.16f}'.format(bf_estimate)\n    deci_estimate = binary_float_to_decimal_float(bf_explicit)\n    \n    # rescale gradient\n    deci_estimate *= gradient_max\n\n    return deci_estimate"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprepare the coefficient matrix for binomial state tomography.", "response": "def _prepare_b_jkl_mn(readout_povm, pauli_basis, pre_channel_ops, post_channel_ops, rho0):\n    \"\"\"\n    Prepare the coefficient matrix for process tomography. This function uses sparse matrices\n    for much greater efficiency. The coefficient matrix is defined as:\n\n    .. math::\n\n            B_{(jkl)(mn)}=\\sum_{r,q}\\pi_{jr}(\\mathcal{R}_{k})_{rm} (\\mathcal{R}_{l})_{nq} (\\rho_0)_q\n\n    where :math:`\\mathcal{R}_{k}` is the transfer matrix of the quantum map corresponding to the\n    k-th pre-measurement channel, while :math:`\\mathcal{R}_{l}` is the transfer matrix of the l-th\n    state preparation process. We also require the overlap\n    between the (generalized) Pauli basis ops and the projection operators\n    :math:`\\pi_{jl}:=\\sbraket{\\Pi_j}{P_l} = \\tr{\\Pi_j P_l}`.\n\n    See the grove documentation on tomography for detailed information.\n\n    :param DiagonalPOVM readout_povm: The POVM corresponding to the readout plus classifier.\n    :param OperatorBasis pauli_basis: The (generalized) Pauli basis employed in the estimation.\n    :param list pre_channel_ops: The state preparation channel operators as `qutip.Qobj`\n    :param list post_channel_ops: The pre-measurement (post circuit) channel operators as `qutip.Qobj`\n    :param qutip.Qobj rho0: The initial state as a density matrix.\n    :return: The coefficient matrix necessary to set up the binomial state tomography problem.\n    :rtype: scipy.sparse.csr_matrix\n    \"\"\"\n    c_jk_m = state_tomography._prepare_c_jk_m(readout_povm, pauli_basis, post_channel_ops)\n    pre_channel_transfer_matrices = [pauli_basis.transfer_matrix(qt.to_super(ek))\n                                     for ek in pre_channel_ops]\n    rho0_q = pauli_basis.project_op(rho0)\n\n    # These next lines hide some very serious (sparse-)matrix index magic,\n    # basically we exploit the same index math as in `qutip.sprepost()`\n    # i.e., if a matrix X is linearly mapped `X -> A.dot(X).dot(B)`\n    # then this can be rewritten as\n    #           `np.kron(B.T, A).dot(X.T.ravel()).reshape((B.shape[1], A.shape[0])).T`\n    # The extra matrix transpose operations are necessary because numpy by default\n    # uses row-major storage, whereas these operations are conventionally defined for column-major\n    # storage.\n    d_ln = spvstack([(rlnq * rho0_q).T for rlnq in pre_channel_transfer_matrices]).tocoo()\n    b_jkl_mn = spkron(d_ln, c_jk_m).real\n    return b_jkl_mn"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_tomography_programs(process, qubits=None,\n                                pre_rotation_generator=tomography.default_rotations,\n                                post_rotation_generator=tomography.default_rotations):\n    \"\"\"\n    Generator that yields tomographic sequences that wrap a process encoded by a QUIL program `proc`\n    in tomographic rotations on the specified `qubits`.\n\n    If `qubits is None`, it assumes all qubits in the program should be\n    tomographically rotated.\n\n    :param Program process: A Quil program\n    :param list|NoneType qubits: The specific qubits for which to generate the tomographic sequences\n    :param pre_rotation_generator: A generator that yields tomographic pre-rotations to perform.\n    :param post_rotation_generator: A generator that yields tomographic post-rotations to perform.\n    :return: Program for process tomography.\n    :rtype: Program\n    \"\"\"\n    if qubits is None:\n        qubits = process.get_qubits()\n    for tomographic_pre_rotation in pre_rotation_generator(*qubits):\n        for tomography_post_rotation in post_rotation_generator(*qubits):\n            process_tomography_program = Program(Pragma(\"PRESERVE_BLOCK\"))\n            process_tomography_program.inst(tomographic_pre_rotation)\n            process_tomography_program.inst(process)\n            process_tomography_program.inst(tomography_post_rotation)\n            process_tomography_program.inst(Pragma(\"END_PRESERVE_BLOCK\"))\n\n            yield process_tomography_program", "response": "Generates tomographic sequences that wrap a process encoded by a QUIL program proc."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_process_tomography(process, nsamples, cxn, qubits=None, use_run=False):\n    return tomography._do_tomography(process, nsamples, cxn, qubits,\n                                     tomography.MAX_QUBITS_PROCESS_TOMO,\n                                     ProcessTomography, process_tomography_programs,\n                                     DEFAULT_PROCESS_TOMO_SETTINGS, use_run=use_run)", "response": "Method to perform a process tomography analysis on a sequence of QVM connections."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nestimates a quantum process from a set of single shot histograms obtained by preparing specific input states and measuring bitstrings in the Z - eigenbasis after application of given channel operators.", "response": "def estimate_from_ssr(histograms, readout_povm, pre_channel_ops, post_channel_ops, settings):\n        \"\"\"\n        Estimate a quantum process from single shot histograms obtained by preparing specific input\n        states and measuring bitstrings in the Z-eigenbasis after application of given channel\n        operators.\n\n        :param numpy.ndarray histograms: The single shot histograms.\n        :param DiagonalPOVM readout_povm: The POVM corresponding to readout plus classifier.\n        :param list pre_channel_ops: The input state preparation channels as `qutip.Qobj`'s.\n        :param list post_channel_ops: The tomography post-process channels as `qutip.Qobj`'s.\n        :param TomographySettings settings: The solver and estimation settings.\n        :return: The ProcessTomography object and results from the the given data.\n        :rtype: ProcessTomography\n        \"\"\"\n        nqc = len(pre_channel_ops[0].dims[0])\n        pauli_basis = grove.tomography.operator_utils.PAULI_BASIS ** nqc\n        pi_basis = readout_povm.pi_basis\n\n        if not histograms.shape[-1] == pi_basis.dim:  # pragma no coverage\n            raise ValueError(\"Currently tomography is only implemented for two-level systems\")\n\n        rho0 = grove.tomography.operator_utils.n_qubit_ground_state(nqc)\n\n        n_lkj = np.asarray(histograms)\n\n        b_jkl_mn = _prepare_b_jkl_mn(readout_povm, pauli_basis, pre_channel_ops,\n                                     post_channel_ops, rho0)\n\n        r_mn = cvxpy.Variable(pauli_basis.dim ** 2)\n        p_jkl = b_jkl_mn.real * r_mn\n        obj = -np.matrix(n_lkj.ravel()) * cvxpy.log(p_jkl)\n\n        # cvxpy has col-major order and we collapse k and l onto single dimension\n        p_jkl_mat = cvxpy.reshape(p_jkl, pi_basis.dim, len(pre_channel_ops) * len(post_channel_ops))\n\n        # Default constraints:\n        # MLE must describe valid probability distribution\n        # i.e., for each k and l, p_jkl must sum to one and be element-wise non-negative:\n        # 1. \\sum_j p_jkl == 1  for all k, l\n        # 2. p_jkl >= 0         for all j, k, l\n        # where p_jkl = \\sum_m b_jkl_mn r_mn\n        constraints = [p_jkl >= 0,\n                       np.matrix(np.ones((1, pi_basis.dim))) * p_jkl_mat == 1]\n        r_mn_mat = cvxpy.reshape(r_mn, pauli_basis.dim, pauli_basis.dim)\n        super_pauli_basis = pauli_basis.super_basis()\n        choi_real_imag = sum((r_mn_mat[jj, kk] * o_ut.to_realimag(\n            super_pauli_basis.ops[jj + kk * pauli_basis.dim])\n                              for jj in range(pauli_basis.dim)\n                              for kk in range(pauli_basis.dim)), 0)\n\n        if COMPLETELY_POSITIVE in settings.constraints:\n            if tomography._SDP_SOLVER.is_functional():\n                constraints.append(choi_real_imag >> 0)\n            else:  # pragma no coverage\n                _log.warning(\"No convex solver capable of semi-definite problems installed.\\n\"\n                             \"Dropping the complete positivity constraint on the process\")\n\n        if TRACE_PRESERVING in settings.constraints:\n            constraints.append(r_mn_mat[0, 0] == 1)\n            constraints.append(r_mn_mat[0, 1:] == 0)\n\n        prob = cvxpy.Problem(cvxpy.Minimize(obj), constraints)\n        _ = prob.solve(solver=tomography.SOLVER, **settings.solver_kwargs)\n\n        r_mn_est = r_mn.value.reshape((pauli_basis.dim, pauli_basis.dim)).transpose()\n        return ProcessTomography(r_mn_est, pauli_basis, settings)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the quantum process fidelity of the estimated state with respect to a unitary .", "response": "def process_fidelity(self, reference_unitary):\n        \"\"\"\n        Compute the quantum process fidelity of the estimated state with respect to a unitary\n        process. For non-sparse reference_unitary, this implementation this will be expensive in\n        higher dimensions.\n\n        :param (qutip.Qobj|matrix-like) reference_unitary: A unitary operator that induces a process\n         as ``rho -> other*rho*other.dag()``, can also be a superoperator or Pauli-transfer matrix.\n        :return: The process fidelity, a real number between 0 and 1.\n        :rtype: float\n        \"\"\"\n        if isinstance(reference_unitary, qt.Qobj):\n            if not reference_unitary.issuper or reference_unitary.superrep != \"super\":\n                sother = qt.to_super(reference_unitary)\n            else:\n                sother = reference_unitary\n            tm_other = self.pauli_basis.transfer_matrix(sother)\n        else:\n            tm_other = csr_matrix(reference_unitary)\n        dimension = self.pauli_basis.ops[0].shape[0]\n        return np.trace(tm_other.T * self.r_est).real / dimension ** 2"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the average gate fidelity of the estimated process with respect to a unitary process.", "response": "def avg_gate_fidelity(self, reference_unitary):\n        \"\"\"\n        Compute the average gate fidelity of the estimated process with respect to a unitary\n         process. See `Chow et al., 2012, <https://doi.org/10.1103/PhysRevLett.109.060501>`_\n\n        :param (qutip.Qobj|matrix-like) reference_unitary: A unitary operator that induces a process\n         as `rho -> other*rho*other.dag()`, alternatively a superoperator or Pauli-transfer matrix.\n        :return: The average gate fidelity, a real number between 1/(d+1) and 1, where d is the\n        Hilbert space dimension.\n        :rtype: float\n        \"\"\"\n        process_fidelity = self.process_fidelity(reference_unitary)\n        dimension = self.pauli_basis.ops[0].shape[0]\n        return (dimension * process_fidelity + 1.0) / (dimension + 1.0)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the Kraus operator representation of the estimated process.", "response": "def to_kraus(self):\n        \"\"\"\n        Compute the Kraus operator representation of the estimated process.\n\n        :return: The process as a list of Kraus operators.\n        :rytpe: List[np.array]\n        \"\"\"\n        return [k.data.toarray() for k in qt.to_kraus(self.sop)]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot the elements of the Pauli transfer matrix.", "response": "def plot_pauli_transfer_matrix(self, ax):\n        \"\"\"\n        Plot the elements of the Pauli transfer matrix.\n\n        :param matplotlib.Axes ax: A matplotlib Axes object to plot into.\n        \"\"\"\n        title = \"Estimated process\"\n        ut.plot_pauli_transfer_matrix(self.r_est, ax, self.pauli_basis.labels, title)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot(self):\n        fig, (ax1) = plt.subplots(1, 1, figsize=(10, 8))\n        self.plot_pauli_transfer_matrix(ax1)\n        return fig", "response": "Visualize the process.\n\n        :return: The generated figure.\n        :rtype: matplotlib.Figure"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a circuit to do bit reversal with.", "response": "def bit_reversal(qubits: List[int]) -> Program:\n    \"\"\"\n    Generate a circuit to do bit reversal.\n\n    :param qubits: Qubits to do bit reversal with.\n    :return: A program to do bit reversal.\n    \"\"\"\n    p = Program()\n    n = len(qubits)\n    for i in range(int(n / 2)):\n        p.inst(SWAP(qubits[i], qubits[-i - 1]))\n    return p"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate the core QFT of the given list of qubits.", "response": "def _core_qft(qubits: List[int], coeff: int) -> Program:\n    \"\"\"\n    Generates the core program to perform the quantum Fourier transform\n    \n    :param qubits: A list of qubit indexes.\n    :param coeff: A modifier for the angle used in rotations (-1 for inverse QFT, 1 for QFT)\n    :return: A Quil program to compute the core (inverse) QFT of the qubits.\n    \"\"\"\n    \n    q = qubits[0]\n    qs = qubits[1:]\n    if 1 == len(qubits):\n        return [H(q)]\n    else:\n        n = 1 + len(qs)\n        cR = []\n        for idx, i in enumerate(range(n - 1, 0, -1)):\n            q_idx = qs[idx]\n            angle = math.pi / 2 ** (n - i)\n            cR.append(CPHASE(coeff * angle, q, q_idx))\n        return _core_qft(qs, coeff) + list(reversed(cR)) + [H(q)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a Quil program to compute the quantum Fourier transform on a set of qubits.", "response": "def qft(qubits: List[int]) -> Program:\n    \"\"\"\n    Generate a program to compute the quantum Fourier transform on a set of qubits.\n\n    :param qubits: A list of qubit indexes.\n    :return: A Quil program to compute the Fourier transform of the qubits.\n    \"\"\"\n\n    p = Program().inst(_core_qft(qubits, 1))\n    return p + bit_reversal(qubits)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a Quil program to compute the inverse quantum Fourier transform on a set of qubits.", "response": "def inverse_qft(qubits: List[int]) -> Program:\n    \"\"\"\n    Generate a program to compute the inverse quantum Fourier transform on a set of qubits.\n\n    :param qubits: A list of qubit indexes.\n    :return: A Quil program to compute the inverse Fourier transform of the qubits.\n    \"\"\"\n    \n    qft_result = Program().inst(_core_qft(qubits, -1))\n    qft_result += bit_reversal(qubits)\n    inverse_qft = Program()\n    while len(qft_result) > 0:\n        new_inst = qft_result.pop()\n        inverse_qft.inst(new_inst)\n    return inverse_qft"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unitary_operator(state_vector):\n\n    if not np.allclose([np.linalg.norm(state_vector)], [1]):\n        raise ValueError(\"Vector must be normalized\")\n\n    if 2 ** get_bits_needed(len(state_vector)) != len(state_vector):\n        raise ValueError(\"Vector length must be a power of two and at least two\")\n\n    mat = np.identity(len(state_vector), dtype=complex)\n    for i in range(len(state_vector)):\n        mat[i, 0] = state_vector[i]\n    U = np.linalg.qr(mat)[0]\n\n    # make sure U|0> = |v>\n    zero_state = np.zeros(len(U))\n    zero_state[0] = 1\n    if np.allclose(U.dot(zero_state), state_vector):\n        return U\n    else:\n        # adjust phase if needed\n        return -1 * U", "response": "Returns a 2d array that can encode an arbitrary normalized vector into a 2d array that can be used as a unitary operator."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fix_norm_and_length(vector):\n    # normalize\n    norm_vector = vector / np.linalg.norm(vector)\n\n    # pad with zeros\n    num_bits = get_bits_needed(len(vector))\n    state_vector = np.zeros(2 ** num_bits, dtype=complex)\n    for i in range(len(vector)):\n        state_vector[i] = norm_vector[i]\n\n    return state_vector", "response": "Create a normalized and zero padded version of vector."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_bits_needed(n):\n\n    assert n > 0, \"Inout should be positive\"\n\n    num_bits = int(np.ceil(np.log2(n)))\n    return max(1, num_bits)", "response": "Calculates the smallest positive integer m for which\n    \\ geq n."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_bv_bitmap(dot_product_vector: str, dot_product_bias: str) -> Dict[str, str]:\n    n_bits = len(dot_product_vector)\n    bit_map = {}\n    for bit_val in range(2 ** n_bits):\n        bit_map[np.binary_repr(bit_val, width=n_bits)] = str(\n            (int(utils.bitwise_dot_product(np.binary_repr(bit_val, width=n_bits),\n                                           dot_product_vector))\n             + int(dot_product_bias, 2)) % 2\n        )\n\n    return bit_map", "response": "This function creates a bitmap from bitstring to function value for a boolean formula."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _compute_unitary_oracle_matrix(bitstring_map: Dict[str, str]) -> Tuple[np.ndarray,\n                                                                               Dict[str, str]]:\n        \"\"\"\n        Computes the unitary matrix that encodes the oracle function  used in the Bernstein-Vazirani\n        algorithm. It generates a dense matrix for a function :math:`f`\n\n        .. math::\n\n           f:\\\\{0,1\\\\}^n\\\\rightarrow \\\\{0,1\\\\}\n\n           \\\\mathbf{x}\\\\rightarrow \\\\mathbf{a}\\\\cdot\\\\mathbf{x}+b\\\\pmod{2}\n\n           (\\\\mathbf{a}\\\\in\\\\{0,1\\\\}^n, b\\\\in\\\\{0,1\\\\})\n\n        where :math:`(\\\\cdot)` is the bitwise dot product, that represents the transition-matrix\n        elements of the corresponding qubit and ancilla subsystems.\n\n        :param Dict[String, String] bitstring_map: truth-table of the input bitstring map in\n            dictionary format\n        :return: a dense matrix containing the permutation of the bit strings and a dictionary\n            containing the indices of the non-zero elements of the computed permutation matrix as\n            key-value-pairs\n        :rtype: Tuple[2darray, Dict[String, String]]\n        \"\"\"\n        n_bits = len(list(bitstring_map.keys())[0])\n        n_ancillas = 1\n\n        # We instantiate an empty matrix of size n_bits + 1 to encode the mapping from n qubits\n        # to one ancillas, which explains the additional +1 overhead.\n        # To construct the matrix we go through all possible state transitions and pad the index\n        # according to all possible states the ancilla-subsystem could be in\n        ufunc = np.zeros(shape=(2 ** (n_bits + 1), 2 ** (n_bits + 1)))\n        index_mapping_dct = defaultdict(dict)\n        for b in range(2**n_ancillas):\n            # padding according to ancilla state\n            pad_str = np.binary_repr(b, width=1)\n            for k, v in bitstring_map.items():\n                # add mapping from initial state to the state in the ancilla system.\n                # pad_str corresponds to the initial state of the ancilla system.\n                index_mapping_dct[pad_str + k] = utils.bitwise_xor(pad_str, v) + k\n                # calculate matrix indices that correspond to the transition-matrix-element\n                # of the oracle unitary\n                i, j = int(pad_str+k, 2), int(utils.bitwise_xor(pad_str, v) + k, 2)\n                ufunc[i, j] = 1\n        return ufunc, index_mapping_dct", "response": "Computes the unitary matrix that encodes the oracle function used in the Bernstein - Vazirani ArcGIS - A - Z implementation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _create_bv_circuit(self, bit_map: Dict[str, str]) -> Program:\n        unitary, _ = self._compute_unitary_oracle_matrix(bit_map)\n        full_bv_circuit = Program()\n\n        full_bv_circuit.defgate(\"BV-ORACLE\", unitary)\n\n        # Put ancilla bit into minus state\n        full_bv_circuit.inst(X(self.ancilla), H(self.ancilla))\n\n        full_bv_circuit.inst([H(i) for i in self.computational_qubits])\n        full_bv_circuit.inst(\n            tuple([\"BV-ORACLE\"] + sorted(self.computational_qubits + [self.ancilla], reverse=True)))\n        full_bv_circuit.inst([H(i) for i in self.computational_qubits])\n        return full_bv_circuit", "response": "Create a full BV - ORACLE circuit that can be used to generate the one - qubit bit vectors for the given bit_map."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self, qc: QuantumComputer, bitstring_map: Dict[str, str]) -> 'BernsteinVazirani':\n        # initialize all attributes\n        self.input_bitmap = bitstring_map\n        self.n_qubits = len(list(bitstring_map.keys())[0])\n        self.computational_qubits = list(range(self.n_qubits))\n        self.ancilla = self.n_qubits  # is the highest index now.\n\n        # construct BV circuit\n        self.bv_circuit = self._create_bv_circuit(bitstring_map)\n\n        # find vector by running the full bv circuit\n        full_circuit = Program()\n        full_ro = full_circuit.declare('ro', 'BIT', len(self.computational_qubits) + 1)\n        full_circuit += self.bv_circuit\n        full_circuit += [MEASURE(qubit, ro) for qubit, ro in zip(self.computational_qubits,\n                                                                 full_ro)]\n        full_executable = qc.compile(full_circuit)\n        full_results = qc.run(full_executable)\n        bv_vector = full_results[0][::-1]\n\n        # To get the bias term we skip the Walsh-Hadamard transform\n        ancilla_circuit = Program()\n        ancilla_ro = ancilla_circuit.declare('ro', 'BIT', len(self.computational_qubits) + 1)\n        ancilla_circuit += self.bv_circuit\n        ancilla_circuit += [MEASURE(self.ancilla, ancilla_ro[self.ancilla])]\n        ancilla_executable = qc.compile(ancilla_circuit)\n        ancilla_results = qc.run(ancilla_executable)\n        bv_bias = ancilla_results[0][0]\n\n        self.solution = ''.join([str(b) for b in bv_vector]), str(bv_bias)\n        return self", "response": "This method runs the Bernstein - Vazirani algorithm."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_solution(self) -> Tuple[str, str]:\n        if self.solution is None:\n            raise AssertionError(\"You need to `run` this algorithm first\")\n        return self.solution", "response": "Returns the solution of the BV algorithm\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if the found solution correctly reproduces the input bitstring.", "response": "def check_solution(self) -> bool:\n        \"\"\"\n        Checks if the the found solution correctly reproduces the input.\n\n        :return: True if solution correctly reproduces input bitstring map\n        :rtype: Bool\n        \"\"\"\n        if self.solution is None:\n            raise AssertionError(\"You need to `run` this algorithm first\")\n        assert_map = create_bv_bitmap(*self.solution)\n        return all([assert_map[k] == v for k, v in self.input_bitmap.items()])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntest if two pauli terms are commuting with a diagonal basis.", "response": "def diagonal_basis_commutes(pauli_a, pauli_b):\n    \"\"\"\n    Test if `pauli_a` and `pauli_b` share a diagonal basis\n\n    Example:\n\n        Check if [A, B] with the constraint that A & B must share a one-qubit\n        diagonalizing basis. If the inputs were [sZ(0), sZ(0) * sZ(1)] then this\n        function would return True.  If the inputs were [sX(5), sZ(4)] this\n        function would return True.  If the inputs were [sX(0), sY(0) * sZ(2)]\n        this function would return False.\n\n    :param pauli_a: Pauli term to check commutation against `pauli_b`\n    :param pauli_b: Pauli term to check commutation against `pauli_a`\n    :return: Boolean of commutation result\n    :rtype: Bool\n    \"\"\"\n    overlapping_active_qubits = set(pauli_a.get_qubits()) & set(pauli_b.get_qubits())\n    for qubit_index in overlapping_active_qubits:\n        if (pauli_a[qubit_index] != 'I' and pauli_b[qubit_index] != 'I' and\n           pauli_a[qubit_index] != pauli_b[qubit_index]):\n            return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_diagonalizing_basis(list_of_pauli_terms):\n    qubit_ops = set(reduce(lambda x, y: x + y,\n                       [list(term._ops.items()) for term in list_of_pauli_terms]))\n    qubit_ops = sorted(list(qubit_ops), key=lambda x: x[0])\n\n    return PauliTerm.from_list(list(map(lambda x: tuple(reversed(x)), qubit_ops)))", "response": "Find the Pauli Term with the most non - identity terms\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the max overlap of a pauli term ID with keys of diagonal_sets", "response": "def _max_key_overlap(pauli_term, diagonal_sets):\n    \"\"\"\n    Calculate the max overlap of a pauli term ID with keys of diagonal_sets\n\n    Returns a different key if we find any collisions.  If no collisions is\n    found then the pauli term is added and the key is updated so it has the\n    largest weight.\n\n    :param pauli_term:\n    :param diagonal_sets:\n    :return: dictionary where key value pair is tuple indicating diagonal basis\n             and list of PauliTerms that share that basis\n    :rtype: dict\n    \"\"\"\n    # a lot of the ugliness comes from the fact that\n    # list(PauliTerm._ops.items()) is not the appropriate input for\n    # Pauliterm.from_list()\n    for key in list(diagonal_sets.keys()):\n        pauli_from_key = PauliTerm.from_list(\n            list(map(lambda x: tuple(reversed(x)), key)))\n        if diagonal_basis_commutes(pauli_term, pauli_from_key):\n            updated_pauli_set = diagonal_sets[key] + [pauli_term]\n            diagonalizing_term = get_diagonalizing_basis(updated_pauli_set)\n            if len(diagonalizing_term) > len(key):\n                del diagonal_sets[key]\n                new_key = tuple(sorted(diagonalizing_term._ops.items(),\n                                       key=lambda x: x[0]))\n                diagonal_sets[new_key] = updated_pauli_set\n            else:\n                diagonal_sets[key] = updated_pauli_set\n            return diagonal_sets\n    # made it through all keys and sets so need to make a new set\n    else:\n        # always need to sort because new pauli term functionality\n        new_key = tuple(sorted(pauli_term._ops.items(), key=lambda x: x[0]))\n        diagonal_sets[new_key] = [pauli_term]\n        return diagonal_sets"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef commuting_sets_by_zbasis(pauli_sums):\n    diagonal_sets = {}\n    for term in pauli_sums:\n        diagonal_sets = _max_key_overlap(term, diagonal_sets)\n\n    return diagonal_sets", "response": "Computes the commuting sets based on the PauliSums associated with the current Z basis."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_trivial_commutation(pauli_list, single_pauli_term):\n    if not isinstance(pauli_list, list):\n        raise TypeError(\"pauli_list should be a list\")\n\n    for term in pauli_list:\n        if not _commutes(term, single_pauli_term):\n            return False\n    return True", "response": "Checks if a PauliTerm trivially commutes with a list of other terms."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef commuting_sets_by_indices(pauli_sums, commutation_check):\n    assert isinstance(pauli_sums, list)\n\n    group_inds = []\n    group_terms = []\n    for i, pauli_sum in enumerate(pauli_sums):\n        for j, term in enumerate(pauli_sum):\n            if len(group_inds) == 0:\n                # Initialization\n                group_inds.append([(i, j)])\n                group_terms.append([term])\n                continue\n\n            for k, group in enumerate(group_terms):\n                if commutation_check(group, term):\n                    group_inds[k] += [(i, j)]\n                    group_terms[k] += [term]\n                    break\n            else:\n                # for ... else means loop completed without a `break`\n                # Which means this needs to start its own group.\n                group_inds.append([(i, j)])\n                group_terms.append([term])\n\n    return group_inds", "response": "Finds commuting sets by pauli sums and returns a list of lists of tuples each containing the pauli sum and the term commuting term."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngroup a pauli term into commuting sets using trivial check.", "response": "def commuting_sets_trivial(pauli_sum):\n    \"\"\"\n    Group a pauli term into commuting sets using trivial check\n\n    :param pauli_sum: PauliSum term\n    :return: list of lists containing individual Pauli Terms\n    \"\"\"\n    if not isinstance(pauli_sum, (PauliTerm, PauliSum)):\n        raise TypeError(\"This method can only group PauliTerm or PauliSum objects\")\n\n    if isinstance(pauli_sum, PauliTerm):\n        pauli_sum = PauliSum([pauli_sum])\n\n    commuting_terms = []\n    for term in pauli_sum:\n        # find the group that it trivially commutes with\n        for term_group in commuting_terms:\n            if check_trivial_commutation(term_group, term):\n                term_group.append(term)\n                break\n        else:\n            commuting_terms.append([term])\n\n    return commuting_terms"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef energy_value(h: List[Union[int, float]],\n                 J: Dict[Tuple[int, int], Union[int, float]],\n                 sol: List[int]) -> Union[int, float]:\n    \"\"\"\n    Obtain energy of an Ising solution for a given Ising problem (h,J).\n\n    :param h: External magnetic term of the Ising problem.\n    :param J: Interaction term of the Ising problem.\n    :param sol: Ising solution.\n    :return: Energy of the Ising string.\n    \"\"\"\n    ener_ising = 0\n    for elm in J.keys():\n        if elm[0] == elm[1]:\n            raise TypeError(\"\"\"Interaction term must connect two different variables\"\"\")\n        else:\n            ener_ising += J[elm] * int(sol[elm[0]]) * int(sol[elm[1]])\n    for i in range(len(h)):\n        ener_ising += h[i] * int(sol[i])\n    return ener_ising", "response": "Calculates the energy of an Ising problem."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef swap_circuit_generator(register_a: List[int], register_b: List[int], ancilla: int) -> Program:\n    if len(register_a) != len(register_b):\n        raise RegisterSizeMismatch(\"registers involve different numbers of qubits\")\n\n    if not isinstance(register_a, list):\n        raise TypeError(\"Register A needs to be list\")\n    if not isinstance(register_b, list):\n        raise TypeError(\"Register B needs to be a list\")\n\n    if ancilla is None:\n        ancilla = max(register_a + register_b) + 1\n\n    swap_program = Program()\n    swap_program += H(ancilla)\n    for a, b in zip(register_a, register_b):\n        swap_program += CSWAP(ancilla, a, b)\n    swap_program += H(ancilla)\n\n    return swap_program", "response": "Generates the swap test circuit primitive."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a function that accepts parameters and returns a new Quil program.", "response": "def get_parameterized_program(self):\n        \"\"\"\n        Return a function that accepts parameters and returns a new Quil program.\n\n        :returns: a function\n        \"\"\"\n        cost_para_programs = []\n        driver_para_programs = []\n\n        for idx in range(self.steps):\n            cost_list = []\n            driver_list = []\n            for cost_pauli_sum in self.cost_ham:\n                for term in cost_pauli_sum.terms:\n                    cost_list.append(exponential_map(term))\n\n            for driver_pauli_sum in self.ref_ham:\n                for term in driver_pauli_sum.terms:\n                    driver_list.append(exponential_map(term))\n\n            cost_para_programs.append(cost_list)\n            driver_para_programs.append(driver_list)\n\n        def psi_ref(params):\n            \"\"\"\n            Construct a Quil program for the vector (beta, gamma).\n\n            :param params: array of 2 . p angles, betas first, then gammas\n            :return: a pyquil program object\n            \"\"\"\n            if len(params) != 2*self.steps:\n                raise ValueError(\"params doesn't match the number of parameters set by `steps`\")\n            betas = params[:self.steps]\n            gammas = params[self.steps:]\n\n            prog = Program()\n            prog += self.ref_state_prep\n            for idx in range(self.steps):\n                for fprog in cost_para_programs[idx]:\n                    prog += fprog(gammas[idx])\n\n                for fprog in driver_para_programs[idx]:\n                    prog += fprog(betas[idx])\n\n            return prog\n\n        return psi_ref"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_angles(self) -> Tuple[List[float], List[float]]:\n        stacked_params = np.hstack((self.betas, self.gammas))\n        vqe = VQE(self.minimizer, minimizer_args=self.minimizer_args,\n                  minimizer_kwargs=self.minimizer_kwargs)\n        cost_ham = reduce(lambda x, y: x + y, self.cost_ham)\n        # maximizing the cost function!\n        param_prog = self.get_parameterized_program()\n        result = vqe.vqe_run(param_prog, cost_ham, stacked_params, qc=self.qc,\n                             **self.vqe_options)\n        self.result = result\n        betas = result.x[:self.steps]\n        gammas = result.x[self.steps:]\n        return betas, gammas", "response": "Finds optimal angles with the quantum variational eigensolver method."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef probabilities(self, angles: List[float]) -> np.ndarray:\n        if isinstance(angles, list):\n            angles = np.array(angles)\n\n        assert angles.shape[0] == 2 * self.steps, \"angles must be 2 * steps\"\n\n        param_prog = self.get_parameterized_program()\n        prog = param_prog(angles)\n        wf = WavefunctionSimulator().wavefunction(prog)\n        wf = wf.amplitudes.reshape((-1, 1))\n        probs = np.zeros_like(wf)\n        for xx in range(2 ** len(self.qubits)):\n            probs[xx] = np.conj(wf[xx]) * wf[xx]\n        return probs", "response": "Computes the probability of each outcome given a particular set of angles."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the most probable string for a set of betas and gamma angles.", "response": "def get_string(self, betas: List[float], gammas: List[float], samples: int = 100):\n        \"\"\"\n        Compute the most probable string.\n\n        The method assumes you have passed init_betas and init_gammas with your\n        pre-computed angles or you have run the VQE loop to determine the\n        angles.  If you have not done this you will be returning the output for\n        a random set of angles.\n\n        :param betas: List of beta angles\n        :param gammas: List of gamma angles\n        :param samples: (Optional) number of samples to get back from the QuantumComputer.\n        :returns: tuple representing the bitstring, Counter object from\n                  collections holding all output bitstrings and their frequency.\n        \"\"\"\n        if samples <= 0 and not isinstance(samples, int):\n            raise ValueError(\"samples variable must be positive integer\")\n        param_prog = self.get_parameterized_program()\n        stacked_params = np.hstack((betas, gammas))\n\n        sampling_prog = Program()\n        ro = sampling_prog.declare('ro', 'BIT', len(self.qubits))\n        sampling_prog += param_prog(stacked_params)\n        sampling_prog += [MEASURE(qubit, r) for qubit, r in zip(self.qubits, ro)]\n        sampling_prog.wrap_in_numshots_loop(samples)\n        executable = self.qc.compile(sampling_prog)\n        bitstring_samples = self.qc.run(executable)\n        bitstring_tuples = list(map(tuple, bitstring_samples))\n        freq = Counter(bitstring_tuples)\n        most_frequent_bit_string = max(freq, key=lambda x: freq[x])\n        return most_frequent_bit_string, freq"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the unitary matrix that encodes the Grover s algorithm for the given bitstring_map.", "response": "def _compute_grover_oracle_matrix(bitstring_map: Dict[str, int]) -> np.ndarray:\n        \"\"\"\n        Computes the unitary matrix that encodes the oracle function for Grover's algorithm\n\n        :param bitstring_map: dict with string keys corresponding to bitstrings,\n         and integer values corresponding to the desired phase on the output state.\n        :return: a numpy array corresponding to the unitary matrix for oracle for the given\n         bitstring_map\n        \"\"\"\n        n_bits = len(list(bitstring_map.keys())[0])\n        oracle_matrix = np.zeros(shape=(2 ** n_bits, 2 ** n_bits))\n        for b in range(2 ** n_bits):\n            pad_str = np.binary_repr(b, n_bits)\n            phase_factor = bitstring_map[pad_str]\n            oracle_matrix[b, b] = phase_factor\n        return oracle_matrix"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _construct_grover_circuit(self) -> None:\n        oracle = Program()\n        oracle_name = \"GROVER_ORACLE\"\n        oracle.defgate(oracle_name, self.unitary_function_mapping)\n        oracle.inst(tuple([oracle_name] + self.qubits))\n        self.grover_circuit = self.oracle_grover(oracle, self.qubits)", "response": "Constructs an instance of Grover s Algorithm using initialized values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _init_attr(self, bitstring_map: Dict[str, int]) -> None:\n        self.bit_map = bitstring_map\n        self.unitary_function_mapping = self._compute_grover_oracle_matrix(bitstring_map)\n        self.n_qubits = self.unitary_function_mapping.shape[0]\n        self.qubits = list(range(int(np.log2(self.n_qubits))))\n        self._construct_grover_circuit()", "response": "Initializes an instance of Grover s Algorithm given a bitstring_map."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning Grover s Algorithm to find a bitstring that is designated by bistring_map.", "response": "def find_bitstring(self, qc: QuantumComputer, bitstring_map: Dict[str, int]) -> str:\n        \"\"\"\n        Runs Grover's Algorithm to find the bitstring that is designated by ``bistring_map``.\n\n        In particular, this will prepare an initial state in the uniform superposition over all bit-\n        strings, an then use Grover's Algorithm to pick out the desired bitstring.\n\n        :param qc: the connection to the Rigetti cloud to run pyQuil programs.\n        :param bitstring_map: a mapping from bitstrings to the phases that the oracle should impart\n            on them. If the oracle should \"look\" for a bitstring, it should have a ``-1``, otherwise\n            it should have a ``1``.\n        :return: Returns the bitstring resulting from measurement after Grover's Algorithm.\n        \"\"\"\n\n        self._init_attr(bitstring_map)\n\n        ro = self.grover_circuit.declare('ro', 'BIT', len(self.qubits))\n        self.grover_circuit += [MEASURE(qubit, ro[idx]) for idx, qubit in enumerate(self.qubits)]\n        executable = qc.compile(self.grover_circuit)\n        sampled_bitstring = qc.run(executable)\n\n        return \"\".join([str(bit) for bit in sampled_bitstring[0]])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef oracle_grover(oracle: Program, qubits: List[int], num_iter: int = None) -> Program:\n        if num_iter is None:\n            num_iter = int(round(np.pi * 2 ** (len(qubits) / 2.0 - 2.0)))\n        uniform_superimposer = Program().inst([H(qubit) for qubit in qubits])\n        amp_prog = amplification_circuit(uniform_superimposer, oracle, qubits, num_iter)\n        return amp_prog", "response": "Returns a program that can be used to generate the Grover s Algorithm for a given set of qubits."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_uniformly_controlled_rotation_matrix(k):\n    M = np.full((2 ** k, 2 ** k), 2 ** -k)\n    for i in range(2 ** k):\n        g_i = i ^ (i >> 1)  # Gray code for i\n        for j in range(2 ** k):\n            M[i, j] *= (-1) ** (bin(j & g_i).count(\"1\"))\n    return M", "response": "Returns the matrix represented by arXiv : quant - ph / 0407010."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of positions for the control qubits of the given number of successive CNOTs.", "response": "def get_cnot_control_positions(k):\n    \"\"\"\n    Returns a list of positions for the controls of the CNOTs used when\n    decomposing uniformly controlled rotations, as outlined in\n    arXiv:quant-ph/0407010.\n\n    Referencing Fig. 2 in the aforementioned paper, this method\n    uses the convention that, going up from the target qubit,\n    the control qubits are labelled :math:`1, 2, \\ldots, k`,\n    where :math:`k` is the number of control qubits.\n    The returned list provides the qubit that controls\n    each successive CNOT, in order from left to right.\n\n    :param int k: the number of control qubits\n    :return: the list of positions of the controls\n    :rtype: list\n    \"\"\"\n    rotation_cnots = [1, 1]\n    for i in range(2, k + 1):\n        # algorithm described is to replace the last control\n        # with a control to the new qubit\n        # and then repeat the sequence twice\n        rotation_cnots[-1] = i\n        rotation_cnots = rotation_cnots + rotation_cnots\n    return rotation_cnots"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsimulate one step of rotations.", "response": "def get_rotation_parameters(phases, magnitudes):\n    \"\"\"\n    Simulates one step of rotations.\n\n    Given lists of phases and magnitudes of the same length :math:`N`,\n    such that :math:`N=2^n` for some positive integer :math:`n`,\n    finds the rotation angles required for one step of phase and magnitude\n    unification.\n\n    :param list phases: real valued phases from :math:`-\\\\pi` to :math:`\\\\pi`.\n    :param list magnitudes: positive, real value magnitudes such that\n                       the sum of the square of each magnitude is\n                       :math:`2^{-m}` for some nonnegative integer :math:`m`.\n    :return: A tuple t of four lists such that\n\n        - t[0] are the z-rotations needed to unify adjacent pairs of phases\n        - t[1] are the y-rotations needed to unify adjacent pairs of magnitudes\n        - t[2] are the updated phases after these rotations are applied\n        - t[3] are the updated magnitudes after these rotations are applied\n\n    :rtype: tuple\n    \"\"\"\n    # will hold the angles for controlled rotations\n    # in the phase unification and probability unification steps,\n    # respectively\n    z_thetas = []\n    y_thetas = []\n\n    # will hold updated phases and magnitudes after rotations\n    new_phases = []\n    new_magnitudes = []\n\n    for i in range(0, len(phases), 2):\n        # find z rotation angles\n        phi = phases[i]\n        psi = phases[i + 1]\n        z_thetas.append(phi - psi)\n\n        # update phases after applying such rotations\n        kappa = (phi + psi) / 2.\n        new_phases.append(kappa)\n\n        # find y rotation angles\n        a = magnitudes[i]\n        b = magnitudes[i + 1]\n        if a == 0 and b == 0:\n            y_thetas.append(0)\n        else:\n            y_thetas.append(\n                2 * np.arcsin((a - b) / (np.sqrt(2 * (a ** 2 + b ** 2)))))\n\n        # update magnitudes after applying such rotations\n        c = np.sqrt((a ** 2 + b ** 2) / 2.)\n        new_magnitudes.append(c)\n\n    return z_thetas, y_thetas, new_phases, new_magnitudes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a Program representing the reversed circuit of the uniformly controlled CNOTs in a Unification step.", "response": "def get_reversed_unification_program(angles, control_indices,\n                                     target, controls, mode):\n    \"\"\"\n    Gets the Program representing the reversed circuit\n    for the decomposition of the uniformly controlled\n    rotations in a unification step.\n\n    If :math:`n` is the number of controls, the indices within control indices\n    must range from 1 to :math:`n`, inclusive. The length of control_indices\n    and the length of angles must both be :math:`2^n`.\n\n    :param list angles: The angles of rotation in the the decomposition,\n                        in order from left to right\n    :param list control_indices: a list of positions for the controls of the\n                                 CNOTs used when decomposing uniformly\n                                 controlled rotations; see\n                                 get_cnot_control_positions for labelling\n                                 conventions.\n    :param int target: Index of the target of all rotations\n    :param list controls: Index of the controls, in order from bottom to top.\n    :param str mode: The unification mode. Is either 'phase', corresponding\n                     to controlled RZ rotations, or 'magnitude', corresponding\n                     to controlled RY rotations.\n    :return: The reversed circuit of this unification step.\n    :rtype: Program\n    \"\"\"\n    if mode == 'phase':\n        gate = RZ\n    elif mode == 'magnitude':\n        gate = RY\n    else:\n        raise ValueError(\"mode must be \\'phase\\' or \\'magnitude\\'\")\n\n    reversed_gates = []\n\n    for j in range(len(angles)):\n        if angles[j] != 0:\n            # angle is negated in conjugated/reversed circuit\n            reversed_gates.append(gate(-angles[j], target))\n        if len(controls) > 0:\n            reversed_gates.append(CNOT(controls[control_indices[j] - 1],\n                                       target))\n\n    return Program().inst(reversed_gates[::-1])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_ancestors(self):\n        node = self\n        ancestor_list = []\n        while node.parent is not None:\n            ancestor_list.append(node.parent)\n            node = node.parent\n\n        return ancestor_list", "response": "Returns a list of ancestors of the node. Ordered from the earliest."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_remainder_set(self, j):\n        result = []\n        ancestors = self.get_update_set(j)\n\n        # This runs in O(log(N)log(N)) where N is the number of qubits.\n        for a in ancestors:\n            for c in a.children:\n                if c.index < j:\n                    result.append(c)\n\n        return result", "response": "Return the set of children with indices less than j of all ancestors\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntries importing the qutip module log an error if unsuccessful.", "response": "def import_qutip():\n    \"\"\"\n    Try importing the qutip module, log an error if unsuccessful.\n\n    :return: The qutip module if successful or None\n    :rtype: Optional[module]\n    \"\"\"\n    global _QUTIP_ERROR_LOGGED\n    try:\n        import qutip\n    except ImportError:  # pragma no coverage\n        qutip = None\n        if not _QUTIP_ERROR_LOGGED:\n            _log.error(\"Could not import qutip. Tomography tools will not function.\")\n            _QUTIP_ERROR_LOGGED = True\n    return qutip"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntry importing the cvxpy module if unsuccessful return None if unsuccessful.", "response": "def import_cvxpy():\n    \"\"\"\n    Try importing the qutip module, log an error if unsuccessful.\n\n    :return: The cvxpy module if successful or None\n    :rtype: Optional[module]\n    \"\"\"\n    global _CVXPY_ERROR_LOGGED\n    try:\n        import cvxpy\n    except ImportError:  # pragma no coverage\n        cvxpy = None\n        if not _CVXPY_ERROR_LOGGED:\n            _log.error(\"Could not import cvxpy. Tomography tools will not function.\")\n            _CVXPY_ERROR_LOGGED = True\n    return cvxpy"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconfigures whether this module should assume that it is in jupyter notebook mode.", "response": "def notebook_mode(m):\n    \"\"\"\n    Configure whether this module should assume that it is being run from a jupyter notebook.\n    This sets some global variables related to how progress for long measurement sequences is\n    indicated.\n\n    :param bool m: If True, assume to be in notebook.\n    :return: None\n    :rtype: NoneType\n    \"\"\"\n    global NOTEBOOK_MODE\n    global TRANGE\n    NOTEBOOK_MODE = m\n    if NOTEBOOK_MODE:\n        TRANGE = tqdm.tnrange\n    else:\n        TRANGE = tqdm.trange"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndraw random samples from a discrete probability distribution with outcomes 0 1... k - 1 draw n random samples.", "response": "def sample_outcomes(probs, n):\n    \"\"\"\n    For a discrete probability distribution ``probs`` with outcomes 0, 1, ..., k-1 draw ``n``\n    random samples.\n\n    :param list probs: A list of probabilities.\n    :param Number n: The number of random samples to draw.\n    :return: An array of samples drawn from distribution probs over 0, ..., len(probs) - 1\n    :rtype: numpy.ndarray\n    \"\"\"\n    dist = np.cumsum(probs)\n    rs = np.random.rand(n)\n    return np.array([(np.where(r < dist)[0][0]) for r in rs])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a sequence of programs that prepares the measurement basis states of some set of qubits.", "response": "def basis_state_preps(*qubits):\n    \"\"\"\n    Generate a sequence of programs that prepares the measurement\n    basis states of some set of qubits in the order such that the qubit\n    with highest index is iterated over the most quickly:\n    E.g., for ``qubits=(0, 1)``, it returns the circuits::\n\n        I_0 I_1\n        I_0 X_1\n        X_0 I_1\n        X_0 X_1\n\n    :param list qubits: Each qubit to include in the basis state preparation.\n    :return: Yields programs for each basis state preparation.\n    :rtype: Program\n    \"\"\"\n    for prep in cartesian_product([I, X], repeat=len(qubits)):\n        basis_prep = Program(Pragma(\"PRESERVE_BLOCK\"))\n        for gate, qubit in zip(prep, qubits):\n            basis_prep.inst(gate(qubit))\n        basis_prep.inst(Pragma(\"END_PRESERVE_BLOCK\"))\n        yield basis_prep"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sample_bad_readout(program, num_samples, assignment_probs, cxn):\n    wf = cxn.wavefunction(program)\n    return sample_outcomes(assignment_probs.dot(abs(wf.amplitudes.ravel())**2), num_samples)", "response": "Generates n samples of measuring all outcomes from a Quil program assuming the assignment probabilities assignment_probs applied to cxn."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the estimated assignment probability matrix for a sequence of single shot histograms that are generated by running the programs generated by basis_state_preps.", "response": "def estimate_assignment_probs(bitstring_prep_histograms):\n    \"\"\"\n    Compute the estimated assignment probability matrix for a sequence of single shot histograms\n    obtained by running the programs generated by `basis_state_preps()`.\n\n        bitstring_prep_histograms[i,j] = #number of measured outcomes j when running program i\n\n    The assignment probability is obtained by transposing and afterwards normalizing the columns.\n\n        p[j, i] = Probability to measure outcome j when preparing the state with program i.\n\n    :param list|numpy.ndarray bitstring_prep_histograms: A nested list or 2d array with shape\n    (d, d), where ``d = 2**nqubits`` is the dimension of the Hilbert space. The first axis varies\n    over the state preparation program index, the second axis corresponds to the measured bitstring.\n    :return: The assignment probability matrix.\n    :rtype: numpy.ndarray\n    \"\"\"\n    p = np.array(bitstring_prep_histograms, dtype=float).T\n    p /= p.sum(axis=0)[np.newaxis, :]\n    return p"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvisualizes the Pauli Transfer Matrix of a process.", "response": "def plot_pauli_transfer_matrix(ptransfermatrix, ax, labels, title):\n    \"\"\"\n    Visualize the Pauli Transfer Matrix of a process.\n\n    :param numpy.ndarray ptransfermatrix: The Pauli Transfer Matrix\n    :param ax: The matplotlib axes.\n    :param labels: The labels for the operator basis states.\n    :param title: The title for the plot\n    :return: The modified axis object.\n    :rtype: AxesSubplot\n\n    \"\"\"\n    im = ax.imshow(ptransfermatrix, interpolation=\"nearest\", cmap=rigetti_3_color_cm, vmin=-1,\n                   vmax=1)\n    dim = len(labels)\n    plt.colorbar(im, ax=ax)\n    ax.set_xticks(range(dim))\n    ax.set_xlabel(\"Input Pauli Operator\", fontsize=20)\n    ax.set_yticks(range(dim))\n    ax.set_ylabel(\"Output Pauli Operator\", fontsize=20)\n    ax.set_title(title, fontsize=25)\n    ax.set_xticklabels(labels, rotation=45)\n    ax.set_yticklabels(labels)\n    ax.grid(False)\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef state_histogram(rho, ax=None, title=\"\", threshold=0.001):\n    rho_amps = rho.data.toarray().ravel()\n    nqc = int(round(np.log2(rho.shape[0])))\n    if ax is None:\n        fig = plt.figure(figsize=(10, 6))\n        ax = Axes3D(fig, azim=-35, elev=35)\n    cmap = rigetti_4_color_cm\n    norm = mpl.colors.Normalize(-np.pi, np.pi)\n    colors = cmap(norm(np.angle(rho_amps)))\n    dzs = abs(rho_amps)\n    colors[:, 3] = 1.0 * (dzs > threshold)\n    xs, ys = np.meshgrid(range(2 ** nqc), range(2 ** nqc))\n    xs = xs.ravel()\n    ys = ys.ravel()\n    zs = np.zeros_like(xs)\n    dxs = dys = np.ones_like(xs) * 0.8\n\n    _ = ax.bar3d(xs, ys, zs, dxs, dys, dzs, color=colors)\n    ax.set_xticks(np.arange(2 ** nqc) + .4)\n    ax.set_xticklabels(basis_labels(nqc))\n    ax.set_yticks(np.arange(2 ** nqc) + .4)\n    ax.set_yticklabels(basis_labels(nqc))\n    ax.set_zlim3d([0, 1])\n\n    cax, kw = mpl.colorbar.make_axes(ax, shrink=.75, pad=.1)\n    cb = mpl.colorbar.ColorbarBase(cax, cmap=cmap, norm=norm)\n    cb.set_ticks([-np.pi, -np.pi / 2, 0, np.pi / 2, np.pi])\n    cb.set_ticklabels((r'$-\\pi$', r'$-\\pi/2$', r'$0$', r'$\\pi/2$', r'$\\pi$'))\n    cb.set_label('arg')\n    ax.view_init(azim=-55, elev=45)\n    ax.set_title(title)\n    return ax", "response": "Visualize a density matrix as a 3d bar plot with complex phase encoded\n    as the bar color."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a binary bitstring into the corresponding unsigned integer.", "response": "def bitlist_to_int(bitlist):\n    \"\"\"Convert a binary bitstring into the corresponding unsigned integer.\n\n    :param list bitlist: A list of ones of zeros.\n    :return: The corresponding integer.\n    :rtype: int\n    \"\"\"\n\n    ret = 0\n    for b in bitlist:\n        ret = (ret << 1) | (int(b) & 1)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsample the assignment probabilities of a set of qubits using nsamples per measurement and compute the estimated assignment probability matrix.", "response": "def sample_assignment_probs(qubits, nsamples, cxn):\n    \"\"\"\n    Sample the assignment probabilities of qubits using nsamples per measurement, and then compute\n    the estimated assignment probability matrix. See the docstring for estimate_assignment_probs for\n    more information.\n\n    :param list qubits: Qubits to sample the assignment probabilities for.\n    :param int nsamples: The number of samples to use in each measurement.\n    :param QPUConnection|QVMConnection cxn: The Connection object to connect to Forest.\n    :return: The assignment probability matrix.\n    :rtype: numpy.ndarray\n    \"\"\"\n    num_qubits = len(qubits)\n    dimension = 2 ** num_qubits\n    hists = []\n    preps = basis_state_preps(*qubits)\n\n    jobs = []\n    _log.info('Submitting jobs...')\n    for jj, p in izip(TRANGE(dimension), preps):\n        jobs.append(cxn.run_and_measure_async(p, qubits, nsamples))\n\n    _log.info('Waiting for results...')\n    for jj, job_id in izip(TRANGE(dimension), jobs):\n        job = cxn.wait_for_job(job_id)\n        results = job.result()\n        idxs = list(map(bitlist_to_int, results))\n        hists.append(make_histogram(idxs, dimension))\n    return estimate_assignment_probs(hists)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_in_parallel(programs, nsamples, cxn, shuffle=True):\n\n    if shuffle:\n        n_groups = len(programs)\n        n_progs_per_group = len(programs[0])\n        permutations = np.outer(np.ones(n_groups, dtype=int),\n                                np.arange(n_progs_per_group, dtype=int))\n        inverse_permutations = np.zeros_like(permutations)\n\n        for jj in range(n_groups):\n            # in-place operation\n            np.random.shuffle(permutations[jj])\n            # store inverse permutation\n            inverse_permutations[jj] = np.argsort(permutations[jj])\n\n        # apply to programs\n        shuffled_programs = np.empty((n_groups, n_progs_per_group), dtype=object)\n        for jdx, (progsj, pj) in enumerate(zip(programs, permutations)):\n            shuffled_programs[jdx] = [progsj[pjk] for pjk in pj]\n\n        shuffled_results = _run_in_parallel(shuffled_programs, nsamples, cxn)\n\n        # reverse shuffling of results\n        results = np.array([resultsj[pj]\n                            for resultsj, pj in zip(shuffled_results, inverse_permutations)])\n        return results\n    else:\n        return _run_in_parallel(programs, nsamples, cxn)", "response": "Runs a list of Protoquil programs on disjoint qubits and returns a list of bitstring histograms for each input program."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning the program in parallel and returns the result of the program execution.", "response": "def _run_in_parallel(programs, nsamples, cxn):\n    \"\"\"\n    See docs for ``run_in_parallel()``.\n\n    :param Union[np.ndarray,List[List[Program]]] programs: A rectangular list of lists, or a 2d\n        array of Quil Programs. The outer list iterates over disjoint qubit groups as targets, the\n        inner list over programs to run on those qubits, e.g., tomographic sequences.\n    :param int nsamples: Number of repetitions for executing each Program.\n    :param QPUConnection|QVMConnection cxn: The quantum machine connection.\n    :return: An array of 2d arrays that provide bitstring histograms for each input program.\n        The axis of the outer array iterates over the disjoint qubit groups, the outer axis of the\n        inner 2d array iterates over the programs for that group and the inner most axis iterates\n        over all possible bitstrings for the qubit group under consideration. The bitstrings are\n        enumerated in lexicographical order, i.e., for a program with qubits {3, 1, 2} the qubits\n        are first sorted -> [1, 2, 3] and then the bitstrings are enumerated as 000, 001, 010,\n        where the bits ijk correspond to the states of qubits 1,2 and 3, respectively.\n    :rtype np.array\n    \"\"\"\n    n_groups = len(programs)\n    n_progs_per_group = len(programs[0])\n\n    for progs in programs[1:]:\n        if not len(progs) == n_progs_per_group:\n            raise ValueError(\"Non-rectangular grid of programs specified: {}\".format(programs))\n\n    # identify qubit groups, ensure disjointedness\n    qubit_groups = [set() for _ in range(n_groups)]\n    for group_idx, group in enumerate(qubit_groups):\n        for prog in programs[group_idx]:\n            group.update(set(prog.get_qubits()))\n\n        # test that groups are actually disjoint by comparing with the ones already created\n        for other_idx, other_group in enumerate(qubit_groups[:group_idx]):\n            intersection = other_group & group\n            if intersection:\n                raise ValueError(\n                    \"Programs from groups {} and {} intersect on qubits {}\".format(\n                        other_idx, group_idx, intersection))\n\n    qubit_groups = [sorted(c) for c in qubit_groups]\n    all_qubits = sum(qubit_groups, [])\n    n_qubits_per_group = [len(c) for c in qubit_groups]\n\n    # create joint programs\n    parallel_programs = [sum(progsj, Program()) for progsj in zip(*programs)]\n\n    # execute on cxn\n    all_results = []\n    for i, prog in izip(TRANGE(n_progs_per_group), parallel_programs):\n        try:\n            results = cxn.run_and_measure(prog, all_qubits, nsamples)\n            all_results.append(np.array(results))\n        except QPUError as e:\n            _log.error(\"Could not execute parallel program:\\n%s\", prog.out())\n            raise e\n\n    # generate histograms per qubit group\n    all_histograms = np.array([np.zeros((n_progs_per_group, 2 ** n_qubits), dtype=int)\n                               for n_qubits in n_qubits_per_group])\n    for idx, results in enumerate(all_results):\n        n_qubits_seen = 0\n        for jdx, n_qubits in enumerate(n_qubits_per_group):\n            group_results = results[:, n_qubits_seen:n_qubits_seen + n_qubits]\n            outcome_labels = list(map(bitlist_to_int, group_results))\n            dimension = 2 ** n_qubits\n            all_histograms[jdx][idx] = make_histogram(outcome_labels, dimension)\n            n_qubits_seen += n_qubits\n\n    return all_histograms"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_imaginary_terms(pauli_sums: PauliSum) -> PauliSum:\n    if not isinstance(pauli_sums, PauliSum):\n        raise TypeError(\"not a pauli sum. please give me one\")\n    new_term = sI(0) * 0.0\n    for term in pauli_sums:\n        new_term += term_with_coeff(term, term.coefficient.real)\n\n    return new_term", "response": "Removes the imaginary component of each term in a Pauli sum."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_rotation_program(pauli_term: PauliTerm) -> Program:\n    meas_basis_change = Program()\n    for index, gate in pauli_term:\n        if gate == 'X':\n            meas_basis_change.inst(RY(-np.pi / 2, index))\n        elif gate == 'Y':\n            meas_basis_change.inst(RX(np.pi / 2, index))\n        elif gate == 'Z':\n            pass\n        else:\n            raise ValueError()\n\n    return meas_basis_change", "response": "Generates a program that can be used to generate one - qubit rotations."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes a one - qubit - controlled version of a matrix.", "response": "def controlled(m: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Make a one-qubit-controlled version of a matrix.\n\n    :param m: A matrix.\n    :return: A controlled version of that matrix.\n    \"\"\"\n    rows, cols = m.shape\n    assert rows == cols\n    n = rows\n    I = np.eye(n)\n    Z = np.zeros((n, n))\n    controlled_m = np.bmat([[I, Z],\n                            [Z, m]])\n    return controlled_m"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a circuit for quantum phase estimation.", "response": "def phase_estimation(U: np.ndarray, accuracy: int, reg_offset: int = 0) -> Program:\n    \"\"\"\n    Generate a circuit for quantum phase estimation.\n\n    :param U: A unitary matrix.\n    :param accuracy: Number of bits of accuracy desired.\n    :param reg_offset: Where to start writing measurements (default 0).\n    :return: A Quil program to perform phase estimation.\n    \"\"\"\n    assert isinstance(accuracy, int)\n    rows, cols = U.shape\n    m = int(log2(rows))\n    output_qubits = range(0, accuracy)\n    U_qubits = range(accuracy, accuracy + m)\n    p = Program()\n    ro = p.declare('ro', 'BIT', len(output_qubits))\n\n    # Hadamard initialization\n    for i in output_qubits:\n        p.inst(H(i))\n    # Controlled unitaries\n    for i in output_qubits:\n        if i > 0:\n            U = np.dot(U, U)\n        cU = controlled(U)\n        name = \"CONTROLLED-U{0}\".format(2 ** i)\n        # define the gate\n        p.defgate(name, cU)\n        # apply it\n        p.inst((name, i) + tuple(U_qubits))\n    # Compute the QFT\n    p = p + inverse_qft(output_qubits)\n    # Perform the measurements\n    for i in output_qubits:\n        p.measure(i, ro[reg_offset + i])\n\n    return p"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a binary floating point number to a decimal floating point number.", "response": "def binary_float_to_decimal_float(number: Union[float, str]) -> float:\n    \"\"\"\n    Convert binary floating point to decimal floating point.\n\n    :param number: Binary floating point.\n    :return: Decimal floating point representation of binary floating point.\n    \"\"\"\n    if isinstance(number, str):\n        if number[0] == '-':\n            n_sign = -1\n        else:\n            n_sign = 1\n    elif isinstance(number, float):\n        n_sign = np.sign(number)\n        number = str(number)\n\n    deci = 0\n    for ndx, val in enumerate(number.split('.')[-1]):\n        deci += float(val) / 2**(ndx+1)\n    deci *= n_sign\n\n    return deci"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef measurements_to_bf(measurements: np.ndarray) -> float:\n    try:\n        measurements.sum(axis=0)\n    except AttributeError:\n        measurements = np.asarray(measurements)\n    finally:\n        stats = measurements.sum(axis=0) / len(measurements)\n\n    stats_str = [str(int(i)) for i in np.round(stats[::-1][1:])]\n    bf_str = '0.' + ''.join(stats_str)\n    bf = float(bf_str)\n\n    return bf", "response": "Convert measurements into gradient binary fraction."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef amplification_circuit(algorithm: Program, oracle: Program,\n                          qubits: List[int],\n                          num_iter: int,\n                          decompose_diffusion: bool = False) -> Program:\n    \"\"\"\n    Returns a program that does ``num_iter`` rounds of amplification, given a measurement-less\n    algorithm, an oracle, and a list of qubits to operate on.\n\n    :param algorithm: A program representing a measurement-less algorithm run on qubits.\n    :param oracle: An oracle maps any basis vector ``|psi>`` to either ``+|psi>`` or\n        ``-|psi>`` depending on whether ``|psi>`` is in the desirable subspace or the undesirable\n        subspace.\n    :param qubits: the qubits to operate on\n    :param num_iter: number of iterations of amplifications to run\n    :param decompose_diffusion: If True, decompose the Grover diffusion gate into two qubit\n     gates. If False, use a defgate to define the gate.\n    :return: The amplified algorithm.\n    \"\"\"\n    program = Program()\n\n    uniform_superimposer = Program().inst([H(qubit) for qubit in qubits])\n    program += uniform_superimposer\n    if decompose_diffusion:\n        diffusion = decomposed_diffusion_program(qubits)\n    else:\n        diffusion = diffusion_program(qubits)\n    # To avoid redefining gates, we collect them before building our program.\n    defined_gates = oracle.defined_gates + algorithm.defined_gates + diffusion.defined_gates\n    for _ in range(num_iter):\n        program += (oracle.instructions\n                 + algorithm.dagger().instructions\n                 + diffusion.instructions\n                 + algorithm.instructions)\n    # We redefine the gates in the new program.\n    for gate in defined_gates:\n        program.defgate(gate.name, gate.matrix)\n    return program", "response": "Returns a program that performs the amplification on the given list of qubits."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decomposed_diffusion_program(qubits: List[int]) -> Program:\n    program = Program()\n    if len(qubits) == 1:\n        program.inst(Z(qubits[0]))\n    else:\n        program.inst([X(q) for q in qubits])\n        program.inst(H(qubits[-1]))\n        program.inst(RZ(-np.pi, qubits[0]))\n        program += (ControlledProgramBuilder()\n                              .with_controls(qubits[:-1])\n                              .with_target(qubits[-1])\n                              .with_operation(X_GATE)\n                              .with_gate_name(X_GATE_LABEL).build())\n        program.inst(RZ(-np.pi, qubits[0]))\n        program.inst(H(qubits[-1]))\n        program.inst([X(q) for q in qubits])\n    return program", "response": "Constructs a decomposed diffusion program for the given list of qubits."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef product_ops(self, indices, conjugate):\n        pterm = PauliTerm('I', 0, 1.0)\n        for conj, index in zip(conjugate, indices):\n            pterm = pterm * self._operator_generator(index, conj)\n\n        pterm = pterm.simplify()\n        return pterm", "response": "Convert a list of site indices and coefficients to a Pauli Operators containing the product operators of the given conjugate and the site the fermionic operator acts on."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _operator_generator(index, conj):\n        pterm = PauliTerm('I', 0, 1.0)\n        Zstring = PauliTerm('I', 0, 1.0)\n        for j in range(index):\n            Zstring = Zstring*PauliTerm('Z', j, 1.0)\n\n        pterm1 = Zstring*PauliTerm('X', index, 0.5)\n        scalar = 0.5 * conj * 1.0j\n        pterm2 = Zstring*PauliTerm('Y', index, scalar)\n        pterm = pterm * (pterm1 + pterm2)\n\n        pterm = pterm.simplify()\n        return pterm", "response": "Internal method to generate the appropriate operator for the appropriate base class"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_1to1_bitmap(mask: str) -> Dict[str, str]:\n    n_bits = len(mask)\n    form_string = \"{0:0\" + str(n_bits) + \"b}\"\n    bit_map_dct = {}\n    for idx in range(2**n_bits):\n        bit_string = form_string.format(idx)\n        bit_map_dct[bit_string] = utils.bitwise_xor(bit_string, mask)\n    return bit_map_dct", "response": "Create a bit map function that can be used to create a 1 - bit tree tree"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef default_rotations(*qubits):\n    for gates in cartesian_product(TOMOGRAPHY_GATES.keys(), repeat=len(qubits)):\n        tomography_program = Program()\n        for qubit, gate in izip(qubits, gates):\n            tomography_program.inst(gate(qubit))\n        yield tomography_program", "response": "Generates the Quil programs for the tomographic pre - and post - rotations of any number of qubits."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates the tomographic pre - and post - rotations of any number of qubits as qutip operators.", "response": "def default_channel_ops(nqubits):\n    \"\"\"\n    Generate the tomographic pre- and post-rotations of any number of qubits as qutip operators.\n\n    :param int nqubits: The number of qubits to perform tomography on.\n    :return: Qutip object corresponding to the tomographic rotation.\n    :rtype: Qobj\n    \"\"\"\n    for gates in cartesian_product(TOMOGRAPHY_GATES.values(), repeat=nqubits):\n        yield qt.tensor(*gates)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_functional(cls):\n        if not cls._tested:\n            cls._tested = True\n            np.random.seed(SEED)\n            test_problem_dimension = 10\n            mat = np.random.randn(test_problem_dimension, test_problem_dimension)\n            posmat = mat.dot(mat.T)\n            posvar = cvxpy.Variable(test_problem_dimension, test_problem_dimension)\n            prob = cvxpy.Problem(cvxpy.Minimize((cvxpy.trace(posmat * posvar)\n                                                 + cvxpy.norm(posvar))),\n                                 [posvar >> 0, cvxpy.trace(posvar) >= 1.])\n\n            try:\n                prob.solve(SOLVER)\n                cls._functional = True\n            except cvxpy.SolverError:  # pragma no coverage\n                _log.warning(\"No convex SDP solver found. You will not be able to solve\"\n                             \" tomography problems with matrix positivity constraints.\")\n        return cls._functional", "response": "Checks lazily whether a convex solver is installed that handles positivity constraints."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _single_projector_generator(ket_op, bra_op, index):\n    if not isinstance(ket_op, int):\n        raise TypeError(\"ket_op needs to be an integer\")\n    if not isinstance(bra_op, int):\n        raise TypeError(\"ket_op needs to be an integer\")\n    if ket_op not in [0, 1] or bra_op not in [0, 1]:\n        raise ValueError(\"bra and ket op needs to be either 0 or 1\")\n\n    if ket_op == 0 and bra_op == 0:\n        return 0.5 * (sZ(index) + sI(index))\n    elif ket_op == 0 and bra_op == 1:\n        return 0.5 * (sX(index) + 1j * sY(index))\n    elif ket_op == 1 and bra_op == 0:\n        return 0.5 * (sX(index) - 1j * sY(index))\n    else:\n        return 0.5 * (sI(index) - sZ(index))", "response": "Generates the pauli sum terms corresponding to |ket_op bra_op| and returns the sum of the terms corresponding to |ket_op > bra_op."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef projector_generator(ket, bra):\n    projectors = []\n    for index, (ket_one_qubit, bra_one_qubit) in enumerate(zip(ket[::-1], bra[::-1])):\n        projectors.append(_single_projector_generator(ket_one_qubit,\n                                                      bra_one_qubit, index))\n    return reduce(lambda x, y: x * y, projectors)", "response": "Generates a Pauli Sum that corresponds to the projection operator ket><bra |ket><bra|"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef measure_wf_coefficients(prep_program, coeff_list, reference_state,\n                            quantum_resource, variance_bound=1.0E-6):\n    \"\"\"\n    Measure a set of coefficients with a phase relative to the reference_state\n\n    :param prep_program: pyQuil program to prepare the state\n    :param coeff_list: list of integers labeling amplitudes to measure\n    :param reference_state: Integer of the computational basis state to use as\n                            a reference\n    :param quantum_resource: An instance of a quantum abstract machine\n    :param variance_bound: Default 1.0E-6.  variance of the monte carlo\n                           estimator for the non-hermitian operator\n    :return: returns a list of reference_state amplitude + coeff_list amplitudes\n    \"\"\"\n    num_qubits = len(prep_program.get_qubits())\n    normalizer_ops = projector_generator(reference_state, reference_state)\n    c0_coeff, _, _ = estimate_locally_commuting_operator(\n        prep_program, normalizer_ops, variance_bound=variance_bound,\n        quantum_resource=quantum_resource)\n    c0_coeff = np.sqrt(c0_coeff)\n\n    amplitudes = []\n    for ii in coeff_list:\n        if ii == reference_state:\n            amplitudes.append(c0_coeff)\n        else:\n            bra = list(map(int, np.binary_repr(ii, width=num_qubits)))\n            c_ii_op = projector_generator(reference_state, bra)\n\n            result = estimate_locally_commuting_operator(\n                prep_program, c_ii_op, variance_bound=variance_bound,\n                quantum_resource=quantum_resource)\n            amplitudes.append(result[0] / c0_coeff)\n\n    return amplitudes", "response": "Measure a set of coefficients with a phase relative to the reference_state."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef measure_pure_state(prep_program, reference_state, quantum_resource,\n                       variance_bound=1.0E-6):\n    \"\"\"\n    Measure the coefficients of the pure state\n\n    :param prep_program: pyQuil program to prepare the state\n    :param reference_state: Integer of the computational basis state to use as\n                            a reference\n    :param quantum_resource: An instance of a quantum abstract machine\n    :param variance_bound: Default 1.0E-6.  variance of the monte carlo\n                           estimator for the non-hermitian operator\n    :return: an estimate of the wavefunction as a numpy.ndarray\n    \"\"\"\n    num_qubits = len(prep_program.get_qubits())\n    amplitudes_to_measure = list(range(2 ** num_qubits))\n\n    amplitudes = measure_wf_coefficients(prep_program, amplitudes_to_measure,\n                                         reference_state,\n                                         quantum_resource,\n                                         variance_bound=variance_bound)\n    wavefunction = np.asarray(amplitudes)\n    return wavefunction.reshape((-1, 1))", "response": "Measure the coefficients of the pure state of a non - hermitian operator."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _operator_generator(self, index, conj):\n        if conj != -1 and conj != +1:\n            raise ValueError(\"Improper conjugate coefficient\")\n        if index >= self.n_qubits or index < 0:\n            raise IndexError(\"Operator index outside number of qubits for \"\n                             \"current Bravyi-Kitaev transform.\")\n\n        # parity set P(j). apply Z to, for parity sign.\n        parity_set = [node.index for node in self.tree.get_parity_set(index)]\n\n        # update set U(j). apply X to, for updating purposes.\n        ancestors = [node.index for node in self.tree.get_update_set(index)]\n\n        # remainder set C(j) = P(j) \\ F(j)\n        ancestor_children = [node.index for node in self.tree.get_remainder_set(index)]\n\n        # Under Majorana basis, creation/annihilation operators given by\n        # a^{\\pm} = (c \\mp id) / 2\n\n        # c_j = a_j + a_j^{\\dagger} = X_{U(j)} X_j Z_{P(j)}\n        c_maj = PauliTerm('X', index)\n        for node_idx in parity_set:\n            c_maj *= PauliTerm('Z', node_idx)\n        for node_idx in ancestors:\n            c_maj *= PauliTerm('X', node_idx)\n\n        # d_j = i(a_j^{\\dagger} - a_j) = X_{U(j)} Y_j Z_{C(j)}\n        d_maj = PauliTerm('Y', index)\n        for node_idx in ancestors:\n            d_maj *= PauliTerm('X', node_idx)\n        for node_idx in ancestor_children:\n            d_maj *= PauliTerm('Z', node_idx)\n\n        result = 0.5 * (c_maj + 1j * conj * d_maj)\n        return result.simplify()", "response": "Generates the appropriate ladder operator at fermion\n        orbital at index."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding this controlled gate.", "response": "def build(self):\n        \"\"\"Builds this controlled gate.\n\n        :return: The controlled gate, defined by this object.\n        :rtype: Program\n        \"\"\"\n        self.defined_gates = set(STANDARD_GATE_NAMES)\n        prog = self._recursive_builder(self.operation,\n                                       self.gate_name,\n                                       self.control_qubits,\n                                       self.target_qubit)\n        return prog"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a defgate to a program.", "response": "def _defgate(self, program, gate_name, gate_matrix):\n        \"\"\"Defines a gate named gate_name with matrix gate_matrix in program. In addition, updates\n         self.defined_gates to track what has been defined.\n\n        :param Program program: Pyquil Program to add the defgate and gate to.\n        :param str gate_name: The name of the gate to add to program.\n        :param numpy.ndarray gate_matrix: The array corresponding to the gate to define.\n        :return: the modified Program.\n        :retype: Program\n        \"\"\"\n        new_program = pq.Program()\n        new_program += program\n        if gate_name not in self.defined_gates:\n            new_program.defgate(gate_name, gate_matrix)\n            self.defined_gates.add(gate_name)\n        return new_program"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parity_even_p(state, marked_qubits):\n    assert isinstance(state, int), \\\n        f\"{state} is not an integer. Must call parity_even_p with an integer state.\"\n    mask = 0\n    for q in marked_qubits:\n        mask |= 1 << q\n    return bin(mask & state).count(\"1\") % 2 == 0", "response": "Calculates the parity of elements at indexes in marked_qubits"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the expectation value of the Zi operator given a wavefunction and a list of bitstrings.", "response": "def expectation_from_sampling(pyquil_program: Program,\n                              marked_qubits: List[int],\n                              qc: QuantumComputer,\n                              samples: int) -> float:\n    \"\"\"\n    Calculation of Z_{i} at marked_qubits\n\n    Given a wavefunctions, this calculates the expectation value of the Zi\n    operator where i ranges over all the qubits given in marked_qubits.\n\n    :param pyquil_program: pyQuil program generating some state\n    :param marked_qubits: The qubits within the support of the Z pauli\n                          operator whose expectation value is being calculated\n    :param qc: A QuantumComputer object.\n    :param samples: Number of bitstrings collected to calculate expectation\n                    from sampling.\n    :returns: The expectation value as a float.\n    \"\"\"\n    program = Program()\n    ro = program.declare('ro', 'BIT', max(marked_qubits) + 1)\n    program += pyquil_program\n    program += [MEASURE(qubit, r) for qubit, r in zip(list(range(max(marked_qubits) + 1)), ro)]\n    program.wrap_in_numshots_loop(samples)\n    executable = qc.compile(program)\n    bitstring_samples = qc.run(executable)\n    bitstring_tuples = list(map(tuple, bitstring_samples))\n\n    freq = Counter(bitstring_tuples)\n\n    # perform weighted average\n    expectation = 0\n    for bitstring, count in freq.items():\n        bitstring_int = int(\"\".join([str(x) for x in bitstring[::-1]]), 2)\n        if parity_even_p(bitstring_int, marked_qubits):\n            expectation += float(count) / samples\n        else:\n            expectation -= float(count) / samples\n    return expectation"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef expectation(pyquil_prog: Program,\n                    pauli_sum: Union[PauliSum, PauliTerm, np.ndarray],\n                    samples: int,\n                    qc: QuantumComputer) -> float:\n        \"\"\"\n        Compute the expectation value of pauli_sum over the distribution generated from pyquil_prog.\n\n        :param pyquil_prog: The state preparation Program to calculate the expectation value of.\n        :param pauli_sum: PauliSum representing the operator of which to calculate the expectation\n            value or a numpy matrix representing the Hamiltonian tensored up to the appropriate\n            size.\n        :param samples: The number of samples used to calculate the expectation value. If samples\n            is None then the expectation value is calculated by calculating <psi|O|psi>. Error\n            models will not work if samples is None.\n        :param qc: The QuantumComputer object.\n\n        :return: A float representing the expectation value of pauli_sum given the distribution\n            generated from quil_prog.\n        \"\"\"\n        if isinstance(pauli_sum, np.ndarray):\n            # debug mode by passing an array\n            wf = WavefunctionSimulator().wavefunction(pyquil_prog)\n            wf = np.reshape(wf.amplitudes, (-1, 1))\n            average_exp = np.conj(wf).T.dot(pauli_sum.dot(wf)).real\n            return average_exp\n        else:\n            if not isinstance(pauli_sum, (PauliTerm, PauliSum)):\n                raise TypeError(\"pauli_sum variable must be a PauliTerm or PauliSum object\")\n\n            if isinstance(pauli_sum, PauliTerm):\n                pauli_sum = PauliSum([pauli_sum])\n\n            if samples is None:\n                operator_progs = []\n                operator_coeffs = []\n                for p_term in pauli_sum.terms:\n                    op_prog = Program()\n                    for qindex, op in p_term:\n                        op_prog.inst(STANDARD_GATES[op](qindex))\n                    operator_progs.append(op_prog)\n                    operator_coeffs.append(p_term.coefficient)\n\n                result_overlaps = WavefunctionSimulator().expectation(pyquil_prog, pauli_sum.terms)\n                result_overlaps = list(result_overlaps)\n                assert len(result_overlaps) == len(operator_progs),\\\n                    \"\"\"Somehow we didn't get the correct number of results back from the QVM\"\"\"\n                expectation = sum(list(map(lambda x: x[0] * x[1],\n                                           zip(result_overlaps, operator_coeffs))))\n                return expectation.real\n            else:\n                if not isinstance(samples, int):\n                    raise TypeError(\"samples variable must be an integer\")\n                if samples <= 0:\n                    raise ValueError(\"samples variable must be a positive integer\")\n\n                # normal execution via fake sampling\n                # stores the sum of contributions to the energy from each operator term\n                expectation = 0.0\n                for j, term in enumerate(pauli_sum.terms):\n                    meas_basis_change = Program()\n                    qubits_to_measure = []\n                    if term.id() == \"\":\n                        meas_outcome = 1.0\n                    else:\n                        for index, gate in term:\n                            qubits_to_measure.append(index)\n                            if gate == 'X':\n                                meas_basis_change.inst(RY(-np.pi / 2, index))\n                            elif gate == 'Y':\n                                meas_basis_change.inst(RX(np.pi / 2, index))\n\n                            meas_outcome = \\\n                                expectation_from_sampling(pyquil_prog + meas_basis_change,\n                                                          qubits_to_measure,\n                                                          qc,\n                                                          samples)\n\n                    expectation += term.coefficient * meas_outcome\n\n                return expectation.real", "response": "Compute the expectation value of a PauliSum or PauliTerm."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_path_from_parent(self, parent):\n    if hasattr(self, 'get_path_from_parent'):\n        return self.get_path_from_parent(parent)\n    if self.model is parent:\n        return []\n    model = self.concrete_model\n    # Get a reversed base chain including both the current and parent\n    # models.\n    chain = model._meta.get_base_chain(parent) or []\n    chain.reverse()\n    chain.append(model)\n    # Construct a list of the PathInfos between models in chain.\n    path = []\n    for i, ancestor in enumerate(chain[:-1]):\n        child = chain[i + 1]\n        link = child._meta.get_ancestor_link(ancestor)\n        path.extend(link.get_reverse_path_info())\n    return path", "response": "Return a list of PathInfos containing the path from the parent to the current model."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the node with the given id.", "response": "def get_node(cls, info, id):\n        \"\"\"\n        Bear in mind that if you are overriding this method get_node(info, pk),\n        you should always call maybe_optimize(info, qs, pk)\n        and never directly call get_optimized_node(info, qs, pk) as it would\n        result to the node being attempted to be optimized when it is not\n        supposed to actually get optimized.\n\n        :param info:\n        :param id:\n        :return:\n        \"\"\"\n        return cls.maybe_optimize(info, cls._meta.model.objects, id)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef patience_sort(xs):\n    '''Patience sort an iterable, xs.\n\n    This function generates a series of pairs (x, pile), where \"pile\"\n    is the 0-based index of the pile \"x\" should be placed on top of.\n    Elements of \"xs\" must be less-than comparable.\n    '''\n    pile_tops = list()\n    for x in xs:\n        pile = bisect.bisect_left(pile_tops, x)\n        if pile == len(pile_tops):\n            pile_tops.append(x)\n        else:\n            pile_tops[pile] = x\n        yield x, pile", "response": "Patience sort an iterable xs."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef longest_monotonic_subseq_length(xs):\n    '''Return the length of the longest monotonic subsequence of xs, second\n    return value is the difference between increasing and decreasing lengths.\n\n    >>> longest_monotonic_subseq_length((4, 5, 1, 2, 3))\n    (3, 1)\n    >>> longest_monotonic_subseq_length((1, 2, 3, 5, 4))\n    (4, 2)\n    >>> longest_monotonic_subseq_length((1, 2, 1))\n    (2, 0)\n    '''\n    li = longest_increasing_subseq_length(xs)\n    ld = longest_decreasing_subseq_length(xs)\n    return max(li, ld), li - ld", "response": "Return the length of the longest monotonic subsequence of xs second\n    return value is the difference between increasing and decreasing lengths."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef longest_increasing_subsequence(xs):\n    '''Return a longest increasing subsequence of xs.\n\n    (Note that there may be more than one such subsequence.)\n    >>> longest_increasing_subsequence(range(3))\n    [0, 1, 2]\n    >>> longest_increasing_subsequence([3, 1, 2, 0])\n    [1, 2]\n    '''\n    # Patience sort xs, stacking (x, prev_ix) pairs on the piles.\n    # Prev_ix indexes the element at the top of the previous pile,\n    # which has a lower x value than the current x value.\n    piles = [[]]  # Create a dummy pile 0\n    for x, p in patience_sort(xs):\n        if p + 1 == len(piles):\n            piles.append([])\n        # backlink to the top of the previous pile\n        piles[p + 1].append((x, len(piles[p]) - 1))\n    # Backtrack to find a longest increasing subsequence\n    npiles = len(piles) - 1\n    prev = 0\n    lis = list()\n    for pile in range(npiles, 0, -1):\n        x, prev = piles[pile][prev]\n        lis.append(x)\n    lis.reverse()\n    return lis", "response": "Return a longest increasing subsequence of xs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef backtracking(a, L, bestsofar):\n    w, j = max(L.items())\n    while j != -1:\n        yield j\n        w, j = bestsofar[j]", "response": "Yields the next set of entries in the given dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the heaviest increasing subsequence for array a. Elements are ( key weight ) pairs.", "response": "def heaviest_increasing_subsequence(a, debug=False):\n    \"\"\"\n    Returns the heaviest increasing subsequence for array a. Elements are (key,\n    weight) pairs.\n\n    >>> heaviest_increasing_subsequence([(3, 3), (2, 2), (1, 1), (0, 5)])\n    ([(0, 5)], 5)\n    \"\"\"\n    # Stores the smallest idx of last element of a subsequence of weight w\n    L = {0: -1}\n    bestsofar = [(0, -1)] * len(a)  # (best weight, from_idx)\n    for i, (key, weight) in enumerate(a):\n\n        for w, j in L.items():\n            if j != -1 and a[j][0] >= key:\n                continue\n\n            new_weight = w + weight\n            if new_weight in L and a[L[new_weight]][0] <= key:\n                continue\n\n            L[new_weight] = i\n            newbest = (new_weight, j)\n            if newbest > bestsofar[i]:\n                bestsofar[i] = newbest\n\n        if debug:\n            #print (key, weight), L\n            print((key, weight), bestsofar)\n\n    tb = reversed(list(backtracking(a, L, bestsofar)))\n    return [a[x] for x in tb], max(L.items())[0]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mappability(args):\n    p = OptionParser(mappability.__doc__)\n    p.add_option(\"--mer\", default=50, type=\"int\", help=\"User mer size\")\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    ref, = args\n    K = opts.mer\n    pf = ref.rsplit(\".\", 1)[0]\n    mm = MakeManager()\n\n    gem = pf + \".gem\"\n    cmd = \"gem-indexer -i {} -o {}\".format(ref, pf)\n    mm.add(ref, gem, cmd)\n\n    mer = pf + \".{}mer\".format(K)\n    mapb = mer + \".mappability\"\n    cmd = \"gem-mappability -I {} -l {} -o {} -T {}\".\\\n                format(gem, K, mer, opts.cpus)\n    mm.add(gem, mapb, cmd)\n\n    wig = mer + \".wig\"\n    cmd = \"gem-2-wig -I {} -i {} -o {}\".format(gem, mapb, mer)\n    mm.add(mapb, wig, cmd)\n\n    bw = mer + \".bw\"\n    cmd = \"wigToBigWig {} {}.sizes {}\".format(wig, mer, bw)\n    mm.add(wig, bw, cmd)\n\n    bg = mer + \".bedGraph\"\n    cmd = \"bigWigToBedGraph {} {}\".format(bw, bg)\n    mm.add(bw, bg, cmd)\n\n    merged = mer + \".filtered-1.merge.bed\"\n    cmd = \"python -m jcvi.formats.bed filterbedgraph {} 1\".format(bg)\n    mm.add(bg, merged, cmd)\n\n    mm.write()", "response": "%prog mappability reference. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gatk(args):\n    p = OptionParser(gatk.__doc__)\n    p.add_option(\"--indelrealign\", default=False, action=\"store_true\",\n                 help=\"Perform indel realignment\")\n    p.set_home(\"gatk\")\n    p.set_home(\"picard\")\n    p.set_phred()\n    p.set_cpus(cpus=24)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    bamfile, ref = args\n    pf = bamfile.rsplit(\".\", 1)[0]\n    mm = MakeManager()\n    picard = \"java -Xmx32g -jar {0}/picard.jar\".format(opts.picard_home)\n    tk = \"java -Xmx32g -jar {0}/GenomeAnalysisTK.jar\".format(opts.gatk_home)\n    tk += \" -R {0}\".format(ref)\n\n    # Step 0 - build reference\n    dictfile = ref.rsplit(\".\", 1)[0] + \".dict\"\n    cmd1 = picard + \" CreateSequenceDictionary\"\n    cmd1 += \" R={0} O={1}\".format(ref, dictfile)\n    cmd2 = \"samtools faidx {0}\".format(ref)\n    mm.add(ref, dictfile, (cmd1, cmd2))\n\n    # Step 1 - sort bam\n    sortedbamfile = pf + \".sorted.bam\"\n    cmd = picard + \" SortSam\"\n    cmd += \" INPUT={0} OUTPUT={1}\".format(bamfile, sortedbamfile)\n    cmd += \" SORT_ORDER=coordinate CREATE_INDEX=true\"\n    mm.add(bamfile, sortedbamfile, cmd)\n\n    # Step 2 - mark duplicates\n    dedupbamfile = pf + \".dedup.bam\"\n    cmd = picard + \" MarkDuplicates\"\n    cmd += \" INPUT={0} OUTPUT={1}\".format(sortedbamfile, dedupbamfile)\n    cmd += \" METRICS_FILE=dedup.log CREATE_INDEX=true\"\n    mm.add(sortedbamfile, dedupbamfile, cmd)\n\n    if opts.indelrealign:\n        # Step 3 - create indel realignment targets\n        intervals = pf + \".intervals\"\n        cmd = tk + \" -T RealignerTargetCreator\"\n        cmd += \" -I {0} -o {1}\".format(dedupbamfile, intervals)\n        mm.add(dedupbamfile, intervals, cmd)\n\n        # Step 4 - indel realignment\n        realignedbamfile = pf + \".realigned.bam\"\n        cmd = tk + \" -T IndelRealigner\"\n        cmd += \" -targetIntervals {0}\".format(intervals)\n        cmd += \" -I {0} -o {1}\".format(dedupbamfile, realignedbamfile)\n        mm.add((dictfile, intervals), realignedbamfile, cmd)\n    else:\n        realignedbamfile = dedupbamfile\n\n    # Step 5 - SNP calling\n    vcf = pf + \".vcf\"\n    cmd = tk + \" -T HaplotypeCaller\"\n    cmd += \" -I {0}\".format(realignedbamfile)\n    cmd += \" --genotyping_mode DISCOVERY\"\n    cmd += \" -stand_emit_conf 10 -stand_call_conf 30\"\n    cmd += \" -nct {0}\".format(opts.cpus)\n    cmd += \" -o {0}\".format(vcf)\n    if opts.phred == \"64\":\n        cmd += \" --fix_misencoded_quality_scores\"\n    mm.add(realignedbamfile, vcf, cmd)\n\n    # Step 6 - SNP filtering\n    filtered_vcf = pf + \".filtered.vcf\"\n    cmd = tk + \" -T VariantFiltration\"\n    cmd += \" -V {0}\".format(vcf)\n    cmd += ' --filterExpression \"DP < 10 || DP > 300 || QD < 2.0 || FS > 60.0 || MQ < 40.0\"'\n    cmd += ' --filterName \"LOWQUAL\"'\n    cmd += ' --genotypeFilterExpression \"isHomVar == 1\"'\n    cmd += ' --genotypeFilterName \"HOMOVAR\"'\n    cmd += ' --genotypeFilterExpression \"isHet == 1\"'\n    cmd += ' --genotypeFilterName \"HET\"'\n    cmd += \" -o {0}\".format(filtered_vcf)\n    mm.add(vcf, filtered_vcf, cmd)\n\n    mm.write()", "response": "%prog gatk bamfile reference. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef somatic(args):\n    p = OptionParser(somatic.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) < 3:\n        sys.exit(not p.print_help())\n\n    ref, bams = args[0], args[1:]\n    tcmd = \"~/export/speedseq/bin/speedseq somatic\"\n    tcmd += \" -t 32 -F .2 -C 3 -q 30\"\n    cmds = []\n    for b in bams:\n        pf = b.split(\".\")[0]\n        cmd = tcmd\n        cmd += \" -o {0}\".format(pf)\n        others = \",\".join(sorted(set(bams) - set([b])))\n        cmd += \" {0} {1} {2}\".format(ref, others, b)\n        cmds.append(cmd)\n\n    write_file(\"somatic.sh\", \"\\n\".join(cmds))", "response": "%prog somatic ref. fasta bams > somatic. sh"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rmdup(args):\n    p = OptionParser(rmdup.__doc__)\n    p.add_option(\"-S\", default=False, action=\"store_true\",\n                 help=\"Treat PE reads as SE in rmdup\")\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    bams = args\n    cmd = \"samtools rmdup\"\n    if opts.S:\n        cmd += \" -S\"\n    for b in bams:\n        if \"rmdup\" in b:\n            continue\n        rb = b.rsplit(\".\", 1)[0] + \".rmdup.bam\"\n        if not need_update(b, rb):\n            continue\n        print(\" \".join((cmd, b, rb)))", "response": "%prog rmdup bams > rmdup. cmds"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef freq(args):\n    p = OptionParser(freq.__doc__)\n    p.add_option(\"--mindepth\", default=3, type=\"int\",\n                 help=\"Minimum depth [default: %default]\")\n    p.add_option(\"--minqual\", default=20, type=\"int\",\n                 help=\"Minimum quality [default: %default]\")\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    fastafile, bamfile = args\n    cmd = \"freebayes -f {0} --pooled-continuous {1}\".format(fastafile, bamfile)\n    cmd += \" -F 0 -C {0}\".format(opts.mindepth)\n    cmd += ' | vcffilter -f \"QUAL > {0}\"'.format(opts.minqual)\n    cmd += \" | vcfkeepinfo - AO RO TYPE\"\n    sh(cmd, outfile=opts.outfile)", "response": "%prog freq fastafile bamfile"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget database username password from. sqshrc file e. g. user username password", "response": "def get_profile(sqshrc=\"~/.sqshrc\", connector='Sybase',\n                hostname=None, username=None, password=None):\n    \"\"\"\n    get database, username, password from .sqshrc file e.g.\n    \\set username=\"user\"\n    \"\"\"\n    if connector == 'Sybase':\n        shost, suser, spass = None, None, None\n        _ = lambda x: x.split(\"=\")[-1].translate(None, \"\\\"'\").strip()\n        sqshrc = op.expanduser(sqshrc)\n        if op.exists(sqshrc):\n            for row in open(sqshrc):\n                row = row.strip()\n                if not row.startswith(\"\\\\set\") or \"prompt\" in row:\n                    continue\n                if \"password\" in row:\n                    spass = _(row)\n                if \"hostname\" in row:\n                    shost = _(row)\n                if \"username\" in row:\n                    suser = _(row)\n        else:\n            print(\"[warning] file `{0}` not found\".format(sqshrc), file=sys.stderr)\n\n        if suser and spass:\n            username, password = suser, spass\n        if shost:\n            hostname = shost\n\n    dhost, duser, dpass = db_defaults(connector=connector)\n    if not password:\n        username, password = duser, dpass\n    elif not username:\n        username = getusername()\n\n    if not hostname:\n        hostname = dhost\n\n    return hostname, username, password"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pull(args):\n    p = OptionParser(pull.__doc__)\n    p.set_db_opts(dbname=\"mtg2\", credentials=None)\n    p.add_option(\"--frag\", default=False, action=\"store_true\",\n            help=\"The command to pull sequences from db [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    libfile, = args\n\n    dbname = opts.dbname\n    frag = opts.frag\n    fp = open(libfile)\n    hostname, username, password = get_profile()\n\n    for row in fp:\n        lib_id, name = row.split(\"|\", 1)\n        sqlfile = lib_id + \".sql\"\n\n        if not op.exists(sqlfile):\n            fw = open(sqlfile, \"w\")\n            print(\"select seq_name from sequence where seq_name like\" + \\\n                    \" '{0}%' and trash is null\".format(lib_id), file=fw)\n            fw.close()\n\n        if frag:\n            cmd = \"pullfrag -D {0} -n {1}.sql -o {1} -q -S {2}\".format(dbname, lib_id, hostname)\n            cmd += \" -U {0} -P {1}\".format(username, password)\n        else:\n            cmd = \"pullseq -D {0} -n {1}.sql -o {1} -q\".format(dbname, lib_id)\n        sh(cmd)", "response": "%prog pull libfile\n\n    Pull the sequences using the first column in the libfile."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_record(fp, first_line=None):\n\n    if first_line is None:\n        first_line = fp.readline()\n\n    if not first_line:\n        raise EOFError()\n\n    match = _START.match(first_line)\n    if not match:\n        raise Exception('Bad start of message', first_line)\n\n    type = match.group(1)\n    message = Message(type)\n\n    while True:\n        row = fp.readline()\n\n        match = _MULTILINE_FIELD.match(row)\n        if match:\n            key = match.group(1)\n            val = \"\"\n            while row:\n                pos = fp.tell()\n                row = fp.readline()\n                if row[0] in '.':\n                    break\n                elif row[0] in '{}':\n                    fp.seek(pos)  # put the line back\n                    break\n                val += row\n            message.contents.append((key, val, True))\n            continue\n\n        match = _FIELD.match(row)\n        if match:\n            key, val = match.group(1), match.group(2)\n            message.contents.append((key, val, False))\n            continue\n\n        match = _START.match(row)\n        if match:\n            message.append(read_record(fp, row))\n            continue\n\n        if row[0] == '}':\n            break\n\n        raise Exception('Bad line', row)\n\n    return message", "response": "Read a record from a file of AMOS messages and return a Message object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef filter(args):\n    p = OptionParser(filter.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    frgfile, idsfile = args\n    assert frgfile.endswith(\".frg\")\n\n    fp = open(idsfile)\n    allowed = set(x.strip() for x in fp)\n    logging.debug(\"A total of {0} allowed ids loaded.\".format(len(allowed)))\n\n    newfrgfile = frgfile.replace(\".frg\", \".filtered.frg\")\n    fp = open(frgfile)\n    fw = open(newfrgfile, \"w\")\n\n    nfrags, discarded_frags = 0, 0\n    nmates, discarded_mates = 0, 0\n    for rec in iter_records(fp):\n        if rec.type == \"FRG\":\n            readname = rec.get_field(\"acc\")\n            readname = readname.rstrip(\"ab\")\n            nfrags += 1\n            if readname not in allowed:\n                discarded_frags += 1\n                continue\n        if rec.type == \"LKG\":\n            readname = rec.get_field(\"frg\")\n            readname = readname.rstrip(\"ab\")\n            nmates += 1\n            if readname not in allowed:\n                discarded_mates += 1\n                continue\n        print(rec, file=fw)\n\n    # Print out a summary\n    survived_frags = nfrags - discarded_frags\n    survived_mates = nmates - discarded_mates\n    print(\"Survived fragments: {0}\".\\\n            format(percentage(survived_frags, nfrags)), file=sys.stderr)\n    print(\"Survived mates: {0}\".\\\n            format(percentage(survived_mates, nmates)), file=sys.stderr)", "response": "%prog filter frgfile idsfile"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef asm(args):\n    p = OptionParser(asm.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(p.print_help())\n\n    asmfile, = args\n    prefix = asmfile.rsplit(\".\", 1)[0]\n    ctgfastafile = prefix + \".ctg.fasta\"\n    scffastafile = prefix + \".scf.fasta\"\n    fp = open(asmfile)\n    ctgfw = open(ctgfastafile, \"w\")\n    scffw = open(scffastafile, \"w\")\n\n    for rec in iter_records(fp):\n        type = rec.type\n        if type == \"CCO\":\n            fw = ctgfw\n            pp = \"ctg\"\n        elif type == \"SCF\":\n            fw = scffw\n            pp = \"scf\"\n        else:\n            continue\n\n        id = rec.get_field(\"acc\")\n        id = id.translate(None, \"()\").split(\",\")[0]\n        seq = rec.get_field(\"cns\").translate(None, \"-\")\n        s = SeqRecord(Seq(seq), id=pp + id, description=\"\")\n        SeqIO.write([s], fw, \"fasta\")\n        fw.flush()\n\n    fw.close()", "response": "%prog asm asmfile\n\n    Extract FASTA sequences from asm reads."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prepare(args):\n    p = OptionParser(prepare.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    counts, families = args\n    countfiles = glob(op.join(counts, \"*.count\"))\n    countsdb = defaultdict(list)\n    for c in countfiles:\n        rs = RiceSample(c)\n        countsdb[(rs.tissue, rs.ind)].append(rs)\n\n    # Merge duplicates - data sequenced in different batches\n    key = lambda x: (x.label, x.rep)\n    for (tissue, ind), rs in sorted(countsdb.items()):\n        rs.sort(key=key)\n        nrs = len(rs)\n        for i in xrange(nrs):\n            ri = rs[i]\n            if not ri.working:\n                continue\n            for j in xrange(i + 1, nrs):\n                rj = rs[j]\n                if key(ri) != key(rj):\n                    continue\n                ri.merge(rj)\n                rj.working = False\n        countsdb[(tissue, ind)] = [x for x in rs if x.working]\n\n    # Group into families\n    mkdir(\"families\")\n    for (tissue, ind), r in sorted(countsdb.items()):\n        r = list(r)\n        if r[0].label != \"F1\":\n            continue\n        P1, P2 = r[0].P1, r[0].P2\n        P1, P2 = countsdb[(tissue, P1)], countsdb[(tissue, P2)]\n        rs = P1 + P2 + r\n        groups = [1] * len(P1) + [2] * len(P2) + [3] * len(r)\n        assert len(rs) == len(groups)\n\n        outfile = \"-\".join((tissue, ind))\n        merge_counts(rs, op.join(families, outfile))\n        groupsfile = outfile + \".groups\"\n        fw = open(op.join(families, groupsfile), \"w\")\n        print(\",\".join(str(x) for x in groups), file=fw)\n        fw.close()", "response": "%prog prepare countfolder families"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef choose_insertsize(readlen=150, step=20, cutoff=.01):\n    print(\"Insert-size\\tOverlap\", file=sys.stderr)\n    for i in range(0, 3 * readlen, step):\n        p = gaussian_prob_le(i, i / 5, 2 * readlen)\n        if p < cutoff or p > 1 - cutoff:\n            continue\n        print(\"{0}bp\\t{1}%\".format(i, int(round(100 * p))), file=sys.stderr)", "response": "Choose insert size for a set of read lengths."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reject_outliers(a, threshold=3.5):\n    if len(a) < 3:\n        return np.zeros(len(a), dtype=bool)\n\n    A = np.array(a, dtype=float)\n    lb, ub = outlier_cutoff(A, threshold=threshold)\n    return np.logical_or(A > ub, A < lb)", "response": "Iglewicz and Hoaglin s robust test for multiple outliers."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef outlier_cutoff(a, threshold=3.5):\n    A = np.array(a, dtype=float)\n    M = np.median(A)\n    D = np.absolute(A - M)\n    MAD = np.median(D)\n    C = threshold / .67449 * MAD\n    return M - C, M + C", "response": "Returns the cutoff values - lower bound and upper bound of the outlier of a given array."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef recomb_probability(cM, method=\"kosambi\"):\n    assert method in (\"kosambi\", \"haldane\")\n    d = cM / 100.\n    if method == \"kosambi\":\n        e4d = exp(4 * d)\n        return (e4d - 1) / (e4d + 1) / 2\n    elif method == \"haldane\":\n        return (1 - exp(-2 * d)) / 2", "response": "Re - combination of the probability of a given CPU level."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the D and variance D given a parameter p.", "response": "def jukesCantorD(p, L=100):\n    \"\"\"\n    >>> jukesCantorD(.1)\n    (0.10732563273050497, 0.001198224852071006)\n    >>> jukesCantorD(.7)\n    (2.0310376508266565, 0.47249999999999864)\n    \"\"\"\n    assert 0 <= p < .75\n\n    rD = 1 - 4. / 3 * p\n    D = -.75 * log(rD)\n    varD = p * (1 - p) / (rD ** 2 * L)\n\n    return D, varD"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef velvet(readsize, genomesize, numreads, K):\n    ram = -109635 + 18977 * readsize + 86326 * genomesize + \\\n            233353 * numreads - 51092 * K\n    print(\"ReadSize: {0}\".format(readsize), file=sys.stderr)\n    print(\"GenomeSize: {0}Mb\".format(genomesize), file=sys.stderr)\n    print(\"NumReads: {0}M\".format(numreads), file=sys.stderr)\n    print(\"K: {0}\".format(K), file=sys.stderr)\n\n    ram = human_size(ram * 1000, a_kilobyte_is_1024_bytes=True)\n    print(\"RAM usage: {0} (MAXKMERLENGTH=31)\".format(ram), file=sys.stderr)", "response": "Calculate the velvet memory requirement for a single sequence of reads."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets BED file from a list of genelist.", "response": "def get_bed_from_phytozome(genelist):\n    \"\"\"\n    >>> data = get_bed_from_phytozome([\"AT5G54690\", \"AT1G01010\"])\n    >>> print data.read()  #doctest: +NORMALIZE_WHITESPACE\n    Chr1\t3631\t5899\tAT1G01010\n    Chr5\t22219224\t22221840\tAT5G54690\n    <BLANKLINE>\n    \"\"\"\n    genelist = \",\".join(genelist)\n    dataset = get_phytozome_dataset()\n    filters = dict(gene_name_filter=genelist)\n    attributes = \"chr_name1,gene_chrom_start,gene_chrom_end,gene_name1\".split(\",\")\n\n    data = dataset.query(filters=filters, attributes=attributes)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bed(args):\n    p = OptionParser(bed.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    idsfile, = args\n    ids = set(x.strip() for x in open(idsfile))\n    data = get_bed_from_phytozome(list(ids))\n\n    pf = idsfile.rsplit(\".\", 1)[0]\n    bedfile = pf + \".bed\"\n    fw = open(bedfile, \"w\")\n    for i, row in enumerate(data):\n        row = row.strip()\n        if row == \"\":\n            continue\n\n        print(row, file=fw)\n\n    logging.debug(\"A total of {0} records written to `{1}`.\".format(i + 1, bedfile))", "response": "%prog bed genes. ids"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bed(args):\n    p = OptionParser(bed.__doc__)\n    p.add_option(\"-o\", dest=\"output\", default=\"stdout\",\n            help=\"Output file name [default: %default]\")\n    p.add_option(\"--cutoff\", dest=\"cutoff\", default=10, type=\"int\",\n            help=\"Minimum read depth to report intervals [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    binfile, fastafile = args\n    fw = must_open(opts.output, \"w\")\n    cutoff = opts.cutoff\n    assert cutoff >= 0, \"Need non-negative cutoff\"\n\n    b = BinFile(binfile)\n    ar = b.array\n\n    fastasize, sizes, offsets = get_offsets(fastafile)\n    s = Sizes(fastafile)\n    for ctg, ctglen in s.iter_sizes():\n        offset = offsets[ctg]\n        subarray = ar[offset:offset + ctglen]\n        key = lambda x: x[1] >= cutoff\n        for tf, array_elements in groupby(enumerate(subarray), key=key):\n            array_elements = list(array_elements)\n            if not tf:\n                continue\n\n            # 0-based system => 1-based system\n            start = array_elements[0][0] + 1\n            end = array_elements[-1][0] + 1\n\n            mean_depth = sum([x[1] for x in array_elements]) / \\\n                len(array_elements)\n            mean_depth = int(mean_depth)\n\n            name = \"na\"\n            print(\"\\t\".join(str(x) for x in (ctg, \\\n                    start - 1, end, name, mean_depth)), file=fw)", "response": "%prog bed binfile fastafile"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef merge(args):\n    p = OptionParser(merge.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) < 2:\n        sys.exit(not p.print_help())\n\n    binfiles = args[:-1]\n    mergedbin = args[-1]\n    if op.exists(mergedbin):\n        logging.error(\"`{0}` file exists. Remove before proceed.\"\\\n                .format(mergedbin))\n        return\n\n    b = BinFile(binfiles[0])\n    ar = b.mmarray\n    fastasize, = ar.shape\n    logging.debug(\"Initialize array of uint16 with size {0}\".format(fastasize))\n\n    merged_ar = np.zeros(fastasize, dtype=np.uint16)\n    for binfile in binfiles:\n        b = BinFile(binfile)\n        merged_ar += b.array\n\n    logging.debug(\"Resetting the count max to 255.\")\n    merged_ar[merged_ar > 255] = 255\n\n    logging.debug(\"Compact array back to uint8 with size {0}\".format(fastasize))\n    merged_ar = np.array(merged_ar, dtype=np.uint8)\n    merged_ar.tofile(mergedbin)\n    logging.debug(\"Merged array written to `{0}`\".format(mergedbin))", "response": "%prog merge binfiles > new. bin"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef query(args):\n    p = OptionParser(query.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 4:\n        sys.exit(not p.print_help())\n\n    binfile, fastafile, ctgID, baseID = args\n    b = BinFile(binfile, fastafile)\n    ar = b.mmarray\n\n    fastasize, sizes, offsets = get_offsets(fastafile)\n    oi = offsets[ctgID] + int(baseID) - 1\n    print(\"\\t\".join((ctgID, baseID, str(ar[oi]))))", "response": "%prog query binfile ctgID baseID"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef count(args):\n    p = OptionParser(count.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    coveragefile, fastafile = args\n\n    countsfile = coveragefile.split(\".\")[0] + \".bin\"\n    if op.exists(countsfile):\n        logging.error(\"`{0}` file exists. Remove before proceed.\"\\\n                .format(countsfile))\n        return\n\n    fastasize, sizes, offsets = get_offsets(fastafile)\n    logging.debug(\"Initialize array of uint8 with size {0}\".format(fastasize))\n    ar = np.zeros(fastasize, dtype=np.uint8)\n\n    update_array(ar, coveragefile, sizes, offsets)\n\n    ar.tofile(countsfile)\n    logging.debug(\"Array written to `{0}`\".format(countsfile))", "response": "%prog count coveragefile fastafile"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a list of edges into a path.", "response": "def edges_to_path(edges):\n    \"\"\"\n    Connect edges and return a path.\n    \"\"\"\n    if not edges:\n        return None\n\n    G = edges_to_graph(edges)\n    path = nx.topological_sort(G)\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a list of edges into a Hamiltonian Path.", "response": "def hamiltonian(edges, directed=False, constraint_generation=True):\n    \"\"\"\n    Calculates shortest path that traverses each node exactly once. Convert\n    Hamiltonian path problem to TSP by adding one dummy point that has a distance\n    of zero to all your other points. Solve the TSP and get rid of the dummy\n    point - what remains is the Hamiltonian Path.\n\n    >>> g = [(1,2), (2,3), (3,4), (4,2), (3,5)]\n    >>> hamiltonian(g)\n    [1, 2, 4, 3, 5]\n    >>> g = [(1,2), (2,3), (1,4), (2,5), (3,6)]\n    >>> hamiltonian(g)\n    \"\"\"\n    edges = populate_edge_weights(edges)\n    incident, nodes = node_to_edge(edges, directed=False)\n    if not directed:  # Make graph symmetric\n        dual_edges = edges[:]\n        for a, b, w in edges:\n            dual_edges.append((b, a, w))\n        edges = dual_edges\n\n    DUMMY = \"DUMMY\"\n    dummy_edges = edges + [(DUMMY, x, 0) for x in nodes] + \\\n                          [(x, DUMMY, 0) for x in nodes]\n\n    #results = tsp(dummy_edges, constraint_generation=constraint_generation)\n    results = tsp_gurobi(dummy_edges)\n    if results:\n        results = [x for x in results if DUMMY not in x]\n        results = edges_to_path(results)\n        if not directed:\n            results = min(results, results[::-1])\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmodeling using GUROBI python example.", "response": "def tsp_gurobi(edges):\n    \"\"\"\n    Modeled using GUROBI python example.\n    \"\"\"\n    from gurobipy import Model, GRB, quicksum\n\n    edges = populate_edge_weights(edges)\n    incoming, outgoing, nodes = node_to_edge(edges)\n    idx = dict((n, i) for i, n in enumerate(nodes))\n    nedges = len(edges)\n    n = len(nodes)\n\n    m = Model()\n\n    def step(x): return \"u_{0}\".format(x)\n    # Create variables\n    vars = {}\n    for i, (a, b, w) in enumerate(edges):\n        vars[i] = m.addVar(obj=w, vtype=GRB.BINARY, name=str(i))\n    for u in nodes[1:]:\n        u = step(u)\n        vars[u] = m.addVar(obj=0, vtype=GRB.INTEGER, name=u)\n    m.update()\n\n    # Bounds for step variables\n    for u in nodes[1:]:\n        u = step(u)\n        vars[u].lb = 1\n        vars[u].ub = n - 1\n\n    # Add degree constraint\n    for v in nodes:\n        incoming_edges = incoming[v]\n        outgoing_edges = outgoing[v]\n        m.addConstr(quicksum(vars[x] for x in incoming_edges) == 1)\n        m.addConstr(quicksum(vars[x] for x in outgoing_edges) == 1)\n\n    # Subtour elimination\n    edge_store = dict(((idx[a], idx[b]), i)\n                      for i, (a, b, w) in enumerate(edges))\n\n    # Given a list of edges, finds the shortest subtour\n    def subtour(s_edges):\n        visited = [False] * n\n        cycles = []\n        lengths = []\n        selected = [[] for i in range(n)]\n        for x, y in s_edges:\n            selected[x].append(y)\n        while True:\n            current = visited.index(False)\n            thiscycle = [current]\n            while True:\n                visited[current] = True\n                neighbors = [x for x in selected[current] if not visited[x]]\n                if len(neighbors) == 0:\n                    break\n                current = neighbors[0]\n                thiscycle.append(current)\n            cycles.append(thiscycle)\n            lengths.append(len(thiscycle))\n            if sum(lengths) == n:\n                break\n        return cycles[lengths.index(min(lengths))]\n\n    def subtourelim(model, where):\n        if where != GRB.callback.MIPSOL:\n            return\n        selected = []\n        # make a list of edges selected in the solution\n        sol = model.cbGetSolution([model._vars[i] for i in range(nedges)])\n        selected = [edges[i] for i, x in enumerate(sol) if x > .5]\n        selected = [(idx[a], idx[b]) for a, b, w in selected]\n        # find the shortest cycle in the selected edge list\n        tour = subtour(selected)\n        if len(tour) == n:\n            return\n        # add a subtour elimination constraint\n        c = tour\n        incident = [edge_store[a, b] for a, b in pairwise(c + [c[0]])]\n        model.cbLazy(quicksum(model._vars[x]\n                              for x in incident) <= len(tour) - 1)\n\n    m.update()\n\n    m._vars = vars\n    m.params.LazyConstraints = 1\n    m.optimize(subtourelim)\n\n    selected = [v.varName for v in m.getVars() if v.x > .5]\n    selected = [int(x) for x in selected if x[:2] != \"u_\"]\n    results = sorted(x for i, x in enumerate(edges) if i in selected) \\\n        if selected else None\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tsp(edges, constraint_generation=False):\n    edges = populate_edge_weights(edges)\n    incoming, outgoing, nodes = node_to_edge(edges)\n\n    nedges, nnodes = len(edges), len(nodes)\n    L = LPInstance()\n\n    L.add_objective(edges, objective=MINIMIZE)\n    balance = []\n    # For each node, select exactly 1 incoming and 1 outgoing edge\n    for v in nodes:\n        incoming_edges = incoming[v]\n        outgoing_edges = outgoing[v]\n        icc = summation(incoming_edges)\n        occ = summation(outgoing_edges)\n        balance.append(\"{0} = 1\".format(icc))\n        balance.append(\"{0} = 1\".format(occ))\n\n    # Subtour elimination - Miller-Tucker-Zemlin (MTZ) formulation\n    # <http://en.wikipedia.org/wiki/Travelling_salesman_problem>\n    # Desrochers and laporte, 1991 (DFJ) has a stronger constraint\n    # See also:\n    # G. Laporte / The traveling salesman problem: Overview of algorithms\n    start_step = nedges + 1\n    u0 = nodes[0]\n    nodes_to_steps = dict((n, start_step + i) for i, n in enumerate(nodes[1:]))\n    edge_store = dict((e[:2], i) for i, e in enumerate(edges))\n    mtz = []\n    for i, e in enumerate(edges):\n        a, b = e[:2]\n        if u0 in (a, b):\n            continue\n        na, nb = nodes_to_steps[a], nodes_to_steps[b]\n        con_ab = \" x{0} - x{1} + {2}x{3}\".format(na, nb, nnodes - 1, i + 1)\n        if (b, a) in edge_store:  # This extra term is the stronger DFJ formulation\n            j = edge_store[(b, a)]\n            con_ab += \" + {0}x{1}\".format(nnodes - 3, j + 1)\n        con_ab += \" <= {0}\".format(nnodes - 2)\n        mtz.append(con_ab)\n\n    # Step variables u_i bound between 1 and n, as additional variables\n    bounds = []\n    for i in xrange(start_step, nedges + nnodes):\n        bounds.append(\" 1 <= x{0} <= {1}\".format(i, nnodes - 1))\n\n    L.add_vars(nedges)\n\n    \"\"\"\n    Constraint generation seek to find 'cuts' in the LP problem, by solving the\n    relaxed form. The subtours were then incrementally added to the constraints.\n    \"\"\"\n    if constraint_generation:\n        L.constraints = balance\n        subtours = []\n        while True:\n            selected, obj_val = L.lpsolve()\n            results = sorted(x for i, x in enumerate(edges) if i in selected) \\\n                if selected else None\n            if not results:\n                break\n            G = edges_to_graph(results)\n            cycles = list(nx.simple_cycles(G))\n            if len(cycles) == 1:\n                break\n            for c in cycles:\n                incident = [edge_store[a, b] for a, b in pairwise(c + [c[0]])]\n                icc = summation(incident)\n                subtours.append(\"{0} <= {1}\".format(icc, len(incident) - 1))\n            L.constraints = balance + subtours\n    else:\n        L.constraints = balance + mtz\n        L.add_vars(nnodes - 1, offset=start_step, binary=False)\n        L.bounds = bounds\n        selected, obj_val = L.lpsolve()\n        results = sorted(x for i, x in enumerate(edges) if i in selected) \\\n            if selected else None\n\n    return results", "response": "Calculates the shortest cycle that traverses each node exactly once. Also known as the Traveling Salesman Problem."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating shortest or longest path from a list of edges.", "response": "def path(edges, source, sink, flavor=\"longest\"):\n    \"\"\"\n    Calculates shortest/longest path from list of edges in a graph\n\n    >>> g = [(1,2,1),(2,3,9),(2,4,3),(2,5,2),(3,6,8),(4,6,10),(4,7,4)]\n    >>> g += [(6,8,7),(7,9,5),(8,9,6),(9,10,11)]\n    >>> path(g, 1, 8, flavor=\"shortest\")\n    ([1, 2, 4, 6, 8], 21)\n    >>> path(g, 1, 8, flavor=\"longest\")\n    ([1, 2, 3, 6, 8], 25)\n    \"\"\"\n    outgoing, incoming, nodes = node_to_edge(edges)\n\n    nedges = len(edges)\n    L = LPInstance()\n\n    assert flavor in (\"longest\", \"shortest\")\n\n    objective = MAXIMIZE if flavor == \"longest\" else MINIMIZE\n    L.add_objective(edges, objective=objective)\n\n    # Balancing constraint, incoming edges equal to outgoing edges except\n    # source and sink\n\n    constraints = []\n    for v in nodes:\n        incoming_edges = incoming[v]\n        outgoing_edges = outgoing[v]\n        icc = summation(incoming_edges)\n        occ = summation(outgoing_edges)\n\n        if v == source:\n            if not outgoing_edges:\n                return None\n            constraints.append(\"{0} = 1\".format(occ))\n        elif v == sink:\n            if not incoming_edges:\n                return None\n            constraints.append(\"{0} = 1\".format(icc))\n        else:\n            # Balancing\n            constraints.append(\"{0}{1} = 0\".format(icc, occ.replace('+', '-')))\n            # Simple path\n            if incoming_edges:\n                constraints.append(\"{0} <= 1\".format(icc))\n            if outgoing_edges:\n                constraints.append(\"{0} <= 1\".format(occ))\n\n    L.constraints = constraints\n    L.add_vars(nedges)\n\n    selected, obj_val = L.lpsolve()\n    results = sorted(x for i, x in enumerate(edges) if i in selected) \\\n        if selected else None\n    results = edges_to_path(results)\n\n    return results, obj_val"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef min_feedback_arc_set(edges, remove=False, maxcycles=20000):\n    G = nx.DiGraph()\n    edge_to_index = {}\n    for i, (a, b, w) in enumerate(edges):\n        G.add_edge(a, b)\n        edge_to_index[a, b] = i\n\n    nedges = len(edges)\n    L = LPInstance()\n\n    L.add_objective(edges, objective=MINIMIZE)\n\n    constraints = []\n    ncycles = 0\n    for c in nx.simple_cycles(G):\n        cycle_edges = []\n        rc = c + [c[0]]  # Rotate the cycle\n        for a, b in pairwise(rc):\n            cycle_edges.append(edge_to_index[a, b])\n        cc = summation(cycle_edges)\n        constraints.append(\"{0} >= 1\".format(cc))\n        ncycles += 1\n        if ncycles == maxcycles:\n            break\n    logging.debug(\"A total of {0} cycles found.\".format(ncycles))\n\n    L.constraints = constraints\n    L.add_vars(nedges)\n\n    selected, obj_val = L.lpsolve(clean=False)\n    if remove:\n        results = [x for i, x in enumerate(edges) if i not in selected] \\\n            if selected else None\n    else:\n        results = [x for i, x in enumerate(edges) if i in selected] \\\n            if selected else None\n\n    return results, obj_val", "response": "Return a minimum feedback arc set for a set of edges."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef max_sum(a):\n\n    max_sum, max_start_index, max_end_index = -Infinity, 0, 0\n    current_max_sum = 0\n    current_start_index = 0\n    for current_end_index, x in enumerate(a):\n        current_max_sum += x\n        if current_max_sum > max_sum:\n            max_sum, max_start_index, max_end_index = current_max_sum, \\\n                current_start_index, current_end_index\n        if current_max_sum < 0:\n            current_max_sum = 0\n            current_start_index = current_end_index + 1\n\n    return max_sum, max_start_index, max_end_index", "response": "For an input array a output the range that gives the largest sum\n           "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef condense(args):\n    from itertools import groupby\n    from jcvi.assembly.patch import merge_ranges\n\n    p = OptionParser(condense.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    bedfile, = args\n    bed = Bed(bedfile, sorted=False)\n    key = lambda x: (x.seqid, x.start, x.end)\n    for k, sb in groupby(bed, key=key):\n        sb = list(sb)\n        b = sb[0]\n        chr, start, end, strand = merge_ranges(sb)\n\n        id = \"{0}:{1}-{2}\".format(chr, start, end)\n        b.accn = id\n        print(b)", "response": "%prog condense OM. bed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef chimera(args):\n    p = OptionParser(chimera.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    bedfile, = args\n    bed = Bed(bedfile)\n    selected = select_bed(bed)\n    mapped = defaultdict(set)  # scaffold => chr\n    chimerabed = \"chimera.bed\"\n    fw = open(chimerabed, \"w\")\n    for b in selected:\n        scf = range_parse(b.accn).seqid\n        chr = b.seqid\n        mapped[scf].add(chr)\n\n    nchimera = 0\n    for s, chrs in sorted(mapped.items()):\n        if len(chrs) == 1:\n            continue\n\n        print(\"=\" * 80, file=sys.stderr)\n        print(\"{0} mapped to multiple locations: {1}\".\\\n                format(s, \",\".join(sorted(chrs))), file=sys.stderr)\n        ranges = []\n        for b in selected:\n            rr = range_parse(b.accn)\n            scf = rr.seqid\n            if scf == s:\n                print(b, file=sys.stderr)\n                ranges.append(rr)\n\n        # Identify breakpoints\n        ranges.sort(key=lambda x: (x.seqid, x.start, x.end))\n        for a, b in pairwise(ranges):\n            seqid = a.seqid\n            if seqid != b.seqid:\n                continue\n\n            start, end = a.end, b.start\n            if start > end:\n                start, end = end, start\n\n            chimeraline = \"\\t\".join(str(x) for x in (seqid, start, end))\n            print(chimeraline, file=fw)\n            print(chimeraline, file=sys.stderr)\n            nchimera += 1\n\n    fw.close()\n    logging.debug(\"A total of {0} junctions written to `{1}`.\".\\\n                  format(nchimera, chimerabed))", "response": "%prog chimera bedfile\nAddon"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef select_bed(bed):\n    ranges = [Range(x.seqid, x.start, x.end, float(x.score), i) for i, x in enumerate(bed)]\n    selected, score = range_chain(ranges)\n    selected = [bed[x.id] for x in selected]\n\n    return selected", "response": "Select non - overlapping set of ranges choosing high scoring blocks over low\n    scoring alignments when there are conflicts."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fasta(args):\n    from jcvi.formats.sizes import Sizes\n    from jcvi.formats.agp import OO, build\n\n    p = OptionParser(fasta.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    bedfile, scffasta, pmolfasta = args\n    pf = bedfile.rsplit(\".\", 1)[0]\n    bed = Bed(bedfile)\n    selected = select_bed(bed)\n    oo = OO()\n    seen = set()\n    sizes = Sizes(scffasta).mapping\n    agpfile = pf + \".agp\"\n    agp = open(agpfile, \"w\")\n    for b in selected:\n        scf = range_parse(b.accn).seqid\n        chr = b.seqid\n        cs = (chr, scf)\n        if cs not in seen:\n            oo.add(chr, scf, sizes[scf], b.strand)\n            seen.add(cs)\n        else:\n            logging.debug(\"Seen {0}, ignored.\".format(cs))\n\n    oo.write_AGP(agp, gaptype=\"contig\")\n    agp.close()\n    build([agpfile, scffasta, pmolfasta])", "response": "%prog fasta bedfile scf. fasta pmol. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bam(args):\n    from jcvi.formats.sizes import Sizes\n    from jcvi.formats.sam import index\n\n    p = OptionParser(bam.__doc__)\n    p.set_home(\"eddyyeh\")\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    gsnapfile, fastafile = args\n    EYHOME = opts.eddyyeh_home\n    pf = gsnapfile.rsplit(\".\", 1)[0]\n    uniqsam = pf + \".unique.sam\"\n    samstats = uniqsam + \".stats\"\n    sizesfile = Sizes(fastafile).filename\n    if need_update((gsnapfile, sizesfile), samstats):\n        cmd = op.join(EYHOME, \"gsnap2gff3.pl\")\n        cmd += \" --format sam -i {0} -o {1}\".format(gsnapfile, uniqsam)\n        cmd += \" -u -l {0} -p {1}\".format(sizesfile, opts.cpus)\n        sh(cmd)\n\n    index([uniqsam])\n\n    return uniqsam", "response": "%prog bam gsnapfile ref. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef index(args):\n    p = OptionParser(index.__doc__)\n    p.add_option(\"--supercat\", default=False, action=\"store_true\",\n                 help=\"Concatenate reference to speed up alignment\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    dbfile, = args\n    check_index(dbfile, supercat=opts.supercat)", "response": "%prog index database. fasta\n"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef gmap(args):\n    p = OptionParser(gmap.__doc__)\n    p.add_option(\"--cross\", default=False, action=\"store_true\",\n                 help=\"Cross-species alignment\")\n    p.add_option(\"--npaths\", default=0, type=\"int\",\n                 help=\"Maximum number of paths to show.\"\n                 \" If set to 0, prints two paths if chimera\"\n                 \" detected, else one.\")\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    dbfile, fastafile = args\n    assert op.exists(dbfile) and op.exists(fastafile)\n    prefix = get_prefix(fastafile, dbfile)\n    logfile = prefix + \".log\"\n    gmapfile = prefix + \".gmap.gff3\"\n\n    if not need_update((dbfile, fastafile), gmapfile):\n        logging.error(\"`{0}` exists. `gmap` already run.\".format(gmapfile))\n    else:\n        dbdir, dbname = check_index(dbfile)\n        cmd = \"gmap -D {0} -d {1}\".format(dbdir, dbname)\n        cmd += \" -f 2 --intronlength=100000\"  # Output format 2\n        cmd += \" -t {0}\".format(opts.cpus)\n        cmd += \" --npaths {0}\".format(opts.npaths)\n        if opts.cross:\n            cmd += \" --cross-species\"\n        cmd += \" \" + fastafile\n\n        sh(cmd, outfile=gmapfile, errfile=logfile)\n\n    return gmapfile, logfile", "response": "%prog gmap database. fasta fastafile"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef align(args):\n    from jcvi.formats.fastq import guessoffset\n\n    p = OptionParser(align.__doc__)\n    p.add_option(\"--rnaseq\", default=False, action=\"store_true\",\n                 help=\"Input is RNA-seq reads, turn splicing on\")\n    p.add_option(\"--native\", default=False, action=\"store_true\",\n                 help=\"Convert GSNAP output to NATIVE format\")\n    p.set_home(\"eddyyeh\")\n    p.set_outdir()\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) == 2:\n        logging.debug(\"Single-end alignment\")\n    elif len(args) == 3:\n        logging.debug(\"Paired-end alignment\")\n    else:\n        sys.exit(not p.print_help())\n\n    dbfile, readfile = args[:2]\n    outdir = opts.outdir\n    assert op.exists(dbfile) and op.exists(readfile)\n    prefix = get_prefix(readfile, dbfile)\n    logfile = op.join(outdir, prefix + \".log\")\n    gsnapfile = op.join(outdir, prefix + \".gsnap\")\n    nativefile = gsnapfile.rsplit(\".\", 1)[0] + \".unique.native\"\n\n    if not need_update((dbfile, readfile), gsnapfile):\n        logging.error(\"`{0}` exists. `gsnap` already run.\".format(gsnapfile))\n    else:\n        dbdir, dbname = check_index(dbfile)\n        cmd = \"gsnap -D {0} -d {1}\".format(dbdir, dbname)\n        cmd += \" -B 5 -m 0.1 -i 2 -n 3\"  # memory, mismatch, indel penalty, nhits\n        if opts.rnaseq:\n            cmd += \" -N 1\"\n        cmd += \" -t {0}\".format(opts.cpus)\n        cmd += \" --gmap-mode none --nofails\"\n        if readfile.endswith(\".gz\"):\n            cmd += \" --gunzip\"\n        try:\n            offset = \"sanger\" if guessoffset([readfile]) == 33 else \"illumina\"\n            cmd += \" --quality-protocol {0}\".format(offset)\n        except AssertionError:\n            pass\n        cmd += \" \" + \" \".join(args[1:])\n        sh(cmd, outfile=gsnapfile, errfile=logfile)\n\n    if opts.native:\n        EYHOME = opts.eddyyeh_home\n        if need_update(gsnapfile, nativefile):\n            cmd = op.join(EYHOME, \"convert2native.pl\")\n            cmd += \" --gsnap {0} -o {1}\".format(gsnapfile, nativefile)\n            cmd += \" -proc {0}\".format(opts.cpus)\n            sh(cmd)\n\n    return gsnapfile, logfile", "response": "%prog align database. fasta read1. fq read2. fq"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_1D_overlap(eclusters, depth=1):\n    overlap_set = set()\n    active = set()\n\n    ends = []\n    for i, (chr, left, right) in enumerate(eclusters):\n        ends.append((chr, left, 0, i))  # 0/1 for left/right-ness\n        ends.append((chr, right, 1, i))\n    ends.sort()\n\n    chr_last = \"\"\n    for chr, pos, left_right, i in ends:\n        if chr != chr_last:\n            active.clear()\n        if left_right == 0:\n            active.add(i)\n        else:\n            active.remove(i)\n\n        if len(active) > depth:\n            overlap_set.add(tuple(sorted(active)))\n\n        chr_last = chr\n\n    return overlap_set", "response": "Find blocks that are 1D overlapping returns cliques of block ids that are in conflict\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a set of 2D overlapping blocks.", "response": "def get_2D_overlap(chain, eclusters):\n    \"\"\"\n    Implements a sweep line algorithm, that has better running time than naive O(n^2):\n    assume block has x_ends, and y_ends for the bounds\n\n    1. sort x_ends, and take a sweep line to scan the x_ends\n    2. if left end, test y-axis intersection of current block with `active` set;\n       also put this block in the `active` set\n    3. if right end, remove block from the `active` set\n    \"\"\"\n    mergeables = Grouper()\n    active = set()\n\n    x_ends = []\n    for i, (range_x, range_y, score) in enumerate(eclusters):\n        chr, left, right = range_x\n        x_ends.append((chr, left, 0, i))  # 0/1 for left/right-ness\n        x_ends.append((chr, right, 1, i))\n    x_ends.sort()\n\n    chr_last = \"\"\n    for chr, pos, left_right, i in x_ends:\n        if chr != chr_last:\n            active.clear()\n        if left_right == 0:\n            active.add(i)\n            for x in active:\n                # check y-overlap\n                if range_overlap(eclusters[x][1], eclusters[i][1]):\n                    mergeables.join(x, i)\n        else:  # right end\n            active.remove(i)\n\n        chr_last = chr\n\n    return mergeables"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert to interval ends from a list of anchors extend modifies the xmax ymax boundary of the box", "response": "def make_range(clusters, extend=0):\n    \"\"\"\n    Convert to interval ends from a list of anchors\n    extend modifies the xmax, ymax boundary of the box,\n    which can be positive or negative\n    very useful when we want to make the range as fuzzy as we specify\n    \"\"\"\n    eclusters = []\n    for cluster in clusters:\n        xlist, ylist, scores = zip(*cluster)\n        score = _score(cluster)\n\n        xchr, xmin = min(xlist)\n        xchr, xmax = max(xlist)\n        ychr, ymin = min(ylist)\n        ychr, ymax = max(ylist)\n\n        # allow fuzziness to the boundary\n        xmax += extend\n        ymax += extend\n        # because extend can be negative values, we don't want it to be less than min\n        if xmax < xmin:\n            xmin, xmax = xmax, xmin\n        if ymax < ymin:\n            ymin, ymax = ymax, ymin\n\n        eclusters.append(((xchr, xmin, xmax),\n                          (ychr, ymin, ymax), score))\n\n    return eclusters"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_constraints(clusters, quota=(1, 1), Nmax=0):\n    qa, qb = quota\n    eclusters = make_range(clusters, extend=-Nmax)\n    # (1-based index, cluster score)\n    nodes = [(i+1, c[-1]) for i, c in enumerate(eclusters)]\n\n    eclusters_x, eclusters_y, scores = zip(*eclusters)\n\n    # represents the contraints over x-axis and y-axis\n    constraints_x = get_1D_overlap(eclusters_x, qa)\n    constraints_y = get_1D_overlap(eclusters_y, qb)\n\n    return nodes, constraints_x, constraints_y", "response": "Check pairwise cluster comparison and determine if they overlap"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nformatting the LP data for the given set of constraints.", "response": "def format_lp(nodes, constraints_x, qa, constraints_y, qb):\n    \"\"\"\n    Maximize\n     4 x1 + 2 x2 + 3 x3 + x4\n    Subject To\n     x1 + x2 <= 1\n    End\n    \"\"\"\n    lp_handle = cStringIO.StringIO()\n\n    lp_handle.write(\"Maximize\\n \")\n    records = 0\n    for i, score in nodes:\n        lp_handle.write(\"+ %d x%d \" % (score, i))\n        # SCIP does not like really long string per row\n        records += 1\n        if records % 10 == 0:\n            lp_handle.write(\"\\n\")\n    lp_handle.write(\"\\n\")\n\n    num_of_constraints = 0\n    lp_handle.write(\"Subject To\\n\")\n    for c in constraints_x:\n        additions = \" + \".join(\"x%d\" % (x+1) for x in c)\n        lp_handle.write(\" %s <= %d\\n\" % (additions, qa))\n    num_of_constraints += len(constraints_x)\n\n    # non-self\n    if not (constraints_x is constraints_y):\n        for c in constraints_y:\n            additions = \" + \".join(\"x%d\" % (x+1) for x in c)\n            lp_handle.write(\" %s <= %d\\n\" % (additions, qb))\n        num_of_constraints += len(constraints_y)\n\n    print(\"number of variables (%d), number of constraints (%d)\" %\n          (len(nodes), num_of_constraints), file=sys.stderr)\n\n    lp_handle.write(\"Binary\\n\")\n    for i, score in nodes:\n        lp_handle.write(\" x%d\\n\" % i)\n\n    lp_handle.write(\"End\\n\")\n\n    lp_data = lp_handle.getvalue()\n    lp_handle.close()\n\n    return lp_data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsolve the formatted LP instance", "response": "def solve_lp(clusters, quota, work_dir=\"work\", Nmax=0,\n             self_match=False, solver=\"SCIP\", verbose=False):\n    \"\"\"\n    Solve the formatted LP instance\n    \"\"\"\n    qb, qa = quota  # flip it\n    nodes, constraints_x, constraints_y = get_constraints(\n        clusters, (qa, qb), Nmax=Nmax)\n\n    if self_match:\n        constraints_x = constraints_y = constraints_x | constraints_y\n\n    lp_data = format_lp(nodes, constraints_x, qa, constraints_y, qb)\n\n    if solver == \"SCIP\":\n        filtered_list = SCIPSolver(lp_data, work_dir, verbose=verbose).results\n        if not filtered_list:\n            print(\"SCIP fails... trying GLPK\", file=sys.stderr)\n            filtered_list = GLPKSolver(\n                lp_data, work_dir, verbose=verbose).results\n\n    elif solver == \"GLPK\":\n        filtered_list = GLPKSolver(lp_data, work_dir, verbose=verbose).results\n        if not filtered_list:\n            print(\"GLPK fails... trying SCIP\", file=sys.stderr)\n            filtered_list = SCIPSolver(\n                lp_data, work_dir, verbose=verbose).results\n\n    return filtered_list"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef print_maps(map_type=None, number=None):\n    if not map_type and not number:\n        print_all_maps()\n\n    elif map_type:\n        print_maps_by_type(map_type, number)\n\n    else:\n        s = ('Invalid parameter combination. '\n             'number without map_type is not supported.')\n        raise ValueError(s)", "response": "Print maps by type and number of defined colors."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef print_maps_by_type(map_type, number=None):\n    map_type = map_type.lower().capitalize()\n    if map_type not in MAP_TYPES:\n        s = 'Invalid map type, must be one of {0}'.format(MAP_TYPES)\n        raise ValueError(s)\n\n    print(map_type)\n\n    map_keys = sorted(COLOR_MAPS[map_type].keys())\n\n    format_str = '{0:8}  :  {1}'\n\n    for mk in map_keys:\n        num_keys = sorted(COLOR_MAPS[map_type][mk].keys(), key=int)\n\n        if not number or str(number) in num_keys:\n            num_str = '{' + ', '.join(num_keys) + '}'\n            print(format_str.format(mk, num_str))", "response": "Prints all available maps of a given type."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a BrewerMap representation of the specified color map.", "response": "def get_map(name, map_type, number, reverse=False):\n    \"\"\"\n    Return a `BrewerMap` representation of the specified color map.\n\n    Parameters\n    ----------\n    name : str\n        Name of color map. Use `print_maps` to see available color maps.\n    map_type : {'Sequential', 'Diverging', 'Qualitative'}\n        Select color map type.\n    number : int\n        Number of defined colors in color map.\n    reverse : bool, optional\n        Set to True to get the reversed color map.\n\n    \"\"\"\n    number = str(number)\n    map_type = map_type.lower().capitalize()\n\n    # check for valid type\n    if map_type not in MAP_TYPES:\n        s = 'Invalid map type, must be one of {0}'.format(MAP_TYPES)\n        raise ValueError(s)\n\n    # make a dict of lower case map name to map name so this can be\n    # insensitive to case.\n    # this would be a perfect spot for a dict comprehension but going to\n    # wait on that to preserve 2.6 compatibility.\n    # map_names = {k.lower(): k for k in COLOR_MAPS[map_type].iterkeys()}\n    map_names = dict((k.lower(), k) for k in COLOR_MAPS[map_type].keys())\n\n    # check for valid name\n    if name.lower() not in map_names:\n        s = 'Invalid color map name {0!r} for type {1!r}.\\n'\n        s = s.format(name, map_type)\n        valid_names = [str(k) for k in COLOR_MAPS[map_type].keys()]\n        valid_names.sort()\n        s += 'Valid names are: {0}'.format(valid_names)\n        raise ValueError(s)\n\n    name = map_names[name.lower()]\n\n    # check for valid number\n    if number not in COLOR_MAPS[map_type][name]:\n        s = 'Invalid number for map type {0!r} and name {1!r}.\\n'\n        s = s.format(map_type, str(name))\n        valid_numbers = [int(k) for k in COLOR_MAPS[map_type][name].keys()]\n        valid_numbers.sort()\n        s += 'Valid numbers are : {0}'.format(valid_numbers)\n        raise ValueError(s)\n\n    colors = COLOR_MAPS[map_type][name][number]['Colors']\n\n    if reverse:\n        name += '_r'\n        colors = [x for x in reversed(colors)]\n\n    return BrewerMap(name, map_type, colors)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _load_maps_by_type(map_type):\n    seq_maps = COLOR_MAPS[map_type]\n\n    loaded_maps = {}\n\n    for map_name in seq_maps:\n        loaded_maps[map_name] = {}\n\n        for num in seq_maps[map_name]:\n            inum = int(num)\n            colors = seq_maps[map_name][num]['Colors']\n\n            bmap = BrewerMap(map_name, map_type, colors)\n\n            loaded_maps[map_name][inum] = bmap\n\n        max_num = int(max(seq_maps[map_name].keys(), key=int))\n        loaded_maps[map_name]['max'] = loaded_maps[map_name][max_num]\n\n    return loaded_maps", "response": "Loads all maps of a given type into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncoloring as a tuple of hex strings.", "response": "def hex_colors(self):\n        \"\"\"\n        Colors as a tuple of hex strings. (e.g. '#A912F4')\n\n        \"\"\"\n        hc = []\n\n        for color in self.colors:\n            h = '#' + ''.join('{0:>02}'.format(hex(c)[2:].upper())\n                              for c in color)\n            hc.append(h)\n\n        return hc"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mpl_colors(self):\n        mc = []\n\n        for color in self.colors:\n            mc.append(tuple([x / 255. for x in color]))\n\n        return mc", "response": "Returns a list of matplotlib colors."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_mpl_colormap(self, **kwargs):\n        if not HAVE_MPL:    # pragma: no cover\n            raise RuntimeError('matplotlib not available.')\n\n        cmap = LinearSegmentedColormap.from_list(self.name,\n                                                 self.mpl_colors, **kwargs)\n\n        return cmap", "response": "Returns a color map that can be used in matplotlib plots."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef show_as_blocks(self, block_size=100):\n        from ipythonblocks import BlockGrid\n\n        grid = BlockGrid(self.number, 1, block_size=block_size)\n\n        for block, color in zip(grid, self.colors):\n            block.rgb = color\n\n        grid.show()", "response": "Show colors in IPython Notebook using ipythonblocks."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef colorbrewer2_url(self):\n        url = 'http://colorbrewer2.org/index.html?type={0}&scheme={1}&n={2}'\n        return url.format(self.type.lower(), self.name, self.number)", "response": "URL that can be used to view the color map at colorbrewer2. org."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef summary(args):\n    from jcvi.formats.fasta import summary as fsummary\n    from jcvi.utils.cbook import percentage, human_size\n\n    p = OptionParser(summary.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    chainfile, oldfasta, newfasta = args\n    chain = Chain(chainfile)\n    ungapped, dt, dq = chain.ungapped, chain.dt, chain.dq\n    print(\"File `{0}` contains {1} chains.\".\\\n                format(chainfile, len(chain)), file=sys.stderr)\n    print(\"ungapped={0} dt={1} dq={2}\".\\\n                format(human_size(ungapped), human_size(dt), human_size(dq)), file=sys.stderr)\n\n    oldreal, oldnn, oldlen = fsummary([oldfasta, \"--outfile=/dev/null\"])\n    print(\"Old fasta (`{0}`) mapped: {1}\".\\\n                format(oldfasta, percentage(ungapped, oldreal)), file=sys.stderr)\n\n    newreal, newnn, newlen = fsummary([newfasta, \"--outfile=/dev/null\"])\n    print(\"New fasta (`{0}`) mapped: {1}\".\\\n                format(newfasta, percentage(ungapped, newreal)), file=sys.stderr)", "response": "%prog summary chainfile old. fasta new. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef frompsl(args):\n    from jcvi.formats.sizes import Sizes\n\n    p = OptionParser(frompsl.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    pslfile, oldfasta, newfasta = args\n    pf = oldfasta.split(\".\")[0]\n\n    # Chain together alignments from using axtChain\n    chainfile = pf + \".chain\"\n    twobitfiles = []\n    for fastafile in (oldfasta, newfasta):\n        tbfile = faToTwoBit(fastafile)\n        twobitfiles.append(tbfile)\n    oldtwobit, newtwobit = twobitfiles\n\n    if need_update(pslfile, chainfile):\n        cmd = \"axtChain -linearGap=medium -psl {0}\".format(pslfile)\n        cmd += \" {0} {1} {2}\".format(oldtwobit, newtwobit, chainfile)\n        sh(cmd)\n\n    # Sort chain files\n    sortedchain = chainfile.rsplit(\".\", 1)[0] + \".sorted.chain\"\n    if need_update(chainfile, sortedchain):\n        cmd = \"chainSort {0} {1}\".format(chainfile, sortedchain)\n        sh(cmd)\n\n    # Make alignment nets from chains\n    netfile = pf + \".net\"\n    oldsizes = Sizes(oldfasta).filename\n    newsizes = Sizes(newfasta).filename\n    if need_update((sortedchain, oldsizes, newsizes), netfile):\n        cmd = \"chainNet {0} {1} {2}\".format(sortedchain, oldsizes, newsizes)\n        cmd += \" {0} /dev/null\".format(netfile)\n        sh(cmd)\n\n    # Create liftOver chain file\n    liftoverfile = pf + \".liftover.chain\"\n    if need_update((netfile, sortedchain), liftoverfile):\n        cmd = \"netChainSubset {0} {1} {2}\".\\\n                format(netfile, sortedchain, liftoverfile)\n        sh(cmd)", "response": "%prog frompsl old. fasta new. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts the lastz tabular to blast tabular", "response": "def lastz_to_blast(row):\n    \"\"\"\n    Convert the lastz tabular to the blast tabular, see headers above\n    Obsolete after LASTZ version 1.02.40\n    \"\"\"\n    atoms = row.strip().split(\"\\t\")\n    name1, name2, coverage, identity, nmismatch, ngap, \\\n            start1, end1, strand1, start2, end2, strand2, score = atoms\n    identity = identity.replace(\"%\", \"\")\n    hitlen = coverage.split(\"/\")[1]\n    score = float(score)\n    same_strand = (strand1 == strand2)\n    if not same_strand:\n        start2, end2 = end2, start2\n\n    evalue = blastz_score_to_ncbi_expectation(score)\n    score = blastz_score_to_ncbi_bits(score)\n    evalue, score = \"%.2g\" % evalue, \"%.1f\" % score\n    return \"\\t\".join((name1, name2, identity, hitlen, nmismatch, ngap, \\\n            start1, end1, start2, end2, evalue, score))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nuse for formats other than BLAST, i.e. lav, maf, etc. which requires the database file to contain a single FASTA record.", "response": "def lastz_2bit(t):\n    \"\"\"\n    Used for formats other than BLAST, i.e. lav, maf, etc. which requires the\n    database file to contain a single FASTA record.\n    \"\"\"\n    bfasta_fn, afasta_fn, outfile, lastz_bin, extra, mask, format = t\n\n    ref_tags = [Darkspace]\n    qry_tags = [Darkspace]\n    ref_tags, qry_tags = add_mask(ref_tags, qry_tags, mask=mask)\n\n    lastz_cmd = Lastz_template.format(lastz_bin, bfasta_fn, ref_tags, \\\n                                                 afasta_fn, qry_tags)\n    if extra:\n        lastz_cmd += \" \" + extra.strip()\n\n    lastz_cmd += \" --format={0}\".format(format)\n    proc = Popen(lastz_cmd)\n    out_fh = open(outfile, \"w\")\n\n    logging.debug(\"job <%d> started: %s\" % (proc.pid, lastz_cmd))\n    for row in proc.stdout:\n        out_fh.write(row)\n        out_fh.flush()\n    logging.debug(\"job <%d> finished\" % proc.pid)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main():\n    p = OptionParser(main.__doc__)\n\n    supported_formats = tuple(x.strip() for x in \\\n        \"lav, lav+text, axt, axt+, maf, maf+, maf-, sam, softsam, \"\\\n        \"sam-, softsam-, cigar, BLASTN, BLASTN-, differences, rdotplot, text\".split(','))\n\n    p.add_option(\"--format\", default=\"BLASTN-\", choices=supported_formats,\n            help=\"Ooutput format [default: %default]\")\n    p.add_option(\"--path\", dest=\"lastz_path\", default=None,\n            help=\"specify LASTZ path\")\n    p.add_option(\"--mask\", dest=\"mask\", default=False, action=\"store_true\",\n            help=\"treat lower-case letters as mask info [default: %default]\")\n    p.add_option(\"--similar\", default=False, action=\"store_true\",\n            help=\"Use options tuned for close comparison [default: %default]\")\n    p.set_cpus(cpus=32)\n    p.set_params()\n    p.set_outfile()\n    opts, args = p.parse_args()\n\n    if len(args) != 2:\n        sys.exit(p.print_help())\n\n    bfasta_fn, afasta_fn = args\n    for fn in (afasta_fn, bfasta_fn):\n        assert op.exists(fn)\n\n    afasta_fn = op.abspath(afasta_fn)\n    bfasta_fn = op.abspath(bfasta_fn)\n    out_fh = must_open(opts.outfile, \"w\")\n\n    extra = opts.extra\n    if opts.similar:\n        extra += similarOptions\n\n    lastz_bin = opts.lastz_path or \"lastz\"\n    assert lastz_bin.endswith(\"lastz\"), \"You need to include lastz in your path\"\n\n    mask = opts.mask\n    cpus = opts.cpus\n    logging.debug(\"Dispatch job to %d cpus\" % cpus)\n    format = opts.format\n    blastline = (format == \"BLASTN-\")\n\n    # The axt, maf, etc. format can only be run on splitted database (i.e. one\n    # FASTA record per file). The splitted files are then parallelized for the\n    # computation, as opposed to splitting queries through \"subsample\".\n    outdir = \"outdir\"\n    if not blastline:\n        from jcvi.formats.fasta import Fasta\n        from jcvi.formats.chain import faToTwoBit\n\n        mkdir(outdir)\n\n        bfasta_2bit = faToTwoBit(bfasta_fn)\n        bids = list(Fasta(bfasta_fn, lazy=True).iterkeys_ordered())\n\n        apf = op.basename(afasta_fn).split(\".\")[0]\n        args = []\n        # bfasta_fn, afasta_fn, outfile, lastz_bin, extra, mask, format\n        for id in bids:\n            bfasta = \"/\".join((bfasta_2bit, id))\n            outfile = op.join(outdir, \"{0}.{1}.{2}\".format(apf, id, format))\n            args.append((bfasta, afasta_fn, outfile, \\\n                         lastz_bin, extra, mask, format))\n\n        p = Pool(cpus)\n        p.map(lastz_2bit, args)\n\n        return\n\n    lock = Lock()\n\n    args = [(k + 1, cpus, bfasta_fn, afasta_fn, out_fh,\n            lock, lastz_bin, extra, mask) for k in xrange(cpus)]\n    g = Jobs(target=lastz, args=args)\n    g.run()", "response": "%prog LASTZ similar to the BLAST interface"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tophat(args):\n    from jcvi.apps.bowtie import check_index\n    from jcvi.formats.fastq import guessoffset\n\n    p = OptionParser(tophat.__doc__)\n    p.add_option(\"--gtf\", help=\"Reference annotation [default: %default]\")\n    p.add_option(\"--single\", default=False, action=\"store_true\",\n                 help=\"Single end mapping\")\n    p.add_option(\"--intron\", default=15000, type=\"int\",\n                 help=\"Max intron size [default: %default]\")\n    p.add_option(\"--dist\", default=-50, type=\"int\",\n                 help=\"Mate inner distance [default: %default]\")\n    p.add_option(\"--stdev\", default=50, type=\"int\",\n                 help=\"Mate standard deviation [default: %default]\")\n    p.set_phred()\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    num = 1 if opts.single else 2\n    folder, reference = args\n    reference = check_index(reference)\n    for p, prefix in iter_project(folder, n=num):\n        outdir = \"{0}_tophat\".format(prefix)\n        outfile = op.join(outdir, \"accepted_hits.bam\")\n        if op.exists(outfile):\n            logging.debug(\"File `{0}` found. Skipping.\".format(outfile))\n            continue\n\n        cmd = \"tophat -p {0}\".format(opts.cpus)\n        if opts.gtf:\n            cmd += \" -G {0}\".format(opts.gtf)\n        cmd += \" -o {0}\".format(outdir)\n\n        if num == 1:  # Single-end\n            a, = p\n        else:  # Paired-end\n            a, b = p\n            cmd += \" --max-intron-length {0}\".format(opts.intron)\n            cmd += \" --mate-inner-dist {0}\".format(opts.dist)\n            cmd += \" --mate-std-dev {0}\".format(opts.stdev)\n\n        phred = opts.phred or str(guessoffset([a]))\n        if phred == \"64\":\n            cmd += \" --phred64-quals\"\n        cmd += \" {0} {1}\".format(reference, \" \".join(p))\n\n        sh(cmd)", "response": "%prog tophat folder reference"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hmean_int(a, a_min=5778, a_max=1149851):\n    from scipy.stats import hmean\n    return int(round(hmean(np.clip(a, a_min, a_max))))", "response": "Harmonic mean of an array returns the closest int\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef golden_array(a, phi=1.61803398875, lb=LB, ub=UB):\n    counts = np.zeros(BB, dtype=int)\n    for x in a:\n        c = int(round(math.log(x, phi)))\n        if c < lb:\n            c = lb\n        if c > ub:\n            c = ub\n        counts[c - lb] += 1\n    return counts", "response": "Given a list of ints we aggregate similar values so that it becomes an array of multiples of phi."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prune_tour_worker(arg):\n    from .chic import score_evaluate_M\n\n    t, stour, tour_score, active_sizes, M = arg\n    stour_score, = score_evaluate_M(stour, active_sizes, M)\n    delta_score = tour_score - stour_score\n    log10d = np.log10(delta_score) if delta_score > 1e-9 else -9\n    return t, log10d", "response": "Worker thread for CLMFile. prune_tour"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mergemat(args):\n    p = OptionParser(mergemat.__doc__)\n    p.set_outfile(outfile=\"out\")\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    npyfiles = args\n    A = np.load(npyfiles[0])\n    logging.debug(\"Load `{}`: matrix of shape {}:; sum={}\"\n                  .format(npyfiles[0], A.shape, A.sum()))\n    for npyfile in npyfiles[1:]:\n        B = np.load(npyfile)\n        A += B\n        logging.debug(\"Load `{}`: sum={}\"\n                      .format(npyfiles[0], A.sum()))\n\n    pf = opts.outfile\n    np.save(pf, A)\n    logging.debug(\"Combined {} files into `{}.npy`\".format(len(npyfiles), pf))", "response": "%prog mergemat npyfiles > new_npy_files"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_seqstarts(bamfile, N):\n    import pysam\n    bamfile = pysam.AlignmentFile(bamfile, \"rb\")\n    seqsize = {}\n    for kv in bamfile.header[\"SQ\"]:\n        if kv[\"LN\"] < 10 * N:\n            continue\n        seqsize[kv[\"SN\"]] = kv[\"LN\"] / N + 1\n\n    allseqs = natsorted(seqsize.keys())\n    allseqsizes = np.array([seqsize[x] for x in allseqs])\n    seqstarts = np.cumsum(allseqsizes)\n    seqstarts = np.roll(seqstarts, 1)\n    total_bins = seqstarts[0]\n    seqstarts[0] = 0\n    seqstarts = dict(zip(allseqs, seqstarts))\n\n    return seqstarts, seqsize, total_bins", "response": "Get the sequence start and size of the sequence in the specified BAM file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting exponentially sized binning for a single resource.", "response": "def get_distbins(start=100, bins=2500, ratio=1.01):\n    \"\"\" Get exponentially sized\n    \"\"\"\n    b = np.ones(bins, dtype=\"float64\")\n    b[0] = 100\n    for i in range(1, bins):\n        b[i] = b[i - 1] * ratio\n    bins = np.around(b).astype(dtype=\"int\")\n    binsizes = np.diff(bins)\n    return bins, binsizes"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite the last file query and subject BED files.", "response": "def write_last_and_beds(pf, GenePositions, ContigStarts):\n    \"\"\"\n    Write LAST file, query and subject BED files.\n    \"\"\"\n    qbedfile = pf + \"tigs.bed\"\n    sbedfile = pf + \"chr.bed\"\n    lastfile = \"{}tigs.{}chr.last\".format(pf, pf)\n    qbedfw = open(qbedfile, \"w\")\n    sbedfw = open(sbedfile, \"w\")\n    lastfw = open(lastfile, \"w\")\n\n    GeneContigs = np.searchsorted(ContigStarts, GenePositions) - 1\n    for i, (c, gstart) in enumerate(zip(GeneContigs, GenePositions)):\n        gene = \"gene{:05d}\".format(i)\n        tig = \"tig{:04d}\".format(c)\n        start = ContigStarts[c]\n        cstart = gstart - start\n        print(\"\\t\".join(str(x) for x in\n                                   (tig, cstart, cstart + 1, gene)), file=qbedfw)\n        print(\"\\t\".join(str(x) for x in\n                                   (\"chr1\", gstart, gstart + 1, gene)), file=sbedfw)\n        lastatoms = [gene, gene, 100] + [0] * 8 + [100]\n        print(\"\\t\".join(str(x) for x in lastatoms), file=lastfw)\n\n    qbedfw.close()\n    sbedfw.close()\n    lastfw.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite CLM file from simulated data.", "response": "def write_clm(pf, ICLinkStartContigs, ICLinkEndContigs,\n              ICLinkStarts, ICLinkEnds,\n              ContigStarts, ContigSizes):\n    \"\"\"\n    Write CLM file from simulated data.\n    \"\"\"\n    clm = defaultdict(list)\n    for start, end, linkstart, linkend in \\\n            zip(ICLinkStartContigs, ICLinkEndContigs,\n                ICLinkStarts, ICLinkEnds):\n        start_a = ContigStarts[start]\n        start_b = start_a + ContigSizes[start]\n        end_a = ContigStarts[end]\n        end_b = end_a + ContigSizes[end]\n        if linkend >= end_b:\n            continue\n        clm[(start, end)].append((linkstart - start_a, start_b - linkstart,\n                                  linkend - end_a, end_b - linkend))\n\n    clmfile = pf + \".clm\"\n    fw = open(clmfile, \"w\")\n\n    def format_array(a):\n        return [str(x) for x in sorted(a) if x > 0]\n\n    for (start, end), links in sorted(clm.items()):\n        start = \"tig{:04d}\".format(start)\n        end = \"tig{:04d}\".format(end)\n        nlinks = len(links)\n        if not nlinks:\n            continue\n        ff = format_array([(b + c) for a, b, c, d in links])\n        fr = format_array([(b + d) for a, b, c, d in links])\n        rf = format_array([(a + c) for a, b, c, d in links])\n        rr = format_array([(a + d) for a, b, c, d in links])\n        print(\"{}+ {}+\\t{}\\t{}\".format(start, end, nlinks, \" \".join(ff)), file=fw)\n        print(\"{}+ {}-\\t{}\\t{}\".format(start, end, nlinks, \" \".join(fr)), file=fw)\n        print(\"{}- {}+\\t{}\\t{}\".format(start, end, nlinks, \" \".join(rf)), file=fw)\n        print(\"{}- {}-\\t{}\\t{}\".format(start, end, nlinks, \" \".join(rr)), file=fw)\n    fw.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef optimize_ordering(fwtour, clm, phase, cpus):\n    from .chic import score_evaluate_M\n\n    # Prepare input files\n    tour_contigs = clm.active_contigs\n    tour_sizes = clm.active_sizes\n    tour_M = clm.M\n    tour = clm.tour\n    signs = clm.signs\n    oo = clm.oo\n\n    def callback(tour, gen, phase, oo):\n        fitness = tour.fitness if hasattr(tour, \"fitness\") else None\n        label = \"GA{}-{}\".format(phase, gen)\n        if fitness:\n            fitness = \"{0}\".format(fitness).split(\",\")[0].replace(\"(\", \"\")\n            label += \"-\" + fitness\n        if gen % 20 == 0:\n            print_tour(fwtour, tour, label, tour_contigs, oo, signs=signs)\n        return tour\n\n    callbacki = partial(callback, phase=phase, oo=oo)\n    toolbox = GA_setup(tour)\n    toolbox.register(\"evaluate\", score_evaluate_M,\n                     tour_sizes=tour_sizes, tour_M=tour_M)\n    tour, tour_fitness = GA_run(toolbox, ngen=1000, npop=100, cpus=cpus,\n                                callback=callbacki)\n    clm.tour = tour\n\n    return tour", "response": "Optimize the ordering of contigs by Genetic Algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef optimize_orientations(fwtour, clm, phase, cpus):\n    # Prepare input files\n    tour_contigs = clm.active_contigs\n    tour = clm.tour\n    oo = clm.oo\n\n    print_tour(fwtour, tour, \"FLIPALL{}\".format(phase),\n               tour_contigs, oo, signs=clm.signs)\n    tag1 = clm.flip_whole(tour)\n    print_tour(fwtour, tour, \"FLIPWHOLE{}\".format(phase),\n               tour_contigs, oo, signs=clm.signs)\n    tag2 = clm.flip_one(tour)\n    print_tour(fwtour, tour, \"FLIPONE{}\".format(phase),\n               tour_contigs, oo, signs=clm.signs)\n\n    return tag1, tag2", "response": "Optimize the orientations of contigs by using heuristic flipping."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prepare_synteny(tourfile, lastfile, odir, p, opts):\n    qbedfile, sbedfile = get_bed_filenames(lastfile, p, opts)\n    qbedfile = op.abspath(qbedfile)\n    sbedfile = op.abspath(sbedfile)\n\n    qbed = Bed(qbedfile, sorted=False)\n    contig_to_beds = dict(qbed.sub_beds())\n\n    # Create a separate directory for the subplots and movie\n    mkdir(odir, overwrite=True)\n    os.chdir(odir)\n    logging.debug(\"Change into subdir `{}`\".format(odir))\n\n    # Make anchorsfile\n    anchorsfile = \".\".join(op.basename(lastfile).split(\".\", 2)[:2]) \\\n                  + \".anchors\"\n    fw = open(anchorsfile, \"w\")\n    for b in Blast(lastfile):\n        print(\"\\t\".join((gene_name(b.query), gene_name(b.subject),\n                                str(int(b.score)))), file=fw)\n    fw.close()\n\n    # Symlink sbed\n    symlink(sbedfile, op.basename(sbedfile))\n\n    return anchorsfile, qbedfile, contig_to_beds", "response": "Prepare synteny plots for movie."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef separate_tour_and_o(row):\n    tour = []\n    tour_o = []\n    for contig in row.split():\n        if contig[-1] in ('+', '-', '?'):\n            tour.append(contig[:-1])\n            tour_o.append(contig[-1])\n        else:  # Unoriented\n            tour.append(contig)\n            tour_o.append('?')\n    return tour, tour_o", "response": "This function separates the names from the orientations."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\niterating through the last tour from a tourfile.", "response": "def iter_last_tour(tourfile, clm):\n    \"\"\"\n    Extract last tour from tourfile. The clm instance is also passed in to see\n    if any contig is covered in the clm.\n    \"\"\"\n    row = open(tourfile).readlines()[-1]\n    _tour, _tour_o = separate_tour_and_o(row)\n    tour = []\n    tour_o = []\n    for tc, to in zip(_tour, _tour_o):\n        if tc not in clm.contigs:\n            logging.debug(\"Contig `{}` in file `{}` not found in `{}`\"\n                          .format(tc, tourfile, clm.idsfile))\n            continue\n        tour.append(tc)\n        tour_o.append(to)\n    return tour, tour_o"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iter_tours(tourfile, frames=1):\n    fp = open(tourfile)\n\n    i = 0\n    for row in fp:\n        if row[0] == '>':\n            label = row[1:].strip()\n            if label.startswith(\"GA\"):\n                pf, j, score = label.split(\"-\", 2)\n                j = int(j)\n            else:\n                j = 0\n            i += 1\n        else:\n            if j % frames != 0:\n                continue\n            tour, tour_o = separate_tour_and_o(row)\n            yield i, label, tour, tour_o\n\n    fp.close()", "response": "Iterates through the tours in the specified file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef score(args):\n    p = OptionParser(score.__doc__)\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    mdir, cdir, contigsfasta = args\n    orderingfiles = natsorted(iglob(mdir, \"*.ordering\"))\n    sizes = Sizes(contigsfasta)\n    contig_names = list(sizes.iter_names())\n    contig_ids = dict((name, i) for (i, name) in enumerate(contig_names))\n\n    oo = []\n    # Load contact matrix\n    glm = op.join(cdir, \"all.GLM\")\n    N = len(contig_ids)\n    M = np.zeros((N, N), dtype=int)\n    fp = open(glm)\n    for row in fp:\n        if row[0] == '#':\n            continue\n        x, y, z = row.split()\n        if x == 'X':\n            continue\n        M[int(x), int(y)] = int(z)\n\n    fwtour = open(\"tour\", \"w\")\n\n    def callback(tour, gen, oo):\n        fitness = tour.fitness if hasattr(tour, \"fitness\") else None\n        label = \"GA-{0}\".format(gen)\n        if fitness:\n            fitness = \"{0}\".format(fitness).split(\",\")[0].replace(\"(\", \"\")\n            label += \"-\" + fitness\n        print_tour(fwtour, tour, label, contig_names, oo)\n        return tour\n\n    for ofile in orderingfiles:\n        co = ContigOrdering(ofile)\n        for x in co:\n            contig_id = contig_ids[x.contig_name]\n            oo.append(contig_id)\n        pf = op.basename(ofile).split(\".\")[0]\n        print(pf)\n        print(oo)\n\n        tour, tour_sizes, tour_M = prepare_ec(oo, sizes, M)\n        # Store INIT tour\n        print_tour(fwtour, tour, \"INIT\", contig_names, oo)\n\n        # Faster Cython version for evaluation\n        from .chic import score_evaluate_M\n        callbacki = partial(callback, oo=oo)\n        toolbox = GA_setup(tour)\n        toolbox.register(\"evaluate\", score_evaluate_M,\n                         tour_sizes=tour_sizes, tour_M=tour_M)\n        tour, tour.fitness = GA_run(toolbox, npop=100, cpus=opts.cpus,\n                                    callback=callbacki)\n        print(tour, tour.fitness)\n        break\n\n    fwtour.close()", "response": "%prog score main_results directory contigsfasta"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prepare_ec(oo, sizes, M):\n    tour = range(len(oo))\n    tour_sizes = np.array([sizes.sizes[x] for x in oo])\n    tour_M = M[oo, :][:, oo]\n    return tour, tour_sizes, tour_M", "response": "This prepares EC and converts from contig_id to an index."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef score_evaluate(tour, tour_sizes=None, tour_M=None):\n    sizes_oo = np.array([tour_sizes[x] for x in tour])\n    sizes_cum = np.cumsum(sizes_oo) - sizes_oo / 2\n    s = 0\n    size = len(tour)\n    for ia in xrange(size):\n        a = tour[ia]\n        for ib in xrange(ia + 1, size):\n            b = tour[ib]\n            links = tour_M[a, b]\n            dist = sizes_cum[ib] - sizes_cum[ia]\n            if dist > 1e7:\n                break\n            s += links * 1. / dist\n    return s,", "response": "SLOW python version of the evaluation function. Do not use in production."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting the ContigOrdering file into AGP format.", "response": "def write_agp(self, obj, sizes, fw=sys.stdout, gapsize=100,\n                  gaptype=\"contig\", evidence=\"map\"):\n        '''Converts the ContigOrdering file into AGP format\n        '''\n        contigorder = [(x.contig_name, x.strand) for x in self]\n        order_to_agp(obj, contigorder, sizes, fw,\n                     gapsize=gapsize, gaptype=gaptype, evidence=evidence)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_ids(self, skiprecover):\n        '''IDS file has a list of contigs that need to be ordered. 'recover',\n        keyword, if available in the third column, is less confident.\n\n        tig00015093     46912\n        tig00035238     46779   recover\n        tig00030900     119291\n        '''\n        idsfile = self.idsfile\n        logging.debug(\"Parse idsfile `{}`\".format(idsfile))\n        fp = open(idsfile)\n        tigs = []\n        for row in fp:\n            if row[0] == '#':  # Header\n                continue\n            atoms = row.split()\n            tig, size = atoms[:2]\n            size = int(size)\n            if skiprecover and len(atoms) == 3 and atoms[2] == 'recover':\n                continue\n            tigs.append((tig, size))\n\n        # Arrange contig names and sizes\n        _tigs, _sizes = zip(*tigs)\n        self.contigs = set(_tigs)\n        self.sizes = np.array(_sizes)\n        self.tig_to_size = dict(tigs)\n\n        # Initially all contigs are considered active\n        self.active = set(_tigs)", "response": "Parse the IDS file and store the contig names and sizes in the self. contigs attribute."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the density of inter - contig links per base.", "response": "def calculate_densities(self):\n        \"\"\"\n        Calculate the density of inter-contig links per base. Strong contigs\n        considered to have high level of inter-contig links in the current\n        partition.\n        \"\"\"\n        active = self.active\n        densities = defaultdict(int)\n        for (at, bt), links in self.contacts.items():\n            if not (at in active and bt in active):\n                continue\n            densities[at] += links\n            densities[bt] += links\n\n        logdensities = {}\n        for x, d in densities.items():\n            s = self.tig_to_size[x]\n            logd = np.log10(d * 1. / min(s, 500000))\n            logdensities[x] = logd\n\n        return logdensities"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef activate(self, tourfile=None, minsize=10000, backuptour=True):\n        if tourfile and (not op.exists(tourfile)):\n            logging.debug(\"Tourfile `{}` not found\".format(tourfile))\n            tourfile = None\n\n        if tourfile:\n            logging.debug(\"Importing tourfile `{}`\".format(tourfile))\n            tour, tour_o = iter_last_tour(tourfile, self)\n            self.active = set(tour)\n            tig_to_idx = self.tig_to_idx\n            tour = [tig_to_idx[x] for x in tour]\n            signs = sorted([(x, FF[o]) for (x, o) in zip(tour, tour_o)])\n            _, signs = zip(*signs)\n            self.signs = np.array(signs, dtype=int)\n            if backuptour:\n                backup(tourfile)\n            tour = array.array('i', tour)\n        else:\n            self.report_active()\n            while True:\n                logdensities = self.calculate_densities()\n                lb, ub = outlier_cutoff(logdensities.values())\n                logging.debug(\"Log10(link_densities) ~ [{}, {}]\"\n                              .format(lb, ub))\n                remove = set(x for x, d in logdensities.items() if\n                             (d < lb and self.tig_to_size[x] < minsize * 10))\n                if remove:\n                    self.active -= remove\n                    self.report_active()\n                else:\n                    break\n\n            logging.debug(\"Remove contigs with size < {}\".format(minsize))\n            self.active = set(x for x in self.active if\n                              self.tig_to_size[x] >= minsize)\n            tour = range(self.N)  # Use starting (random) order otherwise\n            tour = array.array('i', tour)\n\n            # Determine orientations\n            self.flip_all(tour)\n\n        self.report_active()\n        self.tour = tour\n\n        return tour", "response": "Activates the current contig set."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nevaluate the score of a tour", "response": "def evaluate_tour_M(self, tour):\n        \"\"\" Use Cythonized version to evaluate the score of a current tour\n        \"\"\"\n        from .chic import score_evaluate_M\n        return score_evaluate_M(tour, self.active_sizes, self.M)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nevaluating the score of a current tour with better precision on the distance of contigs.", "response": "def evaluate_tour_P(self, tour):\n        \"\"\" Use Cythonized version to evaluate the score of a current tour,\n        with better precision on the distance of the contigs.\n        \"\"\"\n        from .chic import score_evaluate_P\n        return score_evaluate_P(tour, self.active_sizes, self.P)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nevaluate the score of a current tour", "response": "def evaluate_tour_Q(self, tour):\n        \"\"\" Use Cythonized version to evaluate the score of a current tour,\n        taking orientation into consideration. This may be the most accurate\n        evaluation under the right condition.\n        \"\"\"\n        from .chic import score_evaluate_Q\n        return score_evaluate_Q(tour, self.active_sizes, self.Q)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nflip all the orientation of the current tour.", "response": "def flip_all(self, tour):\n        \"\"\" Initialize the orientations based on pairwise O matrix.\n        \"\"\"\n        if self.signs is None:  # First run\n            score = 0\n        else:\n            old_signs = self.signs[:self.N]\n            score, = self.evaluate_tour_Q(tour)\n\n        # Remember we cannot have ambiguous orientation code (0 or '?') here\n        self.signs = get_signs(self.O, validate=False, ambiguous=False)\n        score_flipped, = self.evaluate_tour_Q(tour)\n        if score_flipped >= score:\n            tag = ACCEPT\n        else:\n            self.signs = old_signs[:]\n            tag = REJECT\n        self.flip_log(\"FLIPALL\", score, score_flipped, tag)\n        return tag"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntests flipping all contigs at the same time to see if the score improves.", "response": "def flip_whole(self, tour):\n        \"\"\" Test flipping all contigs at the same time to see if score improves.\n        \"\"\"\n        score, = self.evaluate_tour_Q(tour)\n        self.signs = -self.signs\n        score_flipped, = self.evaluate_tour_Q(tour)\n        if score_flipped > score:\n            tag = ACCEPT\n        else:\n            self.signs = -self.signs\n            tag = REJECT\n        self.flip_log(\"FLIPWHOLE\", score, score_flipped, tag)\n        return tag"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntest flipping every single contig sequentially to see if score improves.", "response": "def flip_one(self, tour):\n        \"\"\" Test flipping every single contig sequentially to see if score\n        improves.\n        \"\"\"\n        n_accepts = n_rejects = 0\n        any_tag_ACCEPT = False\n        for i, t in enumerate(tour):\n            if i == 0:\n                score, = self.evaluate_tour_Q(tour)\n            self.signs[t] = -self.signs[t]\n            score_flipped, = self.evaluate_tour_Q(tour)\n            if score_flipped > score:\n                n_accepts += 1\n                tag = ACCEPT\n            else:\n                self.signs[t] = -self.signs[t]\n                n_rejects += 1\n                tag = REJECT\n            self.flip_log(\"FLIPONE ({}/{})\".format(i + 1, len(self.signs)),\n                          score, score_flipped, tag)\n            if tag == ACCEPT:\n                any_tag_ACCEPT = True\n                score = score_flipped\n        logging.debug(\"FLIPONE: N_accepts={} N_rejects={}\"\n                      .format(n_accepts, n_rejects))\n        return ACCEPT if any_tag_ACCEPT else REJECT"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef prune_tour(self, tour, cpus):\n        while True:\n            tour_score, = self.evaluate_tour_M(tour)\n            logging.debug(\"Starting score: {}\".format(tour_score))\n            active_sizes = self.active_sizes\n            M = self.M\n            args = []\n            for i, t in enumerate(tour):\n                stour = tour[:i] + tour[i + 1:]\n                args.append((t, stour, tour_score, active_sizes, M))\n\n            # Parallel run\n            p = Pool(processes=cpus)\n            results = list(p.imap(prune_tour_worker, args))\n            assert len(tour) == len(results), \\\n                \"Array size mismatch, tour({}) != results({})\"\\\n                .format(len(tour), len(results))\n\n            # Identify outliers\n            active_contigs = self.active_contigs\n            idx, log10deltas = zip(*results)\n            lb, ub = outlier_cutoff(log10deltas)\n            logging.debug(\"Log10(delta_score) ~ [{}, {}]\".format(lb, ub))\n\n            remove = set(active_contigs[x] for (x, d) in results if d < lb)\n            self.active -= remove\n            self.report_active()\n\n            tig_to_idx = self.tig_to_idx\n            tour = [active_contigs[x] for x in tour]\n            tour = array.array('i', [tig_to_idx[x] for x in tour\n                                     if x not in remove])\n            if not remove:\n                break\n\n        self.tour = tour\n        self.flip_all(tour)\n\n        return tour", "response": "Test deleting each contig and check the delta_score ; tour here must be an array of ints."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef M(self):\n        N = self.N\n        tig_to_idx = self.tig_to_idx\n        M = np.zeros((N, N), dtype=int)\n        for (at, bt), links in self.contacts.items():\n            if not (at in tig_to_idx and bt in tig_to_idx):\n                continue\n            ai = tig_to_idx[at]\n            bi = tig_to_idx[bt]\n            M[ai, bi] = M[bi, ai] = links\n        return M", "response": "Contact frequency matrix. Each cell contains how many inter - contig\n        links between i - th and j - th contigs."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncontact frequency matrix with better precision on distance between two contigs.", "response": "def P(self):\n        \"\"\"\n        Contact frequency matrix with better precision on distance between\n        contigs. In the matrix M, the distance is assumed to be the distance\n        between mid-points of two contigs. In matrix Q, however, we compute\n        harmonic mean of the links for the orientation configuration that is\n        shortest. This offers better precision for the distance between big\n        contigs.\n        \"\"\"\n        N = self.N\n        tig_to_idx = self.tig_to_idx\n        P = np.zeros((N, N, 2), dtype=int)\n        for (at, bt), (strandedness, md, mh) in self.orientations.items():\n            if not (at in tig_to_idx and bt in tig_to_idx):\n                continue\n            ai = tig_to_idx[at]\n            bi = tig_to_idx[bt]\n            P[ai, bi, 0] = P[bi, ai, 0] = md\n            P[ai, bi, 1] = P[bi, ai, 1] = mh\n        return P"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncontact frequency matrix when contigs are already oriented.", "response": "def Q(self):\n        \"\"\"\n        Contact frequency matrix when contigs are already oriented. This is s a\n        similar matrix as M, but rather than having the number of links in the\n        cell, it points to an array that has the actual distances.\n        \"\"\"\n        N = self.N\n        tig_to_idx = self.tig_to_idx\n        signs = self.signs\n        Q = np.ones((N, N, BB), dtype=int) * -1  # Use -1 as the sentinel\n        for (at, bt), k in self.contacts_oriented.items():\n            if not (at in tig_to_idx and bt in tig_to_idx):\n                continue\n            ai = tig_to_idx[at]\n            bi = tig_to_idx[bt]\n            ao = signs[ai]\n            bo = signs[bi]\n            Q[ai, bi] = k[(ao, bo)]\n        return Q"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_sim_options(p):\n    p.add_option(\"--distance\", default=500, type=\"int\",\n                 help=\"Outer distance between the two ends [default: %default]\")\n    p.add_option(\"--readlen\", default=150, type=\"int\",\n                 help=\"Length of the read\")\n    p.set_depth(depth=10)\n    p.set_outfile(outfile=None)", "response": "Add options shared by eagle or wgsim.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wgsim(args):\n    p = OptionParser(wgsim.__doc__)\n    p.add_option(\"--erate\", default=.01, type=\"float\",\n                 help=\"Base error rate of the read [default: %default]\")\n    p.add_option(\"--noerrors\", default=False, action=\"store_true\",\n                 help=\"Simulate reads with no errors [default: %default]\")\n    p.add_option(\"--genomesize\", type=\"int\",\n                 help=\"Genome size in Mb [default: estimate from data]\")\n    add_sim_options(p)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    fastafile, = args\n    pf = op.basename(fastafile).split(\".\")[0]\n\n    genomesize = opts.genomesize\n    size = genomesize * 1000000 if genomesize else Fasta(fastafile).totalsize\n    depth = opts.depth\n    readlen = opts.readlen\n    readnum = int(math.ceil(size * depth / (2 * readlen)))\n\n    distance = opts.distance\n    stdev = distance / 10\n\n    outpf = opts.outfile or \"{0}.{1}bp.{2}x\".format(pf, distance, depth)\n\n    logging.debug(\"Total genome size: {0} bp\".format(size))\n    logging.debug(\"Target depth: {0}x\".format(depth))\n    logging.debug(\"Number of read pairs (2x{0}): {1}\".format(readlen, readnum))\n\n    if opts.noerrors:\n        opts.erate = 0\n\n    cmd = \"dwgsim -e {0} -E {0}\".format(opts.erate)\n    if opts.noerrors:\n        cmd += \" -r 0 -R 0 -X 0 -y 0\"\n\n    cmd += \" -d {0} -s {1}\".format(distance, stdev)\n    cmd += \" -N {0} -1 {1} -2 {1}\".format(readnum, readlen)\n    cmd += \" {0} {1}\".format(fastafile, outpf)\n    sh(cmd)", "response": "%prog wgsim fastafile\n\n    Run dwgsim on fastafile."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fig4(args):\n    p = OptionParser(fig4.__doc__)\n    p.add_option(\"--gauge_step\", default=200000, type=\"int\",\n                help=\"Step size for the base scale\")\n    opts, args, iopts = p.set_image_options(args, figsize=\"9x7\")\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    layout, datadir = args\n    layout = F4ALayout(layout, datadir=datadir)\n\n    gs = opts.gauge_step\n    fig = plt.figure(1, (iopts.w, iopts.h))\n    root = fig.add_axes([0, 0, 1, 1])\n\n    block, napusbed, slayout = \"r28.txt\", \"all.bed\", \"r28.layout\"\n    s = Synteny(fig, root, block, napusbed, slayout, chr_label=False)\n    synteny_exts = [(x.xstart, x.xend) for x in s.rr]\n\n    h = .1\n    order = \"bzh,yudal\".split(\",\")\n    labels = (r\"\\textit{B. napus} A$\\mathsf{_n}$2\",\n              r\"\\textit{B. rapa} A$\\mathsf{_r}$2\",\n              r\"\\textit{B. oleracea} C$\\mathsf{_o}$2\",\n              r\"\\textit{B. napus} C$\\mathsf{_n}$2\")\n    for t in layout:\n        xstart, xend = synteny_exts[2 * t.i]\n        canvas = [xstart, t.y, xend - xstart, h]\n        root.text(xstart - h, t.y + h / 2, labels[t.i], ha=\"center\", va=\"center\")\n        ch, ab = t.box_region.split(\":\")\n        a, b = ab.split(\"-\")\n        vlines = [int(x) for x in (a, b)]\n        Coverage(fig, root, canvas, t.seqid, (t.start, t.end), datadir,\n                     order=order, gauge=\"top\", plot_chr_label=False,\n                     gauge_step=gs, palette=\"gray\",\n                     cap=40, hlsuffix=\"regions.forhaibao\",\n                     vlines=vlines)\n\n    # Highlight GSL biosynthesis genes\n    a, b = (3, \"Bra029311\"), (5, \"Bo2g161590\")\n    for gid in (a, b):\n        start, end = s.gg[gid]\n        xstart, ystart = start\n        xend, yend = end\n        x = (xstart + xend) / 2\n        arrow = FancyArrowPatch(posA=(x, ystart - .04),\n                                posB=(x, ystart - .005),\n                                arrowstyle=\"fancy,head_width=6,head_length=8\",\n                                lw=3, fc='k', ec='k', zorder=20)\n        root.add_patch(arrow)\n\n    root.set_xlim(0, 1)\n    root.set_ylim(0, 1)\n    root.set_axis_off()\n\n    image_name = \"napus-fig4.\" + iopts.format\n    savefig(image_name, dpi=iopts.dpi, iopts=iopts)", "response": "%prog fig4 layout data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef deletion(args):\n    import math\n    from jcvi.formats.bed import Bed\n    from jcvi.graphics.chromosome import HorizontalChromosome\n    from jcvi.graphics.base import kb_formatter\n\n    p = OptionParser(deletion.__doc__)\n    opts, args, iopts = p.set_image_options(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    deletion_genes, deletions, bed = args\n    dg = [int(x) for x in open(deletion_genes)]\n    dsg, lsg = \"darkslategray\", \"lightslategray\"\n\n    fig = plt.figure(1, (iopts.w, iopts.h))\n    root = fig.add_axes([0, 0, 1, 1])\n    ax = fig.add_axes([.1, .1, .8, .8])\n    minval = 2 if deletion_genes == \"deleted-genes\" else 2048\n    bins = np.logspace(math.log(minval, 10), math.log(max(dg), 10), 16)\n    n, bins, histpatches = ax.hist(dg, bins=bins, \\\n                                   fc=lsg, alpha=.75)\n    ax.set_xscale('log', basex=2)\n    if deletion_genes == \"deleted-genes\":\n        ax.xaxis.set_major_formatter(mpl.ticker.FormatStrFormatter('%d'))\n        ax.set_xlabel('No. of deleted genes in each segment')\n    else:\n        ax.xaxis.set_major_formatter(kb_formatter)\n        ax.set_xlabel('No. of deleted bases in each segment')\n    ax.yaxis.set_major_formatter(mpl.ticker.FormatStrFormatter('%d'))\n    ax.set_ylabel('No. of segments')\n    ax.patch.set_alpha(0.1)\n\n    # Draw chromosome C2\n    na, nb = .45, .85\n    root.text((na + nb) / 2, .54, \"ChrC02\", ha=\"center\")\n    HorizontalChromosome(root, na, nb, .5, height=.025,\n                             fc=lsg, fill=True)\n\n    order = Bed(bed).order\n    fp = open(deletions)\n    scale = lambda x: na + x * (nb - na) / 52886895\n    for i, row in enumerate(fp):\n        i += 1\n        num, genes = row.split()\n        genes = genes.split(\"|\")\n        ia, a = order[genes[0]]\n        ib, b = order[genes[-1]]\n        mi, mx = a.start, a.end\n        mi, mx = scale(mi), scale(mx)\n        root.add_patch(Rectangle((mi, .475), mx - mi, .05,\n                       fc=\"red\", ec=\"red\"))\n        if i == 1:   # offset between two adjacent regions for aesthetics\n            mi -= .015\n        elif i == 2:\n            mi += .015\n        TextCircle(root, mi, .44, str(i), fc=\"red\")\n\n    for i, mi in zip(range(1, 4), (.83, .78, .73)):\n        TextCircle(root, mi, .2, str(i), fc=\"red\")\n\n    root.set_xlim(0, 1)\n    root.set_ylim(0, 1)\n    root.set_axis_off()\n\n    image_name = deletion_genes + \".pdf\"\n    savefig(image_name, dpi=iopts.dpi, iopts=iopts)", "response": "%prog deletion - genes deletion - bases"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ploidy(args):\n    p = OptionParser(ploidy.__doc__)\n    opts, args, iopts = p.set_image_options(args, figsize=\"8x7\")\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    seqidsfile, klayout = args\n\n    fig = plt.figure(1, (iopts.w, iopts.h))\n    root = fig.add_axes([0, 0, 1, 1])\n\n    Karyotype(fig, root, seqidsfile, klayout)\n\n    fc = \"darkslategrey\"\n    radius = .012\n    ot = -.05  # use this to adjust vertical position of the left panel\n    TextCircle(root, .1, .9 + ot, r'$\\gamma$', radius=radius, fc=fc)\n    root.text(.1, .88 + ot, r\"$\\times3$\", ha=\"center\", va=\"top\", color=fc)\n    TextCircle(root, .08, .79 + ot, r'$\\alpha$', radius=radius, fc=fc)\n    TextCircle(root, .12, .79 + ot, r'$\\beta$', radius=radius, fc=fc)\n    root.text(.1, .77 + ot, r\"$\\times3\\times2\\times2$\", ha=\"center\", va=\"top\", color=fc)\n    root.text(.1, .67 + ot, r\"Brassica triplication\", ha=\"center\",\n                va=\"top\", color=fc, size=11)\n    root.text(.1, .65 + ot, r\"$\\times3\\times2\\times2\\times3$\", ha=\"center\", va=\"top\", color=fc)\n    root.text(.1, .42 + ot, r\"Allo-tetraploidy\", ha=\"center\",\n                va=\"top\", color=fc, size=11)\n    root.text(.1, .4 + ot, r\"$\\times3\\times2\\times2\\times3\\times2$\", ha=\"center\", va=\"top\", color=fc)\n\n    bb = dict(boxstyle=\"round,pad=.5\", fc=\"w\", ec=\"0.5\", alpha=0.5)\n    root.text(.5, .2 + ot, r\"\\noindent\\textit{Brassica napus}\\\\\"\n                \"(A$\\mathsf{_n}$C$\\mathsf{_n}$ genome)\", ha=\"center\",\n                size=16, color=\"k\", bbox=bb)\n\n    root.set_xlim(0, 1)\n    root.set_ylim(0, 1)\n    root.set_axis_off()\n\n    pf = \"napus\"\n    image_name = pf + \".\" + iopts.format\n    savefig(image_name, dpi=iopts.dpi, iopts=iopts)", "response": "%prog ploidy seqids layout"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pasteprepare(args):\n    p = OptionParser(pasteprepare.__doc__)\n    p.add_option(\"--flank\", default=5000, type=\"int\",\n                 help=\"Get the seq of size on two ends [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    goodfasta, = args\n    flank = opts.flank\n    pf = goodfasta.rsplit(\".\", 1)[0]\n    extbed = pf + \".ext.bed\"\n\n    sizes = Sizes(goodfasta)\n    fw = open(extbed, \"w\")\n    for bac, size in sizes.iter_sizes():\n        print(\"\\t\".join(str(x) for x in \\\n                               (bac, 0, min(flank, size), bac + \"L\")), file=fw)\n        print(\"\\t\".join(str(x) for x in \\\n                               (bac, max(size - flank, 0), size, bac + \"R\")), file=fw)\n    fw.close()\n\n    fastaFromBed(extbed, goodfasta, name=True)", "response": "%prog pasteprepare good. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef paste(args):\n    from jcvi.formats.bed import uniq\n\n    p = OptionParser(paste.__doc__)\n    p.add_option(\"--maxsize\", default=300000, type=\"int\",\n            help=\"Maximum size of patchers to be replaced [default: %default]\")\n    p.add_option(\"--prefix\", help=\"Prefix of the new object [default: %default]\")\n    p.set_rclip(rclip=1)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    pbed, blastfile, bbfasta = args\n    maxsize = opts.maxsize  # Max DNA size to replace gap\n    order = Bed(pbed).order\n\n    beforebed, afterbed = blast_to_twobeds(blastfile, order, log=True,\n                                           rclip=opts.rclip, maxsize=maxsize,\n                                           flipbeds=True)\n    beforebed = uniq([beforebed])\n\n    afbed = Bed(beforebed)\n    bfbed = Bed(afterbed)\n\n    shuffle_twobeds(afbed, bfbed, bbfasta, prefix=opts.prefix)", "response": "%prog paste pbed bbfasta"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef eject(args):\n    p = OptionParser(eject.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    candidates, chrfasta = args\n    sizesfile = Sizes(chrfasta).filename\n    cbedfile = complementBed(candidates, sizesfile)\n\n    cbed = Bed(cbedfile)\n    for b in cbed:\n        b.accn = b.seqid\n        b.score = 1000\n        b.strand = '+'\n\n    cbed.print_to_file()", "response": "%prog eject candidates chr. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef closest(args):\n    p = OptionParser(closest.__doc__)\n    p.add_option(\"--om\", default=False, action=\"store_true\",\n                 help=\"The bedfile is OM blocks [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    candidates, gapsbed, fastafile = args\n    sizes = Sizes(fastafile).mapping\n    bed = Bed(candidates)\n    ranges = []\n    for b in bed:\n        r = range_parse(b.accn) if opts.om else b\n        ranges.append([r.seqid, r.start, r.end])\n\n    gapsbed = Bed(gapsbed)\n    granges = [(x.seqid, x.start, x.end) for x in gapsbed]\n\n    ranges = range_merge(ranges)\n    for r in ranges:\n        a = range_closest(granges, r)\n        b = range_closest(granges, r, left=False)\n        seqid = r[0]\n\n        if a is not None and a[0] != seqid:\n            a = None\n        if b is not None and b[0] != seqid:\n            b = None\n\n        mmin = 1 if a is None else a[1]\n        mmax = sizes[seqid] if b is None else b[2]\n\n        print(\"\\t\".join(str(x) for x in (seqid, mmin - 1, mmax)))", "response": "%prog closest candidates. bed gaps. bed fastafile"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef insert(args):\n    from jcvi.formats.agp import mask, bed\n    from jcvi.formats.sizes import agp\n\n    p = OptionParser(insert.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 4:\n        sys.exit(not p.print_help())\n\n    candidates, gapsbed, chrfasta, unplacedfasta = args\n    refinedbed = refine([candidates, gapsbed])\n    sizes = Sizes(unplacedfasta).mapping\n    cbed = Bed(candidates)\n    corder = cbed.order\n    gbed = Bed(gapsbed)\n    gorder = gbed.order\n\n    gpbed = Bed()\n    gappositions = {}  # (chr, start, end) => gapid\n\n    fp = open(refinedbed)\n    gap_to_scf = defaultdict(list)\n    seen = set()\n    for row in fp:\n        atoms = row.split()\n        if len(atoms) <= 6:\n            continue\n        unplaced = atoms[3]\n        strand = atoms[5]\n        gapid = atoms[9]\n        if gapid not in seen:\n            seen.add(gapid)\n            gi, gb = gorder[gapid]\n            gpbed.append(gb)\n            gappositions[(gb.seqid, gb.start, gb.end)] = gapid\n        gap_to_scf[gapid].append((unplaced, strand))\n\n    gpbedfile = \"candidate.gaps.bed\"\n    gpbed.print_to_file(gpbedfile, sorted=True)\n\n    agpfile = agp([chrfasta])\n    maskedagpfile = mask([agpfile, gpbedfile])\n    maskedbedfile = maskedagpfile.rsplit(\".\", 1)[0] + \".bed\"\n    bed([maskedagpfile, \"--outfile={0}\".format(maskedbedfile)])\n\n    mbed = Bed(maskedbedfile)\n    finalbed = Bed()\n    for b in mbed:\n        sid = b.seqid\n        key = (sid, b.start, b.end)\n        if key not in gappositions:\n            finalbed.add(\"{0}\\n\".format(b))\n            continue\n\n        gapid = gappositions[key]\n        scfs = gap_to_scf[gapid]\n\n        # For scaffolds placed in the same gap, sort according to positions\n        scfs.sort(key=lambda x: corder[x[0]][1].start + corder[x[0]][1].end)\n        for scf, strand in scfs:\n            size = sizes[scf]\n            finalbed.add(\"\\t\".join(str(x) for x in \\\n                    (scf, 0, size, sid, 1000, strand)))\n\n    finalbedfile = \"final.bed\"\n    finalbed.print_to_file(finalbedfile)\n\n    # Clean-up\n    toclean = [gpbedfile, agpfile, maskedagpfile, maskedbedfile]\n    FileShredder(toclean)", "response": "%prog insert candidates. bed gaps. bed chrs. fasta unplaced. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef refine(args):\n    p = OptionParser(refine.__doc__)\n    p.add_option(\"--closest\", default=False, action=\"store_true\",\n                 help=\"In case of no gaps, use closest [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    breakpointsbed, gapsbed = args\n    ncols = len(open(breakpointsbed).next().split())\n    logging.debug(\"File {0} contains {1} columns.\".format(breakpointsbed, ncols))\n    cmd = \"intersectBed -wao -a {0} -b {1}\".format(breakpointsbed, gapsbed)\n\n    pf = \"{0}.{1}\".format(breakpointsbed.split(\".\")[0], gapsbed.split(\".\")[0])\n    ingapsbed = pf + \".bed\"\n    sh(cmd, outfile=ingapsbed)\n\n    fp = open(ingapsbed)\n    data = [x.split() for x in fp]\n\n    nogapsbed = pf + \".nogaps.bed\"\n    largestgapsbed = pf + \".largestgaps.bed\"\n    nogapsfw = open(nogapsbed, \"w\")\n    largestgapsfw = open(largestgapsbed, \"w\")\n    for b, gaps in groupby(data, key=lambda x: x[:ncols]):\n        gaps = list(gaps)\n        gap = gaps[0]\n        if len(gaps) == 1 and gap[-1] == \"0\":\n            assert gap[-3] == \".\"\n            print(\"\\t\".join(b), file=nogapsfw)\n            continue\n\n        gaps = [(int(x[-1]), x) for x in gaps]\n        maxgap = max(gaps)[1]\n        print(\"\\t\".join(maxgap), file=largestgapsfw)\n\n    nogapsfw.close()\n    largestgapsfw.close()\n    beds = [largestgapsbed]\n    toclean = [nogapsbed, largestgapsbed]\n\n    if opts.closest:\n        closestgapsbed = pf + \".closestgaps.bed\"\n        cmd = \"closestBed -a {0} -b {1} -d\".format(nogapsbed, gapsbed)\n        sh(cmd, outfile=closestgapsbed)\n        beds += [closestgapsbed]\n        toclean += [closestgapsbed]\n    else:\n        pointbed = pf + \".point.bed\"\n        pbed = Bed()\n        bed = Bed(nogapsbed)\n        for b in bed:\n            pos = (b.start + b.end) / 2\n            b.start, b.end = pos, pos\n            pbed.append(b)\n        pbed.print_to_file(pointbed)\n        beds += [pointbed]\n        toclean += [pointbed]\n\n    refinedbed = pf + \".refined.bed\"\n    FileMerger(beds, outfile=refinedbed).merge()\n\n    # Clean-up\n    FileShredder(toclean)\n\n    return refinedbed", "response": "%prog refine breakpoints. bed gaps. bed"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef meta(args):\n    p = OptionParser(meta.__doc__)\n    p.add_option(\"--cutoff\", default=.5, type=\"float\",\n                 help=\"Percent observed required (chrY half cutoff)\")\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 4:\n        sys.exit(not p.print_help())\n\n    binfile, sampleids, strids, wobed = args\n    cutoff = opts.cutoff\n\n    af_file = \"allele_freq\"\n    if need_update(binfile, af_file):\n        df, m, samples, loci = read_binfile(binfile, sampleids, strids)\n        nalleles = len(samples)\n        fw = must_open(af_file, \"w\")\n        for i, locus in enumerate(loci):\n            a = m[:, i]\n            counts = alleles_to_counts(a)\n            af = counts_to_af(counts)\n            seqid = locus.split(\"_\")[0]\n            remove = counts_filter(counts, nalleles, seqid, cutoff=cutoff)\n            print(\"\\t\".join((locus, af, remove)), file=fw)\n        fw.close()\n\n    logging.debug(\"Load gene intersections from `{}`\".format(wobed))\n    fp = open(wobed)\n    gene_map = defaultdict(set)\n    for row in fp:\n        chr1, start1, end1, chr2, start2, end2, name, ov = row.split()\n        gene_map[(chr1, start1)] |= set(name.split(\",\"))\n    for k, v in gene_map.items():\n        non_enst = sorted(x for x in v if not x.startswith(\"ENST\"))\n        #enst = sorted(x.rsplit(\".\", 1)[0] for x in v if x.startswith(\"ENST\"))\n        gene_map[k] = \",\".join(non_enst)\n\n    TREDS, df = read_treds()\n\n    metafile = \"STRs_{}_SEARCH.meta.tsv\".format(timestamp())\n    write_meta(af_file, gene_map, TREDS, filename=metafile)\n    logging.debug(\"File `{}` written.\".format(metafile))", "response": "%prog meta bin samples STR. ids STR - exons. wo. bed"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bin(args):\n    p = OptionParser(bin.__doc__)\n    p.add_option(\"--dtype\", choices=(\"float32\", \"int32\"),\n                 help=\"dtype of the matrix\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    tsvfile, = args\n    dtype = opts.dtype\n    if dtype is None:  # Guess\n        dtype = np.int32 if \"data\" in tsvfile else np.float32\n    else:\n        dtype = np.int32 if dtype == \"int32\" else np.float32\n\n    print(\"dtype: {}\".format(dtype), file=sys.stderr)\n    fp = open(tsvfile)\n    next(fp)\n    arrays = []\n    for i, row in enumerate(fp):\n        a = np.fromstring(row, sep=\"\\t\", dtype=dtype)\n        a = a[1:]\n        arrays.append(a)\n        print(i, a, file=sys.stderr)\n\n    print(\"Merging\", file=sys.stderr)\n    b = np.concatenate(arrays)\n    print(\"Binary shape: {}\".format(b.shape), file=sys.stderr)\n    binfile = tsvfile.rsplit(\".\", 1)[0] + \".bin\"\n    b.tofile(binfile)", "response": "%prog bin data. tsv\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef data(args):\n    p = OptionParser(data.__doc__)\n    p.add_option(\"--notsv\", default=False, action=\"store_true\",\n                 help=\"Do not write data.tsv\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 4:\n        sys.exit(not p.print_help())\n\n    databin, sampleids, strids, metafile = args\n    final_columns, percentiles = read_meta(metafile)\n    df, m, samples, loci = read_binfile(databin, sampleids, strids)\n\n    # Clean the data\n    m %= 1000  # Get the larger of the two alleles\n    m[m == 999] = -1  # Missing data\n\n    final = set(final_columns)\n    remove = []\n    for i, locus in enumerate(loci):\n        if locus not in final:\n            remove.append(locus)\n            continue\n\n    pf = \"STRs_{}_SEARCH\".format(timestamp())\n    filteredstrids = \"{}.STR.ids\".format(pf)\n    fw = open(filteredstrids, \"w\")\n    print(\"\\n\".join(final_columns), file=fw)\n    fw.close()\n    logging.debug(\"Dropped {} columns; Retained {} columns (`{}`)\".\\\n                    format(len(remove), len(final_columns), filteredstrids))\n\n    # Remove low-quality columns!\n    df.drop(remove, inplace=True, axis=1)\n    df.columns = final_columns\n\n    filtered_bin = \"{}.data.bin\".format(pf)\n    if need_update(databin, filtered_bin):\n        m = df.as_matrix()\n        m.tofile(filtered_bin)\n        logging.debug(\"Filtered binary matrix written to `{}`\".format(filtered_bin))\n\n    # Write data output\n    filtered_tsv = \"{}.data.tsv\".format(pf)\n    if not opts.notsv and need_update(databin, filtered_tsv):\n        df.to_csv(filtered_tsv, sep=\"\\t\", index_label=\"SampleKey\")", "response": "%prog data data. bin samples. ids STR. ids meta. tsv"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mask(args):\n    p = OptionParser(mask.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) not in (2, 4):\n        sys.exit(not p.print_help())\n\n    if len(args) == 4:\n        databin, sampleids, strids, metafile = args\n        df, m, samples, loci = read_binfile(databin, sampleids, strids)\n        mode = \"STRs\"\n    elif len(args) == 2:\n        databin, metafile = args\n        df = pd.read_csv(databin, sep=\"\\t\", index_col=0)\n        m = df.as_matrix()\n        samples = df.index\n        loci = list(df.columns)\n        mode = \"TREDs\"\n\n    pf = \"{}_{}_SEARCH\".format(mode, timestamp())\n    final_columns, percentiles = read_meta(metafile)\n\n    maskfile = pf + \".mask.tsv\"\n    run_args = []\n    for i, locus in enumerate(loci):\n        a = m[:, i]\n        percentile = percentiles[locus]\n        run_args.append((i, a, percentile))\n\n    if mode == \"TREDs\" or need_update(databin, maskfile):\n        cpus = min(8, len(run_args))\n        write_mask(cpus, samples, final_columns, run_args, filename=maskfile)\n        logging.debug(\"File `{}` written.\".format(maskfile))", "response": "%prog mask data. bin samples. ids STR. ids meta. tsv"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mergecsv(args):\n    p = OptionParser(mergecsv.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    csvfiles = args\n    arrays = []\n    samplekeys = []\n    for csvfile in csvfiles:\n        samplekey = op.basename(csvfile).split(\".\")[0]\n        a = np.fromfile(csvfile, sep=\",\", dtype=np.int32)\n        x1 = a[::2]\n        x2 = a[1::2]\n        a = x1 * 1000 + x2\n        a[a < 0] = -1\n        arrays.append(a)\n        samplekeys.append(samplekey)\n        print(samplekey, a, file=sys.stderr)\n    print(\"Merging\", file=sys.stderr)\n    b = np.concatenate(arrays)\n    b.tofile(\"data.bin\")\n\n    fw = open(\"samples\", \"w\")\n    print(\"\\n\".join(samplekeys), file=fw)\n    fw.close()", "response": "%prog mergecsv csvfile > n. txt"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compilevcf(args):\n    p = OptionParser(compilevcf.__doc__)\n    p.add_option(\"--db\", default=\"hg38\", help=\"Use these lobSTR db\")\n    p.add_option(\"--nofilter\", default=False, action=\"store_true\",\n                 help=\"Do not filter the variants\")\n    p.set_home(\"lobstr\")\n    p.set_cpus()\n    p.set_aws_opts(store=\"hli-mv-data-science/htang/str-data\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    samples, = args\n    workdir = opts.workdir\n    store = opts.output_path\n    cleanup = not opts.nocleanup\n    filtered = not opts.nofilter\n    dbs = opts.db.split(\",\")\n    cwd = os.getcwd()\n    mkdir(workdir)\n    os.chdir(workdir)\n    samples = op.join(cwd, samples)\n\n    stridsfile = \"STR.ids\"\n    if samples.endswith((\".vcf\", \".vcf.gz\")):\n        vcffiles = [samples]\n    else:\n        vcffiles = [x.strip() for x in must_open(samples)]\n    if not op.exists(stridsfile):\n        ids = []\n        for db in dbs:\n            ids.extend(STRFile(opts.lobstr_home, db=db).ids)\n        uids = uniqify(ids)\n        logging.debug(\"Combined: {} Unique: {}\".format(len(ids), len(uids)))\n\n        fw = open(stridsfile, \"w\")\n        print(\"\\n\".join(uids), file=fw)\n        fw.close()\n\n    run_args = [(x, filtered, cleanup, store) for x in vcffiles]\n    cpus = min(opts.cpus, len(run_args))\n    p = Pool(processes=cpus)\n    for res in p.map_async(run_compile, run_args).get():\n        continue", "response": "%prog compilevcf samples. csv\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef liftover(args):\n    p = OptionParser(liftover.__doc__)\n    p.add_option(\"--checkvalid\", default=False, action=\"store_true\",\n                help=\"Check minscore, period and length\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    refbed, fastafile = args\n    genome = pyfasta.Fasta(fastafile)\n    edits = []\n    fp = open(refbed)\n    for i, row in enumerate(fp):\n        s = STRLine(row)\n        seq = genome[s.seqid][s.start - 1: s.end].upper()\n        s.motif = get_motif(seq, len(s.motif))\n        s.fix_counts(seq)\n        if opts.checkvalid and not s.is_valid():\n            continue\n        edits.append(s)\n        if i % 10000 == 0:\n            print(i, \"lines read\", file=sys.stderr)\n\n    edits = natsorted(edits, key=lambda x: (x.seqid, x.start))\n    for e in edits:\n        print(str(e))", "response": "%prog liftover lobstr_v3. 0. 2_hg38_ref. bed hg38. upper. fa\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef batchlobstr(args):\n    p = OptionParser(batchlobstr.__doc__)\n    p.add_option(\"--sep\", default=\",\", help=\"Separator for building commandline\")\n    p.set_home(\"lobstr\", default=\"s3://hli-mv-data-science/htang/str-build/lobSTR/\")\n    p.set_aws_opts(store=\"hli-mv-data-science/htang/str-data\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    samplesfile, = args\n    store = opts.output_path\n    computed = ls_s3(store)\n    fp = open(samplesfile)\n    skipped = total = 0\n    for row in fp:\n        total += 1\n        sample, s3file = row.strip().split(\",\")[:2]\n        exec_id, sample_id = sample.split(\"_\")\n        bamfile = s3file.replace(\".gz\", \"\").replace(\".vcf\", \".bam\")\n\n        gzfile = sample + \".{0}.vcf.gz\".format(\"hg38\")\n        if gzfile in computed:\n            skipped += 1\n            continue\n\n        print(opts.sep.join(\"python -m jcvi.variation.str lobstr\".split() + \\\n                            [\"hg38\",\n                            \"--input_bam_path\", bamfile,\n                            \"--output_path\", store,\n                            \"--sample_id\", sample_id,\n                            \"--workflow_execution_id\", exec_id,\n                            \"--lobstr_home\", opts.lobstr_home,\n                            \"--workdir\", opts.workdir]))\n    fp.close()\n    logging.debug(\"Total skipped: {0}\".format(percentage(skipped, total)))", "response": "%prog batchlobstr samples. csv"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef locus(args):\n    from jcvi.formats.sam import get_minibam\n    # See `Format-lobSTR-database.ipynb` for a list of TREDs for validation\n    INCLUDE = [\"HD\", \"SBMA\", \"SCA1\", \"SCA2\", \"SCA8\", \"SCA17\", \"DM1\", \"DM2\",\n               \"FXTAS\"]\n    db_choices = (\"hg38\", \"hg19\")\n\n    p = OptionParser(locus.__doc__)\n    p.add_option(\"--tred\", choices=INCLUDE,\n                 help=\"TRED name\")\n    p.add_option(\"--ref\", choices=db_choices, default=\"hg38\",\n                 help=\"Reference genome\")\n    p.set_home(\"lobstr\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    bamfile, = args\n    ref = opts.ref\n    lhome = opts.lobstr_home\n    tred = opts.tred\n\n    tredsfile = datafile(\"TREDs.meta.csv\")\n    tf = pd.read_csv(tredsfile, index_col=0)\n    row = tf.ix[tred]\n    tag = \"repeat_location\"\n    ldb = \"TREDs\"\n    if ref == \"hg19\":\n        tag += \".\" + ref\n        ldb += \"-\" + ref\n    seqid, start_end = row[tag].split(\":\")\n\n    PAD = 1000\n    start, end = start_end.split('-')\n    start, end = int(start) - PAD, int(end) + PAD\n    region = \"{}:{}-{}\".format(seqid, start, end)\n\n    minibamfile = get_minibam(bamfile, region)\n    c = seqid.replace(\"chr\", \"\")\n    cmd, vcf = allelotype_on_chr(minibamfile, c, lhome, ldb)\n    sh(cmd)\n\n    parser = LobSTRvcf(columnidsfile=None)\n    parser.parse(vcf, filtered=False)\n    items = parser.items()\n    if not items:\n        print(\"No entry found!\", file=sys.stderr)\n        return\n\n    k, v = parser.items()[0]\n    print(\"{} => {}\".format(tred, v.replace(',', '/')), file=sys.stderr)", "response": "%prog locus bamfile\n\n    Extract selected locus from a list of TREDs for validation, and run lobSTR."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef close(args):\n    p = OptionParser(close.__doc__)\n    p.set_home(\"gapfiller\")\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    scaffolds = args[0]\n    libtxt = write_libraries(args[1:], aligner=\"bwa\")\n\n    cmd = \"perl \" + op.join(opts.gapfiller_home, \"GapFiller.pl\")\n    cmd += \" -l {0} -s {1} -T {2}\".format(libtxt, scaffolds, opts.cpus)\n    runsh = \"run.sh\"\n    write_file(runsh, cmd)", "response": "%prog close scaffolds. fasta PE*.fastq"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef embed(args):\n    p = OptionParser(embed.__doc__)\n    p.set_mingap(default=10)\n    p.add_option(\"--min_length\", default=200, type=\"int\",\n                 help=\"Minimum length to consider [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    evidencefile, scaffolds, contigs = args\n    min_length = opts.min_length\n    splitfasta, oagp, cagp = gaps([scaffolds, \"--split\",\n                                   \"--mingap={0}\".format(opts.mingap)])\n\n    agp = AGP(cagp)\n    p = agp.graph\n\n    ef = EvidenceFile(evidencefile, contigs)\n    sizes = ef.sz\n    q = ef.graph\n\n    logging.debug(\"Reference graph: {0}\".format(p))\n    logging.debug(\"Patch graph: {0}\".format(q))\n\n    newagp = deepcopy(agp)\n\n    seen = set()\n    deleted = set()\n    for a in agp:\n        if a.is_gap:\n            continue\n\n        name = a.component_id\n        object = a.object\n        if name in deleted:\n            print(\"* Skip {0}, already embedded\".format(name), file=sys.stderr)\n            continue\n\n        seen.add(name)\n\n        target_name, tag = get_target(p, name)\n        path = q.get_path(name, target_name, tag=tag)\n        path_size = sum([sizes[x.v] for x, t in path]) if path else None\n        status = NO_UPDATE\n\n        # Heuristic, the patch must not be too long\n        if path and path_size > min_length and len(path) > 3:\n            path = None\n\n        if not path:\n            print(name, target_name, path, path_size, status, file=sys.stderr)\n            continue\n\n        backward = False\n        for x, t in path:\n            if x.v in seen:\n                print(\"* Does not allow backward\" \\\n                                     \" patch on {0}\".format(x.v), file=sys.stderr)\n                backward = True\n                break\n\n        if backward:\n            continue\n\n        # Build the path plus the ends\n        vv = q.get_node(name)\n        path.appendleft((vv, tag))\n        if tag == \">\":\n            path.reverse()\n            status = INSERT_BEFORE\n        elif target_name is None:\n            status = INSERT_AFTER\n        else:\n            target = q.get_node(target_name)\n            path.append((target, tag))\n            status = INSERT_BETWEEN\n\n        print(name, target_name, path, path_size, status, file=sys.stderr)\n\n        # Trim the ends off from the constructed AGPLines\n        lines = path_to_agp(q, path, object, sizes, status)\n        if status == INSERT_BEFORE:\n            lines = lines[:-1]\n            td = newagp.insert_lines(name, lines, \\\n                                 delete=True, verbose=True)\n        elif status == INSERT_AFTER:\n            lines = lines[1:]\n            td = newagp.insert_lines(name, lines, after=True, \\\n                                 delete=True, verbose=True)\n        else:\n            lines = lines[1:-1]\n            td = newagp.update_between(name, target_name, lines, \\\n                                 delete=True, verbose=True)\n        deleted |= td\n        seen |= td\n\n    # Recruite big singleton contigs\n    CUTOFF = opts.min_length\n    for ctg, size in sizes.items():\n        if ctg in seen:\n            continue\n        if size < CUTOFF:\n            continue\n        newagp.append(AGPLine.cline(ctg, ctg, sizes, \"?\"))\n\n    # Write a new AGP file\n    newagpfile = \"embedded.agp\"\n    newagp.print_to_file(newagpfile, index=True)\n    tidy([newagpfile, contigs])", "response": "%prog embed evidencefile scaffolds. fasta contigs. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gff(args):\n    align_choices = (\"left\", \"center\", \"right\")\n    p = OptionParser(gff.__doc__)\n    p.add_option(\"--align\", default=\"left\", choices=align_choices,\n                 help=\"Horizontal alignment [default: %default]\")\n    p.add_option(\"--noUTR\", default=False, action=\"store_true\",\n                 help=\"Do not plot UTRs [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    fig = plt.figure(1, (8, 5))\n    root = fig.add_axes([0, 0, 1, 1])\n\n    gffiles = args\n    ngenes = len(gffiles)\n\n    canvas = .6\n    setups, ratio = get_setups(gffiles, canvas=canvas, noUTR=opts.noUTR)\n    align = opts.align\n    xs = .2 if align == \"left\" else .8\n    yinterval = canvas / ngenes\n    ys = .8\n    tip = .01\n    for genename, mrnabed, cdsbeds in setups:\n        ExonGlyph(root, xs, ys, mrnabed, cdsbeds, ratio=ratio, align=align)\n        if align == \"left\":\n            root.text(xs - tip, ys, genename, ha=\"right\", va=\"center\")\n        elif align == \"right\":\n            root.text(xs + tip, ys, genename, ha=\"left\", va=\"center\")\n        ys -= yinterval\n\n    root.set_xlim(0, 1)\n    root.set_ylim(0, 1)\n    root.set_axis_off()\n\n    figname = \"exons.pdf\"\n    savefig(figname, dpi=300)", "response": "%prog gff gfffile > gff2. gff"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef spades(args):\n    from jcvi.formats.fastq import readlen\n\n    p = OptionParser(spades.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) == 0:\n        sys.exit(not p.print_help())\n\n    folder, = args\n    for p, pf in iter_project(folder):\n        rl = readlen([p[0], \"--silent\"])\n\n        # <http://spades.bioinf.spbau.ru/release3.1.0/manual.html#sec3.4>\n        kmers = None\n        if rl >= 150:\n            kmers = \"21,33,55,77\"\n        elif rl >= 250:\n            kmers = \"21,33,55,77,99,127\"\n\n        cmd = \"spades.py\"\n        if kmers:\n            cmd += \" -k {0}\".format(kmers)\n        cmd += \" --careful\"\n        cmd += \" --pe1-1 {0} --pe1-2 {1}\".format(*p)\n        cmd += \" -o {0}_spades\".format(pf)\n        print(cmd)", "response": "%prog spades folder\n\n    Run automated SPADES."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef allpaths(args):\n    p = OptionParser(allpaths.__doc__)\n    p.add_option(\"--ploidy\", default=\"1\", choices=(\"1\", \"2\"),\n                 help=\"Ploidy [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) == 0:\n        sys.exit(not p.print_help())\n\n    folders = args\n    for pf in folders:\n        if not op.isdir(pf):\n            continue\n        assemble_dir(pf, target=[\"final.contigs.fasta\", \"final.assembly.fasta\"],\n                     ploidy=opts.ploidy)", "response": "%prog allpaths folder1 folder2..."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef prepare(args):\n    p = OptionParser(prepare.__doc__)\n    p.add_option(\"--first\", default=0, type=\"int\",\n                 help=\"Use only first N reads [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    jfile, = args\n    metafile = jfile + \".meta\"\n\n    if need_update(jfile, metafile):\n        fp = open(jfile)\n        fastqfiles = [x.strip() for x in fp if \".fastq\" in x]\n        metas = [Meta(x) for x in fastqfiles]\n\n        fw = open(metafile, \"w\")\n        print(\"\\n\".join(str(x) for x in metas), file=fw)\n        print(\"Now modify `{0}`, and restart this script.\".\\\n                            format(metafile), file=sys.stderr)\n        print(\"Each line is : genome library fastqfile\", file=sys.stderr)\n        fw.close()\n        return\n\n    mf = MetaFile(metafile)\n    for m in mf:\n        m.make_link(firstN=opts.first)", "response": "%prog prepare jira. txt"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nassembles pairs of reads into contigs. fasta.", "response": "def assemble_pairs(p, pf, tag, target=[\"final.contigs.fasta\"]):\n    \"\"\"\n    Take one pair of reads and assemble to contigs.fasta.\n    \"\"\"\n    slink(p, pf, tag)\n    assemble_dir(pf, target)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef correct_pairs(p, pf, tag):\n    from jcvi.assembly.preprocess import correct as cr\n\n    logging.debug(\"Work on {0} ({1})\".format(pf, ','.join(p)))\n    itag = tag[0]\n    cm = \".\".join((pf, itag))\n    targets = (cm + \".1.corr.fastq\", cm + \".2.corr.fastq\", \\\n                pf + \".PE-0.corr.fastq\")\n    if not need_update(p, targets):\n        logging.debug(\"Corrected reads found: {0}. Skipped.\".format(targets))\n        return\n\n    slink(p, pf, tag)\n\n    cwd = os.getcwd()\n    os.chdir(pf)\n    cr(sorted(glob(\"*.fastq\") + glob(\"*.fastq.gz\")) + [\"--nofragsdedup\"])\n    sh(\"mv {0}.1.corr.fastq ../{1}\".format(itag, targets[0]))\n    sh(\"mv {0}.2.corr.fastq ../{1}\".format(itag, targets[1]))\n    sh(\"mv frag_reads_corr.corr.fastq ../{0}\".format(targets[2]))\n\n    logging.debug(\"Correction finished: {0}\".format(targets))\n    os.chdir(cwd)", "response": "Takes one pair of reads and correct to generate *. corr. fastq."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef soap_trios(p, pf, tag, extra):\n    from jcvi.assembly.soap import prepare\n\n    logging.debug(\"Work on {0} ({1})\".format(pf, ','.join(p)))\n    asm = \"{0}.closed.scafSeq\".format(pf)\n    if not need_update(p, asm):\n        logging.debug(\"Assembly found: {0}. Skipped.\".format(asm))\n        return\n\n    slink(p, pf, tag, extra)\n\n    cwd = os.getcwd()\n    os.chdir(pf)\n    prepare(sorted(glob(\"*.fastq\") + glob(\"*.fastq.gz\")) + \\\n            [\"--assemble_1st_rank_only\", \"-K 31\"])\n    sh(\"./run.sh\")\n    sh(\"cp asm31.closed.scafSeq ../{0}\".format(asm))\n\n    logging.debug(\"Assembly finished: {0}\".format(asm))\n    os.chdir(cwd)", "response": "Take one pair of reads and 'widow' reads after correction and run SOAP."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef soapX(args):\n    p = OptionParser(soapX.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) < 2:\n        sys.exit(not p.print_help())\n\n    folder, tag = args[:2]\n    extra = args[2:]\n    extra = [get_abs_path(x) for x in extra]\n    tag = tag.split(\",\")\n    for p, pf in iter_project(folder, n=3):\n        soap_trios(p, pf, tag, extra)", "response": "%prog soapX folder tag [. fastq... ]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef correctX(args):\n    p = OptionParser(correctX.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    folder, tag = args\n    tag = tag.split(\",\")\n    for p, pf in iter_project(folder):\n        correct_pairs(p, pf, tag)", "response": "%prog correctX folder tag\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef allpathsX(args):\n    p = OptionParser(allpathsX.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    folder, tag = args\n    tag = tag.split(\",\")\n    for p, pf in iter_project(folder):\n        assemble_pairs(p, pf, tag)", "response": "%prog allpathsX folder tag"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mcluster(args):\n    p = OptionParser(mcluster.__doc__)\n    add_consensus_options(p)\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    consensusfiles = args\n    minlength = opts.minlength\n    cpus = opts.cpus\n    pf = opts.prefix\n    pctid = find_pctid(consensusfiles)\n\n    pf += \".P{0}\".format(pctid)\n    consensusfile = pf + \".consensus.fasta\"\n    if need_update(consensusfiles, consensusfile):\n        fw_cons = must_open(consensusfile, \"w\")\n        totalseqs = 0\n        for cf in consensusfiles:\n            nseqs = 0\n            s = op.basename(cf).split(\".\")[0]\n            for name, seq in parse_fasta(cf):\n                name = '.'.join((s, name))\n                print(\">{0}\\n{1}\".format(name, seq), file=fw_cons)\n                nseqs += 1\n            logging.debug(\"Read `{0}`: {1} seqs\".format(cf, nseqs))\n            totalseqs += nseqs\n        logging.debug(\"Total: {0} seqs\".format(totalseqs))\n        fw_cons.close()\n\n    userfile = pf + \".u\"\n    notmatchedfile = pf + \".notmatched\"\n    if need_update(consensusfile, userfile):\n        cluster_smallmem(consensusfile, userfile, notmatchedfile,\n                         minlength, pctid, cpus)\n\n    clustfile = pf + \".clust\"\n    if need_update((consensusfile, userfile, notmatchedfile), clustfile):\n        makeclust(consensusfile, userfile, notmatchedfile, clustfile)\n\n    clustSfile = pf + \".clustS\"\n    if need_update(clustfile, clustSfile):\n        parallel_musclewrap(clustfile, cpus, minsamp=opts.minsamp)", "response": "%prog mcluster *.consensus\n\n    Cluster across samples using consensus sequences."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mconsensus(args):\n    p = OptionParser(mconsensus.__doc__)\n    p.add_option(\"--allele_counts\", default=\"allele_counts\",\n                 help=\"Directory to generate allele counts\")\n    add_consensus_options(p)\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    consensusfiles = args\n    prefix = opts.prefix\n    acdir = opts.allele_counts\n    store = ClustStores(consensusfiles)\n    pctid = find_pctid(consensusfiles)\n    pf = prefix + \".P{0}\".format(pctid)\n\n    clustSfile = pf + \".clustS\"\n    AC = makeloci(clustSfile, store, prefix,\n                  minsamp=opts.minsamp, pctid=pctid)\n\n    mkdir(acdir)\n    acfile = pf + \".allele_counts\"\n    fw = open(acfile, \"w\")\n    seen = DefaultOrderedDict(list)        # chr, pos => taxa\n    print(\"# \" + \"\\t\".join(ACHEADER), file=fw)\n    # Sort allele counts into separate files\n    for ac in AC:\n        chrpos = ac.chr, ac.pos\n        seen[chrpos].append(ac)\n        print(ac.tostring(taxon=True), file=fw)\n    fw.close()\n\n    logging.debug(\"Populate all taxa and instantiate empty vector if missing\")\n    all_taxa = set([op.basename(x).split(\".\")[0] for x in consensusfiles])\n    taxon_to_ac = defaultdict(list)\n    for chrpos, aclist in seen.items():\n        included_taxa = set([x.taxon for x in aclist])\n        missing_taxa = all_taxa - included_taxa\n        template = deepcopy(aclist[0])\n        template.clear()\n        for ac in aclist:\n            taxon_to_ac[ac.taxon].append(ac)\n        for tx in missing_taxa:\n            taxon_to_ac[tx].append(template)\n\n    logging.debug(\"Write allele counts for all taxa\")\n    for tx, aclist in sorted(taxon_to_ac.items()):\n        tx_acfile = op.join(acdir, tx + \".allele_counts\")\n        fw = open(tx_acfile, \"w\")\n        print(\"# \" + \"\\t\".join(ACHEADER_NO_TAXON), file=fw)\n        for ac in aclist:\n            print(ac.tostring(), file=fw)\n        fw.close()\n        logging.debug(\"Written {0} sites in `{1}`\".\\\n                format(len(aclist), tx_acfile))", "response": "%prog mconsensus *.consensus\n\n    Call consensus along the stacks from cross-sample clustering."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef consensus(args):\n    p = OptionParser(consensus.__doc__)\n    p.add_option(\"--ploidy\", default=2, type=\"int\",\n                 help=\"Number of haplotypes per locus\")\n    add_consensus_options(p)\n    p.set_verbose()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    clustSfile, = args\n    pf = clustSfile.rsplit(\".\", 1)[0]\n    mindepth = opts.mindepth\n    minlength = opts.minlength\n    verbose = opts.verbose\n\n    C = ClustFile(clustSfile)\n    output = []\n    bins = []\n    indices = []\n    start = end = 0  # Index into base count array\n    for data in C:\n        names, seqs, nreps = zip(*data)\n        total_nreps = sum(nreps)\n        # Depth filter\n        if total_nreps < mindepth:\n            continue\n\n        first_name, first_seq, first_nrep = data[0]\n        fname = first_name.split(\";\")[0] + \";size={0};\".format(total_nreps)\n        cons_name, cons_seq, cons_nrep = get_seed(data)\n        if len(data) > 1 and cons_name != CONSTAG:\n            logging.debug(\"Tag {0} not found in cluster {1}\".\\\n                        format(CONSTAG, cons_name))\n\n        # List for sequence data\n        S = [(seq, nrep) for name, seq, nrep in data if nrep]\n        # Pileups for base counting\n        RAD = stack(S)\n\n        if len(data) == 1:   # No computation needed\n            output.append((fname, seq))\n            bins.extend(RAD)\n            start = end\n            end += len(seq)\n            indices.append((fname, start, end))\n            continue\n\n        shortcon, shortRAD = compute_consensus(fname, cons_seq, \\\n                                RAD, S, total_nreps,\n                                mindepth=mindepth, verbose=verbose)\n        if len(shortcon) < minlength:\n            cons_seq = seq\n            shortcon, shortRAD = compute_consensus(fname, first_seq,\\\n                                RAD, S, total_nreps,\n                                mindepth=mindepth, verbose=verbose)\n\n        if len(shortcon) < minlength:   # Stop trying\n            continue\n\n        output.append((fname, shortcon))\n        bins.extend(shortRAD)\n\n        start = end\n        end += len(shortcon)\n        indices.append((fname, start, end))\n\n    consensfile = pf + \".consensus\"\n    consens = open(consensfile, 'w')\n    for k, v in output:\n        print(\"\\n\".join((k, v)), file=consens)\n    consens.close()\n    logging.debug(\"Consensus sequences written to `{0}`\".format(consensfile))\n\n    binfile = consensfile + \".bin\"\n    bins = np.array(bins, dtype=np.uint32)\n    ulimit = 65535\n    bins[bins > ulimit] = ulimit\n    bins = np.array(bins, dtype=np.uint16)  # Compact size\n    bins.tofile(binfile)\n    logging.debug(\"Allele counts written to `{0}`\".format(binfile))\n\n    idxfile = consensfile + \".idx\"\n    fw = open(idxfile, \"w\")\n    for fname, start, end in indices:\n        print(\"\\t\".join(str(x) for x in (fname, start, end)), file=fw)\n    fw.close()\n    logging.debug(\"Serializing indices to `{0}`\".format(idxfile))\n\n    return consensfile, binfile, idxfile", "response": "%prog consensus clustSfile\n\n    Call consensus along the stacks. Tabulate bases at each site, tests for\n    errors according to error rate, calls consensus."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a list of bases at a site D make counts of bases", "response": "def stack(S):\n    \"\"\"\n    From list of bases at a site D,  make counts of bases\n    \"\"\"\n    S, nreps = zip(*S)\n    S = np.array([list(x) for x in S])\n    rows, cols = S.shape\n    counts = []\n    for c in xrange(cols):\n        freq = [0] * NBASES\n        for b, nrep in zip(S[:, c], nreps):\n            freq[BASES.index(b)] += nrep\n        counts.append(freq)\n    return counts"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind the position of the first and last base sequence in a sequence", "response": "def get_left_right(seq):\n    \"\"\"\n    Find position of the first and last base\n    \"\"\"\n    cseq = seq.strip(GAPS)\n    leftjust = seq.index(cseq[0])\n    rightjust = seq.rindex(cseq[-1])\n\n    return leftjust, rightjust"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cons(f, mindepth):\n    C = ClustFile(f)\n    for data in C:\n        names, seqs, nreps = zip(*data)\n        total_nreps = sum(nreps)\n        # Depth filter\n        if total_nreps < mindepth:\n            continue\n\n        S = []\n        for name, seq, nrep in data:\n            # Append sequence * number of dereps\n            S.append([seq, nrep])\n\n        # Make list for each site in sequences\n        res = stack(S)\n        yield [x[:4] for x in res if sum(x[:4]) >= mindepth]", "response": "Returns a list of lists of reads at each site in the file f."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef makeC(N):\n    C = defaultdict(int)\n    for d in N:\n        C[tuple(d)] += 1\n\n    return [i for i in C.items() if (0, 0, 0, 0) not in i]", "response": "Makes a dictionary with counts of base counts [ x to 0 x x"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef estimateHE(args):\n    p = OptionParser(estimateHE.__doc__)\n    add_consensus_options(p)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    clustSfile, = args\n    HEfile = clustSfile.rsplit(\".\", 1)[0] + \".HE\"\n    if not need_update(clustSfile, HEfile):\n        logging.debug(\"File `{0}` found. Computation skipped.\".format(HEfile))\n        return HEfile\n\n    D = []\n    for d in cons(clustSfile, opts.mindepth):\n        D.extend(d)\n\n    logging.debug(\"Computing base frequencies ...\")\n    P = makeP(D)\n    C = makeC(D)\n    logging.debug(\"Solving log-likelihood function ...\")\n    x0 = [.01, .001]  # initital values\n    H, E = scipy.optimize.fmin(LL, x0, args=(P, C))\n\n    fw = must_open(HEfile, \"w\")\n    print(H, E, file=fw)\n    fw.close()\n\n    return HEfile", "response": "%prog estimateHE clustSfile\n\n    Estimate heterozygosity (H) and error rate (E). Idea borrowed heavily from\n    the PyRad paper."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef alignfast(names, seqs):\n    matfile = op.join(datadir, \"blosum80.mat\")\n    cmd = \"poa -read_fasta - -pir stdout {0} -tolower -silent -hb -fuse_all\".format(matfile)\n    p = Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=STDOUT, close_fds=True)\n    s = \"\"\n    for i, j in zip(names, seqs):\n        s += \"\\n\".join((i, j)) + \"\\n\"\n    return p.communicate(s)[0]", "response": "Performs MUSCLE alignments on cluster and returns output as string"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfilters contigs within the same sample.", "response": "def filter_samples(names, seqs, sep='.'):\n    \"\"\"\n    When there are uncollapsed contigs within the same sample, only retain the\n    first seq, or the seq that is most abundant (with cluster_size).\n    \"\"\"\n    seen = set()\n    filtered_names, filtered_seqs = [], []\n    for name, seq in zip(names, seqs):\n        samp = name.split(sep, 1)[0]\n        if samp in seen:\n            continue\n        seen.add(samp)\n        filtered_names.append(name)\n        filtered_seqs.append(seq)\n\n    nfiltered, nnames = len(filtered_names), len(names)\n    assert nfiltered == len(seen)\n\n    return filtered_names, filtered_seqs, seen"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cluster(args):\n    p = OptionParser(cluster.__doc__)\n    add_consensus_options(p)\n    p.set_align(pctid=95)\n    p.set_outdir()\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) < 2:\n        sys.exit(not p.print_help())\n\n    prefix = args[0]\n    fastqfiles = args[1:]\n    cpus = opts.cpus\n    pctid = opts.pctid\n    mindepth = opts.mindepth\n    minlength = opts.minlength\n    fastafile, qualfile = fasta(fastqfiles + [\"--seqtk\",\n                                \"--outdir={0}\".format(opts.outdir),\n                                \"--outfile={0}\".format(prefix + \".fasta\")])\n\n    prefix = op.join(opts.outdir, prefix)\n    pf = prefix + \".P{0}\".format(pctid)\n    derepfile = prefix + \".derep\"\n    if need_update(fastafile, derepfile):\n        derep(fastafile, derepfile, minlength, cpus)\n\n    userfile = pf + \".u\"\n    notmatchedfile = pf + \".notmatched\"\n    if need_update(derepfile, userfile):\n        cluster_smallmem(derepfile, userfile, notmatchedfile,\n                         minlength, pctid, cpus)\n\n    clustfile = pf + \".clust\"\n    if need_update((derepfile, userfile, notmatchedfile), clustfile):\n        makeclust(derepfile, userfile, notmatchedfile, clustfile,\n                  mindepth=mindepth)\n\n    clustSfile = pf + \".clustS\"\n    if need_update(clustfile, clustSfile):\n        parallel_musclewrap(clustfile, cpus)\n\n    statsfile = pf + \".stats\"\n    if need_update(clustSfile, statsfile):\n        makestats(clustSfile, statsfile, mindepth=mindepth)", "response": "%prog cluster prefix fastqfiles"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef asciiplot(x, y, digit=1, width=50, title=None, char=\"=\"):\n    ax = np.array(x)\n    ay = np.array(y)\n\n    if title:\n        print(dark(title), file=sys.stderr)\n\n    az = ay * width / ay.max()\n    tx = [asciiaxis(x, digit=digit) for x in ax]\n    rjust = max([len(x) for x in tx]) + 1\n\n    for x, y, z in zip(tx, ay, az):\n        x = x.rjust(rjust)\n        y = y or \"\"\n        z = green(char * z)\n        print(\"{0} |{1} {2}\".format(x, z, y), file=sys.stderr)", "response": "Print out a horizontal plot using ASCII chars."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef print_colors(palette, outfile=\"Palette.png\"):\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n\n    xmax = 20 * (len(palette) + 1)\n    x1s = np.arange(0, xmax, 20)\n    xintervals = [10] * len(palette)\n    xx = zip(x1s, xintervals)\n    ax.broken_barh(xx, (5, 10), facecolors=palette)\n\n    ax.set_ylim(0, 20)\n    ax.set_xlim(0, xmax)\n    ax.set_axis_off()\n\n    savefig(outfile)", "response": "Print color palette to a PNG file for quick check"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef discrete_rainbow(N=7, cmap=cm.Set1, usepreset=True, shuffle=False, \\\n                     plot=False):\n    \"\"\"\n    Return a discrete colormap and the set of colors.\n\n    modified from\n    <http://www.scipy.org/Cookbook/Matplotlib/ColormapTransformations>\n\n    cmap: colormap instance, eg. cm.jet.\n    N: Number of colors.\n\n    Example\n    >>> x = resize(arange(100), (5,100))\n    >>> djet = cmap_discretize(cm.jet, 5)\n    >>> imshow(x, cmap=djet)\n\n    See available matplotlib colormaps at:\n    <http://dept.astro.lsa.umich.edu/~msshin/science/code/matplotlib_cm/>\n\n    If N>20 the sampled colors might not be very distinctive.\n    If you want to error and try anyway, set usepreset=False\n    \"\"\"\n    import random\n    from scipy import interpolate\n\n    if usepreset:\n        if 0 < N <= 5:\n            cmap = cm.gist_rainbow\n        elif N <= 20:\n            cmap = cm.Set1\n        else:\n            sys.exit(discrete_rainbow.__doc__)\n\n    cdict = cmap._segmentdata.copy()\n    # N colors\n    colors_i = np.linspace(0,1.,N)\n    # N+1 indices\n    indices = np.linspace(0,1.,N+1)\n    rgbs = []\n    for key in ('red','green','blue'):\n       # Find the N colors\n       D = np.array(cdict[key])\n       I = interpolate.interp1d(D[:,0], D[:,1])\n       colors = I(colors_i)\n       rgbs.append(colors)\n       # Place these colors at the correct indices.\n       A = np.zeros((N+1,3), float)\n       A[:,0] = indices\n       A[1:,1] = colors\n       A[:-1,2] = colors\n       # Create a tuple for the dictionary.\n       L = []\n       for l in A:\n           L.append(tuple(l))\n       cdict[key] = tuple(L)\n\n    palette = zip(*rgbs)\n\n    if shuffle:\n        random.shuffle(palette)\n\n    if plot:\n        print_colors(palette)\n\n    # Return (colormap object, RGB tuples)\n    return mpl.colors.LinearSegmentedColormap('colormap',cdict,1024), palette", "response": "Return a discrete colormap and the set of colors."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_messages(ax, messages):\n    tc = \"gray\"\n    axt = ax.transAxes\n    yy = .95\n    for msg in messages:\n        ax.text(.95, yy, msg, color=tc, transform=axt, ha=\"right\")\n        yy -= .05", "response": "Write text on canvas usually on the top right corner."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots a quick plot of the data.", "response": "def quickplot_ax(ax, data, xmin, xmax, xlabel, title=None, ylabel=\"Counts\",\n                 counts=True, percentage=True, highlight=None):\n    '''\n    TODO: redundant with quickplot(), need to be refactored.\n    '''\n    if percentage:\n        total_length = sum(data.values())\n        data = dict((k, v * 100. / total_length) for (k, v) in data.items())\n\n    left, height = zip(*sorted(data.items()))\n    pad = max(height) * .01\n    c1, c2 = \"darkslategray\", \"tomato\"\n    if counts:\n        for l, h in zip(left, height):\n            if xmax and l > xmax:\n                break\n            tag = str(int(h))\n            rotation = 90\n            if percentage:\n                tag = append_percentage(tag) if int(tag) > 0 else \"\"\n                rotation = 0\n            color = c1\n            if highlight is not None and l in highlight:\n                color = c2\n            ax.text(l, h + pad, tag, color=color, size=8,\n                     ha=\"center\", va=\"bottom\", rotation=rotation)\n    if xmax is None:\n        xmax = max(left)\n\n    ax.bar(left, height, align=\"center\", fc=c1)\n    if highlight:\n        for h in highlight:\n            ax.bar([h], [data[h]], align=\"center\", ec=c2, fc=c2)\n\n    ax.set_xlabel(markup(xlabel))\n    if ylabel:\n        ax.set_ylabel(markup(ylabel))\n    if title:\n        ax.set_title(markup(title))\n    ax.set_xlim((xmin - .5, xmax + .5))\n    if percentage:\n        ax.set_ylim(0, 100)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the name parts of the hierarchy.", "response": "def get_name_parts(au):\n    \"\"\"\n    Fares Z. Najar => last, first, initials\n\n    >>> get_name_parts(\"Fares Z. Najar\")\n    ('Najar', 'Fares', 'F.Z.')\n    \"\"\"\n    parts = au.split()\n    first = parts[0]\n    middle = [x for x in parts if x[-1] == '.']\n    middle = \"\".join(middle)\n\n    last = [x for x in parts[1:] if x[-1] != '.']\n    last = \" \".join(last)\n    initials = \"{0}.{1}\".format(first[0], middle)\n    if first[-1] == '.':  # Some people use full middle name\n        middle, last = last.split(None, 1)\n        initials = \"{0}.{1}.\".format(first[0], middle)\n\n    return last, first, initials"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_names(lstfile):\n    from jcvi.formats.base import read_block\n\n    fp = open(lstfile)\n    all_authors = []\n    for header, seq in read_block(fp, \"[\"):\n        seq = \" \".join(seq)\n        authors = []\n        for au in seq.split(\",\"):\n            au = au.strip()\n            if not au:\n                continue\n            au = string.translate(au, None, string.digits)\n            #au = au.replace(\"-\", '')\n            authors.append(au)\n        all_authors.append(authors)\n\n    out = []\n    for authors in all_authors:\n        blocks = []\n        for au in authors:\n            last, first, initials = get_name_parts(au)\n            suffix = \"\"\n            nameblock = NameTemplate.format(last=last, first=first,\n                    initials=initials, suffix=suffix)\n            blocks.append(nameblock)\n        bigblock = \",\\n\".join(blocks)\n        out.append(bigblock)\n\n    return out", "response": "Parses the names of the alternative file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef names(args):\n    p = OptionParser(names.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(p.print_help())\n\n    namelist, templatefile = args\n\n    # First check the alternative format\n    if open(namelist).read()[0] == '[':\n        out = parse_names(namelist)\n        make_template(templatefile, out)\n        return\n\n    reader = csv.reader(open(namelist), delimiter=\"\\t\")\n    header = next(reader)\n    ncols = len(header)\n    assert ncols > 3\n    nextras = ncols - 3\n\n    blocks = []\n    bools = []\n    for row in reader:\n        first, middle, last = row[:3]\n        extras = row[3:]\n        bools.append([(x.upper() == 'Y') for x in extras])\n        middle = middle.strip()\n        if middle != \"\":\n            middle = middle.rstrip('.') + '.'\n        initials = \"{0}.{1}\".format(first[0], middle)\n        suffix = \"\"\n        nameblock = NameTemplate.format(last=last, first=first,\n                initials=initials, suffix=suffix)\n        blocks.append(nameblock)\n\n    selected_idx = zip(*bools)\n    out = [] * nextras\n    for i, sbools in enumerate(selected_idx):\n        selected = []\n        for b, ss in zip(blocks, sbools):\n            if ss:\n                selected.append(b)\n        bigblock = \",\\n\".join(selected)\n        out.append(bigblock)\n        logging.debug(\"List N{0} contains a total of {1} names.\".format(i,\n            len(selected)))\n\n    make_template(templatefile, out)", "response": "%prog names namelist templatefile"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main():\n    p = OptionParser(main.__doc__)\n    p.add_option(\"-g\", \"--graphic\", default=False, action=\"store_true\",\n            help=\"Create boilerplate for a graphic script\")\n\n    opts, args = p.parse_args()\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    script, = args\n    imports = graphic_imports if opts.graphic else default_imports\n    app = graphic_app if opts.graphic else default_app\n    template = default_template.format(imports, app)\n    write_file(script, template)\n\n    message = \"template writes to `{0}`\".format(script)\n    if opts.graphic:\n        message = \"graphic \" + message\n    message = message[0].upper() + message[1:]\n    logging.debug(message)", "response": "%prog scriptname.py\n\n    create a minimal boilerplate for a new script"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nunpacking a list of ambiguous characters into a single list of strings.", "response": "def unpack_ambiguous(s):\n    \"\"\"\n    List sequences with ambiguous characters in all possibilities.\n    \"\"\"\n    sd = [ambiguous_dna_values[x] for x in s]\n    return [\"\".join(x) for x in list(product(*sd))]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef split(args):\n    p = OptionParser(split.__doc__)\n    p.set_outdir(outdir=\"deconv\")\n    p.add_option(\"--nocheckprefix\", default=False, action=\"store_true\",\n                 help=\"Don't check shared prefix [default: %default]\")\n    p.add_option(\"--paired\", default=False, action=\"store_true\",\n                 help=\"Paired-end data [default: %default]\")\n    p.add_option(\"--append\", default=False, action=\"store_true\",\n                 help=\"Append barcode to 2nd read [default: %default]\")\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) < 2:\n        sys.exit(not p.print_help())\n\n    barcodefile = args[0]\n    fastqfile = args[1:]\n    paired = opts.paired\n    append = opts.append\n    if append:\n        assert paired, \"--append only works with --paired\"\n\n    nfiles = len(fastqfile)\n\n    barcodes = []\n    fp = open(barcodefile)\n    for row in fp:\n        id, seq = row.split()\n        for s in unpack_ambiguous(seq):\n            barcodes.append(BarcodeLine._make((id, s)))\n\n    nbc = len(barcodes)\n    logging.debug(\"Imported {0} barcodes (ambiguous codes expanded).\".format(nbc))\n    checkprefix = not opts.nocheckprefix\n\n    if checkprefix:\n        # Sanity check of shared prefix\n        excludebarcodes = []\n        for bc in barcodes:\n            exclude = []\n            for s in barcodes:\n                if bc.id == s.id:\n                    continue\n\n                assert bc.seq != s.seq\n                if s.seq.startswith(bc.seq) and len(s.seq) > len(bc.seq):\n                    logging.error(\"{0} shares same prefix as {1}.\".format(s, bc))\n                    exclude.append(s)\n            excludebarcodes.append(exclude)\n    else:\n        excludebarcodes = nbc * [[]]\n\n    outdir = opts.outdir\n    mkdir(outdir)\n\n    cpus = opts.cpus\n    logging.debug(\"Create a pool of {0} workers.\".format(cpus))\n    pool = Pool(cpus)\n\n    if paired:\n        assert nfiles == 2, \"You asked for --paired, but sent in {0} files\".\\\n                            format(nfiles)\n        split_fun = append_barcode_paired if append else split_barcode_paired\n        mode = \"paired\"\n    else:\n        split_fun = split_barcode\n        mode = \"single\"\n\n    logging.debug(\"Mode: {0}\".format(mode))\n\n    pool.map(split_fun, \\\n             zip(barcodes, excludebarcodes,\n             nbc * [outdir], nbc * [fastqfile]))", "response": "%prog split barcodefile fastqfile1..."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a set of all the alleles in the given hierarchy.", "response": "def expand_alleles(p, tolerance=0):\n    \"\"\"\n    Returns expanded allele set given the tolerance.\n    \"\"\"\n    _p = set()\n    for x in p:\n        _p |= set(range(x - tolerance, x + tolerance + 1))\n    return _p"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a set of possible progenies in a trio.", "response": "def get_progenies(p1, p2, x_linked=False, tolerance=0):\n    \"\"\"\n    Returns possible progenies in a trio.\n    \"\"\"\n    _p1 = expand_alleles(p1, tolerance=tolerance)\n    _p2 = expand_alleles(p2, tolerance=tolerance)\n    possible_progenies = set(tuple(sorted(x)) for x in product(_p1, _p2))\n    if x_linked:  # Add all hemizygotes\n        possible_progenies |= set((x, x) for x in (set(_p1) | set(_p2)))\n    return possible_progenies"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mendelian2(args):\n    p = OptionParser(mendelian2.__doc__)\n    p.add_option(\"--treds\", default=None,\n                 help=\"Extract specific treds, use comma to separate\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    triofile, hlitsv = args\n    repo = TREDsRepo()\n    treds = opts.treds.split(\",\") if opts.treds else repo.names\n    triodata = pd.read_csv(triofile, sep=\"\\t\")\n    samplekey = lambda x: x.split(\"_\")[1]\n    trios = []\n    for i, row in triodata.iterrows():\n        proband = row[\"proband\"]\n        parents = row[\"parents\"]\n        proband_sex = row[\"proband_sex\"]\n        parents_sex = row[\"parent1_sex,parent2_sex\"]\n        proband = samplekey(proband)\n        p1, p2 = parents.split(\",\")\n        p1, p2 = samplekey(p1), samplekey(p2)\n        p1_sex, p2_sex = parents_sex.split(\",\")\n        if p1_sex == \"Male\":\n            p1, p2 = p2, p1\n            p1_sex, p2_sex = p2_sex, p1_sex\n        trios.append((proband, proband_sex, p1, p1_sex, p2, p2_sex))\n\n    header = \"{0}_ID {0}_Sex {0}_Calls\"\n    header += \" {0}_Full {0}_Partial {0}_Repeat {0}_Paired\"\n    tredsdata = pd.read_csv(hlitsv, sep=\"\\t\", low_memory=False)\n    tsvfiles = []\n    summary = open(\"Trios.summary.csv\", \"w\")\n    summary_header = \"Name,Motif,Inheritance,N_Correct,N_Error,N_missing,\" \\\n                     \"ErrorRate [N_Error / (N_Correct + N_Error))]\"\n    print(summary_header, file=summary)\n    print(summary_header)\n    for tred in treds:\n        if tred in (\"FXS\", \"AR\"):\n            continue\n        tr = repo[tred]\n        tsvfile = \"{}.details.tsv\".format(tred)\n        fw = open(tsvfile, \"w\")\n        td = {}\n        for i, row in tredsdata.iterrows():\n            s = str(row[\"SampleKey\"])\n            inferredGender = row[\"inferredGender\"]\n            try:\n                calls = row[tred + \".calls\"]\n                fdp = int(row[tred + \".FDP\"])\n                pdp = int(row[tred + \".PDP\"])\n                rdp = int(row[tred + \".RDP\"])\n                pedp = int(row[tred + \".PEDP\"])\n                td[s] = [str(x) for x in (inferredGender,\n                            calls, fdp, pdp, rdp, pedp)]\n            except:\n                continue\n\n        h = \" \".join((header.format(\"P1\"), header.format(\"P2\"),\n                      header.format(\"Kid\")))\n        print(\"\\t\".join([\"MendelianError\"] + h.split()), file=fw)\n        tredcall = lambda x: td.get(x, [\"\", \"-1|-1\", \"\", \"\", \"\", \"\"])[:]\n        counts = defaultdict(int)\n        is_xlinked = repo[tred].is_xlinked\n        shorten = lambda x: str(int(x[-4:]))   # Simplify SampleKey\n        for proband, proband_sex, p1, p1_sex, p2, p2_sex in trios:\n            tp1 = tredcall(p1)\n            tp2 = tredcall(p2)\n            tpp = tredcall(proband)\n            m = mendelian_check(tp1, tp2, tpp, is_xlinked=is_xlinked)\n            counts[m] += 1\n            if is_xlinked:\n                for (p, p_sex) in ((tp1, p1_sex), (tp2, p2_sex),\n                                   (tpp, proband_sex)):\n                    if p[1].startswith(\"-\"):\n                        p[1] = \"n.a.\"\n            cells = [shorten(p1), p1_sex] + tp1[1:]\n            cells += [shorten(p2), p2_sex] + tp2[1:]\n            cells += [shorten(proband), proband_sex] + tpp[1:]\n            print(\"\\t\".join([m] + cells), file=fw)\n        fw.close()\n        tsvfiles.append(tsvfile)\n\n        error_rate = counts[\"Error\"] * 100. / \\\n            (counts[\"Correct\"] + counts[\"Error\"])\n        line = \",\".join(str(x) for x in (tred, tr.motif, tr.inheritance,\n                        counts[\"Correct\"], counts[\"Error\"], counts[\"Missing\"],\n                        \"{:.1f}%\".format(error_rate)))\n        print(line, file=summary)\n        print(line)\n\n    # Combine into a master spreadsheet\n    import xlwt\n    wb = xlwt.Workbook()\n    converter = lambda x: int(x) if is_number(x, cast=int) else x\n    header = xlwt.easyxf(\"font: bold on, name Helvetica; align: horiz center\")\n    hc = \"font: name Helvetica; align: horiz center;\"\n    horiz_center = xlwt.Style.easyxf(hc)\n    correct = xlwt.Style.easyxf(\n        hc + \"pattern: pattern solid, fore_colour light_green;\")\n    error = xlwt.Style.easyxf(hc + \"pattern: pattern solid, fore_colour rose;\")\n    missing = xlwt.Style.easyxf(\n        hc + \"pattern: pattern solid, fore_colour light_yellow;\")\n    for tsvfile in tsvfiles:\n        sheet = op.basename(tsvfile).split(\".\", 1)[0]\n        ws = wb.add_sheet(sheet)\n        fp = open(tsvfile, 'rb')\n        reader = csv.reader(fp, delimiter=\"\\t\")\n        for r, row in enumerate(reader):\n            style = header if r == 0 else horiz_center\n            for c, col in enumerate(row):\n                    if c == 0 and r:\n                        style = {\"Correct\": correct, \"Error\": error,\n                                \"Missing\": missing}[col]\n            ws.write(r, c, converter(col), style)\n            ws.set_panes_frozen(True)\n            ws.set_horz_split_pos(1)\n\n    wb.save('Trios.xls')\n    summary.close()", "response": "%prog mendelian2 trio hlitsv"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncompares two TRED calls for Parent1 Parent2 and Proband.", "response": "def mendelian_check(tp1, tp2, tpp, is_xlinked=False):\n    \"\"\"\n    Compare TRED calls for Parent1, Parent2 and Proband.\n    \"\"\"\n    call_to_ints = lambda x: tuple(int(_) for _ in x.split(\"|\") if _ != \".\")\n    tp1_sex, tp1_call = tp1[:2]\n    tp2_sex, tp2_call = tp2[:2]\n    tpp_sex, tpp_call = tpp[:2]\n    # tp1_evidence = sum(int(x) for x in tp1[2:])\n    # tp2_evidence = sum(int(x) for x in tp2[2:])\n    # tpp_evidence = sum(int(x) for x in tpp[2:])\n    tp1_call = call_to_ints(tp1_call)\n    tp2_call = call_to_ints(tp2_call)\n    tpp_call = call_to_ints(tpp_call)\n    possible_progenies = set(tuple(sorted(x)) \\\n                    for x in product(tp1_call, tp2_call))\n    if is_xlinked and tpp_sex == \"Male\":\n        possible_progenies = set(tuple((x,)) for x in tp1_call)\n    if -1 in tp1_call or -1 in tp2_call or -1 in tpp_call:\n        tag = \"Missing\"\n    # elif tp1_evidence < 2 or tp2_evidence < 2 or tpp_evidence < 2:\n    #    tag = \"Missing\"\n    else:\n        tag = \"Correct\" if tpp_call in possible_progenies else \"Error\"\n    return tag"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef in_region(rname, rstart, target_chr, target_start, target_end):\n    return (rname == target_chr) and \\\n           (target_start <= rstart <= target_end)", "response": "Quick check if a point is within a target region."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef depth(args):\n    import seaborn as sns\n\n    p = OptionParser(depth.__doc__)\n    opts, args, iopts = p.set_image_options(args, figsize=\"14x14\")\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    tsvfile, = args\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(ncols=2, nrows=2,\n                                    figsize=(iopts.w, iopts.h))\n    plt.tight_layout(pad=6)\n\n    data = pd.read_csv(tsvfile, sep=\"\\t\", low_memory=False)\n\n    ids, treds = read_treds()\n    for (dp, ax, title) in zip((\"FDP\", \"PDP\", \"RDP\", \"PEDP\"),\n                        (ax1, ax2, ax3, ax4),\n                        (\"Spanning reads\", \"Partial reads\",\n                         \"Repeat-only reads\", \"Paired-end reads\")):\n        logging.debug(\"Build {}\".format(title))\n        # Construct related data structure\n        xd = []     # (tred, dp)\n        mdp = []    # (tred, median_dp)\n        for tred, motif in zip(treds[\"abbreviation\"], treds[\"motif\"]):\n            if tred in ignore:\n                logging.debug(\"Ignore {}\".format(tred))\n                continue\n            if len(motif) > 4:\n                if \"/\" in motif:  # CTG/CAG\n                    motif = motif.split(\"/\")[0]\n                else:\n                    motif = motif[:4] + \"..\"\n            xtred = \"{} {}\".format(tred, motif)\n            md = [x for x in data[tred + '.' + dp] if x >= 0]\n            subsample = 10000 if dp == \"RDP\" else 1000\n            md = sample(md, subsample)\n            pmd = [x for x in md if x > 0]\n            median = np.median(pmd) if pmd else 0\n            mdp.append((xtred, median))\n            for d in md:\n                xd.append((xtred, d))\n\n        # Determine order\n        mdp.sort(key=lambda x: x[1])\n        order, mdp = zip(*mdp)\n\n        # OK, now plot\n        xt, xd = zip(*xd)\n        sns.boxplot(xt, xd, ax=ax, order=order, fliersize=2)\n        xticklabels = ax.get_xticklabels()\n        ax.set_xticklabels(xticklabels, rotation=45, ha=\"right\")\n        ax.set_title(\"Number of {} per locus\".format(title), size=18)\n        ylim = 30 if dp == \"RDP\" else 100\n        ax.set_ylim(0, ylim)\n\n        yticklabels = [int(x) for x in ax.get_yticks()]\n        ax.set_yticklabels(yticklabels, family='Helvetica', size=14)\n\n    root = fig.add_axes([0, 0, 1, 1])\n    pad = .04\n    panel_labels(root, ((pad,  1 - pad, \"A\"), (1 / 2. + pad / 2,  1 - pad, \"B\"),\n                        (pad, .5 - pad / 2, \"C\"), (1 / 2. + pad / 2, .5 - pad / 2, \"D\")))\n    normalize_axes(root)\n\n    image_name = \"depth.\" + iopts.format\n    savefig(image_name, dpi=iopts.dpi, iopts=iopts)", "response": "%prog depth DP. tsv"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract_trios(family):\n    self_key = [\"ChildSelf\"]\n    keys = family.keys()\n    spouse_key = [x for x in keys if (\"spouse\" in x.lower())]\n    assert len(spouse_key) <= 1\n    parent_keys = [x for x in keys if \\\n                    (\"parent\" in x.lower()) and (\"grand\" not in x.lower())]\n    sib_keys = [x for x in keys if (\"sibling\" in x.lower()) \\\n                    or (\"twin\" in x.lower())] + self_key\n    child_keys  = [x for x in keys if \\\n                    (\"child\"  in x.lower()) and (\"grand\" not in x.lower()) \\\n                    and (\"self\" not in x.lower())]\n\n    for sk in sib_keys:\n        yield TrioOrDuo(parent_keys, [sk], family)\n    for ck in child_keys:\n        yield TrioOrDuo(self_key + spouse_key, [ck], family)", "response": "Yields all the trios and duos in a family."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_tred_tsv(tsvfile):\n    df = pd.read_csv(tsvfile, sep=\"\\t\", index_col=0, dtype={\"SampleKey\": str})\n    return df", "response": "Read the TRED table into a dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mendelian(args):\n    p = OptionParser(mendelian.__doc__)\n    p.add_option(\"--tolerance\", default=0, type=\"int\",\n                 help=\"Tolernace for differences\")\n    p.set_verbose()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    triosjson, tredtsv = args\n    verbose = opts.verbose\n    tolerance = opts.tolerance\n\n    js = json.load(open(triosjson))\n    allterms = set()\n    duos = set()\n    trios = set()\n    for v in js:\n        allterms |= set(v.keys())\n        for trio_or_duo in extract_trios(v):\n            assert len(trio_or_duo) in (2, 3)\n            if len(trio_or_duo) == 2:\n                duos.add(trio_or_duo)\n            else:\n                trios.add(trio_or_duo)\n    # print \"\\n\".join(allterms)\n    print(\"A total of {} families imported\".format(len(js)))\n\n    # Read in all data\n    df = read_tred_tsv(tredtsv)\n\n    ids, treds = read_treds()\n    table = {}\n    for tred, inheritance in zip(treds[\"abbreviation\"], treds[\"inheritance\"]):\n        x_linked = inheritance[0] == 'X'   # X-linked\n        name = tred\n        if x_linked:\n            name += \" (X-linked)\"\n        print(\"[TRED] {}\".format(name))\n\n        n_total = len(duos)\n        n_error = 0\n        for duo in duos:\n            n_error += duo.check_mendelian(df, tred, tolerance=tolerance,\n                                            x_linked=x_linked, verbose=verbose)\n        tag = \"Duos  - Mendelian errors\"\n        print(\"{}: {}\".format(tag, percentage(n_error, n_total)))\n        duo_error = percentage(n_error, n_total, mode=2)\n        table[(name, tag)] = \"{0:.1f}%\".format(duo_error)\n\n        n_total = len(trios)\n        n_error = 0\n        for trio in trios:\n            n_error += trio.check_mendelian(df, tred, tolerance=tolerance,\n                                            x_linked=x_linked, verbose=verbose)\n        tag = \"Trios - Mendelian errors\"\n        print(\"{}: {}\".format(tag, percentage(n_error, n_total)))\n        trio_error = percentage(n_error, n_total, mode=2)\n        table[(name, tag)] = \"{0:.1f}%\".format(trio_error)\n\n    # Summarize\n    print(tabulate(table))", "response": "%prog mendelian trios_candidate. json hli. 20170424. tred. tsv"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mini(args):\n    p = OptionParser(mini.__doc__)\n    p.add_option(\"--pad\", default=20000, type=\"int\",\n                 help=\"Add padding to the STR reigons\")\n    p.add_option(\"--treds\", default=None,\n                 help=\"Extract specific treds, use comma to separate\")\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    bamfile, minibam = args\n    treds = opts.treds.split(\",\") if opts.treds else None\n    pad = opts.pad\n    bedfile = make_STR_bed(pad=pad, treds=treds)\n\n    get_minibam_bed(bamfile, bedfile, minibam)\n    logging.debug(\"Mini-BAM written to `{}`\".format(minibam))", "response": "%prog mini bamfile minibamfile"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef likelihood(args):\n    p = OptionParser(likelihood.__doc__)\n    opts, args, iopts = p.set_image_options(args, figsize=\"10x5\",\n                                style=\"white\", cmap=\"coolwarm\")\n\n    if len(args) != 0:\n        sys.exit(not p.print_help())\n\n    fig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1,\n                                   figsize=(iopts.w, iopts.h))\n    plt.tight_layout(pad=4)\n\n    # Haploid model\n    LL, CI_h1, CI_h2, MLE = parse_log(\"100_100.log\")\n    data = []\n    for k, v in LL.items():\n        data.append((k[0], v))\n    data.sort()\n    x, y = zip(*data)\n    x = np.array(x)\n    curve, = ax1.plot(x, y, \"-\", color=lsg, lw=2)\n    ax1.set_title(\"Simulated haploid ($h^{truth}=100$)\")\n\n    h_hat, max_LL = max(data, key=lambda x: x[-1])\n    _, min_LL = min(data, key=lambda x: x[-1])\n    ymin, ymax = ax1.get_ylim()\n    ax1.set_ylim([ymin, ymax + 30])\n\n    LL_label = \"log(Likelihood)\"\n    ax1.plot([h_hat, h_hat], [ymin, max_LL], \":\", color=lsg, lw=2)\n    ax1.text(h_hat, max_LL + 10, r\"$\\hat{h}=93$\", color=lsg)\n    ax1.set_xlabel(r\"$h$\")\n    ax1.set_ylabel(LL_label)\n\n    a, b = CI_h1\n    ci = ax1.fill_between(x, [ymin] * len(x), y, where=(x >= a) & (x <= b),\n                     color=lsg, alpha=.5)\n    ax1.legend([curve, ci], [\"Likelihood curve\", r'95$\\%$ CI'], loc='best')\n\n    # Diploid model\n    LL, CI_h1, CI_h2, MLE = parse_log(\"100_20.log\")\n    h_hat, max_LL = max(data, key=lambda x: x[-1])\n    _, min_LL = min(data, key=lambda x: x[-1])\n    data = np.ones((301, 301)) * min_LL\n    for k, v in LL.items():\n        a, b = k\n        data[a, b] = v\n        data[b, a] = v\n\n    data = mask_upper_triangle(data)\n    ax_imshow(ax2, data, opts.cmap, LL_label, 20, 104)\n\n    root = fig.add_axes([0, 0, 1, 1])\n    pad = .04\n    panel_labels(root, ((pad / 2, 1 - pad, \"A\"), (1 / 2., 1 - pad, \"B\")))\n    normalize_axes(root)\n\n    image_name = \"likelihood.\" + iopts.format\n    savefig(image_name, dpi=iopts.dpi, iopts=iopts)", "response": "%prog likelihood\n\n    Plot likelihood surface. Look for two files in the current folder:\n    - 100_100.log, haploid model\n    - 100_20.log, diploid model"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef likelihood3(args):\n    from matplotlib import gridspec\n\n    p = OptionParser(likelihood3.__doc__)\n    opts, args, iopts = p.set_image_options(args, figsize=\"10x10\",\n                                style=\"white\", cmap=\"coolwarm\")\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    jsonfile1, jsonfile2 = args\n    fig = plt.figure(figsize=(iopts.w, iopts.h))\n    gs = gridspec.GridSpec(9, 2)\n    ax1 = fig.add_subplot(gs[:4, 0])\n    ax2 = fig.add_subplot(gs[:2, 1])\n    ax3 = fig.add_subplot(gs[2:4, 1])\n    ax4 = fig.add_subplot(gs[5:, 0])\n    ax5 = fig.add_subplot(gs[5:7, 1])\n    ax6 = fig.add_subplot(gs[7:, 1])\n    plt.tight_layout(pad=2)\n\n    plot_panel(jsonfile1, ax1, ax2, ax3, opts.cmap)\n    plot_panel(jsonfile2, ax4, ax5, ax6, opts.cmap)\n\n    root = fig.add_axes([0, 0, 1, 1])\n    pad = .02\n    panel_labels(root, ((pad, 1 - pad, \"A\"), (pad, 4. / 9, \"B\")))\n    normalize_axes(root)\n\n    image_name = \"likelihood3.\" + iopts.format\n    savefig(image_name, dpi=iopts.dpi, iopts=iopts)", "response": "%prog likelihood3 140_20. json 140_70. json"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef allelefreqall(args):\n    p = OptionParser(allelefreqall.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    reportfile, = args\n    treds, df = read_treds(reportfile)\n    # Prepare 5 pages, each page with 6 distributions\n    treds = sorted(treds)\n    count = 6\n    pdfs = []\n    for page in xrange(len(treds) / count + 1):\n        start = page * count\n        page_treds = treds[start: start + count]\n        if not page_treds:\n            break\n        allelefreq([\",\".join(page_treds), \"--usereport\", reportfile,\n                    \"--nopanels\", \"--figsize\", \"12x16\"])\n        outpdf = \"allelefreq.{}.pdf\".format(page)\n        sh(\"mv allelefreq.pdf {}\".format(outpdf))\n        pdfs.append(outpdf)\n\n    from jcvi.formats.pdf import cat\n\n    pf = op.basename(reportfile).split(\".\")[0]\n    finalpdf = pf + \".allelefreq.pdf\"\n    logging.debug(\"Merging pdfs into `{}`\".format(finalpdf))\n    cat(pdfs + [\"-o\", finalpdf, \"--cleanup\"])", "response": "%prog allelefreqall HN_Platinum_Gold.20180525.tsv.report.txt\n\n    Plot all 30 STR allele frequencies."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef simulate(args):\n    p = OptionParser(simulate.__doc__)\n    p.add_option(\"--method\", choices=(\"wgsim\", \"eagle\"), default=\"eagle\",\n                 help=\"Read simulator\")\n    p.add_option(\"--ref\", default=\"hg38\", choices=(\"hg38\", \"hg19\"),\n                 help=\"Reference genome version\")\n    p.add_option(\"--tred\", default=\"HD\", help=\"TRED locus\")\n    add_simulate_options(p)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    rundir, startunits, endunits = args\n    ref = opts.ref\n    ref_fasta = \"/mnt/ref/{}.upper.fa\".format(ref)\n    startunits, endunits = int(startunits), int(endunits)\n    basecwd = os.getcwd()\n    mkdir(rundir)\n    os.chdir(rundir)\n    cwd = os.getcwd()\n\n    # TRED region (e.g. Huntington)\n    pad_left, pad_right = 1000, 10000\n    repo = TREDsRepo(ref=ref)\n    tred = repo[opts.tred]\n    chr, start, end = tred.chr, tred.repeat_start, tred.repeat_end\n\n    logging.debug(\"Simulating {}\".format(tred))\n    fasta = Fasta(ref_fasta)\n    seq_left = fasta[chr][start - pad_left:start - 1]\n    seq_right = fasta[chr][end: end + pad_right]\n    motif = tred.repeat\n\n    simulate_method = wgsim if opts.method == \"wgsim\" else eagle\n    # Write fake sequence\n    for units in range(startunits, endunits + 1):\n        pf = str(units)\n        mkdir(pf)\n        os.chdir(pf)\n        seq = str(seq_left) + motif * units + str(seq_right)\n        fastafile = pf + \".fasta\"\n        make_fasta(seq, fastafile, id=chr.upper())\n\n        # Simulate reads on it\n        simulate_method([fastafile, \"--depth={}\".format(opts.depth),\n                          \"--readlen={}\".format(opts.readlen),\n                          \"--distance={}\".format(opts.distance),\n                          \"--outfile={}\".format(pf)])\n\n        read1 = pf + \".bwa.read1.fastq\"\n        read2 = pf + \".bwa.read2.fastq\"\n        samfile, _ = align([ref_fasta, read1, read2])\n        indexed_samfile = index([samfile])\n\n        sh(\"mv {} ../{}.bam\".format(indexed_samfile, pf))\n        sh(\"mv {}.bai ../{}.bam.bai\".format(indexed_samfile, pf))\n\n        os.chdir(cwd)\n        shutil.rmtree(pf)\n\n    os.chdir(basecwd)", "response": "%prog simulate run_dir startunits endunits"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef batchlobstr(args):\n    p = OptionParser(batchlobstr.__doc__)\n    p.add_option(\"--haploid\", default=\"chrY,chrM\",\n                 help=\"Use haploid model for these chromosomes\")\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    bamlist, = args\n    cmd = \"python -m jcvi.variation.str lobstr TREDs\"\n    cmd += \" --input_bam_path {}\"\n    cmd += \" --haploid {}\".format(opts.haploid)\n    cmd += \" --simulation\"\n    cmds = [cmd.format(x.strip()) for x in open(bamlist).readlines()]\n    p = Parallel(cmds, cpus=opts.cpus)\n    p.run()", "response": "%prog batchlobstr bamlist\n\n    Run lobSTR on a list of BAMs. The corresponding batch command for TREDPARSE:\n    $ tred.py bamlist --haploid chr4 --workdir tredparse_results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compilevcf(args):\n    from jcvi.variation.str import LobSTRvcf\n\n    p = OptionParser(compilevcf.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    folder, = args\n    vcf_files = iglob(folder, \"*.vcf,*.vcf.gz\")\n    for vcf_file in vcf_files:\n        try:\n            p = LobSTRvcf(columnidsfile=None)\n            p.parse(vcf_file, filtered=False)\n            res = p.items()\n            if res:\n                k, v = res[0]\n                res = v.replace(',', '/')\n            else:\n                res = \"-1/-1\"\n            num = op.basename(vcf_file).split(\".\")[0]\n            print(num, res)\n        except (TypeError, AttributeError) as e:\n            p = TREDPARSEvcf(vcf_file)\n            continue", "response": "%prog compilevcf dir\n\n    Compile vcf outputs into lists."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndrawing a jointplot of the sequence of reads.", "response": "def draw_jointplot(figname, x, y, data=None, kind=\"reg\", color=None,\n                   xlim=None, ylim=None, format=\"pdf\"):\n    \"\"\"\n    Wraps around sns.jointplot\n    \"\"\"\n    import seaborn as sns\n    sns.set_context('talk')\n    plt.clf()\n\n    register = {\"MeanCoverage\": \"Sample Mean Coverage\",\n                \"HD.FDP\": \"Depth of full spanning reads\",\n                \"HD.PDP\": \"Depth of partial spanning reads\",\n                \"HD.PEDP\": \"Depth of paired-end reads\",\n                \"HD.2\": \"Repeat size of the longer allele\"}\n\n    g = sns.jointplot(x, y, data=data, kind=kind, color=color,\n                      xlim=xlim, ylim=ylim)\n    g.ax_joint.set_xlabel(register.get(x, x))\n    g.ax_joint.set_ylabel(register.get(y, y))\n    savefig(figname + \".\" + format, cleanup=False)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_lo_hi_from_CI(s, exclude=None):\n    a, b = s.split(\"|\")\n    ai, aj = a.split(\"-\")\n    bi, bj = b.split(\"-\")\n\n    los = [int(ai), int(bi)]\n    his = [int(aj), int(bj)]\n    if exclude and exclude in los:\n        los.remove(exclude)\n    if exclude and exclude in his:\n        his.remove(exclude)\n    return max(los), max(his)", "response": "Parse the confidence interval from CI."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tredparse(args):\n    p = OptionParser(tredparse.__doc__)\n    p.add_option('--maxinsert', default=300, type=\"int\",\n                 help=\"Maximum number of repeats\")\n    add_simulate_options(p)\n    opts, args, iopts = p.set_image_options(args, figsize=\"10x10\")\n\n    if len(args) != 0:\n        sys.exit(not p.print_help())\n\n    depth = opts.depth\n    max_insert = opts.maxinsert\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(ncols=2, nrows=2,\n                                   figsize=(iopts.w, iopts.h))\n    plt.tight_layout(pad=3)\n\n    # ax1: lobSTR vs TREDPARSE with haploid model\n    lobstr_results = parse_results(\"lobstr_results_homo-20x-150bp-500bp.txt\")\n    tredparse_results = parse_results(\"tredparse_results_homo-20x-150bp-500bp.txt\")\n    title = SIMULATED_HAPLOID + r\" (Depth=$%s\\times$)\" % depth\n    plot_compare(ax1, title, tredparse_results, lobstr_results,\n                 max_insert=max_insert)\n\n    # ax2: lobSTR vs TREDPARSE with diploid model (depth=20x)\n    lobstr_results = parse_results(\"lobstr_results_het-20x-150bp-500bp.txt\", exclude=20)\n    tredparse_results = parse_results(\"tredparse_results_het-20x-150bp-500bp.txt\", exclude=20)\n    title = SIMULATED_DIPLOID + r\" (Depth=$%s\\times$)\" % depth\n    plot_compare(ax2, title, tredparse_results, lobstr_results,\n                 max_insert=max_insert)\n\n    # ax3: lobSTR vs TREDPARSE with diploid model (depth=5x)\n    lobstr_results = parse_results(\"lobstr_results_het-5x-150bp-500bp.txt\", exclude=20)\n    tredparse_results = parse_results(\"tredparse_results_het-5x-150bp-500bp.txt\", exclude=20)\n    title = SIMULATED_DIPLOID + r\" (Depth=$%s\\times$)\" % 5\n    plot_compare(ax3, title, tredparse_results, lobstr_results,\n                 max_insert=max_insert)\n\n    # ax4: lobSTR vs TREDPARSE with diploid model (depth=80x)\n    lobstr_results = parse_results(\"lobstr_results_het-80x-150bp-500bp.txt\", exclude=20)\n    tredparse_results = parse_results(\"tredparse_results_het-80x-150bp-500bp.txt\", exclude=20)\n    title = SIMULATED_DIPLOID + r\" (Depth=$%s\\times$)\" % 80\n    plot_compare(ax4, title, tredparse_results, lobstr_results,\n                 max_insert=max_insert)\n\n    for ax in (ax1, ax2, ax3, ax4):\n        ax.set_xlim(0, max_insert)\n        ax.set_ylim(0, max_insert)\n\n    root = fig.add_axes([0, 0, 1, 1])\n    pad = .03\n    panel_labels(root, ((pad / 2, 1 - pad, \"A\"), (1 / 2., 1 - pad, \"B\"),\n                        (pad / 2, 1 / 2. , \"C\"), (1 / 2., 1 / 2. , \"D\")))\n    normalize_axes(root)\n\n    image_name = \"tredparse.\" + iopts.format\n    savefig(image_name, dpi=iopts.dpi, iopts=iopts)", "response": "%prog tredparse\n\n    Compare performances of various variant callers on simulated STR datasets.\n    Adds coverage comparisons as panel C and D."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplotting a histogram of the number of unique entries in a set of data.", "response": "def loghistogram(data, base=2, ascii=True, title=\"Counts\", summary=False):\n    \"\"\"\n    bins is a dictionary with key: log(x, base), value: counts.\n    \"\"\"\n    from jcvi.utils.cbook import percentage\n\n    if summary:\n        unique = len(data)\n        total = sum(data)\n\n        # Print out a distribution\n        print(\"Unique: {0}\".format(percentage(unique, total)), file=sys.stderr)\n\n    bins = defaultdict(int)\n    for d in data:\n        logd = int(log(d, base))\n        bins[logd] += 1\n\n    x, y = [], []\n    for size, number in sorted(bins.items()):\n        lb, ub = base ** size, base ** (size + 1)\n        x.append((lb, ub))\n        y.append(number)\n\n    asciiplot(x, y, title=title)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stem_leaf_plot(data, vmin, vmax, bins, digit=1, title=None):\n    '''\n    Generate stem and leaf plot given a collection of numbers\n    '''\n    assert bins > 0\n    range = vmax - vmin\n    step = range * 1. / bins\n    if isinstance(range, int):\n        step = int(ceil(step))\n\n    step = step or 1\n\n    bins = np.arange(vmin, vmax + step, step)\n    hist, bin_edges = np.histogram(data, bins=bins)\n    # By default, len(bin_edges) = len(hist) + 1\n    bin_edges = bin_edges[:len(hist)]\n    asciiplot(bin_edges, hist, digit=digit, title=title)\n    print(\"Last bin ends in {0}, inclusive.\".format(vmax), file=sys.stderr)\n\n    return bin_edges, hist", "response": "Generate stem and leaf plot given a collection of numbers vmax and bins"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a histogram of the numberfile.", "response": "def histogram(numberfile, vmin, vmax, xlabel, title, outfmt=\"pdf\",\n              bins=50, skip=0, col=0, ascii=False, base=0, fill=\"white\"):\n    \"\"\"\n    Generate histogram using number from numberfile, and only numbers in the\n    range of (vmin, vmax)\n    \"\"\"\n    if ascii:\n        return texthistogram([numberfile], vmin, vmax, title=title,\n                bins=bins, skip=skip, col=col, base=base)\n\n    data, vmin, vmax = get_data(numberfile, vmin, vmax, skip=skip, col=col)\n    outfile = numberfile + \".base{0}.{1}\".format(base, outfmt) \\\n                if base else numberfile + \".pdf\"\n    template = histogram_log_template if base else histogram_template\n    rtemplate = RTemplate(template, locals())\n    rtemplate.run()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef histogram_multiple(numberfiles, vmin, vmax, xlabel, title, outfmt=\"pdf\",\n                       tags=None, bins=20, skip=0, ascii=False,\n                       facet=False, fill=\"white\", prefix=\"\"):\n    \"\"\"\n    Generate histogram using number from numberfile, and only numbers in the\n    range of (vmin, vmax). First combining multiple files.\n    \"\"\"\n    if ascii:\n        return texthistogram(numberfiles, vmin, vmax, title=title,\n                bins=bins, skip=skip)\n\n    newfile = \"_\".join(op.basename(x).split(\".\")[0] for x in numberfiles)\n\n    fw = open(newfile, \"w\")\n    print(\"{0}\\tgrp\".format(xlabel), file=fw)\n\n    if tags:\n        tags = tags.split(\",\")\n\n    for i, f in enumerate(numberfiles):\n        data, va, vb = get_data(f, vmin, vmax, skip=skip)\n        vmin = min(vmin, va)\n        vmax = max(vmax, vb)\n\n        fp = open(f)\n        if tags:\n            tag = tags[i]\n        else:\n            tag = op.basename(f).rsplit(\".\", 1)[0]\n        for row in fp:\n            val = row.strip()\n            print(\"\\t\".join((val, tag)), file=fw)\n    fw.close()\n\n    numberfile = newfile\n    outfile = numberfile + '.' + outfmt\n    if prefix:\n        outfile = prefix + outfile\n    htemplate = histogram_multiple_template_b \\\n                    if facet else histogram_multiple_template_a\n    rtemplate = RTemplate(htemplate, locals())\n    rtemplate.run()", "response": "Generate a histogram of the numberfiles in the specified range."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prepare(args):\n    valid_enzymes = \"ApeKI|ApoI|BamHI|EcoT22I|HinP1I|HpaII|MseI|MspI|\" \\\n                    \"NdeI|PasI|PstI|Sau3AI|SbfI|AsiSI-MspI|BssHII-MspI|\" \\\n                    \"FseI-MspI|PaeR7I-HhaI|PstI-ApeKI|PstI-EcoT22I|PstI-MspI\" \\\n                    \"PstI-TaqI|SalI-MspI|SbfI-MspI\".split(\"|\")\n    p = OptionParser(prepare.__doc__)\n    p.add_option(\"--enzyme\", default=\"ApeKI\", choices=valid_enzymes,\n                 help=\"Restriction enzyme used [default: %default]\")\n    p.set_home(\"tassel\")\n    p.set_aligner(aligner=\"bwa\")\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    barcode, reference = args\n    thome = opts.tassel_home\n    reference = get_abs_path(reference)\n    folders = (\"fastq\", \"tagCounts\", \"mergedTagCounts\", \"topm\",\n               \"tbt\", \"mergedTBT\", \"hapmap\", \"hapmap/raw\",\n               \"hapmap/mergedSNPs\", \"hapmap/filt\", \"hapmap/bpec\")\n    for f in folders:\n        mkdir(f)\n\n    # Build the pipeline\n    runsh = []\n    o = \"-i fastq -k {0} -e {1} -o tagCounts\".format(barcode, opts.enzyme)\n    cmd = run_pipeline(thome, \"FastqToTagCountPlugin\", o)\n    runsh.append(cmd)\n\n    o = \"-i tagCounts -o mergedTagCounts/myMasterTags.cnt\"\n    o += \" -c 5 -t mergedTagCounts/myMasterTags.cnt.fq\"\n    cmd = run_pipeline(thome, \"MergeMultipleTagCountPlugin\", o)\n    runsh.append(cmd)\n    runsh.append(\"cd mergedTagCounts\")\n\n    cmd = \"python -m jcvi.apps.{0} align --cpus {1}\".\\\n                format(opts.aligner, opts.cpus)\n    cmd += \" {0} myMasterTags.cnt.fq\".format(reference)\n    runsh.append(cmd)\n    runsh.append(\"cd ..\")\n\n    o = \"-i mergedTagCounts/*.sam -o topm/myMasterTags.topm\"\n    cmd = run_pipeline(thome, \"SAMConverterPlugin\", o)\n    runsh.append(cmd)\n\n    o = \"-i mergedTBT/myStudy.tbt.byte -y -m topm/myMasterTags.topm\"\n    o += \" -mUpd topm/myMasterTagsWithVariants.topm\"\n    o += \" -o hapmap/raw/myGBSGenos_chr+.hmp.txt\"\n    o += \" -mnF 0.8 -p myPedigreeFile.ped -mnMAF 0.02 -mnMAC 100000\"\n    o += \" -ref {0} -sC 1 -eC 10\".format(reference)\n    cmd = run_pipeline(thome, \"TagsToSNPByAlignmentPlugin\", o)\n    runsh.append(cmd)\n\n    o = \"-hmp hapmap/raw/myGBSGenos_chr+.hmp.txt\"\n    o += \" -o hapmap/mergedSNPs/myGBSGenos_mergedSNPs_chr+.hmp.txt\"\n    o += \" -misMat 0.1 -p myPedigreeFile.ped -callHets -sC 1 -eC 10\"\n    cmd = run_pipeline(thome, \"MergeDuplicateSNPsPlugin\", o)\n    runsh.append(cmd)\n\n    o = \"-hmp hapmap/mergedSNPs/myGBSGenos_mergedSNPs_chr+.hmp.txt\"\n    o += \" -o hapmap/filt/myGBSGenos_mergedSNPsFilt_chr+.hmp.txt\"\n    o += \" -mnTCov 0.01 -mnSCov 0.2 -mnMAF 0.01 -sC 1 -eC 10\"\n    #o += \"-hLD -mnR2 0.2 -mnBonP 0.005\"\n    cmd = run_pipeline(thome, \"GBSHapMapFiltersPlugin\", o)\n    runsh.append(cmd)\n\n    runfile = \"run.sh\"\n    write_file(runfile, \"\\n\".join(runsh))", "response": "%prog prepare barcode_key. csv reference. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_ImageMapLine(tlx, tly, brx, bry, w, h, dpi, chr, segment_start, segment_end):\n    tlx, brx = [canvas2px(x, w, dpi) for x in (tlx, brx)]\n    tly, bry = [canvas2px(y, h, dpi) for y in (tly, bry)]\n    chr, bac_list = chr.split(':')\n    return '<area shape=\"rect\" coords=\"' + \\\n           \",\".join(str(x) for x in (tlx, tly, brx, bry)) \\\n           + '\" href=\"#' + chr + '\"' \\\n           + ' title=\"' + chr + ':' + str(segment_start) + '..' + str(segment_end) + '\"' \\\n           + ' />'", "response": "Writes out an image map line with the coordinates passed to this function"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef batch(args):\n    p = OptionParser(batch.__doc__)\n    set_align_options(p)\n    p.set_sam_options()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    ref_fasta, proj_dir, outdir = args\n    outdir = outdir.rstrip(\"/\")\n    s3dir = None\n    if outdir.startswith(\"s3://\"):\n        s3dir = outdir\n        outdir = op.basename(outdir)\n        mkdir(outdir)\n\n    mm = MakeManager()\n    for p, pf in iter_project(proj_dir):\n        targs = [ref_fasta] + p\n        cmd1, bamfile = mem(targs, opts)\n        if cmd1:\n            cmd1 = output_bam(cmd1, bamfile)\n        nbamfile = op.join(outdir, bamfile)\n        cmd2 = \"mv {} {}\".format(bamfile, nbamfile)\n        cmds = [cmd1, cmd2]\n\n        if s3dir:\n            cmd = \"aws s3 cp {} {} --sse\".format(nbamfile,\n                                              op.join(s3dir, bamfile))\n            cmds.append(cmd)\n\n        mm.add(p, nbamfile, cmds)\n\n    mm.write()", "response": "%prog batch database. fasta project_dir output_dir\n "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef index(args):\n    p = OptionParser(index.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    dbfile, = args\n    check_index(dbfile)", "response": "%prog index database. fasta\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_align_options(p):\n    p.add_option(\"--bwa\", default=\"bwa\", help=\"Run bwa at this path\")\n    p.add_option(\"--rg\", help=\"Read group\")\n    p.add_option(\"--readtype\",\n                 choices=(\"pacbio\", \"pbread\", \"ont2d\", \"intractg\"),\n                 help=\"Read type in bwa-mem\")\n    p.set_cutoff(cutoff=800)", "response": "Used in align and batch"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef align(args):\n    valid_modes = (\"bwasw\", \"aln\", \"mem\")\n    p = OptionParser(align.__doc__)\n    p.add_option(\"--mode\", default=\"mem\", choices=valid_modes, help=\"BWA mode\")\n    set_align_options(p)\n    p.set_sam_options()\n\n    opts, args = p.parse_args(args)\n    mode = opts.mode\n    nargs = len(args)\n\n    if nargs not in (2, 3):\n        sys.exit(not p.print_help())\n\n    tag = \"bwa-{0}: \".format(mode)\n    c = mem\n    if nargs == 2:\n        tag += \"Single-end alignment\"\n        if mode == \"bwasw\":\n            c = bwasw\n        elif mode == \"aln\":\n            c = samse\n    else:\n        assert mode != \"bwasw\", \"Cannot use --bwasw with paired-end mode\"\n        tag += \"Paired-end alignment\"\n        if mode == \"aln\":\n            c = sampe\n\n    logging.debug(tag)\n    cmd, samfile = c(args, opts)\n    if cmd:\n        cmd = output_bam(cmd, samfile)\n\n    bam = opts.bam\n    unmapped = opts.unmapped\n\n    sh(cmd)\n    if unmapped:\n        dbfile, readfile = args[:2]\n        mopts = [samfile, \"--unmapped\"]\n        if not bam:\n            mopts += [\"--sam\"]\n        mapped(mopts)\n        FileShredder([samfile])\n\n    return samfile, None", "response": "%prog align database. fasta read1. fq read2. fq"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bwasw(args, opts):\n    dbfile, readfile = args\n    dbfile = check_index(dbfile)\n\n    samfile, _, unmapped = get_samfile(readfile, dbfile,\n                                       bam=opts.bam, unmapped=opts.unmapped)\n    if not need_update(dbfile, samfile):\n        logging.error(\"`{0}` exists. `bwa bwasw` already run.\".format(samfile))\n        return \"\", samfile\n\n    cmd = \"bwa bwasw \" + \" \".join(args)\n    cmd += \" -t {0}\".format(opts.cpus)\n    cmd += \" \" + opts.extra\n    return cmd, samfile", "response": "%prog bwasw database. fasta long_read. fastq"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef link(args):\n    from jcvi.apps.base import mkdir\n\n    p = OptionParser(link.__doc__)\n    p.add_option(\"--dir\",\n                 help=\"Place links in a subdirectory [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    meta, = args\n    d = opts.dir\n    if d:\n        mkdir(d)\n\n    fp = open(meta)\n    cwd = op.dirname(get_abs_path(meta))\n    for row in fp:\n        source, target = row.split()\n        source = op.join(cwd, source)\n        if d:\n            target = op.join(d, target)\n        lnsf(source, target, log=True)", "response": "%prog link metafile\n\n    Link source to target based on a tabular file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef touch(args):\n    p = OptionParser(touch.__doc__)\n    opts, args = p.parse_args(args)\n    fp = sys.stdin\n\n    for link_name in fp:\n        link_name = link_name.strip()\n        if not op.islink(link_name):\n            continue\n        if not op.exists(link_name):\n            continue\n\n        source = get_abs_path(link_name)\n        lnsf(source, link_name)", "response": "find . -type l | %prog touch\n\n    Linux commands `touch` wouldn't modify mtime for links, this script can.\n    Use find to pipe in all the symlinks."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clean(args):\n    p = OptionParser(clean.__doc__)\n    opts, args = p.parse_args(args)\n\n    for link_name in os.listdir(os.getcwd()):\n        if not op.islink(link_name):\n            continue\n        logging.debug(\"remove symlink `{0}`\".format(link_name))\n        os.unlink(link_name)", "response": "%prog clean\n\n    Removes all symlinks from current folder"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cp(args):\n    p = OptionParser(cp.__doc__)\n    fp = sys.stdin\n\n    for link_name in fp:\n        link_name = link_name.strip()\n        if not op.exists(link_name):\n            continue\n\n        source = get_abs_path(link_name)\n        link_name = op.basename(link_name)\n        if not op.exists(link_name):\n            os.symlink(source, link_name)\n        logging.debug(\" => \".join((source, link_name)))", "response": "%prog cp\n    find folder - type l"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef size(args):\n    from jcvi.utils.cbook import human_size\n\n    p = OptionParser(size.__doc__)\n    fp = sys.stdin\n\n    results = []\n    for link_name in fp:\n        link_name = link_name.strip()\n        if not op.islink(link_name):\n            continue\n\n        source = get_abs_path(link_name)\n\n        link_name = op.basename(link_name)\n        filesize = op.getsize(source)\n        results.append((filesize, link_name))\n\n    # sort by descending file size\n    for filesize, link_name in sorted(results, reverse=True):\n        filesize = human_size(filesize, a_kilobyte_is_1024_bytes=True)\n        print(\"%10s\\t%s\" % (filesize, link_name), file=sys.stderr)", "response": "%prog size nlink. txt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef nucmer(args):\n    p = OptionParser(nucmer.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 5:\n        sys.exit(not p.print_help())\n\n    mapbed, mtrfasta, asmfasta, chr, idx = args\n    idx = int(idx)\n    m1 = 1000000\n    bedfile = \"sample.bed\"\n    bed = Bed()\n    bed.add(\"\\t\".join(str(x) for x in (chr, (idx - 1) * m1, idx * m1)))\n    bed.print_to_file(bedfile)\n\n    cmd = \"intersectBed -a {0} -b {1} -nonamecheck -sorted | cut -f4\".\\\n                format(mapbed, bedfile)\n    idsfile = \"query.ids\"\n    sh(cmd, outfile=idsfile)\n\n    sfasta = fastaFromBed(bedfile, mtrfasta)\n    qfasta = \"query.fasta\"\n    cmd = \"faSomeRecords {0} {1} {2}\".format(asmfasta, idsfile, qfasta)\n    sh(cmd)\n\n    cmd = \"nucmer {0} {1}\".format(sfasta, qfasta)\n    sh(cmd)\n\n    mummerplot_main([\"out.delta\", \"--refcov=0\"])\n    sh(\"mv out.pdf {0}.{1}.pdf\".format(chr, idx))", "response": "%prog nucmer mappings. bed MTR. fasta assembly. fasta chr1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload the GODag object from the OBO file retrieved from http://obo. cvs. sourceforge. net", "response": "def load_GODag():\n    \"\"\"\n    OBO file retrieved from http://obo.cvs.sourceforge.net/viewvc/obo/obo/ontology/genomic-proteomic/so.obo\n    \"\"\"\n    from jcvi.apps.base import download\n\n    so_file_url = \"http://obo.cvs.sourceforge.net/viewvc/obo/obo/ontology/genomic-proteomic/so.obo\"\n    so_file = download(so_file_url, debug=False)\n\n    return GODag(so_file)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_term(term, so=None, method=\"verify\"):\n    if so is None:\n        so = load_GODag()\n\n    oterm = term\n    if term not in so.valid_names:\n        if \"resolve\" in method:\n            if \"_\" in term:\n                tparts = deque(term.split(\"_\"))\n                tparts.pop() if \"prefix\" in method else tparts.popleft()\n                nterm = \"_\".join(tparts).strip()\n                term = validate_term(nterm, so=so, method=method)\n            if term is None:\n                return None\n        else:\n            logging.error(\"Term `{0}` does not exist\".format(term))\n            sys.exit(1)\n\n    if oterm != term:\n        logging.debug(\"Resolved term `{0}` to `{1}`\".format(oterm, term))\n    return term", "response": "Validate an SO term against so. oo\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef agp(args):\n    p = OptionParser(agp.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    tablefile, = args\n    fp = open(tablefile)\n    for row in fp:\n        atoms = row.split()\n        hr = atoms[0]\n        scaf = atoms[1]\n        scaf_start = int(atoms[2]) + 1\n        scaf_end = int(atoms[3])\n        strand = atoms[4]\n        hr_start = int(atoms[5]) + 1\n        hr_end = int(atoms[6])\n\n        print(\"\\t\".join(str(x) for x in \\\n                (hr, hr_start, hr_end, 1, 'W',\n                 scaf, scaf_start, scaf_end, strand)))", "response": "%prog agp Siirt_Female_pistachio_23May2017_table. txt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef regression(args):\n    p = OptionParser(regression.__doc__)\n    opts, args, iopts = p.set_image_options(args, figsize=\"8x8\")\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    tsvfile, = args\n    df = pd.read_csv(tsvfile, sep=\"\\t\")\n    chrono = \"Chronological age (yr)\"\n    pred = \"Predicted age (yr)\"\n    resdf = pd.DataFrame({chrono: df[\"hli_calc_age_sample_taken\"], pred: df[\"Predicted Age\"]})\n    g = sns.jointplot(chrono, pred, resdf, joint_kws={\"s\": 6},\n                      xlim=(0, 100), ylim=(0, 80))\n    g.fig.set_figwidth(iopts.w)\n    g.fig.set_figheight(iopts.h)\n    outfile = tsvfile.rsplit(\".\", 1)[0] + \".regression.pdf\"\n    savefig(outfile)", "response": "%prog regression postgenomic - s. tsv"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef composite_correlation(df, size=(12, 8)):\n    fig = plt.figure(1, size)\n    ax1 = plt.subplot2grid((2, 2), (0, 0))\n    ax2 = plt.subplot2grid((2, 2), (0, 1))\n    ax3 = plt.subplot2grid((2, 2), (1, 0))\n    ax4 = plt.subplot2grid((2, 2), (1, 1))\n    chemistry = [\"V1\", \"V2\", \"V2.5\", float(\"nan\")]\n    colors = sns.color_palette(\"Set2\", 8)\n    color_map = dict(zip(chemistry, colors))\n\n    age_label = \"Chronological age (yr)\"\n    ax1.scatter(df[\"hli_calc_age_sample_taken\"], df[\"teloLength\"],\n                s=10, marker='.',\n                color=df[\"Chemistry\"].map(color_map))\n    ax1.set_ylim(0, 15)\n    ax1.set_ylabel(\"Telomere length (Kb)\")\n\n    ax2.scatter(df[\"hli_calc_age_sample_taken\"], df[\"ccn.chrX\"],\n                s=10, marker='.',\n                color=df[\"Chemistry\"].map(color_map))\n    ax2.set_ylim(1.8, 2.1)\n    ax2.set_ylabel(\"ChrX copy number\")\n\n    ax4.scatter(df[\"hli_calc_age_sample_taken\"], df[\"ccn.chrY\"],\n                s=10, marker='.',\n                color=df[\"Chemistry\"].map(color_map))\n    ax4.set_ylim(0.8, 1.1)\n    ax4.set_ylabel(\"ChrY copy number\")\n\n    ax3.scatter(df[\"hli_calc_age_sample_taken\"], df[\"TRA.PPM\"],\n                s=10, marker='.',\n                color=df[\"Chemistry\"].map(color_map))\n    ax3.set_ylim(0, 250)\n    ax3.set_ylabel(\"$TCR-\\\\alpha$ deletions (count per million reads)\")\n\n    from matplotlib.lines import Line2D\n    legend_elements = [Line2D([0], [0], marker='.', color='w', label=chem,\n                          markerfacecolor=color, markersize=16) \\\n                        for (chem, color) in zip(chemistry, colors)[:3]]\n    for ax in (ax1, ax2, ax3, ax4):\n        ax.set_xlabel(age_label)\n        ax.legend(handles=legend_elements, loc=\"upper right\")\n\n    plt.tight_layout()\n    root = fig.add_axes((0, 0, 1, 1))\n    labels = ((.02, .98, \"A\"),\n              (.52, .98, \"B\"),\n              (.02, .5, \"C\"),\n              (.52, .5, \"D\"))\n    panel_labels(root, labels)\n    root.set_xlim(0, 1)\n    root.set_ylim(0, 1)\n    root.set_axis_off()", "response": "Plot composite correlation figure"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplotting composite QC figures", "response": "def composite_qc(df_orig, size=(16, 12)):\n    \"\"\" Plot composite QC figures\n    \"\"\"\n    df = df_orig.rename(columns={\"hli_calc_age_sample_taken\": \"Age\",\n                       \"hli_calc_gender\": \"Gender\",\n                       \"eth7_max\": \"Ethnicity\",\n                       \"MeanCoverage\": \"Mean coverage\",\n                       \"Chemistry\": \"Sequencing chemistry\",\n                       \"Release Client\": \"Cohort\",\n\n                      })\n\n    fig = plt.figure(1, size)\n    ax1 = plt.subplot2grid((2, 7), (0, 0), rowspan=1, colspan=2)\n    ax2 = plt.subplot2grid((2, 7), (0, 2), rowspan=1, colspan=2)\n    ax3 = plt.subplot2grid((2, 7), (0, 4), rowspan=1, colspan=3)\n    ax4 = plt.subplot2grid((2, 7), (1, 0), rowspan=1, colspan=2)\n    ax5 = plt.subplot2grid((2, 7), (1, 2), rowspan=1, colspan=2)\n    ax6 = plt.subplot2grid((2, 7), (1, 4), rowspan=1, colspan=3)\n\n    sns.distplot(df[\"Age\"].dropna(), kde=False, ax=ax1)\n    sns.countplot(x=\"Gender\", data=df, ax=ax2)\n    sns.countplot(x=\"Ethnicity\", data=df, ax=ax3,\n                    order = df['Ethnicity'].value_counts().index)\n    sns.distplot(df[\"Mean coverage\"].dropna(), kde=False, ax=ax4)\n    ax4.set_xlim(0, 100)\n    sns.countplot(x=\"Sequencing chemistry\", data=df, ax=ax5)\n    sns.countplot(x=\"Cohort\", data=df, ax=ax6,\n                    order = df['Cohort'].value_counts().index)\n    # Anonymize the cohorts\n    cohorts = ax6.get_xticklabels()\n    newCohorts = []\n    for i, c in enumerate(cohorts):\n        if c.get_text() == \"Spector\":\n            c = \"TwinsUK\"\n        elif c.get_text() != \"Health Nucleus\":\n            c = \"C{}\".format(i + 1)\n        newCohorts.append(c)\n    ax6.set_xticklabels(newCohorts)\n\n    for ax in (ax6,):\n        ax.set_xticklabels(ax.get_xticklabels(), ha=\"right\", rotation=30)\n\n    for ax in (ax1, ax2, ax3, ax4, ax5, ax6):\n        ax.set_title(ax.get_xlabel())\n        ax.set_xlabel(\"\")\n\n    plt.tight_layout()\n\n    root = fig.add_axes((0, 0, 1, 1))\n    labels = ((.02, .96, \"A\"),\n              (.3, .96, \"B\"),\n              (.6, .96, \"C\"),\n              (.02, .52, \"D\"),\n              (.3, .52, \"E\"),\n              (.6, .52, \"F\"))\n    panel_labels(root, labels)\n    root.set_xlim(0, 1)\n    root.set_ylim(0, 1)\n    root.set_axis_off()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract_twin_values(triples, traits, gender=None):\n    # Construct the pairs of trait values\n    traitValuesAbsent = 0\n    nanValues = 0\n    genderSkipped = 0\n    twinValues = []\n    for a, b, t in triples:\n        if gender is not None and t != gender:\n            genderSkipped += 1\n            continue\n        if not (a in traits and b in traits):\n            traitValuesAbsent += 1\n            continue\n        if np.isnan(traits[a]) or np.isnan(traits[b]):\n            nanValues += 1\n            continue\n        twinValues.append((traits[a], traits[b]))\n\n    print(\"A total of {} pairs extracted ({} absent; {} nan; {} genderSkipped)\"\\\n        .format(len(twinValues), traitValuesAbsent, nanValues, genderSkipped))\n    return twinValues", "response": "Calculate the heritability of certain traits in a single tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a composite figure with both absdiff figures and heritability figures.", "response": "def composite(df, sameGenderMZ, sameGenderDZ, size=(16, 24)):\n    \"\"\"Embed both absdiff figures and heritability figures.\n    \"\"\"\n    fig = plt.figure(1, size)\n\n    ax1a = plt.subplot2grid((6, 4), (0, 0), rowspan=2, colspan=1)\n    ax2a = plt.subplot2grid((6, 4), (0, 1), rowspan=2, colspan=1)\n    ax3a = plt.subplot2grid((6, 4), (0, 2), rowspan=2, colspan=1)\n    ax4a = plt.subplot2grid((6, 4), (0, 3), rowspan=2, colspan=1)\n    ax1b = plt.subplot2grid((6, 4), (2, 0), rowspan=2, colspan=2)\n    ax2b = plt.subplot2grid((6, 4), (2, 2), rowspan=2, colspan=2)\n    ax3b = plt.subplot2grid((6, 4), (4, 0), rowspan=2, colspan=2)\n    ax4b = plt.subplot2grid((6, 4), (4, 2), rowspan=2, colspan=2)\n\n    # Telomeres\n    telomeres = extract_trait(df, \"Sample name\", \"telomeres.Length\")\n    mzTelomeres = extract_twin_values(sameGenderMZ, telomeres)\n    dzTelomeres = extract_twin_values(sameGenderDZ, telomeres)\n    plot_paired_values(ax1b, mzTelomeres, dzTelomeres, label=\"Telomere length\")\n    plot_abs_diff(ax1a, mzTelomeres, dzTelomeres, label=\"Telomere length\")\n\n    # CCNX\n    CCNX = extract_trait(df, \"Sample name\", \"ccn.chrX\")\n    mzCCNX = extract_twin_values(sameGenderMZ, CCNX, gender=\"Female\")\n    dzCCNX = extract_twin_values(sameGenderDZ, CCNX, gender=\"Female\")\n    dzCCNX = filter_low_values(dzCCNX, 1.75)\n    plot_paired_values(ax2b, mzCCNX, dzCCNX, gender=\"Female only\", label=\"ChrX copy number\")\n    plot_abs_diff(ax2a, mzCCNX, dzCCNX, label=\"ChrX copy number\")\n\n    # CCNY\n    CCNY = extract_trait(df, \"Sample name\", \"ccn.chrY\")\n    mzCCNY = extract_twin_values(sameGenderMZ, CCNY, gender=\"Male\")\n    dzCCNY = extract_twin_values(sameGenderDZ, CCNY, gender=\"Male\")\n    dzCCNY = filter_low_values(dzCCNY, .75)\n\n    plot_paired_values(ax3b, mzCCNY, dzCCNY, gender=\"Male only\", label=\"ChrY copy number\")\n    plot_abs_diff(ax3a, mzCCNY, dzCCNY, label=\"ChrY copy number\")\n\n    # CCNY\n    TRA = extract_trait(df, \"Sample name\", \"TRA.PPM\")\n    mzTRA = extract_twin_values(sameGenderMZ, TRA)\n    dzTRA = extract_twin_values(sameGenderDZ, TRA)\n    plot_paired_values(ax4b, mzTRA, dzTRA, label=\"TCR-$\\\\alpha$ deletions\")\n    plot_abs_diff(ax4a, mzTRA, dzTRA, label=\"TCR-$\\\\alpha$ deletions\")\n\n    plt.tight_layout()\n\n    root = fig.add_axes((0, 0, 1, 1))\n    # ABCD absdiff, EFGH heritability\n    labels = ((.03, .99, 'A'), (.27, .99, 'B'), (.53, .99, 'C'), (.77, .99, 'D'),\n              (.03, .67, 'E'), (.53, .67, 'F'),\n              (.03, .34, 'G'), (.53, .34, 'H'))\n    panel_labels(root, labels)\n    root.set_xlim(0, 1)\n    root.set_ylim(0, 1)\n    root.set_axis_off()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compile(args):\n    p = OptionParser(compile.__doc__)\n    p.set_outfile(outfile=\"age.tsv\")\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    dfs = []\n    for folder in args:\n        ofolder = os.listdir(folder)\n\n        # telomeres\n        subdir = [x for x in ofolder if x.startswith(\"telomeres\")][0]\n        subdir = op.join(folder, subdir)\n        filename = op.join(subdir, \"tel_lengths.txt\")\n        df = pd.read_csv(filename, sep=\"\\t\")\n        d1 = df.ix[0].to_dict()\n\n        # ccn\n        subdir = [x for x in ofolder if x.startswith(\"ccn\")][0]\n        subdir = op.join(folder, subdir)\n        filename = iglob(subdir, \"*.ccn.json\")[0]\n        js = json.load(open(filename))\n        d1.update(js)\n        df = pd.DataFrame(d1, index=[0])\n        dfs.append(df)\n\n    df = pd.concat(dfs, ignore_index=True)\n    df.to_csv(opts.outfile, sep=\"\\t\", index=False)", "response": "%prog compile directory telomere_lengths. txt ccn. txt"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef simulate_one(fw, name, size):\n    from random import choice\n    seq = Seq(''.join(choice('ACGT') for _ in xrange(size)))\n    s = SeqRecord(seq, id=name, description=\"Fake sequence\")\n    SeqIO.write([s], fw, \"fasta\")", "response": "Simulate a random sequence with name and size"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gc(args):\n    p = OptionParser(gc.__doc__)\n    p.add_option(\"--binsize\", default=500, type=\"int\",\n                 help=\"Bin size to use\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    fastafile, = args\n    binsize = opts.binsize\n    allbins = []\n    for name, seq in parse_fasta(fastafile):\n        for i in range(len(seq) / binsize):\n            atcnt = gccnt = 0\n            for c in seq[i * binsize: (i + 1) * binsize].upper():\n                if c in \"AT\":\n                    atcnt += 1\n                elif c in \"GC\":\n                    gccnt += 1\n            totalcnt = atcnt + gccnt\n            if totalcnt == 0:\n                continue\n            gcpct = gccnt * 100 / totalcnt\n            allbins.append(gcpct)\n\n    from jcvi.graphics.base import asciiplot\n    from collections import Counter\n\n    title = \"Total number of bins={}\".format(len(allbins))\n    c = Counter(allbins)\n    x, y = zip(*sorted(c.items()))\n    asciiplot(x, y, title=title)", "response": "%prog gc fastafile\n\n    Plot G+C content distribution."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef qual(args):\n    from jcvi.formats.sizes import Sizes\n\n    p = OptionParser(qual.__doc__)\n    p.add_option(\"--qv\", default=31, type=\"int\",\n                 help=\"Dummy qv score for extended bases\")\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    fastafile, = args\n    sizes = Sizes(fastafile)\n    qvchar = str(opts.qv)\n    fw = must_open(opts.outfile, \"w\")\n    total = 0\n    for s, slen in sizes.iter_sizes():\n        print(\">\" + s, file=fw)\n        print(\" \".join([qvchar] * slen), file=fw)\n        total += 1\n    fw.close()\n    logging.debug(\"Written {0} records in `{1}`.\".format(total, opts.outfile))", "response": "%prog qual fastafile\n\n    Generate dummy .qual file based on FASTA file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ispcr(args):\n    from jcvi.utils.iter import grouper\n\n    p = OptionParser(ispcr.__doc__)\n    p.add_option(\"-r\", dest=\"rclip\", default=1, type=\"int\",\n            help=\"pair ID is derived from rstrip N chars [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    fastafile, = args\n    ispcrfile = fastafile + \".isPcr\"\n    fw = open(ispcrfile, \"w\")\n\n    N = opts.rclip\n    strip_name = lambda x: x[:-N] if N else str\n\n    npairs = 0\n    fastaiter = SeqIO.parse(fastafile, \"fasta\")\n    for a, b in grouper(fastaiter, 2):\n\n        aid, bid = [strip_name(x) for x in (a.id, b.id)]\n        assert aid == bid, \"Name mismatch {0}\".format((aid, bid))\n\n        print(\"\\t\".join((aid, str(a.seq), str(b.seq))), file=fw)\n        npairs += 1\n\n    fw.close()\n    logging.debug(\"A total of {0} pairs written to `{1}`.\".\\\n                  format(npairs, ispcrfile))", "response": "%prog ispcr fastafile\n\n    Reformat paired primers into isPcr query format, which is three column\n    format: name, forward, reverse"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_fasta(infile, upper=False):\n    '''\n    parse a fasta-formatted file and returns header\n    can be a fasta file that contains multiple records.\n    '''\n    try:\n        fp = must_open(infile)\n    except:\n        fp = infile\n    # keep header\n    fa_iter = (x[1] for x in groupby(fp, lambda row: row[0] == '>'))\n    for header in fa_iter:\n        header = next(header)\n        if header[0] != '>':\n            continue\n        # drop '>'\n        header = header.strip()[1:]\n        # stitch the sequence lines together and make into upper case\n        seq = \"\".join(s.strip() for s in next(fa_iter))\n        if upper:\n            seq = seq.upper()\n        yield header, seq", "response": "parse a fasta - formatted file and returns a sequence header and sequence sequence."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clean(args):\n    p = OptionParser(clean.__doc__)\n    p.add_option(\"--fancy\", default=False, action=\"store_true\",\n                 help=\"Pretty print the sequence [default: %default]\")\n    p.add_option(\"--canonical\", default=False, action=\"store_true\",\n                 help=\"Use only acgtnACGTN [default: %default]\")\n    p.set_outfile()\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    fastafile, = args\n    fw = must_open(opts.outfile, \"w\")\n    if opts.fancy:\n        for header, seq in iter_clean_fasta(fastafile):\n            print(\">\" + header, file=fw)\n            fancyprint(fw, seq)\n\n        return 0\n\n    iterator = iter_canonical_fasta if opts.canonical else iter_clean_fasta\n\n    for header, seq in iterator(fastafile):\n        seq = Seq(seq)\n        s = SeqRecord(seq, id=header, description=\"\")\n        SeqIO.write([s], fw, \"fasta\")", "response": "%prog clean fastafile\n\n    Remove irregular chars in FASTA seqs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filter(args):\n    p = OptionParser(filter.__doc__)\n    p.add_option(\"--less\", default=False, action=\"store_true\",\n                 help=\"filter the sizes < certain cutoff [default: >=]\")\n    p.set_outfile()\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    fastafile, cutoff = args\n    try:\n        cutoff = int(cutoff)\n    except ValueError:\n        sys.exit(not p.print_help())\n\n    f = Fasta(fastafile, lazy=True)\n\n    fw = must_open(opts.outfile, \"w\")\n    for name, rec in f.iteritems_ordered():\n\n        if opts.less and len(rec) >= cutoff:\n            continue\n\n        if (not opts.less) and len(rec) < cutoff:\n            continue\n\n        SeqIO.write([rec], fw, \"fasta\")\n        fw.flush()\n\n    return fw.name", "response": "%prog filter fastafile 100"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pool(args):\n    from jcvi.formats.base import longest_unique_prefix\n\n    p = OptionParser(pool.__doc__)\n    p.add_option(\"--sep\", default=\".\", help=\"Separator between prefix and name\")\n    p.add_option(\"--sequential\", default=False, action=\"store_true\",\n            help=\"Add sequential IDs\")\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    for fastafile in args:\n        pf = longest_unique_prefix(fastafile, args)\n        print(fastafile, \"=>\", pf, file=sys.stderr)\n        prefixopt = \"--prefix={0}{1}\".format(pf, opts.sep)\n        format_args = [fastafile, \"stdout\", prefixopt]\n        if opts.sequential:\n            format_args += [\"--sequential=replace\"]\n        format(format_args)", "response": "%prog pool fastafiles > pool. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ids(args):\n    p = OptionParser(ids.__doc__)\n    p.add_option(\"--until\", default=None,\n             help=\"Truncate the name and description at words [default: %default]\")\n    p.add_option(\"--description\", default=False, action=\"store_true\",\n             help=\"Generate a second column with description [default: %default]\")\n    p.set_outfile()\n\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    until = opts.until\n    fw = must_open(opts.outfile, \"w\")\n    for row in must_open(args):\n        if row[0] == \">\":\n            row = row[1:].rstrip()\n            if until:\n                row = row.split(until)[0]\n\n            atoms = row.split(None, 1)\n            if opts.description:\n                outrow = \"\\t\".join(atoms)\n            else:\n                outrow = atoms[0]\n            print(outrow, file=fw)\n\n    fw.close()", "response": "%prog ids fastafiles\n\n    Generate the FASTA headers without the '>'."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sort(args):\n    p = OptionParser(sort.__doc__)\n    p.add_option(\"--sizes\", default=False, action=\"store_true\",\n                 help=\"Sort by decreasing size [default: %default]\")\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(p.print_help())\n\n    fastafile, = args\n    sortedfastafile = fastafile.rsplit(\".\", 1)[0] + \".sorted.fasta\"\n\n    f = Fasta(fastafile, index=False)\n    fw = must_open(sortedfastafile, \"w\")\n    if opts.sizes:\n        # Sort by decreasing size\n        sortlist = sorted(f.itersizes(), key=lambda x: (-x[1], x[0]))\n        logging.debug(\"Sort by size: max: {0}, min: {1}\".\\\n                        format(sortlist[0], sortlist[-1]))\n        sortlist = [x for x, s in sortlist]\n    else:\n        sortlist = sorted(f.iterkeys())\n\n    for key in sortlist:\n        rec = f[key]\n        SeqIO.write([rec], fw, \"fasta\")\n\n    logging.debug(\"Sorted file written to `{0}`.\".format(sortedfastafile))\n    fw.close()\n\n    return sortedfastafile", "response": "%prog sort fastafile\n\n    Sort a list of sequences and output with sorted IDs, etc."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef join(args):\n    from jcvi.formats.agp import OO, Phases, build\n    from jcvi.formats.sizes import Sizes\n\n    p = OptionParser(join.__doc__)\n    p.add_option(\"--newid\", default=None,\n            help=\"New sequence ID [default: `%default`]\")\n    p.add_option(\"--gapsize\", default=100, type=\"int\",\n            help=\"Number of N's in between the sequences [default: %default]\")\n    p.add_option(\"--gaptype\", default=\"contig\",\n            help=\"Gap type to use in the AGP file [default: %default]\")\n    p.add_option(\"--evidence\", default=\"\",\n            help=\"Linkage evidence to report in the AGP file [default: %default]\")\n    p.add_option(\"--oo\", help=\"Use .oo file generated by bambus [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    nargs = len(args)\n    if nargs not in (1, 2):\n        sys.exit(not p.print_help())\n\n    if nargs == 2:\n        fastafile, phasefile = args\n        phases = DictFile(phasefile)\n        phases = dict((a, Phases[int(b)]) for a, b in phases.items())\n    else:\n        fastafile, = args\n        phases = {}\n\n    sizes = Sizes(fastafile)\n    prefix = fastafile.rsplit(\".\", 1)[0]\n    agpfile = prefix + \".agp\"\n    newid = opts.newid\n    oo = opts.oo\n\n    o = OO(oo, sizes.mapping)\n\n    if oo:\n        seen = o.contigs\n        # The leftover contigs not in the oo file\n        logging.debug(\"A total of {0} contigs ({1} in `{2}`)\".\\\n                    format(len(sizes), len(seen), oo))\n\n        for ctg, size in sizes.iter_sizes():\n            if ctg in seen:\n                continue\n            o.add(ctg, ctg, size)\n\n    else:\n        if newid:\n            for ctg, size in sizes.iter_sizes():\n                o.add(newid, ctg, size)\n        else:\n            for scaffold_number, (ctg, size) in enumerate(sizes.iter_sizes()):\n                object_id = \"scaffold{0:03d}\".format(scaffold_number + 1)\n                o.add(object_id, ctg, size)\n\n    fw = open(agpfile, \"w\")\n    o.write_AGP(fw, gapsize=opts.gapsize, gaptype=opts.gaptype,\n                    evidence=opts.evidence, phases=phases)\n    fw.close()\n\n    joinedfastafile = prefix + \".joined.fasta\"\n    build([agpfile, fastafile, joinedfastafile])\n\n    return joinedfastafile", "response": "%prog join fastafile phasefile"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef format(args):\n    sequential_choices = (\"replace\", \"prefix\", \"suffix\")\n    p = OptionParser(format.__doc__)\n    p.add_option(\"--pairs\", default=False, action=\"store_true\",\n            help=\"Add trailing /1 and /2 for interleaved pairs [default: %default]\")\n    p.add_option(\"--sequential\", default=None, choices=sequential_choices,\n            help=\"Add sequential IDs [default: %default]\")\n    p.add_option(\"--sequentialoffset\", default=0, type=\"int\",\n            help=\"Sequential IDs start at [default: %default]\")\n    p.add_option(\"--pad0\", default=0, type=\"int\",\n            help=\"Pad a few zeros in front of sequential [default: %default]\")\n    p.add_option(\"--gb\", default=False, action=\"store_true\",\n            help=\"For Genbank ID, get the accession [default: %default]\")\n    p.add_option(\"--sep\", default=None,\n            help=\"Split description by certain symbol [default: %default]\")\n    p.add_option(\"--index\", default=0, type=\"int\",\n            help=\"Extract i-th field after split with --sep [default: %default]\")\n    p.add_option(\"--noversion\", default=False, action=\"store_true\",\n            help=\"Remove the gb trailing version [default: %default]\")\n    p.add_option(\"--prefix\", help=\"Prepend prefix to sequence ID\")\n    p.add_option(\"--suffix\", help=\"Append suffix to sequence ID\")\n    p.add_option(\"--template\", default=False, action=\"store_true\",\n            help=\"Extract `template=aaa dir=x library=m` to `m-aaa/x` [default: %default]\")\n    p.add_option(\"--switch\", help=\"Switch ID from two-column file [default: %default]\")\n    p.add_option(\"--annotation\", help=\"Add functional annotation from \"\n                        \"two-column file ('ID <--> Annotation') [default: %default]\")\n    p.add_option(\"--ids\", help=\"Generate ID conversion table [default: %default]\")\n    p.add_option(\"--upper\", default=False, action=\"store_true\",\n            help=\"Convert sequence to upper case [default: %default]\")\n    p.add_option(\"--nodesc\", default=False, action=\"store_true\",\n            help=\"Remove description after identifier\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    infasta, outfasta = args\n    gb = opts.gb\n    pairs = opts.pairs\n    prefix = opts.prefix\n    suffix = opts.suffix\n    noversion = opts.noversion\n    sequential = opts.sequential\n    sequentialoffset = opts.sequentialoffset\n    sep = opts.sep\n    idx = opts.index\n    mapfile = opts.switch\n    annotfile = opts.annotation\n    desc = not opts.nodesc\n    idsfile = opts.ids\n    idsfile = open(idsfile, \"w\") if idsfile else None\n    upper = opts.upper\n\n    if mapfile:\n        mapping = DictFile(mapfile, delimiter=\"\\t\")\n    if annotfile:\n        annotation = DictFile(annotfile, delimiter=\"\\t\")\n\n    fp = SeqIO.parse(must_open(infasta), \"fasta\")\n    fw = must_open(outfasta, \"w\")\n    for i, rec in enumerate(fp):\n        origid = rec.id\n        description = rec.description.replace(origid, \"\").strip()\n        if sep:\n            rec.id = rec.description.split(sep)[idx].strip()\n        if gb:\n            # gi|262233616|gb|GU123895.1| Coffea arabica clone BAC\n            atoms = rec.id.split(\"|\")\n            if len(atoms) >= 3:\n                rec.id = atoms[3]\n            elif len(atoms) == 2:\n                rec.id = atoms[1]\n        if pairs:\n            id = \"/1\" if (i % 2 == 0) else \"/2\"\n            rec.id += id\n        if noversion:\n            rec.id = rec.id.rsplit(\".\", 1)[0]\n        if sequential:\n            rec.id = \"{0:0{1}d}\".format(sequentialoffset, opts.pad0)\n            if sequential == \"prefix\":\n                rec.id = \"{0}-{1}\".format(rec.id, origid)\n            elif sequential == \"suffix\":\n                rec.id = \"{0}-{1}\".format(origid, rec.id)\n            sequentialoffset += 1\n        if opts.template:\n            template, dir, lib = [x.split(\"=\")[-1] for x in\n                    rec.description.split()[1:4]]\n            rec.id = \"{0}-{1}/{2}\".format(lib, template, dir)\n        if mapfile:\n            if origid in mapping:\n                rec.id = mapping[origid]\n            else:\n                logging.error(\"{0} not found in `{1}`. ID unchanged.\".\\\n                        format(origid, mapfile))\n        if prefix:\n            rec.id = prefix + rec.id\n        if suffix:\n            rec.id += suffix\n        if annotfile:\n            rec.description = annotation.get(origid, \"\") if not mapfile \\\n                    else annotation.get(rec.id, \"\")\n        else:\n            rec.description = description if desc else \"\"\n        if idsfile:\n            print(\"\\t\".join((origid, rec.id)), file=idsfile)\n        if upper:\n            rec.seq = rec.seq.upper()\n\n        SeqIO.write(rec, fw, \"fasta\")\n\n    if idsfile:\n        logging.debug(\"Conversion table written to `{0}`.\".\\\n                      format(idsfile.name))\n        idsfile.close()", "response": "%prog format infasta outfasta"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _print_first_difference(arec, brec, ignore_case=False, ignore_N=False,\n         report_match=True):\n    \"\"\"\n    Returns the first different nucleotide in two sequence comparisons\n    \"\"\"\n    aseq, bseq = arec.seq, brec.seq\n    asize, bsize = len(aseq), len(bseq)\n\n    matched = True\n    for i, (a, b) in enumerate(zip_longest(aseq, bseq)):\n        if ignore_case and None not in (a, b):\n            a, b = a.upper(), b.upper()\n\n        if ignore_N and ('N' in (a, b) or 'X' in (a, b)):\n            continue\n\n        if a != b:\n            matched = False\n            break\n\n    if i + 1 == asize and matched:\n        if report_match:\n            print(green(\"Two sequences match\"))\n        match = True\n    else:\n        print(red(\"Two sequences do not match\"))\n\n        snippet_size = 20  # show the context of the difference\n\n        print(red(\"Sequence start to differ at position %d:\" % (i + 1)))\n\n        begin = max(i - snippet_size, 0)\n        aend = min(i + snippet_size, asize)\n        bend = min(i + snippet_size, bsize)\n\n        print(red(aseq[begin:i] + \"|\" + aseq[i:aend]))\n        print(red(bseq[begin:i] + \"|\" + bseq[i:bend]))\n        match = False\n\n    return match", "response": "Print the first different nucleotide in two sequence comparisons"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef diff(args):\n    from jcvi.utils.table import banner\n\n    p = OptionParser(diff.__doc__)\n    p.add_option(\"--ignore_case\", default=False, action=\"store_true\",\n            help=\"ignore case when comparing sequences [default: %default]\")\n    p.add_option(\"--ignore_N\", default=False, action=\"store_true\",\n            help=\"ignore N and X's when comparing sequences [default: %default]\")\n    p.add_option(\"--ignore_stop\", default=False, action=\"store_true\",\n            help=\"ignore stop codon when comparing sequences [default: %default]\")\n    p.add_option(\"--rc\", default=False, action=\"store_true\",\n            help=\"also consider reverse complement [default: %default]\")\n    p.add_option(\"--quiet\", default=False, action=\"store_true\",\n            help=\"don't output comparison details [default: %default]\")\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    afasta, bfasta = args\n\n    afastan = len(Fasta(afasta))\n    bfastan = len(Fasta(bfasta))\n\n    if afastan == bfastan:\n        print(green(\"Two sets contain the same number of sequences ({0}, {1})\".\\\n                format(afastan, bfastan)))\n    else:\n        print(red(\"Two sets contain different number of sequences ({0}, {1})\".\\\n                format(afastan, bfastan)))\n\n    ah = SeqIO.parse(afasta, \"fasta\")\n    bh = SeqIO.parse(bfasta, \"fasta\")\n\n    problem_ids = []\n    for arec, brec in zip(ah, bh):\n\n        if opts.ignore_stop:\n            arec.seq = arec.seq.rstrip(\"*\")\n            brec.seq = brec.seq.rstrip(\"*\")\n\n        asize, bsize = len(arec), len(brec)\n\n        if not opts.quiet:\n            print(banner(str(arec), [str(brec)]))\n            if asize == bsize:\n                print(green(\"Two sequence size match (%d)\" % asize))\n            else:\n                print(red(\"Two sequence size do not match (%d, %d)\" % (asize, bsize)))\n\n        # print out the first place the two sequences diff\n        fd = print_first_difference(arec, brec, ignore_case=opts.ignore_case,\n                ignore_N=opts.ignore_N, rc=opts.rc, report_match=not opts.quiet)\n        if not fd:\n            logging.error(\"Two sets of sequences differ at `{0}`\".format(arec.id))\n            problem_ids.append(\"\\t\".join(str(x) for x in (arec.id, asize, bsize,\n                    abs(asize - bsize))))\n\n    if problem_ids:\n        print(red(\"A total of {0} records mismatch.\".format(len(problem_ids))))\n        fw = must_open(\"Problems.ids\", \"w\")\n        print(\"\\n\".join(problem_ids), file=fw)", "response": "%prog diff afasta bfasta"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates checksum of input sequence element", "response": "def hash_fasta(seq, ignore_case=False, ignore_N=False, ignore_stop=False, checksum=\"MD5\"):\n    \"\"\"\n    Generates checksum of input sequence element\n    \"\"\"\n    if ignore_stop:\n        seq = seq.rstrip(\"*\")\n    if ignore_case:\n        seq = seq.upper()\n    if ignore_N:\n        if not all(c.upper() in 'ATGCN' for c in seq):\n            seq = re.sub('X', '', seq)\n        else:\n            seq = re.sub('N', '', seq)\n\n    if checksum == \"MD5\":\n        hashed = md5(seq).hexdigest()\n    elif checksum == \"GCG\":\n        hashed = seguid(seq)\n\n    return hashed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef identical(args):\n    from jcvi.utils.cbook import AutoVivification\n\n    allowed_checksum = [\"MD5\", \"GCG\"]\n\n    p = OptionParser(identical.__doc__)\n    p.add_option(\"--ignore_case\", default=False, action=\"store_true\",\n            help=\"ignore case when comparing sequences [default: %default]\")\n    p.add_option(\"--ignore_N\", default=False, action=\"store_true\",\n            help=\"ignore N and X's when comparing sequences [default: %default]\")\n    p.add_option(\"--ignore_stop\", default=False, action=\"store_true\",\n            help=\"ignore stop codon when comparing sequences [default: %default]\")\n    p.add_option(\"--output_uniq\", default=False, action=\"store_true\",\n            help=\"output uniq sequences in FASTA format\" + \\\n                 \" [default: %default]\")\n    p.add_option(\"--checksum\", default=\"MD5\", choices=allowed_checksum,\n            help=\"specify checksum method [default: %default]\")\n    p.set_outfile()\n\n    opts, args = p.parse_args(args)\n\n    if len(args) == 0:\n        sys.exit(not p.print_help())\n\n    d = AutoVivification()\n    files = []\n    for fastafile in args:\n        f = Fasta(fastafile)\n        pf = fastafile.rsplit(\".\", 1)[0]\n        files.append(pf)\n\n        logging.debug(\"Hashing individual elements of {0}\".format(fastafile))\n        for name, rec in f.iteritems_ordered():\n            seq = re.sub(' ', '', str(rec.seq))\n            hashed = hash_fasta(seq, ignore_case=opts.ignore_case, ignore_N=opts.ignore_N, \\\n                ignore_stop=opts.ignore_stop, checksum=opts.checksum)\n            if not d[hashed]:\n                d[hashed]['seq'] = seq\n                d[hashed]['count'] = 0\n            if not d[hashed]['names'][pf]:\n                d[hashed]['names'][pf] = set()\n            d[hashed]['names'][pf].add(name)\n\n    fw = must_open(opts.outfile, \"w\")\n    if opts.output_uniq:\n        uniqfile = \"_\".join(files) + \".uniq.fasta\"\n        uniqfw = must_open(uniqfile, \"w\")\n\n    header = \"\\t\".join(str(x) for x in (args))\n    print(\"\\t\".join(str(x) for x in (\"\", header)), file=fw)\n    for idx, hashed in enumerate(d.keys()):\n        line = []\n        line.append(\"t{0}\".format(idx))\n        for fastafile in files:\n            if fastafile in d[hashed]['names'].keys():\n                line.append(\",\".join(d[hashed]['names'][fastafile]))\n                if opts.output_uniq:\n                    d[hashed]['count'] += len(d[hashed]['names'][fastafile])\n            else:\n                line.append(\"na\")\n        print(\"\\t\".join(line), file=fw)\n\n        if opts.output_uniq:\n            seqid = \"\\t\".join(str(x) for x in (\"t{0}\".format(idx), d[hashed]['count']))\n            rec = SeqRecord(Seq(d[hashed]['seq']), id=seqid, description=\"\")\n            SeqIO.write([rec], uniqfw, \"fasta\")\n\n    fw.close()\n    if opts.output_uniq:\n        logging.debug(\"Uniq sequences written to `{0}`\".format(uniqfile))\n        uniqfw.close()", "response": "%prog identical sequence. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the qual file associated with the current folder.", "response": "def get_qual(fastafile, suffix=QUALSUFFIX, check=True):\n    \"\"\"\n    Check if current folder contains a qual file associated with the fastafile\n    \"\"\"\n    qualfile1 = fastafile.rsplit(\".\", 1)[0] + suffix\n    qualfile2 = fastafile + suffix\n\n    if check:\n        if op.exists(qualfile1):\n            logging.debug(\"qual file `{0}` found\".format(qualfile1))\n            return qualfile1\n        elif op.exists(qualfile2):\n            logging.debug(\"qual file `{0}` found\".format(qualfile2))\n            return qualfile2\n        else:\n            logging.warning(\"qual file not found\")\n            return None\n\n    return qualfile1"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef some(args):\n    p = OptionParser(some.__doc__)\n    p.add_option(\"--exclude\", default=False, action=\"store_true\",\n            help=\"Output sequences not in the list file [default: %default]\")\n    p.add_option(\"--uniprot\", default=False, action=\"store_true\",\n            help=\"Header is from uniprot [default: %default]\")\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(p.print_help())\n\n    fastafile, listfile, outfastafile = args\n    outfastahandle = must_open(outfastafile, \"w\")\n    qualfile = get_qual(fastafile)\n\n    names = set(x.strip() for x in open(listfile))\n    if qualfile:\n        outqualfile = outfastafile + \".qual\"\n        outqualhandle = open(outqualfile, \"w\")\n        parser = iter_fasta_qual(fastafile, qualfile)\n    else:\n        parser = SeqIO.parse(fastafile, \"fasta\")\n\n    num_records = 0\n    for rec in parser:\n        name = rec.id\n        if opts.uniprot:\n            name = name.split(\"|\")[-1]\n\n        if opts.exclude:\n            if name in names:\n                continue\n        else:\n            if name not in names:\n                continue\n\n        SeqIO.write([rec], outfastahandle, \"fasta\")\n        if qualfile:\n            SeqIO.write([rec], outqualhandle, \"qual\")\n\n        num_records += 1\n\n    logging.debug(\"A total of %d records written to `%s`\" % \\\n            (num_records, outfastafile))", "response": "%prog some fastafile listfile outfastafile"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fastq(args):\n    from jcvi.formats.fastq import FastqLite\n\n    p = OptionParser(fastq.__doc__)\n    p.add_option(\"--qv\", type=\"int\",\n                 help=\"Use generic qv value [dafault: %default]\")\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    fastafile, = args\n    fastqfile = fastafile.rsplit(\".\", 1)[0] + \".fastq\"\n    fastqhandle = open(fastqfile, \"w\")\n    num_records = 0\n\n    if opts.qv is not None:\n        qv = chr(ord('!') + opts.qv)\n        logging.debug(\"QV char '{0}' ({1})\".format(qv, opts.qv))\n    else:\n        qv = None\n\n    if qv:\n        f = Fasta(fastafile, lazy=True)\n        for name, rec in f.iteritems_ordered():\n            r = FastqLite(\"@\" + name, str(rec.seq).upper(), qv * len(rec.seq))\n            print(r, file=fastqhandle)\n            num_records += 1\n\n    else:\n        qualfile = get_qual(fastafile)\n        for rec in iter_fasta_qual(fastafile, qualfile):\n            SeqIO.write([rec], fastqhandle, \"fastq\")\n            num_records += 1\n\n    fastqhandle.close()\n    logging.debug(\"A total of %d records written to `%s`\" % \\\n            (num_records, fastqfile))", "response": "%prog fastq fastafile\n\n    Generate fastqfile by combining fastafile and fastafile.qual.\n    Also check --qv option to use a default qv score."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pair(args):\n    p = OptionParser(pair.__doc__)\n    p.set_sep(sep=None, help=\"Separator in name to reduce to clone id\" +\\\n                 \"e.g. GFNQ33242/1 use /, BOT01-2453H.b1 use .\")\n    p.add_option(\"-m\", dest=\"matepairs\", default=False, action=\"store_true\",\n            help=\"generate .matepairs file [often used for Celera Assembler]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(p.print_help())\n\n    fastafile, = args\n    qualfile = get_qual(fastafile)\n\n    prefix = fastafile.rsplit(\".\", 1)[0]\n    pairsfile = prefix + \".pairs.fasta\"\n    fragsfile = prefix + \".frags.fasta\"\n    pairsfw = open(pairsfile, \"w\")\n    fragsfw = open(fragsfile, \"w\")\n\n    #TODO: need a class to handle coupled fasta and qual iterating and indexing\n    if opts.matepairs:\n        matepairsfile = prefix + \".matepairs\"\n        matepairsfw = open(matepairsfile, \"w\")\n\n    if qualfile:\n        pairsqualfile = pairsfile + \".qual\"\n        pairsqualhandle = open(pairsqualfile, \"w\")\n        fragsqualfile = fragsfile + \".qual\"\n        fragsqualhandle = open(fragsqualfile, \"w\")\n\n    f = Fasta(fastafile)\n    if qualfile:\n        q = SeqIO.index(qualfile, \"qual\")\n\n    all_keys = list(f.keys())\n    all_keys.sort()\n    sep = opts.sep\n\n    if sep:\n        key_fun = lambda x: x.split(sep, 1)[0]\n    else:\n        key_fun = lambda x: x[:-1]\n\n    for key, variants in groupby(all_keys, key=key_fun):\n        variants = list(variants)\n        paired = (len(variants) == 2)\n\n        if paired and opts.matepairs:\n            print(\"\\t\".join((\"%s/1\" % key, \"%s/2\" % key)), file=matepairsfw)\n\n        fw = pairsfw if paired else fragsfw\n        if qualfile:\n            qualfw = pairsqualhandle if paired else fragsqualhandle\n\n        for i, var in enumerate(variants):\n            rec = f[var]\n            if qualfile:\n                recqual = q[var]\n            newid = \"%s/%d\" % (key, i + 1)\n\n            rec.id = newid\n            rec.description = \"\"\n            SeqIO.write([rec], fw, \"fasta\")\n            if qualfile:\n                recqual.id = newid\n                recqual.description = \"\"\n                SeqIO.write([recqual], qualfw, \"qual\")\n\n    logging.debug(\"sequences written to `%s` and `%s`\" % \\\n            (pairsfile, fragsfile))\n    if opts.matepairs:\n        logging.debug(\"mates written to `%s`\" % matepairsfile)", "response": "%prog pair fastafile\n\n    Generate .pairs.fasta and .fragments.fasta by matching records\n    into the pairs and the rest go to fragments."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pairinplace(args):\n    from jcvi.utils.iter import pairwise\n\n    p = OptionParser(pairinplace.__doc__)\n    p.add_option(\"-r\", dest=\"rclip\", default=1, type=\"int\",\n            help=\"pair ID is derived from rstrip N chars [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    fastafile, = args\n    base = op.basename(fastafile).split(\".\")[0]\n\n    frags = base + \".frags.fasta\"\n    pairs = base + \".pairs.fasta\"\n    if fastafile.endswith(\".gz\"):\n        frags += \".gz\"\n        pairs += \".gz\"\n\n    fragsfw = must_open(frags, \"w\")\n    pairsfw = must_open(pairs, \"w\")\n\n    N = opts.rclip\n    strip_name = lambda x: x[:-N] if N else str\n\n    skipflag = False  # controls the iterator skip\n    fastaiter = SeqIO.parse(fastafile, \"fasta\")\n    for a, b in pairwise(fastaiter):\n\n        aid, bid = [strip_name(x) for x in (a.id, b.id)]\n\n        if skipflag:\n            skipflag = False\n            continue\n\n        if aid == bid:\n            SeqIO.write([a, b], pairsfw, \"fasta\")\n            skipflag = True\n        else:\n            SeqIO.write([a], fragsfw, \"fasta\")\n\n    # don't forget the last one, when b is None\n    if not skipflag:\n        SeqIO.write([a], fragsfw, \"fasta\")\n\n    logging.debug(\"Reads paired into `%s` and `%s`\" % (pairs, frags))", "response": "%prog pairinplace bulk.fasta\n\n    Pair up the records in bulk.fasta by comparing the names for adjacent\n    records. If they match, print to bulk.pairs.fasta, else print to\n    bulk.frags.fasta."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract(args):\n    p = OptionParser(extract.__doc__)\n    p.add_option('--newname', help=\"Use this new name instead\")\n    p.add_option('--include', default=False, action=\"store_true\",\n            help=\"search description line for match [default: %default]\")\n    p.add_option('--exclude', default=False, action=\"store_true\",\n            help=\"exclude description that matches [default: %default]\")\n    p.add_option('--idonly', default=False, action=\"store_true\",\n            help=\"Only search identifier\")\n    p.add_option('--bed', default=None,\n            help=\"path to bed file to guide extraction by matching seqname\")\n    p.set_outfile()\n\n    opts, args = p.parse_args(args)\n\n    if len(args) == 2:\n        fastafile, query = args\n    elif len(args) == 1 and opts.bed:\n        fastafile, = args\n        bedaccns = Bed(opts.bed).accns\n    else:\n        sys.exit(p.print_help())\n\n    if opts.bed:\n        fw = must_open(opts.outfile, \"w\")\n        f = Fasta(fastafile)\n        for accn in bedaccns:\n            try:\n                rec = f[accn]\n            except:\n                logging.error(\"{0} not found in {1}\".format(accn, fastafile))\n                continue\n            SeqIO.write([rec], fw, \"fasta\")\n        return fw.name\n\n    atoms = query.split(\":\")\n    key = atoms[0]\n\n    assert len(atoms) <= 3, \"cannot have more than two ':' in your query\"\n\n    pos = \"\"\n    if len(atoms) in (2, 3):\n        pos = atoms[1]\n\n    strand = \"+\"\n    if len(atoms) == 3:\n        strand = atoms[2]\n\n    assert strand in ('+', '-'), \"strand must be either '+' or '-'\"\n\n    feature = dict(chr=key)\n\n    if \"-\" in pos:\n        start, stop = pos.split(\"-\")\n        try:\n            start, stop = int(start), int(stop)\n        except ValueError as e:\n            logging.error(e)\n            sys.exit(p.print_help())\n\n        feature[\"start\"] = start\n        feature[\"stop\"] = stop\n    else:\n        start, stop = None, None\n\n    assert start < stop or None in (start, stop), \\\n            \"start must be < stop, you have ({0}, {1})\".format(start, stop)\n    feature[\"strand\"] = strand\n\n    include, exclude = opts.include, opts.exclude\n    # conflicting options, cannot be true at the same time\n    assert not (include and exclude), \"--include and --exclude cannot be \"\\\n            \"on at the same time\"\n    fw = must_open(opts.outfile, \"w\")\n\n    if include or exclude:\n        f = Fasta(fastafile, lazy=True)\n        fi = f.iteritems_ordered if opts.idonly else f.iterdescriptions_ordered\n        for k, rec in fi():\n            if include and key not in k:\n                continue\n            if exclude and key in k:\n                continue\n\n            seq = Fasta.subseq(rec, start, stop, strand)\n            newid = rec.id\n            if start is not None:\n                newid += \":{0}-{1}:{2}\".format(start, stop, strand)\n\n            rec = SeqRecord(seq, id=newid, description=k)\n            SeqIO.write([rec], fw, \"fasta\")\n    else:\n        f = Fasta(fastafile)\n        try:\n            seq = f.sequence(feature, asstring=False)\n        except AssertionError as e:\n            logging.error(e)\n            return\n\n        newid = opts.newname or query\n        rec = SeqRecord(seq, id=newid, description=\"\")\n        SeqIO.write([rec], fw, \"fasta\")\n\n    return fw.name", "response": "%prog extract fasta query\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn unique records in a sequence.", "response": "def _uniq_rec(fastafile, seq=False):\n    \"\"\"\n    Returns unique records\n    \"\"\"\n    seen = set()\n    for rec in SeqIO.parse(fastafile, \"fasta\"):\n        name = str(rec.seq) if seq else rec.id\n        if name in seen:\n            logging.debug(\"ignore {0}\".format(rec.id))\n            continue\n        seen.add(name)\n        yield rec"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef random(args):\n    from random import sample\n\n    p = OptionParser(random.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    fastafile, N = args\n    N = int(N)\n    assert N > 0\n\n    f = Fasta(fastafile)\n    fw = must_open(\"stdout\", \"w\")\n\n    for key in sample(f.keys(), N):\n        rec = f[key]\n        SeqIO.write([rec], fw, \"fasta\")\n\n    fw.close()", "response": "%prog random random. fasta N > random100. fasta"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iter_fasta_qual(fastafile, qualfile, defaultqual=OKQUAL, modify=False):\n    from Bio.SeqIO.QualityIO import PairedFastaQualIterator\n    if not qualfile:\n        qualfile = make_qual(fastafile, score=defaultqual)\n\n    rec_iter = PairedFastaQualIterator(open(fastafile), open(qualfile))\n    for rec in rec_iter:\n        yield rec if not modify else modify_qual(rec)", "response": "Iterate over SeqRecords in a FASTA file with quality values equal to the given qualfile."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef trim(args):\n\n    from jcvi.algorithms.maxsum import max_sum\n\n    p = OptionParser(trim.__doc__)\n    p.add_option(\"-c\", dest=\"min_length\", type=\"int\", default=64,\n            help=\"minimum sequence length after trimming\")\n    p.add_option(\"-s\", dest=\"score\", default=QUAL,\n            help=\"quality trimming cutoff [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(p.print_help())\n\n    fastafile, newfastafile = args\n    qualfile = get_qual(fastafile)\n    newqualfile = get_qual(newfastafile, check=False)\n\n    logging.debug(\"Trim bad sequence from fasta file `%s` to `%s`\" % \\\n            (fastafile, newfastafile))\n\n    fw = must_open(newfastafile, \"w\")\n    fw_qual = open(newqualfile, \"w\")\n\n    dropped = trimmed = 0\n\n    for rec in iter_fasta_qual(fastafile, qualfile, modify=True):\n        qv = [x - opts.score for x in \\\n                rec.letter_annotations[\"phred_quality\"]]\n        msum, trim_start, trim_end = max_sum(qv)\n        score = trim_end - trim_start + 1\n\n        if score < opts.min_length:\n            dropped += 1\n            continue\n\n        if score < len(rec):\n            trimmed += 1\n            rec = rec[trim_start:trim_end + 1]\n\n        write_fasta_qual(rec, fw, fw_qual)\n\n    print(\"A total of %d sequences modified.\" % trimmed, file=sys.stderr)\n    print(\"A total of %d sequences dropped (length < %d).\" % \\\n        (dropped, opts.min_length), file=sys.stderr)\n\n    fw.close()\n    fw_qual.close()", "response": "%prog trim fasta. screen newfasta"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sequin(args):\n    p = OptionParser(sequin.__doc__)\n    p.add_option(\"--unk\", default=100, type=\"int\",\n            help=\"The size for unknown gaps [default: %default]\")\n    p.add_option(\"--newid\", default=None,\n            help=\"Use this identifier instead [default: %default]\")\n    p.add_option(\"--chromosome\", default=None,\n            help=\"Add [chromosome= ] to FASTA header [default: %default]\")\n    p.add_option(\"--clone\", default=None,\n            help=\"Add [clone= ] to FASTA header [default: %default]\")\n    p.set_mingap(default=100)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    inputfasta, = args\n    unk = opts.unk\n\n    outputfasta = inputfasta.rsplit(\".\", 1)[0] + \".split\"\n    rec = next(SeqIO.parse(must_open(inputfasta), \"fasta\"))\n    seq = \"\"\n    unknowns, knowns = 0, 0\n    for gap, gap_group in groupby(rec.seq, lambda x: x.upper() == 'N'):\n        subseq = \"\".join(gap_group)\n        if gap:\n            gap_length = len(subseq)\n            if gap_length == unk:\n                subseq = \"\\n>?unk{0}\\n\".format(unk)\n                unknowns += 1\n            elif gap_length >= opts.mingap:\n                subseq = \"\\n>?{0}\\n\".format(gap_length)\n                knowns += 1\n        seq += subseq\n\n    fw = must_open(outputfasta, \"w\")\n    id = opts.newid or rec.id\n    fastaheader = \">{0}\".format(id)\n    if opts.chromosome:\n        fastaheader += \" [chromosome={0}]\".format(opts.chromosome)\n    if opts.clone:\n        fastaheader += \" [clone={0}]\".format(opts.clone)\n\n    print(fastaheader, file=fw)\n    print(seq, file=fw)\n    fw.close()\n    logging.debug(\"Sequin FASTA written to `{0}` (gaps: {1} unknowns, {2} knowns).\".\\\n            format(outputfasta, unknowns, knowns))\n\n    return outputfasta, unknowns + knowns", "response": "%prog sequin inputfasta\n\n    Generate a gapped fasta format with known gap sizes embedded. suitable for\n    Sequin submission.\n\n    A gapped sequence represents a newer method for describing non-contiguous\n    sequences, but only requires a single sequence identifier. A gap is\n    represented by a line that starts with >? and is immediately followed by\n    either a length (for gaps of known length) or \"unk100\" for gaps of unknown\n    length. For example, \">?200\". The next sequence segment continues on the\n    next line, with no separate definition line or identifier. The difference\n    between a gapped sequence and a segmented sequence is that the gapped\n    sequence uses a single identifier and can specify known length gaps.\n    Gapped sequences are preferred over segmented sequences. A sample gapped\n    sequence file is shown here:\n\n    >m_gagei [organism=Mansonia gagei] Mansonia gagei NADH dehydrogenase ...\n    ATGGAGCATACATATCAATATTCATGGATCATACCGTTTGTGCCACTTCCAATTCCTATTTTAATAGGAA\n    TTGGACTCCTACTTTTTCCGACGGCAACAAAAAATCTTCGTCGTATGTGGGCTCTTCCCAATATTTTATT\n    >?200\n    GGTATAATAACAGTATTATTAGGGGCTACTTTAGCTCTTGC\n    TCAAAAAGATATTAAGAGGGGTTTAGCCTATTCTACAATGTCCCAACTGGGTTATATGATGTTAGCTCTA\n    >?unk100\n    TCAATAAAACTATGGGGTAAAGAAGAACAAAAAATAATTAACAGAAATTTTCGTTTATCTCCTTTATTAA\n    TATTAACGATGAATAATAATGAGAAGCCATATAGAATTGGTGATAATGTAAAAAAAGGGGCTCTTATTAC"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef gaps(args):\n    from jcvi.formats.sizes import agp\n    from jcvi.formats.agp import mask, build\n\n    p = OptionParser(gaps.__doc__)\n    p.add_option(\"--split\", default=False, action=\"store_true\",\n            help=\"Generate .split.fasta [default: %default]\")\n    p.set_mingap(default=100)\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    inputfasta, = args\n    mingap = opts.mingap\n    split = opts.split\n    prefix = inputfasta.rsplit(\".\", 1)[0]\n    bedfile = prefix + \".gaps.bed\"\n\n    if need_update(inputfasta, bedfile):\n        write_gaps_bed(inputfasta, prefix, mingap, opts.cpus)\n\n    if split:\n        splitfile = prefix + \".split.fasta\"\n        oagpfile = prefix + \".splitobject.agp\"\n        cagpfile = prefix + \".splitcomponent.agp\"\n\n        if need_update((inputfasta, bedfile), splitfile):\n\n            sizesagpfile = agp([inputfasta])\n\n            maskedagpfile = mask([sizesagpfile, bedfile, \"--splitobject\"])\n            shutil.move(maskedagpfile, oagpfile)\n            logging.debug(\"AGP file written to `{0}`.\".format(oagpfile))\n\n            maskedagpfile = mask([sizesagpfile, bedfile, \"--splitcomponent\"])\n            shutil.move(maskedagpfile, cagpfile)\n            logging.debug(\"AGP file written to `{0}`.\".format(cagpfile))\n\n            build([oagpfile, inputfasta, splitfile])\n            os.remove(sizesagpfile)\n\n        return splitfile, oagpfile, cagpfile", "response": "%prog gaps fastafile\n\n    Print out a list of gaps in BED format (.gaps.bed)."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntake Bio. SeqRecord and slice start stop from it does proper index and error handling", "response": "def subseq(cls, fasta, start=None, stop=None, strand=None):\n        \"\"\"\n        Take Bio.SeqRecord and slice \"start:stop\" from it, does proper\n        index and error handling\n        \"\"\"\n        start = start - 1 if start is not None else 0\n        stop = stop if stop is not None else len(fasta)\n\n        if start < 0:\n            msg = \"start ({0}) must > 0 of `{1}`. Reset to 1\".\\\n                        format(start + 1, fasta.id)\n            logging.error(msg)\n            start = 0\n\n        if stop > len(fasta):\n            msg = \"stop ({0}) must be <= length of `{1}` ({2}). Reset to {2}.\".\\\n                        format(stop, fasta.id, len(fasta))\n            logging.error(msg)\n            stop = len(fasta)\n\n        seq = fasta.seq[start:stop]\n\n        if strand in (-1, '-1', '-'):\n            seq = seq.reverse_complement()\n\n        return seq"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sequence(self, f, asstring=True):\n\n        assert 'chr' in f, \"`chr` field required\"\n        name = f['chr']\n\n        assert name in self, \"feature: %s not in `%s`\" % \\\n                (f, self.filename)\n\n        fasta = self[f['chr']]\n\n        seq = Fasta.subseq(fasta,\n                f.get('start'), f.get('stop'), f.get('strand'))\n\n        if asstring:\n            return str(seq)\n\n        return seq", "response": "Emulate brentp s pyfasta. py sequence method"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef scan_sequence(self, frame, direction):\n        orf_start = None\n        for c, index in self.codons(frame):\n            if (c not in self.stop and (c in self.start or not self.start)\n                and orf_start is None):\n                orf_start = index\n            elif c in self.stop and orf_start is not None:\n                self._update_longest(orf_start, index + 3, direction, frame)\n                orf_start = None\n\n        if orf_start is not None:\n            self._update_longest(orf_start, index + 3, direction, frame)", "response": "Scan the sequence of codepoints in a single reading frame."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clone_name(s, ca=False):\n    if not ca:\n        return s[:-1]\n\n    if s[0] == '1':\n        return s[2:]\n    return s.rstrip('ab')", "response": ">>> clone_name(\"120038881639\")\n    \"0038881639\"\n    >>> clone_name(\"GW11W6RK01DAJDWa\")\n    \"GW11W6RK01DAJDW\""}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self, pbar):\n        'Updates the widget to show the ETA or total time when finished.'\n\n        if pbar.currval == 0:\n            return 'ETA:  --:--:--'\n        elif pbar.finished:\n            return 'Time: %s' % self.format_time(pbar.seconds_elapsed)\n        else:\n            elapsed = pbar.seconds_elapsed\n            eta = elapsed * pbar.maxval / pbar.currval - elapsed\n            return 'ETA:  %s' % self.format_time(eta)", "response": "Updates the widget to show the ETA or total time when finished."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the widget with the current SI prefixed speed.", "response": "def update(self, pbar):\n        'Updates the widget with the current SI prefixed speed.'\n\n        if pbar.seconds_elapsed < 2e-6 or pbar.currval < 2e-6: # =~ 0\n            scaled = power = 0\n        else:\n            speed = pbar.currval / pbar.seconds_elapsed\n            power = int(math.log(speed, 1000))\n            scaled = speed / 1000.**power\n\n        return self._format % (scaled, self.prefixes[power], self.unit)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the widget to show the next marker or the first marker when finished", "response": "def update(self, pbar):\n        '''Updates the widget to show the next marker or the first marker when\n        finished'''\n\n        if pbar.finished: return self.markers[0]\n\n        self.curmark = (self.curmark + 1) % len(self.markers)\n        return self.markers[self.curmark]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the progress bar and its subcomponents", "response": "def update(self, pbar, width):\n        'Updates the progress bar and its subcomponents'\n\n        left, marker, right = (format_updatable(i, pbar) for i in\n                               (self.left, self.marker, self.right))\n\n        width -= len(left) + len(right)\n        # Marker must *always* have length of 1\n        marker *= int(pbar.currval / pbar.maxval * width)\n\n        if self.fill_left:\n            return '%s%s%s' % (left, marker.ljust(width, self.fill), right)\n        else:\n            return '%s%s%s' % (left, marker.rjust(width, self.fill), right)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the progress bar and its subcomponents", "response": "def update(self, pbar, width):\n        'Updates the progress bar and its subcomponents'\n\n        left, marker, right = (format_updatable(i, pbar) for i in\n                               (self.left, self.marker, self.right))\n\n        width -= len(left) + len(right)\n\n        if pbar.finished: return '%s%s%s' % (left, width * marker, right)\n\n        position = int(pbar.currval % (width * 2 - 1))\n        if position > width: position = width * 2 - position\n        lpad = self.fill * (position - 1)\n        rpad = self.fill * (width - len(marker) - len(lpad))\n\n        # Swap if we want to bounce the other way\n        if not self.fill_left: rpad, lpad = lpad, rpad\n\n        return '%s%s%s%s%s' % (left, lpad, marker, rpad, right)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _handle_resize(self, signum=None, frame=None):\n        'Tries to catch resize signals sent from the terminal.'\n\n        h, w = array('h', ioctl(self.fd, termios.TIOCGWINSZ, '\\0' * 8))[:2]\n        self.term_width = w", "response": "Tries to catch resize signals sent from the terminal."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\njoining the widgets and justifies the line", "response": "def _format_line(self):\n        'Joins the widgets and justifies the line'\n\n        widgets = ''.join(self._format_widgets())\n\n        if self.left_justify: return widgets.ljust(self.term_width)\n        else: return widgets.rjust(self.term_width)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _need_update(self):\n        'Returns whether the ProgressBar should redraw the line.'\n        if self.currval >= self.next_update or self.finished: return True\n\n        delta = time.time() - self.last_update_time\n        return self._time_sensitive and delta > self.poll", "response": "Returns whether the ProgressBar should redraw the line."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking all widgets for the time sensitive bit", "response": "def _update_widgets(self):\n        'Checks all widgets for the time sensitive bit'\n\n        self._time_sensitive = any(getattr(w, 'TIME_SENSITIVE', False)\n                                    for w in self.widgets)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the ProgressBar to a new value.", "response": "def update(self, value=None):\n        'Updates the ProgressBar to a new value.'\n\n        if value is not None and value is not UnknownLength:\n            if (self.maxval is not UnknownLength\n                and not 0 <= value <= self.maxval):\n\n                raise ValueError('Value out of range')\n\n            self.currval = value\n\n\n        if not self._need_update(): return\n        if self.start_time is None:\n            raise RuntimeError('You must call \"start\" before calling \"update\"')\n\n        now = time.time()\n        self.seconds_elapsed = now - self.start_time\n        self.next_update = self.currval + self.update_interval\n        self.fd.write(self._format_line() + '\\r')\n        self.last_update_time = now"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start(self):\n        '''Starts measuring time, and prints the bar at 0%.\n\n        It returns self so you can use it like this:\n        >>> pbar = ProgressBar().start()\n        >>> for i in range(100):\n        ...    # do something\n        ...    pbar.update(i+1)\n        ...\n        >>> pbar.finish()\n        '''\n\n        if self.maxval is None:\n            self.maxval = self._DEFAULT_MAXVAL\n\n        self.num_intervals = max(100, self.term_width)\n        self.next_update = 0\n\n        if self.maxval is not UnknownLength:\n            if self.maxval < 0: raise ValueError('Value out of range')\n            self.update_interval = self.maxval / self.num_intervals\n\n\n        self.start_time = self.last_update_time = time.time()\n        self.update(0)\n\n        return self", "response": "Starts measuring time and prints the bar at 0%. This method returns self so you can use it like this."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nputs the ProgressBar bar in the finished state.", "response": "def finish(self):\n        'Puts the ProgressBar bar in the finished state.'\n\n        self.finished = True\n        self.update(self.maxval)\n        self.fd.write('\\n')\n        if self.signal_set:\n            signal.signal(signal.SIGWINCH, signal.SIG_DFL)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot(args):\n    from jcvi.graphics.base import plt, savefig\n    from jcvi.graphics.chromosome import ChromosomeMap\n\n    p = OptionParser(plot.__doc__)\n    p.add_option(\"--firstn\", type=\"int\", help=\"Only plot the first N genes\")\n    p.add_option(\"--ymax\", type=\"int\", help=\"Y-axis max value\")\n    p.add_option(\"--log\", action=\"store_true\",\n                help=\"Write plotting data [default: %default]\")\n    opts, args, iopts = p.set_image_options(args, figsize=\"6x4\")\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    taggedbed, chr = args\n    bed = Bed(taggedbed)\n    beds = list(bed.sub_bed(chr))\n    old, new = [], []\n    i = 0\n    for b in beds:\n        accn = b.extra[0]\n        if \"te\" in accn:\n            continue\n\n        accn, tag = accn.split(\"|\")\n        if tag == \"OVERLAP\":\n            continue\n\n        c, r = atg_name(accn)\n        if tag == \"NEW\":\n            new.append((i, r))\n        else:\n            old.append((i, r))\n        i += 1\n\n    ngenes = i\n    assert ngenes == len(new) + len(old)\n\n    logging.debug(\"Imported {0} ranks on {1}.\".format(ngenes, chr))\n    fig = plt.figure(1, (iopts.w, iopts.h))\n    root = fig.add_axes([0, 0, 1, 1])\n\n    xstart, xend = .2, .8\n    ystart, yend = .2, .8\n    pad = .02\n\n    ngenes = opts.firstn or ngenes\n    ymax = opts.ymax or 500000\n\n    title = \"Assignment of Medtr identifiers\"\n    if opts.ymax:\n        subtitle = \"{0}, first {1} genes\".format(chr, ngenes)\n    else:\n        subtitle = \"{0}, {1} genes ({2} new)\".format(chr, ngenes, len(new))\n\n    chr_map = ChromosomeMap(fig, root, xstart, xend, ystart, yend, pad, 0,\n                        ymax, 5, title, subtitle)\n\n    ax = chr_map.axes\n\n    if opts.log:\n        from jcvi.utils.table import write_csv\n        header = [\"x\", \"y\"]\n        write_csv(header, new, filename=chr + \".new\")\n        write_csv(header, old, filename=chr + \".old\")\n\n    x, y = zip(*new)\n    ax.plot(x, y, \"b,\")\n    x, y = zip(*old)\n    ax.plot(x, y, \"r,\")\n\n    # Legends\n    ymid = (ystart + yend) / 2\n    y = ymid + pad\n    root.plot([.2], [y], \"r.\", lw=2)\n    root.text(.2 + pad, y, \"Existing Medtr ids\", va=\"center\", size=10)\n    y = ymid - pad\n    root.plot([.2], [y], \"b.\", lw=2)\n    root.text(.2 + pad, y, \"Newly instantiated ids\", va=\"center\", size=10)\n\n    ax.set_xlim(0, ngenes)\n    ax.set_ylim(0, ymax)\n    ax.set_axis_off()\n\n    root.set_xlim(0, 1)\n    root.set_ylim(0, 1)\n    root.set_axis_off()\n\n    image_name = chr + \".identifiers.\" + iopts.format\n    savefig(image_name, dpi=iopts.dpi, iopts=iopts)", "response": "%prog plot tagged. new. bed chr1"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef instantiate(args):\n    p = OptionParser(instantiate.__doc__)\n    p.set_annot_reformat_opts()\n    p.add_option(\"--extended_stride\", default=False, action=\"store_true\",\n                 help=\"Toggle extended strides for gene numbering\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    taggedbed, blacklist, gapsbed = args\n    r = NameRegister(prefix=opts.prefix, pad0=opts.pad0, uc=opts.uc)\n    r.get_blacklist(blacklist)\n    r.get_gaps(gapsbed)\n\n    # Run through the bed, identify stretch of NEW ids to instantiate,\n    # identify the flanking FRAMEs, interpolate!\n    bed = Bed(taggedbed)\n    outputbed = taggedbed.rsplit(\".\", 1)[0] + \".new.bed\"\n    fw = open(outputbed, \"w\")\n\n    tagkey = lambda x: x.rsplit(\"|\", 1)[-1]\n    for chr, sbed in bed.sub_beds():\n        current_chr = chr_number(chr)\n        if not current_chr:\n            continue\n\n        sbed = list(sbed)\n\n        ranks = []\n        for i, s in enumerate(sbed):\n            nametag = s.extra[0]\n            tag = tagkey(nametag)\n\n            if tag in (NEW, FRAME):\n                ranks.append((i, nametag))\n\n        blocks = []\n        for tag, names in groupby(ranks, key=lambda x: tagkey(x[-1])):\n            names = list(names)\n            if tag == NEW:\n                blocks.append((tag, [sbed[x[0]] for x in names]))\n            else:\n                start, end = names[0][-1], names[-1][-1]\n                start, end = atg_name(start, retval=\"rank\"), atg_name(end, retval=\"rank\")\n                blocks.append((tag, [start, end]))\n\n        id_table = {}  # old to new name conversion\n        for i, (tag, info) in enumerate(blocks):\n            if tag != NEW:\n                continue\n\n            start_id = 0 if i == 0 else blocks[i - 1][1][-1]\n            end_id = start_id + 10000 if i == len(blocks) -1 \\\n                        else blocks[i + 1][1][0]\n\n            r.allocate(info, chr, start_id, end_id, id_table, extended_stride=opts.extended_stride)\n\n        # Output new names\n        for i, s in enumerate(sbed):\n            nametag = s.extra[0]\n            name, tag = nametag.split(\"|\")\n\n            if tag == NEW:\n                assert name == '.'\n                name = id_table[s.accn]\n            elif tag == OVERLAP:\n                if name in id_table:\n                    name = id_table[name]\n\n            s.extra[0] = \"|\".join((name, tag))\n            print(s, file=fw)\n\n    fw.close()", "response": "%prog instantiate tagged. bed blacklist. ids big_gaps. bed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prepare(bedfile):\n    pf = bedfile.rsplit(\".\", 1)[0]\n    abedfile = pf + \".a.bed\"\n    bbedfile = pf + \".b.bed\"\n    fwa = open(abedfile, \"w\")\n    fwb = open(bbedfile, \"w\")\n\n    bed = Bed(bedfile)\n    seen = set()\n    for b in bed:\n        accns = b.accn.split(\";\")\n        new_accns = []\n        for accn in accns:\n            if \":\" in accn:\n                method, a = accn.split(\":\", 1)\n                if method in (\"liftOver\", \"GMAP\", \"\"):\n                    accn = a\n            if accn in seen:\n                logging.error(\"Duplicate id {0} found. Ignored.\".format(accn))\n                continue\n\n            new_accns.append(accn)\n            b.accn = accn\n            print(b, file=fwa)\n            seen.add(accn)\n\n        b.accn = \";\".join(new_accns)\n        print(b, file=fwb)\n    fwa.close()\n    fwb.close()", "response": "Prepare the structure of the sequence of untagged entries in a BED file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_prefix(identifier):\n    pf, id = (), identifier\n    if \"|\" in identifier:\n        pf, id = tuple(identifier.split('|')[:-1]), identifier.split('|')[-1]\n\n    return pf, id", "response": "Parse identifier such as a |c|le|d|li|re|AT4G00480. 1 and return\n    tuple of prefix and suffix."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef summary(args):\n    p = OptionParser(summary.__doc__)\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    fastafile, = args\n    f = Fasta(fastafile, index=False)\n\n    halfmaskedseqs = set()\n    allmasked = 0\n    allbases = 0\n    cutoff = 50\n    for key, seq in f.iteritems():\n        masked = 0\n        for base in seq:\n            if base not in \"AGCT\":\n                masked += 1\n        seqlen = len(seq)\n        if masked * 100. / seqlen > cutoff:\n            halfmaskedseqs.add(key)\n        allmasked += masked\n        allbases += seqlen\n\n    seqnum = len(f)\n    maskedseqnum = len(halfmaskedseqs)\n\n    print(\"Total masked bases: {0}\".\\\n            format(percentage(allmasked, allbases)), file=sys.stderr)\n    print(\"Total masked sequences (contain > {0}% masked): {1}\".\\\n            format(cutoff, percentage(maskedseqnum, seqnum)), file=sys.stderr)", "response": "%prog summary fastafile\n\n    Report the number of bases and sequences masked."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mask(args):\n    p = OptionParser(mask.__doc__)\n    p.add_option(\"--hard\", dest=\"hard\", default=False, action=\"store_true\",\n            help=\"Hard mask the low-complexity bases [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    genomefile, = args\n\n    # entire pipeline\n    countsfile = genomefile + \".counts\"\n    wm_mk_counts(infile=genomefile, outfile=countsfile)\n\n    maskedfastafile = \"%s.masked%s\" % op.splitext(genomefile)\n    wm_mk_masks(infile=countsfile, outfile=maskedfastafile, genomefile=genomefile)\n\n    if opts.hard:\n        hardmask(maskedfastafile)", "response": "%prog mask fastafile\n\n    This script pipelines the windowmasker in NCBI BLAST+.  Masked fasta file\n    will have an appended suffix of .mask with all the low-complexity bases masked\n    (default to lower case, set --hard for hardmasking)."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_flanker(group, query):\n    group.sort()\n    pos = bisect_left(group, (query, 0))\n    left_flanker = group[0] if pos == 0 else group[pos-1]\n    right_flanker = group[-1] if pos == len(group) else group[pos]\n    # pick the closest flanker\n    if abs(query - left_flanker[0]) < abs(query - right_flanker[0]):\n        flanker, other = left_flanker, right_flanker\n    else:\n        flanker, other = right_flanker, left_flanker\n\n    flanked = not (pos == 0 or pos == len(group) or flanker == query)\n\n    return flanker, other, flanked", "response": "Get the flanker and other from a list of flanker entries."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding all syntenic blocks for a query.", "response": "def find_synteny_region(query, sbed, data, window, cutoff, colinear=False):\n    \"\"\"\n    Get all synteny blocks for a query, algorithm is single linkage\n    anchors are a window centered on query\n\n    Two categories of syntenic regions depending on what query is:\n    (Syntelog): syntenic region is denoted by the syntelog\n    (Gray gene): syntenic region is marked by the closest flanker\n    \"\"\"\n    regions = []\n    ysorted = sorted(data, key=lambda x: x[1])\n    g = Grouper()\n\n    a, b = tee(ysorted)\n    next(b, None)\n    for ia, ib in izip(a, b):\n        pos1, pos2 = ia[1], ib[1]\n        if pos2 - pos1 < window and sbed[pos1].seqid == sbed[pos2].seqid:\n            g.join(ia, ib)\n\n    for group in sorted(g):\n        (qflanker, syntelog), (far_flanker, far_syntelog), flanked = \\\n            get_flanker(group, query)\n\n        # run a mini-dagchainer here, take the direction that gives us most anchors\n        if colinear:\n            y_indexed_group = [(y, i) for i, (x, y) in enumerate(group)]\n            lis = longest_increasing_subsequence(y_indexed_group)\n            lds = longest_decreasing_subsequence(y_indexed_group)\n\n            if len(lis) >= len(lds):\n                track = lis\n                orientation = \"+\"\n            else:\n                track = lds\n                orientation = \"-\"\n\n            group = [group[i] for (y, i) in track]\n\n        xpos, ypos = zip(*group)\n        score = min(len(set(xpos)), len(set(ypos)))\n\n        if qflanker == query:\n            gray = \"S\"\n        else:\n            gray = \"G\" if not flanked else \"F\"\n            score -= 1  # slight penalty for not finding syntelog\n\n        if score < cutoff:\n            continue\n\n        # y-boundary of the block\n        left, right = group[0][1], group[-1][1]\n        # this characterizes a syntenic region (left, right).\n        # syntelog is -1 if it's a gray gene\n        syn_region = (syntelog, far_syntelog, left,\n                      right, gray, orientation, score)\n        regions.append(syn_region)\n\n    return sorted(regions, key=lambda x: -x[-1])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_arrays(blastfile, qpadbed, spadbed, qpadnames, spadnames):\n    m, n = len(qpadnames), len(spadnames)\n    qpadorder, spadorder = qpadbed.order, spadbed.order\n    qpadid = dict((a, i) for i, a in enumerate(qpadnames))\n    spadid = dict((a, i) for i, a in enumerate(spadnames))\n    qpadlen = dict((a, len(b)) for a, b in qpadbed.sub_beds())\n    spadlen = dict((a, len(b)) for a, b in spadbed.sub_beds())\n\n    qsize, ssize = len(qpadbed), len(spadbed)\n\n    assert sum(qpadlen.values()) == qsize\n    assert sum(spadlen.values()) == ssize\n\n    # Populate arrays of observed counts and expected counts\n    logging.debug(\"Initialize array of size ({0} x {1})\".format(m, n))\n    observed = np.zeros((m, n))\n    fp = open(blastfile)\n    all_dots = 0\n    for row in fp:\n        b = BlastLine(row)\n        qi, q = qpadorder[b.query]\n        si, s = spadorder[b.subject]\n        qseqid, sseqid = q.seqid, s.seqid\n        qsi, ssi = qpadid[qseqid], spadid[sseqid]\n        observed[qsi, ssi] += 1\n        all_dots += 1\n\n    assert int(round(observed.sum())) == all_dots\n\n    logging.debug(\"Total area: {0} x {1}\".format(qsize, ssize))\n    S = qsize * ssize\n    expected = np.zeros((m, n))\n    qsum = 0\n    for i, a in enumerate(qpadnames):\n        alen = qpadlen[a]\n        qsum += alen\n        for j, b in enumerate(spadnames):\n            blen = spadlen[b]\n            expected[i, j] = all_dots * alen * blen * 1. / S\n\n    assert int(round(expected.sum())) == all_dots\n\n    # Calculate the statistical significance for each cell\n    from scipy.stats.distributions import poisson\n    M = m * n  # multiple testing\n    logmp = np.zeros((m, n))\n    for i in xrange(m):\n        for j in xrange(n):\n            obs, exp = observed[i, j], expected[i, j]\n            pois = max(poisson.pmf(obs, exp), 1e-250)  # Underflow\n            logmp[i, j] = max(- log(pois), 0)\n\n    return logmp", "response": "This function creates three arrays of observed and expected counts for each comparison. The observed counts are calculated from the sum of observed and expected counts. The logmp counts are calculated from the sum of observed and expected counts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a list of Range objects return a list of segments that can be cut based on their boundaries.", "response": "def get_segments(ranges, extra, minsegment=40):\n    \"\"\"\n    Given a list of Range, perform chaining on the ranges and select a highest\n    scoring subset and cut based on their boundaries. Let's say the projection\n    of the synteny blocks onto one axis look like the following.\n\n    1=====10......20====30....35====~~\n\n    Then the segmentation will yield a block [1, 20), [20, 35), using an\n    arbitrary right extension rule. Extra are additional end breaks for\n    chromosomes.\n    \"\"\"\n    from jcvi.utils.range import range_chain, LEFT, RIGHT\n\n    NUL = 2\n    selected, score = range_chain(ranges)\n\n    endpoints = [(x.start, NUL) for x in selected]\n    endpoints += [(x[0], LEFT) for x in extra]\n    endpoints += [(x[1], RIGHT) for x in extra]\n    endpoints.sort()\n\n    current_left = 0\n    for a, ai in endpoints:\n\n        if ai == LEFT:\n            current_left = a\n        if ai == RIGHT:\n            yield current_left, a\n        elif ai == NUL:\n            if a - current_left < minsegment:\n                continue\n            yield current_left, a - 1\n            current_left = a"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cluster(args):\n    from jcvi.utils.range import Range\n\n    p = OptionParser(cluster.__doc__)\n    p.set_beds()\n    p.add_option(\"--minsize\", default=10, type=\"int\",\n                 help=\"Only segment using blocks >= size [default: %default]\")\n    p.add_option(\"--path\", default=\"~/scratch/bin\",\n                 help=\"Path to the CLUSTER 3.0 binary [default: %default]\")\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    blastfile, anchorfile = args\n    qbed, sbed, qorder, sorder, is_self = check_beds(blastfile, p, opts)\n\n    minsize = opts.minsize\n    ac = AnchorFile(anchorfile)\n    qranges, sranges = [], []\n    qextra = [x[1:] for x in qbed.get_breaks()]\n    sextra = [x[1:] for x in sbed.get_breaks()]\n\n    id = 0\n    for block in ac.iter_blocks(minsize=minsize):\n        q, s = zip(*block)[:2]\n        q = [qorder[x][0] for x in q]\n        s = [sorder[x][0] for x in s]\n        minq, maxq = min(q), max(q)\n        mins, maxs = min(s), max(s)\n        id += 1\n\n        qr = Range(\"0\", minq, maxq, maxq - minq, id)\n        sr = Range(\"0\", mins, maxs, maxs - mins, id)\n        qranges.append(qr)\n        sranges.append(sr)\n\n    qpads = list(get_segments(qranges, qextra))\n    spads = list(get_segments(sranges, sextra))\n\n    suffix = \".pad.bed\"\n    qpf = opts.qbed.split(\".\")[0]\n    spf = opts.sbed.split(\".\")[0]\n    qpadfile = qpf + suffix\n    spadfile = spf + suffix\n    qnpads, qpadnames = write_PAD_bed(qpadfile, qpf, qpads, qbed)\n    snpads, spadnames = write_PAD_bed(spadfile, spf, spads, sbed)\n\n    qpadbed, spadbed = Bed(qpadfile), Bed(spadfile)\n\n    logmp = make_arrays(blastfile, qpadbed, spadbed, qpadnames, spadnames)\n    m, n = logmp.shape\n\n    matrixfile = \".\".join((qpf, spf, \"logmp.txt\"))\n    fw = open(matrixfile, \"w\")\n    header = [\"o\"] + spadnames\n    print(\"\\t\".join(header), file=fw)\n    for i in xrange(m):\n        row = [qpadnames[i]] + [\"{0:.1f}\".format(x) for x in logmp[i, :]]\n        print(\"\\t\".join(row), file=fw)\n\n    fw.close()\n\n    # Run CLUSTER 3.0 (Pearson correlation, average linkage)\n    cmd = op.join(opts.path, \"cluster\")\n    cmd += \" -g 2 -e 2 -m a -f {0}\".format(matrixfile)\n    pf = matrixfile.rsplit(\".\", 1)[0]\n    cdtfile = pf + \".cdt\"\n    if need_update(matrixfile, cdtfile):\n        sh(cmd)", "response": "%prog cluster blastfile anchorfile"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the entropy score of a given set of metagenomic datasets.", "response": "def entropy_score(kmer):\n    \"\"\"\n    Schmieder and Edwards. Quality control and preprocessing of metagenomic datasets. (2011) Bioinformatics\n    https://academic.oup.com/bioinformatics/article/27/6/863/236283/Quality-control-and-preprocessing-of-metagenomic\n    \"\"\"\n    l = len(kmer) - 2\n    k = l if l < 64 else 64\n    counts = defaultdict(int)\n    for i in range(l):\n        trinuc = kmer[i: i + 3]\n        counts[trinuc] += 1\n\n    logk = math.log(k)\n    res = 0\n    for k, v in counts.items():\n        f = v * 1. / l\n        res += f * math.log(f) / logk\n    return res * -100"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bed(args):\n    from jcvi.formats.fasta import rc, parse_fasta\n\n    p = OptionParser(bed.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    fastafile, dumpfile = args\n    fp = open(dumpfile)\n    KMERS = set()\n    for row in fp:\n        kmer = row.split()[0]\n        kmer_rc = rc(kmer)\n        KMERS.add(kmer)\n        KMERS.add(kmer_rc)\n\n    K = len(kmer)\n    logging.debug(\"Imported {} {}-mers\".format(len(KMERS), K))\n\n    for name, seq in parse_fasta(fastafile):\n        name = name.split()[0]\n        for i in range(len(seq) - K):\n            if i % 5000000 == 0:\n                print(\"{}:{}\".format(name, i), file=sys.stderr)\n            kmer = seq[i: i + K]\n            if kmer in KMERS:\n                print(\"\\t\".join(str(x) for x in (name, i, i + K, kmer)))", "response": "%prog bed fastafile kmer. dump. txt\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef kmcop(args):\n    p = OptionParser(kmcop.__doc__)\n    p.add_option(\"--action\", choices=(\"union\", \"intersect\"),\n                default=\"union\", help=\"Action\")\n    p.add_option(\"-o\", default=\"results\", help=\"Output name\")\n    opts, args = p.parse_args(args)\n\n    if len(args) < 2:\n        sys.exit(not p.print_help())\n\n    indices = args\n    ku = KMCComplex(indices)\n    ku.write(opts.o, action=opts.action)", "response": "%prog kmcop suf\n    union kmc indices."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef kmc(args):\n    p = OptionParser(kmc.__doc__)\n    p.add_option(\"-k\", default=21, type=\"int\", help=\"Kmer size\")\n    p.add_option(\"--ci\", default=2, type=\"int\",\n                 help=\"Exclude kmers with less than ci counts\")\n    p.add_option(\"--cs\", default=2, type=\"int\",\n                 help=\"Maximal value of a counter\")\n    p.add_option(\"--cx\", default=None, type=\"int\",\n                 help=\"Exclude kmers with more than cx counts\")\n    p.add_option(\"--single\", default=False, action=\"store_true\",\n                 help=\"Input is single-end data, only one FASTQ/FASTA\")\n    p.add_option(\"--fasta\", default=False, action=\"store_true\",\n                 help=\"Input is FASTA instead of FASTQ\")\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    folder, = args\n    K = opts.k\n    n = 1 if opts.single else 2\n    pattern = \"*.fa,*.fa.gz,*.fasta,*.fasta.gz\" if opts.fasta else \\\n              \"*.fq,*.fq.gz,*.fastq,*.fastq.gz\"\n\n    mm = MakeManager()\n    for p, pf in iter_project(folder, pattern=pattern,\n                              n=n, commonprefix=False):\n        pf = pf.split(\"_\")[0] + \".ms{}\".format(K)\n        infiles = pf + \".infiles\"\n        fw = open(infiles, \"w\")\n        print(\"\\n\".join(p), file=fw)\n        fw.close()\n\n        cmd = \"kmc -k{} -m64 -t{}\".format(K, opts.cpus)\n        cmd += \" -ci{} -cs{}\".format(opts.ci, opts.cs)\n        if opts.cx:\n            cmd += \" -cx{}\".format(opts.cx)\n        if opts.fasta:\n            cmd += \" -fm\"\n        cmd += \" @{} {} .\".format(infiles, pf)\n        outfile = pf + \".kmc_suf\"\n        mm.add(p, outfile, cmd)\n\n    mm.write()", "response": "%prog kmc folder\n\n    Run kmc3 on Illumina reads."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef meryl(args):\n    p = OptionParser(meryl.__doc__)\n    p.add_option(\"-k\", default=19, type=\"int\", help=\"Kmer size\")\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    folder, = args\n    K = opts.k\n    cpus = opts.cpus\n    mm = MakeManager()\n    for p, pf in iter_project(folder):\n        cmds = []\n        mss = []\n        for i, ip in enumerate(p):\n            ms = \"{}{}.ms{}\".format(pf, i + 1, K)\n            mss.append(ms)\n            cmd = \"meryl -B -C -m {} -threads {}\".format(K, cpus)\n            cmd += \" -s {} -o {}\".format(ip, ms)\n            cmds.append(cmd)\n        ams, bms = mss\n        pms = \"{}.ms{}\".format(pf, K)\n        cmd = \"meryl -M add -s {} -s {} -o {}\".format(ams, bms, pms)\n        cmds.append(cmd)\n        cmd = \"rm -f {}.mcdat {}.mcidx {}.mcdat {}.mcidx\".\\\n                    format(ams, ams, bms, bms)\n        cmds.append(cmd)\n        mm.add(p, pms + \".mcdat\", cmds)\n\n    mm.write()", "response": "%prog meryl folder\n\n    Run meryl on Illumina reads."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef model(args):\n    from scipy.stats import binom, poisson\n\n    p = OptionParser(model.__doc__)\n    p.add_option(\"-k\", default=23, type=\"int\", help=\"Kmer size\")\n    p.add_option(\"--cov\", default=50, type=\"int\", help=\"Expected coverage\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    erate, = args\n    erate = float(erate)\n    cov = opts.cov\n    k = opts.k\n\n    xy = []\n    # Range include c although it is unclear what it means to have c=0\n    for c in xrange(0,  cov * 2 + 1):\n        Prob_Yk = 0\n        for i in xrange(k + 1):\n            # Probability of having exactly i errors\n            pi_i = binom.pmf(i, k, erate)\n            # Expected coverage of kmer with exactly i errors\n            mu_i = cov * (erate / 3) ** i * (1 - erate) ** (k - i)\n            # Probability of seeing coverage of c\n            Prob_Yk_i = poisson.pmf(c, mu_i)\n            # Sum i over 0, 1, ... up to k errors\n            Prob_Yk += pi_i * Prob_Yk_i\n        xy.append((c, Prob_Yk))\n\n    x, y = zip(*xy)\n    asciiplot(x, y, title=\"Model\")", "response": "%prog model erate\n\n    Model kmer distribution given error rate. See derivation in FIONA paper:\n    <http://bioinformatics.oxfordjournals.org/content/30/17/i356.full>"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef count(args):\n    from bitarray import bitarray\n\n    p = OptionParser(count.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    fastafile, jfdb = args\n    K = get_K(jfdb)\n    cmd = \"jellyfish query {0} -C | cut -d' ' -f 2\".format(jfdb)\n    t = must_open(\"tmp\", \"w\")\n    proc = Popen(cmd, stdin=PIPE, stdout=t)\n    t.flush()\n\n    f = Fasta(fastafile, lazy=True)\n    for name, rec in f.iteritems_ordered():\n        kmers = list(make_kmers(rec.seq, K))\n        print(\"\\n\".join(kmers), file=proc.stdin)\n    proc.stdin.close()\n    logging.debug(cmd)\n    proc.wait()\n\n    a = bitarray()\n    binfile = \".\".join((fastafile, jfdb, \"bin\"))\n    fw = open(binfile, \"w\")\n    t.seek(0)\n    for row in t:\n        c = row.strip()\n        a.append(int(c))\n    a.tofile(fw)\n    logging.debug(\"Serialize {0} bits to `{1}`.\".format(len(a), binfile))\n    fw.close()\n    sh(\"rm {0}\".format(t.name))\n\n    logging.debug(\"Shared K-mers (K={0}) between `{1}` and `{2}` written to `{3}`.\".\\\n                    format(K, fastafile, jfdb, binfile))\n    cntfile = \".\".join((fastafile, jfdb, \"cnt\"))\n    bincount([fastafile, binfile, \"-o\", cntfile, \"-K {0}\".format(K)])\n    logging.debug(\"Shared K-mer counts written to `{0}`.\".format(cntfile))", "response": "%prog count fastafile jf. db"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bincount(args):\n    from bitarray import bitarray\n    from jcvi.formats.sizes import Sizes\n\n    p = OptionParser(bincount.__doc__)\n    p.add_option(\"-K\", default=23, type=\"int\",\n                 help=\"K-mer size [default: %default]\")\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    fastafile, binfile = args\n    K = opts.K\n\n    fp = open(binfile)\n    a = bitarray()\n    a.fromfile(fp)\n    f = Sizes(fastafile)\n    tsize = 0\n    fw = must_open(opts.outfile, \"w\")\n    for name, seqlen in f.iter_sizes():\n        ksize = seqlen - K + 1\n        b = a[tsize: tsize + ksize]\n        bcount = b.count()\n        print(\"\\t\".join(str(x) for x in (name, bcount)), file=fw)\n        tsize += ksize", "response": "%prog bincount fastafile binfile"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bin(args):\n    from bitarray import bitarray\n    p = OptionParser(bin.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    inp, outp = args\n    fp = must_open(inp)\n    fw = must_open(outp, \"w\")\n    a = bitarray()\n    for row in fp:\n        c = row.split()[-1]\n        a.append(int(c))\n    a.tofile(fw)\n    fw.close()", "response": "%prog bin filename filename. bin\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef merylhistogram(merylfile):\n    pf, sf = op.splitext(merylfile)\n    outfile = pf + \".histogram\"\n    if need_update(merylfile, outfile):\n        cmd = \"meryl -Dh -s {0}\".format(pf)\n        sh(cmd, outfile=outfile)\n\n    return outfile", "response": "Run meryl to dump histogram of the current n - grams."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef multihistogram(args):\n    p = OptionParser(multihistogram.__doc__)\n    p.add_option(\"--kmin\", default=15, type=\"int\",\n            help=\"Minimum K-mer size, inclusive\")\n    p.add_option(\"--kmax\", default=30, type=\"int\",\n            help=\"Maximum K-mer size, inclusive\")\n    p.add_option(\"--vmin\", default=2, type=\"int\",\n            help=\"Minimum value, inclusive\")\n    p.add_option(\"--vmax\", default=100, type=\"int\",\n            help=\"Maximum value, inclusive\")\n    opts, args, iopts = p.set_image_options(args, figsize=\"10x5\", dpi=300)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    histfiles = args[:-1]\n    species = args[-1]\n    fig = plt.figure(1, (iopts.w, iopts.h))\n    root = fig.add_axes([0, 0, 1, 1])\n    A = fig.add_axes([.08, .12, .38, .76])\n    B = fig.add_axes([.58, .12, .38, .76])\n\n    lines = []\n    legends = []\n    genomesizes = []\n    for histfile in histfiles:\n        ks = KmerSpectrum(histfile)\n        x, y = ks.get_xy(opts.vmin, opts.vmax)\n        K = get_number(op.basename(histfile).split(\".\")[0].split(\"-\")[-1])\n        if not opts.kmin <= K <= opts.kmax:\n            continue\n\n        line, = A.plot(x, y, '-', lw=1)\n        lines.append(line)\n        legends.append(\"K = {0}\".format(K))\n        ks.analyze(K=K)\n        genomesizes.append((K, ks.genomesize / 1e6))\n\n    leg = A.legend(lines, legends, shadow=True, fancybox=True)\n    leg.get_frame().set_alpha(.5)\n\n    title = \"{0} genome K-mer histogram\".format(species)\n    A.set_title(markup(title))\n    xlabel, ylabel = \"Coverage (X)\", \"Counts\"\n    A.set_xlabel(xlabel)\n    A.set_ylabel(ylabel)\n    set_human_axis(A)\n\n    title = \"{0} genome size estimate\".format(species)\n    B.set_title(markup(title))\n    x, y = zip(*genomesizes)\n    B.plot(x, y, \"ko\", mfc='w')\n    t = np.linspace(opts.kmin - .5, opts.kmax + .5, 100)\n    p = np.poly1d(np.polyfit(x, y, 2))\n    B.plot(t, p(t), \"r:\")\n\n    xlabel, ylabel = \"K-mer size\", \"Estimated genome size (Mb)\"\n    B.set_xlabel(xlabel)\n    B.set_ylabel(ylabel)\n    set_ticklabels_helvetica(B)\n\n    labels = ((.04, .96, 'A'), (.54, .96, 'B'))\n    panel_labels(root, labels)\n\n    normalize_axes(root)\n    imagename = species + \".multiK.pdf\"\n    savefig(imagename, dpi=iopts.dpi, iopts=iopts)", "response": "%prog multihistogram species histogramfiles"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nanalyze the kmer spectrum and return a list of all possible k - mers and k - mers.", "response": "def analyze(self, ploidy=2, K=23, covmax=1000000):\n        \"\"\"\n        Analyze Kmer spectrum, calculations derived from\n        allpathslg/src/kmers/KmerSpectra.cc\n        \"\"\"\n        from math import sqrt\n\n        data = self.data\n        kf_ceil = max(K for (K, c) in data)\n        if kf_ceil > covmax:\n            exceeds = sum(1 for (K, c) in data if K > covmax)\n            logging.debug(\"A total of {0} distinct K-mers appear > \"\n                          \"{1} times. Ignored ...\".format(exceeds, covmax))\n            kf_ceil = covmax\n\n        nkf = kf_ceil + 1\n        a = [0] * nkf\n        for kf, c in data:\n            if kf > kf_ceil:\n                continue\n            a[kf] = c\n\n        ndk = a  # number of distinct kmers\n        nk = [k * c for k, c in enumerate(a)]  # number of kmers\n        cndk = [0] * nkf  # cumulative number of distinct kmers\n        cnk = [0] * nkf  # cumulative number of kmers\n        for kf in xrange(1, nkf):\n            cndk[kf] = cndk[kf - 1] + .5 * (ndk[kf - 1] + ndk[kf])\n            cnk [kf] = cnk [kf - 1] + .5 * (nk [kf - 1] + nk [kf])\n\n        # Separate kmer spectrum in 5 regions based on the kf\n        # 1        ... kf_min1    : bad kmers with low frequency\n        # kf_min1  ... kf_min2    : good kmers CN = 1/2 (SNPs)\n        # kf_min2  ... kf_min3    : good kmers CN = 1\n        # kf_min3  ... kf_hi      : good kmers CN > 1 (repetitive)\n        # kf_hi    ... inf        : bad kmers with high frequency\n\n        # min1: find first minimum\n        _kf_min1 = 10\n        while (_kf_min1 - 1 >= 2 and nk[_kf_min1 - 1] < nk[_kf_min1]):\n            _kf_min1 -= 1\n        while (_kf_min1 <= kf_ceil and nk [_kf_min1 + 1] < nk[_kf_min1]):\n            _kf_min1 += 1\n\n        # max2: find absolute maximum mx2 above first minimum min1\n        _kf_max2 = _kf_min1\n        for kf in xrange(_kf_min1 + 1, int(0.8 * kf_ceil)):\n            if nk[kf] > nk[_kf_max2]:\n                _kf_max2 = kf\n\n        # max2: resetting max2 for cases of very high polymorphism\n        if ploidy == 2:\n            ndk_half = ndk[_kf_max2 / 2]\n            ndk_double = ndk[_kf_max2 * 2]\n            if ndk_double > ndk_half:\n                _kf_max2 *= 2\n\n        # max1: SNPs local maximum max1 as half global maximum max2\n        _kf_max1 = _kf_max2 / 2\n\n        # min2: SNPs local minimum min2 between max1 and max2\n        _kf_min2 = _kf_max1 * (2 * ndk[_kf_max1] + ndk[_kf_max2]) / \\\n                    (ndk[_kf_max1] + ndk[_kf_max2])\n\n        # min1: refine between min1 and max2/2\n        for kf in xrange(_kf_min1 + 1, _kf_max1):\n            if (nk[kf] < nk[_kf_min1]):\n                _kf_min1 = kf\n\n        # min3: not a minimum, really. upper edge of main peak\n        _kf_min3 = _kf_max2 * 3 / 2\n\n        print(\"kfs:\", _kf_min1, _kf_max1, \\\n                        _kf_min2, _kf_max2, _kf_min3, file=sys.stderr)\n        self.min1 = _kf_min1\n        self.max1 = _kf_max1\n        self.min2 = _kf_min2\n        self.max2 = _kf_max2\n        self.min3 = _kf_min3\n\n        # Define maximum kf above which we neglect data\n        _kf_hi = _kf_max2 * sqrt(4 * ndk[2 * _kf_max2] * _kf_max2) \\\n                    if 2 * _kf_max2 < len(ndk) else \\\n                 _kf_max2 * sqrt(4 * ndk[len(ndk) - 1] * _kf_max2)\n        _kf_hi = int(_kf_hi)\n\n        if _kf_hi > kf_ceil:\n            _kf_hi = kf_ceil\n\n        _nk_total = cnk[len(cnk) - 1]\n        _nk_bad_low_kf = cnk[_kf_min1]\n        _nk_good_uniq = cnk[_kf_min3] - cnk[_kf_min2]\n        _nk_bad_high_kf = _nk_total - cnk[_kf_hi]\n        _ndk_good_snp = cndk[_kf_min2] - cndk[_kf_min1]\n        _ndk_good_uniq = cndk[_kf_min3] - cndk[_kf_min2]\n\n        # kmer coverage C_k\n        _kf_ave_uniq = _nk_good_uniq * 1. / _ndk_good_uniq\n        _genome_size = (_nk_total - _nk_bad_low_kf - _nk_bad_high_kf) / \\\n                        _kf_ave_uniq\n        _genome_size_unique = _ndk_good_uniq + _ndk_good_snp / 2\n        _genome_size_repetitive = _genome_size - _genome_size_unique\n        _coverage = _nk_total / _genome_size if _genome_size else 0\n\n        # SNP rate estimation, assumes uniform distribution of SNPs over the\n        # genome and accounts for the reduction in SNP kmer counts when\n        # polymorphism is very high\n        if ploidy == 2:\n            _d_SNP = 1. / (1. - (1. - .5 * _ndk_good_snp / _genome_size) ** (1. / K)) \\\n                     if _ndk_good_snp > 0 else 1000000\n\n        G = int(_genome_size)\n        G1 = int(_genome_size_unique)\n        GR = int(_genome_size_repetitive)\n        coverage = int(_coverage)\n\n        m = \"Kmer (K={0}) Spectrum Analysis\\n\".format(K)\n        m += \"Genome size estimate = {0}\\n\".format(thousands(G))\n        m += \"Genome size estimate CN = 1 = {0} ({1})\\n\".format(thousands(G1),\n                percentage(G1, G))\n        m += \"Genome size estimate CN > 1 = {0} ({1})\\n\".format(thousands(GR),\n                percentage(GR, G))\n        m += \"Coverage estimate: {0} x\\n\".format(coverage)\n        self.repetitive = \"Repeats: {0} percent\".format(GR * 100 / G)\n\n        if ploidy == 2:\n            d_SNP = int(_d_SNP)\n            self.snprate = \"SNP rate ~= 1/{0}\".format(d_SNP)\n        else:\n            self.snprate = \"SNP rate not computed (Ploidy = {0})\".format(ploidy)\n        m += self.snprate + '\\n'\n\n        self.genomesize = int(round(self.totalKmers * 1. / self.max2))\n\n        print(m, file=sys.stderr)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns VecScreen on a single file.", "response": "def run_vecscreen(infile=None, outfile=None, db=\"UniVec_Core\",\n        pctid=None, hitlen=None):\n    \"\"\"\n    BLASTN parameters reference:\n    http://www.ncbi.nlm.nih.gov/VecScreen/VecScreen_docs.html\n    \"\"\"\n    db = get_abs_path(db)\n    nin = db + \".nin\"\n    run_formatdb(infile=db, outfile=nin)\n\n    cmd = \"blastn\"\n    cmd += \" -task blastn\"\n    cmd += \" -query {0} -db {1} -out {2}\".format(infile, db, outfile)\n    cmd += \" -penalty -5 -gapopen 4 -gapextend 4 -dust yes -soft_masking true\"\n    cmd += \" -searchsp 1750000000000 -evalue 0.01 -outfmt 6 -num_threads 8\"\n    sh(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef blat(args):\n    p = OptionParser(blat.__doc__)\n    p.set_align(pctid=95, hitlen=30)\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    reffasta, queryfasta = args\n    blastfile = get_outfile(reffasta, queryfasta, suffix=\"blat\")\n\n    run_blat(infile=queryfasta, outfile=blastfile, db=reffasta,\n             pctid=opts.pctid, hitlen=opts.hitlen, cpus=opts.cpus,\n             overwrite=False)\n\n    return blastfile", "response": "%prog blat ref. fasta query. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef last(args, dbtype=None):\n    p = OptionParser(last.__doc__)\n    p.add_option(\"--dbtype\", default=\"nucl\",\n                 choices=(\"nucl\", \"prot\"),\n                 help=\"Molecule type of subject database\")\n    p.add_option(\"--path\", help=\"Specify LAST path\")\n    p.add_option(\"--mask\", default=False, action=\"store_true\", help=\"Invoke -c in lastdb\")\n    p.add_option(\"--format\", default=\"BlastTab\",\n                 choices=(\"TAB\", \"MAF\", \"BlastTab\", \"BlastTab+\"),\n                 help=\"Output format\")\n    p.add_option(\"--minlen\", default=0, type=\"int\",\n                 help=\"Filter alignments by how many bases match\")\n    p.add_option(\"--minid\", default=0, type=\"int\", help=\"Minimum sequence identity\")\n    p.set_cpus()\n    p.set_params()\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    subject, query = args\n    path = opts.path\n    cpus = opts.cpus\n    if not dbtype:\n        dbtype = opts.dbtype\n    getpath = lambda x: op.join(path, x) if path else x\n    lastdb_bin = getpath(\"lastdb\")\n    lastal_bin = getpath(\"lastal\")\n\n    subjectdb = subject.rsplit(\".\", 1)[0]\n    run_lastdb(infile=subject, outfile=subjectdb + \".prj\", mask=opts.mask, \\\n              lastdb_bin=lastdb_bin, dbtype=dbtype)\n\n    u = 2 if opts.mask else 0\n    cmd = \"{0} -u {1}\".format(lastal_bin, u)\n    cmd += \" -P {0} -i3G\".format(cpus)\n    cmd += \" -f {0}\".format(opts.format)\n    cmd += \" {0} {1}\".format(subjectdb, query)\n\n    minlen = opts.minlen\n    minid = opts.minid\n    extra = opts.extra\n    assert minid != 100, \"Perfect match not yet supported\"\n    mm = minid / (100 - minid)\n\n    if minlen:\n        extra += \" -e{0}\".format(minlen)\n    if minid:\n        extra += \" -r1 -q{0} -a{0} -b{0}\".format(mm)\n    if extra:\n        cmd += \" \" + extra.strip()\n\n    lastfile = get_outfile(subject, query, suffix=\"last\")\n    sh(cmd, outfile=lastfile)", "response": "%prog last database. fasta query. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef correct(args):\n    p = OptionParser(correct.__doc__)\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    lstfile = \"reads2cor.lst\"\n    fw = open(lstfile, \"w\")\n    print(\"\\n\".join(x for x in args if x[:2] == \"PE\"), file=fw)\n    fw.close()\n\n    p1 = args[0]\n    offset = guessoffset([p1])\n    cpus = opts.cpus\n\n    freq = \"output.freq.cz\"\n    freqlen = freq + \".len\"\n    if need_update(args, (freq, freqlen)):\n        cmd = \"KmerFreq_AR_v2.0 -k 17 -c -1 -q {0}\".format(offset)\n        cmd += \" -m 1 -t {0}\".format(cpus)\n        cmd += \" -p output {0}\".format(lstfile)\n        sh(cmd)\n\n    fw = open(lstfile, \"w\")\n    print(\"\\n\".join(args), file=fw)\n    fw.close()\n\n    cmd = \"Corrector_AR_v2.0 -k 17 -l 3 -m 5 -c 5 -a 0 -e 1 -w 0 -r 45\"\n    cmd += \" -Q {0} -q 30 -x 8 -t {1} -o 1 \".format(offset, cpus)\n    cmd += \" {0} {1} {2}\".format(freq, freqlen, lstfile)\n    sh(cmd)", "response": "%prog correct seq. fasta seq. fasta"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clean(args):\n    p = OptionParser(clean.__doc__)\n    p.add_option(\"-a\", default=0, type=\"int\",\n                 help=\"Trim length at 5' end [default: %default]\")\n    p.add_option(\"-b\", default=50, type=\"int\",\n                 help=\"Trim length at 3' end [default: %default]\")\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) == 2:\n        p1, p2 = args\n        size = get_size(p1)\n    elif len(args) == 3:\n        p1, p2, size = args\n        size = int(size)\n    else:\n        sys.exit(not p.print_help())\n\n    pf = p1.split(\".\")[0]\n    cpus = opts.cpus\n\n    offset = guessoffset([p1])\n    a, b = opts.a, opts.b\n\n    p1_clean = p1 + \".clean\"\n    p1_cleangz = p1_clean + \".gz\"\n    p2_clean = p2 + \".clean\"\n    p2_cleangz = p2_clean + \".gz\"\n    if need_update([p1, p2], [p1_cleangz, p2_cleangz]):\n        cmd = \"SOAPfilter_v2.0 -t {0} -m 2000000 -p -y -z -g\".format(cpus)\n        cmd += \" -q {0} -w 10 -B 50 -f 0\".format(offset)\n        cmd += \" -l {0} -a {1} -b {2} -c {1} -d {2}\".format(size, a, b, a, b)\n        cmd += \" {0} {1} {2}.clean.stat {3} {4}\".\\\n                    format(p1, p2, pf, p1_clean, p2_clean)\n        sh(cmd)", "response": "%prog clean 1. fastq 2. fastq..."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bed(args):\n    p = OptionParser(bed.__doc__)\n    p.set_outfile()\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    pslfile, = args\n    fw = must_open(opts.outfile, \"w\")\n\n    psl = Psl(pslfile)\n    for p in psl:\n        print(p.bed12line, file=fw)", "response": "%prog bed pslfile\n\n    Convert to bed format."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _isProtein(self):\n        last = self.blockCount - 1\n        return ((self.tEnd == self.tStarts[last] + 3 * self.blockSizes[last]) \\\n                and self.strand == \"+\") or \\\n                ((self.tStart == self.tSize - (self.tStarts[last] + 3 * self.blockSizes[last])\\\n                and self.strand == \"-\"))", "response": "check if the sequence is in the protein space"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _milliBad(self, ismRNA=False):\n        sizeMult = self._sizeMult\n\n        qAlnSize, tAlnSize = self.qspan * sizeMult, self.tspan\n        alnSize = min(qAlnSize, tAlnSize)\n        if alnSize <= 0:\n            return 0\n\n        sizeDiff = qAlnSize - tAlnSize\n        if sizeDiff < 0:\n            sizeDiff = 0 if ismRNA else -sizeDiff\n\n        insertFactor = self.qNumInsert\n        if not ismRNA:\n            insertFactor += self.tNumInsert\n\n        total = (self.matches + self.repMatches + self.misMatches) * sizeMult\n\n        return (1000 * (self.misMatches * sizeMult + insertFactor + \\\n                round(3 * math.log(1 + sizeDiff)))) / total if total != 0 else 0", "response": "calculate badness in parts per thousand\n        i. e. number of non - identical matches\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_prefix(dir=\"../\"):\n    prefix = glob(dir + \"*.gkpStore\")[0]\n    prefix = op.basename(prefix).rsplit(\".\", 1)[0]\n\n    return prefix", "response": "Look for prefix. gkpStore in the upper directory."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef trace(args):\n    p = OptionParser(trace.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(p.print_help())\n\n    s, = args\n    version, partID, unitigID = get_ID(s)\n\n    flist = glob(\"../5-consensus/*_{0:03d}.err\".format(int(partID)))\n    assert len(flist) == 1\n    fp = open(flist[0])\n\n    instate = False\n    for row in fp:\n        if working in row and unitigID in row:\n            rows = []\n            instate = True\n        if instate:\n            rows.append(row)\n        if failed in row:\n            instate = False\n            if len(rows) > 20:\n                ignore_line = \"... ({0} lines skipped)\\n\".format(len(rows) - 20)\n                rows = rows[:10] + [ignore_line] + rows[-10:]\n\n    print(\"\".join(rows), file=sys.stderr)", "response": "%prog trace unitig version partID. id unitigID"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef shred(args):\n    p = OptionParser(shred.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    s, = args\n    u = UnitigLayout(s)\n    u.shred()\n    u.print_to_file(inplace=True)", "response": "%prog shred unitigfile\n\n    Shred the unitig into one fragment per unitig to fix. This is the last\n    resort as a desperate fix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pull(args):\n    p = OptionParser(pull.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    prefix = get_prefix()\n    version, partID, unitigID = args\n    s = \".\".join(args)\n\n    cmd = \"tigStore\"\n    cmd += \" -g ../{0}.gkpStore -t ../{0}.tigStore\".format(prefix)\n    cmd += \" {0} -up {1} -d layout -u {2}\".format(version, partID, unitigID)\n\n    unitigfile = \"unitig\" + s\n    sh(cmd, outfile=unitigfile)\n\n    return unitigfile", "response": "%prog pull version partID unitigID"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef push(args):\n    p = OptionParser(push.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    s, = args\n    prefix = get_prefix()\n    version, partID, unitigID = get_ID(s)\n\n    cmd = \"tigStore\"\n    cmd += \" -g ../{0}.gkpStore -t ../{0}.tigStore\".format(prefix)\n    cmd += \" {0} -up {1} -R {2}\".format(version, partID, s)\n\n    sh(cmd)", "response": "%prog push unitig. version partID. unitigID"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pushall(args):\n    p = OptionParser(pushall.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    flist = args\n    for f in flist:\n        if f.endswith(\".log\"):\n            continue\n\n        push([f])", "response": "%prog pushall unitig1. txt"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef birch(args):\n    p = OptionParser(birch.__doc__)\n    opts, args, iopts = p.set_image_options(args, figsize=\"8x6\")\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    seqids, layout = args\n    fig = plt.figure(1, (iopts.w, iopts.h))\n    root = fig.add_axes([0, 0, 1, 1])\n\n    K = Karyotype(fig, root, seqids, layout)\n    L = K.layout\n\n    xs = .79\n    dt = dict(rectangle=False, circle=False)\n    # Embed a phylogenetic tree to the right\n    coords = {}\n    coords[\"Amborella\"] = (xs, L[0].y)\n    coords[\"Vitis\"] = (xs, L[1].y)\n    coords[\"Prunus\"] = (xs, L[2].y)\n    coords[\"Betula\"] = (xs, L[3].y)\n    coords[\"Populus\"] = (xs, L[4].y)\n    coords[\"Arabidopsis\"] = (xs, L[5].y)\n    coords[\"fabids\"] = join_nodes(root, coords, \"Prunus\", \"Betula\", xs, **dt)\n    coords[\"malvids\"] = join_nodes(root, coords, \\\n                                   \"Populus\", \"Arabidopsis\", xs, **dt)\n    coords[\"rosids\"] = join_nodes(root, coords, \"fabids\", \"malvids\", xs, **dt)\n    coords[\"eudicots\"] = join_nodes(root, coords, \"rosids\", \"Vitis\", xs, **dt)\n    coords[\"angiosperm\"] = join_nodes(root, coords, \\\n                                      \"eudicots\", \"Amborella\", xs, **dt)\n\n    # Show branch length\n    branch_length(root, coords[\"Amborella\"], coords[\"angiosperm\"], \">160.0\")\n    branch_length(root, coords[\"eudicots\"], coords[\"angiosperm\"],\n                  \">78.2\", va=\"top\")\n    branch_length(root, coords[\"Vitis\"], coords[\"eudicots\"], \"138.5\")\n    branch_length(root, coords[\"rosids\"], coords[\"eudicots\"],\n                  \"19.8\", va=\"top\")\n    branch_length(root, coords[\"Prunus\"], coords[\"fabids\"],\n                  \"104.2\", ha=\"right\", va=\"top\")\n    branch_length(root, coords[\"Arabidopsis\"], coords[\"malvids\"],\n                  \"110.2\", va=\"top\")\n    branch_length(root, coords[\"fabids\"], coords[\"rosids\"],\n                  \"19.8\", ha=\"right\", va=\"top\")\n    branch_length(root, coords[\"malvids\"], coords[\"rosids\"],\n                  \"8.5\", va=\"top\")\n\n    root.set_xlim(0, 1)\n    root.set_ylim(0, 1)\n    root.set_axis_off()\n\n    pf = \"birch\"\n    image_name = pf + \".\" + iopts.format\n    savefig(image_name, dpi=iopts.dpi, iopts=iopts)", "response": "%prog birch seqids layout"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cotton(args):\n    p = OptionParser(cotton.__doc__)\n    p.add_option(\"--depthfile\",\n                 help=\"Use depth info in this file [default: %default]\")\n    p.add_option(\"--switch\",\n                 help=\"Rename the seqid with two-column file [default: %default]\")\n    opts, args, iopts = p.set_image_options(args, figsize=\"8x7\")\n\n    if len(args) != 5:\n        sys.exit(p.print_help())\n\n    seqidsfile, klayout, datafile, bedfile, slayout = args\n    switch = opts.switch\n    depthfile = opts.depthfile\n\n    fig = plt.figure(1, (iopts.w, iopts.h))\n    root = fig.add_axes([0, 0, 1, 1])\n\n    kt = Karyotype(fig, root, seqidsfile, klayout)\n    Synteny(fig, root, datafile, bedfile, slayout, switch=switch)\n\n    light = \"lightslategrey\"\n    # Show the dup depth along the cotton chromosomes\n    if depthfile:\n        ymin, ymax = .9, .95\n        root.text(.11, .96, \"Cotton duplication level\", color=\"gray\", size=10)\n        root.plot([.1, .95], [ymin, ymin], color=\"gray\")\n        root.text(.96, .9, \"1x\", color=\"gray\", va=\"center\")\n        root.plot([.1, .95], [ymax, ymax], color=\"gray\")\n        root.text(.96, .95, \"6x\", color=\"gray\", va=\"center\")\n\n        fp = open(depthfile)\n        track = kt.tracks[0]  # Cotton\n        depths = []\n        for row in fp:\n            a, b, depth = row.split()\n            depth = int(depth)\n            try:\n                p = track.get_coords(a)\n                depths.append((p, depth))\n            except KeyError:\n                pass\n\n        depths.sort(key=lambda x: (x[0], -x[1]))\n        xx, yy = zip(*depths)\n        yy = [ymin + .01 * (x - 1) for x in yy]\n        root.plot(xx, yy, \"-\", color=light)\n\n    # legend showing the orientation of the genes\n    draw_gene_legend(root, .5, .68, .5)\n\n    # Zoom\n    xpos = .835\n    ytop = .9\n    xmin, xmax = .18, .82\n    ymin, ymax = ytop, .55\n    lc = \"k\"\n    kwargs = dict(lw=3, color=lc, mec=lc, mfc=\"w\", zorder=3)\n    root.plot((xpos, xpos), (ymax, .63), \":o\", **kwargs)\n    root.plot((xpos, xmin), (ymax, ymin), \":o\", **kwargs)\n    root.plot((xpos, xmax), (ymax, ymin), \":o\", **kwargs)\n    RoundRect(root, (.06, .17), .92, .35, fill=False, lw=2, ec=light)\n\n    # Panels\n    root.text(.05, .95, \"a\", size=20, fontweight=\"bold\")\n    root.text(.1, .45, \"b\", size=20, fontweight=\"bold\")\n\n    root.set_xlim(0, 1)\n    root.set_ylim(0, 1)\n    root.set_axis_off()\n\n    pf = \"cotton\"\n    image_name = pf + \".\" + iopts.format\n    savefig(image_name, dpi=iopts.dpi, iopts=iopts)", "response": "%prog cotton seqids karyotype. layout mcscan. out all. bed slayout. txt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_diagram(ax, x, y, label=\"S\", title=\"syntenic\", gradient=True):\n    trackgap = .06\n    tracklen = .12\n    xa, xb = x - tracklen, x + tracklen\n    ya, yb = y + trackgap, y - trackgap\n    hsps = (((60, 150), (50, 130)),\n           ((190, 225), (200, 240)),\n           ((330, 280), (360, 310)))\n\n    for yy in (ya, yb):\n        ax.plot((xa, xb), (yy, yy), \"-\", color=\"gray\", lw=2, zorder=1)\n\n    ytip = .015\n    mrange = 400\n    m = lambda t: xa + t * 1. / mrange * tracklen * 2\n\n    for i, ((a, b), (c, d)) in enumerate(hsps):\n        fb = False\n        if label == \"FB\" and i == 1:\n            c, d = 270, 280\n            fb = True\n        if label == \"G\" and i == 0:\n            c, d = 120, 65\n\n        a, b, c, d = [m(t) for t in (a, b, c, d)]\n        color = \"g\" if i == 1 else \"r\"\n        GeneGlyph(ax, a, b, ya, 2 * ytip, fc=color,\n                  gradient=gradient, zorder=10)\n\n        if i == 1 and label in (\"F\", \"G\", \"FN\"):\n            pass\n        else:\n            if fb:\n                GeneGlyph(ax, c, d, yb, 2 * ytip, fc='w', tip=0,\n                          gradient=gradient, zorder=10)\n            else:\n                GeneGlyph(ax, c, d, yb, 2 * ytip, fc='r',\n                          gradient=gradient, zorder=10)\n\n        r = Polygon(((a, ya - ytip), (c, yb + ytip),\n                      (d, yb + ytip), (b, ya - ytip)),\n                      fc='r', alpha=.2)\n\n        if i == 1 and label not in (\"S\", \"FB\"):\n            pass\n        elif i == 0 and label == \"G\":\n            pass\n        else:\n            ax.add_patch(r)\n\n    if label == \"FN\":\n        ax.text(x + .005, yb, \"NNNNN\", ha=\"center\", size=7)\n\n    title = \"{0}: {1}\".format(label, title)\n    ax.text(x, ya + 5 * ytip, title, size=8, ha=\"center\")", "response": "Plots the diagram of the system."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reads(args):\n    p = OptionParser(reads.__doc__)\n    p.add_option(\"-p\", dest=\"prefix_length\", default=4, type=\"int\",\n            help=\"group the reads based on the first N chars [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(p.print_help())\n\n    frgscffile, = args\n    prefix_length = opts.prefix_length\n\n    fp = open(frgscffile)\n    keyfn = lambda: defaultdict(int)\n    counts = defaultdict(keyfn)\n    for row in fp:\n        f = FrgScfLine(row)\n        fi = f.fragmentID[:prefix_length]\n        counts[f.scaffoldID][fi] += 1\n\n    for scf, count in sorted(counts.items()):\n        print(\"{0}\\t{1}\".format(scf,\n                \", \".join(\"{0}:{1}\".format(*x) for x in sorted(count.items()))))", "response": "%prog reads frgscffile\n\n    Report read counts per scaffold (based on frgscf)."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bed(args):\n    p = OptionParser(bed.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    frgscffile, = args\n    bedfile = frgscffile.rsplit(\".\", 1)[0] + \".bed\"\n    fw = open(bedfile, \"w\")\n\n    fp = open(frgscffile)\n    for row in fp:\n        f = FrgScfLine(row)\n        print(f.bedline, file=fw)\n\n    logging.debug(\"File written to `{0}`.\".format(bedfile))\n\n    return bedfile", "response": "%prog bed frgscffile\n\n    Convert the frgscf posmap file to bed format."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _score(cluster):\n    x, y = zip(*cluster)[:2]\n    return min(len(set(x)), len(set(y)))", "response": "Returns the score of the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the blast file and convert name into coordinates", "response": "def read_blast(blast_file, qorder, sorder, is_self=False, ostrip=True):\n    \"\"\" Read the blast and convert name into coordinates\n    \"\"\"\n    filtered_blast = []\n    seen = set()\n    bl = Blast(blast_file)\n    for b in bl:\n        query, subject = b.query, b.subject\n        if query == subject:\n            continue\n        if ostrip:\n            query, subject = gene_name(query), gene_name(subject)\n        if query not in qorder or subject not in sorder:\n            continue\n\n        qi, q = qorder[query]\n        si, s = sorder[subject]\n\n        if is_self:\n            # remove redundant a<->b to one side when doing self-self BLAST\n            if qi > si:\n                query, subject = subject, query\n                qi, si = si, qi\n                q, s = s, q\n            # Too close to diagonal! possible tandem repeats\n            if q.seqid == s.seqid and si - qi < 40:\n                continue\n\n        key = query, subject\n        if key in seen:\n            continue\n        seen.add(key)\n\n        b.qseqid, b.sseqid = q.seqid, s.seqid\n        b.qi, b.si = qi, si\n        b.query, b.subject = query, subject\n\n        filtered_blast.append(b)\n\n    logging.debug(\"A total of {0} BLAST imported from `{1}`.\".\\\n                  format(len(filtered_blast), blast_file))\n\n    return filtered_blast"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_anchors(ac, qorder, sorder, minsize=0):\n    all_anchors = defaultdict(list)\n    nanchors = 0\n    anchor_to_block = {}\n\n    for a, b, idx in ac.iter_pairs(minsize=minsize):\n        if a not in qorder or b not in sorder:\n            continue\n        qi, q = qorder[a]\n        si, s = sorder[b]\n        pair = (qi, si)\n\n        all_anchors[(q.seqid, s.seqid)].append(pair)\n        anchor_to_block[pair] = idx\n        nanchors += 1\n\n    logging.debug(\"A total of {0} anchors imported.\".format(nanchors))\n    assert nanchors == len(anchor_to_block)\n\n    return all_anchors, anchor_to_block", "response": "Reads the anchor files and returns a dict of all the anchors and anchor_to_block."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun synteny_scan on each chromosome pair", "response": "def batch_scan(points, xdist=20, ydist=20, N=5):\n    \"\"\"\n    runs synteny_scan() per chromosome pair\n    \"\"\"\n    chr_pair_points = group_hits(points)\n\n    clusters = []\n    for chr_pair in sorted(chr_pair_points.keys()):\n        points = chr_pair_points[chr_pair]\n        clusters.extend(synteny_scan(points, xdist, ydist, N))\n\n    return clusters"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef synteny_liftover(points, anchors, dist):\n    from scipy.spatial import cKDTree\n\n    points = np.array(points, dtype=int)\n    ppoints = points[:, :2] if points.shape[1] > 2 else points\n    tree = cKDTree(anchors, leafsize=16)\n    #print tree.data\n    dists, idxs = tree.query(ppoints, p=1, distance_upper_bound=dist)\n\n    for point, dist, idx in zip(points, dists, idxs):\n        if idx == tree.n:  # nearest is out of range\n            continue\n        if dist == 0:  # already in anchors\n            continue\n\n        yield point, tuple(anchors[idx])", "response": "This function returns a generator that yields the nearest anchors for all the points."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse command line options and return blast_file anchor_file", "response": "def add_options(p, args, dist=10):\n    \"\"\"\n    scan and liftover has similar interfaces, so share common options\n    returns opts, files\n    \"\"\"\n    p.set_beds()\n    p.add_option(\"--dist\", default=dist, type=\"int\",\n            help=\"Extent of flanking regions to search [default: %default]\")\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    blast_file, anchor_file = args\n\n    return blast_file, anchor_file, opts.dist, opts"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef layout(args):\n    from jcvi.algorithms.ec import GA_setup, GA_run\n\n    p = OptionParser(layout.__doc__)\n    p.set_beds()\n    p.set_cpus(cpus=32)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    simplefile, qseqids, sseqids = args\n    qbed, sbed, qorder, sorder, is_self = check_beds(simplefile, p, opts)\n\n    qseqids = qseqids.strip().split(\",\")\n    sseqids = sseqids.strip().split(\",\")\n    qseqids_ii = dict((s, i) for i, s in enumerate(qseqids))\n    sseqids_ii = dict((s, i) for i, s in enumerate(sseqids))\n\n    blocks = SimpleFile(simplefile).blocks\n    scores = defaultdict(int)\n    for a, b, c, d, score, orientation, hl in blocks:\n        qi, q = qorder[a]\n        si, s = sorder[c]\n        qseqid, sseqid = q.seqid, s.seqid\n        if sseqid not in sseqids:\n            continue\n        scores[sseqids_ii[sseqid], qseqid] += score\n\n    data = []\n    for (a, b), score in sorted(scores.items()):\n        if b not in qseqids_ii:\n            continue\n        data.append((qseqids_ii[b], score))\n\n    tour = range(len(qseqids))\n    toolbox = GA_setup(tour)\n    toolbox.register(\"evaluate\", colinear_evaluate_weights, data=data)\n    tour, fitness = GA_run(toolbox, ngen=100, npop=100, cpus=opts.cpus)\n    tour = [qseqids[x] for x in tour]\n\n    print(\",\".join(tour))", "response": "%prog layout query. subject. simple query. seqids subject. seqids"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fromaligns(args):\n    p = OptionParser(fromaligns.__doc__)\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    alignsfile, = args\n    fp = must_open(alignsfile)\n    fw = must_open(opts.outfile, \"w\")\n    for row in fp:\n        if row.startswith(\"## Alignment\"):\n            print(\"###\", file=fw)\n            continue\n        if row[0] == '#' or not row.strip():\n            continue\n        atoms = row.split(':')[-1].split()\n        print(\"\\t\".join(atoms[:2]), file=fw)\n    fw.close()", "response": "%prog fromaligns alignsfile > out. aligns"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mcscanq(args):\n    p = OptionParser(mcscanq.__doc__)\n    p.add_option(\"--color\", help=\"Add color highlight, used in plotting\")\n    p.add_option(\"--invert\", default=False, action=\"store_true\",\n                 help=\"Invert query and subject [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) < 2:\n        sys.exit(not p.print_help())\n\n    qids, blocksfile = args\n    b = BlockFile(blocksfile)\n    fp = open(qids)\n    for gene in fp:\n        gene = gene.strip()\n        for line in b.query_gene(gene, color=opts.color, invert=opts.invert):\n            print(line)", "response": "%prog mcscanq query. ids blocksfile"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef spa(args):\n    from jcvi.algorithms.graph import merge_paths\n    from jcvi.utils.cbook import uniqify\n\n    p = OptionParser(spa.__doc__)\n    p.add_option(\"--unmapped\", default=False, action=\"store_true\",\n                 help=\"Include unmapped scaffolds in the list [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    spafiles = args\n    paths = []\n    mappings = []\n    missings = []\n    for spafile in spafiles:\n        fp = open(spafile)\n        path = []\n        mapping = []\n        missing = []\n        for row in fp:\n            if row[0] == '#' or not row.strip():\n                continue\n\n            atoms = row.rstrip().split('\\t')\n            if len(atoms) == 2:\n                a, c2 = atoms\n                assert a == \"unmapped\"\n                missing.append(c2)\n                continue\n\n            c1, c2, orientation = atoms\n            path.append(c1)\n            mapping.append(c2)\n\n        paths.append(uniqify(path))\n        mappings.append(mapping)\n        missings.append(missing)\n\n    ref = merge_paths(paths)\n    print(\"ref\", len(ref), \",\".join(ref))\n    for spafile, mapping, missing in zip(spafiles, mappings, missings):\n        mapping = [x for x in mapping if \"random\" not in x]\n        mapping = uniqify(mapping)\n        if len(mapping) < 50 and opts.unmapped:\n            mapping = uniqify(mapping + missing)\n\n        print(spafile, len(mapping), \",\".join(mapping))", "response": "%prog spa spafiles\n\n    Convert chromosome ordering from SPA to simple lists. First column is the\n    reference order."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rebuild(args):\n    p = OptionParser(rebuild.__doc__)\n    p.add_option(\"--header\", default=False, action=\"store_true\",\n                 help=\"First line is header [default: %default]\")\n    p.add_option(\"--write_blast\", default=False, action=\"store_true\",\n                 help=\"Get blast records of rebuilt anchors [default: %default]\")\n    p.set_beds()\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    blocksfile, blastfile = args\n    bk = BlockFile(blocksfile, header=opts.header)\n    fw = open(\"pairs\", \"w\")\n    for a, b, h in bk.iter_all_pairs():\n        print(\"\\t\".join((a, b)), file=fw)\n    fw.close()\n\n    if opts.write_blast:\n        AnchorFile(\"pairs\").blast(blastfile, \"pairs.blast\")\n\n    fw = open(\"tracks\", \"w\")\n    for g, col in bk.iter_gene_col():\n        print(\"\\t\".join(str(x) for x in (g, col)), file=fw)\n    fw.close()", "response": "%prog rebuild blocksfile blastfile\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef simple(args):\n    p = OptionParser(simple.__doc__)\n    p.add_option(\"--rich\", default=False, action=\"store_true\", \\\n                help=\"Output additional columns [default: %default]\")\n    p.add_option(\"--coords\", default=False, action=\"store_true\",\n                help=\"Output columns with base coordinates [default: %default]\")\n    p.add_option(\"--bed\", default=False, action=\"store_true\",\n                help=\"Generate BED file for the blocks\")\n    p.add_option(\"--noheader\", default=False, action=\"store_true\",\n                help=\"Don't output header [default: %default]\")\n    p.set_beds()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    anchorfile, = args\n    additional = opts.rich\n    coords = opts.coords\n    header = not opts.noheader\n    bed = opts.bed\n    if bed:\n        coords = True\n        bbed = Bed()\n\n    ac = AnchorFile(anchorfile)\n    simplefile = anchorfile.rsplit(\".\", 1)[0] + \".simple\"\n\n    qbed, sbed, qorder, sorder, is_self = check_beds(anchorfile, p, opts)\n    pf = \"-\".join(anchorfile.split(\".\", 2)[:2])\n    blocks = ac.blocks\n\n    if coords:\n        h = \"Block|Chr|Start|End|Span|StartGene|EndGene|GeneSpan|Orientation\"\n    else:\n        h = \"StartGeneA|EndGeneA|StartGeneB|EndGeneB|Orientation|Score\"\n        if additional:\n            h += \"|StartOrderA|EndOrderA|StartOrderB|EndOrderB|\"\\\n                  \"SizeA|SizeB|Size|Block\"\n\n    fws = open(simplefile, \"w\")\n    if header:\n        print(\"\\t\".join(h.split(\"|\")), file=fws)\n\n    atotalbase = btotalbase = 0\n    for i, block in enumerate(blocks):\n\n        a, b, scores = zip(*block)\n        a = [qorder[x] for x in a]\n        b = [sorder[x] for x in b]\n        ia, oa = zip(*a)\n        ib, ob = zip(*b)\n\n        astarti, aendi = min(ia), max(ia)\n        bstarti, bendi = min(ib), max(ib)\n        astart, aend = min(a)[1].accn, max(a)[1].accn\n        bstart, bend = min(b)[1].accn, max(b)[1].accn\n\n        sizeA = len(set(ia))\n        sizeB = len(set(ib))\n        size = len(block)\n\n        slope, intercept = np.polyfit(ia, ib, 1)\n        orientation = \"+\" if slope >= 0 else '-'\n        aspan = aendi - astarti + 1\n        bspan = bendi - bstarti + 1\n        score = int((aspan * bspan) ** .5)\n        score = str(score)\n        block_id = pf + \"-block-{0}\".format(i)\n\n        if coords:\n\n            aseqid, astartbase, aendbase = \\\n                    get_boundary_bases(astart, aend, qorder)\n            bseqid, bstartbase, bendbase = \\\n                    get_boundary_bases(bstart, bend, sorder)\n            abase = aendbase - astartbase + 1\n            bbase = bendbase - bstartbase + 1\n            atotalbase += abase\n            btotalbase += bbase\n\n            # Write dual lines\n            aargs = [block_id, aseqid, astartbase, aendbase,\n                     abase, astart, aend, aspan, \"+\"]\n            bargs = [block_id, bseqid, bstartbase, bendbase,\n                     bbase, bstart, bend, bspan, orientation]\n\n            if bed:\n                bbed.append(BedLine(\"\\t\".join(str(x) for x in \\\n                           (bseqid, bstartbase - 1, bendbase,\n                           \"{}:{}-{}\".format(aseqid, astartbase, aendbase),\n                           size, orientation))))\n\n            for args in (aargs, bargs):\n                print(\"\\t\".join(str(x) for x in args), file=fws)\n            continue\n\n        args = [astart, aend, bstart, bend, score, orientation]\n        if additional:\n            args += [astarti, aendi, bstarti, bendi,\n                     sizeA, sizeB, size, block_id]\n        print(\"\\t\".join(str(x) for x in args), file=fws)\n\n    fws.close()\n    logging.debug(\"A total of {0} blocks written to `{1}`.\".format(i + 1, simplefile))\n\n    if coords:\n        print(\"Total block span in {0}: {1}\".format(qbed.filename, \\\n                        human_size(atotalbase, precision=2)), file=sys.stderr)\n        print(\"Total block span in {0}: {1}\".format(sbed.filename, \\\n                        human_size(btotalbase, precision=2)), file=sys.stderr)\n        print(\"Ratio: {0:.1f}x\".format(\\\n                        max(atotalbase, btotalbase) * 1. / min(atotalbase, btotalbase)), file=sys.stderr)\n\n    if bed:\n        bedfile = simplefile + \".bed\"\n        bbed.print_to_file(filename=bedfile, sorted=True)\n        logging.debug(\"Bed file written to `{}`\".format(bedfile))", "response": "%prog simple anchorfile --qbed=qbedfile --sbed=sbedfile [options]\n\n    Write the block ends for each block in the anchorfile.\n    GeneA1    GeneA2    GeneB1    GeneB2   +/-      score\n\n    Optional additional columns:\n    orderA1   orderA2   orderB1   orderB2  sizeA    sizeB   size    block_id\n\n    With base coordinates (--coords):\n    block_id  seqidA    startA    endA     bpSpanA  GeneA1   GeneA2  geneSpanA\n    block_id  seqidB    startB    endB     bpSpanB  GeneB1   GeneB2  geneSpanB"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef screen(args):\n    p = OptionParser(screen.__doc__)\n    p.set_beds()\n\n    p.add_option(\"--ids\", help=\"File with block IDs (0-based) [default: %default]\")\n    p.add_option(\"--seqids\", help=\"File with seqids [default: %default]\")\n    p.add_option(\"--seqpairs\", help=\"File with seqpairs [default: %default]\")\n    p.add_option(\"--nointra\", action=\"store_true\",\n                 help=\"Remove intra-chromosomal blocks [default: %default]\")\n    p.add_option(\"--minspan\", default=0, type=\"int\",\n                 help=\"Only blocks with span >= [default: %default]\")\n    p.add_option(\"--minsize\", default=0, type=\"int\",\n                 help=\"Only blocks with anchors >= [default: %default]\")\n    p.add_option(\"--simple\", action=\"store_true\",\n                 help=\"Write simple anchorfile with block ends [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    anchorfile, newanchorfile = args\n    ac = AnchorFile(anchorfile)\n    idsfile = opts.ids\n    seqidsfile = opts.seqids\n    seqpairsfile = opts.seqpairs\n    minspan = opts.minspan\n    minsize = opts.minsize\n    osimple = opts.simple\n    nointra = opts.nointra\n    ids, seqids, seqpairs = None, None, None\n\n    if idsfile:\n        ids = SetFile(idsfile, delimiter=',')\n        ids = set(int(x) for x in ids)\n    if seqidsfile:\n        seqids = SetFile(seqidsfile, delimiter=',')\n    if seqpairsfile:\n        fp = open(seqpairsfile)\n        seqpairs = set()\n        for row in fp:\n            a, b = row.strip().split(\",\")\n            seqpairs.add((a, b))\n            seqpairs.add((b, a))\n\n    qbed, sbed, qorder, sorder, is_self = check_beds(anchorfile, p, opts)\n    blocks = ac.blocks\n    selected = 0\n    fw = open(newanchorfile, \"w\")\n\n    for i, block in enumerate(blocks):\n        if ids and i not in ids:\n            continue\n\n        a, b, scores = zip(*block)\n        a = [qorder[x] for x in a]\n        b = [sorder[x] for x in b]\n        ia, oa = zip(*a)\n        ib, ob = zip(*b)\n        aspan = max(ia) - min(ia) + 1\n        bspan = max(ib) - min(ib) + 1\n        aseqid = oa[0].seqid\n        bseqid = ob[0].seqid\n\n        if seqids:\n            if (aseqid not in seqids) or (bseqid not in seqids):\n                continue\n\n        if seqpairs:\n            if (aseqid, bseqid) not in seqpairs:\n                continue\n\n        if nointra and aseqid == bseqid:\n            continue\n\n        if minsize:\n            if len(block) < minsize:\n                continue\n\n        if minspan:\n            if aspan < minspan or bspan < minspan:\n                continue\n\n        selected += 1\n        print(\"###\", file=fw)\n        for line in block:\n            print(\"\\t\".join(line), file=fw)\n\n    fw.close()\n\n    if osimple:\n        simple([newanchorfile, \"--noheader\", \\\n                \"--qbed=\" + qbed.filename, \"--sbed=\" + sbed.filename])\n\n    logging.debug(\"Before: {0} blocks, After: {1} blocks\".\\\n                  format(len(blocks), selected))", "response": "%prog screen anchorfile newanchorfile"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stats(args):\n    from jcvi.utils.cbook import percentage\n\n    p = OptionParser(stats.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    blocksfile, = args\n    fp = open(blocksfile)\n    counts = defaultdict(int)\n    total = orthologous = 0\n    for row in fp:\n        atoms = row.rstrip().split(\"\\t\")\n        hits = [x for x in atoms[1:] if x != '.']\n        counts[len(hits)] += 1\n        total += 1\n        if atoms[1] != '.':\n            orthologous += 1\n\n    print(\"Total lines: {0}\".format(total), file=sys.stderr)\n    for i, n in sorted(counts.items()):\n        print(\"Count {0}: {1}\".format(i, percentage(n, total)), file=sys.stderr)\n\n    print(file=sys.stderr)\n\n    matches = sum(n for i, n in counts.items() if i != 0)\n    print(\"Total lines with matches: {0}\".\\\n                format(percentage(matches, total)), file=sys.stderr)\n    for i, n in sorted(counts.items()):\n        if i == 0:\n            continue\n\n        print(\"Count {0}: {1}\".format(i, percentage(n, matches)), file=sys.stderr)\n\n    print(file=sys.stderr)\n    print(\"Orthologous matches: {0}\".\\\n                format(percentage(orthologous, matches)), file=sys.stderr)", "response": "%prog stats blocksfile\n\n    Provide statistics for MCscan-style blocks. The count of homologs in each\n    pivot gene is recorded."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mcscan(args):\n    p = OptionParser(mcscan.__doc__)\n    p.add_option(\"--iter\", default=100, type=\"int\",\n                 help=\"Max number of chains to output [default: %default]\")\n    p.add_option(\"--ascii\", default=False, action=\"store_true\",\n                 help=\"Output symbols rather than gene names [default: %default]\")\n    p.add_option(\"--Nm\", default=10, type=\"int\",\n                 help=\"Clip block ends to allow slight overlaps [default: %default]\")\n    p.add_option(\"--trackids\", action=\"store_true\",\n                 help=\"Track block IDs in separate file [default: %default]\")\n    p.add_option(\"--mergetandem\", default=None,\n                 help=\"merge tandems genes in output acoording to PATH-TO-TANDEM_FILE, \"\\\n                 \"cannot be used with --ascii\")\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    bedfile, anchorfile = args\n    ascii = opts.ascii\n    clip = opts.Nm\n    trackids = opts.trackids\n    ofile = opts.outfile\n    mergetandem = opts.mergetandem\n    bed = Bed(bedfile)\n    order = bed.order\n\n    if trackids:\n        olog = ofile + \".tracks\"\n        fwlog = must_open(olog, \"w\")\n\n    if mergetandem:\n        assert not ascii\n        tandems = {}\n        for row in file(mergetandem):\n            row = row.split()\n            s = \";\".join(row)\n            for atom in row:\n                tandems[atom] = s\n\n    ac = AnchorFile(anchorfile)\n    ranges = []\n    block_pairs = defaultdict(dict)\n    blocks = ac.blocks\n    for i, ib in enumerate(blocks):\n        q, s, t = zip(*ib)\n        if q[0] not in order:\n            q, s = s, q\n\n        r = get_range(q, s, t, i, order, block_pairs, clip=clip)\n        ranges.append(r)\n\n        assert q[0] in order\n        if s[0] not in order:\n            continue\n\n        # is_self comparison\n        q, s = s, q\n        r = get_range(q, s, t, i, order, block_pairs, clip=clip)\n        ranges.append(r)\n\n    fw = must_open(ofile, \"w\")\n\n    tracks = []\n    print(\"Chain started: {0} blocks\".format(len(ranges)), file=sys.stderr)\n    iteration = 0\n    while ranges:\n        if iteration >= opts.iter:\n            break\n\n        selected, score = range_chain(ranges)\n        tracks.append(selected)\n        selected = set(x.id for x in selected)\n        if trackids:\n            print(\",\".join(str(x) for x in sorted(selected)), file=fwlog)\n\n        ranges = [x for x in ranges if x.id not in selected]\n        msg = \"Chain {0}: score={1}\".format(iteration, score)\n        if ranges:\n            msg += \" {0} blocks remained..\".format(len(ranges))\n        else:\n            msg += \" done!\"\n\n        print(msg, file=sys.stderr)\n        iteration += 1\n\n    mbed = []\n    for b in bed:\n        id = b.accn\n        atoms = []\n        for track in tracks:\n            track_ids = [x.id for x in track]\n            for tid in track_ids:\n                pairs = block_pairs[tid]\n                anchor = pairs.get(id, \".\")\n                if anchor != \".\":\n                    break\n            if ascii and anchor != \".\":\n                anchor = \"x\"\n            atoms.append(anchor)\n        mbed.append((id, atoms))\n\n    for id, atoms in mbed:\n        sep = \"\" if ascii else \"\\t\"\n        if mergetandem:\n            for i, atom in enumerate(atoms):\n                atoms[i] = tandems.get(atom, atom)\n        print(\"\\t\".join((id, sep.join(atoms))), file=fw)\n\n    logging.debug(\"MCscan blocks written to `{0}`.\".format(ofile))\n    if trackids:\n        logging.debug(\"Block IDs written to `{0}`.\".format(olog))", "response": "%prog mcscan bedfile anchorfile"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites per gene depth to filefw", "response": "def write_details(fw, details, bed):\n    \"\"\"\n    Write per gene depth to file\n    \"\"\"\n    for a, b, depth in details:\n        for i in xrange(a, b):\n            gi = bed[i].accn\n            print(\"\\t\".join((gi, str(depth))), file=fw)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef depth(args):\n    from jcvi.utils.range import range_depth\n\n    p = OptionParser(depth.__doc__)\n    p.add_option(\"--depthfile\",\n                 help=\"Generate file with gene and depth [default: %default]\")\n    p.add_option(\"--histogram\", default=False, action=\"store_true\",\n                 help=\"Plot histograms in PDF\")\n    p.add_option(\"--xmax\", type=\"int\", help=\"x-axis maximum to display in plot\")\n    p.add_option(\"--title\", default=None, help=\"Title to display in plot\")\n    p.add_option(\"--quota\", help=\"Force to use this quota, e.g. 1:1, 1:2 ...\")\n    p.set_beds()\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    anchorfile, = args\n    qbed, sbed, qorder, sorder, is_self = check_beds(anchorfile, p, opts)\n    depthfile = opts.depthfile\n    ac = AnchorFile(anchorfile)\n    qranges = []\n    sranges = []\n    blocks = ac.blocks\n    for ib in blocks:\n        q, s, t = zip(*ib)\n        q = [qorder[x] for x in q]\n        s = [sorder[x] for x in s]\n        qrange = (min(q)[0], max(q)[0])\n        srange = (min(s)[0], max(s)[0])\n        qranges.append(qrange)\n        sranges.append(srange)\n        if is_self:\n            qranges.append(srange)\n\n    qgenome = op.basename(qbed.filename).split(\".\")[0]\n    sgenome = op.basename(sbed.filename).split(\".\")[0]\n    qtag = \"Genome {0} depths\".format(qgenome)\n    print(\"{}:\".format(qtag), file=sys.stderr)\n    dsq, details = range_depth(qranges, len(qbed))\n    if depthfile:\n        fw = open(depthfile, \"w\")\n        write_details(fw, details, qbed)\n\n    if is_self:\n        return\n\n    stag = \"Genome {0} depths\".format(sgenome)\n    print(\"{}:\".format(stag), file=sys.stderr)\n    dss, details = range_depth(sranges, len(sbed))\n    if depthfile:\n        write_details(fw, details, sbed)\n        fw.close()\n        logging.debug(\"Depth written to `{0}`.\".format(depthfile))\n\n    if not opts.histogram:\n        return\n\n    from jcvi.graphics.base import plt, quickplot_ax, savefig, normalize_axes\n\n    # Plot two histograms one for query genome, one for subject genome\n    plt.figure(1, (6, 3))\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n\n    xmax = opts.xmax or max(4, max(dsq.keys() + dss.keys()))\n    if opts.quota:\n        speak, qpeak = opts.quota.split(\":\")\n        qpeak, speak = int(qpeak), int(speak)\n    else:\n        qpeak = find_peak(dsq)\n        speak = find_peak(dss)\n\n    qtag = \"# of {} blocks per {} gene\".format(sgenome, qgenome)\n    stag = \"# of {} blocks per {} gene\".format(qgenome, sgenome)\n    quickplot_ax(ax1, dss, 0, xmax, stag, ylabel=\"Percentage of genome\",\n                 highlight=range(1, speak + 1))\n    quickplot_ax(ax2, dsq, 0, xmax, qtag, ylabel=None,\n                 highlight=range(1, qpeak + 1))\n\n    title = opts.title or \"{} vs {} syntenic depths\\n{}:{} pattern\"\\\n                    .format(qgenome, sgenome, speak, qpeak)\n    root = f.add_axes([0, 0, 1, 1])\n    vs, pattern = title.split('\\n')\n    root.text(.5, .97, vs, ha=\"center\", va=\"center\", color=\"darkslategray\")\n    root.text(.5, .925, pattern, ha=\"center\", va=\"center\",\n                                 color=\"tomato\", size=16)\n    print(title, file=sys.stderr)\n\n    normalize_axes(root)\n\n    pf = anchorfile.rsplit(\".\", 1)[0] + \".depth\"\n    image_name = pf + \".pdf\"\n    savefig(image_name)", "response": "%prog depth anchorfile --qbed qbedfile --sbed sbedfile\n\n    Calculate the depths in the two genomes in comparison, given in --qbed and\n    --sbed. The synteny blocks will be layered on the genomes, and the\n    multiplicity will be summarized to stderr."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef scan(args):\n    p = OptionParser(scan.__doc__)\n    p.add_option(\"-n\", \"--min_size\", dest=\"n\", type=\"int\", default=4,\n            help=\"minimum number of anchors in a cluster [default: %default]\")\n    p.add_option(\"--liftover\",\n            help=\"Scan BLAST file to find extra anchors [default: %default]\")\n    p.set_stripnames()\n\n    blast_file, anchor_file, dist, opts = add_options(p, args, dist=20)\n    qbed, sbed, qorder, sorder, is_self = check_beds(blast_file, p, opts)\n\n    filtered_blast = read_blast(blast_file, qorder, sorder, \\\n                                is_self=is_self, ostrip=False)\n\n    fw = open(anchor_file, \"w\")\n    logging.debug(\"Chaining distance = {0}\".format(dist))\n\n    clusters = batch_scan(filtered_blast, xdist=dist, ydist=dist, N=opts.n)\n    for cluster in clusters:\n        print(\"###\", file=fw)\n        for qi, si, score in cluster:\n            query, subject = qbed[qi].accn, sbed[si].accn\n            print(\"\\t\".join((query, subject, str(int(score)))), file=fw)\n\n    fw.close()\n    summary([anchor_file])\n\n    lo = opts.liftover\n    if not lo:\n        return anchor_file\n\n    bedopts = [\"--qbed=\" + opts.qbed, \"--sbed=\" + opts.sbed]\n    ostrip = [] if opts.strip_names else [\"--no_strip_names\"]\n    newanchorfile = liftover([lo, anchor_file] + bedopts + ostrip)\n    return newanchorfile", "response": "%prog scan blastfile anchor_file dist"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef blast(self, blastfile=None, outfile=None):\n        from jcvi.formats.blast import BlastSlow, BlastLineByConversion\n\n        if not outfile:\n            outfile = self.filename + \".blast\"\n\n        if blastfile is not None:\n            blasts = BlastSlow(blastfile).to_dict()\n        else:\n            blasts = None\n\n        fw = must_open(outfile, \"w\", checkexists=True)\n        nlines = 0\n        for a, b, id in self.iter_pairs():\n            if (a, b) in blasts:\n                bline = blasts[(a, b)]\n            elif (b, a) in blasts:\n                bline = blasts[(b, a)]\n            else:\n                line = \"\\t\".join((a, b))\n                bline = BlastLineByConversion(line, mode=\"110000000000\")\n\n            print(bline, file=fw)\n            nlines += 1\n        fw.close()\n\n        logging.debug(\"A total of {0} BLAST lines written to `{1}`.\"\\\n                        .format(nlines, outfile))\n\n        return outfile", "response": "BLAST the sequence of elements in the file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nquery the cache for a specific gene.", "response": "def query_gene(self, gene, color=None, invert=False):\n        \"\"\"\n        Used in mcscanq() for query\n        \"\"\"\n        qi = self.columns[0].index(gene)\n        ndata = len(self.data)\n        for col in self.columns[1:]:\n            upstream_dist = downstream_dist = 1000\n            # search upstream\n            for i in range(qi - 1, -1, -1):\n                if col[i] not in (\".\", \"\"):\n                    upstream = col[i]\n                    upstream_dist = qi - i\n                    break\n            # search downstream\n            for i in range(qi, ndata):\n                if col[i] not in (\".\", \"\"):\n                    downstream = col[i]\n                    downstream_dist = i - qi\n                    break\n            closest = upstream if upstream_dist < downstream_dist \\\n                    else downstream\n            # output in .simple format\n            if invert:\n                line = \"\\t\".join(str(x) for x in \\\n                        (closest, closest, gene, gene, 0, \"+\"))\n            else:\n                line = \"\\t\".join(str(x) for x in \\\n                        (gene, gene, closest, closest, 0, \"+\"))\n            if color is not None:\n                line = color + \"*\" + line\n            yield line"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bites(args):\n\n    p = OptionParser(__doc__)\n    opts, args = p.parse_args()\n\n    fig = plt.figure(1, (6, 6))\n    root = fig.add_axes([0, 0, 1, 1])\n\n    # HSP pairs\n    hsps = (((50, 150), (60, 180)),\n           ((190, 250), (160, 235)),\n           ((300, 360), (270, 330)),\n           ((430, 470), (450, 490)),\n           ((570, 620), (493, 543)),\n           ((540, 555), (370, 385)),  # non-collinear hsps\n          )\n\n    titlepos = (.9, .65, .4)\n    titles = (\"Compare orthologous region\",\n              \"Find collinear HSPs\",\n              \"Scan paired gaps\")\n    ytip = .01\n    mrange = 650.\n    m = lambda x: x / mrange * .7 + .1\n    for i, (ya, title) in enumerate(zip(titlepos, titles)):\n        yb = ya - .1\n        plt.plot((.1, .8), (ya, ya), \"-\", color=\"gray\", lw=2, zorder=1)\n        plt.plot((.1, .8), (yb, yb), \"-\", color=\"gray\", lw=2, zorder=1)\n        RoundLabel(root, .5, ya + 4 * ytip, title)\n        root.text(.9, ya, \"A. thaliana\", ha=\"center\", va=\"center\")\n        root.text(.9, yb, \"B. rapa\", ha=\"center\", va=\"center\")\n        myhsps = hsps\n        if i >= 1:\n            myhsps = hsps[:-1]\n        for (a, b), (c, d) in myhsps:\n            a, b, c, d = [m(x) for x in (a, b, c, d)]\n            r1 = Rectangle((a, ya - ytip), b - a, 2 * ytip, fc='r', lw=0, zorder=2)\n            r2 = Rectangle((c, yb - ytip), d - c, 2 * ytip, fc='r', lw=0, zorder=2)\n            r3 = Rectangle((a, ya - ytip), b - a, 2 * ytip, fill=False, zorder=3)\n            r4 = Rectangle((c, yb - ytip), d - c, 2 * ytip, fill=False, zorder=3)\n            r5 = Polygon(((a, ya - ytip), (c, yb + ytip),\n                          (d, yb + ytip), (b, ya - ytip)),\n                          fc='r', alpha=.2)\n            rr = (r1, r2, r3, r4, r5)\n            if i == 2:\n                rr = rr[:-1]\n            for r in rr:\n                root.add_patch(r)\n\n    # Gap pairs\n    hspa, hspb = zip(*myhsps)\n    gapa, gapb = [], []\n    for (a, b), (c, d) in pairwise(hspa):\n        gapa.append((b + 1, c - 1))\n    for (a, b), (c, d) in pairwise(hspb):\n        gapb.append((b + 1, c - 1))\n    gaps = zip(gapa, gapb)\n    tpos = titlepos[-1]\n\n    yy = tpos - .05\n    for i, ((a, b), (c, d)) in enumerate(gaps):\n        i += 1\n        a, b, c, d = [m(x) for x in (a, b, c, d)]\n        xx = (a + b + c + d) / 4\n        TextCircle(root, xx, yy, str(i))\n\n    # Bites\n    ystart = .24\n    ytip = .05\n    bites = ((\"Bite(40=>-15)\", True),\n             (\"Bite(50=>35)\", False),\n             (\"Bite(70=>120)\", False),\n             (\"Bite(100=>3)\", True))\n    for i, (bite, selected) in enumerate(bites):\n        xx = .15 if (i % 2 == 0) else .55\n        yy = ystart - i / 2 * ytip\n        i += 1\n        TextCircle(root, xx, yy, str(i))\n        color = \"k\" if selected else \"gray\"\n        root.text(xx + ytip, yy, bite, size=10, color=color, va=\"center\")\n\n    root.set_xlim(0, 1)\n    root.set_ylim(0, 1)\n    root.set_axis_off()\n\n    figname = fname() + \".pdf\"\n    savefig(figname, dpi=300)", "response": "%prog bites bites_base. txt"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef scenario(args):\n    p = OptionParser(__doc__)\n    opts, args = p.parse_args()\n\n    fig = plt.figure(1, (5, 5))\n    root = fig.add_axes([0, 0, 1, 1])\n\n    root.set_xlim(0, 1)\n    root.set_ylim(0, 1)\n    root.set_axis_off()\n\n    # Layout format: (x, y, label, (chr lengths))\n    anc = (.5, .9, \"Ancestor\", (1,))\n    s1 = (.2, .6, \"Genome I\", (1,))\n    s2 = (.5, .6, \"Genome II\", (1,))\n    s3 = (.8, .6, \"Genome III\", (1,))\n    tetra = (.35, .4, \"Tetraploid I / II\", (.5, .9))\n    hexa = (.5, .1, \"Hexaploid I / II / III\", (.36, .46, .9))\n    labels = (anc, s1, s2, s3, tetra, hexa)\n    connections = ((anc, s1), (anc, s2), (anc, s3),\\\n            (s1, tetra), (s2, tetra),\n            (tetra, hexa), (s3, hexa))\n\n    xinterval = .02\n    yratio = .05\n    for xx, yy, label, chrl in labels:\n        #RoundLabel(root, xx, yy, label)\n        root.text(xx, yy, label, ha=\"center\", va=\"center\")\n        offset = len(label) * .012\n        for i, c in enumerate(chrl):\n            ya = yy + yratio * c\n            yb = yy - yratio * c\n            Chromosome(root, xx - offset + i * xinterval, ya, yb, width=.01)\n\n    # Comments\n    comments = ((.15, .33, \"II dominant\"),\n                (.25, .03, \"III dominant\"))\n\n    for xx, yy, c in comments:\n        root.text(xx, yy, c, size=9, ha=\"center\", va=\"center\")\n\n    # Branches\n    tip = .04\n    for a, b in connections:\n        xa, ya, la, chra = a\n        xb, yb, lb, chrb = b\n        plt.plot((xa, xb), (ya - tip, yb + 2 * tip), 'k-', lw=2, alpha=.5)\n\n    figname = fname() + \".pdf\"\n    savefig(figname, dpi=300)", "response": "%prog scenario\n\n    Illustration of the two-step genome merger process for B. rapa companion paper."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter(args):\n    p = OptionParser(filter.__doc__)\n    p.add_option(\"--score\", dest=\"score\", default=0, type=\"int\",\n                 help=\"Score cutoff\")\n    p.set_align(pctid=95, hitlen=100, evalue=.01)\n    p.add_option(\"--noself\", default=False, action=\"store_true\",\n                 help=\"Remove self-self hits\")\n    p.add_option(\"--ids\", help=\"Path to file with ids to retain\")\n    p.add_option(\"--inverse\", default=False, action=\"store_true\",\n                 help=\"Similar to grep -v, inverse\")\n    p.set_outfile(outfile=None)\n\n    opts, args = p.parse_args(args)\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    if opts.ids:\n        ids = set()\n        for row in must_open(opts.ids):\n            if row[0] == \"#\":\n                continue\n            row = row.replace(\",\", \"\\t\")\n            ids.update(row.split())\n    else:\n        ids = None\n\n    blastfile, = args\n    inverse = opts.inverse\n    outfile = opts.outfile\n    fp = must_open(blastfile)\n\n    score, pctid, hitlen, evalue, noself = \\\n            opts.score, opts.pctid, opts.hitlen, opts.evalue, opts.noself\n    newblastfile = blastfile + \".P{0}L{1}\".format(int(pctid), hitlen) if \\\n                    outfile is None else outfile\n    if inverse:\n        newblastfile += \".inverse\"\n    fw = must_open(newblastfile, \"w\")\n    for row in fp:\n        if row[0] == '#':\n            continue\n        c = BlastLine(row)\n\n        if ids:\n            if c.query in ids and c.subject in ids:\n                noids = False\n            else:\n                noids = True\n        else:\n            noids = None\n\n        remove = c.score < score or \\\n            c.pctid < pctid or \\\n            c.hitlen < hitlen or \\\n            c.evalue > evalue or \\\n            noids\n\n        if inverse:\n            remove = not remove\n\n        remove = remove or (noself and c.query == c.subject)\n\n        if not remove:\n            print(row.rstrip(), file=fw)\n\n    fw.close()\n\n    return newblastfile", "response": "%prog filter test. blast"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef collect_gaps(blast, use_subject=False):\n    key = lambda x: x.sstart if use_subject else x.qstart\n    blast.sort(key=key)\n\n    for a, b in zip(blast, blast[1:]):\n        if use_subject:\n            if a.sstop < b.sstart:\n                yield b.sstart - a.sstop\n        else:\n            if a.qstop < b.qstart:\n                yield b.qstart - a.qstop", "response": "Collect the gaps between adjacent HSPs in the BLAST file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rbbh(args):\n    p = OptionParser(rbbh.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    abfile, bafile, = args\n    ab = Blast(abfile)\n    ba = Blast(bafile)\n\n    ab_hits = ab.best_hits\n    ba_hits = ba.best_hits\n\n    for aquery in ab_hits:\n        ahit = ab_hits[aquery].subject\n        ba_bline = ba_hits.get(ahit)\n        if ba_bline:\n            bhit = ba_bline.subject\n            if bhit == aquery:\n                print(\"\\t\".join(str(x) for x in (aquery, ahit)))", "response": "%prog rbbh A. ab B. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef score(args):\n    from jcvi.formats.base import SetFile\n    from jcvi.formats.fasta import Fasta\n\n    p = OptionParser(score.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    blastfile, fastafile, idsfile = args\n    ids = SetFile(idsfile)\n\n    blast = Blast(blastfile)\n    scores = defaultdict(int)\n    for b in blast:\n        query = b.query\n        subject = b.subject\n        if subject not in ids:\n            continue\n        scores[query] += b.score\n\n    logging.debug(\"A total of {0} ids loaded.\".format(len(ids)))\n\n    f = Fasta(fastafile)\n    for s in f.iterkeys_ordered():\n        sc = scores.get(s, 0)\n        print(\"\\t\".join((s, str(sc))))", "response": "%prog score blastfile query. fasta idsfile"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef annotation(args):\n    from jcvi.formats.base import DictFile\n\n    p = OptionParser(annotation.__doc__)\n    p.add_option(\"--queryids\", help=\"Query IDS file to switch [default: %default]\")\n    p.add_option(\"--subjectids\", help=\"Subject IDS file to switch [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    blastfile, = args\n\n    d = \"\\t\"\n    qids = DictFile(opts.queryids, delimiter=d) if opts.queryids else None\n    sids = DictFile(opts.subjectids, delimiter=d) if opts.subjectids else None\n    blast = Blast(blastfile)\n    for b in blast:\n        query, subject = b.query, b.subject\n        if qids:\n            query = qids[query]\n        if sids:\n            subject = sids[subject]\n        print(\"\\t\".join((query, subject)))", "response": "%prog annotation blastfile > annotations"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef top10(args):\n    from jcvi.formats.base import DictFile\n\n    p = OptionParser(top10.__doc__)\n    p.add_option(\"--top\", default=10, type=\"int\",\n                help=\"Top N taxa to extract [default: %default]\")\n    p.add_option(\"--ids\", default=None,\n                help=\"Two column ids file to query seqid [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    blastfile, = args\n    mapping = DictFile(opts.ids, delimiter=\"\\t\") if opts.ids else {}\n\n    cmd = \"cut -f2 {0}\".format(blastfile)\n    cmd += \" | sort | uniq -c | sort -k1,1nr | head -n {0}\".format(opts.top)\n    fp = popen(cmd)\n    for row in fp:\n        count, seqid = row.split()\n        nseqid = mapping.get(seqid, seqid)\n        print(\"\\t\".join((count, nseqid)))", "response": "%prog top10 blastfile.best\n\n    Count the most frequent 10 hits. Usually the BLASTFILE needs to be screened\n    the get the best match. You can also provide an .ids file to query the ids.\n    For example the ids file can contain the seqid to species mapping.\n\n    The ids file is two-column, and can sometimes be generated by\n    `jcvi.formats.fasta ids --description`."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sort(args):\n    p = OptionParser(sort.__doc__)\n    p.add_option(\"--query\", default=False, action=\"store_true\",\n            help=\"Sort by query position [default: %default]\")\n    p.add_option(\"--ref\", default=False, action=\"store_true\",\n            help=\"Sort by reference position [default: %default]\")\n    p.add_option(\"--refscore\", default=False, action=\"store_true\",\n            help=\"Sort by reference name, then score descending [default: %default]\")\n    p.add_option(\"--coords\", default=False, action=\"store_true\",\n            help=\"File is .coords generated by NUCMER [default: %default]\")\n    p.set_tmpdir()\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    blastfile, = args\n\n    if opts.coords:\n        if opts.query:\n            key = \"-k13,13 -k3,3n\"\n        elif opts.ref:\n            key = \"-k12,12 -k1,1n\"\n\n    else:\n        if opts.query:\n            key = \"-k1,1 -k7,7n\"\n        elif opts.ref:\n            key = \"-k2,2 -k9,9n\"\n        elif opts.refscore:\n            key = \"-k2,2 -k12,12gr\"\n        else:\n            key = \"-k1,1 -k12,12gr\"\n\n    cmd = \"sort\"\n    if opts.tmpdir:\n        cmd += \" -T {0}\".format(opts.tmpdir)\n    cmd += \" {0} {1} -o {1}\".format(key, blastfile)\n    sh(cmd)", "response": "%prog sort blastfile coordinatesfile"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the distance between two blast HSPs.", "response": "def get_distance(a, b, xaxis=True):\n    \"\"\"\n    Returns the distance between two blast HSPs.\n    \"\"\"\n    if xaxis:\n        arange = (\"0\", a.qstart, a.qstop, a.orientation)  # 0 is the dummy chromosome\n        brange = (\"0\", b.qstart, b.qstop, b.orientation)\n    else:\n        arange = (\"0\", a.sstart, a.sstop, a.orientation)\n        brange = (\"0\", b.sstart, b.sstop, b.orientation)\n\n    dist, oo = range_distance(arange, brange, distmode=\"ee\")\n    dist = abs(dist)\n\n    return dist"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef combine_HSPs(a):\n    m = a[0]\n    if len(a) == 1:\n        return m\n\n    for b in a[1:]:\n        assert m.query == b.query\n        assert m.subject == b.subject\n        m.hitlen += b.hitlen\n        m.nmismatch += b.nmismatch\n        m.ngaps += b.ngaps\n        m.qstart = min(m.qstart, b.qstart)\n        m.qstop = max(m.qstop, b.qstop)\n        m.sstart = min(m.sstart, b.sstart)\n        m.sstop = max(m.sstop, b.sstop)\n        if m.has_score:\n            m.score += b.score\n\n    m.pctid = 100 - (m.nmismatch + m.ngaps) * 100. / m.hitlen\n    return m", "response": "Combine HSPs into a single BlastLine."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a list of BlastLines or a BlastSlow instance returns a list of HSPs.", "response": "def chain_HSPs(blast, xdist=100, ydist=100):\n    \"\"\"\n    Take a list of BlastLines (or a BlastSlow instance), and returns a list of\n    BlastLines.\n    \"\"\"\n    key = lambda x: (x.query, x.subject)\n    blast.sort(key=key)\n\n    clusters = Grouper()\n    for qs, points in groupby(blast, key=key):\n        points = sorted(list(points), \\\n                key=lambda x: (x.qstart, x.qstop, x.sstart, x.sstop))\n\n        n = len(points)\n        for i in xrange(n):\n            a = points[i]\n            clusters.join(a)\n            for j in xrange(i + 1, n):\n                b = points[j]\n\n                # x-axis distance\n                del_x = get_distance(a, b)\n                if del_x > xdist:\n                    break\n                # y-axis distance\n                del_y = get_distance(a, b, xaxis=False)\n                if del_y > ydist:\n                    continue\n                # otherwise join\n                clusters.join(a, b)\n\n    chained_hsps = [combine_HSPs(x) for x in clusters]\n    key = lambda x: (x.query, -x.score if x.has_score else 0)\n    chained_hsps = sorted(chained_hsps, key=key)\n\n    return chained_hsps"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef condense(args):\n    p = OptionParser(condense.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    blastfile, = args\n    blast = BlastSlow(blastfile)\n    key = lambda x: x.query\n    blast.sort(key=key)\n\n    clusters = []\n    for q, lines in groupby(blast, key=key):\n        lines = list(lines)\n        condenser = defaultdict(list)\n\n        for b in lines:\n            condenser[(b.subject, b.orientation)].append(b)\n\n        for bs in condenser.values():\n            clusters.append(bs)\n\n    chained_hsps = [combine_HSPs(x) for x in clusters]\n    chained_hsps = sorted(chained_hsps, key=lambda x: (x.query, -x.score))\n    for b in chained_hsps:\n        print(b)", "response": "%prog condense blastfile > blastfile. condensed\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mismatches(args):\n    from jcvi.utils.cbook import percentage\n    from jcvi.graphics.histogram import stem_leaf_plot\n\n    p = OptionParser(mismatches.__doc__)\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    blastfile, = args\n\n    data = []\n    matches = 0\n    b = Blast(blastfile)\n    for query, bline in b.iter_best_hit():\n        mm = bline.nmismatch + bline.ngaps\n        data.append(mm)\n\n    nonzeros = [x for x in data if x != 0]\n    title = \"Polymorphic sites: {0}\".\\\n            format(percentage(len(nonzeros), len(data)))\n    stem_leaf_plot(data, 0, 20, 20, title=title)", "response": "%prog mismatches blastfile\n\n    Print out histogram of mismatches of HSPs, usually for evaluating SNP level."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef swap(args):\n    p = OptionParser(swap.__doc__)\n\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    blastfile, = args\n    swappedblastfile = blastfile + \".swapped\"\n    fp = must_open(blastfile)\n    fw = must_open(swappedblastfile, \"w\")\n    for row in fp:\n        b = BlastLine(row)\n        print(b.swapped, file=fw)\n\n    fw.close()\n    sort([swappedblastfile])", "response": "%prog swap blastfile\n\n    Print out a new blast file with query and subject swapped."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pairs(args):\n    import jcvi.formats.bed\n\n    p = OptionParser(pairs.__doc__)\n    p.set_pairs()\n    opts, targs = p.parse_args(args)\n\n    if len(targs) != 1:\n        sys.exit(not p.print_help())\n\n    blastfile, = targs\n    bedfile = bed([blastfile])\n    args[args.index(blastfile)] = bedfile\n\n    return jcvi.formats.bed.pairs(args)", "response": "See __doc__ for OptionParser.set_pairs()."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef best(args):\n    p = OptionParser(best.__doc__)\n\n    p.add_option(\"-n\", default=1, type=\"int\",\n            help=\"get best N hits [default: %default]\")\n    p.add_option(\"--nosort\", default=False, action=\"store_true\",\n            help=\"assume BLAST is already sorted [default: %default]\")\n    p.add_option(\"--hsps\", default=False, action=\"store_true\",\n            help=\"get all HSPs for the best pair [default: %default]\")\n    p.add_option(\"--subject\", default=False, action=\"store_true\",\n            help=\"get best hit(s) for subject genome instead [default: %default]\")\n    p.set_tmpdir()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    blastfile, = args\n    n = opts.n\n    hsps = opts.hsps\n    tmpdir = opts.tmpdir\n    ref = \"query\" if not opts.subject else \"subject\"\n\n    if not opts.nosort:\n        sargs = [blastfile]\n        if tmpdir:\n            sargs += [\"-T {0}\".format(tmpdir)]\n        if ref != \"query\":\n            sargs += [\"--refscore\"]\n        sort(sargs)\n    else:\n        logging.debug(\"Assuming sorted BLAST\")\n\n    if not opts.subject:\n        bestblastfile = blastfile + \".best\"\n    else:\n        bestblastfile = blastfile + \".subject.best\"\n    fw = open(bestblastfile, \"w\")\n\n    b = Blast(blastfile)\n    for q, bline in b.iter_best_hit(N=n, hsps=hsps, ref=ref):\n        print(bline, file=fw)\n\n    return bestblastfile", "response": "%prog best blastfile\n\n    print the best hit for each query in the blastfile"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ancestral(args):\n    p = OptionParser(ancestral.__doc__)\n    opts, args, iopts = p.set_image_options(args, figsize=\"8x7\")\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    regionsfile, sizesfile = args\n    regions = RegionsFile(regionsfile)\n    sizes = Sizes(sizesfile).mapping\n    sizes = dict((k, v) for (k, v) in sizes.iteritems() if k[:2] == \"LG\")\n    maxsize = max(sizes.values())\n    ratio = .5 / maxsize\n\n    fig = plt.figure(1, (iopts.w, iopts.h))\n    root = fig.add_axes((0, 0, 1, 1))\n\n    from jcvi.graphics.base import set2\n    a, b, c, d, e, f, g = set2[:7]\n    set2 = (c, g, b, e, d, a, f)\n\n    # Upper panel is the evolution of segments\n    # All segments belong to one of seven karyotypes 1 to 7\n    karyotypes = regions.karyotypes\n    xgap = 1. / (1 + len(karyotypes))\n    ygap = .05\n    mgap = xgap / 4.5\n    gwidth = mgap * .75\n    tip = .02\n    coords = {}\n    for i, k in enumerate(regions.karyotypes):\n        x = (i + 1) * xgap\n        y = .9\n        root.text(x, y + tip, \"Anc\" + k, ha=\"center\")\n        root.plot((x, x), (y, y - ygap), \"k-\", lw=2)\n        y -= 2 * ygap\n        coords['a'] = (x - 1.5 * mgap , y)\n        coords['b'] = (x - .5 * mgap , y)\n        coords['c'] = (x + .5 * mgap , y)\n        coords['d'] = (x + 1.5 * mgap , y)\n        coords['ab'] = join_nodes_vertical(root, coords, 'a', 'b', y + ygap / 2)\n        coords['cd'] = join_nodes_vertical(root, coords, 'c', 'd', y + ygap / 2)\n        coords['abcd'] = join_nodes_vertical(root, coords, 'ab', 'cd', y + ygap)\n        for n in 'abcd':\n            nx, ny = coords[n]\n            root.text(nx, ny - tip, n, ha=\"center\")\n            coords[n] = (nx, ny - ygap / 2)\n\n        kdata = regions.get_karyotype(k)\n        for kd in kdata:\n            g = kd.group\n            gx, gy = coords[g]\n            gsize = ratio * kd.span\n            gy -= gsize\n            p = Rectangle((gx - gwidth / 2, gy),\n                           gwidth, gsize, lw=0, color=set2[i])\n            root.add_patch(p)\n            root.text(gx, gy + gsize / 2, kd.chromosome,\n                      ha=\"center\", va=\"center\", color='w')\n            coords[g] = (gx, gy - tip)\n\n    # Bottom panel shows the location of segments on chromosomes\n    # TODO: redundant code, similar to graphics.chromosome\n    ystart = .54\n    chr_number = len(sizes)\n    xstart, xend = xgap - 2 * mgap, 1 - xgap + 2 * mgap\n    xinterval = (xend - xstart - gwidth) / (chr_number - 1)\n    chrpos = {}\n    for a, (chr, clen) in enumerate(sorted(sizes.items())):\n        chr = get_number(chr)\n        xx = xstart + a * xinterval + gwidth / 2\n        chrpos[chr] = xx\n        root.text(xx, ystart + .01, chr, ha=\"center\")\n        Chromosome(root, xx, ystart, ystart - clen * ratio, width=gwidth)\n\n    # Start painting\n    for r in regions:\n        xx = chrpos[r.chromosome]\n        yystart = ystart - r.start * ratio\n        yyend = ystart - r.end * ratio\n        p = Rectangle((xx - gwidth / 2, yystart), gwidth, yyend - yystart,\n                      color=set2[int(r.karyotype) - 1], lw=0)\n        root.add_patch(p)\n\n    root.set_xlim(0, 1)\n    root.set_ylim(0, 1)\n    root.set_axis_off()\n\n    pf = \"pineapple-karyotype\"\n    image_name = pf + \".\" + iopts.format\n    savefig(image_name, dpi=iopts.dpi, iopts=iopts)", "response": "%prog ancestral regions. fasta sizes. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a list of edges record per node incoming and outgoing edges", "response": "def node_to_edge(edges, directed=True):\n    \"\"\"\n    From list of edges, record per node, incoming and outgoing edges\n    \"\"\"\n    outgoing = defaultdict(set)\n    incoming = defaultdict(set) if directed else outgoing\n    nodes = set()\n    for i, edge in enumerate(edges):\n        a, b, = edge[:2]\n        outgoing[a].add(i)\n        incoming[b].add(i)\n        nodes.add(a)\n        nodes.add(b)\n    nodes = sorted(nodes)\n    if directed:\n        return outgoing, incoming, nodes\n    return outgoing, nodes"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a list of edges into a Hamiltonian Path.", "response": "def hamiltonian(edges, directed=False, precision=0):\n    \"\"\"\n    Calculates shortest path that traverses each node exactly once. Convert\n    Hamiltonian path problem to TSP by adding one dummy point that has a distance\n    of zero to all your other points. Solve the TSP and get rid of the dummy\n    point - what remains is the Hamiltonian Path.\n\n    >>> g = [(1,2), (2,3), (3,4), (4,2), (3,5)]\n    >>> hamiltonian(g)\n    [1, 2, 4, 3, 5]\n    >>> hamiltonian([(1, 2), (2, 3)], directed=True)\n    [1, 2, 3]\n    \"\"\"\n    edges = populate_edge_weights(edges)\n    incident, nodes = node_to_edge(edges, directed=False)\n    DUMMY = \"DUMMY\"\n    dummy_edges = edges + [(DUMMY, x, 0) for x in nodes]\n    if directed:\n        dummy_edges += [(x, DUMMY, 0) for x in nodes]\n        dummy_edges = reformulate_atsp_as_tsp(dummy_edges)\n\n    tour = tsp(dummy_edges, precision=precision)\n\n    dummy_index = tour.index(DUMMY)\n    tour = tour[dummy_index:] + tour[:dummy_index]\n    if directed:\n        dummy_star_index = tour.index((DUMMY, '*'))\n        assert dummy_star_index in (1, len(tour) - 1), tour\n        if dummy_star_index == len(tour) - 1:  # need to flip\n            tour = tour[1:] + tour[:1]\n            tour = tour[::-1]\n        path = tour[1:]\n        path = [x for x in path if not isinstance(x, tuple)]\n    else:\n        path = tour[1:]\n\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreformulate the ATSP as a TSP.", "response": "def reformulate_atsp_as_tsp(edges):\n    \"\"\"\n    To reformulate the ATSP as a TSP, for each city a dummy city (e.g, for New\n    York, a dummy city New York* is added. Between each city and its\n    corresponding dummy city a negative or very small distance with value cheap\n    is used. This makes sure that each cities always occurs in the solution\n    together with its dummy city. The original distances are used between the\n    cities and the dummy cities, where each city is responsible for the distance\n    going to the city and the dummy city is responsible for the distance coming\n    from the city. The distances between all cities and the distances between\n    all dummy cities are set to infeasible.\n    \"\"\"\n    incident, nodes = node_to_edge(edges, directed=False)\n    new_edges = []\n    for a, b, w in edges:\n        new_edges.append(((a, '*'), b, w))\n    for n in nodes:\n        new_edges.append((n, (n, '*'), NEG_INF))  # A negative weight\n    return new_edges"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef print_to_tsplib(self, edges, tspfile, precision=0):\n        fw = must_open(tspfile, \"w\")\n        incident, nodes = node_to_edge(edges, directed=False)\n        self.nodes = nodes\n        nodes_indices = dict((n, i) for i, n in enumerate(nodes))\n        self.nnodes = nnodes = len(nodes)\n\n        # TSPLIB requires explicit weights to be integral, and non-negative\n        weights = [x[-1] for x in edges]\n        max_x, min_x = max(weights), min(weights)\n        inf = 2 * max(abs(max_x), abs(min_x))\n        factor = 10 ** precision\n        logging.debug(\"TSP rescale: max_x={0}, min_x={1}, inf={2}, factor={3}\".\\\n                        format(max_x, min_x, inf, factor))\n\n        print(\"NAME: data\", file=fw)\n        print(\"TYPE: TSP\", file=fw)\n        print(\"DIMENSION: {0}\".format(nnodes), file=fw)\n\n        D = np.ones((nnodes, nnodes), dtype=float) * inf\n        for a, b, w in edges:\n            ia, ib = nodes_indices[a], nodes_indices[b]\n            D[ia, ib] = D[ib, ia] = w\n        D = (D - min_x) * factor\n        D = D.astype(int)\n\n        print(\"EDGE_WEIGHT_TYPE: EXPLICIT\", file=fw)\n        print(\"EDGE_WEIGHT_FORMAT: FULL_MATRIX\", file=fw)\n        print(\"EDGE_WEIGHT_SECTION\", file=fw)\n        for row in D:  # Dump the full matrix\n            print(\" \" + \" \".join(str(x) for x in row), file=fw)\n\n        print(\"EOF\", file=fw)\n        fw.close()\n        logging.debug(\"Write TSP instance to `{0}`\".format(tspfile))", "response": "Print the TSP file to the terminal."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef estimategaps(args):\n    p = OptionParser(estimategaps.__doc__)\n    opts, args, iopts = p.set_image_options(args, figsize=\"6x6\", dpi=300)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    pf, seqid, mlg = args\n    bedfile = pf + \".lifted.bed\"\n    agpfile = pf + \".agp\"\n\n    function = lambda x: x.cm\n    cc = Map(bedfile, scaffold_info=True, function=function)\n    agp = AGP(agpfile)\n\n    g = GapEstimator(cc, agp, seqid, mlg, function=function)\n    pp, chrsize, mlgsize = g.pp, g.chrsize, g.mlgsize\n    spl, spld = g.spl, g.spld\n    g.compute_all_gaps(verbose=False)\n\n    fig = plt.figure(1, (iopts.w, iopts.h))\n    root = fig.add_axes([0, 0, 1, 1])\n\n    # Panel A\n    xstart, ystart = .15, .65\n    w, h = .7, .3\n    t = np.linspace(0, chrsize, 1000)\n    ax = fig.add_axes([xstart, ystart, w, h])\n    mx, my = zip(*g.scatter_data)\n    rho = spearmanr(mx, my)\n\n    dsg = \"g\"\n    ax.vlines(pp, 0, mlgsize, colors=\"beige\")\n    ax.plot(mx, my, \".\", color=set2[3])\n    ax.plot(t, spl(t), \"-\", color=dsg)\n    ax.text(.05, .95, mlg, va=\"top\", transform=ax.transAxes)\n    normalize_lms_axis(ax, xlim=chrsize, ylim=mlgsize,\n                       ylabel=\"Genetic distance (cM)\")\n    if rho < 0:\n        ax.invert_yaxis()\n\n    # Panel B\n    ystart -= .28\n    h = .25\n    ax = fig.add_axes([xstart, ystart, w, h])\n    ax.vlines(pp, 0, mlgsize, colors=\"beige\")\n    ax.plot(t, spld(t), \"-\", lw=2, color=dsg)\n    ax.plot(pp, spld(pp), \"o\", mfc=\"w\", mec=dsg, ms=5)\n    normalize_lms_axis(ax, xlim=chrsize, ylim=25 * 1e-6,\n                       xfactor=1e-6, xlabel=\"Physical position (Mb)\",\n                       yfactor=1000000, ylabel=\"Recomb. rate\\n(cM / Mb)\")\n    ax.xaxis.grid(False)\n\n    # Panel C (specific to JMMale-1)\n    a, b = \"scaffold_1076\", \"scaffold_861\"\n    sizes = dict((x.component_id, (x.object_beg, x.object_end,\n                                   x.component_span, x.orientation)) \\\n                                   for x in g.agp if not x.is_gap)\n    a_beg, a_end, asize, ao = sizes[a]\n    b_beg, b_end, bsize, bo = sizes[b]\n    gapsize = g.get_gapsize(a)\n    total_size = asize + gapsize + bsize\n    ratio = .6 / total_size\n    y = .16\n    pad = .03\n    pb_ratio = w / chrsize\n\n    # Zoom\n    lsg = \"lightslategray\"\n    root.plot((.15 + pb_ratio * a_beg, .2),\n              (ystart, ystart - .14), \":\", color=lsg)\n    root.plot((.15 + pb_ratio * b_end, .3),\n              (ystart, ystart - .08), \":\", color=lsg)\n    ends = []\n    for tag, size, marker, beg in zip((a, b), (asize, bsize), (49213, 81277),\n                              (.2, .2 + (asize + gapsize) * ratio)):\n        end = beg + size * ratio\n        marker = beg + marker * ratio\n        ends.append((beg, end, marker))\n        root.plot((marker,), (y,), \"o\", color=lsg)\n        root.text((beg + end) / 2, y + pad, latex(tag),\n                  ha=\"center\", va=\"center\")\n        HorizontalChromosome(root, beg, end, y, height=.025, fc='gainsboro')\n\n    begs, ends, markers = zip(*ends)\n    fontprop = dict(color=lsg, ha=\"center\", va=\"center\")\n    ypos = y + pad * 2\n    root.plot(markers, (ypos, ypos), \"-\", lw=2, color=lsg)\n    root.text(sum(markers) / 2, ypos + pad,\n              \"Distance: 1.29cM $\\Leftrightarrow$ 211,824bp (6.1 cM/Mb)\", **fontprop)\n\n    ypos = y - pad\n    xx = markers[0], ends[0]\n    root.plot(xx, (ypos, ypos), \"-\", lw=2, color=lsg)\n    root.text(sum(xx) / 2, ypos - pad, \"34,115bp\", **fontprop)\n    xx = markers[1], begs[1]\n    root.plot(xx, (ypos, ypos), \"-\", lw=2, color=lsg)\n    root.text(sum(xx) / 2, ypos - pad, \"81,276bp\", **fontprop)\n\n    root.plot((ends[0], begs[1]), (y, y), \":\", lw=2, color=lsg)\n    root.text(sum(markers) / 2, ypos - 3 * pad, r\"$\\textit{Estimated gap size: 96,433bp}$\",\n                                  color=\"r\", ha=\"center\", va=\"center\")\n\n    labels = ((.05, .95, 'A'), (.05, .6, 'B'), (.05, .27, 'C'))\n    panel_labels(root, labels)\n    normalize_axes(root)\n\n    pf = \"estimategaps\"\n    image_name = pf + \".\" + iopts.format\n    savefig(image_name, dpi=iopts.dpi, iopts=iopts)", "response": "%prog estimategaps JM - 4 chr1 JMMale - 1"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef simulation(args):\n    p = OptionParser(simulation.__doc__)\n    opts, args, iopts = p.set_image_options(args, dpi=300)\n\n    if len(args) != 4:\n        sys.exit(not p.print_help())\n\n    dataA, dataB, dataC, dataD = args\n    fig = plt.figure(1, (iopts.w, iopts.h))\n    root = fig.add_axes([0, 0, 1, 1])\n    A = fig.add_axes([.12, .62, .35, .35])\n    B = fig.add_axes([.62, .62, .35, .35])\n    C = fig.add_axes([.12, .12, .35, .35])\n    D = fig.add_axes([.62, .12, .35, .35])\n    dataA = import_data(dataA)\n    dataB = import_data(dataB)\n    dataC = import_data(dataC)\n    dataD = import_data(dataD)\n    subplot(A, dataA, \"Inversion error rate\", \"Accuracy\", xlim=.5)\n    subplot(B, dataB, \"Translocation error rate\", \"Accuracy\", xlim=.5,\n                      legend=(\"intra-chromosomal\", \"inter-chromosomal\",\n                              \"75\\% intra + 25\\% inter\"))\n    subplot(C, dataC, \"Number of input maps\", \"Accuracy\", xcast=int)\n    subplot(D, dataD, \"Number of input maps\", \"Accuracy\", xcast=int)\n\n    labels = ((.03, .97, \"A\"), (.53, .97, \"B\"),\n              (.03, .47, \"C\"), (.53, .47, \"D\"))\n    panel_labels(root, labels)\n\n    normalize_axes(root)\n    image_name = \"simulation.\" + iopts.format\n    savefig(image_name, dpi=iopts.dpi, iopts=iopts)", "response": "%prog simulation inversion. txt translocation. txt maps. txt multimaps. txt"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef digest(args):\n    p = OptionParser(digest.__doc__)\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    fastafile, enzymes = args\n    enzymes = enzymes.split(\",\")\n    enzymes = [x for x in AllEnzymes if str(x) in enzymes]\n    f = Fasta(fastafile, lazy=True)\n    fw = must_open(opts.outfile, \"w\")\n\n    header = [\"Contig\", \"Length\"] + [str(x) for x in enzymes]\n    print(\"\\t\".join(header), file=fw)\n    for name, rec in f.iteritems_ordered():\n        row = [name, len(rec)]\n        for e in enzymes:\n            pos = e.search(rec.seq)\n            pos = \"na\" if not pos else \"|\".join(str(x) for x in pos)\n            row.append(pos)\n        print(\"\\t\".join(str(x) for x in row), file=fw)", "response": "%prog digest fastafile NspI BfuCI. seq"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extract_full(rec, sites, flank, fw):\n    for s in sites:\n        newid = \"{0}:{1}\".format(rec.name, s)\n        left = max(s - flank, 0)\n        right = min(s + flank, len(rec))\n        frag = rec.seq[left:right].strip(\"Nn\")\n        newrec = SeqRecord(frag, id=newid, description=\"\")\n        SeqIO.write([newrec], fw, \"fasta\")", "response": "Full extraction of seq flanking the sites."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting ends of fragments above certain size.", "response": "def extract_ends(rec, sites, flank, fw, maxfragsize=800):\n    \"\"\"\n    Extraction of ends of fragments above certain size.\n    \"\"\"\n    nsites = len(sites)\n    size = len(rec)\n    for i, s in enumerate(sites):\n        newid = \"{0}:{1}\".format(rec.name, s)\n        recs = []\n\n        if i == 0 or s - sites[i - 1] <= maxfragsize:\n            newidL = newid + \"L\"\n            left = max(s - flank, 0)\n            right = s\n            frag = rec.seq[left:right].strip(\"Nn\")\n            recL = SeqRecord(frag, id=newidL, description=\"\")\n            if i == 0 and s > maxfragsize:  # Contig L-end\n                pass\n            else:\n                recs.append(recL)\n\n        if i == nsites - 1 or sites[i + 1] - s <= maxfragsize:\n            newidR = newid + \"R\"\n            left = s\n            right = min(s + flank, size)\n            frag = rec.seq[left:right].strip(\"Nn\")\n            recR = SeqRecord(frag, id=newidR, description=\"\")\n            if i == nsites - 1 and size - s > maxfragsize:  # Contig R-end\n                pass\n            else:\n                recs.append(recR)\n\n        SeqIO.write(recs, fw, \"fasta\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive an array of contig sizes produce an array of contig sizes a50 N50 and L50 values", "response": "def calculate_A50(ctgsizes, cutoff=0, percent=50):\n    \"\"\"\n    Given an array of contig sizes, produce A50, N50, and L50 values\n    \"\"\"\n\n    ctgsizes = np.array(ctgsizes, dtype=\"int\")\n    ctgsizes = np.sort(ctgsizes)[::-1]\n    ctgsizes = ctgsizes[ctgsizes >= cutoff]\n\n    a50 = np.cumsum(ctgsizes)\n\n    total = np.sum(ctgsizes)\n    idx = bisect(a50, total * percent / 100.)\n    l50 = ctgsizes[idx]\n    n50 = idx + 1\n\n    return a50, l50, n50"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Astat(delta, k, G, n):\n    return n * delta * 1. / G - k * ln2", "response": "calculate Astat of contigs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef n50(args):\n    from jcvi.graphics.histogram import loghistogram\n\n    p = OptionParser(n50.__doc__)\n    p.add_option(\"--print0\", default=False, action=\"store_true\",\n                 help=\"Print size and L50 to stdout [default: %default]\")\n\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    ctgsizes = []\n\n    # Guess file format\n    probe = open(args[0]).readline()[0]\n    isFasta = (probe == '>')\n    if isFasta:\n        for filename in args:\n            f = Fasta(filename)\n            ctgsizes += list(b for a, b in f.itersizes())\n\n    else:\n        for row in must_open(args):\n            try:\n                ctgsize = int(float(row.split()[-1]))\n            except ValueError:\n                continue\n            ctgsizes.append(ctgsize)\n\n    a50, l50, nn50 = calculate_A50(ctgsizes)\n    sumsize = sum(ctgsizes)\n    minsize = min(ctgsizes)\n    maxsize = max(ctgsizes)\n    n = len(ctgsizes)\n    print(\", \".join(args), file=sys.stderr)\n\n    summary = (sumsize, l50, nn50, minsize, maxsize, n)\n    print(\" \".join(\"{0}={1}\".format(a, b) for a, b in \\\n                        zip(header, summary)), file=sys.stderr)\n    loghistogram(ctgsizes)\n\n    if opts.print0:\n        print(\"\\t\".join(str(x) for x in (\",\".join(args), sumsize, l50)))\n\n    return zip(header, summary)", "response": "%prog n50 filename\n\n    Given a file with a list of numbers denoting contig lengths, calculate N50.\n    Input file can be both FASTA or a list of sizes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bed(args):\n    from collections import defaultdict\n    from jcvi.compara.synteny import AnchorFile, check_beds\n    from jcvi.formats.bed import Bed\n    from jcvi.formats.base import get_number\n\n    p = OptionParser(bed.__doc__)\n    p.add_option(\"--switch\", default=False, action=\"store_true\",\n                 help=\"Switch reference and aligned map elements\")\n    p.add_option(\"--scale\", type=\"float\",\n                 help=\"Scale the aligned map distance by factor\")\n    p.set_beds()\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    anchorsfile, = args\n    switch = opts.switch\n    scale = opts.scale\n    ac = AnchorFile(anchorsfile)\n    pairs = defaultdict(list)\n    for a, b, block_id in ac.iter_pairs():\n        pairs[a].append(b)\n\n    qbed, sbed, qorder, sorder, is_self = check_beds(anchorsfile, p, opts)\n    bd = Bed()\n    for q in qbed:\n        qseqid, qstart, qend, qaccn = q.seqid, q.start, q.end, q.accn\n        if qaccn not in pairs:\n            continue\n        for s in pairs[qaccn]:\n            si, s = sorder[s]\n            sseqid, sstart, send, saccn = s.seqid, s.start, s.end, s.accn\n        if switch:\n            qseqid, sseqid = sseqid, qseqid\n            qstart, sstart = sstart, qstart\n            qend, send = send, qend\n            qaccn, saccn = saccn, qaccn\n        if scale:\n            sstart /= scale\n        try:\n            newsseqid = get_number(sseqid)\n        except ValueError:\n            raise ValueError(\"`{0}` is on `{1}` with no number to extract\".\\\n                                format(saccn, sseqid))\n        bedline = \"\\t\".join(str(x) for x in (qseqid, qstart - 1, qend,\n                            \"{0}:{1}\".format(newsseqid, sstart)))\n        bd.add(bedline)\n\n    bd.print_to_file(filename=opts.outfile, sorted=True)", "response": "%prog bed anchorsfile\n\n    Convert ANCHORS file to BED format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef happy_edges(row, prefix=None):\n    trans = maketrans(\"[](){}\", \"      \")\n    row = row.strip().strip(\"+\")\n    row = row.translate(trans)\n    scfs = [x.strip(\"+\") for x in row.split(\":\")]\n    for a, b in pairwise(scfs):\n        oa = '<' if a.strip()[0] == '-' else '>'\n        ob = '<' if b.strip()[0] == '-' else '>'\n\n        is_uncertain = a[-1] == ' ' or b[0] == ' '\n\n        a = a.strip().strip('-')\n        b = b.strip().strip('-')\n\n        if prefix:\n            a = prefix + a\n            b = prefix + b\n\n        yield (a, b, oa, ob), is_uncertain", "response": "Convert a row in HAPPY file and yield edges."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef partition(args):\n    allowed_format = (\"png\", \"ps\")\n    p = OptionParser(partition.__doc__)\n    p.add_option(\"--prefix\", help=\"Add prefix to the name [default: %default]\")\n    p.add_option(\"--namestart\", default=0, type=\"int\",\n                 help=\"Use a shorter name, starting index [default: %default]\")\n    p.add_option(\"--format\", default=\"png\", choices=allowed_format,\n            help=\"Generate image of format [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    happyfile, graphfile = args\n    bg = BiGraph()\n    bg.read(graphfile, color=\"red\")\n    prefix = opts.prefix\n    fp = open(happyfile)\n    for i, row in enumerate(fp):\n        nns = happy_nodes(row, prefix=prefix)\n        nodes = set(nns)\n        edges = happy_edges(row, prefix=prefix)\n\n        small_graph = BiGraph()\n        for (a, b, oa, ob), is_uncertain in edges:\n            color = \"gray\" if is_uncertain else \"black\"\n            small_graph.add_edge(a, b, oa, ob, color=color)\n\n        for (u, v), e in bg.edges.items():\n            # Grab edge if both vertices are on the same line\n            if u in nodes and v in nodes:\n                uv = (str(u), str(v))\n                if uv in small_graph.edges:\n                    e = small_graph.edges[uv]\n                    e.color = \"blue\"  # supported by both evidences\n                else:\n                    small_graph.add_edge(e)\n\n        print(small_graph, file=sys.stderr)\n\n        pngfile = \"A{0:02d}.{1}\".format(i + 1, opts.format)\n        telomeres = (nns[0], nns[-1])\n        small_graph.draw(pngfile, namestart=opts.namestart,\n                         nodehighlight=telomeres, dpi=72)\n\n    legend = [\"Edge colors:\"]\n    legend.append(\"[BLUE] Experimental + Synteny\")\n    legend.append(\"[BLACK] Experimental certain\")\n    legend.append(\"[GRAY] Experimental uncertain\")\n    legend.append(\"[RED] Synteny only\")\n    legend.append(\"Rectangle nodes are telomeres.\")\n    print(\"\\n\".join(legend), file=sys.stderr)", "response": "%prog partition happy. txt synteny. graph"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef happy(args):\n    p = OptionParser(happy.__doc__)\n    p.add_option(\"--prefix\", help=\"Add prefix to the name [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    happyfile, = args\n\n    certain = \"certain.graph\"\n    uncertain = \"uncertain.graph\"\n    fw1 = open(certain, \"w\")\n    fw2 = open(uncertain, \"w\")\n\n    fp = open(happyfile)\n    for row in fp:\n        for e, is_uncertain in happy_edges(row, prefix=opts.prefix):\n            fw = fw2 if is_uncertain else fw1\n            print(e, file=fw)\n\n    logging.debug(\"Edges written to `{0}`\".format(\",\".join((certain, uncertain))))", "response": "%prog happy happy. txt"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef venn(args):\n    from matplotlib_venn import venn2\n\n    p = OptionParser(venn.__doc__)\n    opts, args, iopts = p.set_image_options(args, figsize=\"9x9\")\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    bcs = args\n    fig = plt.figure(1, (iopts.w, iopts.h))\n    root = fig.add_axes([0, 0, 1, 1])\n\n    pad = .02\n    ystart = 1\n    ywidth = 1. / len(bcs)\n    tags = (\"Bowers\", \"YGOB\", \"Schnable\")\n    for bc, tag in zip(bcs, tags):\n        fp = open(bc)\n        data = []\n        for row in fp:\n            prog, pcounts, tcounts, shared = row.split()\n            pcounts = int(pcounts)\n            tcounts = int(tcounts)\n            shared = int(shared)\n            data.append((prog, pcounts, tcounts, shared))\n        xstart = 0\n        xwidth = 1. / len(data)\n        for prog, pcounts, tcounts, shared in data:\n            a, b, c = pcounts - shared, tcounts - shared, shared\n            ax = fig.add_axes([xstart + pad, ystart - ywidth + pad,\n                               xwidth - 2 * pad, ywidth - 2 * pad])\n            venn2(subsets=(a, b, c), set_labels=(prog, tag), ax=ax)\n            message = \"Sn={0} Pu={1}\".\\\n                format(percentage(shared, tcounts, precision=0, mode=-1),\n                       percentage(shared, pcounts, precision=0, mode=-1))\n            print(message, file=sys.stderr)\n            ax.text(.5, .92, latex(message), ha=\"center\", va=\"center\",\n                    transform=ax.transAxes, color='b')\n            ax.set_axis_off()\n            xstart += xwidth\n        ystart -= ywidth\n\n    panel_labels(root, ((.04, .96, \"A\"), (.04, .96 - ywidth, \"B\"),\n                  (.04, .96 - 2 * ywidth, \"C\")))\n    panel_labels(root, ((.5, .98, \"A. thaliana duplicates\"),\n                        (.5, .98 - ywidth, \"14 Yeast genomes\"),\n                        (.5, .98 - 2 * ywidth, \"4 Grass genomes\")))\n    normalize_axes(root)\n    savefig(\"venn.pdf\", dpi=opts.dpi)", "response": "%prog venn *.benchmark\n\n    Display benchmark results as Venn diagram."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef grass(args):\n    p = OptionParser(grass._doc__)\n    p.set_verbose()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    master, james = args\n\n    fp = open(master)\n    next(fp)\n    master_store = defaultdict(set)\n    for row in fp:\n        atoms = row.split()\n        s = set()\n        for x in atoms[1:6]:\n            m = x.split(\",\")\n            s |= set(m)\n        if '-' in s:\n            s.remove('-')\n\n        a = atoms[1]\n        master_store[a] |= set(s)\n\n    fp = open(james)\n    next(fp)\n    james_store = {}\n    tandems = set()\n    for row in fp:\n        atoms = row.split()\n        s = set()\n        Os = set()\n        for x in atoms[:-1]:\n            m = x.split(\"||\")\n            if m[0].startswith(\"Os\"):\n                Os |= set(m)\n            if m[0].startswith(\"http\"):\n                continue\n            if m[0].startswith(\"chr\"):\n                m = [\"proxy\"]\n            if \"||\" in x:\n                tandems |= set(m)\n            s |= set(m)\n\n        for x in Os:\n            james_store[x] = s\n\n    jaccards = []\n    corr_jaccards = []\n    perfect_matches = 0\n    corr_perfect_matches = 0\n    for k, v in james_store.items():\n        if k not in master_store:\n            continue\n        m = master_store[k]\n        jaccard = len(v & m) * 100 / len(v | m)\n        jaccards.append(jaccard)\n        diff = (v ^ m ) - tandems\n        corr_jaccard = 100 - len(diff) * 100 / len(v | m)\n        corr_jaccards.append(corr_jaccard)\n        if opts.verbose:\n            print(k)\n            print(v)\n            print(m)\n            print(diff)\n            print(jaccard)\n        if jaccard > 99:\n            perfect_matches += 1\n        if corr_jaccard > 99:\n            corr_perfect_matches += 1\n\n    logging.debug(\"Perfect matches: {0}\".format(perfect_matches))\n    logging.debug(\"Perfect matches (corrected): {0}\".format(corr_perfect_matches))\n    print(\"Jaccards:\", SummaryStats(jaccards))\n    print(\"Corrected Jaccards:\", SummaryStats(corr_jaccards))", "response": "%prog grass master. txt james. txt"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cartoon(args):\n    p = OptionParser(cartoon.__doc__)\n    opts, args, iopts = p.set_image_options(args, figsize=\"10x7\")\n\n    fig = plt.figure(1, (iopts.w, iopts.h))\n    root = fig.add_axes([0, 0, 1, 1])\n\n    # Panel A\n    A = CartoonRegion(41)\n    A.draw(root, .35, .85, strip=False, color=False)\n    x1, x2 = A.x1, A.x2\n    lsg = \"lightslategray\"\n    pad = .01\n    xc, yc = .35, .88\n    arrowlen = x2 - xc - pad\n    arrowprops = dict(length_includes_head=True, width=.01, fc=lsg, lw=0,\n                      head_length=arrowlen * .15, head_width=.03)\n    p = FancyArrow(xc - pad, yc, -arrowlen, 0, shape=\"left\", **arrowprops)\n    root.add_patch(p)\n    p = FancyArrow(xc + pad, yc, arrowlen, 0, shape=\"right\", **arrowprops)\n    root.add_patch(p)\n\n    yt = yc + 4 * pad\n    root.text((x1 + xc) / 2, yt, \"20 genes upstream\", ha=\"center\")\n    root.text((x2 + xc) / 2, yt, \"20 genes downstream\", ha=\"center\")\n    root.plot((xc,), (yc,), \"o\", mfc='w', mec=lsg, mew=2, lw=2, color=lsg)\n    root.text(xc, yt, \"Query gene\", ha=\"center\")\n\n    # Panel B\n    A.draw(root, .35, .7, strip=False)\n\n    RoundRect(root, (.07, .49), .56, .14, fc='y', alpha=.2)\n    a = deepcopy(A)\n    a.evolve(mode='S', target=10)\n    a.draw(root, .35, .6)\n    b = deepcopy(A)\n    b.evolve(mode='F', target=8)\n    b.draw(root, .35, .56)\n    c = deepcopy(A)\n    c.evolve(mode='G', target=6)\n    c.draw(root, .35, .52)\n\n    for x in (a, b, c):\n        root.text(.64, x.y, \"Score={0}\".format(x.nonwhites), va=\"center\")\n\n    # Panel C\n    A.truncate_between_flankers()\n    a.truncate_between_flankers()\n    b.truncate_between_flankers()\n    c.truncate_between_flankers(target=6)\n\n    plot_diagram(root, .14, .2, A, a, \"S\", \"syntenic\")\n    plot_diagram(root, .37, .2, A, b, \"F\", \"missing, with both flankers\")\n    plot_diagram(root, .6, .2, A, c, \"G\", \"missing, with one flanker\")\n\n    labels = ((.04, .95, 'A'), (.04, .75, 'B'), (.04, .4, 'C'))\n    panel_labels(root, labels)\n\n    # Descriptions\n    xt = .85\n    desc = (\"Extract neighborhood\",\n            \"of *window* size\",\n            \"Count gene pairs within *window*\",\n            \"Find regions above *score* cutoff\",\n            \"Identify flankers\",\n            \"Annotate syntelog class\"\n            )\n    for yt, t in zip((.88, .84, .64, .6, .3, .26), desc):\n        root.text(xt, yt, markup(t), ha=\"center\", va=\"center\")\n\n    root.set_xlim(0, 1)\n    root.set_ylim(0, 1)\n    root.set_axis_off()\n\n    pf = \"cartoon\"\n    image_name = pf + \".\" + iopts.format\n    savefig(image_name, dpi=iopts.dpi, iopts=iopts)", "response": "%prog cartoon synteny. py\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_splits(split_bed, gff_file, stype, key):\n    bed_file = get_bed_file(gff_file, stype, key)\n    cmd = \"intersectBed -a {0} -b {1} -wao\".format(split_bed, bed_file)\n    cmd += \" | cut -f4,10\"\n    p = popen(cmd)\n    splits = defaultdict(set)\n    for row in p:\n        a, b = row.split()\n        splits[a].add(b)\n\n    return splits", "response": "Get the set of split genes mappings for a given gene."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_accuracy(query, gff_file, evidences_bed, sizesfile, type, key):\n    from jcvi.formats.bed import evaluate\n\n    bed_file = get_bed_file(gff_file, type, key)\n    b = evaluate([bed_file, evidences_bed, sizesfile, \"--query={0}\".format(query)])\n\n    return b", "response": "Get sensitivity specificity and accuracy given a gff_file and a query range\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef datastore(args):\n    p = OptionParser(datastore.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    ds, = args\n    fp = open(ds)\n    for row in fp:\n        fn = row.strip()\n        assert op.exists(fn)\n        pp, logfile = op.split(fn)\n        flog = open(fn)\n        for row in flog:\n            ctg, folder, status = row.split()\n            if status != \"FINISHED\":\n                continue\n\n            gff_file = op.join(pp, folder, ctg + \".gff\")\n            assert op.exists(gff_file)\n            print(gff_file)", "response": "%prog datastore datastore. log"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef libsvm(args):\n    from jcvi.formats.base import DictFile\n\n    p = OptionParser(libsvm.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    csvfile, prefixids = args\n    d = DictFile(prefixids)\n    fp = open(csvfile)\n    next(fp)\n    for row in fp:\n        atoms = row.split()\n        klass = atoms[0]\n        kp = klass.split(\"_\")[0]\n        klass = d.get(kp, \"0\")\n        feats = [\"{0}:{1}\".format(i + 1, x) for i, x in enumerate(atoms[1:])]\n        print(\" \".join([klass] + feats))", "response": "%prog libsvm csvfile prefix. ids"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef determine_positions(nodes, edges):\n    N = len(nodes)\n    E = len(edges)\n\n    A = np.zeros((E, N), dtype=int)\n    for i, (a, b, distance) in enumerate(edges):\n        A[i, a] = 1\n        A[i, b] = -1\n\n    K = np.eye(E, dtype=int)\n    L = np.array([x[-1] for x in edges])\n\n    s = spring_system(A, K, L)\n    return np.array([0] + [int(round(x, 0)) for x in s])", "response": "Construct the problem instance to solve the positions of contigs."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines the signs of the pairs on N molecules.", "response": "def determine_signs(nodes, edges, cutoff=1e-10):\n    \"\"\"\n    Construct the orientation matrix for the pairs on N molecules.\n\n    >>> determine_signs([0, 1, 2], [(0, 1, 1), (0, 2, -1), (1, 2, -1)])\n    array([ 1,  1, -1])\n    \"\"\"\n    N = len(nodes)\n    M = np.zeros((N, N), dtype=float)\n    for a, b, w in edges:\n        M[a, b] += w\n    M = symmetrize(M)\n\n    return get_signs(M, cutoff=cutoff, validate=False)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a numpy array M that contains pairwise orientations find the largest eigenvalue and associated eigenvector and return the signs for the largest eigenvalue and associated eigenvector.", "response": "def get_signs(M, cutoff=1e-10, validate=True, ambiguous=True):\n    \"\"\"\n    Given a numpy array M that contains pairwise orientations, find the largest\n    eigenvalue and associated eigenvector and return the signs for the\n    eigenvector. This should correspond to the original orientations for the\n    individual molecule. In the first example below, let's say 3 molecules A, B\n    and C, A-B:same direction, A-C:opposite direction, B-C:opposite\n    direction. The final solution is to flip C.\n\n    >>> M = np.array([[0,1,-1],[1,0,-1],[-1,-1,0]])\n    >>> get_signs(M)\n    array([ 1,  1, -1])\n    >>> M = np.array([[0,1,-1],[1,0,0],[-1,0,0]])\n    >>> get_signs(M)\n    array([ 1,  1, -1])\n    \"\"\"\n    # Is this a symmetric matrix?\n    assert is_symmetric(M), \"the matrix is not symmetric:\\n{0}\".format(str(M))\n    N, x = M.shape\n\n    # eigh() works on symmetric matrix (Hermitian)\n    w, v = np.linalg.eigh(M)\n    m = np.argmax(w)\n    mv = v[:, m]\n    f = lambda x: (x if abs(x) > cutoff else 0)\n    mv = [f(x) for x in mv]\n\n    sign_array = np.array(np.sign(mv), dtype=int)\n\n    # it does not really matter, but we prefer as few flippings as possible\n    if np.sum(sign_array) < 0:\n        sign_array = -sign_array\n\n    if validate:\n        diag = np.matrix(np.eye(N, dtype=int) * sign_array)\n        final = diag * M * diag\n        # The final result should have all pairwise in the same direction\n        assert (final >= 0).all(), \\\n                \"result check fails:\\n{0}\".format(final)\n\n    if not ambiguous:   # Do we allow ambiguous orientation (=0) ?\n        sign_array[sign_array == 0] = 1\n\n    return sign_array"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef spring_system(A, K, L):\n    # Linear equation is A'KAx = -A'KL\n    C = np.dot(A.T, K)\n    left = np.dot(C, A)\n    right = - np.dot(C, L)\n\n    left = left[1:, 1:]\n    right = right[1:]\n    x = np.linalg.solve(left, right)\n\n    return x", "response": "Solves the equilibrium positions of the objects linked by springs of length L and connectivity matrix A."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfix text in the format that is used by the DDB1 language.", "response": "def fix_text(s, ignore_sym_pat=False):\n\n    if not ignore_sym_pat:\n        # Fix descriptions like D7TDB1 (\n        s = re.sub(\"([A-Z0-9]){6} \\(\", \"\", s)\n        s = s.split(\";\")[0]\n\n    # Fix parantheses containing names\n    s = s.translate(None, \"[]\")\n    s = s.replace(\"(-)\", \"[-]\")\n    s = s.replace(\"(+)\", \"[+]\")\n    s = s.replace(\"(Uncharacterized protein)\", \"\")\n    if not ignore_sym_pat:\n        s = s.translate(None, \"()\")\n\n    # fix minor typos, seen in `autonaming` output\n    # change 'protei ' to 'protein '\n    # change 'hypthetical' to 'hypothetical'\n    # fix string starting with 'ytochrome'\n    if 'protei ' in s: s = s.replace('protei ', 'protein ')\n    if 'hypthetical' in s: s = s.replace('hypthetical', 'hypothetical')\n    if s.startswith('ytochrome'): s = s.replace('ytochrome', 'cytochrome')\n\n    # before trimming off at the first \";\", check if name has glycosidic\n    # linkage information (e.g 1,3 or 1,4). If so, also check if multiple\n    # linkages are separated by \";\". If so, replace \";\" by \"-\"\n    m = re.findall(glycosidic_link_pat, s)\n    if m and \";\" in s:\n        s = re.sub(\";\\s*\", \"-\", s)\n\n    # remove underscore from description\n    s = re.sub(\"_\", \" \", s)\n\n    # Cellular locations\n    # Any word that matches e.g. AT5G54690\n    # Any word that matches e.g. Os02g0234800\n    # (fragment)\n    # UPF\n    # Remove 'DDB_G\\d+' ID\n    # '_At[0-9]+g[0-9]+' to ''\n    for pat in (loc_pat, osg_pat, frag_pat, upf_pat, ddb_pat):\n        # below is a hack since word boundaries don't work on /\n        s = s.strip() + \" \"\n        s = re.sub(pat, \"\", s)\n\n    # &apos;? => '\n    s = re.sub(apos_pat, \"'\", s)\n    # &gt => none\n    s = re.sub(gt_pat, \"\", s)\n    # reduce runs such as -- '''\n    s = re.sub(r\"[-]+\", \"-\", s)\n    s = re.sub(r\"[']+\", \"'\", s)\n\n    s = s.strip()\n\n    # -like to -like protein\n    s = re.sub(like_pat, \"-like protein\", s)\n\n    # 'repeat$' to 'repeat protein'\n    if re.search(repeat_pat, s):\n        s += \"-containing protein\"\n\n    # 'binding$' to 'binding protein'\n    if re.search(binding_pat, s):\n        s += \" protein\"\n        if re.match(Protein_pat, s):\n            s = re.sub(Protein_pat, \"\", s)\n\n    # 'domain$' to 'domain-containing protein'\n    if re.search(domain_pat, s):\n        s += \"-containing protein\"\n        if re.search(r\"-domain\", s):\n            s = re.sub(r\"-domain\", \" domain\", s)\n        if re.match(Protein_pat, s):\n            s = re.sub(Protein_pat, \"\", s)\n\n    # 'related$' to '-like protein'\n    if re.search(related_pat, s):\n        s = re.sub(related_pat, \"-like protein\", s)\n        if re.match(Protein_pat, s) and not re.match(r\"Protein kinase\", s):\n            s = re.sub(Protein_pat, \"\", s)\n\n    # '[0-9]+ homolog' to '-like protein'\n    if re.search(homolog_pat1, s):\n        s = re.sub(homolog_pat1, \"-like protein\", s)\n        if re.match(Protein_pat, s):\n            s = re.sub(Protein_pat, \"\", s)\n\n    # 'Protein\\s+(.*)\\s+homolog' to '$1-like protein'\n    match = re.search(homolog_pat2, s)\n    if match and not re.match(r\"Protein kinase\", s):\n        ret = match.group(1)\n        s = re.sub(homolog_pat2, ret + \"-like protein\", s)\n        s = re.sub(r\"^\\s+\", \"\", s)\n        s = s.capitalize()\n\n    # 'homolog protein' to '-like protein'\n    # 'homologue$' to '-like protein'\n    # 'homolog$' to '-like protein'\n    for pat in (homolog_pat3, homolog_pat5, homolog_pat6):\n        if re.search(pat, s):\n            s = re.sub(pat, \"-like protein\", s)\n\n    # 'Agenet domain-containing protein / bromo-adjacent homology (BAH) domain-containing protein'\n    # to 'Agenet and bromo-adjacent homology (BAH) domain-containing protein'\n    if re.search(agenet_pat, s):\n        s = re.sub(agenet_pat, \"Agenet and \", s)\n\n    # plural to singular\n    if re.search(plural_pat, s):\n        if (s.find('biogenesis') == -1 and s.find('Topors') == -1) or (not re.search(with_and_pat, s)):\n            s = re.sub(r\"s$\", \"\", s)\n\n    # 'like_TBP' or 'likeTBP' to 'like TBP'\n    if re.search(tbp_pat, s):\n        s = re.sub(tbp_pat, \"like TBP\", s)\n\n    # 'protein protein' to 'protein'\n    if re.search(prot_pat, s):\n        s = re.sub(prot_pat, \"protein\", s)\n\n    # 'dimerisation' to 'dimerization'\n    if re.search(dimer_pat, s):\n        s = re.sub(dimer_pat, \"dimerization\", s)\n\n    # Any AHRD that matches e.g. \"AT5G54690-like protein\"\n    # Any AHRD that contains the words '^Belongs|^Encoded|^Expression|^highly'\n    for pat in (atg_pat, athila_pat1):\n        if re.search(pat, s):\n            s = Unknown\n\n    # remove 'arabidopsis[ thaliana]' and/or embedded Atg IDs\n    for pat in (atg_id_pat, athila_pat2, athila_pat3, athila_pat4):\n        # below is a hack since word boundaries don't work on /\n        s = s.strip() + \" \"\n        s = re.sub(pat, \"\", s)\n\n    # remove \"\\s+LENGTH=\\d+\" from TAIR deflines\n    if re.search(length_pat, s):\n        s = re.sub(length_pat, \"\", s)\n\n    # if name has a dot followed by a space (\". \") in it and contains multiple\n    # parts separated by a comma, strip name starting from first occurrence of \",\"\n    if re.search(r\"\\. \", s):\n        if re.search(r\",\", s):\n            s = s.split(\",\")[0]\n\n    # if name contains any of the disallowed words,\n    # remove word occurrence from name\n    # if name contains references to any other organism, trim name upto\n    # that occurrence\n    for pat in (disallow_pat, organism_pat):\n        if re.search(pat, s):\n            s = re.sub(pat, \"\", s)\n\n    s = s.strip()\n\n    if not ignore_sym_pat:\n        # 'homolog \\d+' to '-like protein'\n        if re.search(homolog_pat4, s):\n            s = re.sub(homolog_pat4, \"\", s)\n\n        # Trailing protein numeric copy (e.g. Myb 1)\n        if re.search(trail_pat, s):\n            s = re.sub(trail_pat, \"\", s)\n\n        # if name is entirely a gene symbol-like (all capital letters, maybe followed by numbers)\n        # add a \"-like protein\" at the end\n        if (re.search(sym_pat, s) or re.search(lc_sym_pat, s)) \\\n                and not re.search(spada_pat, s):\n            s = s + \"-like protein\"\n\n        # if gene symbol in parantheses at EOL, remove symbol\n        if re.search(eol_sym_pat, s):\n            s = re.sub(eol_sym_pat, \"\", s)\n\n        # if name terminates at a symbol([^A-Za-z0-9_]), trim it off\n        if re.search(r\"\\W{1,}$\", s) and not re.search(r\"\\)$\", s):\n            s = re.sub(\"\\W{1,}$\", \"\", s)\n\n        if \"uncharacterized\" in s:\n            s = \"uncharacterized protein\"\n\n    # change sulfer to sulfur\n    if re.search(sulfer_pat, s):\n        s = re.sub(sulfer_pat, \"sulfur\", s)\n\n    # change sulph to sulf\n    if re.search(sulph_pat, s):\n        s = re.sub(sulph_pat, \"sulf\", s)\n\n    # change monoxy to monooxy\n    if re.search(monoxy_pat, s):\n        s = re.sub(monoxy_pat, \"monooxy\", s)\n\n    # change proteine to protein\n    if re.search(proteine_pat, s):\n        s = re.sub(proteine_pat, \"protein\", s)\n\n    # change signalling to signaling\n    if re.search(signalling_pat, s):\n        s = re.sub(signalling_pat, \"signaling\", s)\n\n    # change aluminium to aluminum\n    if re.search(aluminium_pat, s):\n        s = re.sub(aluminium_pat, \"aluminum\", s)\n\n    # change haem to heme\n    if re.search(haem_pat, s):\n        s = re.sub(haem_pat, \"heme\", s)\n\n    # chage haemo to hemo\n    if re.search(haemo_pat, s):\n        s = re.sub(haemo_pat, \"hemo\", s)\n\n    # change assessory to accessory\n    if re.search(assessory_pat, s):\n        s = re.sub(assessory_pat, \"accessory\", s)\n\n    # change -ise/-ised/-isation to -ize/-ized/-ization\n    match = re.search(ise_pat, s)\n    if match:\n        ret = match.group(1)\n        if match.group(2):\n            suff = match.group(2)\n            s = re.sub(ise_pat, \"{0}ize{1}\".format(ret, suff), s)\n        else:\n            s = re.sub(ise_pat, \"{0}ize\".format(ret), s)\n\n    match = re.search(isation_pat, s)\n    if match:\n        ret = match.group(1)\n        s = re.sub(isation_pat, \"{0}ization\".format(ret), s)\n\n    # change -bre to -ber\n    match = re.search(bre_pat, s)\n    if match:\n        ret = match.group(1)\n        s = re.sub(bre_pat, \"{0}ber\".format(ret), s)\n\n    if not s.startswith(Hypothetical):\n        # 'Candidate|Hypothetical|Novel|Predicted|Possible|Probable|Uncharacterized' to 'Putative'\n        if s.startswith('Uncharacterized') and any(pat in s for pat in ('UCP', 'UPF', 'protein')):\n            pass\n        else:\n            if re.search(put_pat, s):\n                s = re.sub(put_pat, \"Putative\", s)\n\n    \"\"\"\n    case (qr/^Histone-lysine/) { $ahrd =~ s/,\\s+H\\d{1}\\s+lysine\\-\\d+//gs; }\n    \"\"\"\n    sl = s.lower()\n\n    # Any mention of `clone` or `contig` is not informative\n    if \"clone\" in sl or \"contig\" in sl:\n        s = Unknown\n\n    # All that's left is `protein` is not informative\n    if sl in (\"protein\", \"protein, putative\", \"\"):\n        s = Unknown\n\n    if Unknown.lower() in sl:\n        s = Unknown\n\n    if \"FUNCTIONS IN\".lower() in sl and \"unknown\" in sl:\n        s = Unknown\n\n    if \"LOCATED IN\".lower() in sl:\n        s = Unknown\n\n    s = re.sub(r\"[,]*\\s+putative$\", \"\", s)\n\n    if s == Unknown or s.strip() == \"protein\":\n        s = Hypothetical\n\n    # Compact all spaces\n    s = ' '.join(s.split())\n\n    assert s.strip()\n\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef batch(args):\n    p = OptionParser(batch.__doc__)\n\n    ahrd_weights = { \"blastp\": [0.5, 0.3, 0.2],\n                     \"blastx\": [0.6, 0.4, 0.0]\n                   }\n    blast_progs = tuple(ahrd_weights.keys())\n\n    p.add_option(\"--path\", default=\"~/code/AHRD/\",\n                 help=\"Path where AHRD is installed [default: %default]\")\n    p.add_option(\"--blastprog\", default=\"blastp\", choices=blast_progs,\n                help=\"Specify the blast program being run. Based on this option,\" \\\n                   + \" the AHRD parameters (score_weights) will be modified.\" \\\n                   + \" [default: %default]\")\n    p.add_option(\"--iprscan\", default=None,\n                help=\"Specify path to InterProScan results file if available.\" \\\n                   + \" If specified, the yml conf file will be modified\" \\\n                   + \" appropriately. [default: %default]\")\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    splits, output = args\n    mkdir(output)\n\n    bit_score, db_score, ovl_score = ahrd_weights[opts.blastprog]\n\n    for f in glob(\"{0}/*.fa*\".format(splits)):\n        fb = op.basename(f).rsplit(\".\", 1)[0]\n        fw = open(op.join(output, fb + \".yml\"), \"w\")\n\n        path = op.expanduser(opts.path)\n        dir = op.join(path, \"test/resources\")\n        outfile = op.join(output, fb + \".csv\")\n        interpro = iprscanTemplate.format(opts.iprscan) if opts.iprscan else \"\"\n\n        print(Template.format(dir, fb, f, outfile, bit_score, db_score, ovl_score, interpro), file=fw)\n\n    if opts.iprscan:\n        if not op.lexists(\"interpro.xml\"):\n            symlink(op.join(iprscan_datadir, \"interpro.xml\"), \"interpro.xml\")\n\n        if not op.lexists(\"interpro.dtd\"):\n            symlink(op.join(iprscan_datadir, \"interpro.dtd\"), \"interpro.dtd\")", "response": "%prog batch splits output\n\n    The arguments are two folders.\n    Input FASTA sequences are in splits/.\n    Output csv files are in output/.\n\n    Must have folders swissprot/, tair/, trembl/ that contains the respective\n    BLAST output. Once finished, you can run, for example:\n\n    $ parallel java -Xmx2g -jar ~/code/AHRD/dist/ahrd.jar {} ::: output/*.yml"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_attributes(s, gff3=True, keep_attr_order=True):\n    if gff3:\n        \"\"\"\n        hack: temporarily replace the '+' sign in the attributes column\n        with the string 'PlusSign' to prevent urlparse.parse_qsl() from\n        replacing the '+' sign with a space\n        \"\"\"\n        s = s.replace('+', 'PlusSign')\n        d = parse_qs(s, keep_attr_order=keep_attr_order)\n        for key in d:\n            d[key][0] = unquote(d[key][0].replace('PlusSign', '+').replace('\"', ''))\n    else:\n        attributes = s.split(\";\")\n        d = DefaultOrderedDict(list) if keep_attr_order else defaultdict(list)\n        for a in attributes:\n            a = a.strip()\n            if ' ' not in a:\n                continue\n            key, val = a.split(' ', 1)\n            val = unquote(val.replace('\"', '').replace('=', ' ').strip())\n            d[key].append(val)\n\n    for key, val in d.items():\n        d[key] = list(flatten([v.split(\",\") for v in val]))\n\n    return d", "response": "Make a dictionary of attributes from a GFF3 or GFF2 string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_range(obj, score=None, id=None, strand=None):\n    from jcvi.utils.range import Range\n\n    if score or id:\n        _score = score if score else obj.score\n        _id = id if id else obj.id\n        return Range(seqid=obj.seqid, start=obj.start, end=obj.end, \\\n            score=_score, id=_id)\n    elif strand:\n        return (obj.seqid, obj.start, obj.end, obj.strand)\n\n    return (obj.seqid, obj.start, obj.end)", "response": "Convert a GFFutils object to a range object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef addparent(args):\n    p = OptionParser(addparent.__doc__)\n    p.add_option(\"--childfeat\", default=\"CDS\", help=\"Type of children feature\")\n    p.add_option(\"--parentfeat\", default=\"mRNA\", help=\"Type of merged feature\")\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    gff_file, = args\n    gff = Gff(gff_file)\n    data = defaultdict(list)\n    for g in gff:\n        if g.type != opts.childfeat:\n            continue\n        data[g.parent].append(g)\n\n    logging.debug(\"A total of {0} {1} features clustered\".\\\n                    format(len(data), opts.childfeat))\n\n    parents = []\n    for parent, dd in data.items():\n        d = dd[0]\n        start, end = min(x.start for x in dd), max(x.end for x in dd)\n        gffline = \"\\t\".join(str(x) for x in \\\n                        (d.seqid, d.source, opts.parentfeat, start, end,\n                         \".\", d.strand, \".\", \"ID={0};Name={0}\".format(parent)))\n        parents.append(GffLine(gffline))\n    parents.sort(key=lambda x: (x.seqid, x.start))\n    logging.debug(\"Merged feature sorted\")\n\n    fw = must_open(opts.outfile, \"w\")\n    for parent in parents:\n        print(parent, file=fw)\n        parent_id = parent.id\n        for d in data[parent_id]:\n            if d.accn == parent_id:\n                new_id = \"{0}.{1}1\".format(parent_id, opts.childfeat)\n                d.set_attr(\"ID\", new_id)\n                d.set_attr(\"Name\", new_id, update=True)\n            print(d, file=fw)\n    fw.close()", "response": "%prog addparent file.gff\n\n    Merge sister features and infer parents."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn slice of fasta given seqid start stop and strand", "response": "def _fasta_slice(fasta, seqid, start, stop, strand):\n    \"\"\"\n    Return slice of fasta, given (seqid, start, stop, strand)\n    \"\"\"\n    _strand = 1 if strand == '+' else -1\n    return fasta.sequence({'chr': seqid, 'start': start, 'stop': stop, \\\n        'strand': _strand})"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_valid_codon(codon, type='start'):\n    if len(codon) != 3:\n        return False\n\n    if type == 'start':\n        if codon != 'ATG':\n            return False\n    elif type == 'stop':\n        if not any(_codon == codon for _codon in ('TGA', 'TAG', 'TAA')):\n            return False\n    else:\n        logging.error(\"`{0}` is not a valid codon type. \".format(type) + \\\n            \"Should be one of (`start` or `stop`)\")\n        sys.exit()\n\n    return True", "response": "Check if a codon sequence is a valid start or stop codon."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef scan_for_valid_codon(codon_span, strand, seqid, genome, type='start'):\n    s, e = codon_span[0], codon_span[1]\n    while True:\n        if (type == 'start' and strand == '+') or \\\n            (type == 'stop' and strand == '-'):\n            s, e = s - 3, e - 3\n        else:\n            s, e = s + 3, e + 3\n\n        codon = _fasta_slice(genome, seqid, s, e, strand)\n        is_valid = is_valid_codon(codon, type=type)\n        if not is_valid:\n            if type == 'start':\n                ## if we are scanning upstream for a valid start codon,\n                ## stop scanning when we encounter a stop\n                if is_valid_codon(codon, type='stop'):\n                    return (None, None)\n            elif type == 'stop':\n                ## if we are scanning downstream for a valid stop codon,\n                ## stop scanning when we encounter a start\n                if is_valid_codon(codon, type='start'):\n                    return (None, None)\n            continue\n        break\n\n    return (s, e)", "response": "Scan upstream and downstream codon list for a valid in - frame start or stop codon."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef summary(args):\n    from jcvi.formats.base import SetFile\n    from jcvi.formats.bed import BedSummary\n    from jcvi.utils.table import tabulate\n\n    p = OptionParser(summary.__doc__)\n    p.add_option(\"--isoform\", default=False, action=\"store_true\",\n                 help=\"Find longest isoform of each id\")\n    p.add_option(\"--ids\", help=\"Only include features from certain IDs\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    gff_file, = args\n    ids = opts.ids\n\n    if ids:\n        ids = SetFile(ids)\n        logging.debug(\"Total ids loaded: {0}\".format(len(ids)))\n\n        if opts.isoform:\n            pids = set()\n            gff = Gff(gff_file)\n            for g in gff:\n                if g.type != \"mRNA\":\n                    continue\n                if g.parent not in ids:\n                    continue\n                if \"longest\" not in g.attributes:\n                    pids = set(x + \".1\" for x in ids)\n                    break\n                if g.attributes[\"longest\"][0] == \"0\":\n                    continue\n                pids.add(g.id)\n            ids = pids\n            logging.debug(\"After checking longest: {0}\".format(len(ids)))\n\n        # Collects aliases\n        gff = Gff(gff_file)\n        for g in gff:\n            if g.name in ids:\n                ids.add(g.id)\n        logging.debug(\"Total ids including aliases: {0}\".format(len(ids)))\n\n    gff = Gff(gff_file)\n    beds = defaultdict(list)\n    for g in gff:\n        if ids and not (g.id in ids or g.name in ids or g.parent in ids):\n            continue\n\n        beds[g.type].append(g.bedline)\n\n    table = {}\n    for type, bb in sorted(beds.items()):\n        bs = BedSummary(bb)\n        table[(type, \"Features\")] = bs.nfeats\n        table[(type, \"Unique bases\")] = bs.unique_bases\n        table[(type, \"Total bases\")] = bs.total_bases\n\n    print(tabulate(table), file=sys.stdout)", "response": "%prog summary gffile\n\n    Print summary stats for features of different types."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef format(args):\n    from jcvi.formats.obo import load_GODag, validate_term\n\n    valid_multiparent_ops = [\"split\", \"merge\"]\n\n    p = OptionParser(format.__doc__)\n\n    g1 = OptionGroup(p, \"Parameter(s) used to modify GFF attributes (9th column)\")\n    g1.add_option(\"--name\", help=\"Add Name attribute from two-column file [default: %default]\")\n    g1.add_option(\"--note\", help=\"Add Note attribute from two-column file [default: %default]\")\n    g1.add_option(\"--add_attribute\", dest=\"attrib_files\", help=\"Add new attribute(s) \" + \\\n                \"from two-column file(s); attribute name comes from filename; \" + \\\n                \"accepts comma-separated list of files [default: %default]\")\n    g1.add_option(\"--add_dbxref\", dest=\"dbxref_files\", help=\"Add new Dbxref value(s) (DBTAG:ID) \" + \\\n                \"from two-column file(s). DBTAG comes from filename, ID comes from 2nd column; \" + \\\n                \"accepts comma-separated list of files [default: %default]\")\n    g1.add_option(\"--nostrict\", default=False, action=\"store_true\",\n                 help=\"Disable strict parsing of GFF file and/or mapping file [default: %default]\")\n    g1.add_option(\"--remove_attr\", dest=\"remove_attrs\", help=\"Specify attributes to remove; \" + \\\n                \"accepts comma-separated list of attribute names [default: %default]\")\n    g1.add_option(\"--copy_id_attr_to_name\", default=False, action=\"store_true\",\n                 help=\"Copy `ID` attribute value to `Name`, when `Name` is not defined\")\n    g1.add_option(\"--invent_name_attr\", default=False, action=\"store_true\",\n                 help=\"Invent `Name` attribute for 2nd level child features; \" + \\\n                \"Formatted like PARENT:FEAT_TYPE:FEAT_INDEX\")\n    g1.add_option(\"--no_keep_attr_order\", default=False, action=\"store_true\",\n                 help=\"Do not maintain attribute order [default: %default]\")\n    p.add_option_group(g1)\n\n    g2 = OptionGroup(p, \"Parameter(s) used to modify content within columns 1-8\")\n    g2.add_option(\"--seqid\", help=\"Switch seqid from two-column file. If not\" + \\\n                \" a file, value will globally replace GFF seqid [default: %default]\")\n    g2.add_option(\"--source\", help=\"Switch GFF source from two-column file. If not\" + \\\n                \" a file, value will globally replace GFF source [default: %default]\")\n    g2.add_option(\"--type\", help=\"Switch GFF feature type from two-column file. If not\" + \\\n                \" a file, value will globally replace GFF type [default: %default]\")\n    g2.add_option(\"--fixphase\", default=False, action=\"store_true\",\n                 help=\"Change phase 1<->2, 2<->1 [default: %default]\")\n    p.add_option_group(g2)\n\n    g3 = OptionGroup(p, \"Other parameter(s) to perform manipulations to the GFF \" + \\\n                 \"file content\")\n    g3.add_option(\"--unique\", default=False, action=\"store_true\",\n                 help=\"Make IDs unique [default: %default]\")\n    g3.add_option(\"--chaindup\", default=None, dest=\"duptype\",\n                 help=\"Chain duplicate features of a particular GFF3 `type`,\" + \\\n                      \" sharing the same ID attribute [default: %default]\")\n    g3.add_option(\"--multiparents\", default=None, choices=valid_multiparent_ops,\n                 help=\"Split/merge identical features (same `seqid`, `source`, `type` \" + \\\n                 \"`coord-range`, `strand`, `phase`) mapping to multiple parents \" + \\\n                 \"[default: %default]\")\n    g3.add_option(\"--remove_feats\", help=\"Comma separated list of features to remove by type\" + \\\n                \" [default: %default]\")\n    g3.add_option(\"--remove_feats_by_ID\", help=\"List of features to remove by ID;\" + \\\n                \" accepts comma-separated list or list file [default: %default]\")\n    g3.add_option(\"--gsac\", default=False, action=\"store_true\",\n                 help=\"Fix GSAC GFF3 file attributes [default: %default]\")\n    g3.add_option(\"--invent_protein_feat\", default=False, action=\"store_true\",\n                 help=\"Invent a protein feature span (chain CDS feats)\")\n    g3.add_option(\"--process_ftype\", default=None, type=\"str\",\n                 help=\"Specify feature types to process; \"\n                 \"accepts comma-separated list of feature types [default: %default]\")\n    g3.add_option(\"--gff3\", default=False, action=\"store_true\",\n                 help=\"Print output in GFF3 format [default: %default]\")\n    g3.add_option(\"--make_gff_store\", default=False, action=\"store_true\",\n                 help=\"Store entire GFF file in memory during first iteration [default: %default]\")\n    p.add_option_group(g3)\n\n    p.set_outfile()\n    p.set_SO_opts()\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    gffile, = args\n    mapfile = opts.seqid\n    names = opts.name\n    note = opts.note\n    source = opts.source\n    ftype = opts.type\n    attrib_files = opts.attrib_files.split(\",\") if opts.attrib_files else None\n    dbxref_files = opts.dbxref_files.split(\",\") if opts.dbxref_files else None\n    remove_attrs = opts.remove_attrs.split(\",\") if opts.remove_attrs else None\n    process_ftype = opts.process_ftype.split(\",\") if opts.process_ftype else None\n    gsac = opts.gsac\n    assert not (opts.unique and opts.duptype), \\\n        \"Cannot use `--unique` and `--chaindup` together\"\n    assert not(opts.type and opts.duptype), \\\n        \"Cannot use `--type` and `--chaindup` together\"\n    unique = opts.unique\n    duptype = opts.duptype\n    fixphase = opts.fixphase\n    phaseT = {\"1\":\"2\", \"2\":\"1\"}\n    remove_feats = opts.remove_feats.split(\",\") if opts.remove_feats else None\n    remove_feats_by_ID = None\n    if opts.remove_feats_by_ID:\n        remove_feats_by_ID = LineFile(opts.remove_feats_by_ID, load=True).lines \\\n                if op.isfile(opts.remove_feats_by_ID) else \\\n                opts.remove_feats_by_ID.split(\",\")\n    strict = False if opts.nostrict else True\n    make_gff_store = True if gffile in (\"-\", \"stdin\") else opts.make_gff_store\n    assert not (opts.copy_id_attr_to_name and opts.invent_name_attr), \\\n        \"Cannot use `--copy_id_attr_to_name` and `--invent_name_attr` together\"\n    copy_id_attr_to_name = opts.copy_id_attr_to_name\n    invent_name_attr = opts.invent_name_attr\n    invent_protein_feat = opts.invent_protein_feat\n    compute_signature = False\n\n    outfile = opts.outfile\n\n    mapping = None\n    mod_attrs = set()\n    if mapfile and op.isfile(mapfile):\n        mapping = DictFile(mapfile, delimiter=\"\\t\", strict=strict)\n        mod_attrs.add(\"ID\")\n    if note:\n        note = DictFile(note, delimiter=\"\\t\", strict=strict)\n        mod_attrs.add(\"Note\")\n    if source and op.isfile(source):\n        source = DictFile(source, delimiter=\"\\t\", strict=strict)\n    if ftype and op.isfile(ftype):\n        ftype = DictFile(ftype, delimiter=\"\\t\", strict=strict)\n    if names:\n        names = DictFile(names, delimiter=\"\\t\", strict=strict)\n        mod_attrs.add(\"Name\")\n\n    if attrib_files:\n        attr_values = {}\n        for fn in attrib_files:\n            attr_name = op.basename(fn).rsplit(\".\", 1)[0]\n            if attr_name not in reserved_gff_attributes:\n                attr_name = attr_name.lower()\n            attr_values[attr_name] = DictFile(fn, delimiter=\"\\t\", strict=strict)\n            mod_attrs.add(attr_name)\n    if dbxref_files:\n        dbxref_values = {}\n        for fn in dbxref_files:\n            dbtag = op.basename(fn).rsplit(\".\", 1)[0]\n            dbxref_values[dbtag] = DictFile(fn, delimiter=\"\\t\", strict=strict)\n        mod_attrs.add(\"Dbxref\")\n\n    if remove_attrs:\n        mod_remove_attrs = []\n        for remove_attr in remove_attrs:\n            if remove_attr in mod_attrs:\n                mod_remove_attrs.append(remove_attr)\n\n        if mod_remove_attrs:\n            logging.error(\"Attributes `{0}` cannot be removed and modified\".format( \\\n                    \",\".join(mod_remove_attrs)))\n            sys.exit()\n\n    if gsac:  # setting gsac will force IDs to be unique\n        unique = True\n        notes = {}\n\n    remove = set()\n    if unique or duptype or remove_feats or remove_feats_by_ID \\\n            or opts.multiparents == \"merge\" or invent_name_attr or make_gff_store \\\n            or invent_protein_feat:\n        if unique:\n            dupcounts = defaultdict(int)\n            seen = defaultdict(int)\n            newparentid = {}\n        elif duptype:\n            dupranges = AutoVivification()\n            skip = defaultdict(int)\n        if opts.multiparents == \"merge\":\n            merge_feats = AutoVivification()\n        if invent_name_attr:\n            ft = GffFeatureTracker()\n        elif copy_id_attr_to_name:\n            pass\n        if invent_protein_feat:\n            cds_track = {}\n        if opts.multiparents == \"merge\" or invent_name_attr:\n            make_gff_store = compute_signature = True\n        gff = Gff(gffile, keep_attr_order=(not opts.no_keep_attr_order), \\\n                make_gff_store=make_gff_store, compute_signature=compute_signature, \\\n                strict=strict)\n        for g in gff:\n            if process_ftype and g.type not in process_ftype:\n                continue\n            id = g.accn\n            if remove_feats and g.type in remove_feats:\n                remove.add(id)\n            if remove_feats_by_ID and id in remove_feats_by_ID:\n                remove.add(id)\n            if unique:\n                dupcounts[id] += 1\n            elif duptype and g.type == duptype:\n                dupranges[g.seqid][id][g.idx] = (g.start, g.end)\n            if opts.multiparents == \"merge\" and g.type != \"CDS\": #don't merge CDS\n                pp = g.get_attr(\"Parent\", first=False)\n                if pp and len(pp) > 0:\n                    for parent in pp:\n                        if parent not in remove:\n                            sig = g.sign\n                            if sig not in merge_feats:\n                                merge_feats[sig]['parents'] = []\n                            if parent not in merge_feats[sig]['parents']:\n                                merge_feats[sig]['parents'].append(parent)\n            if invent_name_attr:\n                parent, iso = atg_name(g.get_attr(\"Parent\"), retval=\"locus,iso\")\n                if not parent:\n                    parent = g.get_attr(\"Parent\")\n                ft.track(parent, g)\n            if invent_protein_feat:\n                if g.type == 'CDS':\n                    cds_parent = g.get_attr(\"Parent\")\n                    if cds_parent not in cds_track:\n                        cds_track[cds_parent] = []\n                    cds_track[cds_parent].append((g.start, g.end))\n\n    if opts.verifySO:\n        so = load_GODag()\n        valid_soterm = {}\n\n    fw = must_open(outfile, \"w\")\n    if not make_gff_store:\n        gff = Gff(gffile, keep_attr_order=(not opts.no_keep_attr_order), \\\n                strict=strict)\n    for g in gff:\n        if process_ftype and g.type not in process_ftype:\n            print(g, file=fw)\n            continue\n\n        id = g.accn\n\n        if opts.multiparents == \"merge\" and g.type != \"CDS\": #don't merge CDS\n            sig = g.sign\n            if len(merge_feats[sig]['parents']) > 1:\n                if 'candidate' not in merge_feats[sig]:\n                    merge_feats[sig]['candidate'] = id\n                    g.set_attr(\"Parent\", merge_feats[sig]['parents'])\n                else:\n                    continue\n\n        if remove_feats or remove_feats_by_ID:\n            if id in remove:\n                continue\n            else:\n                if \"Parent\" in g.attributes:\n                    keep, parent = [], g.get_attr(\"Parent\", first=False)\n                    for i, pid in enumerate(parent):\n                        if pid not in remove:\n                            keep.append(parent[i])\n                        else:\n                            remove.add(id)\n                    if len(keep) == 0:\n                        continue\n                    parent = g.set_attr(\"Parent\", keep)\n\n        if remove_attrs:\n            for remove_attr in remove_attrs:\n                if remove_attr in g.attributes:\n                    g.set_attr(remove_attr, None)\n\n        if opts.verifySO:\n            if g.type not in valid_soterm:\n                valid_soterm[g.type] = validate_term(g.type, so=so, method=opts.verifySO)\n            ntype = valid_soterm[g.type]\n            if ntype and g.type != ntype:\n                g.type = ntype\n\n        origid = g.seqid\n        if fixphase:\n            phase = g.phase\n            g.phase = phaseT.get(phase, phase)\n\n        if mapfile:\n            if isinstance(mapping, dict):\n                if origid in mapping:\n                    g.seqid = mapping[origid]\n                else:\n                    logging.error(\"{0} not found in `{1}`. ID unchanged.\".\\\n                            format(origid, mapfile))\n            else:\n                g.seqid = mapfile\n\n        if source:\n            if isinstance(source, dict) and g.source in source:\n                g.source = source[g.source]\n            else:\n                g.source = source\n\n        if names:\n            if id in names:\n                g.set_attr(\"Name\", names[id])\n\n        if note:\n            name = g.get_attr(\"Name\")\n            tag = None\n            if id in note:\n                tag = note[id]\n            elif name and name in note:\n                tag = note[name]\n\n            if tag:\n                g.set_attr(\"Note\", tag, update=False)\n\n        if attrib_files:\n            for attr_name in attr_values:\n                name = g.get_attr(\"Name\")\n                if id in attr_values[attr_name]:\n                    g.set_attr(attr_name, attr_values[attr_name][id])\n                elif name and name in attr_values[attr_name]:\n                    g.set_attr(attr_name, attr_values[attr_name][name])\n\n        if dbxref_files:\n            for dbtag in dbxref_values:\n                if id in dbxref_values[dbtag]:\n                    g.set_attr(\"Dbxref\", dbxref_values[dbtag][id], dbtag=dbtag, append=True)\n\n        if unique:\n            if dupcounts[id] > 1:\n                seen[id] += 1\n                old_id = id\n                id = \"{0}-{1}\".format(old_id, seen[old_id])\n                newparentid[old_id] = id\n                g.set_attr(\"ID\", id)\n\n            if \"Parent\" in g.attributes:\n                parent = g.attributes[\"Parent\"][0]\n                if dupcounts[parent] > 1:\n                    g.set_attr(\"Parent\", newparentid[parent])\n\n        if duptype:\n            if duptype == g.type and len(dupranges[g.seqid][id]) > 1:\n                p = sorted(dupranges[g.seqid][id])\n                s, e = dupranges[g.seqid][id][p[0]][0:2]  # get coords of first encountered feature\n                if g.start == s and g.end == e and p[0] == g.idx:\n                    r = [dupranges[g.seqid][id][x] for x in dupranges[g.seqid][id]]\n                    g.start, g.end = range_minmax(r)\n                else:\n                    skip[(g.seqid, g.idx, id, g.start, g.end)] = 1\n\n        if gsac and g.type == \"gene\":\n            notes[id] = g.attributes[\"Name\"]\n\n        if ftype:\n            if isinstance(ftype, dict) and g.type in ftype:\n                g.type = ftype[g.type]\n            else:\n                g.type = ftype\n\n        if invent_name_attr:\n            ft.store_symbol(g)\n            if re.search(ft.ftype, g.type):\n                parent, iso = atg_name(g.get_attr(\"Parent\"), retval=\"locus,iso\")\n                if not parent:\n                    parent = g.get_attr(\"Parent\")\n                if parent in ft.tracker:\n                    fidx = ft.feat_index(parent, g.type, g.strand, (g.start, g.end, g.sign))\n                    symbol = ft.get_symbol(parent)\n                    attr = \"ID\" if symbol == parent else \"Name\"\n                    g.set_attr(attr, \"{0}:{1}:{2}\".format(symbol, g.type, fidx + 1))\n                    if opts.multiparents == \"merge\" and attr == \"Name\":\n                        g.set_attr(\"ID\", \"{0}:{1}:{2}\".format(parent, g.type, fidx + 1))\n        elif copy_id_attr_to_name:\n            if \"Name\" not in g.attributes.keys():\n                g.set_attr(\"Name\", g.get_attr(\"ID\"))\n\n        protein_feat = None\n        if invent_protein_feat:\n            if g.type == 'mRNA':\n                if id in cds_track:\n                    pstart, pstop = range_minmax(cds_track[id])\n                    protein_feat = GffLine(\"\\t\".join(str(x) for x in [g.seqid, g.source, \"protein\", pstart, pstop, \\\n                            \".\", g.strand, \".\", \"ID={0}-Protein;Name={0};Derives_from={0}\".format(id)]))\n            elif g.type == 'CDS':\n                parent = g.get_attr(\"Parent\")\n                if parent in cds_track:\n                    _parent = [parent, \"{0}-Protein\".format(parent)]\n                    g.set_attr(\"Parent\", _parent)\n\n        pp = g.get_attr(\"Parent\", first=False)\n        if opts.multiparents == \"split\" and (pp and len(pp) > 1) and g.type != \"CDS\":  # separate features with multiple parents\n            id = g.get_attr(\"ID\")\n            for i, parent in enumerate(pp):\n                if id: g.set_attr(\"ID\", \"{0}-{1}\".format(id, i + 1))\n                g.set_attr(\"Parent\", parent, update=True, urlquote=True)\n                if gsac:\n                    fix_gsac(g, notes)\n                print(g, file=fw)\n        else:\n            if g.gff3 and not opts.gff3:\n                opts.gff3 = True\n            g.update_attributes(gff3=opts.gff3)\n            if gsac:\n                fix_gsac(g, notes)\n            if duptype == g.type and skip[(g.seqid, g.idx, id, g.start, g.end)] == 1:\n                continue\n            print(g, file=fw)\n            if g.type == 'mRNA' and invent_protein_feat:\n                print(protein_feat, file=fw)\n\n    fw.close()", "response": "%prog format gffile > formatted. gff\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_piles(allgenes):\n    from jcvi.utils.range import Range, range_piles\n\n    ranges = [Range(a.seqid, a.start, a.end, 0, i) \\\n                    for i, a in enumerate(allgenes)]\n\n    for pile in range_piles(ranges):\n        yield [allgenes[x] for x in pile]", "response": "Yields all piles from allgenes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef match_subfeats(f1, f2, dbx1, dbx2, featuretype=None, slop=False):\n    f1c, f2c = list(dbx1.children(f1, featuretype=featuretype, order_by='start')), \\\n            list(dbx2.children(f2, featuretype=featuretype, order_by='start'))\n\n    lf1c, lf2c = len(f1c), len(f2c)\n    if match_nchildren(f1c, f2c):\n        if lf1c > 0 and lf2c > 0:\n            exclN = set()\n            if featuretype.endswith('UTR') or featuretype == 'exon':\n                N = []\n                if featuretype.startswith('five_prime'):\n                    N = [1] if f1.strand == \"+\" else [lf1c]\n                elif featuretype.startswith('three_prime'):\n                    N = [lf1c] if f1.strand == \"+\" else [1]\n                else:   # infer UTR from exon collection\n                    N = [1] if 1 == lf1c else [1, lf1c]\n\n                for n in N:\n                    if match_Nth_child(f1c, f2c, N=n, slop=slop):\n                        exclN.add(n-1)\n                    else:\n                        return False\n\n            for i, (cf1, cf2) in enumerate(zip(f1c, f2c)):\n                if i in exclN: continue\n                if not match_span(cf1, cf2):\n                    return False\n    else:\n        if (lf1c, lf2c) in [(0, 1), (1, 0)] and slop \\\n                and featuretype.endswith('UTR'):\n            return True\n\n        return False\n\n    return True", "response": "Given 2 gffutils features located in 2 separate gffutils databases f1 f2 iterate through all subfeatures and check whether they are identical or not"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fromgtf(args):\n    p = OptionParser(fromgtf.__doc__)\n    p.add_option(\"--transcript_id\", default=\"transcript_id\",\n                 help=\"Field name for transcript [default: %default]\")\n    p.add_option(\"--gene_id\", default=\"gene_id\",\n                 help=\"Field name for gene [default: %default]\")\n    p.add_option(\"--augustus\", default=False, action=\"store_true\",\n                 help=\"Input is AUGUSTUS gtf [default: %default]\")\n    p.set_home(\"augustus\")\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    gtffile, = args\n    outfile = opts.outfile\n    if opts.augustus:\n        ahome = opts.augustus_home\n        s = op.join(ahome, \"scripts/gtf2gff.pl\")\n        cmd = \"{0} --gff3 < {1} --out={2}\".format(s, gtffile, outfile)\n        sh(cmd)\n        return\n\n    gff = Gff(gtffile)\n    fw = must_open(outfile, \"w\")\n    transcript_id = opts.transcript_id\n    gene_id = opts.gene_id\n    nfeats = 0\n    for g in gff:\n        if g.type in (\"transcript\", \"mRNA\"):\n            g.type = \"mRNA\"\n            g.update_tag(transcript_id, \"ID\")\n            g.update_tag(\"mRNA\", \"ID\")\n            g.update_tag(gene_id, \"Parent\")\n            g.update_tag(\"Gene\", \"Parent\")\n        elif g.type in (\"exon\", \"CDS\") or \"UTR\" in g.type:\n            g.update_tag(\"transcript_id\", \"Parent\")\n            g.update_tag(g.type, \"Parent\")\n        elif g.type == \"gene\":\n            g.update_tag(gene_id, \"ID\")\n            g.update_tag(\"Gene\", \"ID\")\n        else:\n            assert 0, \"Don't know how to deal with {0}\".format(g.type)\n\n        g.update_attributes()\n        print(g, file=fw)\n        nfeats += 1\n\n    logging.debug(\"A total of {0} features written.\".format(nfeats))", "response": "%prog fromgtf gtffile > new_base_list"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef frombed(args):\n    p = OptionParser(frombed.__doc__)\n    p.add_option(\"--type\", default=\"match\",\n                 help=\"GFF feature type [default: %default]\")\n    p.add_option(\"--source\", default=\"default\",\n                help=\"GFF source qualifier [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    bedfile, = args\n    bed = Bed(bedfile)\n\n    for b in bed:\n        print(b.gffline(type=opts.type, source=opts.source))", "response": "%prog frombed bedfile > gff_file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gtf(args):\n    p = OptionParser(gtf.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    gffile, = args\n    gff = Gff(gffile)\n    transcript_info = AutoVivification()\n    for g in gff:\n        if g.type.endswith((\"RNA\", \"transcript\")):\n            if \"ID\" in g.attributes and \"Parent\" in g.attributes:\n                transcript_id = g.get_attr(\"ID\")\n                gene_id = g.get_attr(\"Parent\")\n            elif \"mRNA\" in g.attributes and \"Gene\" in g.attributes:\n                transcript_id = g.get_attr(\"mRNA\")\n                gene_id = g.get_attr(\"Gene\")\n            else:\n                transcript_id = g.get_attr(\"ID\")\n                gene_id = transcript_id\n            transcript_info[transcript_id][\"gene_id\"] = gene_id\n            transcript_info[transcript_id][\"gene_type\"] = g.type\n            continue\n\n        if g.type not in valid_gff_to_gtf_type.keys():\n            continue\n\n        try:\n            transcript_id = g.get_attr(\"Parent\", first=False)\n        except IndexError:\n            transcript_id = g.get_attr(\"mRNA\", first=False)\n\n        g.type = valid_gff_to_gtf_type[g.type]\n        for tid in transcript_id:\n            if tid not in transcript_info: continue\n            gene_type = transcript_info[tid][\"gene_type\"]\n            if not gene_type.endswith(\"RNA\") and not gene_type.endswith(\"transcript\"):\n                continue\n            gene_id = transcript_info[tid][\"gene_id\"]\n            g.attributes = dict(gene_id=[gene_id], transcript_id=[tid])\n            g.update_attributes(gtf=True, urlquote=False)\n\n            print(g)", "response": "%prog gtf gffile\n\n    Convert gff to gtf file. In gtf, only exon/CDS features are important. The\n    first 8 columns are the same as gff, but in the attributes field, we need to\n    specify \"gene_id\" and \"transcript_id\"."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef splicecov(args):\n    from tempfile import mkstemp\n    from pybedtools import BedTool\n    from jcvi.utils.cbook import SummaryStats\n\n    p = OptionParser(splicecov.__doc__)\n    p.set_outfile()\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    gfffile, juncsbed, = args\n    tagged = \"{0}.{1}.gff3\".format(gfffile.rsplit(\".\", 1)[0], \"tag_introns\")\n\n    gff3, junc = BedTool(gfffile), BedTool(juncsbed)\n    ab = gff3.intersect(junc, wao=True, f=1.0, s=True)\n    abfh = must_open(ab.fn)\n\n    seen = set()\n    scov = AutoVivification()\n\n    fh, tmpgff = mkstemp(suffix=\".gff3\")\n    fw = must_open(tmpgff, \"w\")\n    for line in abfh:\n        args = line.strip().split(\"\\t\")\n        g = GffLine(\"\\t\".join(str(x) for x in args[:9]))\n        if g.type == \"intron\" and args[10] != -1:\n            ispan, jspan = g.span, int(args[11]) - int(args[10])\n            if ispan == jspan:\n                g.set_attr(\"ID\", args[12], update=True)\n                g.score = int(args[13])\n\n                pparts = g.get_attr(\"Parent\").split(\".\")\n                locus, iso = pparts[0], \".\".join(pparts[1:])\n                seen.add(iso)\n                if not scov[locus][iso]:\n                    scov[locus][iso] = []\n                scov[locus][iso].append(g.score)\n            else:\n                continue\n        print(g, file=fw)\n    fw.close()\n\n    format([tmpgff, \"--unique\", \"-o\", tagged])\n    os.unlink(tmpgff)\n\n    isos = sorted(list(seen))\n    fw = must_open(opts.outfile, \"w\")\n    h1, h2, stats = [\"#\"], [\"#locus\"], [\"N\", \"mean\", \"median\", \"min\", \"max\"]\n    for iso in isos:\n        h1.extend([str(iso)] + [\"\"] * (len(stats) - 1))\n        h2.extend(stats)\n    print(\"\\t\".join(str(x) for x in h1), file=fw)\n    print(\"\\t\".join(str(x) for x in h2), file=fw)\n    for locus in scov.keys():\n        out = [locus]\n        for iso in isos:\n            if iso in scov[locus].keys():\n                juncs = scov[locus][iso]\n                jstats = SummaryStats(juncs, dtype=\"int\")\n                out.extend([jstats.size, jstats.mean, jstats.median, \\\n                        jstats.min, jstats.max])\n            else:\n                out.extend([\"-\"] * len(stats))\n        print(\"\\t\".join(str(x) for x in out), file=fw)\n    fw.close()", "response": "%prog splicecov annotation. gff3 juncs. bed"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_index(gff_file):\n    import gffutils\n    db_file = gff_file + \".db\"\n\n    if need_update(gff_file, db_file):\n        if op.exists(db_file):\n            os.remove(db_file)\n        logging.debug(\"Indexing `{0}`\".format(gff_file))\n        gffutils.create_db(gff_file, db_file, merge_strategy=\"create_unique\")\n    else:\n        logging.debug(\"Load index `{0}`\".format(gff_file))\n\n    return gffutils.FeatureDB(db_file)", "response": "Make a sqlite database for fast retrieval of features."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load(args):\n    '''\n    %prog load gff_file fasta_file [--options]\n\n    Parses the selected features out of GFF, with subfeatures concatenated.\n    For example, to get the CDS sequences, do this:\n    $ %prog load athaliana.gff athaliana.fa --parents mRNA --children CDS\n\n    To get 500bp upstream of a genes Transcription Start Site (TSS), do this:\n    $ %prog load athaliana.gff athaliana.fa --feature=upstream:TSS:500\n\n    Switch TSS with TrSS for Translation Start Site.\n    '''\n    from datetime import datetime as dt\n    from jcvi.formats.fasta import Seq, SeqRecord\n\n    # can request output fasta sequence id to be picked from following attributes\n    valid_id_attributes = [\"ID\", \"Name\", \"Parent\", \"Alias\", \"Target\"]\n\n    p = OptionParser(load.__doc__)\n    p.add_option(\"--parents\", dest=\"parents\", default=\"mRNA\",\n            help=\"list of features to extract, use comma to separate (e.g.\" + \\\n            \"'gene,mRNA') [default: %default]\")\n    p.add_option(\"--children\", dest=\"children\", default=\"CDS\",\n            help=\"list of features to extract, use comma to separate (e.g.\" + \\\n            \"'five_prime_UTR,CDS,three_prime_UTR') [default: %default]\")\n    p.add_option(\"--feature\", dest=\"feature\",\n            help=\"feature type to extract. e.g. `--feature=CDS` or \" + \\\n            \"`--feature=upstream:TSS:500` [default: %default]\")\n    p.add_option(\"--id_attribute\", choices=valid_id_attributes,\n            help=\"The attribute field to extract and use as FASTA sequence ID \" + \\\n            \"[default: %default]\")\n    p.add_option(\"--desc_attribute\", default=\"Note\",\n            help=\"The attribute field to extract and use as FASTA sequence \" + \\\n            \"description [default: %default]\")\n    p.add_option(\"--full_header\", default=None, choices=[\"default\", \"tair\"],\n            help=\"Specify if full FASTA header (with seqid, coordinates and datestamp)\" + \\\n            \" should be generated [default: %default]\")\n\n    g1 = OptionGroup(p, \"Optional parameters (if generating full header)\")\n    g1.add_option(\"--sep\", dest=\"sep\", default=\" \", \\\n            help=\"Specify separator used to delimiter header elements [default: \\\"%default\\\"]\")\n    g1.add_option(\"--datestamp\", dest=\"datestamp\", \\\n            help=\"Specify a datestamp in the format YYYYMMDD or automatically pick `today`\" + \\\n            \" [default: %default]\")\n    g1.add_option(\"--conf_class\", dest=\"conf_class\", default=False, action=\"store_true\",\n            help=\"Specify if `conf_class` attribute should be parsed and placed in the header\" + \\\n            \" [default: %default]\")\n    p.add_option_group(g1)\n\n    p.set_outfile()\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(p.print_help())\n\n    gff_file, fasta_file = args\n\n    if opts.feature:\n        opts.feature, opts.parent, opts.children, upstream_site, upstream_len, \\\n                flag, error_msg = parse_feature_param(opts.feature)\n        if flag:\n            sys.exit(error_msg)\n\n    parents = set(opts.parents.split(','))\n    children_list = set(opts.children.split(','))\n\n    \"\"\"\n    In a situation where we want to extract sequence for only the top-level\n    parent feature, specify feature type of parent == child\n    \"\"\"\n    skipChildren = True if len(parents.symmetric_difference(children_list)) == 0 \\\n            else False\n\n    id_attr = opts.id_attribute\n    desc_attr = opts.desc_attribute\n    sep = opts.sep\n\n    import gffutils\n    g = make_index(gff_file)\n    f = Fasta(fasta_file, index=False)\n    seqlen = {}\n    for seqid, size in f.itersizes():\n        seqlen[seqid] = size\n\n    fw = must_open(opts.outfile, \"w\")\n\n    for feat in get_parents(gff_file, parents):\n        desc = \"\"\n        if desc_attr:\n            fparent = feat.attributes['Parent'][0] \\\n                if 'Parent' in feat.attributes else None\n            if fparent:\n                try:\n                    g_fparent = g[fparent]\n                except gffutils.exceptions.FeatureNotFoundError:\n                    logging.error(\"{} not found in index .. skipped\".format(fparent))\n                    continue\n                if desc_attr in g_fparent.attributes:\n                    desc = \",\".join(g_fparent.attributes[desc_attr])\n            elif desc_attr in feat.attributes:\n                desc = \",\".join(feat.attributes[desc_attr])\n\n        if opts.full_header:\n            desc_parts = []\n            desc_parts.append(desc)\n\n            if opts.conf_class and 'conf_class' in feat.attributes:\n                desc_parts.append(feat.attributes['conf_class'][0])\n\n            if opts.full_header == \"tair\":\n                orient = \"REVERSE\" if feat.strand == \"-\" else \"FORWARD\"\n                feat_coords = \"{0}:{1}-{2} {3} LENGTH=[LEN]\".format(feat.seqid, \\\n                    feat.start, feat.end, orient)\n            else:\n                (s, e) = (feat.start, feat.end) if (feat.strand == \"+\") \\\n                        else (feat.end, feat.start)\n                feat_coords = \"{0}:{1}-{2}\".format(feat.seqid, s, e)\n            desc_parts.append(feat_coords)\n\n            datestamp = opts.datestamp if opts.datestamp else \\\n                    \"{0}{1}{2}\".format(dt.now().year, dt.now().month, dt.now().day)\n            desc_parts.append(datestamp)\n\n            desc = sep.join(str(x) for x in desc_parts)\n            desc = \"\".join(str(x) for x in (sep, desc)).strip()\n\n        if opts.feature == \"upstream\":\n            upstream_start, upstream_stop = get_upstream_coords(upstream_site, upstream_len, \\\n                     seqlen[feat.seqid], feat, children_list, g)\n\n            if not upstream_start or not upstream_stop:\n                continue\n\n            feat_seq = f.sequence(dict(chr=feat.seqid, start=upstream_start,\n                stop=upstream_stop, strand=feat.strand))\n\n            (s, e) = (upstream_start, upstream_stop) \\\n                    if feat.strand == \"+\" else \\\n                     (upstream_stop, upstream_start)\n            upstream_seq_loc = str(feat.seqid) + \":\" + str(s) + \"-\" + str(e)\n            desc = sep.join(str(x) for x in (desc, upstream_seq_loc, \\\n                    \"FLANKLEN=\" + str(upstream_len)))\n        else:\n            children = []\n            if not skipChildren:\n                for c in g.children(feat.id, 1):\n                    if c.featuretype not in children_list:\n                        continue\n                    child = f.sequence(dict(chr=c.chrom, start=c.start, stop=c.stop,\n                        strand=c.strand))\n                    children.append((child, c))\n\n                if not children:\n                    print(\"[warning] %s has no children with type %s\" \\\n                                            % (feat.id, ','.join(children_list)), file=sys.stderr)\n                    continue\n            else:\n                child = f.sequence(dict(chr=feat.seqid, start=feat.start, stop=feat.end,\n                    strand=feat.strand))\n                children.append((child, feat))\n\n            # sort children in incremental position\n            children.sort(key=lambda x: x[1].start)\n            # reverse children if negative strand\n            if feat.strand == '-':\n                children.reverse()\n            feat_seq = ''.join(x[0] for x in children)\n\n        desc = desc.replace(\"\\\"\", \"\")\n\n        id = \",\".join(feat.attributes[id_attr]) if id_attr \\\n                and feat.attributes[id_attr] else \\\n                feat.id\n\n        if opts.full_header == \"tair\":\n            desc = desc.replace(\"[LEN]\", str(len(feat_seq)))\n\n        rec = SeqRecord(Seq(feat_seq), id=id, description=desc)\n        SeqIO.write([rec], fw, \"fasta\")\n        fw.flush()", "response": "%prog load athaliana. gff athaliana. fa athaliana. seq"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the parameter of a GFF file into a base - level structure.", "response": "def parse_feature_param(feature):\n    \"\"\"\n    Take the --feature param (coming from gff.load() and parse it.\n    Returns feature, parents and children terms.\n\n    Also returns length of upstream sequence (and start site) requested\n\n    If erroneous, returns a flag and error message to be displayed on exit\n    \"\"\"\n    # can request upstream sequence only from the following valid sites\n    valid_upstream_sites = [\"TSS\", \"TrSS\"]\n\n    upstream_site, upstream_len = None, None\n    flag, error_msg = None, None\n    parents, children = None, None\n    if re.match(r'upstream', feature):\n        parents, children = \"mRNA\", \"CDS\"\n        feature, upstream_site, upstream_len = re.search(r'([A-z]+):([A-z]+):(\\S+)', \\\n                feature).groups()\n\n        if not is_number(upstream_len):\n            flag, error_msg = 1, \"Error: upstream len `\" + upstream_len + \"` should be an integer\"\n\n        upstream_len = int(upstream_len)\n        if(upstream_len < 0):\n            flag, error_msg = 1, \"Error: upstream len `\" + str(upstream_len) + \"` should be > 0\"\n\n        if not upstream_site in valid_upstream_sites:\n            flag, error_msg = 1, \"Error: upstream site `\" + upstream_site + \"` not valid.\" + \\\n                    \" Please choose from \" + valid_upstream_sites\n    elif feature == \"CDS\":\n        parents, children = \"mRNA\", \"CDS\"\n    else:\n        flag, error_msg = 1, \"Error: unrecognized option --feature=\" + feature\n\n    return feature, parents, children, upstream_site, upstream_len, flag, error_msg"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_upstream_coords(uSite, uLen, seqlen, feat, children_list, gffdb):\n    if uSite == \"TSS\":\n        (upstream_start, upstream_stop) = \\\n                (feat.start - uLen, feat.start - 1) \\\n                if feat.strand == \"+\" else \\\n                (feat.end + 1, feat.end + uLen)\n    elif uSite == \"TrSS\":\n        children = []\n        for c in gffdb.children(feat.id, 1):\n\n            if c.featuretype not in children_list:\n                continue\n            children.append((c.start, c.stop))\n\n        if not children:\n            print(\"[warning] %s has no children with type %s\" \\\n                                    % (feat.id, ','.join(children_list)), file=sys.stderr)\n            return None, None\n\n        cds_start, cds_stop = range_minmax(children)\n        (upstream_start, upstream_stop) = \\\n                (cds_start - uLen, cds_start - 1) \\\n                if feat.strand == \"+\" else \\\n                (cds_stop + 1, cds_stop + uLen)\n\n    if feat.strand == \"+\" and upstream_start < 1:\n        upstream_start = 1\n    elif feat.strand == \"-\" and upstream_stop > seqlen:\n        upstream_stop = seqlen\n\n    actual_uLen = upstream_stop - upstream_start + 1\n    if actual_uLen < uLen:\n        print(\"[warning] sequence upstream of {0} ({1} bp) is less than upstream length {2}\" \\\n                .format(feat.id, actual_uLen, uLen), file=sys.stderr)\n        return None, None\n\n    return upstream_start, upstream_stop", "response": "This function extracts the upstream start and stop coordinates of a single mRNA feature from the list of children of the child mRNA features and returns the upstream start and stop coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bed12(args):\n    p = OptionParser(bed12.__doc__)\n    p.add_option(\"--parent\", default=\"mRNA\",\n            help=\"Top feature type [default: %default]\")\n    p.add_option(\"--block\", default=\"exon\",\n            help=\"Feature type for regular blocks [default: %default]\")\n    p.add_option(\"--thick\", default=\"CDS\",\n            help=\"Feature type for thick blocks [default: %default]\")\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    gffile, = args\n    parent, block, thick = opts.parent, opts.block, opts.thick\n    outfile = opts.outfile\n\n    g = make_index(gffile)\n    fw = must_open(outfile, \"w\")\n\n    for f in g.features_of_type(parent):\n\n        chrom = f.chrom\n        chromStart = f.start - 1\n        chromEnd = f.stop\n        name = f.id\n        score = 0\n        strand = f.strand\n        thickStart = 1e15\n        thickEnd = 0\n        blocks = []\n\n        for c in g.children(name, 1):\n\n            cstart, cend = c.start - 1, c.stop\n\n            if c.featuretype == block:\n                blockStart = cstart - chromStart\n                blockSize = cend - cstart\n                blocks.append((blockStart, blockSize))\n\n            elif c.featuretype == thick:\n                thickStart = min(thickStart, cstart)\n                thickEnd = max(thickEnd, cend)\n\n        blocks.sort()\n        blockStarts, blockSizes = zip(*blocks)\n        blockCount = len(blocks)\n        blockSizes = \",\".join(str(x) for x in blockSizes) + \",\"\n        blockStarts = \",\".join(str(x) for x in blockStarts) + \",\"\n        itemRgb = 0\n\n        print(\"\\t\".join(str(x) for x in (chrom, chromStart, chromEnd, \\\n                name, score, strand, thickStart, thickEnd, itemRgb,\n                blockCount, blockSizes, blockStarts)), file=fw)", "response": "%prog bed12 gffile > bedfile\n\n    Produce bed12 file for coding features. The exons will be converted to blocks.\n    The CDS range will be shown between thickStart to thickEnd. For reference,\n    bed format consists of the following fields:\n\n    1. chrom\n    2. chromStart\n    3. chromEnd\n    4. name\n    5. score\n    6. strand\n    7. thickStart\n    8. thickEnd\n    9. itemRgb\n    10. blockCount\n    11. blockSizes\n    12. blockStarts"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a unique signature for any GFF line based on joining the columns 1 2 3 4 5 6 7 8", "response": "def signature(self):\n        \"\"\"\n        create a unique signature for any GFF line based on joining\n        columns 1,2,3,4,5,7,8 (into a comma separated string)\n        \"\"\"\n        sig_elems = [self.seqid, self.source, self.type, \\\n                    self.start, self.end, self.strand, \\\n                    self.phase]\n        if re.search(\"exon|CDS|UTR\", self.type):\n            parent = self.get_attr(\"Parent\")\n            if parent:\n                (locus, iso) = atg_name(parent, retval=\"locus,iso\", \\\n                        trimpad0=False)\n                if locus:\n                    sig_elems.append(locus)\n        else:\n            sig_elems.extend([self.accn])\n\n        return \",\".join(str(elem) for elem in sig_elems)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef blast(args):\n    '''\n    %prog blast maffiles > out.blast\n\n    From a folder of .maf files, generate .blast file with tabular format.\n    '''\n    p = OptionParser(blast.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) == 0:\n        sys.exit(p.print_help())\n\n    flist = args\n\n    for f in flist:\n        maf_to_blast8(f)", "response": "%prog blast maffiles > out. blast\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild an index file from a MAF file.", "response": "def build_index(self, filename, indexfile):\n        \"\"\"\n        Recipe from Brad Chapman's blog\n        <http://bcbio.wordpress.com/2009/07/26/sorting-genomic-alignments-using-python/>\n        \"\"\"\n        indexes = interval_index_file.Indexes()\n        in_handle = open(filename)\n\n        reader = maf.Reader(in_handle)\n        while True:\n            pos = reader.file.tell()\n            rec = next(reader)\n            if rec is None:\n                break\n            for c in rec.components:\n                indexes.add(c.src, c.forward_strand_start,\n                        c.forward_strand_end, pos, max=c.src_size )\n\n        index_handle = open(indexfile, \"w\")\n        indexes.write(index_handle)\n        index_handle.close()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the mutants created by inversion mutation on the candidates.", "response": "def genome_mutation(candidate):\n    \"\"\"Return the mutants created by inversion mutation on the candidates.\n\n    This function performs inversion or insertion. It randomly chooses two\n    locations along the candidate and reverses the values within that\n    slice. Insertion is done by popping one item and insert it back at random\n    position.\n    \"\"\"\n    size = len(candidate)\n    prob = random.random()\n    if prob > .5:    # Inversion\n        p = random.randint(0, size-1)\n        q = random.randint(0, size-1)\n        if p > q:\n            p, q = q, p\n        q += 1\n        s = candidate[p:q]\n        x = candidate[:p] + s[::-1] + candidate[q:]\n        return creator.Individual(x),\n    else:            # Insertion\n        p = random.randint(0, size-1)\n        q = random.randint(0, size-1)\n        cq = candidate.pop(q)\n        candidate.insert(p, cq)\n        return candidate,"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef frombed(args):\n    from jcvi.formats.fasta import Fasta\n    from jcvi.formats.bed import Bed\n    from jcvi.utils.cbook import fill\n\n    p = OptionParser(frombed.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    bedfile, contigfasta, readfasta = args\n    prefix = bedfile.rsplit(\".\", 1)[0]\n    contigfile = prefix + \".contig\"\n    idsfile = prefix + \".ids\"\n\n    contigfasta = Fasta(contigfasta)\n    readfasta = Fasta(readfasta)\n\n    bed = Bed(bedfile)\n    checksum = \"00000000 checksum.\"\n    fw_ids = open(idsfile, \"w\")\n    fw = open(contigfile, \"w\")\n\n    for ctg, reads in bed.sub_beds():\n        ctgseq = contigfasta[ctg]\n        ctgline = \"##{0} {1} {2} bases, {3}\".format(\\\n                ctg, len(reads), len(ctgseq), checksum)\n\n        print(ctg, file=fw_ids)\n        print(ctgline, file=fw)\n        print(fill(ctgseq.seq), file=fw)\n\n        for b in reads:\n            read = b.accn\n            strand = b.strand\n            readseq = readfasta[read]\n            rc = \" [RC]\" if strand == \"-\" else \"\"\n            readlen = len(readseq)\n            rstart, rend = 1, readlen\n            if strand == \"-\":\n                rstart, rend = rend, rstart\n\n            readrange = \"{{{0} {1}}}\".format(rstart, rend)\n            conrange = \"<{0} {1}>\".format(b.start, b.end)\n            readline = \"#{0}(0){1} {2} bases, {3} {4} {5}\".format(\\\n                    read, rc, readlen, checksum, readrange, conrange)\n            print(readline, file=fw)\n            print(fill(readseq.seq), file=fw)\n\n    logging.debug(\"Mapped contigs written to `{0}`.\".format(contigfile))\n    logging.debug(\"Contig IDs written to `{0}`.\".format(idsfile))", "response": "%prog frombed contigfasta readfasta"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_tesseract(input_filename, output_filename_base, lang=None, boxes=False):\n    '''\n    runs the command:\n        `tesseract_cmd` `input_filename` `output_filename_base`\n\n    returns the exit status of tesseract, as well as tesseract's stderr output\n\n    '''\n\n    command = [tesseract_cmd, input_filename, output_filename_base]\n\n    if lang is not None:\n        command += ['-l', lang]\n\n    if boxes:\n        command += ['batch.nochop', 'makebox']\n\n    proc = subprocess.Popen(command,\n                            stderr=subprocess.PIPE)\n    return (proc.wait(), proc.stderr.read())", "response": "Runs the tesseract command input_filename output_filename_base returns the exit status of tesseract s stderr output_filename_base returns the exit status of tesseract s stderr"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn all lines in the error_string that start with the string Error", "response": "def get_errors(error_string):\n    '''\n    returns all lines in the error_string that start with the string \"error\"\n\n    '''\n\n    lines = error_string.splitlines()\n    error_lines = tuple(line for line in lines if line.find('Error') >= 0)\n    if len(error_lines) > 0:\n        return '\\n'.join(error_lines)\n    else:\n        return error_string.strip()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a temporary file - name", "response": "def tempnam():\n    ''' returns a temporary file-name '''\n\n    # prevent os.tmpname from printing an error...\n    stderr = sys.stderr\n    try:\n        sys.stderr = cStringIO.StringIO()\n        return os.tempnam(None, 'tess_')\n    finally:\n        sys.stderr = stderr"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef image_to_string(image, lang=None, boxes=False):\n    '''\n    Runs tesseract on the specified image. First, the image is written to disk,\n    and then the tesseract command is run on the image. Resseract's result is\n    read, and the temporary files are erased.\n\n    '''\n\n    input_file_name = '%s.bmp' % tempnam()\n    output_file_name_base = tempnam()\n    if not boxes:\n        output_file_name = '%s.txt' % output_file_name_base\n    else:\n        output_file_name = '%s.box' % output_file_name_base\n    try:\n        image.save(input_file_name)\n        status, error_string = run_tesseract(input_file_name,\n                                             output_file_name_base,\n                                             lang=lang,\n                                             boxes=boxes)\n        if status:\n            errors = get_errors(error_string)\n            raise TesseractError(status, errors)\n        f = file(output_file_name)\n        try:\n            return f.read().strip()\n        finally:\n            f.close()\n    finally:\n        cleanup(input_file_name)\n        cleanup(output_file_name)", "response": "Runs tesseract on the specified image and returns the string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mitocompile(args):\n    from jcvi.formats.vcf import VcfLine\n    from six.moves.urllib.parse import parse_qsl\n\n    p = OptionParser(mitocompile.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    vcfs = args\n    print(\"\\t\".join(\"vcf samplekey depth seqid pos alt svlen pe sr\".split()))\n    for i, vcf in enumerate(vcfs):\n        if (i + 1) % 100 == 0:\n            logging.debug(\"Process `{}` [{}]\".\n                          format(vcf, percentage(i + 1, len(vcfs))))\n        depthfile = vcf.replace(\".sv.vcf.gz\", \".depth\")\n        fp = must_open(depthfile)\n        chrm, depth = fp.next().split()\n        depth = int(float(depth))\n        samplekey = op.basename(vcf).split(\"_\")[0]\n\n        fp = must_open(vcf)\n        for row in fp:\n            if row[0] == '#':\n                continue\n            v = VcfLine(row)\n            info = dict(parse_qsl(v.info))\n            print(\"\\t\".join(str(x) for x in (vcf,\n                                             samplekey, depth,\n                                             v.seqid, v.pos, v.alt,\n                                             info.get(\"SVLEN\"), info[\"PE\"], info[\"SR\"])))", "response": "%prog mitcompile vcf. gz"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mito(args):\n    p = OptionParser(mito.__doc__)\n    p.set_aws_opts(store=\"hli-mv-data-science/htang/mito-deletions\")\n    p.add_option(\"--realignonly\", default=False, action=\"store_true\",\n                 help=\"Realign only\")\n    p.add_option(\"--svonly\", default=False, action=\"store_true\",\n                 help=\"Run Realign => SV calls only\")\n    p.add_option(\"--support\", default=1, type=\"int\",\n                 help=\"Minimum number of supporting reads\")\n    p.set_home(\"speedseq\", default=\"/mnt/software/speedseq/bin\")\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    chrMfa, bamfile = args\n    store = opts.output_path\n    cleanup = not opts.nocleanup\n\n    if not op.exists(chrMfa):\n        logging.debug(\"File `{}` missing. Exiting.\".format(chrMfa))\n        return\n\n    chrMfai = chrMfa + \".fai\"\n    if not op.exists(chrMfai):\n        cmd = \"samtools index {}\".format(chrMfa)\n        sh(cmd)\n\n    if not bamfile.endswith(\".bam\"):\n        bamfiles = [x.strip() for x in open(bamfile)]\n    else:\n        bamfiles = [bamfile]\n\n    if store:\n        computed = ls_s3(store)\n        computed = [op.basename(x).split('.')[0] for x in computed if\n                    x.endswith(\".depth\")]\n        remaining_samples = [x for x in bamfiles\n                             if op.basename(x).split(\".\")[0] not in computed]\n\n        logging.debug(\"Already computed on `{}`: {}\".\n                      format(store, len(bamfiles) - len(remaining_samples)))\n        bamfiles = remaining_samples\n\n    logging.debug(\"Total samples: {}\".format(len(bamfiles)))\n\n    for bamfile in bamfiles:\n        run_mito(chrMfa, bamfile, opts,\n                 realignonly=opts.realignonly,\n                 svonly=opts.svonly,\n                 store=store, cleanup=cleanup)", "response": "%prog mito chrM. fa bamfile"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fcs(args):\n    p = OptionParser(fcs.__doc__)\n    p.add_option(\"--cutoff\", default=200,\n                 help=\"Skip small components less than [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    fcsfile, = args\n    cutoff = opts.cutoff\n    fp = open(fcsfile)\n    for row in fp:\n        if row[0] == \"#\":\n            continue\n\n        sep = \"\\t\" if \"\\t\" in row else None\n        atoms = row.rstrip().split(sep, 3)\n        contig, length = atoms[:2]\n        length = int(length)\n        label = atoms[-1]\n        label = label.replace(\" \", \"_\")\n\n        if len(atoms) == 3:\n            ranges = \"{0}..{1}\".format(1, length)\n        else:\n            assert len(atoms) == 4\n            ranges = atoms[2]\n\n        for ab in ranges.split(\",\"):\n            a, b = ab.split(\"..\")\n            a, b = int(a), int(b)\n            assert a <= b\n            ahang = a - 1\n            bhang = length - b\n            if ahang < cutoff:\n                a = 1\n            if bhang < cutoff:\n                b = length\n            print(\"\\t\".join(str(x) for x in (contig, a - 1, b, label)))", "response": "%prog fcs fcsfile\n\n    Process the results from Genbank contaminant screen. An example of the file\n    looks like:\n\n    contig name, length, span(s), apparent source\n    contig0746      11760   1..141  vector\n    contig0751      14226   13476..14226    vector\n    contig0800      124133  30512..30559    primer/adapter"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef asn(args):\n    from jcvi.formats.base import must_open\n\n    p = OptionParser(asn.__doc__)\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    fw = must_open(opts.outfile, \"w\")\n    for asnfile in args:\n        fp = open(asnfile)\n        ingeneralblock = False\n        ingenbankblock = False\n        gb, name = None, None\n        for row in fp:\n            if row.strip() == \"\":\n                continue\n\n            tag = row.split()[0]\n\n            if tag == \"general\":\n                ingeneralblock = True\n            if ingeneralblock and tag == \"str\":\n                if name is None:  # Only allow first assignment\n                    name = row.split(\"\\\"\")[1]\n                ingeneralblock = False\n\n            if tag == \"genbank\":\n                ingenbankblock = True\n            if ingenbankblock and tag == \"accession\":\n                if gb is None:\n                    gb = row.split(\"\\\"\")[1]\n                ingenbankblock = False\n\n        assert gb and name\n        print(\"{0}\\t{1}\".format(gb, name), file=fw)", "response": "%prog asn asnfiles\nAddon"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef htgnew(args):\n    from jcvi.formats.fasta import sequin\n\n    p = OptionParser(htgnew.__doc__)\n    p.add_option(\"--comment\", default=\"\",\n            help=\"Comments for this submission [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    fastafile, phasefile, sbtfile = args\n    comment = opts.comment\n\n    fastadir = \"fasta\"\n    sqndir = \"sqn\"\n    mkdir(fastadir)\n    mkdir(sqndir)\n\n    cmd = \"faSplit byname {0} {1}/\".format(fastafile, fastadir)\n    sh(cmd, outfile=\"/dev/null\", errfile=\"/dev/null\")\n\n    acmd = 'tbl2asn -a z -p fasta -r {sqndir}'\n    acmd += ' -i {splitfile} -t {sbtfile} -C tigr'\n    acmd += ' -j \"[tech=htgs {phase}] [organism=Medicago truncatula] [strain=A17]\"'\n    acmd += ' -o {sqndir}/{accession_nv}.sqn -V Vbr'\n    acmd += ' -y \"{comment}\" -W T -T T'\n\n    nupdated = 0\n    for row in open(phasefile):\n        name, phase = row.split()[:2]\n        fafile = op.join(fastadir, name + \".fa\")\n        cloneopt = \"--clone={0}\".format(name)\n        splitfile, gaps = sequin([fafile, cloneopt])\n        splitfile = op.basename(splitfile)\n        accession = accession_nv = name\n\n        phase = int(phase)\n        assert phase in (1, 2, 3)\n\n        cmd = acmd.format(accession_nv=accession_nv, sqndir=sqndir,\n                sbtfile=sbtfile, splitfile=splitfile, phase=phase,\n                comment=comment)\n        sh(cmd)\n\n        verify_sqn(sqndir, accession)\n        nupdated += 1\n\n    print(\"A total of {0} records updated.\".format(nupdated), file=sys.stderr)", "response": "%prog htgnew fastafile phasefile template. sbt"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef htg(args):\n    from jcvi.formats.fasta import sequin, ids\n    from jcvi.formats.agp import phase\n    from jcvi.apps.fetch import entrez\n\n    p = OptionParser(htg.__doc__)\n    p.add_option(\"--phases\", default=None,\n            help=\"Use another phasefile to override [default: %default]\")\n    p.add_option(\"--comment\", default=\"\",\n            help=\"Comments for this update [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    fastafile, sbtfile = args\n    pf = fastafile.rsplit(\".\", 1)[0]\n\n    idsfile = pf + \".ids\"\n    phasefile = pf + \".phases\"\n    namesfile = pf + \".names\"\n\n    ids([fastafile, \"--outfile={0}\".format(idsfile)])\n\n    asndir = \"asn.1\"\n    mkdir(asndir)\n    entrez([idsfile, \"--format=asn.1\", \"--outdir={0}\".format(asndir)])\n    asn(glob(\"{0}/*\".format(asndir)) + \\\n            [\"--outfile={0}\".format(namesfile)])\n\n    if opts.phases is None:\n        gbdir = \"gb\"\n        mkdir(gbdir)\n        entrez([idsfile, \"--format=gb\", \"--outdir={0}\".format(gbdir)])\n        phase(glob(\"{0}/*\".format(gbdir)) + \\\n                [\"--outfile={0}\".format(phasefile)])\n    else:\n        phasefile = opts.phases\n\n    assert op.exists(namesfile) and op.exists(phasefile)\n\n    newphasefile = phasefile + \".new\"\n    newphasefw = open(newphasefile, \"w\")\n    comment = opts.comment\n\n    fastadir = \"fasta\"\n    sqndir = \"sqn\"\n    mkdir(fastadir)\n    mkdir(sqndir)\n\n    from jcvi.graphics.histogram import stem_leaf_plot\n\n    names = DictFile(namesfile)\n    assert len(set(names.keys())) == len(set(names.values()))\n\n    phases = DictFile(phasefile)\n    ph = [int(x) for x in phases.values()]\n    # vmin 1, vmax 4, bins 3\n    stem_leaf_plot(ph, 1, 4, 3, title=\"Counts of phases before updates\")\n    logging.debug(\"Information loaded for {0} records.\".format(len(phases)))\n    assert len(names) == len(phases)\n\n    newph = []\n\n    cmd = \"faSplit byname {0} {1}/\".format(fastafile, fastadir)\n    sh(cmd, outfile=\"/dev/null\", errfile=\"/dev/null\")\n\n    acmd = 'tbl2asn -a z -p fasta -r {sqndir}'\n    acmd += ' -i {splitfile} -t {sbtfile} -C tigr'\n    acmd += ' -j \"{qualifiers}\"'\n    acmd += ' -A {accession_nv} -o {sqndir}/{accession_nv}.sqn -V Vbr'\n    acmd += ' -y \"{comment}\" -W T -T T'\n\n    qq = \"[tech=htgs {phase}] [organism=Medicago truncatula] [strain=A17]\"\n\n    nupdated = 0\n    for row in open(phasefile):\n        atoms = row.rstrip().split(\"\\t\")\n        # see formats.agp.phase() for column contents\n        accession, phase, clone = atoms[0], atoms[1], atoms[-1]\n        fafile = op.join(fastadir, accession + \".fa\")\n        accession_nv = accession.split(\".\", 1)[0]\n\n        newid = names[accession_nv]\n        newidopt = \"--newid={0}\".format(newid)\n        cloneopt = \"--clone={0}\".format(clone)\n        splitfile, gaps = sequin([fafile, newidopt, cloneopt])\n        splitfile = op.basename(splitfile)\n        phase = int(phase)\n        assert phase in (1, 2, 3)\n\n        oldphase = phase\n        if gaps == 0 and phase != 3:\n            phase = 3\n\n        if gaps != 0 and phase == 3:\n            phase = 2\n\n        print(\"{0}\\t{1}\\t{2}\".\\\n                format(accession_nv, oldphase, phase), file=newphasefw)\n        newph.append(phase)\n\n        qualifiers = qq.format(phase=phase)\n        if \";\" in clone:\n            qualifiers += \" [keyword=HTGS_POOLED_MULTICLONE]\"\n\n        cmd = acmd.format(accession=accession, accession_nv=accession_nv,\n                sqndir=sqndir, sbtfile=sbtfile, splitfile=splitfile,\n                qualifiers=qualifiers, comment=comment)\n        sh(cmd)\n\n        verify_sqn(sqndir, accession)\n        nupdated += 1\n\n    stem_leaf_plot(newph, 1, 4, 3, title=\"Counts of phases after updates\")\n    print(\"A total of {0} records updated.\".format(nupdated), file=sys.stderr)", "response": "%prog htg fastafile template. sbt\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convert_96_to_384(c96, quad, nrows=Nrows, ncols=Ncols):\n    rows, cols = get_rows_cols()\n    plate, splate = get_plate()\n\n    n96 = rows.index(c96[0]) * ncols / 2 + int(c96[1:])\n    q = \"{0:02d}{1}\".format(n96, \"ABCD\"[quad - 1])\n    return splate[q]", "response": "Convert 96 - well number and quad number to 384 - well number"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_description(s):\n    s = \"\".join(s.split()[1:]).replace(\"/\", \";\")\n    a = parse_qs(s)\n    return a", "response": "Parse a description string into a dictionary"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gss(args):\n    p = OptionParser(gss.__doc__)\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(p.print_help())\n\n    fastafile, mappingfile = args\n    seen = defaultdict(int)\n    clone = defaultdict(set)\n\n    plateMapping = DictFile(mappingfile)\n\n    fw = open(\"MetaData.txt\", \"w\")\n    print(PublicationTemplate.format(**vars), file=fw)\n    print(LibraryTemplate.format(**vars), file=fw)\n    print(ContactTemplate.format(**vars), file=fw)\n    logging.debug(\"Meta data written to `{0}`\".format(fw.name))\n\n    fw = open(\"GSS.txt\", \"w\")\n    fw_log = open(\"GSS.log\", \"w\")\n    for rec in SeqIO.parse(fastafile, \"fasta\"):\n        # First pass just check well number matchings and populate sequences in\n        # the same clone\n        description = rec.description\n        a = parse_description(description)\n        direction = a[\"direction\"][0]\n        sequencer_plate_barcode = a[\"sequencer_plate_barcode\"][0]\n        sequencer_plate_well_coordinates = \\\n            a[\"sequencer_plate_well_coordinates\"][0]\n        sequencer_plate_96well_quadrant = \\\n            a[\"sequencer_plate_96well_quadrant\"][0]\n        sequencer_plate_96well_coordinates = \\\n            a[\"sequencer_plate_96well_coordinates\"][0]\n\n        # Check the 96-well ID is correctly converted to 384-well ID\n        w96 = sequencer_plate_96well_coordinates\n        w96quad = int(sequencer_plate_96well_quadrant)\n        w384 = sequencer_plate_well_coordinates\n        assert convert_96_to_384(w96, w96quad) == w384\n\n        plate = sequencer_plate_barcode\n        assert plate in plateMapping, \\\n            \"{0} not found in `{1}` !\".format(plate, mappingfile)\n\n        plate = plateMapping[plate]\n        d = Directions[direction]\n\n        cloneID = \"{0}{1}\".format(plate, w384)\n        gssID = \"{0}{1}\".format(cloneID, d)\n        seen[gssID] += 1\n\n        if seen[gssID] > 1:\n            gssID = \"{0}{1}\".format(gssID, seen[gssID])\n\n        seen[gssID] += 1\n        clone[cloneID].add(gssID)\n\n    seen = defaultdict(int)\n    for rec in SeqIO.parse(fastafile, \"fasta\"):\n        # need to populate gssID, mateID, cloneID, seq, plate, row, column\n        description = rec.description\n        a = parse_description(description)\n        direction = a[\"direction\"][0]\n        sequencer_plate_barcode = a[\"sequencer_plate_barcode\"][0]\n        sequencer_plate_well_coordinates = \\\n            a[\"sequencer_plate_well_coordinates\"][0]\n        w384 = sequencer_plate_well_coordinates\n\n        plate = sequencer_plate_barcode\n        plate = plateMapping[plate]\n        d = Directions[direction]\n\n        cloneID = \"{0}{1}\".format(plate, w384)\n        gssID = \"{0}{1}\".format(cloneID, d)\n        seen[gssID] += 1\n\n        if seen[gssID] > 1:\n            logging.error(\"duplicate key {0} found\".format(gssID))\n            gssID = \"{0}{1}\".format(gssID, seen[gssID])\n\n        othergss = clone[cloneID] - set([gssID])\n        othergss = \", \".join(sorted(othergss))\n        vars.update(locals())\n\n        print(GSSTemplate.format(**vars), file=fw)\n\n        # Write conversion logs to log file\n        print(\"{0}\\t{1}\".format(gssID, description), file=fw_log)\n        print(\"=\" * 60, file=fw_log)\n\n    logging.debug(\"A total of {0} seqs written to `{1}`\".\\\n            format(len(seen), fw.name))\n    fw.close()\n    fw_log.close()", "response": "%prog gss fastafile plateMapping"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef scaffold(args):\n\n    from jcvi.formats.base import FileMerger\n    from jcvi.formats.bed import mates\n    from jcvi.formats.contig import frombed\n    from jcvi.formats.fasta import join\n    from jcvi.utils.iter import grouper\n\n    p = OptionParser(scaffold.__doc__)\n    p.set_rclip(rclip=1)\n    p.add_option(\"--conf\", help=\"BAMBUS configuration file [default: %default]\")\n    p.add_option(\"--prefix\", default=False, action=\"store_true\",\n            help=\"Only keep links between IDs with same prefix [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    nargs = len(args)\n    if nargs < 3 or nargs % 2 != 1:\n        sys.exit(not p.print_help())\n\n    rclip = opts.rclip\n    ctgfasta = args[0]\n    duos = list(grouper(args[1:], 2))\n    trios = []\n    for fastafile, bedfile in duos:\n        prefix = bedfile.rsplit(\".\", 1)[0]\n        matefile = prefix + \".mates\"\n        matebedfile = matefile + \".bed\"\n        if need_update(bedfile, [matefile, matebedfile]):\n            matesopt = [bedfile, \"--lib\", \"--nointra\",\n                        \"--rclip={0}\".format(rclip),\n                        \"--cutoff={0}\".format(opts.cutoff)]\n            if opts.prefix:\n                matesopt += [\"--prefix\"]\n            matefile, matebedfile = mates(matesopt)\n        trios.append((fastafile, matebedfile, matefile))\n\n    # Merge the readfasta, bedfile and matefile\n    bbfasta, bbbed, bbmate = \"bambus.reads.fasta\", \"bambus.bed\", \"bambus.mates\"\n\n    for files, outfile in zip(zip(*trios), (bbfasta, bbbed, bbmate)):\n        FileMerger(files, outfile=outfile).merge(checkexists=True)\n\n    ctgfile = \"bambus.contig\"\n    idsfile = \"bambus.ids\"\n    frombedInputs = [bbbed, ctgfasta, bbfasta]\n    if need_update(frombedInputs, ctgfile):\n        frombed(frombedInputs)\n\n    inputfasta = \"bambus.contigs.fasta\"\n    singletonfasta = \"bambus.singletons.fasta\"\n    cmd = \"faSomeRecords {0} {1} \".format(ctgfasta, idsfile)\n    sh(cmd + inputfasta)\n    sh(cmd + singletonfasta + \" -exclude\")\n\n    # Run bambus\n    prefix = \"bambus\"\n    cmd = \"goBambus -c {0} -m {1} -o {2}\".format(ctgfile, bbmate, prefix)\n    if opts.conf:\n        cmd += \" -C {0}\".format(opts.conf)\n    sh(cmd)\n\n    cmd = \"untangle -e {0}.evidence.xml -s {0}.out.xml -o {0}.untangle.xml\".\\\n            format(prefix)\n    sh(cmd)\n\n    final = \"final\"\n    cmd = \"printScaff -e {0}.evidence.xml -s {0}.untangle.xml -l {0}.lib \" \\\n          \"-merge -detail -oo -sum -o {1}\".format(prefix, final)\n    sh(cmd)\n\n    oofile = final + \".oo\"\n    join([inputfasta, \"--oo={0}\".format(oofile)])", "response": "%prog scaffold ctgfasta reads1. fasta mapping1. bed mapping2. bed..."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef expand(args):\n    import math\n\n    from jcvi.formats.fasta import Fasta, SeqIO\n    from jcvi.formats.fastq import readlen, first, fasta\n    from jcvi.formats.blast import Blast\n    from jcvi.formats.base import FileShredder\n    from jcvi.apps.bowtie import align, get_samfile\n    from jcvi.apps.align import blast\n\n    p = OptionParser(expand.__doc__)\n    p.set_depth(depth=200)\n    p.set_firstN()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    bes, reads = args\n    size = Fasta(bes).totalsize\n    rl = readlen([reads])\n    expected_size = size + 2 * rl\n    nreads = expected_size * opts.depth / rl\n    nreads = int(math.ceil(nreads / 1000.)) * 1000\n\n    # Attract reads\n    samfile, logfile = align([bes, reads, \"--reorder\", \"--mapped\",\n           \"--firstN={0}\".format(opts.firstN)])\n\n    samfile, mapped, _ = get_samfile(reads, bes, bowtie=True, mapped=True)\n    logging.debug(\"Extract first {0} reads from `{1}`.\".format(nreads, mapped))\n\n    pf = mapped.split(\".\")[0]\n    pf = pf.split(\"-\")[0]\n    bespf = bes.split(\".\")[0]\n    reads = pf + \".expand.fastq\"\n    first([str(nreads), mapped, \"-o\", reads])\n\n    # Perform mini-assembly\n    fastafile = reads.rsplit(\".\", 1)[0] + \".fasta\"\n    qualfile = \"\"\n    if need_update(reads, fastafile):\n        fastafile, qualfile = fasta([reads])\n\n    contigs = op.join(pf, \"454LargeContigs.fna\")\n    if need_update(fastafile, contigs):\n        cmd = \"runAssembly -o {0} -cpu 8 {1}\".format(pf, fastafile)\n        sh(cmd)\n    assert op.exists(contigs)\n\n    # Annotate contigs\n    blastfile = blast([bes, contigs])\n    mapping = {}\n    for query, b in Blast(blastfile).iter_best_hit():\n        mapping[query] = b\n\n    f = Fasta(contigs, lazy=True)\n    annotatedfasta = \".\".join((pf, bespf, \"fasta\"))\n    fw = open(annotatedfasta, \"w\")\n    keys = list(Fasta(bes).iterkeys_ordered())  # keep an ordered list\n    recs = []\n    for key, v in f.iteritems_ordered():\n        vid = v.id\n        if vid not in mapping:\n            continue\n        b = mapping[vid]\n        subject = b.subject\n        rec = v.reverse_complement() if b.orientation == '-' else v\n        rec.id = rid = \"_\".join((pf, vid, subject))\n        rec.description = \"\"\n        recs.append((keys.index(subject), rid, rec))\n\n    recs = [x[-1] for x in sorted(recs)]\n    SeqIO.write(recs, fw, \"fasta\")\n    fw.close()\n\n    FileShredder([samfile, logfile, mapped, reads, fastafile, qualfile, blastfile, pf])\n    logging.debug(\"Annotated seqs (n={0}) written to `{1}`.\".\\\n                    format(len(recs), annotatedfasta))\n\n    return annotatedfasta", "response": "%prog expand bes. fasta reads. fastq\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef contamination(args):\n    from jcvi.apps.bowtie import BowtieLogFile, align\n\n    p = OptionParser(contamination.__doc__)\n    p.set_firstN()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    ecoli, genome, fq = args\n    firstN_opt = \"--firstN={0}\".format(opts.firstN)\n    samfile, logfile = align([ecoli, fq, firstN_opt])\n    bl = BowtieLogFile(logfile)\n    lowerbound = bl.rate\n    samfile, logfile = align([genome, fq, firstN_opt])\n    bl = BowtieLogFile(logfile)\n    upperbound = 100 - bl.rate\n\n    median = (lowerbound + upperbound) / 2\n\n    clogfile = fq + \".Ecoli\"\n    fw = open(clogfile, \"w\")\n    lowerbound = \"{0:.1f}\".format(lowerbound)\n    upperbound = \"{0:.1f}\".format(upperbound)\n    median = \"{0:.1f}\".format(median)\n\n    print(\"\\t\".join((fq, lowerbound, median, upperbound)), file=fw)\n    print(\"{0}: Ecoli contamination rate {1}-{2}\".\\\n                        format(fq, lowerbound, upperbound), file=sys.stderr)\n    fw.close()", "response": "%prog contamination ecoli genome. fasta read. fastq"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef alignextend(args):\n    choices = \"prepare,align,filter,rmdup,genreads\".split(\",\")\n    p = OptionParser(alignextend.__doc__)\n    p.add_option(\"--nosuffix\", default=False, action=\"store_true\",\n                 help=\"Do not add /1/2 suffix to the read [default: %default]\")\n    p.add_option(\"--rc\", default=False, action=\"store_true\",\n                 help=\"Reverse complement the reads before alignment\")\n    p.add_option(\"--len\", default=100, type=\"int\",\n                 help=\"Extend to this length\")\n    p.add_option(\"--stage\", default=\"prepare\", choices=choices,\n                 help=\"Start from certain stage\")\n    p.add_option(\"--dup\", default=10, type=\"int\",\n                 help=\"Filter duplicates with coordinates within this distance\")\n    p.add_option(\"--maxdiff\", default=1, type=\"int\",\n                 help=\"Maximum number of differences\")\n    p.set_home(\"amos\")\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    ref, r1, r2 = args\n    pf = op.basename(r1).split(\".\")[0]\n    cmd = op.join(opts.amos_home, \"src/Experimental/alignextend.pl\")\n    if not opts.nosuffix:\n        cmd += \" -suffix\"\n    bwa_idx = \"{0}.ref.fa.sa\".format(pf)\n    if not need_update(ref, bwa_idx):\n        cmd += \" -noindex\"\n    cmd += \" -threads {0}\".format(opts.cpus)\n    offset = guessoffset([r1])\n    if offset == 64:\n        cmd += \" -I\"\n    if opts.rc:\n        cmd += \" -rc\"\n    cmd += \" -allow -len {0} -dup {1}\".format(opts.len, opts.dup)\n    cmd += \" -min {0} -max {1}\".format(2 * opts.len, 20 * opts.len)\n    cmd += \" -maxdiff {0}\".format(opts.maxdiff)\n    cmd += \" -stage {0}\".format(opts.stage)\n    cmd += \" \".join((\"\", pf, ref, r1, r2))\n    sh(cmd)", "response": "%prog alignextend ref. fasta read1. fastq read2. fastq"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef count(args):\n    from jcvi.utils.table import loadtable, write_csv\n\n    p = OptionParser(count.__doc__)\n    p.add_option(\"--dir\",\n                help=\"Sub-directory where FASTQC was run [default: %default]\")\n    p.add_option(\"--human\", default=False, action=\"store_true\",\n                help=\"Human friendly numbers [default: %default]\")\n    p.set_table()\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    filenames = args\n    subdir = opts.dir\n    header = \"Filename|Total Sequences|Sequence length|Total Bases\".split(\"|\")\n    rows = []\n    human = opts.human\n    for f in filenames:\n        folder = f.replace(\".gz\", \"\").rsplit(\".\", 1)[0] + \"_fastqc\"\n        if subdir:\n            folder = op.join(subdir, folder)\n        summaryfile = op.join(folder, \"fastqc_data.txt\")\n\n        fqcdata = FastQCdata(summaryfile, human=human)\n        row = [fqcdata[x] for x in header]\n        rows.append(row)\n\n    print(loadtable(header, rows), file=sys.stderr)\n    write_csv(header, rows, sep=opts.sep,\n              filename=opts.outfile, align=opts.align)", "response": "%prog count dir seq. gz seq. seq seq. seq seq. seq seq. seq seq. seq seq. seq seq. seq seq. seq seq. seq seq. seq seq"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hetsmooth(args):\n    p = OptionParser(hetsmooth.__doc__)\n    p.add_option(\"-K\", default=23, type=\"int\",\n                 help=\"K-mer size [default: %default]\")\n    p.add_option(\"-L\", type=\"int\",\n                 help=\"Bottom threshold, first min [default: %default]\")\n    p.add_option(\"-U\", type=\"int\",\n                 help=\"Top threshold, second min [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    reads1fq, reads2fq, jfdb = args\n    K = opts.K\n    L = opts.L\n    U = opts.U\n\n    assert L is not None and U is not None, \"Please specify -L and -U\"\n\n    cmd = \"het-smooth --kmer-len={0}\".format(K)\n    cmd += \" --bottom-threshold={0} --top-threshold={1}\".format(L, U)\n    cmd += \" --no-multibase-replacements --jellyfish-hash-file={0}\".format(jfdb)\n    cmd += \" --no-reads-log\"\n    cmd += \" \" + \" \".join((reads1fq, reads2fq))\n\n    sh(cmd)", "response": "%prog hetsmooth reads1. fq reads2. fq jf - 23_0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef trim(args):\n    tv = \"0.32\"\n    TrimJar = \"trimmomatic-{0}.jar\".format(tv)\n    p = OptionParser(trim.__doc__)\n    p.add_option(\"--path\", default=op.join(\"~/bin\", TrimJar),\n            help=\"Path to trimmomatic jar file [default: %default]\")\n    p.set_phred()\n    p.add_option(\"--nofrags\", default=False, action=\"store_true\",\n            help=\"Discard frags file in PE mode [default: %default]\")\n    p.add_option(\"--minqv\", default=15, type=\"int\",\n            help=\"Average qv after trimming [default: %default]\")\n    p.add_option(\"--minlen\", default=36, type=\"int\",\n            help=\"Minimum length after trimming [default: %default]\")\n    p.add_option(\"--adapteronly\", default=False, action=\"store_true\",\n            help=\"Only trim adapters with no qv trimming [default: %default]\")\n    p.add_option(\"--nogz\", default=False, action=\"store_true\",\n            help=\"Do not write to gzipped files [default: %default]\")\n    p.add_option(\"--log\", default=None, dest=\"trimlog\",\n            help=\"Specify a `trimlog` file [default: %default]\")\n    p.set_cpus(cpus=4)\n    opts, args = p.parse_args(args)\n\n    if len(args) not in (1, 2):\n        sys.exit(not p.print_help())\n\n    path = op.expanduser(opts.path)\n    url = \\\n    \"http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-{0}.zip\"\\\n    .format(tv)\n\n    if not op.exists(path):\n        path = download(url)\n        TrimUnzipped = \"Trimmomatic-\" + tv\n        if not op.exists(TrimUnzipped):\n            sh(\"unzip \" + path)\n        os.remove(path)\n        path = op.join(TrimUnzipped, TrimJar)\n\n    assert op.exists(path), \\\n        \"Couldn't find Trimmomatic jar file at `{0}`\".\\\n        format(path)\n\n    adaptersfile = \"adapters.fasta\"\n    Adapters = must_open(op.join(datadir, adaptersfile)).read()\n    write_file(adaptersfile, Adapters, skipcheck=True)\n\n    assert op.exists(adaptersfile), \\\n        \"Please place the illumina adapter sequence in `{0}`\".\\\n        format(adaptersfile)\n\n    if opts.phred is None:\n        offset = guessoffset([args[0]])\n    else:\n        offset = int(opts.phred)\n\n    phredflag = \" -phred{0}\".format(offset)\n    threadsflag = \" -threads {0}\".format(opts.cpus)\n    if opts.trimlog:\n        trimlog = \" -trimlog {0}\".format(opts.trimlog)\n\n    cmd = \"java -Xmx4g -jar {0}\".format(path)\n    frags = \".frags.fastq\"\n    pairs = \".pairs.fastq\"\n    if not opts.nogz:\n        frags += \".gz\"\n        pairs += \".gz\"\n\n    get_prefix = lambda x: op.basename(x).replace(\".gz\", \"\").rsplit(\".\", 1)[0]\n    get_dirname = lambda x: \"{0}/\".format(op.dirname(x)) if op.dirname(x) else ''\n    if len(args) == 1:\n        cmd += \" SE\"\n        cmd += phredflag\n        cmd += threadsflag\n        if opts.trimlog:\n            cmd += trimlog\n        fastqfile, = args\n        prefix = get_prefix(fastqfile)\n        dirname = get_dirname(fastqfile)\n        frags1 = dirname + prefix + frags\n        cmd += \" {0}\".format(\" \".join((fastqfile, frags1)))\n    else:\n        cmd += \" PE\"\n        cmd += phredflag\n        cmd += threadsflag\n        if opts.trimlog:\n            cmd += trimlog\n        fastqfile1, fastqfile2 = args\n        prefix1 = get_prefix(fastqfile1)\n        dirname1 = get_dirname(fastqfile1)\n        prefix2 = get_prefix(fastqfile2)\n        dirname2 = get_dirname(fastqfile2)\n        pairs1 = dirname1 + prefix1 + pairs\n        pairs2 = dirname2 + prefix2 + pairs\n        frags1 = dirname1 + prefix1 + frags\n        frags2 = dirname2 + prefix2 + frags\n        if opts.nofrags:\n            frags1 = \"/dev/null\"\n            frags2 = \"/dev/null\"\n        cmd += \" {0}\".format(\" \".join((fastqfile1, fastqfile2, \\\n                pairs1, frags1, pairs2, frags2)))\n\n    cmd += \" ILLUMINACLIP:{0}:2:30:10\".format(adaptersfile)\n\n    if not opts.adapteronly:\n        cmd += \" LEADING:3 TRAILING:3\"\n        cmd += \" SLIDINGWINDOW:4:{0}\".format(opts.minqv)\n\n    cmd += \" MINLEN:{0}\".format(opts.minlen)\n\n    if offset != 33:\n        cmd += \" TOPHRED33\"\n    sh(cmd)", "response": "%prog trim fastqfiles\n\n    Trim reads using TRIMMOMATIC. If two fastqfiles are given, then it invokes\n    the paired reads mode. See manual:\n\n    <http://www.usadellab.org/cms/index.php?page=trimmomatic>"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fetch(args):\n    import re\n    import csv\n    p = OptionParser(fetch.__doc__)\n\n    p.add_option(\"--format\", default=\"tab\", choices=valid_formats,\n                 help=\"download format [default: %default]\")\n    p.add_option(\"--columns\", default=\"entry name, protein names, genes,organism\",\n                 help=\"columns to download, if --format is `tab` or `xls`.\" +\n                 \" [default: %default]\")\n    p.add_option(\"--include\", default=False, action=\"store_true\",\n                 help=\"Include isoforms when --format is `fasta` or include `description` when\" +\n                 \" --format is `rdf`. [default: %default]\")\n    p.add_option(\"--limit\", default=10, type=\"int\",\n                 help=\"Max number of results to retrieve [default: %default]\")\n    p.add_option(\"--offset\", default=0, type=\"int\",\n                 help=\"Offset of first result, used with --limit [default: %default]\")\n    p.add_option(\"--skipcheck\", default=False, action=\"store_true\",\n                 help=\"turn off prompt to check file existence [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    query, = args\n    url_params = {}\n    if op.exists(query):\n        pf = query.rsplit(\".\", 1)[0]\n        list_of_queries = [row.strip() for row in open(query)]\n    else:\n        # the query is the search term\n        pf = query.strip().strip('\\\"')\n        list_of_queries = [pf]\n        pf = re.sub(r\"\\s+\", '_', pf)\n\n    assert len(list_of_queries) > 0, \\\n        \"Please provide atleast one input query\"\n\n    url_params['format'] = opts.format\n\n    if opts.columns and opts.format in valid_column_formats:\n        reader = csv.reader([opts.columns], skipinitialspace=True)\n        cols = [col for r in reader for col in r]\n        for col in cols:\n            assert col in valid_columns, \\\n                \"Column '{0}' is not a valid. Allowed options are {1}\".\\\n                format(col, valid_columns)\n        url_params['columns'] = \",\".join(cols)\n\n    if opts.include and opts.format in valid_include_formats:\n        url_params['include'] = \"yes\"\n\n    url_params['limit'] = opts.limit\n    url_params['offset'] = opts.offset\n\n    outfile = \"{0}.{1}\".format(pf, opts.format)\n\n    # If noprompt, will not check file existence\n    fw = must_open(outfile, \"w\", checkexists=True,\n                   skipcheck=opts.skipcheck)\n    if fw is None:\n        return\n\n    seen = set()\n    for query in list_of_queries:\n        if query in seen:\n            logging.error(\"Duplicate query ({0}) found\".format(query))\n            continue\n\n        url_params['query'] = query\n\n        data = urlencode(url_params)\n        try:\n            request = Request(uniprot_url, data)\n            response = urlopen(request)\n        except (HTTPError, URLError,\n                RuntimeError, KeyError) as e:\n            logging.error(e)\n            logging.debug(\"wait 5 seconds to reconnect...\")\n            time.sleep(5)\n\n        page = response.read()\n        if not page:\n            logging.error(\"query `{0}` yielded no results\".format(query))\n            continue\n\n        print(page, file=fw)\n\n        seen.add(query)\n\n    if seen:\n        print(\"A total of {0} out of {1} queries returned results.\".\n              format(len(seen), len(list_of_queries)), file=sys.stderr)", "response": "%prog fetch \"query\"\n        OR\n    %prog fetch queries.txt\n\n    Please provide a UniProt compatible `query` to retrieve data. If `query` contains\n    spaces, please remember to \"quote\" it.\n\n    You can also specify a `filename` which contains queries, one per line.\n\n    Follow this syntax <http://www.uniprot.org/help/text-search#text-search-syntax>\n    to query any of the documented fields <http://www.uniprot.org/help/query-fields>"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef range_intersect(a, b, extend=0):\n    a_min, a_max = a\n    if a_min > a_max:\n        a_min, a_max = a_max, a_min\n    b_min, b_max = b\n    if b_min > b_max:\n        b_min, b_max = b_max, b_min\n\n    if a_max + extend < b_min or b_max + extend < a_min:\n        return None\n    i_min = max(a_min, b_min)\n    i_max = min(a_max, b_max)\n    if i_min > i_max + extend:\n        return None\n\n    return [i_min, i_max]", "response": "Returns the intersection between two reanges."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ranges_intersect(rset):\n    if not rset:\n        return None\n\n    a = rset[0]\n    for b in rset[1:]:\n        if not a:\n            return None\n        a = range_intersect(a, b)\n\n    return a", "response": "Returns the range_intersect function of the specified range set."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn whether two ranges overlap. Set percentage = True returns overlap ratio over the shorter range of the two.", "response": "def range_overlap(a, b, ratio=False):\n    \"\"\"\n    Returns whether two ranges overlap. Set percentage=True returns overlap\n    ratio over the shorter range of the two.\n\n    >>> range_overlap((\"1\", 30, 45), (\"1\", 41, 55))\n    5\n    >>> range_overlap((\"1\", 21, 45), (\"1\", 41, 75), ratio=True)\n    0.2\n    >>> range_overlap((\"1\", 30, 45), (\"1\", 15, 55))\n    16\n    >>> range_overlap((\"1\", 30, 45), (\"1\", 15, 55), ratio=True)\n    1.0\n    >>> range_overlap((\"1\", 30, 45), (\"1\", 57, 68))\n    0\n    >>> range_overlap((\"1\", 30, 45), (\"2\", 42, 55))\n    0\n    >>> range_overlap((\"1\", 30, 45), (\"2\", 42, 55), ratio=True)\n    0.0\n    \"\"\"\n    a_chr, a_min, a_max = a\n    b_chr, b_min, b_max = b\n    a_min, a_max = sorted((a_min, a_max))\n    b_min, b_max = sorted((b_min, b_max))\n    shorter = min((a_max - a_min), (b_max - b_min)) + 1\n    # must be on the same chromosome\n    if a_chr != b_chr:\n        ov = 0\n    else:\n        ov = min(shorter, (a_max - b_min + 1), (b_max - a_min + 1))\n        ov = max(ov, 0)\n    if ratio:\n        ov /= float(shorter)\n    return ov"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef range_distance(a, b, distmode='ss'):\n    assert distmode in ('ss', 'ee')\n\n    a_chr, a_min, a_max, a_strand = a\n    b_chr, b_min, b_max, b_strand = b\n    # must be on the same chromosome\n    if a_chr != b_chr:\n        dist = -1\n    #elif range_overlap(a[:3], b[:3]):\n    #    dist = 0\n    else:\n        # If the two ranges do not overlap, check stranded-ness and distance\n        if a_min > b_min:\n            a_min, b_min = b_min, a_min\n            a_max, b_max = b_max, a_max\n            a_strand, b_strand = b_strand, a_strand\n\n        if distmode == \"ss\":\n            dist = b_max - a_min + 1\n        elif distmode == \"ee\":\n            dist = b_min - a_max - 1\n\n    orientation = a_strand + b_strand\n\n    return dist, orientation", "response": "Returns the distance between two ranges."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the span of a collection of ranges where start and end are the smallest of all starts and end are the largest of all ends.", "response": "def range_minmax(ranges):\n    \"\"\"\n    Returns the span of a collection of ranges where start is the smallest of\n    all starts, and end is the largest of all ends.\n\n    >>> ranges = [(30, 45), (40, 50), (10, 100)]\n    >>> range_minmax(ranges)\n    (10, 100)\n    \"\"\"\n    rmin = min(ranges)[0]\n    rmax = max(ranges, key=lambda x: x[1])[1]\n    return rmin, rmax"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef range_closest(ranges, b, left=True):\n    from jcvi.utils.orderedcollections import SortedCollection\n    key = (lambda x: x) if left else (lambda x: (x[0], x[2], x[1]))\n    rr = SortedCollection(ranges, key=key)\n    try:\n        if left:\n            s = rr.find_le(b)\n            assert key(s) <= key(b), (s, b)\n        else:\n            s = rr.find_ge(b)\n            assert key(s) >= key(b), (s, b)\n    except ValueError:\n        s = None\n\n    return s", "response": "Returns the range that s closest to the given position."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef range_interleave(ranges, sizes={}, empty=False):\n    from jcvi.utils.iter import pairwise\n    ranges = range_merge(ranges)\n    interleaved_ranges = []\n\n    for ch, cranges in groupby(ranges, key=lambda x: x[0]):\n        cranges = list(cranges)\n        size = sizes.get(ch, None)\n        if size:\n            ch, astart, aend = cranges[0]\n            if astart > 1:\n                interleaved_ranges.append((ch, 1, astart - 1))\n            elif empty:\n                interleaved_ranges.append(None)\n\n        for a, b in pairwise(cranges):\n            ch, astart, aend = a\n            ch, bstart, bend = b\n            istart, iend = aend + 1, bstart - 1\n            if istart <= iend:\n                interleaved_ranges.append((ch, istart, iend))\n            elif empty:\n                interleaved_ranges.append(None)\n\n        if size:\n            ch, astart, aend = cranges[-1]\n            if aend < size:\n                interleaved_ranges.append((ch, aend + 1, size))\n            elif empty:\n                interleaved_ranges.append(None)\n\n    return interleaved_ranges", "response": "Returns the ranges in between the given ranges."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning merged range. Similar to range_union, except this returns new ranges. >>> ranges = [(\"1\", 30, 45), (\"1\", 40, 50), (\"1\", 10, 50)] >>> range_merge(ranges) [('1', 10, 50)] >>> ranges = [(\"1\", 30, 40), (\"1\", 45, 50)] >>> range_merge(ranges) [('1', 30, 40), ('1', 45, 50)] >>> ranges = [(\"1\", 30, 40), (\"1\", 45, 50)] >>> range_merge(ranges, dist=5) [('1', 30, 50)]", "response": "def range_merge(ranges, dist=0):\n    \"\"\"\n    Returns merged range. Similar to range_union, except this returns\n    new ranges.\n\n    >>> ranges = [(\"1\", 30, 45), (\"1\", 40, 50), (\"1\", 10, 50)]\n    >>> range_merge(ranges)\n    [('1', 10, 50)]\n    >>> ranges = [(\"1\", 30, 40), (\"1\", 45, 50)]\n    >>> range_merge(ranges)\n    [('1', 30, 40), ('1', 45, 50)]\n    >>> ranges = [(\"1\", 30, 40), (\"1\", 45, 50)]\n    >>> range_merge(ranges, dist=5)\n    [('1', 30, 50)]\n    \"\"\"\n    if not ranges:\n        return []\n\n    ranges.sort()\n\n    cur_range = list(ranges[0])\n    merged_ranges = []\n    for r in ranges[1:]:\n        # open new range if start > cur_end or seqid != cur_seqid\n        if r[1] - cur_range[2] > dist or r[0] != cur_range[0]:\n            merged_ranges.append(tuple(cur_range))\n            cur_range = list(r)\n        else:\n            cur_range[2] = max(cur_range[2], r[2])\n    merged_ranges.append(tuple(cur_range))\n\n    return merged_ranges"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn total size of ranges expect range as chr left right", "response": "def range_union(ranges):\n    \"\"\"\n    Returns total size of ranges, expect range as (chr, left, right)\n\n    >>> ranges = [(\"1\", 30, 45), (\"1\", 40, 50), (\"1\", 10, 50)]\n    >>> range_union(ranges)\n    41\n    >>> ranges = [(\"1\", 30, 45), (\"2\", 40, 50)]\n    >>> range_union(ranges)\n    27\n    >>> ranges = [(\"1\", 30, 45), (\"1\", 45, 50)]\n    >>> range_union(ranges)\n    21\n    >>> range_union([])\n    0\n    \"\"\"\n    if not ranges:\n        return 0\n\n    ranges.sort()\n\n    total_len = 0\n    cur_chr, cur_left, cur_right = ranges[0]  # left-most range\n    for r in ranges:\n        # open new range if left > cur_right or chr != cur_chr\n        if r[1] > cur_right or r[0] != cur_chr:\n            total_len += cur_right - cur_left + 1\n            cur_chr, cur_left, cur_right = r\n        else:\n            # update cur_right\n            cur_right = max(r[2], cur_right)\n\n    # the last one\n    total_len += cur_right - cur_left + 1\n\n    return total_len"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef range_span(ranges):\n    if not ranges:\n        return 0\n\n    ranges.sort()\n    ans = 0\n    for seq, lt in groupby(ranges, key=lambda x: x[0]):\n        lt = list(lt)\n        ans += max(max(lt)[1:]) - min(min(lt)[1:]) + 1\n    return ans", "response": "Returns the total span between the left most range to the right most range."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef range_piles(ranges):\n    endpoints = _make_endpoints(ranges)\n\n    for seqid, ends in groupby(endpoints, lambda x: x[0]):\n        active = []\n        depth = 0\n        for seqid, pos, leftright, i, score in ends:\n            if leftright == LEFT:\n                active.append(i)\n                depth += 1\n            else:\n                depth -= 1\n\n            if depth == 0 and active:\n                yield active\n                active = []", "response": "Return piles of intervals that overlap."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a generator that yields all blocks that are in conflict.", "response": "def range_conflict(ranges, depth=1):\n    \"\"\"\n    Find intervals that are overlapping in 1-dimension.\n    Return groups of block IDs that are in conflict.\n\n    >>> ranges = [Range(\"2\", 0, 1, 3, 0), Range(\"2\", 1, 4, 3, 1), Range(\"3\", 5, 7, 3, 2)]\n    >>> list(range_conflict(ranges))\n    [(0, 1)]\n    \"\"\"\n    overlap = set()\n    active = set()\n    endpoints = _make_endpoints(ranges)\n\n    for seqid, ends in groupby(endpoints, lambda x: x[0]):\n        active.clear()\n        for seqid, pos, leftright, i, score in ends:\n            if leftright == LEFT:\n                active.add(i)\n            else:\n                active.remove(i)\n\n            if len(active) > depth:\n                overlap.add(tuple(sorted(active)))\n\n    for ov in overlap:\n        yield ov"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a list of weighted intervals find non - overlapping set with max weight.", "response": "def range_chain(ranges):\n    \"\"\"\n    Take list of weighted intervals, find non-overlapping set with max weight.\n    We proceed with each end point (sorted by their relative positions).\n\n    The input are a list of ranges of the form (start, stop, score), output is\n    subset of the non-overlapping ranges that give the highest score, score\n\n    >>> ranges = [Range(\"1\", 0, 9, 22, 0), Range(\"1\", 3, 18, 24, 1), Range(\"1\", 10, 28, 20, 2)]\n    >>> range_chain(ranges)\n    ([Range(seqid='1', start=0, end=9, score=22, id=0), Range(seqid='1', start=10, end=28, score=20, id=2)], 42)\n    >>> ranges = [Range(\"2\", 0, 1, 3, 0), Range(\"2\", 1, 4, 3, 1), Range(\"3\", 5, 7, 3, 2)]\n    >>> range_chain(ranges)\n    ([Range(seqid='2', start=0, end=1, score=3, id=0), Range(seqid='3', start=5, end=7, score=3, id=2)], 6)\n    \"\"\"\n    endpoints = _make_endpoints(ranges)\n\n    # stores the left end index for quick retrieval\n    left_index = {}\n    # dynamic programming, each entry [score, from_index, which_chain]\n    scores = []\n\n    for i, (seqid, pos, leftright, j, score) in enumerate(endpoints):\n\n        cur_score = [0, -1, -1] if i == 0 else scores[-1][:]\n\n        if leftright == LEFT:\n            left_index[j] = i\n\n        else:  # this is right end of j-th interval\n            # update if chaining j-th interval gives a better score\n            left_j = left_index[j]\n            chain_score = scores[left_j][0] + score\n            if chain_score > cur_score[0]:\n                cur_score = [chain_score, left_j, j]\n\n        scores.append(cur_score)\n\n    chains = []\n    score, last, chain_id = scores[-1]  # start backtracking\n    while last != -1:\n        if chain_id != -1:\n            chains.append(chain_id)\n        _, last, chain_id = scores[last]\n\n    chains.reverse()\n\n    selected = [ranges[x] for x in chains]\n\n    return selected, score"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ranges_depth(ranges, sizes, verbose=True):\n    ranges.sort()\n    for seqid, rrs in groupby(ranges, key=lambda x: x[0]):\n        rrs = [(a, b) for (s, a, b) in rrs]\n        size = sizes[seqid]\n        ds, depthdetails = range_depth(rrs, size, verbose=verbose)\n        depthdetails = [(seqid, s, e, d) for s, e, d in depthdetails]\n        yield depthdetails", "response": "Generate depth details for a list of ranges."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef range_depth(ranges, size, verbose=True):\n    from jcvi.utils.iter import pairwise\n    from jcvi.utils.cbook import percentage\n\n    # Make endpoints\n    endpoints = []\n    for a, b in ranges:\n        endpoints.append((a, LEFT))\n        endpoints.append((b, RIGHT))\n    endpoints.sort()\n    vstart, vend = min(endpoints)[0], max(endpoints)[0]\n\n    assert 0 <= vstart < size\n    assert 0 <= vend < size\n\n    depth = 0\n    depthstore = defaultdict(int)\n    depthstore[depth] += vstart\n    depthdetails = [(0, vstart, depth)]\n\n    for (a, atag), (b, btag) in pairwise(endpoints):\n        if atag == LEFT:\n            depth += 1\n        elif atag == RIGHT:\n            depth -= 1\n        depthstore[depth] += b - a\n        depthdetails.append((a, b, depth))\n\n    assert btag == RIGHT\n    depth -= 1\n\n    assert depth == 0\n    depthstore[depth] += size - vend\n    depthdetails.append((vend, size, depth))\n\n    assert sum(depthstore.values()) == size\n    if verbose:\n        for depth, count in sorted(depthstore.items()):\n            print(\"Depth {0}: {1}\".\\\n                    format(depth, percentage(count, size)), file=sys.stderr)\n\n    return depthstore, depthdetails", "response": "Given a list of ranges and a size return a depth store and a depth details list."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints a tabular output with horizontal separators", "response": "def loadtable(header, rows, major='=', minor='-', thousands=True):\n    \"\"\"\n    Print a tabular output, with horizontal separators\n    \"\"\"\n    formatted = load_csv(header, rows, sep=\"   \", thousands=thousands)\n    header, rows = formatted[0], formatted[1:]\n\n    return banner(header, rows)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tabulate(d, transpose=False, thousands=True, key_fun=None, sep=',', align=True):\n    pairs = d.keys()\n    rows, cols = zip(*pairs)\n    if transpose:\n        rows, cols = cols, rows\n\n    rows = sorted(set(rows))\n    cols = sorted(set(cols))\n    header = [\"o\"] + list(cols)\n    table = []\n    for r in rows:\n        combo = [(r, c) for c in cols]\n        if transpose:\n            combo = [(c, r) for (r, c) in combo]\n        data = [d.get(x, \"n/a\") for x in combo]\n        data = [\"{0:.1f}\".format(x) if isinstance(x, float) else x\n                for x in data]\n        if key_fun:\n            data = [key_fun(x) for x in data]\n        table.append([str(r)] + data)\n\n    if not align:\n        formatted = load_csv(header, table, sep=sep)\n        return \"\\n\".join(formatted)\n\n    return loadtable(header, table, thousands=thousands)", "response": "Tabulate a dictionary of items in a tree structure."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting a CSV file with the specified header and contents.", "response": "def write_csv(header, contents, sep=\",\", filename=\"stdout\", thousands=False,\n              tee=False, align=True, comment=False):\n    \"\"\"\n    Write csv that are aligned with the column headers.\n\n    >>> header = [\"x_value\", \"y_value\"]\n    >>> contents = [(1, 100), (2, 200)]\n    >>> write_csv(header, contents)\n    x_value, y_value\n          1,     100\n          2,     200\n    \"\"\"\n    from jcvi.formats.base import must_open\n\n    formatted = load_csv(header, contents,\n                         sep=sep, thousands=thousands, align=align)\n    if comment:\n        formatted[0] = '#' + formatted[0][1:]\n    formatted = \"\\n\".join(formatted)\n    fw = must_open(filename, \"w\")\n    print(formatted, file=fw)\n    if tee and filename != \"stdout\":\n        print(formatted)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dotplot(args):\n    from jcvi.assembly.allmaps import CSVMapLine\n    from jcvi.formats.sizes import Sizes\n    from jcvi.utils.natsort import natsorted\n    from jcvi.graphics.base import shorten\n    from jcvi.graphics.dotplot import plt, savefig, markup, normalize_axes, \\\n                    downsample, plot_breaks_and_labels, thousands\n\n    p = OptionParser(dotplot.__doc__)\n    p.set_outfile(outfile=None)\n    opts, args, iopts = p.set_image_options(args, figsize=\"8x8\",\n                                            style=\"dark\", dpi=90, cmap=\"copper\")\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    csvfile, fastafile = args\n    sizes = natsorted(Sizes(fastafile).mapping.items())\n    seen = set()\n    raw_data = []\n\n    fig = plt.figure(1, (iopts.w, iopts.h))\n    root = fig.add_axes([0, 0, 1, 1])  # the whole canvas\n    ax = fig.add_axes([.1, .1, .8, .8])  # the dot plot\n\n    fp = must_open(csvfile)\n    for row in fp:\n        m = CSVMapLine(row)\n        seen.add(m.seqid)\n        raw_data.append(m)\n\n    # X-axis is the genome assembly\n    ctgs, ctg_sizes = zip(*sizes)\n    xsize = sum(ctg_sizes)\n    qb = list(np.cumsum(ctg_sizes))\n    qbreaks = list(zip(ctgs, [0] + qb, qb))\n    qstarts = dict(zip(ctgs, [0] + qb))\n\n    # Y-axis is the map\n    key = lambda x: x.lg\n    raw_data.sort(key=key)\n    ssizes = {}\n    for lg, d in groupby(raw_data, key=key):\n        ssizes[lg] = max([x.cm for x in d])\n    ssizes = natsorted(ssizes.items())\n    lgs, lg_sizes = zip(*ssizes)\n    ysize = sum(lg_sizes)\n    sb = list(np.cumsum(lg_sizes))\n    sbreaks = list(zip([(\"LG\" + x) for x in lgs], [0] + sb, sb))\n    sstarts = dict(zip(lgs, [0] + sb))\n\n    # Re-code all the scatter dots\n    data = [(qstarts[x.seqid] + x.pos, sstarts[x.lg] + x.cm, 'g') \\\n                for x in raw_data if (x.seqid in qstarts)]\n    npairs = downsample(data)\n\n    x, y, c = zip(*data)\n    ax.scatter(x, y, c=c, edgecolors=\"none\", s=2, lw=0)\n\n    # Flip X-Y label\n    gy, gx = op.basename(csvfile).split(\".\")[:2]\n    gx, gy = shorten(gx, maxchar=30), shorten(gy, maxchar=30)\n    xlim, ylim = plot_breaks_and_labels(fig, root, ax, gx, gy,\n                                xsize, ysize, qbreaks, sbreaks)\n    ax.set_xlim(xlim)\n    ax.set_ylim(ylim)\n\n    title = \"Alignment: {} vs {}\".format(gx, gy)\n    title += \" ({} markers)\".format(thousands(npairs))\n    root.set_title(markup(title), x=.5, y=.96, color=\"k\")\n    logging.debug(title)\n    normalize_axes(root)\n\n    image_name = opts.outfile or \\\n                (csvfile.rsplit(\".\", 1)[0] + \".\" + iopts.format)\n    savefig(image_name, dpi=iopts.dpi, iopts=iopts)\n    fig.clear()", "response": "%prog dotplot map. csv ref. fasta"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bed(args):\n    p = OptionParser(bed.__doc__)\n    p.add_option(\"--switch\", default=False, action=\"store_true\",\n                 help=\"Switch reference and aligned map elements [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    mapout, = args\n    pf = mapout.split(\".\")[0]\n    mapbed = pf + \".bed\"\n    bm = BinMap(mapout)\n    bm.print_to_bed(mapbed, switch=opts.switch)\n\n    return mapbed", "response": "%prog fasta map. out\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fasta(args):\n    from jcvi.formats.sizes import Sizes\n\n    p = OptionParser(fasta.__doc__)\n    p.add_option(\"--extend\", default=1000, type=\"int\",\n                 help=\"Extend seq flanking the gaps [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    mapout, sfasta = args\n    Flank = opts.extend\n    pf = mapout.split(\".\")[0]\n    mapbed = pf + \".bed\"\n    bm = BinMap(mapout)\n    bm.print_to_bed(mapbed)\n\n    bed = Bed(mapbed, sorted=False)\n    markersbed = pf + \".markers.bed\"\n    fw = open(markersbed, \"w\")\n    sizes = Sizes(sfasta).mapping\n    for b in bed:\n        accn = b.accn\n        scf, pos = accn.split(\".\")\n        pos = int(pos)\n        start = max(0, pos - Flank)\n        end = min(pos + Flank, sizes[scf])\n        print(\"\\t\".join(str(x) for x in \\\n                    (scf, start, end, accn)), file=fw)\n\n    fw.close()\n\n    fastaFromBed(markersbed, sfasta, name=True)", "response": "%prog fasta map. out scaffolds. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef trimNs(seq, line, newagp):\n    start, end = line.component_beg, line.component_end\n    size = end - start + 1\n    leftNs, rightNs = 0, 0\n    lid, lo = line.component_id, line.orientation\n    for s in seq:\n        if s in 'nN':\n            leftNs += 1\n        else:\n            break\n    for s in seq[::-1]:\n        if s in 'nN':\n            rightNs += 1\n        else:\n            break\n\n    if lo == '-':\n        trimstart = start + rightNs\n        trimend = end - leftNs\n    else:\n        trimstart = start + leftNs\n        trimend = end - rightNs\n\n    trimrange = (trimstart, trimend)\n    oldrange = (start, end)\n\n    if trimrange != oldrange:\n        logging.debug(\"{0} trimmed of N's: {1} => {2}\".\\\n                        format(lid, oldrange, trimrange))\n\n        if leftNs:\n            print(\"\\t\".join(str(x) for x in (line.object, 0, 0, 0,\n                    'N', leftNs, \"fragment\", \"yes\", \"\")), file=newagp)\n        if trimend > trimstart:\n            print(\"\\t\".join(str(x) for x in (line.object, 0, 0, 0,\n                    line.component_type, lid, trimstart, trimend, lo)), file=newagp)\n        if rightNs and rightNs != size:\n            print(\"\\t\".join(str(x) for x in (line.object, 0, 0, 0,\n                    'N', rightNs, \"fragment\", \"yes\", \"\")), file=newagp)\n    else:\n        print(line, file=newagp)", "response": "Test if the sequences contain dangling N s on both sides."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compress(args):\n    p = OptionParser(compress.__doc__)\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    aagpfile, bagpfile = args\n    # First AGP provides the mapping\n    store = {}\n    agp = AGP(aagpfile)\n    for a in agp:\n        if a.is_gap:\n            continue\n        # Ignore '?' in the mapping\n        if a.sign == 0:\n            a.sign = 1\n        store[(a.object, a.object_beg, a.object_end)] = \\\n              (a.component_id, a.component_beg, a.component_end, a.sign)\n\n    # Second AGP forms the backbone\n    agp = AGP(bagpfile)\n    fw = must_open(opts.outfile, \"w\")\n    print(\"\\n\".join(agp.header), file=fw)\n    for a in agp:\n        if a.is_gap:\n            print(a, file=fw)\n            continue\n        component_id, component_beg, component_end, sign = \\\n            store[(a.component_id, a.component_beg, a.component_end)]\n\n        orientation = {1: '+', -1: '-', 0: '?'}.get(sign * a.sign)\n        atoms = (a.object, a.object_beg, a.object_end, a.part_number, a.component_type,\n                 component_id, component_beg, component_end, orientation)\n        a = AGPLine(\"\\t\".join(str(x) for x in atoms))\n        print(a, file=fw)", "response": "%prog compress a. agp bagp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cut(args):\n    p = OptionParser(cut.__doc__)\n    p.add_option(\"--sep\", default=\".\", help=\"Separator for splits\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    agpfile, bedfile = args\n    sep = opts.sep\n\n    agp = AGP(agpfile)\n    bed = Bed(bedfile)\n    simple_agp = agp.order\n    newagpfile = agpfile.replace(\".agp\", \".cut.agp\")\n    fw = open(newagpfile, \"w\")\n\n    agp_fixes = defaultdict(list)\n    for component, intervals in bed.sub_beds():\n        i, a = simple_agp[component]\n        object = a.object\n        component_span = a.component_span\n        orientation = a.orientation\n\n        assert a.component_beg, a.component_end\n        cuts = set()\n        for i in intervals:\n            start, end = i.start, i.end\n            end -= 1\n\n            assert start <= end\n            cuts.add(start)\n            cuts.add(end)\n\n        cuts.add(0)\n        cuts.add(component_span)\n        cuts = list(sorted(cuts))\n\n        sum_of_spans = 0\n        for i, (a, b) in enumerate(pairwise(cuts)):\n            oid = object + \"{0}{1}\".format(sep, i + 1)\n            aline = [oid, 0, 0, 0]\n            cspan = b - a\n            aline += ['D', component, a + 1, b, orientation]\n            sum_of_spans += cspan\n\n            aline = \"\\t\".join(str(x) for x in aline)\n            agp_fixes[component].append(aline)\n\n        assert component_span == sum_of_spans\n\n    # Finally write the masked agp\n    for a in agp:\n        if not a.is_gap and a.component_id in agp_fixes:\n            print(\"\\n\".join(agp_fixes[a.component_id]), file=fw)\n        else:\n            print(a, file=fw)\n\n    fw.close()\n    # Reindex\n    reindex([newagpfile, \"--inplace\"])\n\n    return newagpfile", "response": "%prog cut agpfile bedfile"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef summary(args):\n    from jcvi.utils.table import write_csv\n\n    p = OptionParser(summary.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(p.print_help())\n\n    agpfile, = args\n    header = \"Chromosome #_Distinct #_Components #_Scaffolds \" \\\n             \"Scaff_N50 Scaff_L50 Length\".split()\n\n    agp = AGP(agpfile)\n    data = list(agp.summary_all())\n    write_csv(header, data, sep=\" \")", "response": "%prog summary agpfile\n\n    print a table of scaffold statistics, number of BACs, no of scaffolds,\n    scaffold N50, scaffold L50, actual sequence, PSMOL NNNs, PSMOL-length, % of\n    PSMOL sequenced."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tpf(args):\n    p = OptionParser(tpf.__doc__)\n    p.add_option(\"--noversion\", default=False, action=\"store_true\",\n            help=\"Remove trailing accession versions [default: %default]\")\n    p.add_option(\"--gaps\", default=False, action=\"store_true\",\n            help=\"Include gaps in the output [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    agpfile, = args\n    agp = AGP(agpfile)\n    for a in agp:\n        object = a.object\n        if a.is_gap:\n            if opts.gaps and a.isCloneGap:\n                print(\"\\t\".join((a.gap_type, object, \"na\")))\n            continue\n\n        component_id = a.component_id\n        orientation = a.orientation\n\n        if opts.noversion:\n            component_id = component_id.rsplit(\".\", 1)[0]\n\n        print(\"\\t\".join((component_id, object, orientation)))", "response": "%prog tpf agpfile\n\n    Print out a list of ids, one per line. Also known as the Tiling Path.\n\n    AC225490.9  chr6\n\n    Can optionally output scaffold gaps."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bed(args):\n    from jcvi.formats.obo import validate_term\n\n    p = OptionParser(bed.__doc__)\n    p.add_option(\"--gaps\", default=False, action=\"store_true\",\n            help=\"Only print bed lines for gaps [default: %default]\")\n    p.add_option(\"--nogaps\", default=False, action=\"store_true\",\n            help=\"Do not print bed lines for gaps [default: %default]\")\n    p.add_option(\"--bed12\", default=False, action=\"store_true\",\n            help=\"Produce bed12 formatted output [default: %default]\")\n    p.add_option(\"--component\", default=False, action=\"store_true\",\n            help=\"Generate bed file for components [default: %default]\")\n    p.set_outfile()\n    g1 = OptionGroup(p, \"GFF specific parameters\",\n            \"Note: If not specified, output will be in `bed` format\")\n    g1.add_option(\"--gff\", default=False, action=\"store_true\",\n            help=\"Produce gff3 formatted output. By default, ignores \" +\\\n                 \"AGP gap lines. [default: %default]\")\n    g1.add_option(\"--source\", default=\"MGSC\",\n            help=\"Specify a gff3 source [default: `%default`]\")\n    g1.add_option(\"--feature\", default=\"golden_path_fragment\",\n            help=\"Specify a gff3 feature type [default: `%default`]\")\n    p.add_option_group(g1)\n    p.set_SO_opts()\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    if opts.component:\n        opts.nogaps = True\n\n    # If output format is gff3 and 'verifySO' option is invoked, validate the SO term\n    if opts.gff and opts.verifySO:\n        validate_term(opts.feature, method=opts.verifySO)\n\n    agpfile, = args\n    agp = AGP(agpfile)\n    fw = must_open(opts.outfile, \"w\")\n    if opts.gff:\n        print(\"##gff-version 3\", file=fw)\n\n    for a in agp:\n        if opts.nogaps and a.is_gap:\n            continue\n        if opts.gaps and not a.is_gap:\n            continue\n        if opts.bed12:\n            print(a.bed12line, file=fw)\n        elif opts.gff:\n            print(a.gffline(gff_source=opts.source, gff_feat_type=opts.feature), file=fw)\n        elif opts.component:\n            name = \"{0}:{1}-{2}\".\\\n                        format(a.component_id, a.component_beg, a.component_end)\n            print(\"\\t\".join(str(x) for x in (a.component_id, a.component_beg - 1,\n                                a.component_end, name,\n                                a.component_type, a.orientation)), file=fw)\n        else:\n            print(a.bedline, file=fw)\n    fw.close()\n\n    return fw.name", "response": "%prog bed agpfile\n\n    print out the tiling paths in bed/gff3 format"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extendbed(args):\n    from jcvi.formats.sizes import Sizes\n\n    p = OptionParser(extendbed.__doc__)\n    p.add_option(\"--nogaps\", default=False, action=\"store_true\",\n            help=\"Do not print bed lines for gaps [default: %default]\")\n    p.add_option(\"--bed12\", default=False, action=\"store_true\",\n            help=\"Produce bed12 formatted output [default: %default]\")\n    p.add_option(\"--gff\", default=False, action=\"store_true\",\n            help=\"Produce gff3 formatted output. By default, ignores \" +\\\n                 \" AGP gap lines. [default: %default]\")\n    p.set_outfile()\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    # If output format is GFF3, ignore AGP gap lines.\n    if opts.gff:\n        opts.nogaps = True\n\n    agpfile, fastafile = args\n    agp = AGP(agpfile)\n    fw = must_open(opts.outfile, \"w\")\n    if opts.gff:\n        print(\"##gff-version 3\", file=fw)\n\n    ranges = defaultdict(list)\n    thickCoords = []  # These are the coordinates before modify ranges\n    # Make the first pass to record all the component ranges\n    for a in agp:\n        thickCoords.append((a.object_beg, a.object_end))\n        if a.is_gap:\n            continue\n        ranges[a.component_id].append(a)\n\n    # Modify the ranges\n    sizes = Sizes(fastafile).mapping\n    for accn, rr in ranges.items():\n        alen = sizes[accn]\n\n        a = rr[0]\n        if a.orientation == \"+\":\n            hang = a.component_beg - 1\n        else:\n            hang = alen - a.component_end\n        a.object_beg -= hang\n\n        a = rr[-1]\n        if a.orientation == \"+\":\n            hang = alen - a.component_end\n        else:\n            hang = a.component_beg - 1\n        a.object_end += hang\n\n    for a, (ts, te) in zip(agp, thickCoords):\n        if opts.nogaps and a.is_gap:\n            continue\n        if opts.bed12:\n            line = a.bedline\n            a.object_beg, a.object_end = ts, te\n            line += \"\\t\" + a.bedextra\n            print(line, file=fw)\n        elif opts.gff:\n            print(a.gffline(), file=fw)\n        else:\n            print(a.bedline, file=fw)", "response": "%prog extend agpfile componentfasta"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate(args):\n    p = OptionParser(validate.__doc__)\n\n    opts, args = p.parse_args(args)\n\n    try:\n        agpfile, componentfasta, targetfasta = args\n    except Exception as e:\n        sys.exit(p.print_help())\n\n    agp = AGP(agpfile)\n    build = Fasta(targetfasta)\n    bacs = Fasta(componentfasta, index=False)\n\n    # go through this line by line\n    for aline in agp:\n        try:\n            build_seq = build.sequence(dict(chr=aline.object,\n                start=aline.object_beg, stop=aline.object_end))\n\n            if aline.is_gap:\n                assert build_seq.upper() == aline.gap_length * 'N', \\\n                    \"gap mismatch: %s\" % aline\n            else:\n                bac_seq = bacs.sequence(dict(chr=aline.component_id,\n                    start=aline.component_beg, stop=aline.component_end,\n                    strand=aline.orientation))\n\n                assert build_seq.upper() == bac_seq.upper(), \\\n                        \"sequence mismatch: %s\" % aline\n\n            logging.debug(\"%s:%d-%d verified\" % (aline.object,\n                aline.object_beg, aline.object_end))\n\n        except Exception as e:\n            logging.error(e)", "response": "%prog validate agpfile componentfasta targetfasta\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dict with component_id => agpline", "response": "def order(self):\n        \"\"\"\n        Returns a dict with component_id => (i, agpline)\n        \"\"\"\n        d = {}\n        for (i, x) in enumerate(self):\n            if x.is_gap:\n                continue\n            xid = x.component_id\n            d[xid] = (i, x)\n\n            xid = xid.rsplit(\".\", 1)[0]  # Remove Genbank version\n            if xid not in d:\n                d[xid] = (i, x)\n\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getNorthSouthClone(self, i):\n        north = self.getAdjacentClone(i, south=False)\n        south = self.getAdjacentClone(i)\n        return north, south", "response": "Returns the adjacent clone name from both sides."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncopying the header to a new file.", "response": "def transfer_header(self, fw=sys.stdout):\n        \"\"\"\n        transfer_header() copies header to a new file.\n        print_header() creates a new header.\n        \"\"\"\n        print(\"\\n\".join(self.header), file=fw)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_one(self, object, lines, fasta, fw, newagp=None):\n        components = []\n\n        total_bp = 0\n        for line in lines:\n\n            if line.is_gap:\n                seq = 'N' * line.gap_length\n                if newagp:\n                    print(line, file=newagp)\n            else:\n                seq = fasta.sequence(dict(chr=line.component_id,\n                        start=line.component_beg,\n                        stop=line.component_end,\n                        strand=line.orientation))\n                # Check for dangling N's\n                if newagp:\n                    trimNs(seq, line, newagp)\n\n            components.append(seq)\n            total_bp += len(seq)\n\n            if self.validate:\n                assert total_bp == line.object_end, \\\n                        \"cumulative base pairs (%d) does not match (%d)\" % \\\n                        (total_bp, line.object_end)\n\n        if not newagp:\n            rec = SeqRecord(Seq(''.join(components)), id=object, description=\"\")\n            SeqIO.write([rec], fw, \"fasta\")\n            if len(rec) > 1000000:\n                logging.debug(\"Write object %s to `%s`\" % (object, fw.name))", "response": "Build a single object from a list of lines."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getAdjacentClone(self, i, south=True):\n        rr = xrange(i + 1, len(self)) if south else xrange(i - 1, -1, -1)\n        a = self[i]\n        for ix in rr:\n            x = self[ix]\n            if x.object != a.object:\n                break\n            return x\n        return None", "response": "Returns adjacent clone name either the line before or after the current\n        line."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pasa(args):\n    from jcvi.formats.base import SetFile\n    from jcvi.formats.gff import Gff\n\n    p = OptionParser(pasa.__doc__)\n    p.set_home(\"pasa\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    fastafile, gffile = args\n    transcodergff = fastafile + \".transdecoder.gff3\"\n    transcodergenomegff = fastafile + \".transdecoder.genome.gff3\"\n    if need_update((fastafile, gffile), (transcodergff, transcodergenomegff)):\n        cmd = \"{0}/scripts/pasa_asmbls_to_training_set.dbi\".format(opts.pasa_home)\n        cmd += \" --pasa_transcripts_fasta {0} --pasa_transcripts_gff3 {1}\".\\\n                format(fastafile, gffile)\n        sh(cmd)\n\n    completeids = fastafile.rsplit(\".\", 1)[0] + \".complete.ids\"\n    if need_update(transcodergff, completeids):\n        cmd = \"grep complete {0} | cut -f1 | sort -u\".format(transcodergff)\n        sh(cmd, outfile=completeids)\n\n    complete = SetFile(completeids)\n    seen = set()\n    completegff = transcodergenomegff.rsplit(\".\", 1)[0] + \".complete.gff3\"\n    fw = open(completegff, \"w\")\n    gff = Gff(transcodergenomegff)\n    for g in gff:\n        a = g.attributes\n        if \"Parent\" in a:\n            id = a[\"Parent\"][0]\n        else:\n            id = a[\"ID\"][0]\n        asmbl_id = id.split(\"|\")[0]\n        if asmbl_id not in complete:\n            continue\n        print(g, file=fw)\n        if g.type == \"gene\":\n            seen.add(id)\n\n    fw.close()\n    logging.debug(\"A total of {0} complete models extracted to `{1}`.\".\\\n                    format(len(seen), completegff))", "response": "%prog pasa fastafile gffile"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef genemark(args):\n    p = OptionParser(genemark.__doc__)\n    p.add_option(\"--junctions\", help=\"Path to `junctions.bed` from Tophat2\")\n    p.set_home(\"gmes\")\n    p.set_cpus(cpus=32)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    species, fastafile = args\n    junctions = opts.junctions\n    mhome = opts.gmes_home\n\n    license = op.expanduser(\"~/.gm_key\")\n    assert op.exists(license), \"License key ({0}) not found!\".format(license)\n    cmd = \"{0}/gmes_petap.pl --sequence {1}\".format(mhome, fastafile)\n    cmd += \" --cores {0}\".format(opts.cpus)\n    if junctions:\n        intronsgff = \"introns.gff\"\n        if need_update(junctions, intronsgff):\n            jcmd = \"{0}/bet_to_gff.pl\".format(mhome)\n            jcmd += \" --bed {0} --gff {1} --label Tophat2\".\\\n                    format(junctions, intronsgff)\n            sh(jcmd)\n        cmd += \" --ET {0} --et_score 10\".format(intronsgff)\n    else:\n        cmd += \" --ES\"\n    sh(cmd)\n\n    logging.debug(\"GENEMARK matrix written to `output/gmhmm.mod\")", "response": "%prog genemark species fastafile"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef snap(args):\n    p = OptionParser(snap.__doc__)\n    p.set_home(\"maker\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    species, gffile, fastafile = args\n    mhome = opts.maker_home\n    snapdir = \"snap\"\n    mkdir(snapdir)\n\n    cwd = os.getcwd()\n    os.chdir(snapdir)\n\n    newgffile = \"training.gff3\"\n    logging.debug(\"Construct GFF file combined with sequence ...\")\n    sh(\"cat ../{0} > {1}\".format(gffile, newgffile))\n    sh('echo \"##FASTA\" >> {0}'.format(newgffile))\n    sh(\"cat ../{0} >> {1}\".format(fastafile, newgffile))\n\n    logging.debug(\"Make models ...\")\n    sh(\"{0}/src/bin/maker2zff training.gff3\".format(mhome))\n    sh(\"{0}/exe/snap/fathom -categorize 1000 genome.ann genome.dna\".format(mhome))\n    sh(\"{0}/exe/snap/fathom -export 1000 -plus uni.ann uni.dna\".format(mhome))\n    sh(\"{0}/exe/snap/forge export.ann export.dna\".format(mhome))\n    sh(\"{0}/exe/snap/hmm-assembler.pl {1} . > {1}.hmm\".format(mhome, species))\n\n    os.chdir(cwd)\n    logging.debug(\"SNAP matrix written to `{0}/{1}.hmm`\".format(snapdir, species))", "response": "%prog snap species gffile fastafile"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef augustus(args):\n    p = OptionParser(augustus.__doc__)\n    p.add_option(\"--autotrain\", default=False, action=\"store_true\",\n                 help=\"Run autoAugTrain.pl to iteratively train AUGUSTUS\")\n    p.set_home(\"augustus\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    species, gffile, fastafile = args\n    mhome = opts.augustus_home\n    augdir = \"augustus\"\n\n    cwd = os.getcwd()\n    mkdir(augdir)\n    os.chdir(augdir)\n    target = \"{0}/config/species/{1}\".format(mhome, species)\n\n    if op.exists(target):\n        logging.debug(\"Removing existing target `{0}`\".format(target))\n        sh(\"rm -rf {0}\".format(target))\n\n    sh(\"{0}/scripts/new_species.pl --species={1}\".format(mhome, species))\n    sh(\"{0}/scripts/gff2gbSmallDNA.pl ../{1} ../{2} 1000 raw.gb\".\\\n            format(mhome, gffile, fastafile))\n    sh(\"{0}/bin/etraining --species={1} raw.gb 2> train.err\".\\\n            format(mhome, species))\n    sh(\"cat train.err | perl -pe 's/.*in sequence (\\S+): .*/$1/' > badgenes.lst\")\n    sh(\"{0}/scripts/filterGenes.pl badgenes.lst raw.gb > training.gb\".\\\n            format(mhome))\n    sh(\"grep -c LOCUS raw.gb training.gb\")\n\n    # autoAugTrain failed to execute, disable for now\n    if opts.autotrain:\n        sh(\"rm -rf {0}\".format(target))\n        sh(\"{0}/scripts/autoAugTrain.pl --trainingset=training.gb --species={1}\".\\\n                format(mhome, species))\n\n    os.chdir(cwd)\n    sh(\"cp -r {0} augustus/\".format(target))", "response": "%prog augustus species gffile fastafile"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef removecontains(args):\n    p = OptionParser(removecontains.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    contains, gkpStore = args\n\n    s = set()\n    fp = open(contains)\n    for row in fp:\n        if row[0] == '#':\n            continue\n        iid = int(row.split()[0])\n        s.add(iid)\n\n    cmd = \"gatekeeper -dumpfragments -lastfragiid {}\".format(gkpStore)\n    gkpmsg = popen(cmd).read()\n    last_iid = int(gkpmsg.strip().split()[-1])\n\n    ndeleted = 0\n    editfile = \"delete.edit\"\n    fw = open(editfile, \"w\")\n    for iid in xrange(1, last_iid + 1):\n        if iid in s:\n            print(\"frg iid {0} isdeleted 1\".format(iid), file=fw)\n            ndeleted += 1\n\n    fw.close()\n    assert len(s) == ndeleted\n    logging.debug(\"A total of {0} contained reads flagged as deleted.\"\\\n                  .format(ndeleted))\n    print(\"Now you can run:\", file=sys.stderr)\n    print(\"$ gatekeeper --edit {0} {1}\".format(editfile, gkpStore), file=sys.stderr)", "response": "%prog removecontains 4 - unitigger. contains asm. gkpStore"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef overlap(args):\n    from jcvi.apps.console import green\n\n    p = OptionParser(overlap.__doc__)\n    p.add_option(\"--maxerr\", default=2, type=\"int\", help=\"Maximum error rate\")\n    p.add_option(\"--canvas\", default=100, type=\"int\", help=\"Canvas size\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    bestcontains, iid = args\n    canvas = opts.canvas\n\n    bestcontainscache = bestcontains + \".cache\"\n    if need_update(bestcontains, bestcontainscache):\n        fp = open(bestcontains)\n        fw = open(bestcontainscache, \"w\")\n        exclude = set()\n        for row in fp:\n            if row[0] == '#':\n                continue\n            j = int(row.split()[0])\n            exclude.add(j)\n        dump(exclude, fw)\n        fw.close()\n\n    exclude = load(open(bestcontainscache))\n    logging.debug(\"A total of {0} reads to exclude\".format(len(exclude)))\n\n    cmd = \"overlapStore -d ../asm.ovlStore -b {0} -e {0}\".format(iid)\n    cmd += \" -E {0}\".format(opts.maxerr)\n    frags = []\n    for row in popen(cmd):\n        r = OverlapLine(row)\n        if r.bid in exclude:\n            continue\n        frags.append(r)\n\n    # Also include to query fragment\n    frags.append(OverlapLine(\"{0} {0} N 0 0 0 0\".format(iid)))\n    frags.sort(key=lambda x: x.ahang)\n\n    # Determine size of the query fragment\n    cmd = \"gatekeeper -b {0} -e {0}\".format(iid)\n    cmd += \" -tabular -dumpfragments ../asm.gkpStore\"\n    fp = popen(cmd)\n    row = next(fp)\n    size = int(fp.next().split()[-1])\n\n    # Determine size of canvas\n    xmin = min(x.ahang for x in frags)\n    xmax = max(x.bhang for x in frags)\n    xsize = -xmin + size + xmax\n    ratio = xsize / canvas\n\n    fw = sys.stdout\n    for f in frags:\n        fsize = -f.ahang + size + f.bhang\n        a = (f.ahang - xmin) / ratio\n        b = fsize / ratio\n        t = '-' * b\n        if f.orientation == 'N':\n            t = t[:-1] + '>'\n        else:\n            t = '<' + t[1:]\n        if f.ahang == 0 and f.bhang == 0:\n            t = green(t)\n        c = canvas - a - b\n        fw.write(' ' * a)\n        fw.write(t)\n        fw.write(' ' * c)\n        print(\"{0} ({1})\".format(str(f.bid).rjust(10), f.erate_adj), file=fw)", "response": "%prog overlap best. contains iid"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef merger(args):\n    p = OptionParser(merger.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    layout, gkpstore, contigs = args\n    fp = open(layout)\n    pf = \"0\"\n    iidfile = pf + \".iids\"\n    for i, row in enumerate(fp):\n        logging.debug(\"Read unitig {0}\".format(i))\n        fw = open(iidfile, \"w\")\n        layout = row.split(\"|\")\n        print(\"\\n\".join(layout), file=fw)\n        fw.close()\n        cmd = \"gatekeeper -iid {0}.iids -dumpfasta {0} {1}\".format(pf, gkpstore)\n        sh(cmd)\n\n        fastafile = \"{0}.fasta\".format(pf)\n        newfastafile = \"{0}.new.fasta\".format(pf)\n        format([fastafile, newfastafile, \"--sequential=replace\", \\\n                \"--sequentialoffset=1\", \"--nodesc\"])\n        fasta([newfastafile])\n\n        sh(\"rm -rf {0}\".format(pf))\n        cmd = \"runCA {0}.frg -p {0} -d {0} consensus=pbutgcns\".format(pf)\n        cmd += \" unitigger=bogart doFragmentCorrection=0 doUnitigSplitting=0\"\n        sh(cmd)\n        outdir = \"{0}/9-terminator\".format(pf)\n\n        cmd = \"cat {0}/{1}.ctg.fasta {0}/{1}.deg.fasta {0}/{1}.singleton.fasta\"\\\n                .format(outdir, pf)\n        sh(cmd, outfile=contigs, append=True)", "response": "%prog merger layout gkpStore contigs. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef astat(args):\n    p = OptionParser(astat.__doc__)\n    p.add_option(\"--cutoff\", default=1000, type=\"int\",\n                 help=\"Length cutoff [default: %default]\")\n    p.add_option(\"--genome\", default=\"\",\n                 help=\"Genome name [default: %default]\")\n    p.add_option(\"--arrDist\", default=False, action=\"store_true\",\n                 help=\"Use arrDist instead [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    covfile, = args\n    cutoff = opts.cutoff\n    genome = opts.genome\n    plot_arrDist = opts.arrDist\n\n    suffix = \".{0}\".format(cutoff)\n    small_covfile = covfile + suffix\n    update_covfile = need_update(covfile, small_covfile)\n    if update_covfile:\n        fw = open(small_covfile, \"w\")\n    else:\n        logging.debug(\"Found `{0}`, will use this one\".format(small_covfile))\n        covfile = small_covfile\n\n    fp = open(covfile)\n    header = next(fp)\n    if update_covfile:\n        fw.write(header)\n\n    data = []\n    msg = \"{0} tigs scanned ...\"\n    for row in fp:\n        tigID, rho, covStat, arrDist = row.split()\n        tigID = int(tigID)\n        if tigID % 1000000 == 0:\n            sys.stderr.write(msg.format(tigID) + \"\\r\")\n\n        rho, covStat, arrDist = [float(x) for x in (rho, covStat, arrDist)]\n        if rho < cutoff:\n            continue\n\n        if update_covfile:\n            fw.write(row)\n        data.append((tigID, rho, covStat, arrDist))\n\n    print(msg.format(tigID), file=sys.stderr)\n\n    from jcvi.graphics.base import plt, savefig\n\n    logging.debug(\"Plotting {0} data points.\".format(len(data)))\n    tigID, rho, covStat, arrDist = zip(*data)\n\n    y = arrDist if plot_arrDist else covStat\n    ytag = \"arrDist\" if plot_arrDist else \"covStat\"\n\n    fig = plt.figure(1, (7, 7))\n    ax = fig.add_axes([.12, .1, .8, .8])\n    ax.plot(rho, y, \".\", color=\"lightslategrey\")\n\n    xtag = \"rho\"\n    info = (genome, xtag, ytag)\n    title = \"{0} {1} vs. {2}\".format(*info)\n    ax.set_title(title)\n    ax.set_xlabel(xtag)\n    ax.set_ylabel(ytag)\n\n    if plot_arrDist:\n        ax.set_yscale('log')\n\n    imagename = \"{0}.png\".format(\".\".join(info))\n    savefig(imagename, dpi=150)", "response": "%prog astat coverage.log\n\n    Create coverage-rho scatter plot."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef emitFragment(fw, fragID, libID, shredded_seq, clr=None, qvchar='l', fasta=False):\n    if fasta:\n        s = SeqRecord(shredded_seq, id=fragID, description=\"\")\n        SeqIO.write([s], fw, \"fasta\")\n        return\n\n    seq = str(shredded_seq)\n    slen = len(seq)\n    qvs = qvchar * slen  # shredded reads have default low qv\n\n    if clr is None:\n        clr_beg, clr_end = 0, slen\n    else:\n        clr_beg, clr_end = clr\n\n    print(frgTemplate.format(fragID=fragID, libID=libID,\n        seq=seq, qvs=qvs, clr_beg=clr_beg, clr_end=clr_end), file=fw)", "response": "Print out the fragment with the given ID and libID."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tracedb(args):\n    p = OptionParser(tracedb.__doc__)\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(p.print_help())\n\n    action, = args\n    assert action in (\"xml\", \"lib\", \"frg\")\n\n    CMD = \"tracedb-to-frg.pl\"\n    xmls = glob(\"xml*\")\n\n    if action == \"xml\":\n        for xml in xmls:\n            cmd = CMD + \" -xml {0}\".format(xml)\n            sh(cmd, outfile=\"/dev/null\", errfile=\"/dev/null\", background=True)\n\n    elif action == \"lib\":\n        cmd = CMD + \" -lib {0}\".format(\" \".join(xmls))\n        sh(cmd)\n\n    elif action == \"frg\":\n        for xml in xmls:\n            cmd = CMD + \" -frg {0}\".format(xml)\n            sh(cmd, background=True)", "response": "%prog tracedb xml|lib | frg > availabe. xml"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking the matepairs file for the next sequence", "response": "def make_matepairs(fastafile):\n    \"\"\"\n    Assumes the mates are adjacent sequence records\n    \"\"\"\n    assert op.exists(fastafile)\n\n    matefile = fastafile.rsplit(\".\", 1)[0] + \".mates\"\n    if op.exists(matefile):\n        logging.debug(\"matepairs file `{0}` found\".format(matefile))\n    else:\n        logging.debug(\"parsing matepairs from `{0}`\".format(fastafile))\n        matefw = open(matefile, \"w\")\n        it = SeqIO.parse(fastafile, \"fasta\")\n        for fwd, rev in zip(it, it):\n            print(\"{0}\\t{1}\".format(fwd.id, rev.id), file=matefw)\n\n        matefw.close()\n\n    return matefile"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fasta(args):\n    from jcvi.formats.fasta import clean, make_qual\n\n    p = OptionParser(fasta.__doc__)\n    p.add_option(\"--clean\", default=False, action=\"store_true\",\n                 help=\"Clean up irregular chars in seq\")\n    p.add_option(\"--matefile\", help=\"Matepairs file\")\n    p.add_option(\"--maxreadlen\", default=262143, type=\"int\",\n                 help=\"Maximum read length allowed\")\n    p.add_option(\"--minreadlen\", default=1000, type=\"int\",\n                 help=\"Minimum read length allowed\")\n    p.add_option(\"--sequential\", default=False, action=\"store_true\",\n                 help=\"Overwrite read name (e.g. long Pacbio name)\")\n    p.set_size()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    fastafile, = args\n    maxreadlen = opts.maxreadlen\n    minreadlen = opts.minreadlen\n    if maxreadlen > 0:\n        split = False\n        f = Fasta(fastafile, lazy=True)\n        for id, size in f.itersizes_ordered():\n            if size > maxreadlen:\n                logging.debug(\"Sequence {0} (size={1}) longer than max read len {2}\".\\\n                                format(id, size, maxreadlen))\n                split = True\n                break\n\n        if split:\n            for f in split_fastafile(fastafile, maxreadlen=maxreadlen):\n                fasta([f, \"--maxreadlen=0\"])\n            return\n\n    plate = op.basename(fastafile).split(\".\")[0]\n\n    mated = (opts.size != 0)\n    mean, sv = get_mean_sv(opts.size)\n\n    if mated:\n        libname = \"Sanger{0}Kb-\".format(opts.size / 1000) + plate\n    else:\n        libname = plate\n\n    frgfile = libname + \".frg\"\n\n    if opts.clean:\n        cleanfasta = fastafile.rsplit(\".\", 1)[0] + \".clean.fasta\"\n        if need_update(fastafile, cleanfasta):\n            clean([fastafile, \"--canonical\", \"-o\", cleanfasta])\n        fastafile = cleanfasta\n\n    if mated:\n        qualfile = make_qual(fastafile, score=21)\n        if opts.matefile:\n            matefile = opts.matefile\n            assert op.exists(matefile)\n        else:\n            matefile = make_matepairs(fastafile)\n\n        cmd = \"convert-fasta-to-v2.pl\"\n        cmd += \" -l {0} -s {1} -q {2} \".format(libname, fastafile, qualfile)\n        if mated:\n            cmd += \"-mean {0} -stddev {1} -m {2} \".format(mean, sv, matefile)\n\n        sh(cmd, outfile=frgfile)\n        return\n\n    fw = must_open(frgfile, \"w\")\n    print(headerTemplate.format(libID=libname), file=fw)\n\n    sequential = opts.sequential\n    i = j = 0\n    for fragID, seq in parse_fasta(fastafile):\n        if len(seq) < minreadlen:\n            j += 1\n            continue\n        i += 1\n        if sequential:\n            fragID = libname + str(100000000 + i)\n        emitFragment(fw, fragID, libname, seq)\n    fw.close()\n\n    logging.debug(\"A total of {0} fragments written to `{1}` ({2} discarded).\".\\\n                    format(i, frgfile, j))", "response": "%prog fasta fastafile\n\n    Convert reads formatted as FASTA file, and convert to CA frg file. If .qual\n    file is found, then use it, otherwise just make a fake qual file. Mates are\n    assumed as adjacent sequence records (i.e. /1, /2, /1, /2 ...) unless a\n    matefile is given."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef truncate_name(name, rule=None):\n    import re\n\n    if rule is None:\n        return name\n\n    k = re.search(\"(?<=^head)[0-9]{1,2}$\", rule)\n    if k:\n        k = k.group(0)\n        tname = name[int(k):]\n    else:\n        k = re.search(\"(?<=^ohead)[0-9]{1,2}$\", rule)\n        if k:\n            k = k.group(0)\n            tname = name[:int(k)]\n        else:\n            k = re.search(\"(?<=^tail)[0-9]{1,2}$\", rule)\n            if k:\n                k = k.group(0)\n                tname = name[:-int(k)]\n            else:\n                k = re.search(\"(?<=^otail)[0-9]{1,2}$\", rule)\n                if k:\n                    k = k.group(0)\n                    tname = name[-int(k):]\n                else:\n                    print(truncate_name.__doc__, file=sys.stderr)\n                    raise ValueError('Wrong rule for truncation!')\n    return tname", "response": "shorten taxa names for tree display"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndecoding a sequence name according to a barcode map.", "response": "def decode_name(name, barcodemap):\n    \"\"\"\n    rename seq/taxon name, typically for a tree display,\n    according to a barcode map given in a dictionary\n\n    By definition barcodes should be distinctive.\n    \"\"\"\n    for barcode in barcodemap:\n        if barcode in name:\n            return barcodemap[barcode]\n\n    return name"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef draw_tree(ax, tx, rmargin=.3,\n              treecolor=\"k\", leafcolor=\"k\", supportcolor=\"k\",\n              outgroup=None, reroot=True, gffdir=None, sizes=None,\n              trunc_name=None, SH=None, scutoff=0, barcodefile=None,\n              leafcolorfile=None, leaffont=12):\n    \"\"\"\n    main function for drawing phylogenetic tree\n    \"\"\"\n\n    t = Tree(tx)\n    if reroot:\n        if outgroup:\n            R = t.get_common_ancestor(*outgroup)\n        else:\n            # Calculate the midpoint node\n            R = t.get_midpoint_outgroup()\n\n        if R != t:\n            t.set_outgroup(R)\n\n    farthest, max_dist = t.get_farthest_leaf()\n\n    margin = .05\n    xstart = margin\n    ystart = 1 - margin\n    canvas = 1 - rmargin - 2 * margin\n    tip = .005\n    # scale the tree\n    scale = canvas / max_dist\n\n    num_leaves = len(t.get_leaf_names())\n    yinterval = canvas / (num_leaves + 1)\n\n    # get exons structures, if any\n    structures = {}\n    if gffdir:\n        gffiles = glob(\"{0}/*.gff*\".format(gffdir))\n        setups, ratio = get_setups(gffiles, canvas=rmargin / 2, noUTR=True)\n        structures = dict((a, (b, c)) for a, b, c in setups)\n\n    if sizes:\n        sizes = Sizes(sizes).mapping\n\n    if barcodefile:\n        barcodemap = DictFile(barcodefile, delimiter=\"\\t\")\n\n    if leafcolorfile:\n        leafcolors = DictFile(leafcolorfile, delimiter=\"\\t\")\n\n    coords = {}\n    i = 0\n    for n in t.traverse(\"postorder\"):\n        dist = n.get_distance(t)\n        xx = xstart + scale * dist\n\n        if n.is_leaf():\n            yy = ystart - i * yinterval\n            i += 1\n\n            if trunc_name:\n                name = truncate_name(n.name, rule=trunc_name)\n            else:\n                name = n.name\n\n            if barcodefile:\n                name = decode_name(name, barcodemap)\n\n            sname = name.replace(\"_\", \"-\")\n\n            try:\n                lc = leafcolors[n.name]\n            except Exception:\n                lc = leafcolor\n            else:\n                # if color is given as \"R,G,B\"\n                if \",\" in lc:\n                    lc = map(float, lc.split(\",\"))\n\n            ax.text(xx + tip, yy, sname, va=\"center\",\n                    fontstyle=\"italic\", size=leaffont, color=lc)\n\n            gname = n.name.split(\"_\")[0]\n            if gname in structures:\n                mrnabed, cdsbeds = structures[gname]\n                ExonGlyph(ax, 1 - rmargin / 2, yy, mrnabed, cdsbeds,\n                          align=\"right\", ratio=ratio)\n            if sizes and gname in sizes:\n                size = sizes[gname]\n                size = size / 3 - 1  # base pair converted to amino acid\n                size = \"{0}aa\".format(size)\n                ax.text(1 - rmargin / 2 + tip, yy, size, size=leaffont)\n\n        else:\n            children = [coords[x] for x in n.get_children()]\n            children_x, children_y = zip(*children)\n            min_y, max_y = min(children_y), max(children_y)\n            # plot the vertical bar\n            ax.plot((xx, xx), (min_y, max_y), \"-\", color=treecolor)\n            # plot the horizontal bar\n            for cx, cy in children:\n                ax.plot((xx, cx), (cy, cy), \"-\", color=treecolor)\n            yy = sum(children_y) * 1. / len(children_y)\n            support = n.support\n            if support > 1:\n                support = support / 100.\n            if not n.is_root():\n                if support > scutoff / 100.:\n                    ax.text(xx, yy+.005, \"{0:d}\".format(int(abs(support * 100))),\n                            ha=\"right\", size=leaffont, color=supportcolor)\n\n        coords[n] = (xx, yy)\n\n    # scale bar\n    br = .1\n    x1 = xstart + .1\n    x2 = x1 + br * scale\n    yy = ystart - i * yinterval\n    ax.plot([x1, x1], [yy - tip, yy + tip], \"-\", color=treecolor)\n    ax.plot([x2, x2], [yy - tip, yy + tip], \"-\", color=treecolor)\n    ax.plot([x1, x2], [yy, yy], \"-\", color=treecolor)\n    ax.text((x1 + x2) / 2, yy - tip, \"{0:g}\".format(br),\n            va=\"top\", ha=\"center\", size=leaffont, color=treecolor)\n\n    if SH is not None:\n        xs = x1\n        ys = (margin + yy) / 2.\n        ax.text(xs, ys, \"SH test against ref tree: {0}\"\n                .format(SH), ha=\"left\", size=leaffont, color=\"g\")\n\n    normalize_axes(ax)", "response": "draw a phylogenetic tree"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndrawing a geoscale on the given axis.", "response": "def draw_geoscale(ax, minx=0, maxx=175):\n    \"\"\"\n    Draw geological epoch on million year ago (mya) scale.\n    \"\"\"\n    a, b = .1, .6  # Correspond to 200mya and 0mya\n    def cv(x): return b - (x - b) / (maxx - minx) * (b - a)\n    ax.plot((a, b), (.5, .5), \"k-\")\n    tick = .015\n    for mya in xrange(maxx - 25, 0, -25):\n        p = cv(mya)\n        ax.plot((p, p), (.5, .5 - tick), \"k-\")\n        ax.text(p, .5 - 2.5 * tick, str(mya), ha=\"center\", va=\"center\")\n    ax.text((a + b) / 2, .5 - 5 * tick, \"Time before present (million years)\",\n            ha=\"center\", va=\"center\")\n\n    # Source:\n    # http://www.weston.org/schools/ms/biologyweb/evolution/handouts/GSAchron09.jpg\n    Geo = ((\"Neogene\", 2.6, 23.0, \"#fee400\"),\n           (\"Paleogene\", 23.0, 65.5, \"#ff9a65\"),\n           (\"Cretaceous\", 65.5, 145.5, \"#80ff40\"),\n           (\"Jurassic\", 145.5, 201.6, \"#33fff3\"))\n    h = .05\n    for era, start, end, color in Geo:\n        start, end = cv(start), cv(end)\n        end = max(a, end)\n        p = Rectangle((end, .5 + tick / 2), abs(start - end),\n                      h, lw=1, ec=\"w\", fc=color)\n        ax.text((start + end) / 2, .5 + (tick + h) / 2, era,\n                ha=\"center\", va=\"center\", size=9)\n        ax.add_patch(p)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main(args):\n    p = OptionParser(main.__doc__)\n    p.add_option(\"--outgroup\", help=\"Outgroup for rerooting the tree. \" +\n                 \"Use comma to separate multiple taxa.\")\n    p.add_option(\"--noreroot\", default=False, action=\"store_true\",\n                 help=\"Don't reroot the input tree [default: %default]\")\n    p.add_option(\"--rmargin\", default=.3, type=\"float\",\n                 help=\"Set blank rmargin to the right [default: %default]\")\n    p.add_option(\"--gffdir\", default=None,\n                 help=\"The directory that contain GFF files [default: %default]\")\n    p.add_option(\"--sizes\", default=None,\n                 help=\"The FASTA file or the sizes file [default: %default]\")\n    p.add_option(\"--SH\", default=None, type=\"string\",\n                 help=\"SH test p-value [default: %default]\")\n    p.add_option(\"--scutoff\", default=0, type=\"int\",\n                 help=\"cutoff for displaying node support, 0-100 [default: %default]\")\n    p.add_option(\"--barcode\", default=None,\n                 help=\"path to seq names barcode mapping file: \"\n                 \"barcode<tab>new_name [default: %default]\")\n    p.add_option(\"--leafcolor\", default=\"k\",\n                 help=\"Font color for the OTUs, or path to a file \"\n                 \"containing color mappings: leafname<tab>color [default: %default]\")\n    p.add_option(\"--leaffont\", default=12, help=\"Font size for the OTUs\")\n    p.add_option(\"--geoscale\", default=False, action=\"store_true\",\n                 help=\"Plot geological scale\")\n\n    opts, args, iopts = p.set_image_options(args, figsize=\"8x6\")\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    datafile, = args\n    outgroup = None\n    reroot = not opts.noreroot\n    if opts.outgroup:\n        outgroup = opts.outgroup.split(\",\")\n\n    if datafile == \"demo\":\n        tx = \"\"\"(((Os02g0681100:0.1151,Sb04g031800:0.11220)1.0:0.0537,\n        (Os04g0578800:0.04318,Sb06g026210:0.04798)-1.0:0.08870)1.0:0.06985,\n        ((Os03g0124100:0.08845,Sb01g048930:0.09055)1.0:0.05332,\n        (Os10g0534700:0.06592,Sb01g030630:0.04824)-1.0:0.07886):0.09389);\"\"\"\n    else:\n        logging.debug(\"Load tree file `{0}`.\".format(datafile))\n        tx = open(datafile).read()\n\n    pf = datafile.rsplit(\".\", 1)[0]\n\n    fig = plt.figure(1, (iopts.w, iopts.h))\n    root = fig.add_axes([0, 0, 1, 1])\n\n    if opts.geoscale:\n        draw_geoscale(root)\n\n    else:\n        if op.isfile(opts.leafcolor):\n            leafcolor = \"k\"\n            leafcolorfile = opts.leafcolor\n        else:\n            leafcolor = opts.leafcolor\n            leafcolorfile = None\n\n        draw_tree(root, tx, rmargin=opts.rmargin, leafcolor=leafcolor,\n                  outgroup=outgroup, reroot=reroot, gffdir=opts.gffdir,\n                  sizes=opts.sizes, SH=opts.SH, scutoff=opts.scutoff,\n                  barcodefile=opts.barcode, leafcolorfile=leafcolorfile,\n                  leaffont=opts.leaffont)\n\n    root.set_xlim(0, 1)\n    root.set_ylim(0, 1)\n    root.set_axis_off()\n\n    image_name = pf + \".\" + iopts.format\n    savefig(image_name, dpi=iopts.dpi, iopts=iopts)", "response": "%prog newicktree\n\n    Plot Newick formatted tree. The gene structure can be plotted along if\n    --gffdir is given. The gff file needs to be `genename.gff`. If --sizes is\n    on, also show the number of amino acids.\n\n    With --barcode a mapping file can be provided to convert seq names to\n    eg. species names, useful in unified tree display. This file should have\n    distinctive barcodes in column1 and new names in column2, tab delimited."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the ML tree closest to the species tree", "response": "def run_treefix(input, stree_file, smap_file, a_ext=\".fasta\", \\\n                o_ext=\".dnd\", n_ext = \".treefix.dnd\", **kwargs):\n    \"\"\"\n    get the ML tree closest to the species tree\n    \"\"\"\n    cl = TreeFixCommandline(input=input, \\\n            stree_file=stree_file, smap_file=smap_file, a_ext=a_ext, \\\n            o=o_ext, n=n_ext, **kwargs)\n    outtreefile = input.rsplit(o_ext, 1)[0] + n_ext\n    print(\"TreeFix:\", cl, file=sys.stderr)\n    r, e = cl.run()\n\n    if e:\n        print(\"***TreeFix could not run\", file=sys.stderr)\n        return None\n    else:\n        logging.debug(\"new tree written to {0}\".format(outtreefile))\n        return outtreefile"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun Gblocks and return the alignment file", "response": "def run_gblocks(align_fasta_file, **kwargs):\n    \"\"\"\n    remove poorly aligned positions and divergent regions with Gblocks\n    \"\"\"\n    cl = GblocksCommandline(aln_file=align_fasta_file, **kwargs)\n    r, e = cl.run()\n\n    print(\"Gblocks:\", cl, file=sys.stderr)\n\n    if e:\n        print(\"***Gblocks could not run\", file=sys.stderr)\n        return None\n    else:\n        print(r, file=sys.stderr)\n        alignp = re.sub(r'.*Gblocks alignment:.*\\(([0-9]{1,3}) %\\).*', \\\n            r'\\1', r, flags=re.DOTALL)\n        alignp = int(alignp)\n        if alignp <= 10:\n            print(\"** WARNING ** Only %s %% positions retained by Gblocks. \" \\\n                \"Results aborted. Using original alignment instead.\\n\" % alignp, file=sys.stderr)\n            return None\n        else:\n            return align_fasta_file+\"-gb\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_ffitch(distfile, outtreefile, intreefile=None, **kwargs):\n    cl = FfitchCommandline(datafile=distfile, outtreefile=outtreefile, \\\n            intreefile=intreefile, **kwargs)\n    r, e = cl.run()\n\n    if e:\n        print(\"***ffitch could not run\", file=sys.stderr)\n        return None\n    else:\n        print(\"ffitch:\", cl, file=sys.stderr)\n        return outtreefile", "response": "Run ffitch on the given tree file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef smart_reroot(treefile, outgroupfile, outfile, format=0):\n    tree = Tree(treefile, format=format)\n    leaves = [t.name for t in tree.get_leaves()][::-1]\n    outgroup = []\n    for o in must_open(outgroupfile):\n        o = o.strip()\n        for leaf in leaves:\n            if leaf[:len(o)] == o:\n                outgroup.append(leaf)\n        if outgroup:\n            break\n\n    if not outgroup:\n        print(\"Outgroup not found. Tree {0} cannot be rerooted.\".format(treefile), file=sys.stderr)\n        return treefile\n\n    try:\n        tree.set_outgroup(tree.get_common_ancestor(*outgroup))\n    except ValueError:\n        assert type(outgroup) == list\n        outgroup = outgroup[0]\n        tree.set_outgroup(outgroup)\n    tree.write(outfile=outfile, format=format)\n\n    logging.debug(\"Rerooted tree printed to {0}\".format(outfile))\n    return outfile", "response": "reroots a tree using ete2"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_nj_phylip(alignment, outfile, outgroup, work_dir=\".\"):\n\n    phy_file = op.join(work_dir, \"work\", \"aln.phy\")\n    try:\n        AlignIO.write(alignment, file(phy_file, \"w\"), \"phylip\")\n    except ValueError:\n        print(\"Repeated seq name, possibly due to truncation. NJ tree not built.\", file=sys.stderr)\n        return None\n\n    seqboot_out = phy_file.rsplit(\".\",1)[0] + \".fseqboot\"\n    seqboot_cl = FSeqBootCommandline(FPHYLIP_BIN(\"fseqboot\"), \\\n        sequence=phy_file, outfile=seqboot_out, \\\n        seqtype=\"d\", reps=100, seed=12345)\n    stdout, stderr = seqboot_cl()\n    logging.debug(\"Resampling alignment: %s\" % seqboot_cl)\n\n    dnadist_out = phy_file.rsplit(\".\",1)[0] + \".fdnadist\"\n    dnadist_cl = FDNADistCommandline(FPHYLIP_BIN(\"fdnadist\"), \\\n        sequence=seqboot_out, outfile=dnadist_out, method=\"f\")\n    stdout, stderr = dnadist_cl()\n    logging.debug\\\n        (\"Calculating distance for bootstrapped alignments: %s\" % dnadist_cl)\n\n    neighbor_out = phy_file.rsplit(\".\",1)[0] + \".njtree\"\n    e = phy_file.rsplit(\".\",1)[0] + \".fneighbor\"\n    neighbor_cl = FNeighborCommandline(FPHYLIP_BIN(\"fneighbor\"), \\\n        datafile=dnadist_out, outfile=e, outtreefile=neighbor_out)\n    stdout, stderr = neighbor_cl()\n    logging.debug(\"Building Neighbor Joining tree: %s\" % neighbor_cl)\n\n    consense_out = phy_file.rsplit(\".\",1)[0] + \".consensustree.nodesupport\"\n    e = phy_file.rsplit(\".\",1)[0] + \".fconsense\"\n    consense_cl = FConsenseCommandline(FPHYLIP_BIN(\"fconsense\"), \\\n        intreefile=neighbor_out, outfile=e, outtreefile=consense_out)\n    stdout, stderr = consense_cl()\n    logging.debug(\"Building consensus tree: %s\" % consense_cl)\n\n    # distance without bootstrapping\n    dnadist_out0 = phy_file.rsplit(\".\",1)[0] + \".fdnadist0\"\n    dnadist_cl0 = FDNADistCommandline(FPHYLIP_BIN(\"fdnadist\"), \\\n        sequence=phy_file, outfile=dnadist_out0, method=\"f\")\n    stdout, stderr = dnadist_cl0()\n    logging.debug\\\n        (\"Calculating distance for original alignment: %s\" % dnadist_cl0)\n\n    # infer branch length on consensus tree\n    consensustree1 = phy_file.rsplit(\".\",1)[0] + \".consensustree.branchlength\"\n    run_ffitch(distfile=dnadist_out0, outtreefile=consensustree1, \\\n            intreefile=consense_out)\n\n    # write final tree\n    ct_s = Tree(consense_out)\n\n    if outgroup:\n        t1 = consensustree1 + \".rooted\"\n        t2 = smart_reroot(consensustree1, outgroup, t1)\n        if t2 == t1:\n            outfile = outfile.replace(\".unrooted\", \"\")\n        ct_b = Tree(t2)\n    else:\n        ct_b = Tree(consensustree1)\n\n    nodesupport = {}\n    for node in ct_s.traverse(\"postorder\"):\n        node_children = tuple(sorted([f.name for f in node]))\n        if len(node_children) > 1:\n            nodesupport[node_children] = node.dist/100.\n\n    for k,v in nodesupport.items():\n        ct_b.get_common_ancestor(*k).support = v\n    print(ct_b)\n    ct_b.write(format=0, outfile=outfile)\n\n    try:\n        s = op.getsize(outfile)\n    except OSError:\n        s = 0\n    if s:\n        logging.debug(\"NJ tree printed to %s\" % outfile)\n        return outfile, phy_file\n    else:\n        logging.debug(\"Something was wrong. NJ tree was not built.\")\n        return None", "response": "build NJ tree from DNA sequences with PHYLIP in EMBOSS"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_ml_phyml(alignment, outfile, work_dir=\".\", **kwargs):\n    phy_file = op.join(work_dir, \"work\", \"aln.phy\")\n    AlignIO.write(alignment, file(phy_file, \"w\"), \"phylip-relaxed\")\n\n    phyml_cl = PhymlCommandline(cmd=PHYML_BIN(\"phyml\"), input=phy_file, **kwargs)\n    logging.debug(\"Building ML tree using PhyML: %s\" % phyml_cl)\n    stdout, stderr = phyml_cl()\n\n    tree_file = phy_file + \"_phyml_tree.txt\"\n    if not op.exists(tree_file):\n        print(\"***PhyML failed.\", file=sys.stderr)\n        return None\n    sh(\"cp {0} {1}\".format(tree_file, outfile), log=False)\n\n    logging.debug(\"ML tree printed to %s\" % outfile)\n\n    return outfile, phy_file", "response": "build maximum likelihood tree of DNA seqs with PhyML"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_ml_raxml(alignment, outfile, work_dir=\".\", **kwargs):\n    work_dir = op.join(work_dir, \"work\")\n    mkdir(work_dir)\n    phy_file = op.join(work_dir, \"aln.phy\")\n    AlignIO.write(alignment, file(phy_file, \"w\"), \"phylip-relaxed\")\n\n    raxml_work = op.abspath(op.join(op.dirname(phy_file), \"raxml_work\"))\n    mkdir(raxml_work)\n    raxml_cl = RaxmlCommandline(cmd=RAXML_BIN(\"raxmlHPC\"), \\\n        sequences=phy_file, algorithm=\"a\", model=\"GTRGAMMA\", \\\n        parsimony_seed=12345, rapid_bootstrap_seed=12345, \\\n        num_replicates=100, name=\"aln\", \\\n        working_dir=raxml_work, **kwargs)\n\n    logging.debug(\"Building ML tree using RAxML: %s\" % raxml_cl)\n    stdout, stderr = raxml_cl()\n\n    tree_file = \"{0}/RAxML_bipartitions.aln\".format(raxml_work)\n    if not op.exists(tree_file):\n        print(\"***RAxML failed.\", file=sys.stderr)\n        sh(\"rm -rf %s\" % raxml_work, log=False)\n        return None\n    sh(\"cp {0} {1}\".format(tree_file, outfile), log=False)\n\n    logging.debug(\"ML tree printed to %s\" % outfile)\n    sh(\"rm -rf %s\" % raxml_work)\n\n    return outfile, phy_file", "response": "build a maximum likelihood tree of DNA seqs with RAxML"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun SH test using RAxML", "response": "def SH_raxml(reftree, querytree, phy_file, shout=\"SH_out.txt\"):\n    \"\"\"\n    SH test using RAxML\n\n    querytree can be a single tree or a bunch of trees (eg. from bootstrapping)\n    \"\"\"\n    assert op.isfile(reftree)\n    shout = must_open(shout, \"a\")\n\n    raxml_work = op.abspath(op.join(op.dirname(phy_file), \"raxml_work\"))\n    mkdir(raxml_work)\n    raxml_cl = RaxmlCommandline(cmd=RAXML_BIN(\"raxmlHPC\"), \\\n    sequences=phy_file, algorithm=\"h\", model=\"GTRGAMMA\", \\\n    name=\"SH\", starting_tree=reftree, bipartition_filename=querytree, \\\n    working_dir=raxml_work)\n\n    logging.debug(\"Running SH test in RAxML: %s\" % raxml_cl)\n    o, stderr = raxml_cl()\n    # hard coded\n    try:\n        pval = re.search('(Significantly.*:.*)', o).group(0)\n    except:\n        print(\"SH test failed.\", file=sys.stderr)\n    else:\n        pval = pval.strip().replace(\"\\t\",\" \").replace(\"%\",\"\\%\")\n        print(\"{0}\\t{1}\".format(op.basename(querytree), pval), file=shout)\n        logging.debug(\"SH p-value appended to %s\" % shout.name)\n\n    shout.close()\n    return shout.name"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef subalignment(alnfle, subtype, alntype=\"fasta\"):\n    aln = AlignIO.read(alnfle, alntype)\n    alnlen = aln.get_alignment_length()\n    nseq = len(aln)\n    subaln = None\n    subalnfile = alnfle.rsplit(\".\", 1)[0] + \"_{0}.{1}\".format(subtype, alntype)\n\n    if subtype == \"synonymous\":\n        for j in range( 0, alnlen, 3 ):\n            aa = None\n            for i in range(nseq):\n                codon = str(aln[i, j: j + 3].seq)\n                if codon not in CODON_TRANSLATION:\n                    break\n                if aa and CODON_TRANSLATION[codon] != aa:\n                    break\n                else:\n                    aa = CODON_TRANSLATION[codon]\n            else:\n                if subaln is None:\n                    subaln = aln[:, j: j + 3]\n                else:\n                    subaln += aln[:, j: j + 3]\n\n    if subtype == \"fourfold\":\n        for j in range( 0, alnlen, 3 ):\n            for i in range(nseq):\n                codon = str(aln[i, j: j + 3].seq)\n                if codon not in FOURFOLD:\n                    break\n            else:\n                if subaln is None:\n                    subaln = aln[:, j: j + 3]\n                else:\n                    subaln += aln[:, j: j + 3]\n\n    if subaln:\n        AlignIO.write(subaln, subalnfile, alntype)\n        return subalnfile\n    else:\n        print(\"No sites {0} selected.\".format(subtype), file=sys.stderr)\n        return None", "response": "This function returns the alignment file with subsequences of the specified subtype"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef merge_rows_local(filename, ignore=\".\", colsep=\"\\t\", local=10, \\\n    fieldcheck=True, fsep=\",\"):\n    \"\"\"\n    merge overlapping rows within given row count distance\n    \"\"\"\n    fw = must_open(filename+\".merged\", \"w\")\n    rows = file(filename).readlines()\n    rows = [row.strip().split(colsep) for row in rows]\n    l = len(rows[0])\n\n    for rowi, row in enumerate(rows):\n        n = len(rows)\n        i = rowi+1\n        while i <= min(rowi+local, n-1):\n            merge = 1\n            row2 = rows[i]\n            for j in range(l):\n                a = row[j]\n                b = row2[j]\n                if fieldcheck:\n                    a = set(a.split(fsep))\n                    a = fsep.join(sorted(list(a)))\n                    b = set(b.split(fsep))\n                    b = fsep.join(sorted(list(b)))\n\n                if all([a!=ignore, b!=ignore, a not in b, b not in a]):\n                    merge = 0\n                    i += 1\n                    break\n\n            if merge:\n                for x in range(l):\n                    if row[x] == ignore:\n                        rows[rowi][x] = row2[x]\n                    elif row[x] in row2[x]:\n                        rows[rowi][x] = row2[x]\n                    else:\n                        rows[rowi][x] = row[x]\n                row = rows[rowi]\n                rows.remove(row2)\n\n        print(colsep.join(row), file=fw)\n    fw.close()\n\n    return fw.name", "response": "Merge overlapping rows within given row count distance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_tandems(mcscanfile, tandemfile):\n    tandems = [f.strip().split(\",\") for f in file(tandemfile)]\n    fw = must_open(mcscanfile+\".withtandems\", \"w\")\n    fp = must_open(mcscanfile)\n    seen =set()\n    for i, row in enumerate(fp):\n        if row[0] == '#':\n            continue\n        anchorslist = row.strip().split(\"\\t\")\n        anchors = set([a.split(\",\")[0] for a in anchorslist])\n        anchors.remove(\".\")\n        if anchors & seen == anchors:\n            continue\n\n        newanchors = []\n        for a in anchorslist:\n            if a == \".\":\n                newanchors.append(a)\n                continue\n            for t in tandems:\n                if a in t:\n                    newanchors.append(\",\".join(t))\n                    seen.update(t)\n                    break\n            else:\n                newanchors.append(a)\n                seen.add(a)\n        print(\"\\t\".join(newanchors), file=fw)\n\n    fw.close()\n    newmcscanfile = merge_rows_local(fw.name)\n\n    logging.debug(\"Tandems added to `{0}`. Results in `{1}`\".\\\n        format(mcscanfile, newmcscanfile))\n    fp.seek(0)\n    logging.debug(\"{0} rows merged to {1} rows\".\\\n        format(len(fp.readlines()), len(file(newmcscanfile).readlines())))\n    sh(\"rm %s\" % fw.name)\n\n    return newmcscanfile", "response": "Add tandem genes to mcscan file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prepare(args):\n    from jcvi.graphics.base import discrete_rainbow\n\n    p = OptionParser(prepare.__doc__)\n    p.add_option(\"--addtandem\", help=\"path to tandemfile [default: %default]\")\n    p.add_option(\"--writecolors\", default=False, action=\"store_true\", \\\n        help=\"generate a gene_name to color mapping file which will be taken \" \\\n        \"by jcvi.apps.phylo.draw [default: %default]\")\n    p.set_outdir(outdir=\"sequences\")\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    mcscanfile, cdsfile = args\n\n    if opts.addtandem:\n        tandemfile = opts.addtandem\n        mcscanfile_with_tandems = add_tandems(mcscanfile, tandemfile)\n        mcscanfile = mcscanfile_with_tandems\n\n    seqdir = opts.outdir\n    mkdir(seqdir)\n    f = Fasta(cdsfile)\n    fp = must_open(mcscanfile)\n    if opts.writecolors:\n        fc = must_open(\"leafcolors.txt\", \"w\")\n\n    n = 0\n    for i, row in enumerate(fp):\n        row = row.strip().split(\"\\t\")\n        if i == 0:\n            l = len(row)\n            if l <= 20:\n                colors = discrete_rainbow(l, shuffle=False)[1]\n            else:\n                colors = discrete_rainbow(l, usepreset=False, shuffle=False)[1]\n                warnings.warn(\"*** WARNING ***\\n\" \\\n                    \"Too many columns. Colors may not be all distinctive.\")\n\n        assert len(row)==l, \"All rows should have same number of fields.\"\n\n        anchors = set()\n        for j, atom in enumerate(row):\n            color = \"%s,%s,%s\" % colors[j]\n            if atom == \".\":\n                continue\n            elif \",\" in atom:\n                atom = atom.split(\",\")\n                for a in atom:\n                    fc.write(\"{0}\\t{1}\\n\".format(a, color))\n                    anchors.add(a)\n            else:\n                fc.write(\"{0}\\t{1}\\n\".format(atom, color))\n                anchors.add(atom)\n\n        if len(anchors) <= 3:\n            print(\"Not enough seqs to build trees for {0}\".format(anchors), file=sys.stderr)\n            continue\n\n        pivot = row[0]\n        fw = must_open(\"%s/%s.cds\" % (seqdir, pivot), \"w\")\n        for a in anchors:\n            if a not in f:\n                print(a)\n                a = find_first_isoform(a, f)\n                assert a, a\n            arec = f[a]\n            SeqIO.write((arec), fw, \"fasta\")\n        fw.close()\n        n+=1\n\n    if opts.writecolors:\n        fc.close()\n        logging.debug(\"leaf colors written to `{0}`\".format(fc.name))\n\n    logging.debug(\"cds of {0} syntelog groups written to {1}/\".format(n, seqdir))\n\n    return seqdir", "response": "%prog prepare mcscanfile cdsfile"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndrawing one or multiple trees on one plot.", "response": "def _draw_trees(trees, nrow=1, ncol=1, rmargin=.3, iopts=None, outdir=\".\",\n    shfile=None, **kwargs):\n    \"\"\"\n    Draw one or multiple trees on one plot.\n    \"\"\"\n    from jcvi.graphics.tree import draw_tree\n\n    if shfile:\n        SHs = DictFile(shfile, delimiter=\"\\t\")\n\n    ntrees = len(trees)\n    n = nrow * ncol\n    for x in xrange(int(ceil(float(ntrees)/n))):\n        fig = plt.figure(1, (iopts.w, iopts.h)) if iopts \\\n              else plt.figure(1, (5, 5))\n        root = fig.add_axes([0, 0, 1, 1])\n\n        xiv = 1. / ncol\n        yiv = 1. / nrow\n        xstart = list(np.arange(0, 1, xiv)) * nrow\n        ystart = list(chain(*zip(*[list(np.arange(0, 1, yiv))[::-1]] * ncol)))\n        for i in xrange(n*x, n*(x+1)):\n            if i == ntrees:\n                break\n            ax = fig.add_axes([xstart[i%n], ystart[i%n], xiv, yiv])\n            f = trees.keys()[i]\n            tree = trees[f]\n            try:\n                SH = SHs[f]\n            except:\n                SH = None\n            draw_tree(ax, tree, rmargin=rmargin, reroot=False, \\\n                supportcolor=\"r\", SH=SH, **kwargs)\n\n        root.set_xlim(0, 1)\n        root.set_ylim(0, 1)\n        root.set_axis_off()\n\n        format = iopts.format if iopts else \"pdf\"\n        dpi = iopts.dpi if iopts else 300\n        if n == 1:\n            image_name = f.rsplit(\".\", 1)[0] + \".\" + format\n        else:\n            image_name = \"trees{0}.{1}\".format(x, format)\n        image_name = op.join(outdir, image_name)\n        savefig(image_name, dpi=dpi, iopts=iopts)\n        plt.clf()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef draw(args):\n    trunc_name_options = ['headn', 'oheadn', 'tailn', 'otailn']\n    p = OptionParser(draw.__doc__)\n    p.add_option(\"--input\", help=\"path to single input tree file or a dir \"\\\n                 \"containing ONLY the input tree files\")\n    p.add_option(\"--combine\", type=\"string\", default=\"1x1\", \\\n                 help=\"combine multiple trees into one plot in nrowxncol\")\n    p.add_option(\"--trunc_name\", default=None, help=\"Options are: {0}. \" \\\n                 \"truncate first n chars, retains only first n chars, \" \\\n                 \"truncate last n chars, retain only last chars. \" \\\n                 \"n=1~99. [default: %default]\".format(trunc_name_options))\n    p.add_option(\"--SH\", default=None,\n                 help=\"path to a file containing SH test p-values in format:\" \\\n                 \"tree_file_name<tab>p-values \" \\\n                 \"This file can be generated with jcvi.apps.phylo build [default: %default]\")\n    p.add_option(\"--scutoff\", default=50, type=\"int\",\n                 help=\"cutoff for displaying node support, 0-100 [default: %default]\")\n    p.add_option(\"--barcode\", default=None,\n                 help=\"path to seq/taxon name barcode mapping file: \" \\\n                 \"barcode<tab>new_name \" \\\n                 \"This option is downstream of `--trunc_name` [default: %default]\")\n    p.add_option(\"--leafcolorfile\", default=None,\n                 help=\"path to a mapping file containing font colors \" \\\n                 \"for the OTUs: leafname<tab>color [default: %default]\")\n    p.set_outdir()\n    opts, args, iopts = p.set_image_options(figsize=\"8x6\")\n    input = opts.input\n    outdir = opts.outdir\n    combine = opts.combine.split(\"x\")\n    trunc_name = opts.trunc_name\n    SH = opts.SH\n\n    mkdir(outdir)\n    if not input:\n        sys.exit(not p.print_help())\n    elif op.isfile(input):\n        trees_file = input\n        treenames = [op.basename(input)]\n    elif op.isdir(input):\n        trees_file = op.join(outdir, \"alltrees.dnd\")\n        treenames = []\n        for f in sorted(os.listdir(input)):\n            sh(\"cat {0}/{1} >> {2}\".format(input, f, trees_file), log=False)\n            treenames.append(f)\n    else:\n        sys.exit(not p.print_help())\n\n    trees = OrderedDict()\n    tree = \"\"\n    i = 0\n    for row in LineFile(trees_file, comment=\"#\", load=True).lines:\n        if i == len(treenames):\n            break\n        if not len(row):\n            continue\n\n        if \";\" in row:\n            # sanity check\n            if row.index(\";\") != len(row)-1:\n                ts = row.split(\";\")\n                for ii in xrange(len(ts)-1):\n                    ts[ii] += \";\"\n            else:\n                ts = [row]\n            for t in ts:\n                if \";\" in t:\n                    tree += t\n                    if tree:\n                        trees[treenames[i]] = tree\n                        tree = \"\"\n                        i+=1\n                else:\n                    tree += t\n        else:\n            tree += row\n\n    logging.debug(\"A total of {0} trees imported.\".format(len(trees)))\n    sh(\"rm {0}\".format(op.join(outdir, \"alltrees.dnd\")))\n\n    _draw_trees(trees, nrow=int(combine[0]), ncol=int(combine[1]), rmargin=.3,\\\n         iopts=iopts, outdir=outdir, shfile=SH, trunc_name=trunc_name, \\\n         scutoff=opts.scutoff, barcodefile = opts.barcode,\n         leafcolorfile=opts.leafcolorfile)", "response": "%prog draw --input newicktrees [options]\n\n    Draw phylogenetic trees into single or combined plots.\n    Input trees should be one of the following:\n    1.  single Newick format tree file\n    2.  a dir containing *ONLY* the tree files to be drawn\n\n    Newick format:\n    http://evolution.genetics.washington.edu/phylip/newicktree.html\n\n    This function wraps on jcvi.graphics.tree\n    This function is better used for trees generated by jcvi.apps.phylo (rooted\n    if possible). For drawing general Newick trees from external sources invoke\n    jcvi.graphics.tree directly, which also gives more drawing options."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef enrich(args):\n    p = OptionParser(enrich.__doc__)\n    p.add_option(\"--ghost\", default=False, action=\"store_true\",\n                 help=\"Add ghost homologs already used [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    omgfile, groupsfile, ntaxa = args\n    ntaxa = int(ntaxa)\n    ghost = opts.ghost\n\n    # Get gene pair => weight mapping\n    weights = get_edges()\n    info = get_info()\n    # Get gene => taxon mapping\n    info = dict((k, v.split()[5]) for k, v in info.items())\n\n    groups = Grouper()\n\n    fp = open(groupsfile)\n    for row in fp:\n        members = row.strip().split(\",\")\n        groups.join(*members)\n\n    logging.debug(\"Imported {0} families with {1} members.\".\\\n                    format(len(groups), groups.num_members))\n\n    seen = set()\n    omggroups = Grouper()\n    fp = open(omgfile)\n    for row in fp:\n        genes, idxs = row.split()\n        genes = genes.split(\",\")\n        seen.update(genes)\n        omggroups.join(*genes)\n\n    nmembers = omggroups.num_members\n    logging.debug(\"Imported {0} OMG families with {1} members.\".\\\n                    format(len(omggroups), nmembers))\n    assert nmembers == len(seen)\n\n    alltaxa = set(str(x) for x in range(ntaxa))\n    recruited = []\n    fp = open(omgfile)\n    for row in fp:\n        genes, idxs = row.split()\n        genes = genes.split(\",\")\n        a = genes[0]\n\n        idxs = set(idxs.split(\",\"))\n        missing_taxa = alltaxa - idxs\n        if not missing_taxa:\n            print(row.rstrip())\n            continue\n\n        leftover = groups[a]\n        if not ghost:\n            leftover = set(leftover) - seen\n\n        if not leftover:\n            print(row.rstrip())\n            continue\n\n        leftover_sorted_by_taxa = dict((k, \\\n                             [x for x in leftover if info[x] == k]) \\\n                                for k in missing_taxa)\n\n        #print genes, leftover\n        #print leftover_sorted_by_taxa\n        solutions = []\n        for solution in product(*leftover_sorted_by_taxa.values()):\n            score = sum(weights.get((a, b), 0) for a in solution for b in genes)\n            if score == 0:\n                continue\n            score += sum(weights.get((a, b), 0) for a, b in combinations(solution, 2))\n            solutions.append((score, solution))\n            #print solution, score\n\n        best_solution = max(solutions) if solutions else None\n        if best_solution is None:\n            print(row.rstrip())\n            continue\n\n        #print \"best ==>\", best_solution\n        best_score, best_addition = best_solution\n        genes.extend(best_addition)\n        recruited.extend(best_addition)\n\n        genes = sorted([(info[x], x) for x in genes])\n        idxs, genes = zip(*genes)\n\n        if ghost:  # decorate additions so it's clear that they were added\n            pgenes = []\n            for g in genes:\n                if g in recruited and g in seen:\n                    pgenes.append(\"|{0}|\".format(g))\n                else:\n                    pgenes.append(g)\n            genes = pgenes\n\n        print(\"\\t\".join((\",\".join(genes), \",\".join(idxs))))\n        if not ghost:\n            seen.update(best_addition)\n\n    logging.debug(\"Recruited {0} new genes.\".format(len(recruited)))", "response": "%prog enrich omgfile groups ntaxa > enriched. omgfile"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sort_layout(thread, listfile, column=0):\n    from jcvi.formats.base import DictFile\n\n    outfile = listfile.rsplit(\".\", 1)[0] + \".sorted.list\"\n    threadorder = thread.order\n    fw = open(outfile, \"w\")\n    lt = DictFile(listfile, keypos=column, valuepos=None)\n    threaded = []\n    imported = set()\n    for t in thread:\n        accn = t.accn\n        if accn not in lt:\n            continue\n\n        imported.add(accn)\n        atoms = lt[accn]\n        threaded.append(atoms)\n\n    assert len(threaded) == len(imported)\n\n    total = sum(1 for x in open(listfile))\n    logging.debug(\"Total: {0}, currently threaded: {1}\".format(total, len(threaded)))\n    fp = open(listfile)\n    for row in fp:\n        atoms = row.split()\n        accn = atoms[0]\n        if accn in imported:\n            continue\n        insert_into_threaded(atoms, threaded, threadorder)\n\n    for atoms in threaded:\n        print(\"\\t\".join(atoms), file=fw)\n\n    fw.close()\n    logging.debug(\"File `{0}` sorted to `{1}`.\".format(outfile, thread.filename))", "response": "Sort the syntelog table according to chromomomal positions."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef omg(args):\n    p = OptionParser(omg.__doc__)\n\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    weightsfiles = args\n    groupfile = group(weightsfiles + [\"--outfile=groups\"])\n\n    weights = get_weights(weightsfiles)\n    info = get_info()\n\n    fp = open(groupfile)\n\n    work = \"work\"\n    mkdir(work)\n    for i, row in enumerate(fp):\n        gf = op.join(work, \"gf{0:05d}\".format(i))\n        genes = row.rstrip().split(\",\")\n\n        fw = open(gf, \"w\")\n        contents = \"\"\n        npairs = 0\n        for gene in genes:\n            gene_pairs = weights[gene]\n            for a, b, c in gene_pairs:\n                if b not in genes:\n                    continue\n\n                contents += \"weight {0}\".format(c) + '\\n'\n                contents += info[a] + '\\n'\n                contents += info[b] + '\\n\\n'\n                npairs += 1\n\n        header = \"a group of genes  :length ={0}\".format(npairs)\n        print(header, file=fw)\n        print(contents, file=fw)\n\n        fw.close()", "response": "%prog omg weightsfile\n\n    Run Sankoff's OMG algorithm to get orthologs. Download OMG code at:\n    <http://137.122.149.195/IsbraSoftware/OMGMec.html>\n\n    This script only writes the partitions, but not launch OMGMec. You may need to:\n\n    $ parallel \"java -cp ~/code/OMGMec TestOMGMec {} 4 > {}.out\" ::: work/gf?????\n\n    Then followed by omgparse() to get the gene lists."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ortholog(args):\n    from jcvi.apps.align import last as last_main\n    from jcvi.compara.blastfilter import main as blastfilter_main\n    from jcvi.compara.quota import main as quota_main\n    from jcvi.compara.synteny import scan, mcscan, liftover\n    from jcvi.formats.blast import cscore, filter\n\n    p = OptionParser(ortholog.__doc__)\n    p.add_option(\"--dbtype\", default=\"nucl\",\n                 choices=(\"nucl\", \"prot\"),\n                 help=\"Molecule type of subject database\")\n    p.add_option(\"--full\", default=False, action=\"store_true\",\n                 help=\"Run in full mode, including blocks and RBH\")\n    p.add_option(\"--cscore\", default=0.7, type=\"float\",\n                 help=\"C-score cutoff [default: %default]\")\n    p.add_option(\"--dist\", default=20, type=\"int\",\n                 help=\"Extent of flanking regions to search\")\n    p.add_option(\"--quota\", help=\"Quota align parameter\")\n    p.add_option(\"--nostdpf\", default=False, action=\"store_true\",\n            help=\"Do not standardize contig names\")\n    p.add_option(\"--no_strip_names\", default=False, action=\"store_true\",\n            help=\"Do not strip alternative splicing \"\n            \"(e.g. At5g06540.1 -> At5g06540)\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    a, b = args\n    dbtype = opts.dbtype\n    suffix = \".cds\" if dbtype == \"nucl\" else \".pep\"\n    abed, afasta = a + \".bed\", a + suffix\n    bbed, bfasta = b + \".bed\", b + suffix\n    ccscore = opts.cscore\n    quota = opts.quota\n    dist = \"--dist={0}\".format(opts.dist)\n\n    aprefix = afasta.split(\".\")[0]\n    bprefix = bfasta.split(\".\")[0]\n    pprefix = \".\".join((aprefix, bprefix))\n    qprefix = \".\".join((bprefix, aprefix))\n    last = pprefix + \".last\"\n    if need_update((afasta, bfasta), last):\n        last_main([bfasta, afasta], dbtype)\n\n    if a == b:\n        lastself = last + \".P98L0.inverse\"\n        if need_update(last, lastself):\n            filter([last, \"--hitlen=0\", \"--pctid=98\", \"--inverse\", \"--noself\"])\n        last = lastself\n\n    filtered_last = last + \".filtered\"\n    if need_update(last, filtered_last):\n        if opts.no_strip_names:\n            blastfilter_main([last, \"--cscore={0}\".format(ccscore), \"--no_strip_names\"])\n        else:\n            blastfilter_main([last, \"--cscore={0}\".format(ccscore)])\n\n    anchors = pprefix + \".anchors\"\n    lifted_anchors = pprefix + \".lifted.anchors\"\n    pdf = pprefix + \".pdf\"\n    if not opts.full:\n        if need_update(filtered_last, lifted_anchors):\n            if opts.no_strip_names:\n                scan([filtered_last, anchors, dist,\n                        \"--liftover={0}\".format(last), \"--no_strip_names\"])\n            else:\n                scan([filtered_last, anchors, dist,\n                        \"--liftover={0}\".format(last)])\n        if quota:\n            quota_main([lifted_anchors,\n                        \"--quota={0}\".format(quota), \"--screen\"])\n        if need_update(anchors, pdf):\n            from jcvi.graphics.dotplot import dotplot_main\n            dargs = [anchors]\n            if opts.nostdpf:\n                dargs += [\"--nostdpf\", \"--skipempty\"]\n            dotplot_main(dargs)\n        return\n\n    if need_update(filtered_last, anchors):\n        if opts.no_strip_names:\n            scan([filtered_last, anchors, dist, \"--no_strip_names\"])\n        else:\n            scan([filtered_last, anchors, dist])\n\n    ooanchors = pprefix + \".1x1.anchors\"\n    if need_update(anchors, ooanchors):\n        quota_main([anchors, \"--quota=1:1\", \"--screen\"])\n\n    lifted_anchors = pprefix + \".1x1.lifted.anchors\"\n    if need_update((last, ooanchors), lifted_anchors):\n        if opts.no_strip_names:\n            liftover([last, ooanchors, dist, \"--no_strip_names\"])\n        else:\n            liftover([last, ooanchors, dist])\n\n    pblocks = pprefix + \".1x1.blocks\"\n    qblocks = qprefix + \".1x1.blocks\"\n    if need_update(lifted_anchors, [pblocks, qblocks]):\n        mcscan([abed, lifted_anchors, \"--iter=1\", \"-o\", pblocks])\n        mcscan([bbed, lifted_anchors, \"--iter=1\", \"-o\", qblocks])\n\n    rbh = pprefix + \".rbh\"\n    if need_update(last, rbh):\n        cscore([last, \"-o\", rbh])\n\n    portho = pprefix + \".ortholog\"\n    qortho = qprefix + \".ortholog\"\n    if need_update([pblocks, qblocks, rbh], [portho, qortho]):\n        make_ortholog(pblocks, rbh, portho)\n        make_ortholog(qblocks, rbh, qortho)", "response": "%prog ortholog species_a species_b"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tandem(args):\n    p = OptionParser(tandem.__doc__)\n    p.add_option(\"--tandem_Nmax\", dest=\"tandem_Nmax\", type=\"int\", default=3,\n               help=\"merge tandem genes within distance [default: %default]\")\n    p.add_option(\"--percent_overlap\", type=\"int\", default=50,\n               help=\"tandem genes have >=x% aligned sequence, x=0-100 \\\n               [default: %default]\")\n    p.set_align(evalue=.01)\n    p.add_option(\"--not_self\", default=False, action=\"store_true\",\n                 help=\"provided is not self blast file [default: %default]\")\n    p.add_option(\"--strip_gene_name\", dest=\"sep\", type=\"string\", default=\".\",\n               help=\"strip alternative splicing. Use None for no stripping. \\\n               [default: %default]\")\n    p.add_option(\"--genefamily\", dest=\"genefam\", action=\"store_true\",\n                 help=\"compile gene families based on similarity [default: %default]\")\n    p.set_outfile()\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    blast_file, cds_file, bed_file = args\n    N = opts.tandem_Nmax\n    P = opts.percent_overlap\n    is_self = not opts.not_self\n    sep = opts.sep\n    ofile = opts.outfile\n\n    tandem_main(blast_file, cds_file, bed_file, N=N, P=P, is_self=is_self, \\\n        evalue=opts.evalue, strip_name=sep, ofile=ofile, genefam=opts.genefam)", "response": "%prog tandem blast cds_file bed_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a query string and return a dict.", "response": "def parse_qs(qs, keep_blank_values=0, strict_parsing=0, keep_attr_order=True):\n    \"\"\"\n    Kind of like urlparse.parse_qs, except returns an ordered dict.\n    Also avoids replicating that function's bad habit of overriding the\n    built-in 'dict' type.\n\n    Taken from below with modification:\n    <https://bitbucket.org/btubbs/thumpy/raw/8cdece404f15/thumpy.py>\n    \"\"\"\n    od = DefaultOrderedDict(list) if keep_attr_order else defaultdict(list)\n    for name, value in parse_qsl(qs, keep_blank_values, strict_parsing):\n        od[name].append(value)\n\n    return od"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef index(self, item):\n        'Find the position of an item.  Raise ValueError if not found.'\n        k = self._key(item)\n        i = bisect_left(self._keys, k)\n        j = bisect_right(self._keys, k)\n        return self._items[i:j].index(item) + i", "response": "Find the position of an item. Raise ValueError if not found."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninserting a new item. If equal keys are found add to the left", "response": "def insert(self, item):\n        'Insert a new item.  If equal keys are found, add to the left'\n        k = self._key(item)\n        i = bisect_left(self._keys, k)\n        self._keys.insert(i, k)\n        self._items.insert(i, item)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef insert_right(self, item):\n        'Insert a new item.  If equal keys are found, add to the right'\n        k = self._key(item)\n        i = bisect_right(self._keys, k)\n        self._keys.insert(i, k)\n        self._items.insert(i, item)", "response": "Insert a new item. If equal keys are found add to the right"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove first occurence of item. Raise ValueError if not found", "response": "def remove(self, item):\n        'Remove first occurence of item.  Raise ValueError if not found'\n        i = self.index(item)\n        del self._keys[i]\n        del self._items[i]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn first item with a key > = equal to item. Raise ValueError if not found", "response": "def find_ge(self, item):\n        'Return first item with a key >= equal to item.  Raise ValueError if not found'\n        k = self._key(item)\n        i = bisect_left(self._keys, k)\n        if i != len(self):\n            return self._items[i]\n        raise ValueError('No item found with key at or above: %r' % (k,))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_gt(self, item):\n        'Return first item with a key > item.  Raise ValueError if not found'\n        k = self._key(item)\n        i = bisect_right(self._keys, k)\n        if i != len(self):\n            return self._items[i]\n        raise ValueError('No item found with key above: %r' % (k,))", "response": "Return first item with a key > item. Raise ValueError if not found"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extract_pairs(abed, bbed, groups):\n    agenome = op.basename(abed.filename).split(\".\")[0]\n    bgenome = op.basename(bbed.filename).split(\".\")[0]\n    aorder = abed.order\n    border = bbed.order\n    pairsfile = \"{0}.{1}.pairs\".format(agenome, bgenome)\n    fw = open(pairsfile, \"w\")\n\n    is_self = abed.filename == bbed.filename\n    npairs = 0\n    for group in groups:\n        iter = combinations(group, 2) if is_self \\\n                    else product(group, repeat=2)\n\n        for a, b in iter:\n            if a not in aorder or b not in border:\n                continue\n\n            print(\"\\t\".join((a, b)), file=fw)\n            npairs += 1\n\n    logging.debug(\"File `{0}` written with {1} pairs.\".format(pairsfile, npairs))", "response": "Called by fromgroups(), extract pairs specific to a pair of species."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calc(args):\n    from jcvi.formats.fasta import translate\n\n    p = OptionParser(calc.__doc__)\n    p.add_option(\"--longest\", action=\"store_true\",\n                 help=\"Get longest ORF, only works if no pep file, \"\\\n                      \"e.g. ESTs [default: %default]\")\n    p.add_option(\"--msa\", default=\"clustalw\", choices=(\"clustalw\", \"muscle\"),\n                 help=\"software used to align the proteins [default: %default]\")\n    p.add_option(\"--workdir\", default=os.getcwd(), help=\"Work directory\")\n    p.set_outfile()\n\n    opts, args = p.parse_args(args)\n\n    if len(args) == 1:\n        protein_file, dna_file = None, args[0]\n    elif len(args) == 2:\n        protein_file, dna_file = args\n    else:\n        print(\"Incorrect arguments\", file=sys.stderr)\n        sys.exit(not p.print_help())\n\n    output_h = must_open(opts.outfile, \"w\")\n    print(fields, file=output_h)\n    work_dir = op.join(opts.workdir, \"syn_analysis\")\n    mkdir(work_dir)\n\n    if not protein_file:\n        protein_file = dna_file + \".pep\"\n        translate_args = [dna_file, \"--outfile=\" + protein_file]\n        if opts.longest:\n            translate_args += [\"--longest\"]\n        dna_file, protein_file = translate(translate_args)\n\n    prot_iterator = SeqIO.parse(open(protein_file), \"fasta\")\n    dna_iterator = SeqIO.parse(open(dna_file), \"fasta\")\n    for p_rec_1, p_rec_2, n_rec_1, n_rec_2 in \\\n            zip(prot_iterator, prot_iterator, dna_iterator, dna_iterator):\n\n        print(\"--------\", p_rec_1.name, p_rec_2.name, file=sys.stderr)\n        if opts.msa == \"clustalw\":\n            align_fasta = clustal_align_protein((p_rec_1, p_rec_2), work_dir)\n        elif opts.msa == \"muscle\":\n            align_fasta = muscle_align_protein((p_rec_1, p_rec_2), work_dir)\n        mrtrans_fasta = run_mrtrans(align_fasta, (n_rec_1, n_rec_2), work_dir)\n        if mrtrans_fasta:\n            ds_subs_yn, dn_subs_yn, ds_subs_ng, dn_subs_ng = \\\n                    find_synonymous(mrtrans_fasta, work_dir)\n            if ds_subs_yn is not None:\n                pair_name = \"%s;%s\" % (p_rec_1.name, p_rec_2.name)\n                output_h.write(\"%s\\n\" % (\",\".join(str(x) for x in (pair_name,\n                        ds_subs_yn, dn_subs_yn, ds_subs_ng, dn_subs_ng))))\n                output_h.flush()\n\n    # Clean-up\n    sh(\"rm -rf 2YN.t 2YN.dN 2YN.dS rst rub rst1 syn_analysis\")", "response": "%prog calc prot. fasta nu. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_synonymous(input_file, work_dir):\n    cwd = os.getcwd()\n    os.chdir(work_dir)\n    # create the .ctl file\n    ctl_file = \"yn-input.ctl\"\n    output_file = \"nuc-subs.yn\"\n    ctl_h = open(ctl_file, \"w\")\n    ctl_h.write(\"seqfile = %s\\noutfile = %s\\nverbose = 0\\n\" %\n                (op.basename(input_file), output_file))\n    ctl_h.write(\"icode = 0\\nweighting = 0\\ncommonf3x4 = 0\\n\")\n    ctl_h.close()\n\n    cl = YnCommandline(ctl_file)\n    print(\"\\tyn00:\", cl, file=sys.stderr)\n    r, e = cl.run()\n    ds_value_yn = None\n    ds_value_ng = None\n    dn_value_yn = None\n    dn_value_ng = None\n\n    # Nei-Gojobori\n    output_h = open(output_file)\n    row = output_h.readline()\n    while row:\n        if row.find(\"Nei & Gojobori\") >=0:\n            for x in xrange(5):\n                row = next(output_h)\n            dn_value_ng, ds_value_ng = row.split('(')[1].split(')')[0].split()\n            break\n        row = output_h.readline()\n    output_h.close()\n\n    # Yang\n    output_h = open(output_file)\n    for line in output_h:\n        if line.find(\"+-\") >= 0 and line.find(\"dS\") == -1:\n            parts = line.split(\" +-\")\n            ds_value_yn = extract_subs_value(parts[1])\n            dn_value_yn = extract_subs_value(parts[0])\n\n    if ds_value_yn is None or ds_value_ng is None:\n        h = open(output_file)\n        print(\"yn00 didn't work: \\n%s\" % h.read(), file=sys.stderr)\n\n    os.chdir(cwd)\n    return ds_value_yn, dn_value_yn, ds_value_ng, dn_value_ng", "response": "Run yn00 to find the synonymous subsitution rate for the alignment."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\naligns nucleotide sequences with mrtrans and the protein alignment.", "response": "def run_mrtrans(align_fasta, recs, work_dir, outfmt=\"paml\"):\n    \"\"\"Align nucleotide sequences with mrtrans and the protein alignment.\n    \"\"\"\n    align_file = op.join(work_dir, \"prot-align.fasta\")\n    nuc_file = op.join(work_dir, \"nuc.fasta\")\n    output_file = op.join(work_dir, \"nuc-align.mrtrans\")\n\n    # make the prot_align file and nucleotide file\n    align_h0 = open(align_file + \"0\", \"w\")\n    align_h0.write(str(align_fasta))\n    align_h0.close()\n    prot_seqs = {}\n    i = 0\n    for rec in SeqIO.parse(align_h0.name, \"fasta\"):\n        prot_seqs[i] = rec.seq\n        i += 1\n    align_h = open(align_file, \"w\")\n    for i, rec in enumerate(recs):\n        if len(rec.id) > 30:\n            rec.id = rec.id[:28] + \"_\" + str(i)\n            rec.description = \"\"\n        print(\">{0}\\n{1}\".format(rec.id, prot_seqs[i]), file=align_h)\n    align_h.close()\n    SeqIO.write(recs, file(nuc_file, \"w\"), \"fasta\")\n\n    # run the program\n    cl = MrTransCommandline(align_file, nuc_file, output_file, outfmt=outfmt)\n    r, e = cl.run()\n    if e is None:\n        print(\"\\tpal2nal:\", cl, file=sys.stderr)\n        return output_file\n    elif e.read().find(\"could not translate\") >= 0:\n        print(\"***pal2nal could not translate\", file=sys.stderr)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clustal_align_protein(recs, work_dir, outfmt=\"fasta\"):\n    fasta_file = op.join(work_dir, \"prot-start.fasta\")\n    align_file = op.join(work_dir, \"prot.aln\")\n    SeqIO.write(recs, file(fasta_file, \"w\"), \"fasta\")\n\n    clustal_cl = ClustalwCommandline(cmd=CLUSTALW_BIN(\"clustalw2\"),\n            infile=fasta_file, outfile=align_file, outorder=\"INPUT\",\n            type=\"PROTEIN\")\n    stdout, stderr = clustal_cl()\n\n    aln_file = file(clustal_cl.outfile)\n    alignment = AlignIO.read(aln_file, \"clustal\")\n    print(\"\\tDoing clustalw alignment: %s\" % clustal_cl, file=sys.stderr)\n    if outfmt == \"fasta\":\n        return alignment.format(\"fasta\")\n    if outfmt == \"clustal\":\n        return alignment", "response": "Align given proteins with clustalw."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef muscle_align_protein(recs, work_dir, outfmt=\"fasta\", inputorder=True):\n    fasta_file = op.join(work_dir, \"prot-start.fasta\")\n    align_file = op.join(work_dir, \"prot.aln\")\n    SeqIO.write(recs, file(fasta_file, \"w\"), \"fasta\")\n\n    muscle_cl = MuscleCommandline(cmd=MUSCLE_BIN(\"muscle\"),\n            input=fasta_file, out=align_file, seqtype=\"protein\",\n            clwstrict=True)\n    stdout, stderr = muscle_cl()\n    alignment = AlignIO.read(muscle_cl.out, \"clustal\")\n\n    if inputorder:\n        try:\n            muscle_inputorder(muscle_cl.input, muscle_cl.out)\n        except ValueError:\n            return \"\"\n        alignment = AlignIO.read(muscle_cl.out, \"fasta\")\n\n    print(\"\\tDoing muscle alignment: %s\" % muscle_cl, file=sys.stderr)\n    if outfmt == \"fasta\":\n        return alignment.format(\"fasta\")\n    if outfmt == \"clustal\":\n        return alignment.format(\"clustal\")", "response": "Align given proteins with muscle."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef muscle_inputorder(inputfastafile, alnfile, trunc_name=True):\n    sh(\"cp {0} {0}.old\".format(alnfile), log=False)\n    maxi = 30 if trunc_name else 1000\n\n    aa = AlignIO.read(alnfile, \"clustal\")\n    alignment = dict((a.id[:maxi], a) for a in aa)\n    if trunc_name and len(alignment) < len(aa):\n        raise ValueError\\\n            (\"ERROR: The first 30 chars of your seq names are not unique\")\n\n    fw = must_open(alnfile, \"w\")\n    for rec in SeqIO.parse(inputfastafile, \"fasta\"):\n        a = alignment[rec.id[:maxi]]\n        fw.write(\">{0}\\n{1}\\n\".format(a.id[:maxi], a.seq))\n\n    fw.close()\n    sh(\"rm {0}.old\".format(alnfile), log=False)", "response": "This function will sort the sequences in order to get the order of the sequence in order to be used in the MUSCle alignment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef subset(args):\n    p = OptionParser(subset.__doc__)\n    p.add_option(\"--noheader\", action=\"store_true\",\n                 help=\"don't write ksfile header line [default: %default]\")\n    p.add_option(\"--block\", action=\"store_true\",\n                 help=\"preserve block structure in input [default: %default]\")\n    p.set_stripnames()\n    p.set_outfile()\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    pairsfile, ksfiles = args[0], args[1:]\n    noheader = opts.noheader\n    block = opts.block\n    if block:\n        noheader = True\n    outfile = opts.outfile\n\n    ksvals = {}\n    for ksfile in ksfiles:\n        ksvals.update(dict((line.name, line) for line in \\\n                        KsFile(ksfile, strip_names=opts.strip_names)))\n\n    fp = open(pairsfile)\n    fw = must_open(outfile, \"w\")\n\n    if not noheader:\n        print(fields, file=fw)\n\n    i = j = 0\n    for row in fp:\n        if row[0] == '#':\n            if block:\n                print(row.strip(), file=fw)\n            continue\n        a, b = row.split()[:2]\n        name = \";\".join((a, b))\n        if name not in ksvals:\n            name = \";\".join((b, a))\n            if name not in ksvals:\n                j += 1\n                print(\"\\t\".join((a, b, \".\", \".\")), file=fw)\n                continue\n        ksline = ksvals[name]\n        if block:\n            print(\"\\t\".join(str(x) for x in (a, b, ksline.ks)), file=fw)\n        else:\n            ksline.name = \";\".join((a, b))\n            print(ksline, file=fw)\n        i += 1\n    fw.close()\n\n    logging.debug(\"{0} pairs not found in ksfiles\".format(j))\n    logging.debug(\"{0} ks records written to `{1}`\".format(i, outfile))\n    return outfile", "response": "%prog subset pairsfile ksfile1 ksfile2... - o pairs. ks\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the mixture of the data.", "response": "def get_mixture(data, components):\n    \"\"\"\n    probs = [.476, .509]\n    mus = [.69069, -.15038]\n    variances = [.468982e-1, .959052e-1]\n    \"\"\"\n    from jcvi.apps.base import popen\n\n    probs, mus, sigmas = [], [], []\n    fw = must_open(\"tmp\", \"w\")\n    log_data = [log(x) for x in data if x > .05]\n    data = \"\\n\".join([\"%.4f\" % x for x in log_data]).replace(\"inf\\n\", \"\")\n    fw.write(data)\n    fw.close()\n\n    cmd = \"gmm-bic {0} {1} {2}\".format(components, len(log_data), fw.name)\n    pipe = popen(cmd)\n\n    for row in pipe:\n        if row[0] != '#':\n            continue\n\n        atoms = row.split(\",\")\n        a, b, c = atoms[1:4]\n        a = float(a)\n        b = float(b)\n        c = float(c)\n\n        mus.append(a)\n        sigmas.append(b)\n        probs.append(c)\n\n    os.remove(fw.name)\n    return probs, mus, sigmas"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef report(args):\n    '''\n    %prog report ksfile\n\n    generate a report given a Ks result file (as produced by synonymous_calc.py).\n    describe the median Ks, Ka values, as well as the distribution in stem-leaf plot\n    '''\n    from jcvi.utils.cbook import SummaryStats\n    from jcvi.graphics.histogram import stem_leaf_plot\n\n    p = OptionParser(report.__doc__)\n    p.add_option(\"--pdf\", default=False, action=\"store_true\",\n            help=\"Generate graphic output for the histogram [default: %default]\")\n    p.add_option(\"--components\", default=1, type=\"int\",\n            help=\"Number of components to decompose peaks [default: %default]\")\n    add_plot_options(p)\n    opts, args, iopts = p.set_image_options(args, figsize=\"5x5\")\n\n    if len(args) !=  1:\n        sys.exit(not p.print_help())\n\n    ks_file, = args\n    data = KsFile(ks_file)\n    ks_min = opts.vmin\n    ks_max = opts.vmax\n    bins = opts.bins\n\n    for f in fields.split(\",\")[1:]:\n        columndata = [getattr(x, f) for x in data]\n        ks = (\"ks\" in f)\n        if not ks:\n            continue\n\n        columndata = [x for x in columndata if ks_min <= x <= ks_max]\n\n        st = SummaryStats(columndata)\n        title = \"{0} ({1}): \".format(descriptions[f], ks_file)\n        title += \"Median:{0:.3f} (1Q:{1:.3f}|3Q:{2:.3f}||\".\\\n                format(st.median, st.firstq, st.thirdq)\n        title += \"Mean:{0:.3f}|Std:{1:.3f}||N:{2})\".\\\n                format(st.mean, st.sd, st.size)\n\n        tbins = (0, ks_max, bins) if ks else (0, .6, 10)\n        digit = 2 if (ks_max * 1. / bins) < .1 else 1\n        stem_leaf_plot(columndata, *tbins, digit=digit, title=title)\n\n    if not opts.pdf:\n        return\n\n    components = opts.components\n    data = [x.ng_ks for x in data]\n    data = [x for x in data if ks_min <= x <= ks_max]\n\n    fig = plt.figure(1, (iopts.w, iopts.h))\n    ax = fig.add_axes([.12, .1, .8, .8])\n    kp = KsPlot(ax, ks_max, opts.bins, legendp=opts.legendp)\n    kp.add_data(data, components, fill=opts.fill, fitted=opts.fit)\n    kp.draw(title=opts.title)", "response": "%prog report ksfile\nMimeType"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef passthrough(args):\n    p = OptionParser(passthrough.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    vcffile, newvcffile = args\n    fp = open(vcffile)\n    fw = open(newvcffile, \"w\")\n    gg = [\"0/0\", \"0/1\", \"1/1\"]\n    for row in fp:\n        if row[0] == \"#\":\n            print(row.strip(), file=fw)\n            continue\n\n        v = VcfLine(row)\n        v.filter = \"PASS\"\n        v.format = \"GT:GP\"\n        probs = [0] * 3\n        probs[gg.index(v.genotype)] = 1\n        v.genotype = v.genotype.replace(\"/\", \"|\") + \\\n                \":{0}\".format(\",\".join(\"{0:.3f}\".format(x) for x in probs))\n        print(v, file=fw)\n    fw.close()", "response": "%prog passthrough chrY. vcf chrY. new. vcf"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef minimac(args):\n    p = OptionParser(minimac.__doc__)\n    p.set_home(\"shapeit\")\n    p.set_home(\"minimac\")\n    p.set_outfile()\n    p.set_chr()\n    p.set_ref()\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    txtfile, = args\n    ref = opts.ref\n    mm = MakeManager()\n    pf = txtfile.split(\".\")[0]\n    allrawvcf = []\n    alloutvcf = []\n    chrs = opts.chr.split(\",\")\n    for x in chrs:\n        px = CM[x]\n        chrvcf = pf + \".{0}.vcf\".format(px)\n        if txtfile.endswith(\".vcf\"):\n            cmd = \"vcftools --vcf {0} --chr {1}\".format(txtfile, x)\n            cmd += \" --out {0}.{1} --recode\".format(pf, px)\n            cmd += \" && mv {0}.{1}.recode.vcf {2}\".format(pf, px, chrvcf)\n        else:  # 23andme\n            cmd = \"python -m jcvi.formats.vcf from23andme {0} {1}\".format(txtfile, x)\n            cmd += \" --ref {0}\".format(ref)\n        mm.add(txtfile, chrvcf, cmd)\n\n        chrvcf_hg38 = pf + \".{0}.23andme.hg38.vcf\".format(px)\n        minimac_liftover(mm, chrvcf, chrvcf_hg38, opts)\n        allrawvcf.append(chrvcf_hg38)\n\n        minimacvcf = \"{0}.{1}.minimac.dose.vcf\".format(pf, px)\n        if x == \"X\":\n            minimac_X(mm, x, chrvcf, opts)\n        elif x in [\"Y\", \"MT\"]:\n            cmd = \"python -m jcvi.variation.impute passthrough\"\n            cmd += \" {0} {1}\".format(chrvcf, minimacvcf)\n            mm.add(chrvcf, minimacvcf, cmd)\n        else:\n            minimac_autosome(mm, x, chrvcf, opts)\n\n        # keep the best line for multi-allelic markers\n        uniqvcf= \"{0}.{1}.minimac.uniq.vcf\".format(pf, px)\n        cmd = \"python -m jcvi.formats.vcf uniq {0} > {1}\".\\\n                    format(minimacvcf, uniqvcf)\n        mm.add(minimacvcf, uniqvcf, cmd)\n\n        minimacvcf_hg38 = \"{0}.{1}.minimac.hg38.vcf\".format(pf, px)\n        minimac_liftover(mm, uniqvcf, minimacvcf_hg38, opts)\n        alloutvcf.append(minimacvcf_hg38)\n\n    if len(allrawvcf) > 1:\n        rawhg38vcfgz = pf + \".all.23andme.hg38.vcf.gz\"\n        cmd = \"vcf-concat {0} | bgzip > {1}\".format(\" \".join(allrawvcf), rawhg38vcfgz)\n        mm.add(allrawvcf, rawhg38vcfgz, cmd)\n\n    if len(alloutvcf) > 1:\n        outhg38vcfgz = pf + \".all.minimac.hg38.vcf.gz\"\n        cmd = \"vcf-concat {0} | bgzip > {1}\".format(\" \".join(alloutvcf), outhg38vcfgz)\n        mm.add(alloutvcf, outhg38vcfgz, cmd)\n\n    mm.write()", "response": "%prog batchminimac input. txt\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef minimac_X(mm, chr, vcffile, opts):\n    pf = vcffile.rsplit(\".\", 1)[0]\n    ranges = [(1, 2699519), (2699520, 154931043), (154931044, 155270560)]\n    tags = [\"PAR1\", \"NONPAR\", \"PAR2\"]\n    Xvcf = []\n    phasedfiles = []\n    for tag, (start, end) in zip(tags, ranges):\n        recodefile = pf + \"_{0}.recode.vcf\".format(tag)\n        cmd = \"vcftools --vcf {0} --out {1}_{2}\".format(vcffile, pf, tag)\n        cmd += \" --chr X --from-bp {0} --to-bp {1} --recode\".format(start, end)\n        mm.add(vcffile, recodefile, cmd)\n\n        phasedfile = shapeit_phasing(mm, chr + \"_{0}\".format(tag), recodefile, opts)\n        phasedfiles.append(phasedfile)\n\n    pars = [x for x in phasedfiles if \"_PAR\" in x]\n    parfile = pf + \"_PAR.recode.phased.vcf\"\n    nonparfile = pf + \"_NONPAR.recode.phased.vcf\"\n    cmd = \"vcf-concat {0} > {1}\".format(\" \".join(pars), parfile)\n    mm.add(pars, parfile, cmd)\n\n    for phasedfile in (parfile, nonparfile):\n        outvcf = minimac_autosome(mm, chr, phasedfile, opts, phasing=False)\n        Xvcf.append(outvcf)\n\n    minimacvcf = pf + \".minimac.dose.vcf\"\n    cmd = \"vcf-concat {0} | vcf-sort -c > {1}\".format(\" \".join(Xvcf), minimacvcf)\n    mm.add(Xvcf, minimacvcf, cmd)", "response": "Add the X - chromosome recode and phased files to the MM."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef beagle(args):\n    p = OptionParser(beagle.__doc__)\n    p.set_home(\"beagle\")\n    p.set_ref()\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    vcffile, chr = args\n    pf = vcffile.rsplit(\".\", 1)[0]\n    outpf = pf + \".beagle\"\n    outfile = outpf + \".vcf.gz\"\n\n    mm = MakeManager()\n    beagle_cmd = opts.beagle_home\n    kg = op.join(opts.ref, \"1000GP_Phase3\")\n    cmd = beagle_cmd + \" gt={0}\".format(vcffile)\n    cmd += \" ref={0}/chr{1}.1kg.phase3.v5a.bref\".format(kg, chr)\n    cmd += \" map={0}/plink.chr{1}.GRCh37.map\".format(kg, chr)\n    cmd += \" out={0}\".format(outpf)\n    cmd += \" nthreads=16 gprobs=true\"\n    mm.add(vcffile, outfile, cmd)\n\n    mm.write()", "response": "%prog beagle input. vcf 1"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef impute(args):\n    from pyfaidx import Fasta\n\n    p = OptionParser(impute.__doc__)\n    p.set_home(\"shapeit\")\n    p.set_home(\"impute\")\n    p.set_ref()\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    vcffile, fastafile, chr = args\n    mm = MakeManager()\n    pf = vcffile.rsplit(\".\", 1)[0]\n    hapsfile = pf + \".haps\"\n    kg = op.join(opts.ref, \"1000GP_Phase3\")\n    shapeit_phasing(mm, chr, vcffile, opts)\n\n    fasta = Fasta(fastafile)\n    size = len(fasta[chr])\n    binsize = 5000000\n    bins = size / binsize  # 5Mb bins\n    if size % binsize:\n        bins += 1\n    impute_cmd = op.join(opts.impute_home, \"impute2\")\n    chunks = []\n    for x in xrange(bins + 1):\n        chunk_start = x * binsize + 1\n        chunk_end = min(chunk_start + binsize - 1, size)\n        outfile = pf + \".chunk{0:02d}.impute2\".format(x)\n        mapfile = \"{0}/genetic_map_chr{1}_combined_b37.txt\".format(kg, chr)\n        rpf = \"{0}/1000GP_Phase3_chr{1}\".format(kg, chr)\n        cmd = impute_cmd + \" -m {0}\".format(mapfile)\n        cmd += \" -known_haps_g {0}\".format(hapsfile)\n        cmd += \" -h {0}.hap.gz -l {0}.legend.gz\".format(rpf)\n        cmd += \" -Ne 20000 -int {0} {1}\".format(chunk_start, chunk_end)\n        cmd += \" -o {0} -allow_large_regions -seed 367946\".format(outfile)\n        cmd += \" && touch {0}\".format(outfile)\n        mm.add(hapsfile, outfile, cmd)\n        chunks.append(outfile)\n\n    # Combine all the files\n    imputefile = pf + \".impute2\"\n    cmd = \"cat {0} > {1}\".format(\" \".join(chunks), imputefile)\n    mm.add(chunks, imputefile, cmd)\n\n    # Convert to vcf\n    vcffile = pf + \".impute2.vcf\"\n    cmd = \"python -m jcvi.formats.vcf fromimpute2 {0} {1} {2} > {3}\".\\\n                format(imputefile, fastafile, chr, vcffile)\n    mm.add(imputefile, vcffile, cmd)\n    mm.write()", "response": "%prog impute input. vcf hs37d5. fasta chr1"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef genestats(args):\n    p = OptionParser(genestats.__doc__)\n    p.add_option(\"--groupby\", default=\"conf_class\",\n                 help=\"Print separate stats groupby\")\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    gff_file, = args\n    gb = opts.groupby\n    g = make_index(gff_file)\n\n    tf = \"transcript.sizes\"\n    if need_update(gff_file, tf):\n        fw = open(tf, \"w\")\n        for feat in g.features_of_type(\"mRNA\"):\n            fid = feat.id\n            conf_class = feat.attributes.get(gb, \"all\")\n            tsize = sum((c.stop - c.start + 1) for c in g.children(fid, 1) \\\n                             if c.featuretype == \"exon\")\n            print(\"\\t\".join((fid, str(tsize), conf_class)), file=fw)\n        fw.close()\n\n    tsizes = DictFile(tf, cast=int)\n    conf_classes = DictFile(tf, valuepos=2)\n    logging.debug(\"A total of {0} transcripts populated.\".format(len(tsizes)))\n\n    genes = []\n    for feat in g.features_of_type(\"gene\"):\n        fid = feat.id\n        transcripts = [c.id for c in g.children(fid, 1) \\\n                         if c.featuretype == \"mRNA\"]\n        transcript_sizes = [tsizes[x] for x in transcripts]\n        exons = set((c.chrom, c.start, c.stop) for c in g.children(fid, 2) \\\n                         if c.featuretype == \"exon\")\n        conf_class = conf_classes[transcripts[0]]\n        gs = GeneStats(feat, conf_class, transcript_sizes, exons)\n        genes.append(gs)\n\n    r = {}  # Report\n    distinct_groups = set(conf_classes.values())\n    for g in distinct_groups:\n        num_genes = num_single_exon_genes = num_multi_exon_genes = 0\n        num_genes_with_alts = num_transcripts = num_exons = max_transcripts = 0\n        cum_locus_size = cum_transcript_size = cum_exon_size = 0\n        for gs in genes:\n            if gs.conf_class != g:\n                continue\n            num_genes += 1\n            if gs.num_exons == 1:\n                num_single_exon_genes += 1\n            else:\n                num_multi_exon_genes += 1\n            num_exons += gs.num_exons\n            if gs.num_transcripts > 1:\n                num_genes_with_alts += 1\n            if gs.num_transcripts > max_transcripts:\n                max_transcripts = gs.num_transcripts\n            num_transcripts += gs.num_transcripts\n            cum_locus_size += gs.locus_size\n            cum_transcript_size += gs.cum_transcript_size\n            cum_exon_size += gs.cum_exon_size\n\n        mean_num_exons = num_exons * 1. / num_genes\n        mean_num_transcripts = num_transcripts * 1. / num_genes\n        mean_locus_size = cum_locus_size * 1. / num_genes\n        mean_transcript_size = cum_transcript_size * 1. / num_transcripts\n        mean_exon_size = cum_exon_size * 1. / num_exons\n\n        r[(\"Number of genes\", g)] = num_genes\n        r[(\"Number of single-exon genes\", g)] = \\\n            percentage(num_single_exon_genes, num_genes, mode=1)\n        r[(\"Number of multi-exon genes\", g)] = \\\n            percentage(num_multi_exon_genes, num_genes, mode=1)\n        r[(\"Number of distinct exons\", g)] = num_exons\n        r[(\"Number of genes with alternative transcript variants\", g)] = \\\n            percentage(num_genes_with_alts, num_genes, mode=1)\n        r[(\"Number of predicted transcripts\", g)] = num_transcripts\n        r[(\"Mean number of distinct exons per gene\", g)] = mean_num_exons\n        r[(\"Mean number of transcripts per gene\", g)] = mean_num_transcripts\n        r[(\"Max number of transcripts per gene\", g)] = max_transcripts\n        r[(\"Mean gene locus size (first to last exon)\", g)] = mean_locus_size\n        r[(\"Mean transcript size (UTR, CDS)\", g)] = mean_transcript_size\n        r[(\"Mean exon size\", g)] = mean_exon_size\n\n    fw = must_open(opts.outfile, \"w\")\n    print(tabulate(r), file=fw)\n    fw.close()", "response": "%prog genestats gffile\n\n    Print summary stats, including:\n    - Number of genes\n    - Number of single-exon genes\n    - Number of multi-exon genes\n    - Number of distinct exons\n    - Number of genes with alternative transcript variants\n    - Number of predicted transcripts\n    - Mean number of distinct exons per gene\n    - Mean number of transcripts per gene\n    - Mean gene locus size (first to last exon)\n    - Mean transcript size (UTR, CDS)\n    - Mean exon size\n\n    Stats modeled after barley genome paper Table 1.\n    A physical, genetic and functional sequence assembly of the barley genome"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef histogram(args):\n    from jcvi.graphics.histogram import histogram_multiple\n\n    p = OptionParser(histogram.__doc__)\n    p.add_option(\"--bins\", dest=\"bins\", default=40, type=\"int\",\n            help=\"number of bins to plot in the histogram [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    gff_files = args\n    # metrics = (\"Exon_Length\", \"Intron_Length\", \"Gene_Length\", \"Exon_Count\")\n    colors = (\"red\", \"green\", \"blue\", \"black\")\n    vmaxes = (1000, 1000, 4000, 20)\n    xlabels = (\"bp\", \"bp\", \"bp\", \"number\")\n    for metric, color, vmax, xlabel in zip(metrics, colors, vmaxes, xlabels):\n        logging.debug(\"Parsing files in `{0}`..\".format(metric))\n        numberfiles = [op.join(metric, op.basename(x).split(\".\")[0] + \".txt\") \\\n                        for x in gff_files]\n\n        histogram_multiple(numberfiles, 0, vmax, xlabel, metric,\n                       bins=opts.bins, facet=True, fill=color,\n                       prefix=metric + \".\")", "response": "%prog histogram *.gff\n\n    Plot gene statistics based on output of stats. For each gff file, look to\n    see if the metrics folder (i.e. Exon_Length) contains the data and plot\n    them."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stats(args):\n    p = OptionParser(stats.__doc__)\n    p.add_option(\"--gene\", default=\"mRNA\",\n                 help=\"The gene type [default: %default]\")\n    p.add_option(\"--exon\", default=\"CDS\",\n                 help=\"The exon type [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    gff_file, = args\n    g = make_index(gff_file)\n    exon_lengths = []\n    intron_lengths = []\n    gene_lengths = []\n    exon_counts = []\n    for feat in g.features_of_type(opts.gene):\n        exons = []\n        for c in g.children(feat.id, 1):\n            if c.featuretype != opts.exon:\n                continue\n            exons.append((c.chrom, c.start, c.stop))\n        introns = range_interleave(exons)\n        feat_exon_lengths = [(stop - start + 1) for (chrom, start, stop) in exons]\n        feat_intron_lengths = [(stop - start + 1) for (chrom, start, stop) in introns]\n        exon_lengths += feat_exon_lengths\n        intron_lengths += feat_intron_lengths\n        gene_lengths.append(sum(feat_exon_lengths))\n        exon_counts.append(len(feat_exon_lengths))\n\n    a = SummaryStats(exon_lengths)\n    b = SummaryStats(intron_lengths)\n    c = SummaryStats(gene_lengths)\n    d = SummaryStats(exon_counts)\n    for x, title in zip((a, b, c, d), metrics):\n        x.title = title\n        print(x, file=sys.stderr)\n\n    prefix = gff_file.split(\".\")[0]\n    for x in (a, b, c, d):\n        dirname = x.title\n        mkdir(dirname)\n        txtfile = op.join(dirname, prefix + \".txt\")\n        x.tofile(txtfile)", "response": "%prog stats infile.gff\n\n    Collect gene statistics based on gff file. There are some terminology issues\n    here and so normally we call \"gene\" are actually mRNA, and sometimes \"exon\"\n    are actually CDS, but they are configurable.\n\n    Thee numbers are written to text file in four separate folders,\n    corresponding to the four metrics:\n\n    Exon length, Intron length, Gene length, Exon count\n\n    With data written to disk then you can run %prog histogram"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wheel(args):\n    p = OptionParser(wheel.__doc__)\n    p.add_option(\"--column\", default=\"score\", choices=(\"score\", \"percentile\"),\n                    help=\"Which column to extract from `datafile.csv`\")\n    opts, args, iopts = p.set_image_options(args, figsize=\"5x5\", format=\"png\")\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    datafile, groupsfile = args\n    column = opts.column\n    linecolor = \"#d6d6d6\"\n    df = parse_data(datafile, score_column=opts.column)\n    groups = parse_groups(groupsfile)\n    labels = [g for g in groups if g in df]\n    print(labels)\n    df = [df[g] for g in labels]\n    print(df)\n    groups = [groups[g] for g in labels]\n    print(groups)\n\n    pf = datafile.rsplit(\".\", 1)[0]\n    fig = plt.figure(1, (iopts.w, iopts.h))\n    root = fig.add_axes([0, 0, 1, 1])\n    categories = len(df)\n    #ax = plt.subplot(111, projection='polar')\n    ax = fig.add_axes([0.1, 0.1, 0.8, 0.8], polar=True)\n\n    brewer = [ \"#FF3B30\", \"#DD43A0\", \"#5856D6\",\n\t       \"#007AFE\", \"#56BDEC\", \"#4CD8BA\",\n\t       \"#4CD864\", \"#B0F457\", \"#FEF221\",\n\t       \"#FFCC01\", \"#FF9500\", \"#FF3B30\",\n    ]\n\n    # Baseline\n    theta = np.linspace(1.5 * np.pi, 3.5 * np.pi, endpoint=False, num=categories)\n    _theta = np.linspace(1.5 * np.pi, 3.5 * np.pi)\n    R = max(max(df), 10)\n    xlim = (-R, R) if column == \"score\" else (-100, 100)\n    plim = (-R / 2, R) if column == \"score\" else (0, 100)\n    ci = (-.5, 2) if column == \"score\" else (10, 90)\n\n    # Grid\n    if column == \"score\":\n        for t in theta:\n            ax.plot([t, t], plim, color=linecolor)\n    ax.axis('off')\n\n    # Contours\n    for t in plim:\n        ax.plot(_theta, [t] * len(_theta), color=linecolor)\n\n    # Sectors (groupings)\n    collapsed_groups = []\n    gg = []\n    for group, c in groupby(enumerate(groups), lambda x: x[1]):\n        c = [x[0] for x in list(c)]\n        collapsed_groups.append(group)\n        gg.append(c)\n\n    sector = False\n    if sector:\n        theta_interval = 2 * np.pi / categories\n        theta_pad = theta_interval / 2 * .9\n        for color, group in zip(brewer, gg):\n            tmin, tmax = min(group), max(group)\n            sector(ax, theta[tmin], theta[tmax], theta_pad, R * .95,\n                       \"-\", color=color, lw=2)\n\n    # Data\n    r = df\n    closed_plot(ax, theta, r, color=\"lightslategray\", alpha=.25)\n    all_data = []\n    for color, group in zip(brewer, gg):\n        hidden_data = [(theta[x], r[x]) for x in group if \\\n                            (ci[0] <= r[x] <= ci[1])]\n        shown_data = [(theta[x], r[x]) for x in group if (r[x] < ci[0] \\\n                            or r[x] > ci[1])]\n        all_data.append((theta[x], labels[x], r[x]))\n        for alpha, data in zip((1, 1), (hidden_data, shown_data)):\n            if not data:\n                continue\n            color_theta, color_r = zip(*data)\n            ax.plot(color_theta, color_r, \"o\", color=color, alpha=alpha)\n\n    # Print out data\n    diseaseNames, risks = labels, df\n    print(\"var theta = [{}]\".format(\",\".join(\"{:.1f}\".format(degrees(x)) for x in theta)))\n    print(\"var risks = [{}]\".format(\",\".join(str(x) for x in risks)))\n    print(\"var diseaseNames = [{}]\".format(\",\".join(\\\n                    ['\"{}\"'.format(x) for x in diseaseNames])))\n\n    # Labels\n    from math import cos, sin\n    r = .5\n    for i, label in enumerate(labels):\n        tl = theta[i]\n        x, y = .5 + r * cos(tl), .5 + r * sin(tl)\n        d = degrees(tl)\n        if 90 < d % 360 < 270:  # On the left quardrants\n            d -= 180\n        root.text(x, y, label, size=4, rotation=d,\n                  ha=\"center\", va=\"center\", color=linecolor)\n        print(x, y, label)\n\n    # Add baseline\n    baseline = 0 if column == \"score\" else 50\n    _r = len(_theta) * [baseline]\n    closed_plot(ax, _theta, _r, \"k:\", lw=1, ms=4)\n\n    # Add confidence interval\n    if column == \"percentile\":\n        barcolor = \"#eeeeee\"\n        ax.bar([0], [ci[1] - ci[0]], width=2 * np.pi, bottom=ci[0], fc=barcolor)\n\n    ax.set_rmin(xlim[0])\n    ax.set_rmax(xlim[1])\n\n    normalize_axes(root)\n\n    image_name = pf + \"-\" + column + \".\" + iopts.format\n    savefig(image_name, dpi=iopts.dpi, iopts=iopts)", "response": "%prog wheel datafile. csv groups. csv"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rdotplot(args):\n    p = OptionParser(rdotplot.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    dotplot_template = \"\"\"\n    dots <- read.table('$rdotplotfile', header=T)\n    png('$pngfile')\n    plot(dots, type='l')\n    dev.off()\n    \"\"\"\n\n    rdotplotfile, = args\n    assert rdotplotfile.endswith(\".rdotplot\")\n    pngfile = rdotplotfile.replace(\".rdotplot\", \".png\")\n\n    rtemplate = RTemplate(dotplot_template, locals())\n    rtemplate.run()", "response": "%prog rdotplotfile\n\n    Dot plot to visualize relationship between two sequences, by plotting\n    .rdotplot file (often generated by LASTZ)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self, clean=True):\n        template = self.template\n        parameters = self.parameters\n        # write to a temporary R script\n        fw = must_open(\"tmp\", \"w\")\n        path = fw.name\n\n        fw.write(template.safe_substitute(**parameters))\n        fw.close()\n\n        sh(\"Rscript %s\" % path)\n        if clean:\n            os.remove(path)\n            # I have no idea why using ggsave, there is one extra image\n            # generated, but here I remove it\n            rplotspdf = \"Rplots.pdf\"\n            if op.exists(rplotspdf):\n                os.remove(rplotspdf)", "response": "Create a temporary file and run it\n           "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef graph_simplify(G):\n    spurs = []\n    path_nodes = []\n    for k, d in G.degree().iteritems():\n        if d == 1:\n            spurs.append(k)\n        elif d == 2:\n            path_nodes.append(k)\n\n    logging.debug(\"Remove {0} spurs.\".format(len(spurs)))\n    G.remove_nodes_from(spurs)\n\n    SG = G.subgraph(path_nodes)\n    cc = nx.connected_components(SG)\n    for c in cc:\n        if len(c) == 1:\n            continue\n        c = set(c)\n        neighbors = set()\n        for x in c:\n            neighbors |= set(G.neighbors(x))\n        neighbors -= c\n        newtag = list(c)[0] + \"*\"\n        for n in neighbors:\n            G.add_edge(newtag, n)\n        G.remove_nodes_from(c)\n    logging.debug(\"Contract {0} path nodes into {1} nodes.\".\\\n                    format(len(path_nodes), len(cc)))", "response": "Simplify big graphs remove spurs and contract unique paths."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_paths(paths, weights=None):\n    npaths = len(paths)\n    weights = weights or [1] * npaths\n    assert len(paths) == len(weights)\n\n    G = nx.DiGraph()\n    for path, w in zip(paths, weights):\n        for a, b in pairwise(path):\n            update_weight(G, a, b, w)\n    return G", "response": "Zip together paths. Called by merge_paths."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reduce_paths(G):\n    from jcvi.algorithms.lpsolve import min_feedback_arc_set\n\n    while not nx.is_directed_acyclic_graph(G):\n        edges = []\n        for a, b, w in G.edges_iter(data=True):\n            w = w['weight']\n            edges.append((a, b, w))\n        mf, mf_score = min_feedback_arc_set(edges)\n        for a, b, w in mf:\n            G.remove_edge(a, b)\n\n    assert nx.is_directed_acyclic_graph(G)\n    G = transitive_reduction(G)\n    return G", "response": "Reduces the paths in a directed acyclic graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transitive_reduction(G):\n    H = G.copy()\n    for a, b, w in G.edges_iter(data=True):\n        # Try deleting the edge, see if we still have a path\n        # between the vertices\n        H.remove_edge(a, b)\n        if not nx.has_path(H, a, b):  # we shouldn't have deleted it\n            H.add_edge(a, b, w)\n    return H", "response": "Returns a transitive reduction of a graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmerges paths into a single list.", "response": "def merge_paths(paths, weights=None):\n    \"\"\"\n    Zip together sorted lists.\n\n    >>> paths = [[1, 2, 3], [1, 3, 4], [2, 4, 5]]\n    >>> G = merge_paths(paths)\n    >>> nx.topological_sort(G)\n    [1, 2, 3, 4, 5]\n    >>> paths = [[1, 2, 3, 4], [1, 2, 3, 2, 4]]\n    >>> G = merge_paths(paths, weights=(1, 2))\n    >>> nx.topological_sort(G)\n    [1, 2, 3, 4]\n    \"\"\"\n    G = make_paths(paths, weights=weights)\n    G = reduce_paths(G)\n    return G"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef longest_path_weighted_nodes(G, source, target, weights=None):\n    assert nx.is_directed_acyclic_graph(G)\n\n    tree = nx.topological_sort(G)\n    node_to_index = dict((t, i) for i, t in enumerate(tree))\n\n    nnodes = len(tree)\n    weights = [weights.get(x, 1) for x in tree] if weights else [1] * nnodes\n    score, fromc = weights[:], [-1] * nnodes\n    si = node_to_index[source]\n    ti = node_to_index[target]\n    for a in tree[si: ti]:\n        ai = node_to_index[a]\n        for b, w in G[a].items():\n            bi = node_to_index[b]\n            w = w.get('weight', 1)\n            d = score[ai] + weights[bi] * w  # Favor heavier edges\n            if d <= score[bi]:\n                continue\n            score[bi] = d  # Update longest distance so far\n            fromc[bi] = ai\n\n    # Backtracking\n    path = []\n    while ti != -1:\n        path.append(ti)\n        ti = fromc[ti]\n\n    path = [tree[x] for x in path[::-1]]\n    return path, score[ti]", "response": "This function finds the longest path of a given source and target node in a given graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_next(self, tag=\"<\"):\n        next, ntag = None, None\n\n        L = self.outs if tag == \"<\" else self.ins\n\n        if len(L) == 1:\n            e, = L\n            if e.v1.v == self.v:\n                next, ntag = e.v2, e.o2\n                ntag = \"<\" if ntag == \">\" else \">\"  # Flip tag if on other end\n            else:\n                next, ntag = e.v1, e.o1\n\n        if next:  # Validate the next vertex\n            B = next.ins if ntag == \"<\" else next.outs\n            if len(B) > 1:\n                return None, None\n\n        return next, ntag", "response": "This function returns the next vertex in the list of untagged edges."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate Alternative BED from FASTA file. Used for sanity check.", "response": "def fasta2bed(fastafile):\n    \"\"\"\n    Alternative BED generation from FASTA file. Used for sanity check.\n    \"\"\"\n    dustfasta = fastafile.rsplit(\".\", 1)[0] + \".dust.fasta\"\n    for name, seq in parse_fasta(dustfasta):\n        for islower, ss in groupby(enumerate(seq), key=lambda x: x[-1].islower()):\n            if not islower:\n                continue\n            ss = list(ss)\n            ms, mn = min(ss)\n            xs, xn = max(ss)\n            print(\"\\t\".join(str(x) for x in (name, ms, xs)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dedup(args):\n    from jcvi.formats.blast import BlastLine\n\n    p = OptionParser(dedup.__doc__)\n    p.set_align(pctid=0, pctcov=98)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    blastfile, fastafile = args\n    cov = opts.pctcov / 100.\n    sizes = Sizes(fastafile).mapping\n    fp = open(blastfile)\n    removed = set()\n    for row in fp:\n        b = BlastLine(row)\n        query, subject = b.query, b.subject\n        if query == subject:\n            continue\n        qsize, ssize = sizes[query], sizes[subject]\n        qspan = abs(b.qstop - b.qstart)\n        if qspan < qsize * cov:\n            continue\n        if (qsize, query) < (ssize, subject):\n            removed.add(query)\n\n    print(\"\\n\".join(sorted(removed)))", "response": "%prog dedup assembly. blast assembly. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build(args):\n    from jcvi.apps.cdhit import deduplicate\n    from jcvi.apps.vecscreen import mask\n    from jcvi.formats.fasta import sort\n\n    p = OptionParser(build.__doc__)\n    p.add_option(\"--nodedup\", default=False, action=\"store_true\",\n                 help=\"Do not deduplicate [default: deduplicate]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    fastafile, bacteria, pf = args\n    dd = deduplicate([fastafile, \"--pctid=100\"]) \\\n                if not opts.nodedup else fastafile\n    screenfasta = screen([dd, bacteria])\n    tidyfasta = mask([screenfasta])\n    sortedfasta = sort([tidyfasta, \"--sizes\"])\n    scaffoldfasta = pf + \".assembly.fasta\"\n    format([sortedfasta, scaffoldfasta, \"--prefix=scaffold_\", \"--sequential\"])\n    gapsplitfasta = pf + \".gapSplit.fasta\"\n    cmd = \"gapSplit -minGap=10 {0} {1}\".format(scaffoldfasta, gapsplitfasta)\n    sh(cmd)\n    contigsfasta = pf + \".contigs.fasta\"\n    format([gapsplitfasta, contigsfasta, \"--prefix=contig_\", \"--sequential\"])", "response": "%prog build current. fasta Bacteria_Virus. fasta"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef scaffold(args):\n    from jcvi.formats.agp import bed, order_to_agp, build\n    from jcvi.formats.bed import Bed\n\n    p = OptionParser(scaffold.__doc__)\n    p.add_option(\"--prefix\", default=False, action=\"store_true\",\n            help=\"Keep IDs with same prefix together [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    ctgfasta, agpfile = args\n    sizes = Sizes(ctgfasta).mapping\n\n    pf = ctgfasta.rsplit(\".\", 1)[0]\n    phasefile = pf + \".phases\"\n    fwphase = open(phasefile, \"w\")\n    newagpfile = pf + \".new.agp\"\n    fwagp = open(newagpfile, \"w\")\n\n    scaffoldbuckets = defaultdict(list)\n\n    bedfile = bed([agpfile, \"--nogaps\", \"--outfile=tmp\"])\n    bb = Bed(bedfile)\n    for s, partialorder in bb.sub_beds():\n        name = partialorder[0].accn\n        bname = name.rsplit(\"_\", 1)[0] if opts.prefix else s\n        scaffoldbuckets[bname].append([(b.accn, b.strand) for b in partialorder])\n\n    # Now the buckets contain a mixture of singletons and partially resolved\n    # scaffolds. Print the scaffolds first then remaining singletons.\n    for bname, scaffolds in sorted(scaffoldbuckets.items()):\n        ctgorder = []\n        singletons = set()\n        for scaf in sorted(scaffolds):\n            for node, orientation in scaf:\n                ctgorder.append((node, orientation))\n            if len(scaf) == 1:\n                singletons.add(node)\n        nscaffolds = len(scaffolds)\n        nsingletons = len(singletons)\n        if nsingletons == 1 and nscaffolds == 0:\n            phase = 3\n        elif nsingletons == 0 and nscaffolds == 1:\n            phase = 2\n        else:\n            phase = 1\n\n        msg = \"{0}: Scaffolds={1} Singletons={2} Phase={3}\".\\\n            format(bname, nscaffolds, nsingletons, phase)\n        print(msg, file=sys.stderr)\n        print(\"\\t\".join((bname, str(phase))), file=fwphase)\n\n        order_to_agp(bname, ctgorder, sizes, fwagp)\n\n    fwagp.close()\n    os.remove(bedfile)\n\n    fastafile = \"final.fasta\"\n    build([newagpfile, ctgfasta, fastafile])\n    tidy([fastafile])", "response": "%prog scaffold ctgfasta agpfile"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef kill(args):\n    import shlex\n    from jcvi.apps.base import sh, getusername\n    from subprocess import check_output, CalledProcessError\n    import xml.etree.ElementTree as ET\n\n    valid_methods = (\"pattern\", \"jobid\")\n    p = OptionParser(kill.__doc__)\n    p.add_option(\"--method\", choices=valid_methods,\n                 help=\"Identify jobs based on [default: guess]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    username = getusername()\n    tag, = args\n    tag = tag.strip()\n\n    if tag == \"all\":\n        sh(\"qdel -u {0}\".format(username))\n        return\n\n    valid_jobids = set()\n    method = opts.method or guess_method(tag)\n    if method == \"jobid\":\n        jobids = tag.split(\",\")\n        valid_jobids |= set(jobids)\n    elif method == \"pattern\":\n        qsxmlcmd = 'qstat -u \"{0}\" -j \"{1}\" -nenv -njd -xml'.\\\n                                format(username, tag)\n        try:\n            qsxml = check_output(shlex.split(qsxmlcmd)).strip()\n        except CalledProcessError as e:\n            qsxml = None\n            logging.debug('No jobs matching the pattern \"{0}\"'.format(tag))\n\n        if qsxml is not None:\n            for job in ET.fromstring(qsxml).findall(\"djob_info\"):\n                for elem in job.findall(\"element\"):\n                    jobid = elem.find(\"JB_job_number\").text\n                    valid_jobids.add(jobid)\n\n    if valid_jobids:\n        sh(\"qdel {0}\".format(\",\".join(valid_jobids)))", "response": "%prog kill JOBNAMEPAT or JOBIDs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef qc(args):\n    from jcvi.graphics.glyph import Bezier\n\n    p = OptionParser(qc.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(p.print_help())\n\n    prefix, = args\n    scf = prefix\n\n    # All these files *must* be present in the current folder\n    bedpefile = prefix + \".bedpe\"\n    fastafile = prefix + \".fasta\"\n    sizesfile = prefix + \".sizes\"\n    gapsbedfile = prefix + \".gaps.bed\"\n    bedfile = prefix + \".bed\"\n    bedpefile = prefix + \".bedpe\"\n    pairsbedfile = prefix + \".pairs.bed\"\n\n    sizes = Sizes(fastafile).mapping\n    size = sizes[scf]\n\n    fig = plt.figure(1, (8, 5))\n    root = fig.add_axes([0, 0, 1, 1])\n\n    # the scaffold\n    root.add_patch(Rectangle((.1, .15), .8, .03, fc='k'))\n\n    # basecoverage and matecoverage\n    ax = fig.add_axes([.1, .45, .8, .45])\n\n    bins = 200  # Smooth the curve\n    basecoverage = Coverage(bedfile, sizesfile)\n    matecoverage = Coverage(pairsbedfile, sizesfile)\n\n    x, y = basecoverage.get_plot_data(scf, bins=bins)\n    baseline, = ax.plot(x, y, 'g-')\n    x, y = matecoverage.get_plot_data(scf, bins=bins)\n    mateline, = ax.plot(x, y, 'r-')\n    legends = (\"Base coverage\", \"Mate coverage\")\n    leg = ax.legend((baseline, mateline), legends, shadow=True, fancybox=True)\n    leg.get_frame().set_alpha(.5)\n    ax.set_xlim(0, size)\n\n    # draw the read pairs\n    fp = open(bedpefile)\n    pairs = []\n    for row in fp:\n        scf, astart, aend, scf, bstart, bend, clonename = row.split()\n        astart, bstart = int(astart), int(bstart)\n        aend, bend = int(aend), int(bend)\n        start = min(astart, bstart) + 1\n        end = max(aend, bend)\n        pairs.append((start, end))\n\n    bpratio = .8 / size\n    cutoff = 1000  # inserts smaller than this are not plotted\n    # this convert from base => x-coordinate\n    pos = lambda x: (.1 + x * bpratio)\n    ypos = .15 + .03\n    for start, end in pairs:\n        dist = end - start\n\n        if dist < cutoff:\n            continue\n\n        dist = min(dist, 10000)\n        # 10Kb == .25 canvas height\n        height = .25 * dist / 10000\n        xstart = pos(start)\n        xend = pos(end)\n        p0 = (xstart, ypos)\n        p1 = (xstart, ypos + height)\n        p2 = (xend, ypos + height)\n        p3 = (xend, ypos)\n        Bezier(root, p0, p1, p2, p3)\n\n    # gaps on the scaffold\n    fp = open(gapsbedfile)\n    for row in fp:\n        b = BedLine(row)\n        start, end = b.start, b.end\n        xstart = pos(start)\n        xend = pos(end)\n        root.add_patch(Rectangle((xstart, .15), xend - xstart, .03, fc='w'))\n\n    root.text(.5, .1, scf, color='b', ha=\"center\")\n    warn_msg = \"Only the inserts > {0}bp are shown\".format(cutoff)\n    root.text(.5, .1, scf, color='b', ha=\"center\")\n    root.text(.5, .05, warn_msg, color='gray', ha=\"center\")\n    # clean up and output\n    set_human_base_axis(ax)\n    root.set_xlim(0, 1)\n    root.set_ylim(0, 1)\n    root.set_axis_off()\n\n    figname = prefix + \".pdf\"\n    savefig(figname, dpi=300)", "response": "%prog qc prefix\n\n    Expects data files including:\n    1. `prefix.bedpe` draws Bezier curve between paired reads\n    2. `prefix.sizes` draws length of the contig/scaffold\n    3. `prefix.gaps.bed` mark the position of the gaps in sequence\n    4. `prefix.bed.coverage` plots the base coverage\n    5. `prefix.pairs.bed.coverage` plots the clone coverage\n\n    See assembly.coverage.posmap() for the generation of these files."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef A50(args):\n    p = OptionParser(A50.__doc__)\n    p.add_option(\"--overwrite\", default=False, action=\"store_true\",\n            help=\"overwrite .rplot file if exists [default: %default]\")\n    p.add_option(\"--cutoff\", default=0, type=\"int\", dest=\"cutoff\",\n            help=\"use contigs above certain size [default: %default]\")\n    p.add_option(\"--stepsize\", default=10, type=\"int\", dest=\"stepsize\",\n            help=\"stepsize for the distribution [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if not args:\n        sys.exit(p.print_help())\n\n    import numpy as np\n    from jcvi.utils.table import loadtable\n\n    stepsize = opts.stepsize  # use stepsize to speed up drawing\n    rplot = \"A50.rplot\"\n    if not op.exists(rplot) or opts.overwrite:\n        fw = open(rplot, \"w\")\n        header = \"\\t\".join((\"index\", \"cumsize\", \"fasta\"))\n        statsheader = (\"Fasta\", \"L50\", \"N50\", \"Min\", \"Max\", \"Average\", \"Sum\",\n                \"Counts\")\n        statsrows = []\n        print(header, file=fw)\n        for fastafile in args:\n            f = Fasta(fastafile, index=False)\n            ctgsizes = [length for k, length in f.itersizes()]\n            ctgsizes = np.array(ctgsizes)\n\n            a50, l50, n50 = calculate_A50(ctgsizes, cutoff=opts.cutoff)\n            cmin, cmax, cmean = min(ctgsizes), max(ctgsizes), np.mean(ctgsizes)\n            csum, counts = np.sum(ctgsizes), len(ctgsizes)\n            cmean = int(round(cmean))\n            statsrows.append((fastafile, l50, n50, cmin, cmax, cmean, csum,\n                counts))\n\n            logging.debug(\"`{0}` ctgsizes: {1}\".format(fastafile, ctgsizes))\n\n            tag = \"{0} (L50={1})\".format(\\\n                    op.basename(fastafile).rsplit(\".\", 1)[0], l50)\n            logging.debug(tag)\n\n            for i, s in zip(xrange(0, len(a50), stepsize), a50[::stepsize]):\n                print(\"\\t\".join((str(i), str(s / 1000000.), tag)), file=fw)\n        fw.close()\n\n        table = loadtable(statsheader, statsrows)\n        print(table, file=sys.stderr)\n\n    generate_plot(rplot)", "response": "%prog A50 contigs_A. fasta contigs_B. fasta..."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef merge(args):\n    p = OptionParser(merge.__doc__)\n    p.set_outfile(outfile=\"merged_results.delta\")\n    opts, args = p.parse_args(args)\n\n    if len(args) < 3:\n        sys.exit(not p.print_help())\n\n    ref, query = args[:2]\n    deltafiles = args[2:]\n    outfile = opts.outfile\n\n    ref = get_abs_path(ref)\n    query = get_abs_path(query)\n    fw = must_open(outfile, \"w\")\n    print(\" \".join((ref, query)), file=fw)\n    print(\"NUCMER\", file=fw)\n    fw.close()\n\n    for d in deltafiles:\n        cmd = \"awk 'NR > 2 {{print $0}}' {0}\".format(d)\n        sh(cmd, outfile=outfile, append=True)", "response": "%prog merge ref. fasta query. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef blast(args):\n    p = OptionParser(blast.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    deltafile, = args\n    blastfile = deltafile.rsplit(\".\", 1)[0] + \".blast\"\n\n    if need_update(deltafile, blastfile):\n        coords = Coords(deltafile)\n        fw = open(blastfile, \"w\")\n        for c in coords:\n            print(c.blastline, file=fw)", "response": "%prog blast deltafile | coordsfile > blast"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sort(args):\n    import jcvi.formats.blast\n\n    return jcvi.formats.blast.sort(args + [\"--coords\"])", "response": "%prog sort coordsfile\n    Sort coordsfile based on query or ref."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef summary(args):\n    from jcvi.formats.blast import AlignStats\n\n    p = OptionParser(summary.__doc__)\n    p.add_option(\"-s\", dest=\"single\", default=False, action=\"store_true\",\n            help=\"provide stats per reference seq\")\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(p.print_help())\n\n    coordsfile, = args\n    alignstats = get_stats(coordsfile)\n    alignstats.print_stats()", "response": "%prog summary coordsfile\n\n    provide summary on id% and cov%, for both query and reference"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bed(args):\n    p = OptionParser(bed.__doc__)\n    p.add_option(\"--query\", default=False, action=\"store_true\",\n            help=\"print out query intervals rather than ref [default: %default]\")\n    p.add_option(\"--pctid\", default=False, action=\"store_true\",\n            help=\"use pctid in score [default: %default]\")\n    p.add_option(\"--cutoff\", dest=\"cutoff\", default=0, type=\"float\",\n            help=\"get all the alignments with quality above threshold \" +\\\n                 \"[default: %default]\")\n\n    opts, args = p.parse_args(args)\n    if len(args) != 1:\n        sys.exit(p.print_help())\n\n    coordsfile, = args\n    query = opts.query\n    pctid = opts.pctid\n    quality_cutoff = opts.cutoff\n\n    coords = Coords(coordsfile)\n\n    for c in coords:\n        if c.quality < quality_cutoff:\n            continue\n        line = c.qbedline(pctid=pctid) if query else c.bedline(pctid=pctid)\n        print(line)", "response": "%prog bed coordsfile\n\n    will produce a bed list of mapped position and orientation (needs to\n    be beyond quality cutoff, say 50) in bed format"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine the type of overlap given query ref alignment coordinates", "response": "def overlap(self, max_hang=100):\n        \"\"\"\n        Determine the type of overlap given query, ref alignment coordinates\n        Consider the following alignment between sequence a and b:\n\n        aLhang \\              / aRhang\n                \\------------/\n                /------------\\\n        bLhang /              \\ bRhang\n\n        Terminal overlap: a before b, b before a\n        Contain overlap: a in b, b in a\n        \"\"\"\n        aL, aR = 1, self.reflen\n        bL, bR = 1, self.querylen\n        aLhang, aRhang = self.start1 - aL, aR - self.end1\n        bLhang, bRhang = self.start2 - bL, bR - self.end2\n        if self.orientation == '-':\n            bLhang, bRhang = bRhang, bLhang\n\n        s1 = aLhang + bRhang\n        s2 = aRhang + bLhang\n        s3 = aLhang + aRhang\n        s4 = bLhang + bRhang\n\n        # Dovetail (terminal) overlap\n        if s1 < max_hang:\n            type = 2  # b ~ a\n        elif s2 < max_hang:\n            type = 1  # a ~ b\n        # Containment overlap\n        elif s3 < max_hang:\n            type = 3  # a in b\n        elif s4 < max_hang:\n            type = 4  # b in a\n        else:\n            type = 0\n\n        return type"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hits(self):\n        self.quality_sort()\n\n        hits = dict((query, list(blines)) for (query, blines) in \\\n                        groupby(self, lambda x: x.query))\n\n        self.ref_sort()\n\n        return hits", "response": "returns a dict with query => blastline\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef best_hits(self):\n        self.quality_sort()\n\n        best_hits = dict((query, next(blines)) for (query, blines) in \\\n                        groupby(self, lambda x: x.query))\n\n        self.ref_sort()\n\n        return best_hits", "response": "returns a dict with query => best mapped position"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread a specific section from a ConfigParser object and return a dict of all key - value pairs in that section.", "response": "def ConfigSectionMap(Config, section):\n    \"\"\"\n    Read a specific section from a ConfigParser() object and return\n    a dict() of all key-value pairs in that section\n    \"\"\"\n    cfg = {}\n    options = Config.options(section)\n    for option in options:\n        try:\n            cfg[option] = Config.get(section, option)\n            if cfg[option] == -1:\n                logging.debug(\"skip: {0}\".format(option))\n        except:\n            logging.debug(\"exception on {0}!\".format(option))\n            cfg[option] = None\n    return cfg"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting module - level docstring of Python module at filepath e. g. path / to / file. py.", "response": "def get_module_docstring(filepath):\n    \"Get module-level docstring of Python module at filepath, e.g. 'path/to/file.py'.\"\n    co = compile(open(filepath).read(), filepath, 'exec')\n    if co.co_consts and isinstance(co.co_consts[0], six.string_types):\n        docstring = co.co_consts[0]\n    else:\n        docstring = None\n    return docstring"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Popen(cmd, stdin=None, stdout=PIPE, debug=False, shell=\"/bin/bash\"):\n    from subprocess import Popen as P\n    if debug:\n        logging.debug(cmd)\n    # See: <https://blog.nelhage.com/2010/02/a-very-subtle-bug/>\n    proc = P(cmd, bufsize=1, stdin=stdin, stdout=stdout, \\\n             shell=True, executable=shell)\n    return proc", "response": "A wrapper around subprocess. Popen that captures the cmd stdout output to a file handle."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a sorted list of files matching the given pattern.", "response": "def glob(pathname, pattern=None):\n    \"\"\"\n    Wraps around glob.glob(), but return a sorted list.\n    \"\"\"\n    import glob as gl\n    if pattern:\n        pathname = op.join(pathname, pattern)\n    return natsorted(gl.glob(pathname))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of all files that match the given glob patterns.", "response": "def iglob(pathname, patterns):\n    \"\"\"\n    Allow multiple file formats. This is also recursive. For example:\n\n    >>> iglob(\"apps\", \"*.py,*.pyc\")\n    \"\"\"\n    matches = []\n    patterns = patterns.split(\",\") if \",\" in patterns else listify(patterns)\n    for root, dirnames, filenames in os.walk(pathname):\n        matching = []\n        for pattern in patterns:\n            matching.extend(fnmatch.filter(filenames, pattern))\n        for filename in matching:\n            matches.append(op.join(root, filename))\n    return natsorted(matches)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new folder if it does not exist.", "response": "def mkdir(dirname, overwrite=False):\n    \"\"\"\n    Wraps around os.mkdir(), but checks for existence first.\n    \"\"\"\n    if op.isdir(dirname):\n        if overwrite:\n            shutil.rmtree(dirname)\n            os.mkdir(dirname)\n            logging.debug(\"Overwrite folder `{0}`.\".format(dirname))\n        else:\n            return False  # Nothing is changed\n    else:\n        try:\n            os.mkdir(dirname)\n        except:\n            os.makedirs(dirname)\n        logging.debug(\"`{0}` not found. Creating new.\".format(dirname))\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_newer_file(a, b):\n    if not (op.exists(a) and op.exists(b)):\n        return False\n    am = os.stat(a).st_mtime\n    bm = os.stat(b).st_mtime\n    return am > bm", "response": "Check if the file a is newer than file b."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if file a is newer than file b and decide whether or not to update file b.", "response": "def need_update(a, b):\n    \"\"\"\n    Check if file a is newer than file b and decide whether or not to update\n    file b. Can generalize to two lists.\n    \"\"\"\n    a = listify(a)\n    b = listify(b)\n\n    return any((not op.exists(x)) for x in b) or \\\n           all((os.stat(x).st_size == 0 for x in b)) or \\\n           any(is_newer_file(x, y) for x in a for y in b)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef debug(level=logging.DEBUG):\n    from jcvi.apps.console import magenta, yellow\n\n    format = yellow(\"%(asctime)s [%(module)s]\")\n    format += magenta(\" %(message)s\")\n    logging.basicConfig(level=level,\n            format=format,\n            datefmt=\"%H:%M:%S\")", "response": "Turn on the debugging\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef expand(args):\n    p = OptionParser(expand.__doc__)\n    p.add_option(\"--symlink\", default=False, action=\"store_true\",\n                 help=\"Create symbolic link [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    seen = set()\n    for a in args:\n        oa = a.replace(\"/\", \"_\")\n        if oa in seen:\n            logging.debug(\"Name collision `{0}`, ignored.\".format(oa))\n            continue\n\n        cmd = \"cp -s\" if opts.symlink else \"mv\"\n        cmd += \" {0} {1}\".format(a, oa)\n        sh(cmd)\n        seen.add(oa)", "response": "%prog expand avec. txt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef timestamp(args):\n    p = OptionParser(timestamp.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    path, = args\n    for root, dirs, files in os.walk(path):\n        for f in files:\n            filename = op.join(root, f)\n            atime, mtime = get_times(filename)\n            print(filename, atime, mtime)", "response": "%prog timestamp path > timestamp. info"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef touch(args):\n    from time import ctime\n\n    p = OptionParser(touch.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    info, = args\n    fp = open(info)\n    for row in fp:\n        path, atime, mtime = row.split()\n        atime = float(atime)\n        mtime = float(mtime)\n        current_atime, current_mtime = get_times(path)\n\n        # Check if the time has changed, with resolution up to 1 sec\n        if int(atime) == int(current_atime) and \\\n           int(mtime) == int(current_mtime):\n            continue\n\n        times = [ctime(x) for x in (current_atime, current_mtime, atime, mtime)]\n        msg = \"{0} : \".format(path)\n        msg += \"({0}, {1}) => ({2}, {3})\".format(*times)\n        print(msg, file=sys.stderr)\n        os.utime(path, (atime, mtime))", "response": "%prog touch timestamp. info\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends push notifications using pre - existing APIs", "response": "def pushnotify(subject, message, api=\"pushover\", priority=0, timestamp=None):\n    \"\"\"\n    Send push notifications using pre-existing APIs\n\n    Requires a config `pushnotify.ini` file in the user home area containing\n    the necessary api tokens and user keys.\n\n    Default API: \"pushover\"\n\n    Config file format:\n    -------------------\n        [pushover]\n        token: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n        user: yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n\n        [nma]\n        apikey: zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n\n        [pushbullet]\n        apikey: bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n        iden: dddddddddddddddddddddddddddddddddddd\n    \"\"\"\n    import types\n    assert type(priority) is int and -1 <= priority <= 2, \\\n            \"Priority should be and int() between -1 and 2\"\n\n    cfgfile = op.join(op.expanduser(\"~\"), \"pushnotify.ini\")\n    Config = ConfigParser()\n    if op.exists(cfgfile):\n        Config.read(cfgfile)\n    else:\n        sys.exit(\"Push notification config file `{0}`\".format(cfgfile) + \\\n                 \" does not exist!\")\n\n    if api == \"pushover\":\n        cfg = ConfigSectionMap(Config, api)\n        token, key = cfg[\"token\"], cfg[\"user\"]\n        pushover(message, token, key, title=subject, \\\n                priority=priority, timestamp=timestamp)\n    elif api == \"nma\":\n        cfg = ConfigSectionMap(Config, api)\n        apikey = cfg[\"apikey\"]\n        nma(message, apikey, event=subject, \\\n                priority=priority)\n    elif api == \"pushbullet\":\n        cfg = ConfigSectionMap(Config, api)\n        apikey, iden = cfg[\"apikey\"], cfg['iden']\n        pushbullet(message, apikey, iden, title=subject, \\\n                type=\"note\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send_email(fromaddr, toaddr, subject, message):\n    from smtplib import SMTP\n    from email.mime.text import MIMEText\n\n    SERVER = \"localhost\"\n    _message = MIMEText(message)\n    _message['Subject'] = subject\n    _message['From'] = fromaddr\n    _message['To'] = \", \".join(toaddr)\n\n    server = SMTP(SERVER)\n    server.sendmail(fromaddr, toaddr, _message.as_string())\n    server.quit()", "response": "Send an email message using the smtplib module"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_email_address(whoami=\"user\"):\n    if whoami == \"user\":\n        username = getusername()\n        domain = getdomainname()\n\n        myemail = \"{0}@{1}\".format(username, domain)\n        return myemail\n    else:\n        fromaddr = \"notifier-donotreply@{0}\".format(getdomainname())\n        return fromaddr", "response": "Auto - generate the FROM and TO email address"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_valid_email(email):\n    import re\n\n    qtext = '[^\\\\x0d\\\\x22\\\\x5c\\\\x80-\\\\xff]'\n    dtext = '[^\\\\x0d\\\\x5b-\\\\x5d\\\\x80-\\\\xff]'\n    atom = '[^\\\\x00-\\\\x20\\\\x22\\\\x28\\\\x29\\\\x2c\\\\x2e\\\\x3a-\\\\x3c\\\\x3e\\\\x40\\\\x5b-\\\\x5d\\\\x7f-\\\\xff]+'\n    quoted_pair = '\\\\x5c[\\\\x00-\\\\x7f]'\n    domain_literal = \"\\\\x5b(?:%s|%s)*\\\\x5d\" % (dtext, quoted_pair)\n    quoted_string = \"\\\\x22(?:%s|%s)*\\\\x22\" % (qtext, quoted_pair)\n    domain_ref = atom\n    sub_domain = \"(?:%s|%s)\" % (domain_ref, domain_literal)\n    word = \"(?:%s|%s)\" % (atom, quoted_string)\n    domain = \"%s(?:\\\\x2e%s)*\" % (sub_domain, sub_domain)\n    local_part = \"%s(?:\\\\x2e%s)*\" % (word, word)\n    addr_spec = \"%s\\\\x40%s\" % (local_part, domain)\n\n    email_address = re.compile('\\A%s\\Z' % addr_spec)\n    if email_address.match(email):\n        return True\n    return False", "response": "Returns True if the given email is a valid RFC822 - like email address."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef notify(args):\n    from jcvi.utils.iter import flatten\n\n    valid_notif_methods.extend(available_push_api.keys())\n\n    fromaddr = get_email_address(whoami=\"notifier\")\n\n    p = OptionParser(notify.__doc__)\n    p.add_option(\"--method\", default=\"email\", choices=valid_notif_methods,\n                 help=\"Specify the mode of notification [default: %default]\")\n    p.add_option(\"--subject\", default=\"JCVI: job monitor\",\n                 help=\"Specify the subject of the notification message\")\n    p.set_email()\n\n    g1 = OptionGroup(p, \"Optional `push` parameters\")\n    g1.add_option(\"--api\", default=\"pushover\", \\\n                  choices=list(flatten(available_push_api.values())),\n                  help=\"Specify API used to send the push notification\")\n    g1.add_option(\"--priority\", default=0, type=\"int\",\n                  help=\"Message priority (-1 <= p <= 2) [default: %default]\")\n    g1.add_option(\"--timestamp\", default=None, type=\"int\", \\\n                  dest=\"timestamp\", \\\n                  help=\"Message timestamp in unix format [default: %default]\")\n    p.add_option_group(g1)\n\n    opts, args = p.parse_args(args)\n\n    if len(args) == 0:\n        logging.error(\"Please provide a brief message to be sent\")\n        sys.exit(not p.print_help())\n\n    subject = opts.subject\n    message = \" \".join(args).strip()\n\n    if opts.method == \"email\":\n        toaddr = opts.email.split(\",\")   # TO address should be in a list\n        for addr in toaddr:\n            if not is_valid_email(addr):\n                logging.debug(\"Email address `{0}` is not valid!\".format(addr))\n                sys.exit()\n        send_email(fromaddr, toaddr, subject, message)\n    else:\n        pushnotify(subject, message, api=opts.api, priority=opts.priority, \\\n                   timestamp=opts.timestamp)", "response": "%prog notify Message to be sent"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the full path of a common binaries.", "response": "def getpath(cmd, name=None, url=None, cfg=\"~/.jcvirc\", warn=\"exit\"):\n    \"\"\"\n    Get install locations of common binaries\n    First, check ~/.jcvirc file to get the full path\n    If not present, ask on the console and store\n    \"\"\"\n    p = which(cmd)  # if in PATH, just returns it\n    if p:\n        return p\n\n    PATH = \"Path\"\n    config = RawConfigParser()\n    cfg = op.expanduser(cfg)\n    changed = False\n    if op.exists(cfg):\n        config.read(cfg)\n\n    assert name is not None, \"Need a program name\"\n\n    try:\n        fullpath = config.get(PATH, name)\n    except NoSectionError:\n        config.add_section(PATH)\n        changed = True\n    except:\n        pass\n\n    try:\n        fullpath = config.get(PATH, name)\n    except NoOptionError:\n        msg = \"=== Configure path for {0} ===\\n\".format(name, cfg)\n        if url:\n            msg += \"URL: {0}\\n\".format(url)\n        msg += \"[Directory that contains `{0}`]: \".format(cmd)\n        fullpath = input(msg).strip()\n        config.set(PATH, name, fullpath)\n        changed = True\n\n    path = op.join(op.expanduser(fullpath), cmd)\n    if warn == \"exit\":\n        try:\n            assert is_exe(path), \\\n                \"***ERROR: Cannot execute binary `{0}`. \".format(path)\n        except AssertionError as e:\n            sys.exit(\"{0!s}Please verify and rerun.\".format(e))\n\n    if changed:\n        configfile = open(cfg, \"w\")\n        config.write(configfile)\n        logging.debug(\"Configuration written to `{0}`.\".format(cfg))\n\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsample N random elements from a.", "response": "def sample_N(a, N):\n    \"\"\" When size of N is >= size of a, random.sample() will emit an error:\n    ValueError: sample larger than population\n\n    This method handles such restrictions by repeatedly sampling when that\n    happens.\n\n    Examples:\n    >>> sample_N([1, 2, 3], 2)\n    >>> sample_N([1, 2, 3], 3)\n    >>> sample_N([1, 2, 3], 4)\n    \"\"\"\n    import random\n\n    if N < len(a):\n        return random.sample(a, N)\n\n    return [random.choice(a) for x in range(N)]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_params(self, prog=None, params=\"\"):\n        dest_prog = \"to {0}\".format(prog) if prog else \"\"\n        self.add_option(\"--params\", dest=\"extra\", default=params,\n                help=\"Extra parameters to pass {0}\".format(dest_prog) + \\\n                     \" (these WILL NOT be validated)\")", "response": "Add extra parameters to the command line programs\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_cpus(self, cpus=0):\n        from multiprocessing import cpu_count\n\n        max_cpus = cpu_count()\n        if not 0 < cpus < max_cpus:\n            cpus = max_cpus\n        self.add_option(\"--cpus\", default=cpus, type=\"int\",\n                     help=\"Number of CPUs to use, 0=unlimited [default: %default]\")", "response": "Set the number of CPUs for the current process."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_db_opts(self, dbname=\"mta4\", credentials=True):\n        from jcvi.utils.db import valid_dbconn, get_profile\n\n        self.add_option(\"--db\", default=dbname, dest=\"dbname\",\n                help=\"Specify name of database to query [default: %default]\")\n        self.add_option(\"--connector\", default=\"Sybase\", dest=\"dbconn\",\n                choices=valid_dbconn.keys(), help=\"Specify database connector [default: %default]\")\n        hostname, username, password = get_profile()\n        if credentials:\n            self.add_option(\"--hostname\", default=hostname,\n                    help=\"Specify hostname [default: %default]\")\n            self.add_option(\"--username\", default=username,\n                    help=\"Username to connect to database [default: %default]\")\n            self.add_option(\"--password\", default=password,\n                    help=\"Password to connect to database [default: %default]\")\n        self.add_option(\"--port\", type=\"int\",\n                help=\"Specify port number [default: %default]\")", "response": "Add database connection specific options"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_image_options(self, args=None, figsize=\"6x6\", dpi=300,\n                          format=\"pdf\", font=\"Helvetica\", palette=\"deep\",\n                          style=\"darkgrid\", cmap=\"jet\"):\n        \"\"\"\n        Add image format options for given command line programs.\n        \"\"\"\n        from jcvi.graphics.base import ImageOptions, setup_theme\n\n        allowed_format = (\"emf\", \"eps\", \"pdf\", \"png\", \"ps\", \\\n                          \"raw\", \"rgba\", \"svg\", \"svgz\")\n        allowed_fonts = (\"Helvetica\", \"Palatino\", \"Schoolbook\", \"Arial\")\n        allowed_styles = (\"darkgrid\", \"whitegrid\", \"dark\", \"white\", \"ticks\")\n        allowed_diverge = (\"BrBG\", \"PiYG\", \"PRGn\", \"PuOr\", \"RdBu\", \\\n                           \"RdGy\", \"RdYlBu\", \"RdYlGn\", \"Spectral\")\n\n        group = OptionGroup(self, \"Image options\")\n        self.add_option_group(group)\n\n        group.add_option(\"--figsize\", default=figsize,\n                help=\"Figure size `width`x`height` in inches [default: %default]\")\n        group.add_option(\"--dpi\", default=dpi, type=\"int\",\n                help=\"Physical dot density (dots per inch) [default: %default]\")\n        group.add_option(\"--format\", default=format, choices=allowed_format,\n                help=\"Generate image of format [default: %default]\")\n        group.add_option(\"--font\", default=font, choices=allowed_fonts,\n                help=\"Font name\")\n        group.add_option(\"--style\", default=style, choices=allowed_styles,\n                help=\"Axes background\")\n        group.add_option(\"--diverge\", default=\"PiYG\", choices=allowed_diverge,\n                help=\"Contrasting color scheme\")\n        group.add_option(\"--cmap\", default=cmap, help=\"Use this color map\")\n        group.add_option(\"--notex\", default=False, action=\"store_true\",\n                help=\"Do not use tex\")\n\n        if args is None:\n            args = sys.argv[1:]\n\n        opts, args = self.parse_args(args)\n\n        assert opts.dpi > 0\n        assert \"x\" in opts.figsize\n\n        setup_theme(style=opts.style, font=opts.font, usetex=(not opts.notex))\n\n        return opts, args, ImageOptions(opts)", "response": "Add image format options for given command line programs."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the options for the set_pairs command", "response": "def set_pairs(self):\n        \"\"\"\n        %prog pairs <blastfile|samfile|bedfile>\n\n        Report how many paired ends mapped, avg distance between paired ends, etc.\n        Paired reads must have the same prefix, use --rclip to remove trailing\n        part, e.g. /1, /2, or .f, .r, default behavior is to truncate until last\n        char.\n        \"\"\"\n        self.set_usage(self.set_pairs.__doc__)\n\n        self.add_option(\"--pairsfile\", default=None,\n                help=\"Write valid pairs to pairsfile [default: %default]\")\n        self.add_option(\"--nrows\", default=200000, type=\"int\",\n                help=\"Only use the first n lines [default: %default]\")\n        self.set_mates()\n        self.add_option(\"--pdf\", default=False, action=\"store_true\",\n                help=\"Print PDF instead ASCII histogram [default: %default]\")\n        self.add_option(\"--bins\", default=20, type=\"int\",\n                help=\"Number of bins in the histogram [default: %default]\")\n        self.add_option(\"--distmode\", default=\"ss\", choices=(\"ss\", \"ee\"),\n                help=\"Distance mode between paired reads, ss is outer distance, \" \\\n                     \"ee is inner distance [default: %default]\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_shred_id(id):\n    try:\n        parts = id.split(\".\")\n        aid = \".\".join(parts[:2])\n        fid = int(parts[2].replace(\"frag\", \"\"))\n    except:\n        aid, fid = None, None\n    return aid, fid", "response": "Get the ID of a Shred object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef anneal(args):\n    p = OptionParser(anneal.__doc__)\n    p.set_align(pctid=GoodPct, hitlen=GoodOverlap)\n    p.add_option(\"--hang\", default=GoodOverhang, type=\"int\",\n                 help=\"Maximum overhang length [default: %default]\")\n    p.set_outdir(outdir=\"outdir\")\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    agpfile, contigs = args\n    outdir = opts.outdir\n    if not op.exists(outdir):\n        mkdir(outdir)\n        cmd = \"faSplit byname {0} {1}/\".format(contigs, outdir)\n        sh(cmd)\n\n    cutoff = Cutoff(opts.pctid, opts.hitlen, opts.hang)\n    logging.debug(str(cutoff))\n\n    agp = AGP(agpfile)\n    blastfile = agpfile.replace(\".agp\", \".blast\")\n    if not op.exists(blastfile):\n        populate_blastfile(blastfile, agp, outdir, opts)\n\n    assert op.exists(blastfile)\n    logging.debug(\"File `{0}` found. Start loading.\".format(blastfile))\n    blast = BlastSlow(blastfile).to_dict()\n\n    annealedagp = \"annealed.agp\"\n    annealedfasta = \"annealed.fasta\"\n\n    newagp = deepcopy(agp)\n    clrstore = {}\n    for a, b, qreverse in agp.iter_paired_components():\n        aid = a.component_id\n        bid = b.component_id\n\n        pair = (aid, bid)\n        if pair in blast:\n            bl = blast[pair]\n        else:\n            oopts = get_overlap_opts(aid, bid, qreverse, outdir, opts)\n            o = overlap(oopts)\n            if not o:\n                continue\n            bl = o.blastline\n\n        o = Overlap(bl, a.component_span, b.component_span,\n                        cutoff, qreverse=qreverse)\n\n        if aid not in clrstore:\n            clrstore[aid] = CLR.from_agpline(a)\n        if bid not in clrstore:\n            clrstore[bid] = CLR.from_agpline(b)\n\n        aclr, bclr = clrstore[aid], clrstore[bid]\n\n        o.print_graphic()\n        if o.anneal(aclr, bclr):\n            newagp.delete_between(aid, bid, verbose=True)\n\n        if o.otype == 2:  # b ~ a\n            o = o.swapped\n            o.print_graphic()\n            if o.anneal(bclr, aclr):\n                newagp.switch_between(bid, aid, verbose=True)\n                newagp.delete_between(bid, aid, verbose=True)\n\n    logging.debug(\"A total of {0} components with modified CLR.\".\\\n                    format(len(clrstore)))\n\n    for cid, c in clrstore.items():\n        if c.is_valid:\n            continue\n        print(\"Remove {0}\".format(c), file=sys.stderr)\n        newagp.convert_to_gap(cid, verbose=True)\n\n    # Update all ranges that has modified clr\n    for a in newagp:\n        if a.is_gap:\n            continue\n        aid = a.component_id\n        if aid in clrstore:\n            c = clrstore[aid]\n            a.component_beg = c.start\n            a.component_end = c.end\n\n    newagp.print_to_file(annealedagp)\n    tidyagp = tidy([annealedagp, contigs])\n\n    build([tidyagp, contigs, annealedfasta])\n    return annealedfasta", "response": "%prog anneal agpfile contigs. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bes(args):\n    from jcvi.apps.align import run_blat\n\n    p = OptionParser(bes.__doc__)\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    bacfasta, clonename = args\n\n    entrez([clonename, \"--database=nucgss\", \"--skipcheck\"])\n    besfasta = clonename + \".fasta\"\n    blatfile = clonename + \".bes.blat\"\n    run_blat(infile=besfasta, outfile=blatfile, db=bacfasta, \\\n             pctid=95, hitlen=100, cpus=opts.cpus)\n\n    aid, asize = next(Fasta(bacfasta).itersizes())\n\n    width = 50\n    msg = \"=\" * width\n    msg += \"  \" + aid\n    print(msg, file=sys.stderr)\n\n    ratio = width * 1. / asize\n    _ = lambda x: int(round(x * ratio, 0))\n    blasts = [BlastLine(x) for x in open(blatfile)]\n    for b in blasts:\n        if b.orientation == '+':\n            msg = \" \" * _(b.sstart) + \"->\"\n        else:\n            msg = \" \" * (_(b.sstop) - 2) + \"<-\"\n        msg += \" \" * (width - len(msg) + 2)\n        msg += b.query\n        if b.orientation == '+':\n            msg += \" (hang={0})\".format(b.sstart - 1)\n        else:\n            msg += \" (hang={0})\".format(asize - b.sstop)\n\n        print(msg, file=sys.stderr)", "response": "%prog bes bacfasta clonename"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef flip(args):\n    p = OptionParser(flip.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    fastafile, = args\n    outfastafile = fastafile.rsplit(\".\", 1)[0] + \".flipped.fasta\"\n    fo = open(outfastafile, \"w\")\n    f = Fasta(fastafile, lazy=True)\n    for name, rec in f.iteritems_ordered():\n        tmpfasta = \"a.fasta\"\n        fw = open(tmpfasta, \"w\")\n        SeqIO.write([rec], fw, \"fasta\")\n        fw.close()\n\n        o = overlap([tmpfasta, name])\n        if o.orientation == '-':\n            rec.seq = rec.seq.reverse_complement()\n\n        SeqIO.write([rec], fo, \"fasta\")\n        os.remove(tmpfasta)", "response": "%prog flip fastafile\n\n    Go through each FASTA record, check against Genbank file and determines\n    whether or not to flip the sequence. This is useful before updates of the\n    sequences to make sure the same orientation is used."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef batchoverlap(args):\n    p = OptionParser(batchoverlap.__doc__)\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    pairsfile, outdir = args\n    fp = open(pairsfile)\n    cmds = []\n    mkdir(\"overlaps\")\n    for row in fp:\n        a, b = row.split()[:2]\n        oa = op.join(outdir, a + \".fa\")\n        ob = op.join(outdir, b + \".fa\")\n        cmd = \"python -m jcvi.assembly.goldenpath overlap {0} {1}\".format(oa, ob)\n        cmd += \" -o overlaps/{0}_{1}.ov\".format(a, b)\n        cmds.append(cmd)\n\n    print(\"\\n\".join(cmds))", "response": "%prog batchoverlap pairs. txt outdir"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef overlap(args):\n    from jcvi.formats.blast import chain_HSPs\n\n    p = OptionParser(overlap.__doc__)\n    p.add_option(\"--dir\", default=os.getcwd(),\n            help=\"Download sequences to dir [default: %default]\")\n    p.add_option(\"--suffix\", default=\"fasta\",\n            help=\"Suffix of the sequence file in dir [default: %default]\")\n    p.add_option(\"--qreverse\", default=False, action=\"store_true\",\n            help=\"Reverse seq a [default: %default]\")\n    p.add_option(\"--nochain\", default=False, action=\"store_true\",\n            help=\"Do not chain adjacent HSPs [default: chain HSPs]\")\n    p.set_align(pctid=GoodPct, hitlen=GoodOverlap, evalue=.01)\n    p.set_outfile(outfile=None)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    afasta, bfasta = args\n    dir = opts.dir\n    chain = not opts.nochain\n    suffix = opts.suffix\n    evalue = opts.evalue\n    pctid = opts.pctid\n    hitlen = opts.hitlen\n    cutoff = Cutoff(pctid, hitlen)\n\n    # Check first whether it is file or accession name\n    if not op.exists(afasta):\n        af = op.join(dir, \".\".join((afasta, suffix)))\n        if not op.exists(af):  # Check to avoid redownload\n            entrez([afasta, \"--skipcheck\", \"--outdir=\" + dir])\n        afasta = af\n\n    if not op.exists(bfasta):\n        bf = op.join(dir, \".\".join((bfasta, suffix)))\n        if not op.exists(bf):\n            entrez([bfasta, \"--skipcheck\", \"--outdir=\" + dir])\n        bfasta = bf\n\n    assert op.exists(afasta) and op.exists(bfasta)\n\n    cmd = \"blastn -dust no\"\n    cmd += \" -query {0} -subject {1}\".format(afasta, bfasta)\n    cmd += \" -evalue {0} -outfmt 6 -perc_identity {1}\".format(evalue, pctid)\n\n    fp = popen(cmd)\n    hsps = fp.readlines()\n\n    hsps = [BlastLine(x) for x in hsps]\n    hsps = [x for x in hsps if x.hitlen >= hitlen]\n    if chain:\n        logging.debug(\"Chain HSPs in the Blast output.\")\n        dist = 2 * hitlen  # Distance to chain the HSPs\n        hsps = chain_HSPs(hsps, xdist=dist, ydist=dist)\n\n    if len(hsps) == 0:\n        print(\"No match found.\", file=sys.stderr)\n        return None\n\n    besthsp = hsps[0]\n\n    aid, asize = next(Fasta(afasta).itersizes())\n    bid, bsize = next(Fasta(bfasta).itersizes())\n    o = Overlap(besthsp, asize, bsize, cutoff, qreverse=opts.qreverse)\n    o.print_graphic()\n\n    if opts.outfile:\n        fw = must_open(opts.outfile, \"w\")\n        print(str(o), file=fw)\n        fw.close()\n\n    return o", "response": "%prog overlap a. fasta b. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef certificate(args):\n    p = OptionParser(certificate.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    tpffile, certificatefile = args\n    fastadir = \"fasta\"\n\n    tpf = TPF(tpffile)\n\n    data = check_certificate(certificatefile)\n    fw = must_open(certificatefile, \"w\")\n    for i, a in enumerate(tpf):\n        if a.is_gap:\n            continue\n\n        aid = a.component_id\n\n        af = op.join(fastadir, aid + \".fasta\")\n        if not op.exists(af):  # Check to avoid redownload\n            entrez([aid, \"--skipcheck\", \"--outdir=\" + fastadir])\n\n        north, south = tpf.getNorthSouthClone(i)\n        aphase, asize = phase(aid)\n\n        for tag, p in ((\"North\", north), (\"South\", south)):\n            if not p:  # end of the chromosome\n                ov = \"telomere\\t{0}\".format(asize)\n            elif p.isCloneGap:\n                bphase = \"0\"\n                ov = \"{0}\\t{1}\".format(p.gap_type, asize)\n            else:\n                bid = p.component_id\n                bphase, bsize = phase(bid)\n                key = (tag, aid, bid)\n                if key in data:\n                    print(data[key], file=fw)\n                    continue\n\n                ar = [aid, bid, \"--dir=\" + fastadir]\n                o = overlap(ar)\n                ov = o.certificateline if o \\\n                        else \"{0}\\t{1}\\tNone\".format(bid, asize)\n\n            print(\"\\t\".join(str(x) for x in \\\n                    (tag, a.object, aphase, bphase, aid, ov)), file=fw)\n            fw.flush()", "response": "%prog certificate tpffile certificatefile"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef agp(args):\n    from jcvi.formats.base import DictFile\n\n    p = OptionParser(agp.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    tpffile, certificatefile, agpfile = args\n    orientationguide = DictFile(tpffile, valuepos=2)\n    cert = Certificate(certificatefile)\n    cert.write_AGP(agpfile, orientationguide=orientationguide)", "response": "%prog agp tpffile certificatefile agpfile"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_hangs(self):\n        aLhang, aRhang = self.qstart - 1, self.asize - self.qstop\n        bLhang, bRhang = self.sstart - 1, self.bsize - self.sstop\n        if self.orientation == '-':\n            bLhang, bRhang = bRhang, bLhang\n        if self.qreverse:\n            aLhang, aRhang = aRhang, aLhang\n            bLhang, bRhang = bRhang, bLhang\n\n        return aLhang, aRhang, bLhang, bRhang", "response": "Return the list of possible hangs for the given resource type and alignment."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the start and end of the two sequences in the rule.", "response": "def update_clr(self, aclr, bclr):\n        \"\"\"\n        Zip the two sequences together, using \"left-greedy\" rule\n\n        =============                   seqA\n                 ||||\n                 ====(===============)  seqB\n        \"\"\"\n        print(aclr, bclr, file=sys.stderr)\n        otype = self.otype\n\n        if otype == 1:\n            if aclr.orientation == '+':\n                aclr.end = self.qstop\n            else:\n                aclr.start = self.qstart\n            if bclr.orientation == '+':\n                bclr.start = self.sstop + 1\n            else:\n                bclr.end = self.sstart - 1\n\n        elif otype == 3:\n            aclr.start = aclr.end\n\n        elif otype == 4:\n            bclr.start = bclr.end\n\n        print(aclr, bclr, file=sys.stderr)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef print_graphic(self):\n        aLhang, aRhang, bLhang, bRhang = self.get_hangs()\n\n        achar = \">\"\n        bchar = \"<\" if self.orientation == '-' else \">\"\n        if self.qreverse:\n            achar = \"<\"\n            bchar = {\">\" : \"<\", \"<\" : \">\"}[bchar]\n\n        print(aLhang, aRhang, bLhang, bRhang, file=sys.stderr)\n        width = 50  # Canvas\n        hitlen = self.hitlen\n        lmax = max(aLhang, bLhang)\n        rmax = max(aRhang, bRhang)\n        bpwidth = lmax + hitlen + rmax\n        ratio = width * 1. / bpwidth\n\n        _ = lambda x: int(round(x * ratio, 0))\n        a1, a2 = _(aLhang), _(aRhang)\n        b1, b2 = _(bLhang), _(bRhang)\n        hit = max(_(hitlen), 1)\n\n        msg = \" \" * max(b1 - a1, 0)\n        msg += achar * (a1 + hit + a2)\n        msg += \" \" * (width - len(msg) + 2)\n        msg += \"{0} ({1})\".format(self.aid, self.asize)\n        print(msg, file=sys.stderr)\n\n        msg = \" \" * max(a1, b1)\n        msg += \"|\" * hit\n        print(msg, file=sys.stderr)\n\n        msg = \" \" * max(a1 - b1, 0)\n        msg += bchar * (b1 + hit + b2)\n        msg += \" \" * (width - len(msg) + 2)\n        msg += \"{0} ({1})\".format(self.bid, self.bsize)\n        print(msg, file=sys.stderr)\n        print(self, file=sys.stderr)", "response": "Print the current entry and its associated information."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites the AGP file for each component.", "response": "def write_AGP(self, filename, orientationguide={}):\n        \"\"\"\n        For each component, we have two overlaps: North and South.\n\n        =======\n           ||||             South\n           ====(=================)  Current BAC\n           North             ||||\n                             ===============\n\n        For the case that says \"Non-terminal\", the overlap will not be\n        considered. North-South would suggest a '+' orientation, South-North\n        would suggest a '-' orientation. In most cases, unless the overlap\n        involves phase1 BAC, the selected range will be shown as the brackets\n        above - exclude North overlap, and include South overlap (aka the\n        \"left-greedy\" rule).\n        \"\"\"\n        fw = must_open(filename, \"w\")\n        for aid, bb in groupby(self.lines, key=lambda x: x.aid):\n            bb = list(bb)\n            north, south = bb\n            aid = north.aid\n            assert aid == south.aid\n\n            aphase = north.aphase\n            chr = north.chr\n            size = north.asize\n            ar = [chr, 0, 0, 0]\n\n            northline = southline = None\n            northrange = southrange = None\n\n            # Warn if adjacent components do not have valid overlaps\n            if south.is_no_overlap:\n                print(south, file=sys.stderr)\n\n            # Most gaps, except telomeres occur twice, so only do the \"North\"\n            if north.is_gap:\n                bar = ar + self.get_agp_gap(north.bid)\n                northline = \"\\t\".join(str(x) for x in bar)\n            else:\n                if north.isTerminal:\n                    northrange = north.astart, north.astop\n\n            if south.is_gap:\n                if south.bid == \"telomere\":\n                    bar = ar + self.get_agp_gap(south.bid)\n                    southline = \"\\t\".join(str(x) for x in bar)\n            else:\n                if south.isTerminal:\n                    southrange = south.astart, south.astop\n                else:\n                    bar = ar + self.get_agp_gap(\"fragment\")\n                    southline = \"\\t\".join(str(x) for x in bar)\n\n            # Determine the orientation and clear range for the current BAC\n            clr = [1, size]\n            orientation = sorientation = None\n            if northrange:\n                start, stop = northrange\n                Lhang = start - 1\n                Rhang = size - stop\n\n                orientation = '+' if Lhang < Rhang else '-'\n                if north.bphase == 1 and north.bphase < aphase:\n                    if Lhang < Rhang:  # North overlap at 5`\n                        clr[0] = start\n                    else:\n                        clr[1] = stop\n                # Override left-greedy (also see below)\n                else:\n                    if Lhang < Rhang:\n                        clr[0] = stop + 1\n                    else:\n                        clr[1] = start - 1\n\n            if southrange:\n                start, stop = southrange\n                Lhang = start - 1\n                Rhang = size - stop\n\n                sorientation = '+' if Lhang > Rhang else '-'\n                # Override left-greedy (also see above)\n                if aphase == 1 and aphase < south.bphase:\n                    if Lhang < Rhang:  # South overlap at 5`\n                        clr[0] = stop + 1\n                    else:\n                        clr[1] = start - 1\n                else:\n                    if Lhang < Rhang:\n                        clr[0] = start\n                    else:\n                        clr[1] = stop\n\n            if orientation:\n                if sorientation:\n                    try:\n                        assert orientation == sorientation, \\\n                                \"Orientation conflicts:\\n{0}\\n{1}\".format(north, south)\n                    except AssertionError as e:\n                        logging.debug(e)\n            else:\n                if sorientation:\n                    orientation = sorientation\n                else:  # Both overlaps fail to define orientation\n                    orientation = orientationguide.get(aid, \"+\")\n\n            component_type = \"D\" if aphase in (1, 2) else \"F\"\n            bar = ar + [component_type, aid, clr[0], clr[1], orientation]\n            cline = \"\\t\".join(str(x) for x in bar)\n\n            if northline:\n                print(northline, file=fw)\n            print(cline, file=fw)\n            if southline:\n                print(southline, file=fw)\n\n        fw.close()\n\n        reindex([filename, \"--inplace\"])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef exonunion(args):\n    p = OptionParser(exonunion.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    gencodebed, = args\n    beds = BedTool(gencodebed)\n    # fields[3] is gene_id; fields[6] is gene_name\n    for g, gb in groupby(beds, key=lambda x: x.fields[3]):\n        gb = BedTool(gb)\n        sys.stdout.write(str(gb.sort().merge(c=\"4,5,6,7\",\n                             o=','.join(['first'] * 4))))", "response": "%prog exonunion gencode. bed"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract GAIN and LOSS summary from a VCF file", "response": "def get_gain_loss_summary(vcffile):\n    \"\"\" Extract Canvas:GAIN/LOSS/REF/LOH tags\n    \"\"\"\n    from cyvcf2 import VCF\n\n    counter = Counter()\n    for v in VCF(vcffile):\n        tag = v.ID.split(\":\")[1]\n        counter[tag] += 1\n\n    return counter"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts all copy number segments from a CANVAS file", "response": "def parse_segments(vcffile):\n    \"\"\" Extract all copy number segments from a CANVAS file\n\n    VCF line looks like:\n    chr1    788879  Canvas:GAIN:chr1:788880-821005  N       <CNV>   2       q10\n    SVTYPE=CNV;END=821005;CNVLEN=32126      RC:BC:CN:MCC    157:4:3:2\n    \"\"\"\n    from cStringIO import StringIO\n    from cyvcf2 import VCF\n\n    output = StringIO()\n    for v in VCF(vcffile):\n        chrom = v.CHROM\n        start = v.start\n        end = v.INFO.get('END') - 1\n        cn, = v.format('CN')[0]\n        print(\"\\t\".join(str(x) for x in (chrom, start, end, cn)), file=output)\n\n    beds = BedTool(output.getvalue(), from_string=True)\n    return beds"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the mean and median value of a counter", "response": "def counter_mean_and_median(counter):\n    \"\"\" Calculate the mean and median value of a counter\n    \"\"\"\n    if not counter:\n        return np.nan, np.nan\n\n    total = sum(v for k, v in counter.items())\n    mid = total / 2\n    weighted_sum = 0\n    items_seen = 0\n    median_found = False\n    for k, v in sorted(counter.items()):\n        weighted_sum += k * v\n        items_seen += v\n        if not median_found and items_seen >= mid:\n            median = k\n            median_found = True\n    mean = weighted_sum * 1. / total\n    return mean, median"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gcn(args):\n    p = OptionParser(gcn.__doc__)\n    p.set_cpus()\n    p.set_tmpdir(tmpdir=\"tmp\")\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) < 2:\n        sys.exit(not p.print_help())\n\n    exonbed = args[0]\n    canvasvcfs = args[1:]\n    tsvfile = opts.outfile\n    tmpdir = opts.tmpdir\n\n    mkdir(tmpdir)\n    set_tempdir(tmpdir)\n\n    df = vcf_to_df(canvasvcfs, exonbed, opts.cpus)\n    for suffix in (\".avgcn\", \".medcn\"):\n        df_to_tsv(df, tsvfile, suffix)", "response": "%prog gcn exon. bed canvas. vcf"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef vcf_to_df_worker(arg):\n    canvasvcf, exonbed, i = arg\n    logging.debug(\"Working on job {}: {}\".format(i, canvasvcf))\n    samplekey = op.basename(canvasvcf).split(\".\")[0].rsplit('_', 1)[0]\n    d = {'SampleKey': samplekey}\n\n    exons = BedTool(exonbed)\n    cn = parse_segments(canvasvcf)\n    overlaps = exons.intersect(cn, wao=True)\n    gcn_store = {}\n    for ov in overlaps:\n        # Example of ov.fields:\n        # [u'chr1', u'11868', u'12227', u'ENSG00000223972.5',\n        # u'ENST00000456328.2', u'transcribed_unprocessed_pseudogene',\n        # u'DDX11L1', u'.', u'-1', u'-1', u'.', u'0']\n        gene_name = \"|\".join((ov.fields[6], ov.fields[3], ov.fields[5]))\n        if gene_name not in gcn_store:\n            gcn_store[gene_name] = defaultdict(int)\n\n        cn = ov.fields[-2]\n        if cn == \".\":\n            continue\n        cn = int(cn)\n        if cn > 10:\n            cn = 10\n        amt = int(ov.fields[-1])\n        gcn_store[gene_name][cn] += amt\n\n    for k, v in sorted(gcn_store.items()):\n        v_mean, v_median = counter_mean_and_median(v)\n        d[k + \".avgcn\"] = v_mean\n        d[k + \".medcn\"] = v_median\n    cleanup()\n    return d", "response": "Convert a CANVAS vcf to a dict of single thread\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncompiles a number of vcf files into a Pandas DataFrame", "response": "def vcf_to_df(canvasvcfs, exonbed, cpus):\n    \"\"\" Compile a number of vcf files into tsv file for easy manipulation\n    \"\"\"\n    df = pd.DataFrame()\n    p = Pool(processes=cpus)\n    results = []\n    args = [(x, exonbed, i) for (i, x) in enumerate(canvasvcfs)]\n    r = p.map_async(vcf_to_df_worker, args,\n                    callback=results.append)\n    r.wait()\n\n    for res in results:\n        df = df.append(res, ignore_index=True)\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef df_to_tsv(df, tsvfile, suffix):\n    tsvfile += suffix\n    columns = [\"SampleKey\"] + sorted(x for x in df.columns\n                                     if x.endswith(suffix))\n    tf = df.reindex_axis(columns, axis='columns')\n    tf.sort_values(\"SampleKey\")\n    tf.to_csv(tsvfile, sep='\\t', index=False, float_format='%.4g', na_rep=\"na\")\n    print(\"TSV output written to `{}` (# samples={})\"\\\n                .format(tsvfile, tf.shape[0]), file=sys.stderr)", "response": "Serialize the dataframe as a tsv file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot(args):\n    from jcvi.graphics.base import savefig\n\n    p = OptionParser(plot.__doc__)\n    opts, args, iopts = p.set_image_options(args, figsize=\"8x7\", format=\"png\")\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    workdir, sample_key, chrs = args\n    chrs = chrs.split(\",\")\n    hmm = CopyNumberHMM(workdir=workdir)\n    hmm.plot(sample_key, chrs=chrs)\n\n    image_name = sample_key + \"_cn.\" + iopts.format\n    savefig(image_name, dpi=iopts.dpi, iopts=iopts)", "response": "%prog plot workdir sample chr1 chr2"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sweep(args):\n    p = OptionParser(sweep.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    workdir, sample_key = args\n    golden_ratio = (1 + 5 ** .5) / 2\n    cmd = \"python -m jcvi.variation.cnv hmm {} {}\".format(workdir, sample_key)\n    cmd += \" --mu {:.5f} --sigma {:.3f} --threshold {:.3f}\"\n    mus = [.00012 * golden_ratio ** x for x in range(10)]\n    sigmas = [.0012 * golden_ratio ** x for x in range(20)]\n    thresholds = [.1 * golden_ratio ** x for x in range(10)]\n    print(mus, file=sys.stderr)\n    print(sigmas, file=sys.stderr)\n    print(thresholds, file=sys.stderr)\n    for mu in mus:\n        for sigma in sigmas:\n            for threshold in thresholds:\n                tcmd = cmd.format(mu, sigma, threshold)\n                print(tcmd)", "response": "%prog sweep workdir 102340_NA12878"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compare(args):\n    p = OptionParser(compare.__doc__)\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) < 2:\n        sys.exit(not p.print_help())\n\n    truths = args[0]\n    cnvoutputs = args[1:]\n    cpus = min(len(cnvoutputs), opts.cpus)\n    p = Pool(processes=cpus)\n    results = []\n    files = [(x, truths) for x in cnvoutputs]\n    r = p.map_async(compare_worker, files, callback=results.append)\n    r.wait()\n\n    for res in results:\n        print(\"\\n\".join(res))", "response": "%prog compare cnv. txt cnv. txt"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cib(args):\n    p = OptionParser(cib.__doc__)\n    p.add_option(\"--prefix\", help=\"Report seqids with this prefix only\")\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    bamfile, samplekey = args\n    mkdir(samplekey)\n    bam = pysam.AlignmentFile(bamfile, \"rb\")\n    refs = [x for x in bam.header[\"SQ\"]]\n    prefix = opts.prefix\n    if prefix:\n        refs = [x for x in refs if x[\"SN\"].startswith(prefix)]\n\n    task_args = []\n    for r in refs:\n        task_args.append((bamfile, r, samplekey))\n    cpus = min(opts.cpus, len(task_args))\n    logging.debug(\"Use {} cpus\".format(cpus))\n\n    p = Pool(processes=cpus)\n    for res in p.imap(bam_to_cib, task_args):\n        continue", "response": "%prog cib bamfile samplekey"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef batchccn(args):\n    p = OptionParser(batchccn.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    csvfile, = args\n    mm = MakeManager()\n    pf = op.basename(csvfile).split(\".\")[0]\n    mkdir(pf)\n\n    header = next(open(csvfile))\n    header = None if header.strip().endswith(\".bam\") else \"infer\"\n    logging.debug(\"Header={}\".format(header))\n    df = pd.read_csv(csvfile, header=header)\n    cmd = \"perl /mnt/software/ccn_gcn_hg38_script/ccn_gcn_hg38.pl\"\n    cmd += \" -n {} -b {}\"\n    cmd += \" -o {} -r hg38\".format(pf)\n    for i, (sample_key, bam) in df.iterrows():\n        cmdi = cmd.format(sample_key, bam)\n        outfile = \"{}/{}/{}.ccn\".format(pf, sample_key, sample_key)\n        mm.add(csvfile, outfile, cmdi)\n    mm.write()", "response": "%prog batchccn test. csv"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mergecn(args):\n    p = OptionParser(mergecn.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    csvfile, = args\n    samples = [x.replace(\"-cn\", \"\").strip().strip(\"/\") for x in open(csvfile)]\n    betadir = \"beta\"\n    mkdir(betadir)\n    for seqid in allsomes:\n        names = [op.join(s + \"-cn\", \"{}.{}.cn\".\n                 format(op.basename(s), seqid)) for s in samples]\n        arrays = [np.fromfile(name, dtype=np.float) for name in names]\n        shapes = [x.shape[0] for x in arrays]\n        med_shape = np.median(shapes)\n        arrays = [x for x in arrays if x.shape[0] == med_shape]\n        ploidy = 2 if seqid not in (\"chrY\", \"chrM\") else 1\n        if seqid in sexsomes:\n            chr_med = [np.median([x for x in a if x > 0]) for a in arrays]\n            chr_med = np.array(chr_med)\n            idx = get_kmeans(chr_med, k=2)\n            zero_med = np.median(chr_med[idx == 0])\n            one_med = np.median(chr_med[idx == 1])\n            logging.debug(\"K-means with {} c0:{} c1:{}\"\n                          .format(seqid, zero_med, one_med))\n            higher_idx = 1 if one_med > zero_med else 0\n            # Use the higher mean coverage componen\n            arrays = np.array(arrays)[idx == higher_idx]\n        arrays = [[x] for x in arrays]\n        ar = np.concatenate(arrays)\n        print(seqid, ar.shape)\n        rows, columns = ar.shape\n        beta = []\n        std = []\n        for j in xrange(columns):\n            a = ar[:, j]\n            beta.append(np.median(a))\n            std.append(np.std(a) / np.mean(a))\n        beta = np.array(beta) / ploidy\n        betafile = op.join(betadir, \"{}.beta\".format(seqid))\n        beta.tofile(betafile)\n        stdfile = op.join(betadir, \"{}.std\".format(seqid))\n        std = np.array(std)\n        std.tofile(stdfile)\n        logging.debug(\"Written to `{}`\".format(betafile))\n        ar.tofile(\"{}.bin\".format(seqid))", "response": "%prog mergecn allsomes. csv"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreports the copy number and start - end segment of the given compressed idices.", "response": "def annotate_segments(self, Z):\n        \"\"\" Report the copy number and start-end segment\n        \"\"\"\n        # We need a way to go from compressed idices to original indices\n        P = Z.copy()\n        P[~np.isfinite(P)] = -1\n        _, mapping = np.unique(np.cumsum(P >= 0), return_index=True)\n\n        dZ = Z.compressed()\n        uniq, idx = np.unique(dZ, return_inverse=True)\n        segments = []\n        for i, mean_cn in enumerate(uniq):\n            if not np.isfinite(mean_cn):\n                continue\n            for rr in contiguous_regions(idx == i):\n                segments.append((mean_cn, mapping[rr]))\n\n        return segments"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start(args):\n    p = OptionParser(start.__doc__)\n    p.add_option(\"--ondemand\", default=False, action=\"store_true\",\n                 help=\"Do we want a more expensive on-demand instance\")\n    p.add_option(\"--profile\", default=\"mvrad-datasci-role\", help=\"Profile name\")\n    p.add_option(\"--price\", default=4.0, type=float, help=\"Spot price\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 0:\n        sys.exit(not p.print_help())\n\n    role([\"htang\"])\n    session = boto3.Session(profile_name=opts.profile)\n    client = session.client('ec2')\n    s = InstanceSkeleton()\n\n    # Make sure the instance id is empty\n    instance_id = s.instance_id\n    if instance_id != \"\":\n        logging.error(\"Instance exists {}\".format(instance_id))\n        sys.exit(1)\n\n    launch_spec = s.launch_spec\n    instance_id = \"\"\n\n    if opts.ondemand:\n        # Launch on-demand instance\n        response = client.run_instances(\n            BlockDeviceMappings=s.block_device_mappings,\n            MaxCount=1, MinCount=1,\n            ImageId=s.image_id,\n            InstanceType=s.instance_type,\n            KeyName=s.key_name,\n            Placement={\"AvailabilityZone\": s.availability_zone},\n            SecurityGroupIds=s.security_group_ids,\n            SubnetId=s.subnet_id,\n            EbsOptimized=s.ebs_optimized,\n            IamInstanceProfile=s.iam_instance_profile,\n        )\n        instance_id = response[\"Instances\"][0][\"InstanceId\"]\n\n    else:\n        # Launch spot instance\n        response = client.request_spot_instances(\n            SpotPrice=str(opts.price),\n            InstanceCount=1,\n            Type=\"one-time\",\n            AvailabilityZoneGroup=s.availability_zone,\n            LaunchSpecification=launch_spec\n        )\n\n        request_id = response[\"SpotInstanceRequests\"][0][\"SpotInstanceRequestId\"]\n        print(\"Request id {}\".format(request_id), file=sys.stderr)\n\n        while not instance_id:\n            response = client.describe_spot_instance_requests(\n                SpotInstanceRequestIds=[request_id]\n            )\n            if \"InstanceId\" in response[\"SpotInstanceRequests\"][0]:\n                instance_id = response[\"SpotInstanceRequests\"][0][\"InstanceId\"]\n            else:\n                logging.debug(\"Waiting to be fulfilled ...\")\n                time.sleep(10)\n\n    # Check if the instance is running\n    print(\"Instance id {}\".format(instance_id), file=sys.stderr)\n    status = \"\"\n    while status != \"running\":\n        logging.debug(\"Waiting instance to run ...\")\n        time.sleep(3)\n        response = client.describe_instance_status(InstanceIds=[instance_id])\n        if len(response[\"InstanceStatuses\"]) > 0:\n            status = response[\"InstanceStatuses\"][0][\"InstanceState\"][\"Name\"]\n\n    # Tagging\n    name = \"htang-lx-ondemand\" if opts.ondemand else \"htang-lx-spot\"\n    response = client.create_tags(\n        Resources=[instance_id],\n        Tags=[{\"Key\": k, \"Value\": v} for k, v in { \\\n                    \"Name\": name,\n                    \"owner\": \"htang\",\n                    \"project\": \"mv-bioinformatics\"\n                }.items()]\n    )\n\n    # Attach working volumes\n    volumes = s.volumes\n    for volume in volumes:\n        response = client.attach_volume(\n            VolumeId=volume[\"VolumeId\"],\n            InstanceId=instance_id,\n            Device=volume[\"Device\"]\n        )\n\n    # Save instance id and ip\n    response = client.describe_instances(InstanceIds=[instance_id])\n    ip_address = response[\"Reservations\"][0][\"Instances\"][0][\"PrivateIpAddress\"]\n    print(\"IP address {}\".format(ip_address), file=sys.stderr)\n\n    s.save_instance_id(instance_id, ip_address)", "response": "%prog start\n\n    Launch ec2 instance through command line."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cp(args):\n    p = OptionParser(cp.__doc__)\n    p.add_option(\"--force\", default=False,\n                 action=\"store_true\", help=\"Force overwrite if exists\")\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    store, folder = args\n    force = opts.force\n    cpus = opts.cpus\n    if op.exists(store):\n        contents = [x.strip().split(\",\") for x in open(store)]\n    else:\n        contents = glob_s3(store)\n\n    tasks = []\n    for c in contents:\n        if isinstance(c, six.string_types):\n            oc = op.basename(c)\n            tc = op.join(folder, oc)\n        else:\n            if len(c) == 2:\n                c, tc = c\n            else:\n                c, = c\n                tc = op.basename(c)\n        tasks.append((c, tc, force))\n\n    worker_pool = Pool(cpus)\n    worker_pool.map(worker, tasks)\n    worker_pool.close()\n    worker_pool.join()", "response": "%prog cp store folder"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef role(args):\n    src_acct, src_username, dst_acct, dst_role = \\\n        \"205134639408 htang 114692162163 mvrad-datasci-role\".split()\n\n    p = OptionParser(role.__doc__)\n    p.add_option(\"--profile\", default=\"mvrad-datasci-role\", help=\"Profile name\")\n    p.add_option('--device',\n                    default=\"arn:aws:iam::\" + src_acct + \":mfa/\" + src_username,\n                    metavar='arn:aws:iam::123456788990:mfa/dudeman',\n                    help=\"The MFA Device ARN. This value can also be \"\n                    \"provided via the environment variable 'MFA_DEVICE' or\"\n                    \" the ~/.aws/credentials variable 'aws_mfa_device'.\")\n    p.add_option('--duration',\n                    type=int, default=3600,\n                    help=\"The duration, in seconds, that the temporary \"\n                            \"credentials should remain valid. Minimum value: \"\n                            \"900 (15 minutes). Maximum: 129600 (36 hours). \"\n                            \"Defaults to 43200 (12 hours), or 3600 (one \"\n                            \"hour) when using '--assume-role'. This value \"\n                            \"can also be provided via the environment \"\n                            \"variable 'MFA_STS_DURATION'. \")\n    p.add_option('--assume-role', '--assume',\n                    default=\"arn:aws:iam::\" + dst_acct + \":role/\" + dst_role,\n                    metavar='arn:aws:iam::123456788990:role/RoleName',\n                    help=\"The ARN of the AWS IAM Role you would like to \"\n                    \"assume, if specified. This value can also be provided\"\n                    \" via the environment variable 'MFA_ASSUME_ROLE'\")\n    p.add_option('--role-session-name',\n                    help=\"Friendly session name required when using \"\n                    \"--assume-role\",\n                    default=getpass.getuser())\n    p.add_option('--force',\n                    help=\"Refresh credentials even if currently valid.\",\n                    action=\"store_true\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    # Use a config to check the expiration of session token\n    config = get_config(AWS_CREDS_PATH)\n    validate(opts, config)", "response": "%prog role htang - > role"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvalidate if the config file is properly structured", "response": "def validate(args, config):\n    \"\"\"Validate if the config file is properly structured\n    \"\"\"\n    profile = args.profile\n    if not args.profile:\n        if os.environ.get('AWS_PROFILE'):\n            args.profile = os.environ.get('AWS_PROFILE')\n        else:\n            args.profile = 'default'\n\n    if args.assume_role:\n        role_msg = \"with assumed role: %s\" % (args.assume_role,)\n    elif config.has_option(args.profile, 'assumed_role_arn'):\n        role_msg = \"with assumed role: %s\" % (\n            config.get(args.profile, 'assumed_role_arn'))\n    else:\n        role_msg = \"\"\n    logging.info('Validating credentials for profile: %s %s' %\n                (profile, role_msg))\n    reup_message = \"Obtaining credentials for a new role or profile.\"\n\n    try:\n        key_id = config.get(profile, 'aws_access_key_id')\n        access_key = config.get(profile, 'aws_secret_access_key')\n    except NoSectionError:\n        log_error_and_exit(\"Credentials session '[%s]' is missing. \"\n                           \"You must add this section to your credentials file \"\n                           \"along with your long term 'aws_access_key_id' and \"\n                           \"'aws_secret_access_key'\" % (profile,))\n    except NoOptionError as e:\n        log_error_and_exit(e)\n\n    # get device from param, env var or config\n    if not args.device:\n        if os.environ.get('MFA_DEVICE'):\n            args.device = os.environ.get('MFA_DEVICE')\n        elif config.has_option(profile, 'aws_mfa_device'):\n            args.device = config.get(profile, 'aws_mfa_device')\n        else:\n            log_error_and_exit('You must provide --device or MFA_DEVICE or set '\n                               '\"aws_mfa_device\" in \".aws/credentials\"')\n\n    # get assume_role from param or env var\n    if not args.assume_role:\n        if os.environ.get('MFA_ASSUME_ROLE'):\n            args.assume_role = os.environ.get('MFA_ASSUME_ROLE')\n        elif config.has_option(profile, 'assume_role'):\n            args.assume_role = config.get(profile, 'assume_role')\n\n    # get duration from param, env var or set default\n    if not args.duration:\n        if os.environ.get('MFA_STS_DURATION'):\n            args.duration = int(os.environ.get('MFA_STS_DURATION'))\n        else:\n            args.duration = 3600 if args.assume_role else 43200\n\n    # If this is False, only refresh credentials if expired. Otherwise\n    # always refresh.\n    force_refresh = False\n\n    # Validate presence of profile-term section\n    if not config.has_section(profile):\n        config.add_section(profile)\n        force_refresh = True\n    # Validate option integrity of profile section\n    else:\n        required_options = ['assumed_role',\n                            'aws_access_key_id', 'aws_secret_access_key',\n                            'aws_session_token', 'aws_security_token',\n                            'expiration']\n        try:\n            short_term = {}\n            for option in required_options:\n                short_term[option] = config.get(profile, option)\n        except NoOptionError:\n            logging.warn(\"Your existing credentials are missing or invalid, \"\n                        \"obtaining new credentials.\")\n            force_refresh = True\n\n        try:\n            current_role = config.get(profile, 'assumed_role_arn')\n        except NoOptionError:\n            current_role = None\n\n        if args.force:\n            logging.info(\"Forcing refresh of credentials.\")\n            force_refresh = True\n        # There are not credentials for an assumed role,\n        # but the user is trying to assume one\n        elif current_role is None and args.assume_role:\n            logging.info(reup_message)\n            force_refresh = True\n        # There are current credentials for a role and\n        # the role arn being provided is the same.\n        elif (current_role is not None and\n                args.assume_role and current_role == args.assume_role):\n            pass\n        # There are credentials for a current role and the role\n        # that is attempting to be assumed is different\n        elif (current_role is not None and\n              args.assume_role and current_role != args.assume_role):\n            logging.info(reup_message)\n            force_refresh = True\n        # There are credentials for a current role and no role arn is\n        # being supplied\n        elif current_role is not None and args.assume_role is None:\n            logging.info(reup_message)\n            force_refresh = True\n\n    should_refresh = True\n\n    # Unless we're forcing a refresh, check expiration.\n    if not force_refresh:\n        exp = datetime.strptime(\n            config.get(profile, 'expiration'), '%Y-%m-%d %H:%M:%S')\n        diff = exp - datetime.utcnow()\n        if diff.total_seconds() <= 0:\n            logging.info(\"Your credentials have expired, renewing.\")\n        else:\n            should_refresh = False\n            logging.info(\n                \"Your credentials are still valid for %s seconds\"\n                \" they will expire at %s\"\n                % (diff.total_seconds(), exp))\n\n    if should_refresh:\n        get_credentials(profile, args, config)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef query(args):\n    p = OptionParser(query.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    locifile, contig = args\n    idx = build_index(locifile)\n    pos = idx[contig]\n    logging.debug(\"Contig {0} found at pos {1}\".format(contig, pos))\n    fp = open(locifile)\n    fp.seek(pos)\n    section = []\n    while True:\n        row = fp.readline()\n        if row.startswith(\"//\") and row.split()[1] != contig:\n            break\n        section.append(row)\n    print(\"\".join(section))", "response": "%prog query out. loci contig\n    Random access to loci file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef weblogo(args):\n    import numpy as np\n    from jcvi.utils.progressbar import ProgressBar, Percentage, Bar, ETA\n\n    p = OptionParser(weblogo.__doc__)\n    p.add_option(\"-N\", default=10, type=\"int\",\n                 help=\"Count the first and last N bases\")\n    p.add_option(\"--nreads\", default=1000000, type=\"int\",\n                 help=\"Parse first N reads\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    fastqfile, = args\n    N = opts.N\n    nreads = opts.nreads\n\n    pat = \"ATCG\"\n    L = np.zeros((4, N), dtype=\"int32\")\n    R = np.zeros((4, N), dtype=\"int32\")\n    p = dict((a, i) for (i, a) in enumerate(pat))\n    L4, R3 = Counter(), Counter()\n    widgets = ['Parse reads: ', Percentage(), ' ',\n               Bar(marker='>', left='[', right=']'), ' ', ETA()]\n    pr = ProgressBar(maxval=nreads, term_width=60, widgets=widgets).start()\n\n    k = 0\n    fw_L = open(\"L.fasta\", \"w\")\n    fw_R = open(\"R.fasta\", \"w\")\n    fastq = fastqfile.endswith(\".fastq\")\n    it = iter_fastq(fastqfile) if fastq else \\\n        SeqIO.parse(must_open(fastqfile), \"fasta\")\n    for rec in it:\n        k += 1\n        if k % 1000 == 0:\n            pr.update(k)\n        if k > nreads:\n            break\n        if rec is None:\n            break\n        s = str(rec.seq)\n        for i, a in enumerate(s[:N]):\n            if a in p:\n                a = p[a]\n                L[a][i] += 1\n        for j, a in enumerate(s[-N:][::-1]):\n            if a in p:\n                a = p[a]\n                R[a][N - 1 - j] += 1\n        l4, r3 = s[:4], s[-3:]\n        L4[l4] += 1\n        R3[r3] += 1\n        print(\">{0}\\n{1}\".format(k, s[:N]), file=fw_L)\n        print(\">{0}\\n{1}\".format(k, s[-N:]), file=fw_R)\n\n    fw_L.close()\n    fw_R.close()\n\n    cmd = \"weblogo -F png -s large -f {0}.fasta -o {0}.png\"\n    cmd += \" --color-scheme classic --composition none -U probability\"\n    cmd += \" --title {1}\"\n    sh(cmd.format('L', \"First_10_bases\"))\n    sh(cmd.format('R', \"Last_10_bases\"))\n\n    np.savetxt(\"L.{0}.csv\".format(pat), L, delimiter=',', fmt=\"%d\")\n    np.savetxt(\"R.{0}.csv\".format(pat), R, delimiter=',', fmt=\"%d\")\n\n    fw = open(\"L4.common\", \"w\")\n    for p, c in L4.most_common(N):\n        print(\"\\t\".join((p, str(c))), file=fw)\n    fw.close()\n\n    fw = open(\"R3.common\", \"w\")\n    for p, c in R3.most_common(N):\n        print(\"\\t\".join((p, str(c))), file=fw)\n    fw.close()", "response": "%prog weblogo fastqfile > base composition. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef novo2(args):\n    p = OptionParser(novo2.__doc__)\n    p.set_fastq_names()\n    p.set_align(pctid=95)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    trimmed, pf = args\n    pctid = opts.pctid\n    reads, samples = scan_read_files(trimmed, opts.names)\n\n    # Set up directory structure\n    clustdir = \"uclust\"\n    acdir = \"allele_counts\"\n    for d in (clustdir, acdir):\n        mkdir(d)\n\n    mm = MakeManager()\n    clustfiles = []\n    # Step 0 - clustering within sample\n    for s in samples:\n        flist = [x for x in reads if op.basename(x).split(\".\")[0] == s]\n        outfile = s + \".P{0}.clustS\".format(pctid)\n        outfile = op.join(clustdir, outfile)\n        cmd = \"python -m jcvi.apps.uclust cluster --cpus=8\"\n        cmd += \" {0} {1}\".format(s, \" \".join(flist))\n        cmd += \" --outdir={0}\".format(clustdir)\n        cmd += \" --pctid={0}\".format(pctid)\n        mm.add(flist, outfile, cmd)\n        clustfiles.append(outfile)\n\n    # Step 1 - make consensus within sample\n    allcons = []\n    for s, clustfile in zip(samples, clustfiles):\n        outfile = s + \".P{0}.consensus\".format(pctid)\n        outfile = op.join(clustdir, outfile)\n        cmd = \"python -m jcvi.apps.uclust consensus\"\n        cmd += \" {0}\".format(clustfile)\n        mm.add(clustfile, outfile, cmd)\n        allcons.append(outfile)\n\n    # Step 2 - clustering across samples\n    clustSfile = pf + \".P{0}.clustS\".format(pctid)\n    cmd = \"python -m jcvi.apps.uclust mcluster {0}\".format(\" \".join(allcons))\n    cmd += \" --prefix={0}\".format(pf)\n    mm.add(allcons, clustSfile, cmd)\n\n    # Step 3 - make consensus across samples\n    locifile = pf + \".P{0}.loci\".format(pctid)\n    cmd = \"python -m jcvi.apps.uclust mconsensus {0}\".format(\" \".join(allcons))\n    cmd += \" --prefix={0}\".format(pf)\n    mm.add(allcons + [clustSfile], locifile, cmd)\n\n    mm.write()", "response": "%prog novo2 trimmed projectname read. fastq"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef filterm4(args):\n    p = OptionParser(filterm4.__doc__)\n    p.add_option(\"--best\", default=1, type=\"int\", help=\"Only retain best N hits\")\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    m4file, = args\n    best = opts.best\n    fp = open(m4file)\n    fw = must_open(opts.outfile, \"w\")\n    seen = defaultdict(int)\n    retained = total = 0\n    for row in fp:\n        r = M4Line(row)\n        total += 1\n        if total % 100000 == 0:\n            logging.debug(\"Retained {0} lines\".\\\n                            format(percentage(retained, total)))\n        if seen.get(r.query, 0) < best:\n            fw.write(row)\n            seen[r.query] += 1\n            retained += 1\n    fw.close()", "response": "%prog filterm4 sample. m4"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef spancount(args):\n    import json\n\n    p = OptionParser(spancount.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    fof, = args\n    fp = open(fof)\n    flist = [row.strip() for row in fp]\n    spanCount = \"spanCount\"\n    avgSpanBases = \"avgSpanBases\"\n    fw = open(spanCount, \"w\")\n    for f in flist:\n        fp = open(f)\n        j = json.load(fp)\n        sc = j.get(spanCount, None)\n        asb = j.get(avgSpanBases, None)\n        print(f, asb, sc, file=fw)\n        fw.flush()\n    fw.close()", "response": "%prog spancount list_of_fillMetrics can\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prepare(args):\n    p = OptionParser(prepare.__doc__ + FastqNamings)\n    p.add_option(\"-K\", default=51, type=\"int\", help=\"K-mer size\")\n    p.set_cpus(cpus=32)\n    opts, args = p.parse_args(args)\n\n    if len(args) < 2:\n        sys.exit(not p.print_help())\n\n    genomesize = float(args[0]) / 1000\n    fnames = args[1:]\n    for x in fnames:\n        assert op.exists(x), \"File `{0}` not found.\".format(x)\n\n    s = comment_banner(\"Meraculous params file\") + \"\\n\"\n    s += comment_banner(\"Basic parameters\") + \"\\n\"\n    s += \"# Describe the libraries ( one line per library )\\n\"\n    s += \"# \" + \" \".join(header.split()) + \"\\n\"\n\n    libs = get_libs(fnames)\n    lib_seqs = []\n    rank = 0\n    for lib, fs in libs:\n        size = lib.size\n        if size == 0:\n            continue\n        rank += 1\n        library_name = lib.library_name\n        name = library_name.replace(\"-\", \"\")\n        wildcard = \"{0}*.1.*,{0}*.2.*\".format(library_name)\n        rl = max(readlen([x]) for x in fs)\n        lib_seq = lib.get_lib_seq(wildcard, name, rl, rank)\n        lib_seqs.append(lib_seq)\n\n    s += \"\\n\" + \"\\n\".join(load_csv(None, lib_seqs, sep=\" \")) + \"\\n\"\n    params = [(\"genome_size\", genomesize),\n              (\"is_diploid\", 0),\n              (\"mer_size\", opts.K),\n              (\"num_prefix_blocks\", 1),\n              (\"no_read_validation\", 0),\n              (\"local_num_procs\", opts.cpus)]\n    s += \"\\n\" + \"\\n\".join(load_csv(None, params, sep=\" \")) + \"\\n\"\n\n    cfgfile = \"meraculous.config\"\n    write_file(cfgfile, s, tee=True)\n\n    s = \"~/export/meraculous/bin/run_meraculous.sh -c {0}\"\\\n                .format(cfgfile)\n    runsh = \"run.sh\"\n    write_file(runsh, s)", "response": "%prog prepare genomesize Mb. fastq"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets names of a list of taxids.", "response": "def get_names(list_of_taxids):\n    \"\"\"\n    >>> mylist = [3702, 3649, 3694, 3880]\n    >>> get_names(mylist)\n    ['Arabidopsis thaliana', 'Carica papaya', 'Populus trichocarpa', 'Medicago truncatula']\n    \"\"\"\n    from jcvi.apps.fetch import batch_taxonomy\n    list_of_taxids = [str(x) for x in list_of_taxids]\n    return list(batch_taxonomy(list_of_taxids))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of taxids for a list of names.", "response": "def get_taxids(list_of_names):\n    \"\"\"\n    >>> mylist = ['Arabidopsis thaliana', 'Carica papaya']\n    >>> get_taxids(mylist)\n    [1, 2]\n    \"\"\"\n    from jcvi.apps.fetch import batch_taxids\n    return [int(x) for x in batch_taxids(list_of_names)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef MRCA(list_of_taxids):\n\n    from ete2 import Tree\n\n    t = TaxIDTree(list_of_taxids)\n    t = Tree(str(t), format=8)\n\n    ancestor = t.get_common_ancestor(*t.get_leaves())\n\n    return ancestor.name", "response": "This gets the most recent common ancester for a list of taxids"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if the taxonomy is a plant origin", "response": "def isPlantOrigin(taxid):\n    \"\"\"\n    Given a taxid, this gets the expanded tree which can then be checked to\n    see if the organism is a plant or not\n\n    >>> isPlantOrigin(29760)\n    True\n    \"\"\"\n\n    assert isinstance(taxid, int)\n\n    t = TaxIDTree(taxid)\n    try:\n        return \"Viridiplantae\" in str(t)\n    except AttributeError:\n        raise ValueError(\"{0} is not a valid ID\".format(taxid))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef newick(args):\n    p = OptionParser(newick.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    idsfile, = args\n    mylist = [x.strip() for x in open(idsfile) if x.strip()]\n    print(get_taxids(mylist))\n\n    t = TaxIDTree(mylist)\n    print(t)", "response": "%prog newick idslist\n\n    Query a list of IDs to retrieve phylogeny."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_minibam_bed(bamfile, bedfile, minibam=None):\n    pf = op.basename(bedfile).split(\".\")[0]\n    minibamfile = minibam or op.basename(bamfile).replace(\".bam\", \".{}.bam\".format(pf))\n    minisamfile = minibam.replace(\".bam\", \".sam\")\n    baifile = minibamfile + \".bai\"\n    if op.exists(baifile):\n        sh(\"rm {}\".format(baifile))\n\n    cmd = \"samtools view -H {} > {}\".format(bamfile, minisamfile)\n    sh(cmd)\n\n    cmd = \"cat {}\".format(bedfile)\n    cmd += \" | perl -lane 'print \\\"$F[0]:$F[1]-$F[2]\\\"'\"\n    cmd += \" | xargs -n1 -t -I \\{\\}\"\n    cmd += \" samtools view {}\".format(bamfile)\n    cmd += \" \\{\\} >> \" + minisamfile\n    sh(cmd)\n\n    cmd = \"samtools view {} -b\".format(minisamfile)\n    cmd += \" | samtools sort -\"\n    cmd += \" -o {0}\".format(minibamfile)\n\n    sh(cmd)\n    sh(\"samtools index {0}\".format(minibamfile))\n    return minibamfile", "response": "Get the minibam file from the BAM file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef append(args):\n    p = OptionParser(append.__doc__)\n    p.add_option(\"--prepend\", help=\"Prepend string to read names\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    bamfile, = args\n    prepend = opts.prepend\n\n    icmd = \"samtools view -h {0}\".format(bamfile)\n    bamfile = bamfile.rsplit(\".\", 1)[0] + \".append.bam\"\n    ocmd = \"samtools view -b -@ 64 - -o {0}\".format(bamfile)\n    p = Popen(ocmd, stdin=PIPE)\n    for row in popen(icmd):\n        if row[0] == '@':\n            print(row.strip(), file=p.stdin)\n        else:\n            s = SamLine(row)\n            if prepend:\n                s.qname = prepend + \"_\" + s.qname\n            else:\n                s.update_readname()\n            print(s, file=p.stdin)", "response": "%prog append bamfile\n\n    Append /1 or /2 to read names. Useful for using the Tophat2 bam file for\n    training AUGUSTUS gene models."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef merge(args):\n    from jcvi.apps.grid import MakeManager\n\n    p = OptionParser(merge.__doc__)\n    p.set_sep(sep=\"_\", help=\"Separator to group per prefix\")\n    opts, args = p.parse_args(args)\n\n    if len(args) < 2:\n        sys.exit(not p.print_help())\n\n    merged_bams = args[0]\n    bamdirs = args[1:]\n\n    mkdir(merged_bams)\n    bams = []\n    for x in bamdirs:\n        bams += glob(op.join(x, \"*.bam\"))\n    bams = [x for x in bams if \"nsorted\" not in x]\n\n    logging.debug(\"Found a total of {0} BAM files.\".format(len(bams)))\n\n    sep = opts.sep\n    key = lambda x: op.basename(x).split(sep)[0]\n    bams.sort(key=key)\n    mm = MakeManager()\n    for prefix, files in groupby(bams, key=key):\n        files = sorted(list(files))\n        nfiles = len(files)\n        source = \" \".join(files)\n        target = op.join(merged_bams, op.basename(files[0]))\n        if nfiles == 1:\n            source = get_abs_path(source)\n            cmd = \"ln -s {0} {1}\".format(source, target)\n            mm.add(\"\", target, cmd)\n        else:\n            cmd = \"samtools merge -@ 8 {0} {1}\".format(target, source)\n            mm.add(files, target, cmd, remove=True)\n    mm.write()", "response": "%prog merge merged_bams bams1_dir bams2_dir..."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef count(args):\n    p = OptionParser(count.__doc__)\n    p.add_option(\"--type\", default=\"exon\",\n                 help=\"Only count feature type\")\n    p.set_cpus(cpus=8)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    bamfile, gtf = args\n    cpus = opts.cpus\n    pf = bamfile.split(\".\")[0]\n    countfile = pf + \".count\"\n    if not need_update(bamfile, countfile):\n        return\n\n    nsorted = pf + \"_nsorted\"\n    nsortedbam, nsortedsam = nsorted + \".bam\", nsorted + \".sam\"\n    if need_update(bamfile, nsortedsam):\n        cmd = \"samtools sort -@ {0} -n {1} {2}\".format(cpus, bamfile, nsorted)\n        sh(cmd)\n        cmd = \"samtools view -@ {0} -h {1}\".format(cpus, nsortedbam)\n        sh(cmd, outfile=nsortedsam)\n\n    if need_update(nsortedsam, countfile):\n        cmd = \"htseq-count --stranded=no --minaqual=10\"\n        cmd += \" -t {0}\".format(opts.type)\n        cmd += \" {0} {1}\".format(nsortedsam, gtf)\n        sh(cmd, outfile=countfile)", "response": "%prog count bamfile gtf"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fpkm(args):\n    p = OptionParser(fpkm.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) < 2:\n        sys.exit(not p.print_help())\n\n    fastafile = args[0]\n    bamfiles = args[1:]\n    # Create a DUMMY gff file for cuffdiff\n    gffile = fastafile.rsplit(\".\", 1)[0] + \".gff\"\n    if need_update(fastafile, gffile):\n        fw = open(gffile, \"w\")\n        f = Fasta(fastafile, lazy=True)\n        for key, size in f.itersizes_ordered():\n            print(\"\\t\".join(str(x) for x in (key, \"dummy\", \"transcript\",\\\n                1, size, \".\", \".\", \".\", \"ID=\" + key)), file=fw)\n        fw.close()\n        logging.debug(\"Dummy GFF created: {0}\".format(gffile))\n\n    cmd = \"cuffdiff {0} {1}\".format(gffile, \" \".join(bamfiles))\n    sh(cmd)", "response": "%prog fpkm fastafile bamfiles"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pairs(args):\n    import jcvi.formats.bed\n\n    p = OptionParser(pairs.__doc__)\n    p.set_pairs()\n    opts, targs = p.parse_args(args)\n\n    if len(targs) != 1:\n        sys.exit(not p.print_help())\n\n    samfile, = targs\n    bedfile = samfile.rsplit(\".\", 1)[0] + \".bed\"\n    if need_update(samfile, bedfile):\n        cmd = \"bamToBed -i {0}\".format(samfile)\n        sh(cmd, outfile=bedfile)\n\n    args[args.index(samfile)] = bedfile\n\n    return jcvi.formats.bed.pairs(args)", "response": "See __doc__ for OptionParser.set_pairs()."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef consensus(args):\n    p = OptionParser(consensus.__doc__)\n    p.add_option(\"--fasta\", default=False, action=\"store_true\",\n            help=\"Generate consensus FASTA sequences [default: %default]\")\n    p.add_option(\"--mask\", default=0, type=\"int\",\n            help=\"Mask bases with quality lower than\")\n    opts, args = p.parse_args(args)\n\n    if len(args) < 2:\n        sys.exit(not p.print_help())\n\n    fastafile, bamfile = args\n    fasta = opts.fasta\n    suffix = \"fasta\" if fasta else \"fastq\"\n    pf = bamfile.rsplit(\".\", 1)[0]\n    cnsfile = pf + \".cns.{0}\".format(suffix)\n    vcfgzfile = pf + \".vcf.gz\"\n    vcf([fastafile, bamfile, \"-o\", vcfgzfile])\n    cmd += \"zcat {0} | vcfutils.pl vcf2fq\".format(vcfgzfile)\n    if fasta:\n        cmd += \" | seqtk seq -q {0} -A -\".format(opts.mask)\n\n    sh(cmd, outfile=cnsfile)", "response": "%prog consensus fastafile bamfile"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef index(args):\n    p = OptionParser(index.__doc__)\n    p.add_option(\"--fasta\", dest=\"fasta\", default=None,\n            help=\"add @SQ header to the BAM file [default: %default]\")\n    p.add_option(\"--unique\", default=False, action=\"store_true\",\n            help=\"only retain uniquely mapped reads [default: %default]\")\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(p.print_help())\n\n    samfile, = args\n    cpus = opts.cpus\n    fastafile = opts.fasta\n    if fastafile:\n        assert op.exists(fastafile)\n\n    bamfile = samfile.replace(\".sam\", \".bam\")\n    if fastafile:\n        faifile = fastafile + \".fai\"\n        if need_update(fastafile, faifile):\n            sh(\"samtools faidx {0}\".format(fastafile))\n        cmd = \"samtools view -bt {0} {1} -o {2}\".\\\n                format(faifile, samfile, bamfile)\n    else:\n        cmd = \"samtools view -bS {0} -o {1}\".\\\n                format(samfile, bamfile)\n\n    cmd += \" -@ {0}\".format(cpus)\n    if opts.unique:\n        cmd += \" -q 1\"\n\n    if samfile.endswith(\".sam\") and need_update(samfile, bamfile):\n        sh(cmd)\n\n    # Already sorted?\n    if bamfile.endswith(\".sorted.bam\"):\n        sortedbamfile = bamfile\n    else:\n        prefix = bamfile.replace(\".bam\", \"\")\n        sortedbamfile = prefix + \".sorted.bam\"\n\n    if need_update(bamfile, sortedbamfile):\n        cmd = \"samtools sort {0} -o {1}\".format(bamfile, sortedbamfile)\n        cmd += \" -@ {0}\".format(cpus)\n        sh(cmd)\n\n    baifile = sortedbamfile + \".bai\"\n    if need_update(sortedbamfile, baifile):\n        sh(\"samtools index {0}\".format(sortedbamfile))\n\n    return sortedbamfile", "response": "%prog index samfile/bamfile\n\n    If SAM file, convert to BAM, sort and then index, using SAMTOOLS"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a pysam row to a sequence based on the cigar string.", "response": "def cigar_to_seq(a, gap='*'):\n    \"\"\"\n    Accepts a pysam row.\n\n    cigar alignment is presented as a list of tuples (operation,length). For\n    example, the tuple [ (0,3), (1,5), (0,2) ] refers to an alignment with 3\n    matches, 5 insertions and another 2 matches.\n\n    Op BAM Description\n    M 0 alignment match (can be a sequence match or mismatch)\n    I 1 insertion to the reference\n    D 2 deletion from the reference\n    N 3 skipped region from the reference\n    S 4 soft clipping (clipped sequences present in SEQ)\n    H 5 hard clipping (clipped sequences NOT present in SEQ)\n    P 6 padding (silent deletion from padded reference)\n    = 7 sequence match\n    X 8 sequence mismatch\n\n    convert the sequence based on the cigar string. For example:\n    \"\"\"\n    seq, cigar = a.seq, a.cigar\n    start = 0\n    subseqs = []\n    npadded = 0\n    if cigar is None:\n        return None, npadded\n\n    for operation, length in cigar:\n        end = start if operation == 2 else start + length\n\n        if operation == 0:  # match\n            subseq = seq[start:end]\n        elif operation == 1:  # insertion\n            subseq = \"\"\n        elif operation == 2:  # deletion\n            subseq = gap * length\n            npadded += length\n        elif operation == 3:  # skipped\n            subseq = 'N' * length\n        elif operation in (4, 5):  # clip\n            subseq = \"\"\n        else:\n            raise NotImplementedError\n\n        subseqs.append(subseq)\n        start = end\n\n    return \"\".join(subseqs), npadded"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ace(args):\n    p = OptionParser(ace.__doc__)\n    p.add_option(\"--splitdir\", dest=\"splitdir\", default=\"outRoot\",\n            help=\"split the ace per contig to dir [default: %default]\")\n    p.add_option(\"--unpaired\", dest=\"unpaired\", default=False,\n            help=\"remove read pairs on the same contig [default: %default]\")\n    p.add_option(\"--minreadno\", dest=\"minreadno\", default=3, type=\"int\",\n            help=\"minimum read numbers per contig [default: %default]\")\n    p.add_option(\"--minctgsize\", dest=\"minctgsize\", default=100, type=\"int\",\n            help=\"minimum contig size per contig [default: %default]\")\n    p.add_option(\"--astat\", default=False, action=\"store_true\",\n            help=\"create .astat to list repetitiveness [default: %default]\")\n    p.add_option(\"--readids\", default=False, action=\"store_true\",\n            help=\"create file of mapped and unmapped ids [default: %default]\")\n\n    from pysam import Samfile\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    bamfile, fastafile = args\n    astat = opts.astat\n    readids = opts.readids\n\n    f = Fasta(fastafile)\n    prefix = bamfile.split(\".\")[0]\n    acefile = prefix + \".ace\"\n    readsfile = prefix + \".reads\"\n    astatfile = prefix + \".astat\"\n\n    logging.debug(\"Load {0}\".format(bamfile))\n    s = Samfile(bamfile, \"rb\")\n\n    ncontigs = s.nreferences\n    genomesize = sum(x for a, x in f.itersizes())\n    logging.debug(\"Total {0} contigs with size {1} base\".format(ncontigs,\n        genomesize))\n    qual = \"20\"  # default qual\n\n    totalreads = sum(s.count(x) for x in s.references)\n    logging.debug(\"Total {0} reads mapped\".format(totalreads))\n\n    fw = open(acefile, \"w\")\n    if astat:\n        astatfw = open(astatfile, \"w\")\n    if readids:\n        readsfw = open(readsfile, \"w\")\n\n    print(\"AS {0} {1}\".format(ncontigs, totalreads), file=fw)\n    print(file=fw)\n\n    for i, contig in enumerate(s.references):\n        cseq = f[contig]\n        nbases = len(cseq)\n\n        mapped_reads = [x for x in s.fetch(contig) if not x.is_unmapped]\n        nreads = len(mapped_reads)\n\n        nsegments = 0\n        print(\"CO {0} {1} {2} {3} U\".format(contig, nbases, nreads,\n                nsegments), file=fw)\n        print(fill(str(cseq.seq)), file=fw)\n        print(file=fw)\n\n        if astat:\n            astat = Astat(nbases, nreads, genomesize, totalreads)\n            print(\"{0}\\t{1:.1f}\".format(contig, astat), file=astatfw)\n\n        text = fill([qual] * nbases, delimiter=\" \", width=30)\n        print(\"BQ\\n{0}\".format(text), file=fw)\n        print(file=fw)\n\n        rnames = []\n        for a in mapped_reads:\n            readname = a.qname\n            rname = readname\n\n            if readids:\n                print(readname, file=readsfw)\n            rnames.append(rname)\n\n            strand = \"C\" if a.is_reverse else \"U\"\n            paddedstart = a.pos + 1  # 0-based to 1-based\n            af = \"AF {0} {1} {2}\".format(rname, strand, paddedstart)\n            print(af, file=fw)\n\n        print(file=fw)\n\n        for a, rname in zip(mapped_reads, rnames):\n            aseq, npadded = cigar_to_seq(a)\n            if aseq is None:\n                continue\n\n            ninfos = 0\n            ntags = 0\n            alen = len(aseq)\n            rd = \"RD {0} {1} {2} {3}\\n{4}\".format(rname, alen, ninfos, ntags,\n                    fill(aseq))\n            qs = \"QA 1 {0} 1 {0}\".format(alen)\n\n            print(rd, file=fw)\n            print(file=fw)\n            print(qs, file=fw)\n            print(file=fw)", "response": "%prog ace bamfile fastafile"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fixpairs(args):\n    p = OptionParser(fixpairs.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    pairsfile, sep, sd = args\n    newpairsfile = pairsfile.rsplit(\".\", 1)[0] + \".new.pairs\"\n    sep = int(sep)\n    sd = int(sd)\n\n    p = PairsFile(pairsfile)\n    p.fixLibraryStats(sep, sd)\n    p.write(newpairsfile)", "response": "%prog fixpairs pairsfile sep sd\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract adjacent pairs from a fastq file.", "response": "def extract_pairs(fastqfile, p1fw, p2fw, fragsfw, p, suffix=False):\n    \"\"\"\n    Take fastqfile and array of pair ID, extract adjacent pairs to outfile.\n    Perform check on numbers when done. p1fw, p2fw is a list of file handles,\n    each for one end. p is a Pairs instance.\n    \"\"\"\n    fp = open(fastqfile)\n    currentID = 0\n    npairs = nfrags = 0\n    for x, lib in izip(p.r1, p.libs):\n        while currentID != x:\n            fragsfw.writelines(islice(fp, 4))  # Exhaust the iterator\n            currentID += 1\n            nfrags += 1\n        a = list(islice(fp, 4))\n        b = list(islice(fp, 4))\n        if suffix:\n            name = a[0].rstrip()\n            a[0] = name + \"/1\\n\"\n            b[0] = name + \"/2\\n\"\n        else:\n            b[0] = a[0]  # Keep same read ID for pairs\n\n        p1fw[lib].writelines(a)\n        p2fw[lib].writelines(b)\n        currentID += 2\n        npairs += 2\n\n    # Write the remaining single reads\n    while True:\n        contents = list(islice(fp, 4))\n        if not contents:\n            break\n        fragsfw.writelines(contents)\n        nfrags += 1\n\n    logging.debug(\"A total of {0} paired reads written to `{1}`.\".\\\n                  format(npairs, \",\".join(x.name for x in p1fw + p2fw)))\n    logging.debug(\"A total of {0} single reads written to `{1}`.\".\\\n                  format(nfrags, fragsfw.name))\n\n    # Validate the numbers\n    expected_pairs = 2 * p.npairs\n    expected_frags = p.nreads - 2 * p.npairs\n    assert npairs == expected_pairs, \"Expect {0} paired reads, got {1} instead\".\\\n              format(expected_pairs, npairs)\n    assert nfrags == expected_frags, \"Expect {0} single reads, got {1} instead\".\\\n              format(expected_frags, nfrags)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pairs(args):\n    from jcvi.assembly.preprocess import run_FastbAndQualb2Fastq\n\n    p = OptionParser(pairs.__doc__)\n    p.add_option(\"--header\", default=False, action=\"store_true\",\n            help=\"Print header only [default: %default]\")\n    p.add_option(\"--suffix\", default=False, action=\"store_true\",\n                 help=\"Add suffix /1, /2 to read names\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    pairsfile, fastqfile = args\n    pf = op.basename(fastqfile).split(\".\")[0]\n    p = PairsFile(pairsfile)\n    print(p.header, file=sys.stderr)\n\n    if opts.header:\n        return\n\n    if fastqfile.endswith(\".fastb\"):\n        fastbfile = fastqfile\n        fastqfile = fastbfile.replace(\".fastb\", \".fastq\")\n        run_FastbAndQualb2Fastq(infile=fastbfile, outfile=fastqfile)\n\n    p1file = \"{0}.1.corr.fastq\"\n    p2file = \"{0}.2.corr.fastq\"\n    fragsfile = \"{0}.corr.fastq\"\n    p1fw = [open(p1file.format(x), \"w\") for x in p.libnames]\n    p2fw = [open(p2file.format(x), \"w\") for x in p.libnames]\n    fragsfw = open(fragsfile.format(pf), \"w\")\n\n    extract_pairs(fastqfile, p1fw, p2fw, fragsfw, p, suffix=opts.suffix)", "response": "%prog pairs pairsfile fastqfile"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prepare(args):\n    from jcvi.utils.table import write_csv\n    from jcvi.formats.base import write_file\n    from jcvi.formats.fastq import guessoffset, readlen\n\n    p = OptionParser(prepare.__doc__ + FastqNamings)\n    p.add_option(\"--corr\", default=False, action=\"store_true\",\n                 help=\"Extra parameters for corrected data [default: %default]\")\n    p.add_option(\"--norun\", default=False, action=\"store_true\",\n                 help=\"Don't write `run.sh` script [default: %default]\")\n    p.add_option(\"--ploidy\", default=\"2\", choices=(\"1\", \"2\"),\n                 help=\"Ploidy [default: %default]\")\n    p.set_cpus()\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    organism_name = args[0]\n    project_name = \"\".join(x[0] for x in organism_name.split()).upper()\n    fnames = sorted(glob(\"*.fastq*\") if len(args) == 1 else args[1:])\n    for x in fnames:\n        assert op.exists(x), \"File `{0}` not found.\".format(x)\n\n    groupheader = \"group_name library_name file_name\".split()\n    libheader = \"library_name project_name organism_name type paired \"\\\n        \"frag_size frag_stddev insert_size insert_stddev read_orientation \"\\\n        \"genomic_start genomic_end\".split()\n    groups_33 = []\n    groups_64 = []\n    libs = []\n    for file_name in fnames:\n        offset = guessoffset([file_name])\n        group_name = op.basename(file_name).split(\".\")[0]\n        library_name = \"-\".join(group_name.split(\"-\")[:2])\n\n        # Handle paired files and convert to wildcard\n        if \".1.\" in file_name:\n            file_name = file_name.replace(\".1.\", \".?.\")\n        elif \".2.\" in file_name:\n            continue\n\n        groupscontents = groups_64 if offset == 64 else groups_33\n        groupscontents.append((group_name, library_name, file_name))\n        if library_name not in libs:\n            libs.append(library_name)\n\n    libcontents = []\n    for library_name in libs:\n        L = Library(library_name)\n        size = L.size\n        stddev = L.stddev\n        type = L.type\n        paired = L.paired\n        read_orientation = L.read_orientation\n\n        size = size or \"\"\n        stddev = stddev or \"\"\n        frag_size = size if type == \"fragment\" else \"\"\n        frag_stddev = stddev if type == \"fragment\" else \"\"\n        insert_size = size if type != \"fragment\" else \"\"\n        insert_stddev = stddev if type != \"fragment\" else \"\"\n        genomic_start, genomic_end = \"\", \"\"\n        libcontents.append((library_name, project_name, organism_name, type, \\\n            paired, frag_size, frag_stddev, insert_size, insert_stddev, \\\n            read_orientation, genomic_start, genomic_end))\n\n    for groups, csvfile in ((groups_33, \"in_groups_33.csv\"), \\\n                            (groups_64, \"in_groups_64.csv\"), \\\n                            (groups_33 + groups_64, \"in_groups.csv\")):\n        if not groups:\n            continue\n        write_csv(groupheader, groups, filename=csvfile, tee=True)\n        logging.debug(\"`{0}` created (# of groups = {1}).\".\\\n            format(csvfile, len(groups)))\n\n    write_csv(libheader, libcontents, filename=\"in_libs.csv\", tee=True)\n    logging.debug(\"`in_libs.csv` created (# of libs = {0}).\".\\\n        format(len(libcontents)))\n\n    runfile = \"run.sh\"\n\n    # ALLPATHS stalls on reads over 250bp <https://www.biostars.org/p/122091/>\n    max_rd_len = max(readlen([f]) for f in fnames)\n    extra = \"CLOSE_UNIPATH_GAPS=False \" if max_rd_len > 200 else \"\"\n    if opts.corr:\n        extra += \"FE_NUM_CYCLES=1 EC_K=28 FE_QUAL_CEIL_RADIUS=0\"\n        extra += \" REMOVE_DODGY_READS_FRAG=False FE_MAX_KMER_FREQ_TO_MARK=1\"\n\n    if not opts.norun:\n        contents = ALLPATHSRUN.format(opts.ploidy, opts.cpus, extra)\n        write_file(runfile, contents)", "response": "%prog prepare organism_name project_name"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\njoin given arguments into the same set. Accepts one or more arguments.", "response": "def join(self, a, *args):\n        \"\"\"\n        Join given arguments into the same set. Accepts one or more arguments.\n        \"\"\"\n        mapping = self._mapping\n        set_a = mapping.setdefault(a, [a])\n\n        for arg in args:\n            set_b = mapping.get(arg)\n            if set_b is None:\n                set_a.append(arg)\n                mapping[arg] = set_a\n            elif set_b is not set_a:\n                if len(set_b) > len(set_a):\n                    set_a, set_b = set_b, set_a\n                set_a.extend(set_b)\n                for elem in set_b:\n                    mapping[elem] = set_a"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if a and b are members of the same set.", "response": "def joined(self, a, b):\n        \"\"\"\n        Returns True if a and b are members of the same set.\n        \"\"\"\n        mapping = self._mapping\n        try:\n            return mapping[a] is mapping[b]\n        except KeyError:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fromcsv(args):\n    from csv import reader\n    from xlwt import Workbook, easyxf\n    from jcvi.formats.base import flexible_cast\n\n    p = OptionParser(fromcsv.__doc__)\n    p.add_option(\"--noheader\", default=False, action=\"store_true\",\n                 help=\"Do not treat the first row as header\")\n    p.add_option(\"--rgb\", default=-1, type=\"int\",\n                 help=\"Show RGB color box\")\n    p.set_sep()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    csvfile, = args\n    header = not opts.noheader\n    rgb = opts.rgb\n    excelfile = csvfile.rsplit(\".\", 1)[0] + \".xls\"\n\n    data = []\n    for row in reader(open(csvfile), delimiter=opts.sep):\n        data.append(row)\n\n    w = Workbook()\n    s = w.add_sheet(op.basename(csvfile))\n\n    header_style = easyxf('font: bold on')\n    if header:\n        s.panes_frozen = True\n        s.horz_split_pos = 1\n\n    cm = ColorMatcher()\n    for i, row in enumerate(data):\n        for j, cell in enumerate(row):\n            cell = flexible_cast(cell)\n            if header and i == 0:\n                s.write(i, j, cell, header_style)\n            else:\n                if j == rgb:\n                    cix = cm.match_color_index(cell)\n                    color_style = easyxf('font: color_index {0}'.format(cix))\n                    s.write(i, j, cell, color_style)\n                else:\n                    s.write(i, j, cell)\n\n    w.save(excelfile)\n    logging.debug(\"File written to `{0}`.\".format(excelfile))\n    return excelfile", "response": "%prog fromcsv csvfile\n\n    Convert csv file to EXCEL."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef csv(args):\n    from xlrd import open_workbook\n\n    p = OptionParser(csv.__doc__)\n    p.set_sep(sep=',')\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    excelfile, = args\n    sep = opts.sep\n    csvfile = excelfile.rsplit(\".\", 1)[0] + \".csv\"\n    wb = open_workbook(excelfile)\n    fw = open(csvfile, \"w\")\n    for s in wb.sheets():\n        print('Sheet:',s.name, file=sys.stderr)\n        for row in range(s.nrows):\n            values = []\n            for col in range(s.ncols):\n                values.append(s.cell(row, col).value)\n            print(sep.join(str(x) for x in values), file=fw)", "response": "%prog csv excelfile\n\n    Convert EXCEL to csv file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef match_color_index(self, color):\n        from jcvi.utils.webcolors import color_diff\n        if isinstance(color, int):\n            return color\n        if color:\n            if isinstance(color, six.string_types):\n                rgb = map(int, color.split(','))\n            else:\n                rgb = color.Get()\n            logging.disable(logging.DEBUG)\n            distances = [color_diff(rgb, x) for x in self.xlwt_colors]\n            logging.disable(logging.NOTSET)\n            result = distances.index(min(distances))\n            self.unused_colors.discard(self.xlwt_colors[result])\n            return result", "response": "Takes an R G B string or wx. Color and returns a matching xlwt\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_unused_color(self):\n        if not self.unused_colors:\n            # If we somehow run out of colors, reset the color matcher.\n            self.reset()\n        used_colors = [c for c in self.xlwt_colors if c not in self.unused_colors]\n        result_color = max(self.unused_colors,\n                           key=lambda c: min(self.color_distance(c, c2)\n                                             for c2 in used_colors))\n        result_index = self.xlwt_colors.index(result_color)\n        self.unused_colors.discard(result_color)\n        return result_index", "response": "Returns an xlwt color index that has not been previously returned by\n       ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate(args):\n    import pyfasta\n\n    p = OptionParser(validate.__doc__)\n    p.add_option(\"--prefix\", help=\"Add prefix to seqid\")\n    opts, args = p.parse_args(args)\n\n    vcffile, fastafile = args\n    pf = opts.prefix\n    genome = pyfasta.Fasta(fastafile, record_class=pyfasta.MemoryRecord)\n    fp = must_open(vcffile)\n    match_ref = match_alt = total = 0\n    for row in fp:\n        if row[0] == '#':\n            continue\n        seqid, pos, id, ref, alt = row.split()[:5]\n        total += 1\n        if pf:\n            seqid = pf + seqid\n        pos = int(pos)\n        if seqid not in genome:\n            continue\n        true_ref = genome[seqid][pos - 1]\n        if total % 100000 == 0:\n            print(total, \"sites parsed\", file=sys.stderr)\n        if ref == true_ref:\n            match_ref += 1\n        elif alt == true_ref:\n            match_alt += 1\n\n    logging.debug(\"Match REF: {}\".format(percentage(match_ref, total)))\n    logging.debug(\"Match ALT: {}\".format(percentage(match_alt, total)))", "response": "%prog validate input. vcf genome. fasta\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sample(args):\n    from random import random\n\n    p = OptionParser(sample.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    vcffile, ratio = args\n    ratio = float(ratio)\n    fp = open(vcffile)\n    pf = vcffile.rsplit(\".\", 1)[0]\n    kept = pf + \".kept.vcf\"\n    withheld = pf + \".withheld.vcf\"\n    fwk = open(kept, \"w\")\n    fww = open(withheld, \"w\")\n    nkept = nwithheld = 0\n    for row in fp:\n        if row[0] == '#':\n            print(row.strip(), file=fwk)\n            continue\n        if random() < ratio:\n            nkept += 1\n            print(row.strip(), file=fwk)\n        else:\n            nwithheld += 1\n            print(row.strip(), file=fww)\n    logging.debug(\"{0} records kept to `{1}`\".format(nkept, kept))\n    logging.debug(\"{0} records withheld to `{1}`\".format(nwithheld, withheld))", "response": "%prog sample vcffile ratio"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fromimpute2(args):\n    p = OptionParser(fromimpute2.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    impute2file, fastafile, chr = args\n    fasta = Fasta(fastafile)\n    print(get_vcfstanza(fastafile, fasta))\n    fp = open(impute2file)\n    seen = set()\n    for row in fp:\n        snp_id, rsid, pos, ref, alt, aa, ab, bb = row.split()\n        pos = int(pos)\n        if pos in seen:\n            continue\n        seen.add(pos)\n        code = max((float(aa), \"0/0\"), (float(ab), \"0/1\"), (float(bb), \"1/1\"))[-1]\n        tag = \"PR\" if snp_id == chr else \"IM\"\n        print(\"\\t\".join(str(x) for x in \\\n                (chr, pos, rsid, ref, alt, \".\", \".\", tag, \\\n                \"GT:GP\", code + \":\" + \",\".join((aa, ab, bb)))))", "response": "%prog fromimpute2 impute2file fastafile 1\n\n    Convert impute2 output to vcf file. Imputed file looks like:\n\n    --- 1:10177:A:AC 10177 A AC 0.451 0.547 0.002"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from23andme(args):\n    p = OptionParser(from23andme.__doc__)\n    p.set_ref()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    txtfile, seqid = args\n    ref_dir = opts.ref\n    fastafile = op.join(ref_dir, \"hs37d5.fa\")\n    fasta = Fasta(fastafile)\n\n    pf = txtfile.rsplit(\".\", 1)[0]\n    px = CM[seqid]\n    chrvcf = pf + \".{0}.vcf\".format(px)\n    legend = op.join(ref_dir, \"1000GP_Phase3/{0}.rsids\".format(px))\n    register = read_rsid(seqid, legend)\n\n    fw = open(chrvcf, \"w\")\n    print(get_vcfstanza(fastafile, fasta, txtfile), file=fw)\n\n    fp = open(txtfile)\n    seen = set()\n    duplicates = skipped = missing = 0\n    for row in fp:\n        if row[0] == '#':\n            continue\n        rsid, chr, pos, genotype = row.split()\n        if chr != seqid:\n            continue\n        pos = int(pos)\n        if (chr, pos) in seen:\n            duplicates += 1\n            continue\n        seen.add((chr, pos))\n        genotype = list(genotype)\n        if \"-\" in genotype:  # missing daa\n            missing += 1\n            continue\n\n        # Y or MT\n        if not register:\n            assert len(genotype) == 1\n            ref = fasta[chr][pos - 1].seq.upper()\n            if \"D\" in genotype or \"I\" in genotype:\n                skipped += 1\n                continue\n            genotype = genotype[0]\n            code = \"0/0\" if ref == genotype else \"1/1\"\n            alt = \".\" if ref == genotype else genotype\n            print(\"\\t\".join(str(x) for x in \\\n                    (chr, pos, rsid, ref, alt, \".\", \".\", \"PR\", \"GT\", code)), file=fw)\n            continue\n\n        # If rsid is seen in the db, use that\n        if rsid in register:\n            pos, ref, alt = register[rsid]\n        elif pos in register:\n            pos, ref, alt = register[pos]\n        else:\n            skipped += 1  # Not in reference panel\n            continue\n\n        assert fasta[chr][pos - 1:pos + len(ref) - 1].seq.upper() == ref\n        # Keep it bi-allelic\n        not_seen = [x for x in alt if x not in genotype]\n        while len(alt) > 1 and not_seen:\n            alt.remove(not_seen.pop())\n        if len(alt) > 1:\n            alt = [alt[0]]\n        alleles = [ref] + alt\n\n        if len(genotype) == 1:\n            genotype = [genotype[0]] * 2\n\n        alt = \",\".join(alt) or \".\"\n        if \"D\" in genotype or \"I\" in genotype:\n            max_allele = max((len(x), x) for x in alleles)[1]\n            alleles = [(\"I\" if x == max_allele else \"D\") for x in alleles]\n            assert \"I\" in alleles and \"D\" in alleles\n        a, b = genotype\n        try:\n            ia, ib = alleles.index(a), alleles.index(b)\n        except ValueError:  # alleles not seen\n            logging.error(\"{0}: alleles={1}, genotype={2}\".\\\n                            format(rsid, alleles, genotype))\n            skipped += 1\n            continue\n        code = \"/\".join(str(x) for x in sorted((ia, ib)))\n\n        print(\"\\t\".join(str(x) for x in \\\n                (chr, pos, rsid, ref, alt, \".\", \".\", \"PR\", \"GT\", code)), file=fw)\n\n    logging.debug(\"duplicates={0} skipped={1} missing={2}\".\\\n                    format(duplicates, skipped, missing))", "response": "%prog from23andme txtfile 1\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nencode a genotype string into a base - 2 string.", "response": "def encode_genotype(s, mindepth=3, depth_index=2, nohet=False):\n    \"\"\"\n    >>> encode_genotype(\"1/1:128,18,0:6:18\")  # homozygote B\n    'B'\n    >>> encode_genotype(\"0/1:0,0,0:0:3\")      # missing data\n    '-'\n    >>> encode_genotype(\"0/1:128,0,26:7:22\")  # heterozygous A/B\n    'X'\n    \"\"\"\n    atoms = s.split(\":\")\n    if len(atoms) < 3:\n        return g2x[atoms[0]]\n\n    inferred = atoms[0]\n    depth = int(atoms[depth_index])\n    if depth < mindepth:\n        return '-'\n    if inferred == '0/0':\n        return 'A'\n    if inferred == '0/1':\n        return '-' if nohet else 'X'\n    if inferred == '1/1':\n        return 'B'\n    return '-'"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving chromosome, position in 1-based co-ordinates, This will use pyliftover to liftover a CPRA, will return a (c,p) tuple or raise NonUniqueLiftover if no unique and strand maintaining liftover is possible :param chromosome: string with the chromosome as it's represented in the from_genome :param position: position on chromosome (will be cast to int) :return: ((str) chromosome, (int) position) or None if no liftover", "response": "def liftover_cpra(self, chromosome, position, verbose=False):\n        \"\"\"\n        Given chromosome, position in 1-based co-ordinates,\n        This will use pyliftover to liftover a CPRA, will return a (c,p) tuple or raise NonUniqueLiftover if no unique\n        and strand maintaining liftover is possible\n        :param chromosome: string with the chromosome as it's represented in the from_genome\n        :param position: position on chromosome (will be cast to int)\n        :return: ((str) chromosome, (int) position) or None if no liftover\n        \"\"\"\n\n        chromosome = str(chromosome)\n        position = int(position)\n\n        # Perform the liftover lookup, shift the position by 1 as pyliftover deals in 0-based co-ords\n        new = self.liftover.convert_coordinate(chromosome, position - 1)\n        # This has to be here as new will be NoneType when the chromosome doesn't exist in the chainfile\n        if new:\n            # If the liftover is unique\n            if len(new) == 1:\n                # If the liftover hasn't changed strand\n                if new[0][2] == \"+\":\n                    # Set the co-ordinates to the lifted-over ones and write out\n                    new_chromosome = str(new[0][0])\n                    # Shift the position forward by one to convert back to a 1-based co-ords\n                    new_position = int(new[0][1]) + 1\n                    return new_chromosome, new_position\n                else:\n                    exception_string = \"{},{} has a flipped strand in liftover: {}\".format(chromosome, position, new)\n            else:\n                exception_string = \"{},{} lifts over to multiple positions: {}\".format(chromosome, position, new)\n        elif new is None:\n            exception_string = \"Chromosome '{}' provided not in chain file\".format(chromosome)\n\n        if verbose:\n            logging.error(exception_string)\n        return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef composite(args):\n    from jcvi.graphics.chromosome import HorizontalChromosome\n\n    p = OptionParser(composite.__doc__)\n    p.add_option(\"--lines\",\n                 help=\"Features to plot in lineplot [default: %default]\")\n    p.add_option(\"--bars\",\n                 help=\"Features to plot in bars [default: %default]\")\n    p.add_option(\"--altbars\",\n                 help=\"Features to plot in alt-bars [default: %default]\")\n    p.add_option(\"--fatten\", default=False, action=\"store_true\",\n                 help=\"Help visualize certain narrow features [default: %default]\")\n    p.add_option(\"--mode\", default=\"span\", choices=(\"span\", \"count\", \"score\"),\n                 help=\"Accumulate feature based on [default: %default]\")\n    add_window_options(p)\n    opts, args, iopts = p.set_image_options(args, figsize=\"8x5\")\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    fastafile, chr = args\n    window, shift, subtract, merge = check_window_options(opts)\n    linebeds, barbeds, altbarbeds = [], [], []\n    fatten = opts.fatten\n    if opts.lines:\n        lines = opts.lines.split(\",\")\n        linebeds = get_beds(lines)\n    if opts.bars:\n        bars = opts.bars.split(\",\")\n        barbeds = get_beds(bars)\n    if opts.altbars:\n        altbars = opts.altbars.split(\",\")\n        altbarbeds = get_beds(altbars)\n\n    linebins = get_binfiles(linebeds, fastafile, shift,\n                            mode=opts.mode, merge=merge)\n\n    margin = .12\n    clen = Sizes(fastafile).mapping[chr]\n    nbins = get_nbins(clen, shift)\n\n    plt.rcParams[\"xtick.major.size\"] = 0\n    plt.rcParams[\"ytick.major.size\"] = 0\n\n    fig = plt.figure(1, (iopts.w, iopts.h))\n    root = fig.add_axes([0, 0, 1, 1])\n\n    root.text(.5, .95, chr, ha=\"center\", color=\"darkslategray\")\n\n    xstart, xend = margin, 1 - margin\n    xlen = xend - xstart\n    ratio = xlen / clen\n    # Line plots\n    ax = fig.add_axes([xstart, .6, xlen, .3])\n    lineplot(ax, linebins, nbins, chr, window, shift)\n\n    # Bar plots\n    yy = .5\n    yinterval = .08\n    xs = lambda x: xstart + ratio * x\n    r = .01\n    fattend = .0025\n    for bb in barbeds:\n        root.text(xend + .01, yy, bb.split(\".\")[0], va=\"center\")\n        HorizontalChromosome(root, xstart, xend, yy, height=.02)\n        bb = Bed(bb)\n        for b in bb:\n            start, end = xs(b.start), xs(b.end)\n            span = end - start\n            if fatten and span < fattend:\n                span = fattend\n\n            root.add_patch(Rectangle((start, yy - r), span, 2 * r, \\\n                            lw=0, fc=\"darkslategray\"))\n        yy -= yinterval\n\n    # Alternative bar plots\n    offset = r / 2\n    for bb in altbarbeds:\n        root.text(xend + .01, yy, bb.split(\".\")[0], va=\"center\")\n        bb = Bed(bb)\n        for i, b in enumerate(bb):\n            start, end = xs(b.start), xs(b.end)\n            span = end - start\n            if span < .0001:\n                continue\n            offset = -offset\n            root.add_patch(Rectangle((start, yy + offset), end - start, .003, \\\n                            lw=0, fc=\"darkslategray\"))\n        yy -= yinterval\n\n    root.set_xlim(0, 1)\n    root.set_ylim(0, 1)\n    root.set_axis_off()\n\n    image_name = chr + \".\" + iopts.format\n    savefig(image_name, dpi=iopts.dpi, iopts=iopts)", "response": "%prog composite fastafile chr1"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef multilineplot(args):\n    p = OptionParser(multilineplot.__doc__)\n    p.add_option(\"--lines\",\n                 help=\"Features to plot in lineplot [default: %default]\")\n    p.add_option(\"--colors\",\n                 help=\"List of colors matching number of input bed files\")\n    p.add_option(\"--mode\", default=\"span\", choices=(\"span\", \"count\", \"score\"),\n                 help=\"Accumulate feature based on [default: %default]\")\n    p.add_option(\"--binned\", default=False, action=\"store_true\",\n                 help=\"Specify whether the input is already binned; \" +\n                 \"if True, input files are considered to be binfiles\")\n    p.add_option(\"--ymax\", type=\"int\", help=\"Set Y-axis max\")\n    add_window_options(p)\n    opts, args, iopts = p.set_image_options(args, figsize=\"8x5\")\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    fastafile, chr = args\n    window, shift, subtract, merge = check_window_options(opts)\n    linebeds = []\n    colors = opts.colors\n    if opts.lines:\n        lines = opts.lines.split(\",\")\n        assert len(colors) == len(lines), \"Number of chosen colors must match\" + \\\n                \" number of input bed files\"\n        linebeds = get_beds(lines, binned=opts.binned)\n\n    linebins = get_binfiles(linebeds, fastafile, shift, mode=opts.mode,\n                            binned=opts.binned, merge=merge)\n\n    clen = Sizes(fastafile).mapping[chr]\n    nbins = get_nbins(clen, shift)\n\n    plt.rcParams[\"xtick.major.size\"] = 0\n    plt.rcParams[\"ytick.major.size\"] = 0\n    plt.rcParams[\"figure.figsize\"] = iopts.w, iopts.h\n\n    fig, axarr = plt.subplots(nrows=len(lines))\n    if len(linebeds) == 1:\n        axarr = (axarr, )\n    fig.suptitle(latex(chr), color=\"darkslategray\")\n\n    for i, ax in enumerate(axarr):\n        lineplot(ax, [linebins[i]], nbins, chr, window, shift, \\\n                color=\"{0}{1}\".format(colors[i], 'r'))\n\n    if opts.ymax:\n        ax.set_ylim(0, opts.ymax)\n\n    plt.subplots_adjust(hspace=0.5)\n\n    image_name = chr + \".\" + iopts.format\n    savefig(image_name, dpi=iopts.dpi, iopts=iopts)", "response": "%prog multilineplot fastafile chr1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef heatmap(args):\n    p = OptionParser(heatmap.__doc__)\n    p.add_option(\"--stacks\",\n                 default=\"Exons,Introns,DNA_transposons,Retrotransposons\",\n                 help=\"Features to plot in stackplot [default: %default]\")\n    p.add_option(\"--heatmaps\",\n                 default=\"Copia,Gypsy,hAT,Helitron,Introns,Exons\",\n                 help=\"Features to plot in heatmaps [default: %default]\")\n    p.add_option(\"--meres\", default=None,\n                 help=\"Extra centromere / telomere features [default: %default]\")\n    add_window_options(p)\n    opts, args, iopts = p.set_image_options(args, figsize=\"8x5\")\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    fastafile, chr = args\n    window, shift, subtract, merge = check_window_options(opts)\n\n    stacks = opts.stacks.split(\",\")\n    heatmaps = opts.heatmaps.split(\",\")\n    stackbeds = get_beds(stacks)\n    heatmapbeds = get_beds(heatmaps)\n    stackbins = get_binfiles(stackbeds, fastafile, shift,\n                             subtract=subtract, merge=merge)\n    heatmapbins = get_binfiles(heatmapbeds, fastafile, shift,\n                               subtract=subtract, merge=merge)\n\n    margin = .06\n    inner = .015\n    clen = Sizes(fastafile).mapping[chr]\n\n    fig = plt.figure(1, (iopts.w, iopts.h))\n    root = fig.add_axes([0, 0, 1, 1])\n\n    # Gauge\n    ratio = draw_gauge(root, margin, clen, rightmargin=4 * margin)\n    yinterval = .3\n    xx = margin\n    yy = 1 - margin\n    yy -= yinterval\n    xlen = clen / ratio\n    cc = chr\n    if \"_\" in chr:\n        ca, cb = chr.split(\"_\")\n        cc = ca[0].upper() + cb\n\n    root.add_patch(Rectangle((xx, yy), xlen, yinterval - inner, color=gray))\n    ax = fig.add_axes([xx, yy, xlen, yinterval - inner])\n\n    nbins = get_nbins(clen, shift)\n\n    owindow = clen / 100\n    if owindow > window:\n        window = owindow / shift * shift\n\n    stackplot(ax, stackbins, nbins, palette, chr, window, shift)\n    ax.text(.1, .9, cc, va=\"top\", zorder=100, transform=ax.transAxes,\n              bbox=dict(boxstyle=\"round\", fc=\"w\", alpha=.5))\n\n    # Legends\n    xx += xlen + .01\n    yspace = (yinterval - inner) / (len(stackbins) + 1)\n    yy = 1 - margin - yinterval\n    for s, p in zip(stacks, palette):\n        s = s.replace(\"_\", \" \")\n        s = Registration.get(s, s)\n\n        yy += yspace\n        root.add_patch(Rectangle((xx, yy), inner, inner, color=p, lw=0))\n        root.text(xx + 1.5 * inner, yy, s, size=10)\n\n    yh = .05  # Heatmap height\n    # Heatmaps\n    xx = margin\n    yy = 1 - margin - yinterval - inner\n    for s, p in zip(heatmaps, heatmapbins):\n        s = s.replace(\"_\", \" \")\n        s = Registration.get(s, s)\n\n        yy -= yh\n        m = stackarray(p, chr, window, shift)\n\n        Y = np.array([m, m])\n        root.imshow(Y, extent=(xx, xx + xlen, yy, yy + yh - inner),\n                    interpolation=\"nearest\", aspect=\"auto\", cmap=iopts.cmap)\n        root.text(xx + xlen + .01, yy, s, size=10)\n\n    yy -= yh\n\n    meres = opts.meres\n    if meres:\n        bed = Bed(meres)\n        for b in bed:\n            if b.seqid != chr:\n                continue\n            pos = (b.start + b.end) / 2\n            cpos = pos / ratio\n            xx = margin + cpos\n            accn = b.accn.capitalize()\n            root.add_patch(CirclePolygon((xx, yy), radius=.01, fc=\"m\", ec=\"m\"))\n            root.text(xx + .014, yy, accn, va=\"center\", color=\"m\")\n\n    root.set_xlim(0, 1)\n    root.set_ylim(0, 1)\n    root.set_axis_off()\n\n    image_name = chr + \".\" + iopts.format\n    savefig(image_name, dpi=iopts.dpi, iopts=iopts)", "response": "%prog heatmap fastafile chr1"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _needle(fa, fb, needlefile, a, b, results):\n    from Bio.Emboss.Applications import NeedleCommandline\n\n    needle_cline = NeedleCommandline(asequence=fa, bsequence=fb,\n                        gapopen=10, gapextend=0.5, outfile=needlefile)\n    stdout, stderr = needle_cline()\n    nh = NeedleHeader(needlefile)\n    FileShredder([fa, fb, needlefile], verbose=False)\n    r = [\"\\t\".join((a, b, nh.identity, nh.score))]\n\n    results.extend(r)", "response": "Run single needle job\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef maker(args):\n    from jcvi.formats.base import SetFile, FileShredder\n\n    A, T, P = \"ABINITIO_PREDICTION\", \"TRANSCRIPT\", \"PROTEIN\"\n    # Stores default weights and types\n    Registry = {\\\n        \"maker\": (A, 5),\n        \"augustus_masked\": (A, 1),\n        \"snap_masked\": (A, 1),\n        \"genemark\": (A, 1),\n        \"est2genome\": (T, 5),\n        \"est_gff\": (T, 5),\n        \"protein2genome\": (P, 5),\n        \"blastx\": (P, 1)\n    }\n\n    p = OptionParser(maker.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    gffile, fastafile = args\n\n    types = \"type.ids\"\n    if need_update(gffile, types):\n        cmd = \"cut -f2 -s {0} | sort -u\".format(gffile)\n        sh(cmd, outfile=types)\n\n    types = SetFile(types)\n    reg = defaultdict(list)\n    weightsfile = \"weights.txt\"\n    contents = []\n    for s in types:\n        rs = s.split(\":\")[0]\n        if rs not in Registry:\n            continue\n\n        type, weight = Registry[rs]\n        reg[type].append(s)\n        contents.append(\"\\t\".join(str(x) for x in (type, s, weight)))\n\n    contents = \"\\n\".join(sorted(contents))\n    write_file(weightsfile, contents)\n\n    evs = [x + \".gff\" for x in (A, T, P)]\n    FileShredder(evs)\n\n    for type, tracks in reg.items():\n        for t in tracks:\n            cmd = \"grep '\\t{0}' {1} | grep -v '_match\\t' >> {2}.gff\".format(t, gffile, type)\n            sh(cmd)\n\n    partition(evs)\n    runfile = \"run.sh\"\n    contents = EVMRUN.format(*evs)\n    write_file(runfile, contents)", "response": "%prog maker gffile genome. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tigrload(args):\n    p = OptionParser(tigrload.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    db, ev_type = args\n\n    runfile = \"load.sh\"\n    contents = EVMLOAD.format(db, ev_type)\n    write_file(runfile, contents)", "response": "%prog tigrload db ev_type\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pasa(args):\n    p = OptionParser(pasa.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    pasa_db, fastafile = args\n\n    termexons = \"pasa.terminal_exons.gff3\"\n    if need_update(fastafile, termexons):\n        cmd = \"$ANNOT_DEVEL/PASA2/scripts/pasa_asmbls_to_training_set.dbi\"\n        cmd += ' -M \"{0}:mysql.tigr.org\" -p \"access:access\"'.format(pasa_db)\n        cmd += ' -g {0}'.format(fastafile)\n        sh(cmd)\n\n        cmd = \"$EVM/PasaUtils/retrieve_terminal_CDS_exons.pl\"\n        cmd += \" trainingSetCandidates.fasta trainingSetCandidates.gff\"\n        sh(cmd, outfile=termexons)\n\n    return termexons", "response": "%prog pasa pasa_db fastafile\n\n    Run EVM in TIGR-only mode."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tigrprepare(args):\n    p = OptionParser(tigrprepare.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 4:\n        sys.exit(not p.print_help())\n\n    fastafile, asmbl_id, db, pasa_db = args\n    if asmbl_id == 'all':\n        idsfile = fastafile + \".ids\"\n        if need_update(fastafile, idsfile):\n            ids([fastafile, \"-o\", idsfile])\n    else:\n        idsfile = asmbl_id\n\n    oneid = open(idsfile).next().strip()\n\n    weightsfile = \"weights.txt\"\n    if need_update(idsfile, weightsfile):\n        cmd = \"$EVM/TIGR-only/create_sample_weights_file.dbi\"\n        cmd += \" {0} {1} | tee weights.txt\".format(db, oneid)\n        sh(cmd)\n\n    evs = [\"gene_predictions.gff3\", \"transcript_alignments.gff3\",\n           \"protein_alignments.gff3\"]\n    if need_update(weightsfile, evs):\n        cmd = \"$EVM/TIGR-only/write_GFF3_files.dbi\"\n        cmd += \" --db {0} --asmbl_id {1} --weights {2}\".\\\n                format(db, idsfile, weightsfile)\n        sh(cmd)\n\n    evs[1] = fix_transcript()\n\n    partition(evs)\n    runfile = \"run.sh\"\n    contents = EVMRUN.format(*evs)\n    write_file(runfile, contents)", "response": "%prog tigrprepare asmbl. fasta asmbl. id db pasa. terminal_exons. gff3"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef uniq(args):\n\n    p = OptionParser(uniq.__doc__)\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    gffile, cdsfasta = args\n    gff = Gff(gffile)\n    sizes = Sizes(cdsfasta).mapping\n    gene_register = {}\n    for g in gff:\n        if g.type != \"mRNA\":\n            continue\n        aed = float(g.attributes[\"_AED\"][0])\n        gene_register[g.parent] = (1 - aed) * sizes[g.accn]\n\n    allgenes = import_feats(gffile)\n    g = get_piles(allgenes)\n\n    bestids = set()\n    for group in g:\n        ranges = [to_range(x, score=gene_register[x.accn], id=x.accn) \\\n                    for x in group]\n        selected_chain, score = range_chain(ranges)\n        bestids |= set(x.id for x in selected_chain)\n\n    removed = set(x.accn for x in allgenes) - bestids\n    fw = open(\"removed.ids\", \"w\")\n    print(\"\\n\".join(sorted(removed)), file=fw)\n    fw.close()\n    populate_children(opts.outfile, bestids, gffile, \"gene\")", "response": "%prog uniq gffile cdsfasta"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef trimUTR(args):\n    import gffutils\n    from jcvi.formats.base import SetFile\n\n    p = OptionParser(trimUTR.__doc__)\n    p.add_option(\"--trim5\", default=None, type=\"str\", \\\n        help=\"File containing gene list for 5' UTR trimming\")\n    p.add_option(\"--trim3\", default=None, type=\"str\", \\\n        help=\"File containing gene list for 3' UTR trimming\")\n    p.add_option(\"--trimrange\", default=None, type=\"str\", \\\n        help=\"File containing gene list for UTR trim back\" + \\\n             \"based on suggested (start, stop) coordinate range\")\n    p.add_option(\"--refgff\", default=None, type=\"str\", \\\n        help=\"Reference GFF3 used as fallback to replace UTRs\")\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    gffile, = args\n    gff = make_index(gffile)\n\n    trim_both = False if (opts.trim5 or opts.trim3) else True\n    trim5 = SetFile(opts.trim5) if opts.trim5 else set()\n    trim3 = SetFile(opts.trim3) if opts.trim3 else set()\n    trimrange = dict()\n    if opts.trimrange:\n        trf = must_open(opts.trimrange)\n        for tr in trf:\n            assert len(tr.split(\"\\t\")) == 3, \\\n                \"Must specify (start, stop) coordinate range\"\n            id, start, stop = tr.split(\"\\t\")\n            trimrange[id] = (int(start), int(stop))\n        trf.close()\n\n    refgff = make_index(opts.refgff) if opts.refgff else None\n\n    fw = must_open(opts.outfile, \"w\")\n    for feat in gff.iter_by_parent_childs(featuretype=\"gene\", order_by=(\"seqid\", \"start\"), level=1):\n        for c in feat:\n            cid, ctype, cparent = c.id, c.featuretype, \\\n                c.attributes.get('Parent', [None])[0]\n            t5, t3 = False, False\n            if ctype == \"gene\":\n                t5 = True if cid in trim5 else False\n                t3 = True if cid in trim3 else False\n                start, end = get_cds_minmax(gff, cid)\n                trim(c, start, end, trim5=t5, trim3=t3, both=trim_both)\n                fprint(c, fw)\n            elif ctype == \"mRNA\":\n                utr_types, extras = [], set()\n                if any(id in trim5 for id in (cid, cparent)):\n                    t5 = True\n                    trim5.add(cid)\n                if any(id in trim3 for id in (cid, cparent)):\n                    t3 = True\n                    trim3.add(cid)\n                refc = None\n                if refgff:\n                    try:\n                        refc = refgff[cid]\n                        refctype = refc.featuretype\n                        refptype = refgff[refc.attributes['Parent'][0]].featuretype\n                        if refctype == \"mRNA\" and refptype == \"gene\":\n                            if cmp_children(cid, gff, refgff, cftype=\"CDS\"):\n                                reinstate(c, refc, trim5=t5, trim3=t3, both=trim_both)\n                                if t5: utr_types.append('five_prime_UTR')\n                                if t3: utr_types.append('three_prime_UTR')\n                                for utr_type in utr_types:\n                                    for utr in refgff.children(refc, featuretype=utr_type):\n                                        extras.add(utr)\n                                        for exon in refgff.region(region=utr, featuretype=\"exon\"):\n                                            if exon.attributes['Parent'][0] == cid:\n                                                extras.add(exon)\n                        else:\n                            refc = None\n                    except gffutils.exceptions.FeatureNotFoundError:\n                        pass\n                start, end = get_cds_minmax(gff, cid, level=1)\n                if cid in trimrange:\n                    start, end = range_minmax([trimrange[cid], (start, end)])\n                if not refc:\n                    trim(c, start, end, trim5=t5, trim3=t3, both=trim_both)\n                fprint(c, fw)\n                for cc in gff.children(cid, order_by=(\"start\")):\n                    _ctype = cc.featuretype\n                    if _ctype not in utr_types:\n                        if _ctype != \"CDS\":\n                            if _ctype == \"exon\":\n                                eskip = [range_overlap(to_range(cc), to_range(x)) \\\n                                    for x in extras if x.featuretype == 'exon']\n                                if any(skip for skip in eskip): continue\n                            trim(cc, start, end, trim5=t5, trim3=t3, both=trim_both)\n                            fprint(cc, fw)\n                        else:\n                            fprint(cc, fw)\n                for x in extras:\n                    fprint(x, fw)\n    fw.close()", "response": "%prog trimUTR gffile\n\n    Remove UTRs in the annotation set.\n\n    If reference GFF3 is provided, reinstate UTRs from reference\n    transcripts after trimming.\n\n    Note: After running trimUTR, it is advised to also run\n    `python -m jcvi.formats.gff fixboundaries` on the resultant GFF3\n    to adjust the boundaries of all parent 'gene' features"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprinting the edges of the current node and the other nodes.", "response": "def print_edges(G, bed, families):\n    \"\"\"\n    Instead of going through the graph construction, just print the edges.\n    \"\"\"\n    symbols = {'+': '>', '-': '<'}\n    for seqid, bs in bed.sub_beds():\n        prev_node, prev_strand = None, '+'\n        for b in bs:\n            accn = b.accn\n            strand = b.strand\n            node = \"=\".join(families[accn])\n            if prev_node:\n                print(\"{}{}--{}{}\".format(prev_node, symbols[prev_strand],\n                                          symbols[strand], node))\n            prev_node, prev_strand = node, strand"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pairs(args):\n    p = OptionParser(pairs.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    anchorfile, prefix = args\n    outfile = prefix + \".pairs\"\n    fw = open(outfile, \"w\")\n\n    af = AnchorFile(anchorfile)\n    blocks = af.blocks\n    pad = len(str(len(blocks)))\n    npairs = 0\n    for i, block in enumerate(blocks):\n        block_id = \"{0}{1:0{2}d}\".format(prefix, i + 1, pad)\n        lines = []\n        for q, s, score in block:\n            npairs += 1\n            score = score.replace('L', '')\n            lines.append(\"\\t\".join((q, s, score, block_id)))\n        print(\"\\n\".join(sorted(lines)), file=fw)\n\n    fw.close()\n    logging.debug(\"A total of {0} pairs written to `{1}`.\".\n                  format(npairs, outfile))", "response": "%prog pairs anchorsfile prefix"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef zipbed(args):\n    p = OptionParser(zipbed.__doc__)\n    p.add_option(\"--prefix\", default=\"b\",\n                 help=\"Prefix for the new seqid [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    bedfile, anchorfile = args\n    prefix = opts.prefix\n    bed = Bed(bedfile)\n    order = bed.order\n    newbedfile = prefix + \".bed\"\n    fw = open(newbedfile, \"w\")\n\n    af = AnchorFile(anchorfile)\n    blocks = af.blocks\n    pad = len(str(len(blocks)))\n    for i, block in enumerate(blocks):\n        block_id = \"{0}{1:0{2}d}\".format(prefix, i + 1, pad)\n        pairs = []\n        for q, s, score in block:\n            qi, q = order[q]\n            si, s = order[s]\n            pairs.append((qi, si))\n        newbed = list(interleave_pairs(pairs))\n        for i, b in enumerate(newbed):\n            accn = bed[b].accn\n            print(\"\\t\".join(str(x)\n                            for x in (block_id, i, i + 1, accn)), file=fw)\n\n    logging.debug(\"Reconstructed bedfile written to `{0}`.\".format(newbedfile))", "response": "%prog zipbed species. bed anchorfile > ancestral. txt"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef counts(args):\n    p = OptionParser(counts.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    vcffile, = args\n    vcf_reader = vcf.Reader(open(vcffile))\n    for r in vcf_reader:\n        v = CPRA(r)\n        if not v.is_valid:\n            continue\n        for sample in r.samples:\n            ro = sample[\"RO\"]\n            ao = sample[\"AO\"]\n            print(\"\\t\".join(str(x) for x in (v, ro, ao)))", "response": "%prog counts vcffile\n    Collect allele counts from RO and AO fields."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_valid(self):\n        return len(self.ref) == 1 and \\\n               len(self.alt) == 1 and \\\n               len(self.alt[0]) == 1", "response": "Check if the record set is valid."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _number_finder(s, regex, numconv):\n\n    # Split.  If there are no splits, return now\n    s = regex.split(s)\n    if len(s) == 1:\n        return tuple(s)\n\n    # Now convert the numbers to numbers, and leave strings as strings\n    s = remove_empty(s)\n    for i in range(len(s)):\n        try:\n            s[i] = numconv(s[i])\n        except ValueError:\n            pass\n\n    # If the list begins with a number, lead with an empty string.\n    # This is used to get around the \"unorderable types\" issue.\n    if not isinstance(s[0], six.string_types):\n        return [''] + s\n    else:\n        return s", "response": "Helper function to split numbers into lists of lists of numbers and strings."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef natsorted(seq, key=lambda x: x, number_type=float, signed=True, exp=True):\n    return sorted(seq, key=lambda x: natsort_key(key(x),\n                                                 number_type=number_type,\n                                                 signed=signed, exp=exp))", "response": "\\ Naturally sorts a sequence naturally and numerically."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calibrate(args):\n    xargs = args[2:]\n    p = OptionParser(calibrate.__doc__)\n    opts, args, iopts = add_seeds_options(p, args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    imagefile, boxsize = args\n    boxsize = float(boxsize)\n\n    # Read in color checker\n    colorcheckerfile = op.join(datadir, \"colorchecker.txt\")\n    colorchecker = []\n    expected = 0\n    for row in open(colorcheckerfile):\n        boxes = row.split()\n        colorchecker.append(boxes)\n        expected += len(boxes)\n\n    folder = op.split(imagefile)[0]\n    objects = seeds([imagefile, \"--outdir={0}\".format(folder)] + xargs)\n    nseeds = len(objects)\n    logging.debug(\"Found {0} boxes (expected={1})\".format(nseeds, expected))\n    assert expected - 4 <= nseeds <= expected + 4, \\\n                \"Number of boxes drastically different from {0}\".format(expected)\n\n    # Calculate pixel-cm ratio\n    boxes = [t.area for t in objects]\n    reject = reject_outliers(boxes)\n    retained_boxes = [b for r, b in zip(reject, boxes) if not r]\n    mbox = np.median(retained_boxes)  # in pixels\n    pixel_cm_ratio = (mbox / boxsize) ** .5\n    logging.debug(\"Median box size: {0} pixels. Measured box size: {1} cm2\".\\\n                format(mbox, boxsize))\n    logging.debug(\"Pixel-cm ratio: {0}\".format(pixel_cm_ratio))\n\n    xs = [t.x for t in objects]\n    ys = [t.y for t in objects]\n    idx_xs = get_kmeans(xs, 6)\n    idx_ys = get_kmeans(ys, 4)\n    for xi, yi, s in zip(idx_xs, idx_ys, objects):\n        s.rank = (yi, xi)\n\n    objects.sort(key=lambda x: x.rank)\n\n    colormap = []\n    for s in objects:\n        x, y = s.rank\n        observed, expected = s.rgb, rgb_to_triplet(colorchecker[x][y])\n        colormap.append((np.array(observed), np.array(expected)))\n\n    # Color transfer\n    tr0 = np.eye(3).flatten()\n    print(\"Initial distance:\", total_error(tr0, colormap), file=sys.stderr)\n    tr = fmin(total_error, tr0, args=(colormap,))\n    tr.resize((3, 3))\n    print(\"RGB linear transform:\\n\", tr, file=sys.stderr)\n    calib = {\"PixelCMratio\": pixel_cm_ratio,\n             \"RGBtransform\": tr.tolist()}\n\n    jsonfile = op.join(folder, \"calibrate.json\")\n    fw = must_open(jsonfile, \"w\")\n    print(json.dumps(calib, indent=4), file=fw)\n    fw.close()\n    logging.debug(\"Calibration specs written to `{0}`.\".format(jsonfile))\n\n    return jsonfile", "response": "%prog calibrate imagefile boxsize"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef seeds(args):\n    p = OptionParser(seeds.__doc__)\n    p.set_outfile()\n    opts, args, iopts = add_seeds_options(p, args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    pngfile, = args\n    pf = opts.prefix or op.basename(pngfile).rsplit(\".\", 1)[0]\n    sigma, kernel = opts.sigma, opts.kernel\n    rows, cols = opts.rows, opts.cols\n    labelrows, labelcols = opts.labelrows, opts.labelcols\n    ff = opts.filter\n    calib = opts.calibrate\n    outdir = opts.outdir\n    if outdir != '.':\n        mkdir(outdir)\n    if calib:\n        calib = json.load(must_open(calib))\n        pixel_cm_ratio, tr = calib[\"PixelCMratio\"], calib[\"RGBtransform\"]\n        tr = np.array(tr)\n\n    resizefile, mainfile, labelfile, exif = \\\n                      convert_image(pngfile, pf, outdir=outdir,\n                                    rotate=opts.rotate,\n                                    rows=rows, cols=cols,\n                                    labelrows=labelrows, labelcols=labelcols)\n\n    oimg = load_image(resizefile)\n    img = load_image(mainfile)\n\n    fig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols=4, nrows=1,\n                                             figsize=(iopts.w, iopts.h))\n\n    # Edge detection\n    img_gray = rgb2gray(img)\n    logging.debug(\"Running {0} edge detection ...\".format(ff))\n    if ff == \"canny\":\n        edges = canny(img_gray, sigma=opts.sigma)\n    elif ff == \"roberts\":\n        edges = roberts(img_gray)\n    elif ff == \"sobel\":\n        edges = sobel(img_gray)\n    edges = clear_border(edges, buffer_size=opts.border)\n    selem = disk(kernel)\n    closed = closing(edges, selem) if kernel else edges\n    filled = binary_fill_holes(closed)\n\n    # Watershed algorithm\n    if opts.watershed:\n        distance = distance_transform_edt(filled)\n        local_maxi = peak_local_max(distance, threshold_rel=.05, indices=False)\n        coordinates = peak_local_max(distance, threshold_rel=.05)\n        markers, nmarkers = label(local_maxi, return_num=True)\n        logging.debug(\"Identified {0} watershed markers\".format(nmarkers))\n        labels = watershed(closed, markers, mask=filled)\n    else:\n        labels = label(filled)\n\n    # Object size filtering\n    w, h = img_gray.shape\n    canvas_size = w * h\n    min_size = int(round(canvas_size * opts.minsize / 100))\n    max_size = int(round(canvas_size * opts.maxsize / 100))\n    logging.debug(\"Find objects with pixels between {0} ({1}%) and {2} ({3}%)\"\\\n                    .format(min_size, opts.minsize, max_size, opts.maxsize))\n\n    # Plotting\n    ax1.set_title('Original picture')\n    ax1.imshow(oimg)\n\n    params = \"{0}, $\\sigma$={1}, $k$={2}\".format(ff, sigma, kernel)\n    if opts.watershed:\n        params += \", watershed\"\n    ax2.set_title('Edge detection\\n({0})'.format(params))\n    closed = gray2rgb(closed)\n    ax2_img = labels\n    if opts.edges:\n        ax2_img = closed\n    elif opts.watershed:\n        ax2.plot(coordinates[:, 1], coordinates[:, 0], 'g.')\n    ax2.imshow(ax2_img, cmap=iopts.cmap)\n\n    ax3.set_title('Object detection')\n    ax3.imshow(img)\n\n    filename = op.basename(pngfile)\n    if labelfile:\n        accession = extract_label(labelfile)\n    else:\n        accession = pf\n\n    # Calculate region properties\n    rp = regionprops(labels)\n    rp = [x for x in rp if min_size <= x.area <= max_size]\n    nb_labels = len(rp)\n    logging.debug(\"A total of {0} objects identified.\".format(nb_labels))\n    objects = []\n    for i, props in enumerate(rp):\n        i += 1\n        if i > opts.count:\n            break\n\n        y0, x0 = props.centroid\n        orientation = props.orientation\n        major, minor = props.major_axis_length, props.minor_axis_length\n        major_dx = cos(orientation) * major / 2\n        major_dy = sin(orientation) * major / 2\n        minor_dx = sin(orientation) * minor / 2\n        minor_dy = cos(orientation) * minor / 2\n        ax2.plot((x0 - major_dx, x0 + major_dx),\n                 (y0 + major_dy, y0 - major_dy), 'r-')\n        ax2.plot((x0 - minor_dx, x0 + minor_dx),\n                 (y0 - minor_dy, y0 + minor_dy), 'r-')\n\n        npixels = int(props.area)\n        # Sample the center of the blob for color\n        d = min(int(round(minor / 2 * .35)) + 1, 50)\n        x0d, y0d = int(round(x0)), int(round(y0))\n        square = img[(y0d - d):(y0d + d), (x0d - d):(x0d + d)]\n        pixels = []\n        for row in square:\n            pixels.extend(row)\n        logging.debug(\"Seed #{0}: {1} pixels ({2} sampled) - {3:.2f}%\".\\\n                        format(i, npixels, len(pixels), 100. * npixels / canvas_size))\n\n        rgb = pixel_stats(pixels)\n        objects.append(Seed(filename, accession, i, rgb, props, exif))\n        minr, minc, maxr, maxc = props.bbox\n        rect = Rectangle((minc, minr), maxc - minc, maxr - minr,\n                                  fill=False, ec='w', lw=1)\n        ax3.add_patch(rect)\n        mc, mr = (minc + maxc) / 2, (minr + maxr) / 2\n        ax3.text(mc, mr, \"{0}\".format(i), color='w',\n                    ha=\"center\", va=\"center\", size=6)\n\n    for ax in (ax2, ax3):\n        ax.set_xlim(0, h)\n        ax.set_ylim(w, 0)\n\n    # Output identified seed stats\n    ax4.text(.1, .92, \"File: {0}\".format(latex(filename)), color='g')\n    ax4.text(.1, .86, \"Label: {0}\".format(latex(accession)), color='m')\n    yy = .8\n    fw = must_open(opts.outfile, \"w\")\n    if not opts.noheader:\n        print(Seed.header(calibrate=calib), file=fw)\n    for o in objects:\n        if calib:\n            o.calibrate(pixel_cm_ratio, tr)\n        print(o, file=fw)\n        i = o.seedno\n        if i > 7:\n            continue\n        ax4.text(.01, yy, str(i), va=\"center\", bbox=dict(fc='none', ec='k'))\n        ax4.text(.1, yy, o.pixeltag, va=\"center\")\n        yy -= .04\n        ax4.add_patch(Rectangle((.1, yy - .025), .12, .05, lw=0,\n                      fc=rgb_to_hex(o.rgb)))\n        ax4.text(.27, yy, o.hashtag, va=\"center\")\n        yy -= .06\n    ax4.text(.1 , yy, \"(A total of {0} objects displayed)\".format(nb_labels),\n             color=\"darkslategrey\")\n    normalize_axes(ax4)\n\n    for ax in (ax1, ax2, ax3):\n        xticklabels = [int(x) for x in ax.get_xticks()]\n        yticklabels = [int(x) for x in ax.get_yticks()]\n        ax.set_xticklabels(xticklabels, family='Helvetica', size=8)\n        ax.set_yticklabels(yticklabels, family='Helvetica', size=8)\n\n    image_name = op.join(outdir, pf + \".\" + iopts.format)\n    savefig(image_name, dpi=iopts.dpi, iopts=iopts)\n    return objects", "response": "%prog seeds [pngfile|jpgfile]\n\n    Extract seed metrics from [pngfile|jpgfile]. Use --rows and --cols to crop image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tiling(args):\n    p = OptionParser(tiling.__doc__)\n    p.add_option(\"--overlap\", default=3000, type=\"int\",\n                 help=\"Minimum amount of overlaps required\")\n    p.set_verbose()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    bedfile, = args\n    ov = opts.overlap\n\n    bed = Bed(bedfile)\n    inf = len(bed)\n    selected = Bed()\n    for seqid, sbed in bed.sub_beds():\n        g = Grouper()\n        current = sbed[0]\n        # Partition connected features\n        for a in sbed:\n            g.join(a)\n            # requires a real overlap\n            if a.start < current.end - ov:\n                g.join(a, current)\n            if a.end > current.end:\n                current = a\n\n        # Process per partition\n        for gbed in g:\n            end = max(x.end for x in gbed)\n            gbed.sort(key=lambda x: (x.start, -x.end))\n            entries = len(gbed)\n            counts = [inf] * entries\n            counts[0] = 1\n            traceback = [-1] * entries\n            for i, a in enumerate(gbed):\n                for j in xrange(i + 1, entries):\n                    b = gbed[j]\n                    if b.start >= a.end - ov:\n                        break\n                    # Two ranges overlap!\n                    if counts[i] + 1 < counts[j]:\n                        counts[j] = counts[i] + 1\n                        traceback[j] = i\n            endi = [i for i, a in enumerate(gbed) if a.end == end]\n            last = min((traceback[i], i) for i in endi)[1]\n            chain = []\n            while last != -1:\n                chain.append(last)\n                last = traceback[last]\n            chain = chain[::-1]\n            selected.extend([gbed[x] for x in chain])\n\n            if opts.verbose:\n                print(counts)\n                print(traceback)\n                print(chain)\n                print(\"\\n\".join(str(x) for x in gbed))\n                print(\"*\" * 30)\n                print(\"\\n\".join(str(gbed[x]) for x in chain))\n                print()\n\n    tilingbedfile = bedfile.rsplit(\".\", 1)[0] + \".tiling.bed\"\n    selected.print_to_file(filename=tilingbedfile, sorted=True)\n    logging.debug(\"A total of {} tiling features written to `{}`\"\\\n                    .format(len(selected), tilingbedfile))", "response": "%prog tiling bedfile\n\n    Compute minimum tiling path using as few clones as possible. Implemented\n    with dynamic programming. Greedy algorithm may also work according a\n    stackoverflow source."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef chain(args):\n    p = OptionParser(chain.__doc__)\n    p.add_option(\"--dist\", default=100000, help=\"Chaining distance\")\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    bedfile, = args\n    cmd = \"sort -k4,4 -k1,1 -k2,2n -k3,3n {0} -o {0}\".format(bedfile)\n    sh(cmd)\n    bed = Bed(bedfile, sorted=False)\n    newbed = Bed()\n    for accn, bb in groupby(bed, key=lambda x: x.accn):\n        bb = list(bb)\n        g = Grouper()\n        for a in bb:\n            g.join(a)\n        for a, b in pairwise(bb):\n            if a.seqid == b.seqid and b.start - a.end < opts.dist:\n                g.join(a, b)\n        data = []\n        for p in g:\n            seqid = p[0].seqid\n            start = min(x.start for x in p)\n            end = max(x.end for x in p)\n            score = sum(x.span for x in p)\n            data.append((seqid, start - 1, end, accn, score))\n\n        d = max(data, key=lambda x: x[-1])\n        newbed.append(BedLine(\"\\t\".join(str(x) for x in d)))\n\n    newbed.print_to_file(opts.outfile, sorted=True)", "response": "%prog chain bedfile\n\n    Chain BED segments together."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef density(args):\n    p = OptionParser(density.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    bedfile, fastafile = args\n    bed = Bed(bedfile)\n    sizes = Sizes(fastafile).mapping\n    header = \"seqid features size density_per_Mb\".split()\n    print(\"\\t\".join(header))\n    for seqid, bb in bed.sub_beds():\n        nfeats = len(bb)\n        size = sizes[seqid]\n        ds = nfeats * 1e6 / size\n        print(\"\\t\".join(str(x) for x in \\\n                    (seqid, nfeats, size, \"{0:.1f}\".format(ds))))", "response": "%prog density bedfile ref. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clr(args):\n    p = OptionParser(clr.__doc__)\n    p.set_bedpe()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    bedpe, ref = args\n    if bedpe.endswith(\".bam\"):\n        bedpefile = bedpe.replace(\".bam\", \".bedpe\")\n        if need_update(bedpe, bedpefile):\n            cmd = \"bamToBed -bedpe -i {0}\".format(bedpe)\n            sh(cmd, outfile=bedpefile)\n        bedpe = bedpefile\n\n    filtered = bedpe + \".filtered\"\n    if need_update(bedpe, filtered):\n        filter_bedpe(bedpe, filtered, ref, rc=opts.rc,\n                     minlen=opts.minlen, maxlen=opts.maxlen)\n\n    rmdup = filtered + \".sorted.rmdup\"\n    if need_update(filtered, rmdup):\n        rmdup_bedpe(filtered, rmdup, dupwiggle=opts.dup)\n\n    converted = rmdup + \".converted\"\n    if need_update(rmdup, converted):\n        fp = open(rmdup)\n        fw = open(converted, \"w\")\n        for row in fp:\n            r = BedpeLine(row)\n            print(r.bedline, file=fw)\n        fw.close()\n\n    merged = converted + \".merge.bed\"\n    if need_update(converted, merged):\n        mergeBed(converted)", "response": "%prog clr bedfile ref. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef alignextend(args):\n    p = OptionParser(alignextend.__doc__)\n    p.add_option(\"--len\", default=100, type=\"int\",\n                 help=\"Extend to this length\")\n    p.add_option(\"--qv\", default=31, type=\"int\",\n                 help=\"Dummy qv score for extended bases\")\n    p.add_option(\"--bedonly\", default=False, action=\"store_true\",\n                 help=\"Only generate bed files, no FASTA\")\n    p.set_bedpe()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    bedpe, ref = args\n    qvchar = chr(opts.qv + 33)\n    pf = bedpe.split(\".\")[0]\n\n    filtered = bedpe + \".filtered\"\n    if need_update(bedpe, filtered):\n        filter_bedpe(bedpe, filtered, ref, rc=opts.rc,\n                     minlen=opts.minlen, maxlen=opts.maxlen, rlen=opts.rlen)\n\n    rmdup = filtered + \".filtered.sorted.rmdup\"\n    if need_update(filtered, rmdup):\n        rmdup_bedpe(filtered, rmdup, dupwiggle=opts.dup)\n\n    if opts.bedonly:\n        return\n\n    bed1, bed2 = pf + \".1e.bed\", pf + \".2e.bed\"\n    if need_update(rmdup, (bed1, bed2)):\n        sh(\"cut -f1-3,7-9 {0}\".format(rmdup), outfile=bed1)\n        sh(\"cut -f4-6,7-8,10 {0}\".format(rmdup), outfile=bed2)\n\n    sfa1, sfa2 = pf + \".1e.sfa\", pf + \".2e.sfa\"\n    if need_update((bed1, bed2, ref), (sfa1, sfa2)):\n        for bed in (bed1, bed2):\n            fastaFromBed(bed, ref, name=True, tab=True, stranded=True)\n\n    fq1, fq2 = pf + \".1e.fq\", pf + \".2e.fq\"\n    if need_update((sfa1, sfa2), (fq1, fq2)):\n        for sfa in (sfa1, sfa2):\n            sfa_to_fq(sfa, qvchar)", "response": "%prog alignextend bedpefile ref. fasta"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef random(args):\n    from random import sample\n    from jcvi.formats.base import flexible_cast\n\n    p = OptionParser(random.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    bedfile, N = args\n    assert is_number(N)\n\n    b = Bed(bedfile)\n    NN = flexible_cast(N)\n    if NN < 1:\n        NN = int(round(NN * len(b)))\n\n    beds = sample(b, NN)\n    new_bed = Bed()\n    new_bed.extend(beds)\n\n    outfile = bedfile.rsplit(\".\", 1)[0] + \".{0}.bed\".format(N)\n    new_bed.print_to_file(outfile)\n    logging.debug(\"Write {0} features to `{1}`\".format(NN, outfile))", "response": "%prog random bedfile number_of_features"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef depth(args):\n    p = OptionParser(depth.__doc__)\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    readsbed, featsbed = args\n    fp = open(featsbed)\n    nargs = len(fp.readline().split(\"\\t\"))\n    keepcols = \",\".join(str(x) for x in range(1, nargs + 1))\n    cmd = \"coverageBed -a {0} -b {1} -d\".format(readsbed, featsbed)\n    cmd += \" | groupBy -g {0} -c {1} -o mean\".format(keepcols, nargs + 2)\n    sh(cmd, outfile=opts.outfile)", "response": "%prog depth reads. bed features. bed"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves ISOforms from a list of GMAP IDs.", "response": "def remove_isoforms(ids):\n    \"\"\"\n    This is more or less a hack to remove the GMAP multiple mappings. Multiple\n    GMAP mappings can be seen given the names .mrna1, .mrna2, etc.\n    \"\"\"\n    key = lambda x: x.rsplit(\".\", 1)[0]\n    iso_number = lambda x: get_number(x.split(\".\")[-1])\n    ids = sorted(ids, key=key)\n    newids = []\n    for k, ii in groupby(ids, key=key):\n        min_i = min(list(ii), key=iso_number)\n        newids.append(min_i)\n    return newids"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef merge(args):\n    p = OptionParser(merge.__doc__)\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    bedfiles = args\n    fw = must_open(opts.outfile, \"w\")\n    for bedfile in bedfiles:\n        bed = Bed(bedfile)\n        pf = op.basename(bedfile).split(\".\")[0]\n        for b in bed:\n            b.seqid = \"_\".join((pf, b.seqid))\n            print(b, file=fw)", "response": "%prog merge bedfiles > newbedfile\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef uniq(args):\n    from jcvi.formats.sizes import Sizes\n\n    p = OptionParser(uniq.__doc__)\n    p.add_option(\"--sizes\", help=\"Use sequence length as score\")\n    p.add_option(\"--mode\", default=\"span\", choices=(\"span\", \"score\"),\n                 help=\"Pile mode\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    bedfile, = args\n    uniqbedfile = bedfile.split(\".\")[0] + \".uniq.bed\"\n    bed = Bed(bedfile)\n\n    if opts.sizes:\n        sizes = Sizes(opts.sizes).mapping\n        ranges = [Range(x.seqid, x.start, x.end, sizes[x.accn], i) \\\n                    for i, x in enumerate(bed)]\n    else:\n        if opts.mode == \"span\":\n            ranges = [Range(x.seqid, x.start, x.end, x.end - x.start + 1, i) \\\n                        for i, x in enumerate(bed)]\n        else:\n            ranges = [Range(x.seqid, x.start, x.end, float(x.score), i) \\\n                        for i, x in enumerate(bed)]\n\n    selected, score = range_chain(ranges)\n    selected = [x.id for x in selected]\n    selected_ids = set(selected)\n    selected = [bed[x] for x in selected]\n    notselected = [x for i, x in enumerate(bed) if i not in selected_ids]\n\n    newbed = Bed()\n    newbed.extend(selected)\n    newbed.print_to_file(uniqbedfile, sorted=True)\n\n    if notselected:\n        leftoverfile = bedfile.split(\".\")[0] + \".leftover.bed\"\n        leftoverbed = Bed()\n        leftoverbed.extend(notselected)\n        leftoverbed.print_to_file(leftoverfile, sorted=True)\n\n    logging.debug(\"Imported: {0}, Exported: {1}\".format(len(bed), len(newbed)))\n\n    return uniqbedfile", "response": "%prog uniq bedfile\n\n    Remove overlapping features with higher scores."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef refine(args):\n    p = OptionParser(refine.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    abedfile, bbedfile, refinedbed = args\n    fw = open(refinedbed, \"w\")\n    intersected = refined = 0\n    for a, b in intersectBed_wao(abedfile, bbedfile):\n        if b is None:\n            print(a, file=fw)\n            continue\n\n        intersected += 1\n        aspan_before = a.span\n        arange = (a.start, a.end)\n        brange = (b.start, b.end)\n        irange = range_intersect(arange, brange)\n        a.start, a.end = irange\n        aspan_after = a.span\n        if aspan_before > aspan_after:\n            refined += 1\n        print(a, file=fw)\n\n    fw.close()\n    print(\"Total intersected: {0}\".format(intersected), file=sys.stderr)\n    print(\"Total refined: {0}\".format(refined), file=sys.stderr)\n    summary([abedfile])\n    summary([refinedbed])", "response": "%prog refine bedfile1 bedfile2 refinedbed"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef analyze_dists(dists, cutoff=1000, alpha=.1):\n    peak0 = [d for d in dists if d < cutoff]\n    peak1 = [d for d in dists if d >= cutoff]\n    c0, c1 = len(peak0), len(peak1)\n    logging.debug(\"Component counts: {0} {1}\".format(c0, c1))\n    if c0 == 0 or c1 == 0 or float(c1) / len(dists) < alpha:\n        logging.debug(\"Single peak identified ({0} / {1} < {2})\".\\\n                        format(c1, len(dists), alpha))\n        return np.median(dists)\n\n    peak0_median = np.median(peak0)\n    peak1_median = np.median(peak1)\n    logging.debug(\"Dual peaks identified: {0}bp ({1}), {2}bp ({3}) (selected)\".\\\n                format(int(peak0_median), c0, int(peak1_median), c1))\n\n    return peak1_median", "response": "Analyze the given list of distributions and return the median of the two peaks."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef report_pairs(data, cutoff=0, mateorientation=None,\n        pairsfile=None, insertsfile=None, rclip=1, ascii=False, bins=20,\n        distmode=\"ss\", mpcutoff=1000):\n    \"\"\"\n    This subroutine is used by the pairs function in blast.py and cas.py.\n    Reports number of fragments and pairs as well as linked pairs\n    \"\"\"\n    allowed_mateorientations = (\"++\", \"--\", \"+-\", \"-+\")\n\n    if mateorientation:\n        assert mateorientation in allowed_mateorientations\n\n    num_fragments, num_pairs = 0, 0\n\n    all_dist = []\n    linked_dist = []\n    # +- (forward-backward) is `innie`, -+ (backward-forward) is `outie`\n    orientations = defaultdict(int)\n\n    # clip how many chars from end of the read name to get pair name\n    key = (lambda x: x.accn[:-rclip]) if rclip else (lambda x: x.accn)\n    data.sort(key=key)\n\n    if pairsfile:\n        pairsfw = open(pairsfile, \"w\")\n    if insertsfile:\n        insertsfw = open(insertsfile, \"w\")\n\n    for pe, lines in groupby(data, key=key):\n        lines = list(lines)\n        if len(lines) != 2:\n            num_fragments += len(lines)\n            continue\n\n        num_pairs += 1\n        a, b = lines\n\n        asubject, astart, astop = a.seqid, a.start, a.end\n        bsubject, bstart, bstop = b.seqid, b.start, b.end\n\n        aquery, bquery = a.accn, b.accn\n        astrand, bstrand = a.strand, b.strand\n\n        dist, orientation = range_distance(\\\n                (asubject, astart, astop, astrand),\n                (bsubject, bstart, bstop, bstrand),\n                distmode=distmode)\n\n        if dist >= 0:\n            all_dist.append((dist, orientation, aquery, bquery))\n\n    # select only pairs with certain orientations - e.g. innies, outies, etc.\n    if mateorientation:\n        all_dist = [x for x in all_dist if x[1] == mateorientation]\n\n    # try to infer cutoff as twice the median until convergence\n    if cutoff <= 0:\n        dists = np.array([x[0] for x in all_dist], dtype=\"int\")\n        p0 = analyze_dists(dists, cutoff=mpcutoff)\n        cutoff = int(2 * p0)  # initial estimate\n        cutoff = int(math.ceil(cutoff / bins)) * bins\n        logging.debug(\"Insert size cutoff set to {0}, \".format(cutoff) +\n            \"use '--cutoff' to override\")\n\n    for dist, orientation, aquery, bquery in all_dist:\n        if dist > cutoff:\n            continue\n        if cutoff > 2 * mpcutoff and dist < mpcutoff:\n            continue\n\n        linked_dist.append(dist)\n        if pairsfile:\n            print(\"{0}\\t{1}\\t{2}\".format(aquery, bquery, dist), file=pairsfw)\n        orientations[orientation] += 1\n\n    print(\"{0} fragments, {1} pairs ({2} total)\".\\\n                format(num_fragments, num_pairs, num_fragments + num_pairs * 2), file=sys.stderr)\n\n    s = SummaryStats(linked_dist, dtype=\"int\")\n    num_links = s.size\n\n    meandist, stdev = s.mean, s.sd\n    p0, p1, p2 = s.median, s.p1, s.p2\n\n    print(\"%d pairs (%.1f%%) are linked (cutoff=%d)\" % \\\n            (num_links, num_links * 100. / num_pairs, cutoff), file=sys.stderr)\n    print(\"mean distance between mates: {0} +/- {1}\".\\\n            format(meandist, stdev), file=sys.stderr)\n    print(\"median distance between mates: {0}\".format(p0), file=sys.stderr)\n    print(\"95% distance range: {0} - {1}\".format(p1, p2), file=sys.stderr)\n    print(\"\\nOrientations:\", file=sys.stderr)\n\n    orientation_summary = []\n    for orientation, count in sorted(orientations.items()):\n        o = \"{0}:{1}\".format(orientation, \\\n                percentage(count, num_links, mode=1))\n        orientation_summary.append(o.split()[0])\n        print(o, file=sys.stderr)\n\n    if insertsfile:\n        from jcvi.graphics.histogram import histogram\n\n        print(\"\\n\".join(str(x) for x in linked_dist), file=insertsfw)\n        insertsfw.close()\n        prefix = insertsfile.rsplit(\".\", 1)[0]\n        if prefix > 10:\n            prefix = prefix.split(\"-\")[0]\n        osummary = \" \".join(orientation_summary)\n        title=\"{0} ({1}; median:{2} bp)\".format(prefix, osummary, p0)\n        histogram(insertsfile, vmin=0, vmax=cutoff, bins=bins,\n                xlabel=\"Insertsize\", title=title, ascii=ascii)\n        if op.exists(insertsfile):\n            os.remove(insertsfile)\n\n    return s", "response": "Report the pairs of the read names in data."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsees __doc__ for OptionParser.set_pairs().", "response": "def pairs(args):\n    \"\"\"\n    See __doc__ for OptionParser.set_pairs().\n    \"\"\"\n    p = OptionParser(pairs.__doc__)\n    p.set_pairs()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    bedfile, = args\n\n    basename = bedfile.split(\".\")[0]\n    insertsfile = \".\".join((basename, \"inserts\"))\n    bedfile = sort([bedfile, \"--accn\"])\n\n    fp = open(bedfile)\n    data = [BedLine(row) for i, row in enumerate(fp) if i < opts.nrows]\n\n    ascii = not opts.pdf\n    return bedfile, report_pairs(data, opts.cutoff, opts.mateorientation,\n           pairsfile=opts.pairsfile, insertsfile=insertsfile,\n           rclip=opts.rclip, ascii=ascii, bins=opts.bins,\n           distmode=opts.distmode)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef summary(args):\n    p = OptionParser(summary.__doc__)\n    p.add_option(\"--sizes\", default=False, action=\"store_true\",\n                 help=\"Write .sizes file\")\n    p.add_option(\"--all\", default=False, action=\"store_true\",\n                 help=\"Write summary stats per seqid\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    bedfile, = args\n    bed = Bed(bedfile)\n    bs = BedSummary(bed)\n    if opts.sizes:\n        sizesfile = bedfile + \".sizes\"\n        fw = open(sizesfile, \"w\")\n        for span, accn in bs.mspans:\n            print(span, file=fw)\n        fw.close()\n        logging.debug(\"Spans written to `{0}`.\".format(sizesfile))\n        return bs\n\n    if not opts.all:\n        bs.report()\n        return bs\n\n    for seqid, subbeds in bed.sub_beds():\n        bs = BedSummary(subbeds)\n        print(\"\\t\".join((seqid, str(bs))))", "response": "%prog summary bedfile\n\n    Sum the total lengths of the intervals."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sort(args):\n    p = OptionParser(sort.__doc__)\n    p.add_option(\"-i\", \"--inplace\", dest=\"inplace\",\n            default=False, action=\"store_true\",\n            help=\"Sort bed file in place [default: %default]\")\n    p.add_option(\"-u\", dest=\"unique\",\n            default=False, action=\"store_true\",\n            help=\"Uniqify the bed file\")\n    p.add_option(\"--accn\", default=False, action=\"store_true\",\n            help=\"Sort based on the accessions [default: %default]\")\n    p.set_outfile(outfile=None)\n    p.set_tmpdir()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    bedfile, = args\n    inplace = opts.inplace\n\n    if not inplace and \".sorted.\" in bedfile:\n        return bedfile\n\n    sortedbed = opts.outfile\n    if inplace:\n        sortedbed = bedfile\n    elif opts.outfile is None:\n        pf, sf = op.basename(bedfile).rsplit(\".\", 1)\n        sortedbed = pf + \".sorted.\" + sf\n\n    sortopt = \"-k1,1 -k2,2n -k3,3n -k4,4\" if not opts.accn else \\\n              \"-k4,4 -k1,1 -k2,2n -k3,3n\"\n    cmd = \"sort\"\n    if opts.tmpdir:\n        cmd += \" -T {0}\".format(opts.tmpdir)\n    if opts.unique:\n        cmd += \" -u\"\n    cmd += \" {0} {1} -o {2}\".format(sortopt, bedfile, sortedbed)\n\n    if inplace or need_update(bedfile, sortedbed):\n        sh(cmd)\n\n    return sortedbed", "response": "%prog sort bedfile\n\n    Sort bed file to have ascending order of seqid, then start. It uses the\n    `sort` command."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mates(args):\n    p = OptionParser(mates.__doc__)\n    p.add_option(\"--lib\", default=False, action=\"store_true\",\n            help=\"Output library information along with pairs [default: %default]\")\n    p.add_option(\"--nointra\", default=False, action=\"store_true\",\n            help=\"Remove mates that are intra-scaffold [default: %default]\")\n    p.add_option(\"--prefix\", default=False, action=\"store_true\",\n            help=\"Only keep links between IDs with same prefix [default: %default]\")\n    p.set_mates()\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    bedfile, = args\n    rclip = opts.rclip\n\n    key = (lambda x: x.accn[:-rclip]) if rclip else (lambda x: x.accn)\n    bed = Bed(bedfile, key=key)\n\n    pf = bedfile.rsplit(\".\", 1)[0]\n    matesfile = pf + \".mates\"\n    lib = pf if opts.lib else None\n    fw = open(matesfile, \"w\")\n    if lib:\n        bedfile, stats = pairs([bedfile, \\\n                        \"--rclip={0}\".format(rclip),\n                        \"--cutoff={0}\".format(opts.cutoff)])\n        sv = int(2 * stats.sd)\n        mindist = max(stats.mean - sv, 1)\n        maxdist = stats.mean + sv\n        print(\"\\t\".join(str(x) for x in \\\n                (\"library\", pf, mindist, maxdist)), file=fw)\n\n    num_fragments = num_pairs = 0\n    matesbedfile = matesfile + \".bed\"\n    fwm = open(matesbedfile, \"w\")\n    for pe, lines in groupby(bed, key=key):\n        lines = list(lines)\n        if len(lines) != 2:\n            num_fragments += len(lines)\n            continue\n\n        a, b = lines\n\n        if opts.nointra and a.seqid == b.seqid:\n            continue\n\n        # Use --prefix to limit the links between seqids with the same prefix\n        # For example, contigs of the same BAC, mth2-23j10_001, mth-23j10_002\n        if opts.prefix:\n            aprefix = a.seqid.split(\"_\")[0]\n            bprefix = b.seqid.split(\"_\")[0]\n            if aprefix != bprefix:\n                continue\n\n        num_pairs += 1\n        pair = [a.accn, b.accn]\n        if lib:\n            pair.append(lib)\n        print(\"\\t\".join(pair), file=fw)\n\n        print(a, file=fwm)\n        print(b, file=fwm)\n\n    logging.debug(\"Discard {0} frags and write {1} pairs to `{2}` and `{3}`.\".\\\n            format(num_fragments, num_pairs, matesfile, matesbedfile))\n\n    fw.close()\n    fwm.close()\n\n    return matesfile, matesbedfile", "response": "%prog mates bedfile\n\n    Generate the mates file by inferring from the names."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new BlastLine with the swapped query and subject.", "response": "def swapped(self):\n        \"\"\"\n        Swap query and subject.\n        \"\"\"\n        args = [getattr(self, attr) for attr in BlastLine.__slots__[:12]]\n        args[0:2] = [self.subject, self.query]\n        args[6:10] = [self.sstart, self.sstop, self.qstart, self.qstop]\n        if self.orientation == '-':\n            args[8], args[9] = args[9], args[8]\n        b = \"\\t\".join(str(x) for x in args)\n        return BlastLine(b)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gff(args):\n    p = OptionParser(gff.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    gbkfile, = args\n    MultiGenBank(gbkfile)", "response": "%prog gff seq. gbk\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints out the quaures of a locus.", "response": "def print_locus_quals(locus_tag, locus, quals_ftypes):\n    \"\"\"\n    Given a locus_tag and dict of feaures, print out 3-column output:\n        locus_tag, qualifier, value\n\n    Replace locus_tag with protein_id if processing an \"mRNA\" or \"CDS\"\n    \"\"\"\n    prot_id = None\n    for ftype in quals_ftypes:\n        for i, quals in enumerate(locus[locus_tag][ftype]):\n            for elem in quals:\n                elem_id = elem[0]\n                if len(locus[locus_tag][\"protein_id\"]) > 0 and ftype in (\"mRNA\", \"CDS\"):\n                    elem_id = locus[locus_tag][\"protein_id\"][i]\n                if ftype == 'misc_RNA': ftype = 'ncRNA'\n                print(\"\\t\".join(str(x) for x in (elem_id, elem[1], elem[2], ftype)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadvance the iterator n - steps ahead. If n is none consume entirely.", "response": "def consume(iterator, n):\n    \"Advance the iterator n-steps ahead. If n is none, consume entirely.\"\n    # Use functions that consume iterators at C speed.\n    if n is None:\n        # feed the entire iterator into a zero-length deque\n        collections.deque(iterator, maxlen=0)\n    else:\n        # advance to the empty slice starting at position n\n        next(islice(iterator, n, n), None)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tee_lookahead(t, i):\n    for value in islice(t.__copy__(), i, None):\n        return value\n    raise IndexError(i)", "response": "Inspect the i - th upcomping value from a tee object while leaving the tee object at its current position. Raise an IndexError if the underlying iterator doesn t have enough values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nyield n items from an iterable into a list of n items.", "response": "def chunked(iterable, n):\n    \"\"\"Break an iterable into lists of a given length::\n\n        >>> list(chunked([1, 2, 3, 4, 5, 6, 7], 3))\n        [[1, 2, 3], [4, 5, 6], [7]]\n\n    If the length of ``iterable`` is not evenly divisible by ``n``, the last\n    returned list will be shorter.\n\n    This is useful for splitting up a computation on a large number of keys\n    into batches, to be pickled and sent off to worker processes. One example\n    is operations on rows in MySQL, which does not implement server-side\n    cursors properly and would otherwise load the entire dataset into RAM on\n    the client.\n\n    \"\"\"\n    # Doesn't seem to run into any number-of-args limits.\n    for group in (list(g) for g in zip_longest(*[iter(iterable)] * n,\n                                               fillvalue=_marker)):\n        if group[-1] is _marker:\n            # If this is the last group, shuck off the padding:\n            del group[group.index(_marker):]\n        yield group"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the next item from the iterator.", "response": "def peek(self, default=_marker):\n        \"\"\"Return the item that will be next returned from ``next()``.\n\n        Return ``default`` if there are no items left. If ``default`` is not\n        provided, raise ``StopIteration``.\n\n        \"\"\"\n        if not hasattr(self, '_peek'):\n            try:\n                self._peek = next(self._it)\n            except StopIteration:\n                if default is _marker:\n                    raise\n                return default\n        return self._peek"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef readlen(args):\n    p = OptionParser(readlen.__doc__)\n    p.set_firstN()\n    p.add_option(\"--silent\", default=False, action=\"store_true\",\n                 help=\"Do not print read length stats\")\n    p.add_option(\"--nocheck\", default=False, action=\"store_true\",\n                 help=\"Do not check file type suffix\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    f, = args\n    if (not opts.nocheck) and (not is_fastq(f)):\n        logging.debug(\"File `{}` does not endswith .fastq or .fq\".format(f))\n        return 0\n\n    s = calc_readlen(f, opts.firstN)\n    if not opts.silent:\n        print(\"\\t\".join(str(x) for x in (f, s.min, s.max, s.mean, s.median)))\n\n    return int(s.max)", "response": "%prog readlen fastqfile\n\n    Calculate read length, will only try the first N reads. Output min, max, and\n    avg for each file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format(args):\n    p = OptionParser(format.__doc__)\n\n    p.add_option(\"--convert\", default=None, choices=[\">=1.8\", \"<1.8\", \"sra\"],\n                help=\"Convert fastq header to a different format\" +\n                \" [default: %default]\")\n    p.set_tag(specify_tag=True)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    fastqfile, = args\n    ai = iter_fastq(fastqfile)\n    rec = next(ai)\n    dialect = None\n    while rec:\n        h = FastqHeader(rec.header)\n        if not dialect:\n            dialect = h.dialect\n            logging.debug(\"Input fastq dialect: `{0}`\".format(dialect))\n            if opts.convert:\n                logging.debug(\"Output fastq dialect: `{0}`\".format(opts.convert))\n\n        rec.name = h.format_header(dialect=opts.convert, tag=opts.tag)\n\n        print(rec)\n        rec = next(ai)", "response": "%prog format fastqfile\n\n    Format FASTQ file. Currently provides option to convert FASTQ header from\n    one dialect to another."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef some(args):\n    p = OptionParser(some.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) not in (2, 3):\n        sys.exit(not p.print_help())\n\n    idsfile, afastq, = args[:2]\n    bfastq = args[2] if len(args) == 3 else None\n\n    ids = DictFile(idsfile, valuepos=None)\n\n    ai = iter_fastq(open(afastq))\n    arec = next(ai)\n    if bfastq:\n        bi = iter_fastq(open(bfastq))\n        brec = next(bi)\n\n    while arec:\n        if arec.name[1:] in ids:\n            print(arec)\n            if bfastq:\n                print(brec)\n\n        arec = next(ai)\n        if bfastq:\n            brec = next(bi)", "response": "%prog some idsfile a. fastq b. fastq"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef trim(args):\n    p = OptionParser(trim.__doc__)\n    p.add_option(\"-f\", dest=\"first\", default=0, type=\"int\",\n            help=\"First base to keep. Default is 1.\")\n    p.add_option(\"-l\", dest=\"last\", default=0, type=\"int\",\n            help=\"Last base to keep. Default is entire read.\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    fastqfile, = args\n    obfastqfile = op.basename(fastqfile)\n    fq = obfastqfile.rsplit(\".\", 1)[0] + \".ntrimmed.fastq\"\n    if fastqfile.endswith(\".gz\"):\n        fq = obfastqfile.rsplit(\".\", 2)[0] + \".ntrimmed.fastq.gz\"\n\n    cmd = \"fastx_trimmer -Q33 \"\n    if opts.first:\n        cmd += \"-f {0.first} \".format(opts)\n    if opts.last:\n        cmd += \"-l {0.last} \".format(opts)\n\n    sh(cmd, infile=fastqfile, outfile=fq)", "response": "%prog trim fastqfile\n\n    Wraps `fastx_trimmer` to trim from begin or end of reads."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef catread(args):\n    p = OptionParser(catread.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    r1, r2 = args\n    p1fp, p2fp = FastqPairedIterator(r1, r2)\n    outfile = pairspf((r1, r2)) + \".cat.fastq\"\n    fw = must_open(outfile, \"w\")\n    while True:\n        a = list(islice(p1fp, 4))\n        if not a:\n            break\n        atitle, aseq, _, aqual = a\n        btitle, bseq, _, bqual = list(islice(p2fp, 4))\n        print(\"\\n\".join((atitle.strip(), aseq.strip() + bseq.strip(), \\\n                                \"+\", aqual.strip() + bqual.strip())), file=fw)", "response": "%prog catread fastqfile1 fastqfile2..."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef size(args):\n    p = OptionParser(size.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    total_size = total_numrecords = 0\n    for f in args:\n        cur_size = cur_numrecords = 0\n        for rec in iter_fastq(f):\n            if not rec:\n                break\n            cur_numrecords += 1\n            cur_size += len(rec)\n\n        print(\" \".join(str(x) for x in \\\n                (op.basename(f), cur_numrecords, cur_size)))\n        total_numrecords += cur_numrecords\n        total_size += cur_size\n\n    if len(args) > 1:\n        print(\" \".join(str(x) for x in \\\n                (\"Total\", total_numrecords, total_size)))", "response": "%prog size fastqfile\n\n    Find the total base pairs in a list of fastq files"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert(args):\n    p = OptionParser(convert.__doc__)\n    p.set_phred()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    infastq, = args\n    phred = opts.phred or str(guessoffset([infastq]))\n    ophred = {\"64\": \"33\", \"33\": \"64\"}[phred]\n\n    gz = infastq.endswith(\".gz\")\n    outfastq = infastq.rsplit(\".\", 1)[0] if gz else infastq\n    pf, sf = outfastq.rsplit(\".\", 1)\n    outfastq = \"{0}.q{1}.{2}\".format(pf, ophred, sf)\n    if gz:\n        outfastq += \".gz\"\n\n    fin = \"illumina\" if phred == \"64\" else \"sanger\"\n    fout = \"sanger\" if phred == \"64\" else \"illumina\"\n\n    seqret = \"seqret\"\n    if infastq.endswith(\".gz\"):\n        cmd = \"zcat {0} | \".format(infastq)\n        cmd += seqret + \" fastq-{0}::stdin fastq-{1}::stdout\".\\\n                format(fin, fout)\n    else:\n        cmd = seqret + \" fastq-{0}::{1} fastq-{2}::stdout\".\\\n                format(fin, infastq, fout)\n\n    sh(cmd, outfile=outfastq)\n\n    return outfastq", "response": "%prog convert in. fastq"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gff(args):\n    from jcvi.utils.range import range_minmax\n    from jcvi.formats.gff import valid_gff_parent_child, valid_gff_type\n\n    p = OptionParser(gff.__doc__)\n    p.add_option(\"--source\", default=None, help=\"Specify GFF source.\" +\n                \" By default, it picks algorithm used to generate btab file.\" +\n                \" [default: %default]\")\n    p.add_option(\"--type\", default=\"protein_match\", choices=valid_gff_type,\n                help=\"GFF feature type [default: %default]\")\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    btabfile, = args\n    btabdict = {}\n    btab = Btab(btabfile, aat_dialect=True)\n    osource = opts.source or \"aat\"\n    otype = opts.type\n    octype = valid_gff_parent_child[otype]\n    for b in btab:\n        nargs = b.nargs\n        id = b.query + \"-\" + otype + \"{0:05d}\".format(b.chainNum)\n        key = b.key\n        if key not in btabdict:\n            btabdict[key] = { 'id': id,\n                              'method': b.method,\n                              'query': b.query,\n                              'subject': b.subject,\n                              'strand': b.qStrand,\n                              'sDesc': b.sDesc,\n                              'coords': [],\n                              'children': []\n                            }\n\n        btabdict[key]['coords'].append((b.qStart, b.qStop))\n        btabdict[key]['children'].append(b.gffline(source=osource, type=octype, id=id))\n\n    for v in btabdict.itervalues():\n        b = BtabLine(\"\\t\".join(str(x) for x in [0] * nargs), aat_dialect=True)\n        id = v['id']\n        b.query = v['query']\n        b.method = v['method']\n        b.subject = v['subject']\n        b.qStrand = v['strand']\n        b.sDesc = v['sDesc']\n        b.qStart, b.qStop = range_minmax(v['coords'])\n        print(b.gffline(source=osource, type=otype, primary_tag=\"ID\", id=id))\n        print(\"\\n\".join(v['children']))", "response": "%prog gff btabfile\n\n    Convert btab file generated by AAT to gff3 format."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nyielding Latin names for a list of taxids", "response": "def batch_taxonomy(list_of_taxids):\n    \"\"\"\n    Convert list of taxids to Latin names\n    \"\"\"\n    for taxid in list_of_taxids:\n        handle = Entrez.efetch(db='Taxonomy', id=taxid, retmode=\"xml\")\n        records = Entrez.read(handle)\n        yield records[0][\"ScientificName\"]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nyields a list of taxonomy IDs for a list of Latin names.", "response": "def batch_taxids(list_of_names):\n    \"\"\"\n    Opposite of batch_taxonomy():\n\n    Convert list of Latin names to taxids\n    \"\"\"\n    for name in list_of_names:\n        handle = Entrez.esearch(db='Taxonomy', term=name, retmode=\"xml\")\n        records = Entrez.read(handle)\n        yield records[\"IdList\"][0]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving multiple rather than a single record for a list of terms.", "response": "def batch_entrez(list_of_terms, db=\"nuccore\", retmax=1, rettype=\"fasta\",\n                 batchsize=1, email=myEmail):\n    \"\"\"\n    Retrieve multiple rather than a single record\n    \"\"\"\n\n    for term in list_of_terms:\n\n        logging.debug(\"Search term %s\" % term)\n        success = False\n        ids = None\n        if not term:\n            continue\n\n        while not success:\n            try:\n                search_handle = Entrez.esearch(db=db, retmax=retmax, term=term)\n                rec = Entrez.read(search_handle)\n                success = True\n                ids = rec[\"IdList\"]\n            except (HTTPError, URLError,\n                    RuntimeError, KeyError) as e:\n                logging.error(e)\n                logging.debug(\"wait 5 seconds to reconnect...\")\n                time.sleep(5)\n\n        if not ids:\n            logging.error(\"term {0} not found\".format(term))\n            continue\n\n        assert ids\n        nids = len(ids)\n        if nids > 1:\n            logging.debug(\"A total of {0} results found.\".format(nids))\n\n        if batchsize != 1:\n            logging.debug(\"Use a batch size of {0}.\".format(batchsize))\n\n        ids = list(grouper(ids, batchsize))\n\n        for id in ids:\n            id = [x for x in id if x]\n            size = len(id)\n            id = \",\".join(id)\n\n            success = False\n            while not success:\n                try:\n                    fetch_handle = Entrez.efetch(db=db, id=id, rettype=rettype,\n                                                 email=email)\n                    success = True\n                except (HTTPError, URLError,\n                        RuntimeError) as e:\n                    logging.error(e)\n                    logging.debug(\"wait 5 seconds to reconnect...\")\n                    time.sleep(5)\n\n            yield id, size, term, fetch_handle"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef phytozome10(args):\n    from jcvi.apps.biomart import GlobusXMLParser\n\n    p = OptionParser(phytozome10.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    species, = args\n    \"\"\"\n    cookies = get_cookies()\n    # Get directory listing\n    dlist = \\\n    \"http://genome.jgi.doe.gov/ext-api/downloads/get-directory?organism=PhytozomeV10\"\n    d = download(dlist, debug=True, cookies=cookies)\n    \"\"\"\n    fp = open(\"get-directory.html\")\n    g = GlobusXMLParser(fp)\n    g.parse_folder()", "response": "%prog phytozome species\n\n    Retrieve genomes and annotations from phytozome using Globus API. Available\n    species listed below. Use comma to give a list of species to download. For\n    example:\n\n    $ %prog phytozome Athaliana,Vvinifera,Osativa,Sbicolor,Slycopersicum"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_first_rec(fastafile):\n    f = list(SeqIO.parse(fastafile, \"fasta\"))\n\n    if len(f) > 1:\n        logging.debug(\"{0} records found in {1}, using the first one\".\n                      format(len(f), fastafile))\n\n    return f[0]", "response": "Returns the first record in the fastafile"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef inspect(item, maxchar=80):\n    for i in dir(item):\n        try:\n            member = str(getattr(item, i))\n            if maxchar and len(member) > maxchar:\n                member = member[:maxchar] + \"...\"\n        except:\n            member = \"[ERROR]\"\n        print(\"{}: {}\".format(i, member), file=sys.stderr)", "response": "Inspect the attributes of an item."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms reverse enumeration returning an iterator with decrementing index and position values", "response": "def enumerate_reversed(sequence):\n    \"\"\"\n    Perform reverse enumeration, returning an iterator with decrementing\n    index/position values\n\n    Source: http://stackoverflow.com/questions/529424/traverse-a-list-in-reverse-order-in-python\n    \"\"\"\n    for index in reversed(xrange(len(sequence))):\n        yield index, sequence[index]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef thousands(x):\n    import locale\n    try:\n        locale.setlocale(locale.LC_ALL, \"en_US.utf8\")\n    except Exception:\n        locale.setlocale(locale.LC_ALL, \"en_US.UTF-8\")\n    finally:\n        s = '%d' % x\n        groups = []\n        while s and s[-1].isdigit():\n            groups.append(s[-3:])\n            s = s[:-3]\n        return s + ','.join(reversed(groups))\n    return locale.format('%d', x, True)", "response": ">>> thousands x >>>"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a file size in bytes to human - readable form.", "response": "def human_size(size, a_kilobyte_is_1024_bytes=False, precision=1, target=None):\n    '''Convert a file size to human-readable form.\n\n    Keyword arguments:\n    size -- file size in bytes\n    a_kilobyte_is_1024_bytes -- if True (default), use multiples of 1024\n                                if False, use multiples of 1000\n\n    Returns: string\n    Credit: <http://diveintopython3.org/your-first-python-program.html>\n\n    >>> print(human_size(1000000000000, True))\n    931.3GiB\n    >>> print(human_size(1000000000000))\n    1.0Tb\n    >>> print(human_size(300))\n    300.0\n    '''\n    if size < 0:\n        raise ValueError('number must be non-negative')\n\n    multiple = 1024 if a_kilobyte_is_1024_bytes else 1000\n    for suffix in SUFFIXES[multiple]:\n\n        if target:\n            if suffix == target:\n                break\n            size /= float(multiple)\n        else:\n            if size >= multiple:\n                size /= float(multiple)\n            else:\n                break\n\n    return '{0:.{1}f}{2}'.format(size, precision, suffix)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef autoscale(bp, optimal=6):\n    slen = str(bp)\n    tlen = slen[0:2] if len(slen) > 1 else slen[0]\n    precision = len(slen) - 2  # how many zeros we need to pad?\n    bp_len_scaled = int(tlen)  # scale bp_len to range (0, 100)\n    tick_diffs = [(x, abs(bp_len_scaled / x - optimal)) for x in [1, 2, 5, 10]]\n    best_stride, best_tick_diff = min(tick_diffs, key=lambda x: x[1])\n\n    while precision > 0:\n        best_stride *= 10\n        precision -= 1\n\n    return best_stride", "response": "Auto - scales the base - interval of a BP to a number of digits."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef gene_name(st, exclude=(\"ev\",), sep=\".\"):\n    if any(st.startswith(x) for x in exclude):\n        sep = None\n    st = st.split('|')[0]\n\n    if sep and sep in st:\n        name, suffix = st.rsplit(sep, 1)\n    else:\n        name, suffix = st, \"\"\n\n    # We only want to remove suffix that are isoforms, longer suffix would\n    # suggest that it is part of the right gene name\n    if len(suffix) != 1:\n        name = st\n\n    return name", "response": "Returns the name of the given gene name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fixChromName(name, orgn=\"medicago\"):\n    import re\n    mtr_pat1 = re.compile(r\"Mt[0-9]+\\.[0-9]+[\\.[0-9]+]{0,}_([a-z]+[0-9]+)\")\n    mtr_pat2 = re.compile(r\"([A-z0-9]+)_[A-z]+_[A-z]+\")\n\n    zmays_pat = re.compile(\n        r\"[a-z]+:[A-z0-9]+:([A-z0-9]+):[0-9]+:[0-9]+:[0-9]+\")\n    zmays_sub = {'mitochondrion': 'Mt', 'chloroplast': 'Pt'}\n    if orgn == \"medicago\":\n        for mtr_pat in (mtr_pat1, mtr_pat2):\n            match = re.search(mtr_pat, name)\n            if match:\n                n = match.group(1)\n                n = n.replace(\"0\", \"\")\n                name = re.sub(mtr_pat, n, name)\n    elif orgn == \"maize\":\n        match = re.search(zmays_pat, name)\n        if match:\n            n = match.group(1)\n            name = re.sub(zmays_pat, n, name)\n            if name in zmays_sub:\n                name = zmays_sub[name]\n\n    return name", "response": "Fix the chromosomes in the same release file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fill(text, delimiter=\"\", width=70):\n    texts = []\n    for i in xrange(0, len(text), width):\n        t = delimiter.join(text[i:i + width])\n        texts.append(t)\n    return \"\\n\".join(texts)", "response": "Wrap text with width per line"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef uniqify(L):\n    seen = set()\n    nL = []\n    for a in L:\n        if a in seen:\n            continue\n        nL.append(a)\n        seen.add(a)\n\n    return nL", "response": "Uniqify a list maintains order."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef normalize_lms_axis(ax, xlim=None, ylim=None, xfactor=1e-6, yfactor=1,\n                       xlabel=None, ylabel=\"Map (cM)\"):\n    \"\"\" Normalize the axis limits and labels to beautify axis.\n    \"\"\"\n    if xlim:\n        ax.set_xlim(0, xlim)\n    if ylim:\n        ax.set_ylim(0, ylim)\n    if xlabel:\n        xticklabels = [int(round(x * xfactor)) for x in ax.get_xticks()]\n        ax.set_xticklabels(xticklabels, family='Helvetica')\n        ax.set_xlabel(xlabel)\n    else:\n        ax.set_xticks([])\n    if ylabel:\n        yticklabels = [int(round(x * yfactor)) for x in ax.get_yticks()]\n        ax.set_yticklabels(yticklabels, family='Helvetica')\n        ax.set_ylabel(ylabel)\n    else:\n        ax.set_yticks([])", "response": "Normalize the axis limits and labels to beautify axis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fake(args):\n    from math import ceil\n    from random import choice\n\n    from Bio import SeqIO\n    from Bio.Seq import Seq\n    from Bio.SeqRecord import SeqRecord\n\n    p = OptionParser(fake.__doc__)\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    inputbed, = args\n    bed = Bed(inputbed)\n    recs = []\n    for seqid, sb in bed.sub_beds():\n        maxend = max(x.end for x in sb)\n        size = int(ceil(maxend / 1000.) * 1000)\n        seq = \"\".join([choice(\"ACGT\") for x in xrange(size)])\n        rec = SeqRecord(Seq(seq), id=seqid, description=\"\")\n        recs.append(rec)\n\n    fw = must_open(opts.outfile, \"w\")\n    SeqIO.write(recs, fw, \"fasta\")", "response": "%prog fake input. bed\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compute_score(markers, bonus, penalty):\n    nmarkers = len(markers)\n    s = [bonus] * nmarkers   # score\n    f = [-1] * nmarkers  # from\n    for i in xrange(1, nmarkers):\n        for j in xrange(i):\n            mi, mj = markers[i], markers[j]\n            t = bonus if mi.mlg == mj.mlg else penalty + bonus\n            if s[i] < s[j] + t:\n                s[i] = s[j] + t\n                f[i] = j\n    # Recover the highest scoring chain\n    highest_score = max(s)\n    si = s.index(highest_score)\n    onchain = set()\n    while True:\n        if si < 0:\n            break\n        si = f[si]\n        onchain.add(si)\n    return [x for i, x in enumerate(markers) if i in onchain]", "response": "Compute chain score using dynamic programming."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef split(args):\n    p = OptionParser(split.__doc__)\n    p.add_option(\"--chunk\", default=4, type=\"int\",\n                 help=\"Split chunks of at least N markers\")\n    p.add_option(\"--splitsingle\", default=False, action=\"store_true\",\n                 help=\"Split breakpoint range right in the middle\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    inputbed, = args\n    bonus = 2\n    nchunk = opts.chunk\n    nbreaks = 0\n    penalty = -(nchunk * bonus - 1)\n    bed = Bed(inputbed)\n    for seqid, bb in bed.sub_beds():\n        markers = [Marker(x) for x in bb]\n        markers = compute_score(markers, bonus, penalty)\n        for mi, mj in pairwise(markers):\n            if mi.mlg == mj.mlg:\n                continue\n            assert mi.seqid == mj.seqid\n            start, end = mi.pos, mj.pos\n            if start > end:\n                start, end = end, start\n            if opts.splitsingle:\n                start = end = (start + end) / 2\n            print(\"\\t\".join(str(x) for x in (mi.seqid, start - 1, end)))\n            nbreaks += 1\n    logging.debug(\"A total of {} breakpoints inferred (--chunk={})\".\\\n                    format(nbreaks, nchunk))", "response": "%prog split input. bed"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef movie(args):\n    p = OptionParser(movie.__doc__)\n    p.add_option(\"--gapsize\", default=100, type=\"int\",\n                 help=\"Insert gaps of size between scaffolds\")\n    add_allmaps_plot_options(p)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    inputbed, scaffoldsfasta, seqid = args\n    gapsize = opts.gapsize\n    pf = inputbed.rsplit(\".\", 1)[0]\n    agpfile = pf + \".chr.agp\"\n    tourfile = pf + \".tour\"\n\n    fp = open(tourfile)\n    sizes = Sizes(scaffoldsfasta).mapping\n    ffmpeg = \"ffmpeg\"\n    mkdir(ffmpeg)\n    score = cur_score = None\n    i = 1\n    for header, block in read_block(fp, \">\"):\n        s, tag, label = header[1:].split()\n        if s != seqid:\n            continue\n        tour = block[0].split()\n        tour = [(x[:-1], x[-1]) for x in tour]\n        if label.startswith(\"GA\"):\n            cur_score = label.split(\"-\")[-1]\n            if cur_score == score:\n                i += 1\n                continue\n            score = cur_score\n\n        image_name = \".\".join((seqid, \"{0:04d}\".format(i), label, \"pdf\"))\n        if need_update(tourfile, image_name):\n            fwagp = must_open(agpfile, \"w\")\n            order_to_agp(seqid, tour, sizes, fwagp, gapsize=gapsize,\n                         gaptype=\"map\")\n            fwagp.close()\n            logging.debug(\"{0} written to `{1}`\".format(header, agpfile))\n            build([inputbed, scaffoldsfasta, \"--cleanup\"])\n            pdf_name = plot([inputbed, seqid, \"--title={0}\".format(label)])\n            sh(\"mv {0} {1}\".format(pdf_name, image_name))\n        if label in (\"INIT\", \"FLIP\", \"TSP\", \"FINAL\"):\n            for j in xrange(5):  # Delay for 5 frames\n                image_delay = image_name.rsplit(\".\", 1)[0] + \\\n                              \".d{0}.pdf\".format(j)\n                sh(\"cp {0} {1}/{2}\".format(image_name, ffmpeg, image_delay))\n        else:\n            sh(\"cp {0} {1}/\".format(image_name, ffmpeg))\n        i += 1\n\n    make_movie(ffmpeg, pf)", "response": "%prog movie input. bed scaffolds. fasta seqid"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking a movie using either ffmpeg or gifsicle.", "response": "def make_movie(workdir, pf, dpi=120, fps=1, format=\"pdf\", engine=\"ffmpeg\"):\n    \"\"\" Make the movie using either ffmpeg or gifsicle.\n    \"\"\"\n    os.chdir(workdir)\n    if format != \"png\":\n        cmd  = \"parallel convert -density {}\".format(dpi)\n        cmd += \" {} {.}.png ::: \" + \"*.{}\".format(format)\n        sh(cmd)\n\n    assert engine in (\"ffmpeg\", \"gifsicle\"), \\\n            \"Only ffmpeg or gifsicle is currently supported\"\n    if engine == \"ffmpeg\":\n        cmd = \"ffmpeg -framerate {} -pattern_type glob -i '*.png' {}.mp4\"\\\n                .format(fps, pf)\n    elif engine == \"gifsicle\":\n        cmd  = \"convert *.png gif:- |\"\n        cmd += \" gifsicle --delay {} --loop --optimize=3\".format(100 / fps)\n        cmd += \" --colors=256 --multifile - > {}.gif\".format(pf)\n\n    sh(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef estimategaps(args):\n    p = OptionParser(estimategaps.__doc__)\n    p.add_option(\"--minsize\", default=100, type=\"int\",\n                 help=\"Minimum gap size\")\n    p.add_option(\"--maxsize\", default=500000, type=\"int\",\n                 help=\"Maximum gap size\")\n    p.add_option(\"--links\", default=10, type=\"int\",\n                 help=\"Only use linkage grounds with matchings more than\")\n    p.set_verbose(help=\"Print details for each gap calculation\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    inputbed, = args\n    pf = inputbed.rsplit(\".\", 1)[0]\n    agpfile = pf + \".chr.agp\"\n    bedfile = pf + \".lifted.bed\"\n\n    cc = Map(bedfile, scaffold_info=True)\n    agp = AGP(agpfile)\n    minsize, maxsize = opts.minsize, opts.maxsize\n    links = opts.links\n    verbose = opts.verbose\n\n    outagpfile = pf + \".estimategaps.agp\"\n    fw = must_open(outagpfile, \"w\")\n\n    for ob, components in agp.iter_object():\n        components = list(components)\n        s = Scaffold(ob, cc)\n        mlg_counts = s.mlg_counts\n        gaps = [x for x in components if x.is_gap]\n        gapsizes = [None] * len(gaps)   # master\n        for mlg, count in mlg_counts.items():\n            if count < links:\n                continue\n            g = GapEstimator(cc, agp, ob, mlg)\n            g.compute_all_gaps(minsize=minsize, maxsize=maxsize, \\\n                               verbose=verbose)\n            # Merge evidence from this mlg into master\n            assert len(g.gapsizes) == len(gaps)\n            for i, gs in enumerate(gapsizes):\n                gg = g.gapsizes[i]\n                if gs is None:\n                    gapsizes[i] = gg\n                elif gg:\n                    gapsizes[i] = min(gs, gg)\n\n        print(gapsizes)\n        # Modify AGP\n        i = 0\n        for x in components:\n            if x.is_gap:\n                x.gap_length = gapsizes[i] or minsize\n                x.component_type = 'U' if x.gap_length == 100 else 'N'\n                i += 1\n            print(x, file=fw)\n\n    fw.close()\n    reindex([outagpfile, \"--inplace\"])", "response": "%prog estimategaps input. bed"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef summary(args):\n    p = OptionParser(summary.__doc__)\n    p.set_table(sep=\"|\", align=True)\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    inputbed, scaffolds = args\n    pf = inputbed.rsplit(\".\", 1)[0]\n    mapbed = pf + \".bed\"\n    chr_agp = pf + \".chr.agp\"\n    sep = opts.sep\n    align = opts.align\n    cc = Map(mapbed)\n    mapnames = cc.mapnames\n    s = Sizes(scaffolds)\n    total, l50, n50 = s.summary\n    r = {}\n    maps = []\n\n    fw = must_open(opts.outfile, \"w\")\n    print(\"*** Summary for each individual map ***\", file=fw)\n    for mapname in mapnames:\n        markers = [x for x in cc if x.mapname == mapname]\n        ms = MapSummary(markers, l50, s)\n        r[\"Linkage Groups\", mapname] = ms.num_lgs\n        ms.export_table(r, mapname, total)\n        maps.append(ms)\n    print(tabulate(r, sep=sep, align=align), file=fw)\n\n    r = {}\n    agp = AGP(chr_agp)\n    print(\"*** Summary for consensus map ***\", file=fw)\n    consensus_scaffolds = set(x.component_id for x in agp if not x.is_gap)\n    oriented_scaffolds = set(x.component_id for x in agp \\\n                            if (not x.is_gap) and x.orientation != '?')\n    unplaced_scaffolds = set(s.mapping.keys()) - consensus_scaffolds\n\n    for mapname, sc in ((\"Anchored\", consensus_scaffolds),\n                    (\"Oriented\", oriented_scaffolds),\n                    (\"Unplaced\", unplaced_scaffolds)):\n        markers = [x for x in cc if x.seqid in sc]\n        ms = MapSummary(markers, l50, s, scaffolds=sc)\n        ms.export_table(r, mapname, total)\n    print(tabulate(r, sep=sep, align=align), file=fw)", "response": "%prog summary input. bed scaffolds. fasta"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nassigns scaffolds to the right and left - most linkage nodes.", "response": "def assign_order(self):\n        \"\"\"\n        The goal is to assign scaffold orders. To help order the scaffolds, two\n        dummy node, START and END, mark the ends of the chromosome. We connect\n        START to each scaffold (directed), and each scaffold to END.\n        \"\"\"\n        linkage_groups = self.linkage_groups\n        for mlg in linkage_groups:\n            mapname = mlg.mapname\n            if mapname == self.pivot:\n                pivot_position = mlg.position\n\n        for mlg in linkage_groups:\n            position = mlg.position\n            # Flip order if path goes in the opposite direction to the pivot\n            common = []\n            for a, ap in position.items():\n                if a not in pivot_position:\n                    continue\n                pp = pivot_position[a]\n                common.append((ap, pp))\n\n            mlg.rho = get_rho(common)\n            if mlg.rho < 0:\n                mlg.path = mlg.path[::-1]\n\n            mlg.populate_pairwise_distance()\n\n        # Preparation of TSP\n        distances = defaultdict(list)\n        for mlg in linkage_groups:\n            mapname = mlg.mapname\n            position = mlg.position\n            length = mlg.length\n            path = mlg.path\n            rho = mlg.rho\n            dd = mlg.distances\n            for a, b in combinations(path, 2):\n                d = dd[a, b]\n                distances[a, b].append((d, mapname))\n            for p in path:\n                adist, bdist = position[p], length - position[p]\n                if rho < 0:\n                    adist, bdist = bdist, adist\n                distances[START, p].append((adist, mapname))\n                distances[p, END].append((bdist, mapname))\n\n        self.distances = distances\n        tour = self.distances_to_tour()\n        return tour"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_orientation(self, si, sj):\n        if not si or not sj:\n            return 0\n        # Same orientation configuration\n        a = lms(si + sj)\n        b = lms(sj + si)\n        # Opposite orientation configuration\n        c = lms(si + sj[::-1])\n        d = lms(sj[::-1] + si)\n        return max(a, b)[0] - max(c, d)[0]", "response": "Compute the orientation of the two number series."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fix_tour(self, tour):\n        scaffolds, oos = zip(*tour)\n        keep = set()\n        for mlg in self.linkage_groups:\n            lg = mlg.lg\n            for s, o in tour:\n                i = scaffolds.index(s)\n                L = [self.get_series(lg, x, xo) for x, xo in tour[:i]]\n                U = [self.get_series(lg, x, xo) for x, xo in tour[i + 1:]]\n                L, U = list(flatten(L)), list(flatten(U))\n                M = self.get_series(lg, s, o)\n                score_with = lms(L + M + U)[0]\n                score_without = lms(L + U)[0]\n                assert score_with >= score_without\n                if score_with > score_without:\n                    keep.add(s)\n        dropped = len(tour) - len(keep)\n        logging.debug(\"Dropped {0} minor scaffolds\".format(dropped))\n        return [(s, o) for (s, o) in tour if s in keep]", "response": "Test each scaffold if dropping does not decrease LMS."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fix_orientation(self, tour):\n        orientations = dict(tour)  # old configuration here\n        scaffold_oo = defaultdict(list)\n        scaffolds, oos = zip(*tour)\n        for mlg in self.linkage_groups:\n            lg = mlg.lg\n            mapname = mlg.mapname\n            for s, o in tour:\n                i = scaffolds.index(s)\n                L = [self.get_series(lg, x, xo) for x, xo in tour[:i]]\n                U = [self.get_series(lg, x, xo) for x, xo in tour[i + 1:]]\n                L, U = list(flatten(L)), list(flatten(U))\n                M = self.get_series(lg, s)\n                plus = lms(L + M + U)\n                minus = lms(L + M[::-1] + U)\n                d = plus[0] - minus[0]\n                if not d:\n                    continue\n                scaffold_oo[s].append((d, mapname))  # reset orientation\n\n        fixed = 0\n        for s, v in scaffold_oo.items():\n            d = self.weighted_mean(v)\n            old_d = orientations[s]\n            new_d = np.sign(d)\n            if new_d != old_d:\n                orientations[s] = new_d\n                fixed += 1\n\n        tour = [(x, orientations[x]) for x in scaffolds]\n        logging.debug(\"Fixed orientations for {0} scaffolds.\".format(fixed))\n        return tour", "response": "Fix the orientation of each scaffold if flipping will increass longest monotonic chain length."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nperform a single spin", "response": "def spin(self):\n        \"\"\" Perform a single spin \"\"\"\n\n        for x in self.spinchars:\n            self.string = self.msg + \"...\\t\" + x + \"\\r\"\n            self.out.write(self.string.encode('utf-8'))\n            self.out.flush()\n            time.sleep(self.waittime)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes unique nodes for sequence graph.", "response": "def make_sequence(seq, name=\"S\"):\n    \"\"\"\n    Make unique nodes for sequence graph.\n    \"\"\"\n    return [\"{}_{}_{}\".format(name, i, x) for i, x in enumerate(seq)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef zip_sequences(G, allseqs, color=\"white\"):\n    for s in zip(*allseqs):\n        groups = defaultdict(list)\n        for x in s:\n            part = x.split('_', 1)[1]\n            groups[part].append(x)\n        for part, g in groups.items():\n            with G.subgraph(name=\"cluster_\" + part) as c:\n                for x in g:\n                    c.node(x)\n                c.attr(style=\"invis\")", "response": "Fuse certain nodes together with same data except for the\n    sequence name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unescape(s, unicode_action=\"replace\"):\n    import HTMLParser\n    hp = HTMLParser.HTMLParser()\n    s = hp.unescape(s)\n    s = s.encode('ascii', unicode_action)\n    s = s.replace(\"\\n\", \"\").strip()\n    return s", "response": "Unescape HTML strings and convert &amp ; etc."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the longest unique prefix for filename when compared against a list of filenames.", "response": "def longest_unique_prefix(query, targets, remove_self=True):\n    \"\"\"\n    Find the longest unique prefix for filename, when compared against a list of\n    filenames. Useful to simplify file names in a pool of files. See usage in\n    formats.fasta.pool().\n    \"\"\"\n    query = op.basename(query)\n    targets = [op.basename(x) for x in targets]\n    prefix_lengths = [len(op.commonprefix([query, name])) for name in targets]\n    if remove_self and len(query) in prefix_lengths:\n        prefix_lengths.remove(len(query))\n    longest_length = max(prefix_lengths)\n    return query[:longest_length + 1]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_exists(filename, oappend=False):\n    if op.exists(filename):\n        if oappend:\n            return oappend\n        logging.error(\"`{0}` found, overwrite (Y/N)?\".format(filename))\n        overwrite = (raw_input() == 'Y')\n    else:\n        overwrite = True\n\n    return overwrite", "response": "Check if a file exists and overwrite it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nopen a file and return a filehandle.", "response": "def must_open(filename, mode=\"r\", checkexists=False, skipcheck=False, \\\n            oappend=False):\n    \"\"\"\n    Accepts filename and returns filehandle.\n\n    Checks on multiple files, stdin/stdout/stderr, .gz or .bz2 file.\n    \"\"\"\n    if isinstance(filename, list):\n        assert \"r\" in mode\n\n        if filename[0].endswith((\".gz\", \".bz2\")):\n            filename = \" \".join(filename)  # allow opening multiple gz/bz2 files\n        else:\n            import fileinput\n            return fileinput.input(filename)\n\n    if filename.startswith(\"s3://\"):\n        from jcvi.utils.aws import pull_from_s3\n        filename = pull_from_s3(filename)\n\n    if filename in (\"-\", \"stdin\"):\n        assert \"r\" in mode\n        fp = sys.stdin\n\n    elif filename == \"stdout\":\n        assert \"w\" in mode\n        fp = sys.stdout\n\n    elif filename == \"stderr\":\n        assert \"w\" in mode\n        fp = sys.stderr\n\n    elif filename == \"tmp\" and mode == \"w\":\n        from tempfile import NamedTemporaryFile\n        fp = NamedTemporaryFile(delete=False)\n\n    elif filename.endswith(\".gz\"):\n        if 'r' in mode:\n            cmd = \"gunzip -c {0}\".format(filename)\n            fp = popen(cmd, debug=False)\n        elif 'w' in mode:\n            import gzip\n            fp = gzip.open(filename, mode)\n\n    elif filename.endswith(\".bz2\"):\n        if 'r' in mode:\n            cmd = \"bzcat {0}\".format(filename)\n            fp = popen(cmd, debug=False)\n        elif 'w' in mode:\n            import bz2\n            fp = bz2.BZ2File(filename, mode)\n\n    else:\n        if checkexists:\n            assert mode == \"w\"\n            overwrite = (not op.exists(filename)) if skipcheck \\\n                        else check_exists(filename, oappend)\n            if overwrite:\n                if oappend:\n                    fp = open(filename, \"a\")\n                else:\n                    fp = open(filename, \"w\")\n            else:\n                logging.debug(\"File `{0}` already exists. Skipped.\"\\\n                        .format(filename))\n                return None\n        else:\n            fp = open(filename, mode)\n\n    return fp"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_block(handle, signal):\n    signal_len = len(signal)\n    it = (x[1] for x in groupby(handle,\n        key=lambda row: row.strip()[:signal_len] == signal))\n    found_signal = False\n    for header in it:\n        header = list(header)\n        for h in header[:-1]:\n            h = h.strip()\n            if h[:signal_len] != signal:\n                continue\n            yield h, []  # Header only, no contents\n        header = header[-1].strip()\n        if header[:signal_len] != signal:\n            continue\n        found_signal = True\n        seq = list(s.strip() for s in next(it))\n        yield header, seq\n\n    if not found_signal:\n        handle.seek(0)\n        seq = list(s.strip() for s in handle)\n        yield None, seq", "response": "Read a single block from a file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_number(s, cast=int):\n    import string\n    d = \"\".join(x for x in str(s) if x in string.digits)\n    return cast(d)", "response": "Try to get a number out of a string and cast it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef group(args):\n    from jcvi.utils.cbook import AutoVivification\n    from jcvi.utils.grouper import Grouper\n\n    p = OptionParser(group.__doc__)\n    p.set_sep()\n    p.add_option(\"--groupby\", default=None, type='int',\n                 help=\"Default column to groupby [default: %default]\")\n    p.add_option(\"--groupsep\", default=',',\n                 help=\"Separator to join the grouped elements [default: `%default`]\")\n    p.add_option(\"--nouniq\", default=False, action=\"store_true\",\n                 help=\"Do not uniqify the grouped elements [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    tabfile, = args\n    sep = opts.sep\n    groupby = opts.groupby\n    groupsep = opts.groupsep\n\n    cols = []\n    grouper = AutoVivification() if groupby is not None else Grouper()\n    fp = must_open(tabfile)\n    for row in fp:\n        row = row.rstrip()\n        atoms = row.split(sep)\n        if groupby is not None:\n            if len(cols) < len(atoms):\n                cols = [x for x in xrange(len(atoms))]\n            if groupby not in cols:\n                logging.error(\"groupby col index `{0}` is out of range\".format(groupby))\n                sys.exit()\n\n            key = atoms[groupby]\n            for col in cols:\n                if col == groupby:\n                    continue\n                if not grouper[key][col]:\n                    grouper[key][col] = [] if opts.nouniq else set()\n                if col < len(atoms):\n                    if groupsep in atoms[col]:\n                        for atom in atoms[col].split(groupsep):\n                            if opts.nouniq:\n                                grouper[key][col].append(atom)\n                            else:\n                                grouper[key][col].add(atom)\n                    else:\n                        if opts.nouniq:\n                            grouper[key][col].append(atoms[col])\n                        else:\n                            grouper[key][col].add(atoms[col])\n        else:\n            grouper.join(*atoms)\n\n    for key in grouper:\n        if groupby is not None:\n            line = []\n            for col in cols:\n                if col == groupby:\n                    line.append(key)\n                elif col in grouper[key].keys():\n                    line.append(groupsep.join(grouper[key][col]))\n                else:\n                    line.append(\"na\")\n            print(sep.join(line))\n        else:\n            print(groupsep.join(key))", "response": "%prog group tabfile > tabfile.grouped\n\n    Given a tab-delimited file, either group all elements within the file or\n    group the elements in the value column(s) based on the key (groupby) column\n\n    For example, convert this | into this\n    ---------------------------------------\n    a\t2    3    4           | a,2,3,4,5,6\n    a\t5    6                | b,7,8\n    b\t7    8                | c,9,10,11\n    c\t9                     |\n    c \t10   11               |\n\n    If grouping by a particular column,\n    convert this              | into this:\n    ---------------------------------------------\n    a\t2    3    4           | a\t2,5   3,6   4\n    a\t5    6                | b\t7     8\n    b\t7    8                | c\t9,10  11\n    c\t9                     |\n    c \t10   11               |\n\n    By default, it uniqifies all the grouped elements"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reorder(args):\n    import csv\n\n    p = OptionParser(reorder.__doc__)\n    p.set_sep()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    tabfile, order = args\n    sep = opts.sep\n    order = [int(x) - 1 for x in order.split(\",\")]\n    reader = csv.reader(must_open(tabfile), delimiter=sep)\n    writer = csv.writer(sys.stdout, delimiter=sep)\n    for row in reader:\n        newrow = [row[x] for x in order]\n        writer.writerow(newrow)", "response": "%prog reorder tabfile order"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef split(args):\n    p = OptionParser(split.__doc__)\n    mode_choices = (\"batch\", \"cycle\", \"optimal\")\n    p.add_option(\"--all\", default=False, action=\"store_true\",\n            help=\"split all records [default: %default]\")\n    p.add_option(\"--mode\", default=\"optimal\", choices=mode_choices,\n            help=\"Mode when splitting records [default: %default]\")\n    p.add_option(\"--format\", choices=(\"fasta\", \"fastq\", \"txt\", \"clust\"),\n            help=\"input file format [default: %default]\")\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    filename, outdir, N = args\n    fs = FileSplitter(filename, outputdir=outdir,\n                      format=opts.format, mode=opts.mode)\n\n    if opts.all:\n        logging.debug(\"option -all override N\")\n        N = fs.num_records\n    else:\n        N = min(fs.num_records, int(N))\n        assert N > 0, \"N must be > 0\"\n\n    logging.debug(\"split file into %d chunks\" % N)\n    fs.split(N)\n\n    return fs", "response": "%prog split file outdir N"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef join(args):\n    p = OptionParser(join.__doc__)\n    p.add_option(\"--column\", default=\"0\",\n                 help=\"0-based column id, multiple values allowed [default: %default]\")\n    p.set_sep(multiple=True)\n    p.add_option(\"--noheader\", default=False, action=\"store_true\",\n                 help=\"Do not print header [default: %default]\")\n    p.add_option(\"--na\", default=\"na\",\n                 help=\"Value for unjoined data [default: %default]\")\n    p.add_option(\"--compact\", default=False, action=\"store_true\",\n                 help=\"Do not repeat pivotal columns in output\")\n    p.add_option(\"--keysep\", default=\",\",\n                 help=\"specify separator joining multiple elements in the key column\"\n                 + \" of the pivot file [default: %default]\")\n    p.set_outfile()\n\n    opts, args = p.parse_args(args)\n    nargs = len(args)\n\n    keysep = opts.keysep\n    compact = opts.compact\n\n    if len(args) < 2:\n        sys.exit(not p.print_help())\n\n    na = opts.na\n    c = opts.column\n    if \",\" in c:\n        cc = [int(x) for x in c.split(\",\")]\n    else:\n        cc = [int(c)] * nargs\n\n    assert len(cc) == nargs, \"Column index number != File number\"\n\n    s = opts.sep\n    if \",\" in s:\n        ss = [x for x in s.split(\",\")]\n    else:\n        ss = [s] * nargs\n\n    assert len(ss) == nargs, \"column separator number != File number\"\n\n    # Maintain the first file line order, and combine other files into it\n    pivotfile = args[0]\n    files = [DictFile(f, keypos=c, valuepos=None, delimiter=s) \\\n                        for f, c, s in zip(args, cc, ss)]\n    otherfiles = files[1:]\n    # The header contains filenames\n    headers = []\n    for i, x in enumerate(files):\n        ncols = x.ncols\n        if i and compact:\n            ncols -= 1\n        headers += [op.basename(x.filename)] * ncols\n    header = \"\\t\".join(headers)\n\n    fp = must_open(pivotfile)\n    fw = must_open(opts.outfile, \"w\")\n    if not opts.noheader:\n        print(header, file=fw)\n\n    for row in fp:\n        row = row.rstrip()\n        atoms = row.split(ss[0])\n        newrow = atoms\n        key = atoms[cc[0]]\n        keys = key.split(keysep) if keysep in key else [key]\n        for d in otherfiles:\n            drows = list()\n            for key in keys:\n                krow = d.get(key, [na] * d.ncols)\n                if compact:\n                    krow.pop(d.keypos)\n                drows.append(krow)\n            drow = [keysep.join(x) for x in list(zip(*drows))]\n            newrow += drow\n        print(\"\\t\".join(newrow), file=fw)", "response": "%prog join file1. txt pivotfile file2. txt..."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _batch_iterator(self, N=1):\n        batch_size = math.ceil(self.num_records / float(N))\n        handle = self._open(self.filename)\n        while True:\n            batch = list(islice(handle, batch_size))\n            if not batch:\n                break\n            yield batch", "response": "Returns N lists of records from a file handle."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsplits the records into N records and returns the new records.", "response": "def split(self, N, force=False):\n        \"\"\"\n        There are two modes of splitting the records\n        - batch: splitting is sequentially to records/N chunks\n        - cycle: placing each record in the splitted files and cycles\n\n        use `cycle` if the len of the record is not evenly distributed\n        \"\"\"\n        mode = self.mode\n        assert mode in (\"batch\", \"cycle\", \"optimal\")\n        logging.debug(\"set split mode=%s\" % mode)\n\n        self.names = self.__class__.get_names(self.filename, N)\n        if self.outputdir:\n            self.names = [op.join(self.outputdir, x) for x in self.names]\n\n        if not need_update(self.filename, self.names) and not force:\n            logging.error(\"file %s already existed, skip file splitting\" % \\\n                    self.names[0])\n            return\n\n        filehandles = [open(x, \"w\") for x in self.names]\n\n        if mode == \"batch\":\n            for batch, fw in zip(self._batch_iterator(N), filehandles):\n                count = self.write(fw, batch)\n                logging.debug(\"write %d records to %s\" % (count, fw.name))\n\n        elif mode == \"cycle\":\n            handle = self._open(self.filename)\n            for record, fw in zip(handle, cycle(filehandles)):\n                count = self.write(fw, [record])\n\n        elif mode == \"optimal\":\n            \"\"\"\n            This mode is based on Longest Processing Time (LPT) algorithm:\n\n            A simple, often-used algorithm is the LPT algorithm (Longest\n            Processing Time) which sorts the jobs by its processing time and\n            then assigns them to the machine with the earliest end time so far.\n            This algorithm achieves an upper bound of 4/3 - 1/(3m) OPT.\n\n            Citation: <http://en.wikipedia.org/wiki/Multiprocessor_scheduling>\n            \"\"\"\n            endtime = [0] * N\n            handle = self._open(self.filename)\n            for record in handle:\n                mt, mi = min((x, i) for (i, x) in enumerate(endtime))\n                fw = filehandles[mi]\n                count = self.write(fw, [record])\n                endtime[mi] += len(record)\n\n        for fw in filehandles:\n            fw.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning `cap3` on a single multi FASTA file containing reads or a folder containing several multi FASTA files. Allows for tweaking of `cap3` parameters max_gap_len, ovl_pct_id, etc.", "response": "def assemble(args):\n    \"\"\"\n    Run `cap3` on a single multi FASTA file containing reads or a folder containing several\n    multi FASTA files. Allows for tweaking of `cap3` parameters max_gap_len, ovl_pct_id, etc.\n    \"\"\"\n    p = OptionParser(assemble.__doc__)\n    g1 = OptionGroup(p, \"Input file options (required)\",\n            \"Note: Please choose from and provide values for one of the following parameters\")\n    g1.add_option(\"--input_file\", default=None,\n            help=\"input file of reads [default: %default]\")\n    g1.add_option(\"--input_folder\", default=None,\n            help=\"input folder containing multi FASTA files of reads [default: %default]\")\n    g1.add_option(\"--input_file_list\", default=None,\n            help=\"list file containing paths to multi FASTA files of reads [default: %default]\")\n    p.add_option_group(g1)\n\n    g2 = OptionGroup(p, \"Optional parameters\",\n            \"Note: If not specified, `cap3` defaults will be used\")\n    g2.add_option(\"-f\", \"--max_gap_len\", default=20, type=\"int\",\n            help=\"maximum gap length in any overlap [default: %default]\\n\" +\\\n                 \"Same as cap3 `-f` parameter.\")\n    g2.add_option(\"-p\", \"--ovl_pct_id\", default=90, type=\"int\",\n            help=\"overlap percent identity cutoff [default: %default]\\n\" +\\\n                 \"Same as cap3 `-p` parameter.\")\n    g2.add_option(\"-s\", \"--ovl_sim_score\", default=900, type=\"int\",\n            help=\"overlap similarity score cutoff [default: %default]\\n\" +\\\n                 \"Same as cap3 `-s` parameter.\")\n    g2.add_option(\"-x\", \"--prefix\", dest=\"prefix\", default=\"cap3\",\n            help=\"prefix string for output file name [default: %default]\")\n    p.add_option_group(g2)\n\n    p.set_params()\n\n    opts, args = p.parse_args(args)\n\n    if opts.max_gap_len and opts.max_gap_len <= 1:\n        logging.error(\"--max_gap_len should be > 1\")\n        sys.exit()\n    elif opts.ovl_pct_id and opts.ovl_pct_id <= 65:\n        logging.error(\"--ovl_pct_id should be > 65\")\n        sys.exit()\n    elif opts.ovl_sim_score and opts.ovl_sim_score <= 250:\n        logging.error(\"--ovl_sim_score should be > 250\")\n        sys.exit()\n\n    file_list = []\n    if opts.input_file_list:\n        if not op.isfile(opts.input_file_list):\n            logging.error(\"Input file list {0} does not exist\".format(opts.input_file_list))\n            sys.exit()\n        with open(opts.input_file_list, 'r') as f:\n            file_list = f.read().splitlines()\n    elif opts.input_folder:\n        if not op.isdir(opts.input_folder):\n            logging.error(\"Input folder {0} does not exist\".format(opts.input_folder))\n            sys.exit()\n\n        file_list = [file for file in os.listdir(opts.input_folder) \\\n                if file.lower().endswith(('.fa', '.fasta'))]\n        folder = opts.input_folder\n        folder = folder.rstrip('/')\n        for i in xrange(len(file_list)):\n            file_list[i] = folder + \"/\" + file_list[i]\n    elif opts.input_file:\n        file_list.append(opts.input_file)\n    else:\n        logging.error(\"Please specify one of the options for input files\")\n        sys.exit(not p.print_help())\n\n    if len(file_list) == 0:\n        logging.warning(\"List of files to process is empty. Please check your input!\")\n        sys.exit()\n\n    for file in file_list:\n        if not op.isfile(file):\n            logging.warning(\"Input file {0} does not exist\".format(file))\n        else:\n            cmd = \"cap3 {0} -f {1} -p {2} -s {3} -x {4}\".format(file, opts.max_gap_len, \\\n                    opts.ovl_pct_id, opts.ovl_sim_score, opts.prefix)\n            if opts.extra:\n                cmd += \" {0}\".format(opts.extra)\n            logfile = \"{0}.{1}.log\".format(file, opts.prefix)\n\n            sh(cmd, outfile=logfile)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract(args):\n    p = OptionParser(extract.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    idsfile, sizesfile = args\n    sizes = Sizes(sizesfile).mapping\n    fp = open(idsfile)\n    for row in fp:\n        name = row.strip()\n        size = sizes[name]\n        print(\"\\t\".join(str(x) for x in (name, size)))", "response": "%prog extract idsfile sizesfile"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _reversedict(d):\n    return dict(list(zip(list(d.values()), list(d.keys()))))", "response": "Internal helper for generating reverse mappings ; given a\n    dictionary returns a new dictionary with keys and values swapped."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef normalize_hex(hex_value):\n    try:\n        hex_digits = HEX_COLOR_RE.match(hex_value).groups()[0]\n    except AttributeError:\n        raise ValueError(\"'%s' is not a valid hexadecimal color value.\" % hex_value)\n    if len(hex_digits) == 3:\n        hex_digits = ''.join([2 * s for s in hex_digits])\n    return '#%s' % hex_digits.lower()", "response": "Normalizes a hexadecimal color value to the following form and\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _normalize_percent_rgb(value):\n    percent = value.split('%')[0]\n    percent = float(percent) if '.' in percent else int(percent)\n\n    if 0 <= percent <= 100:\n        return '%s%%' % percent\n    if percent < 0:\n        return '0%'\n    if percent > 100:\n        return '100%'", "response": "Normalize value for use in a percentage rgb triplet."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef name_to_hex(name, spec='css3'):\n    if spec not in SUPPORTED_SPECIFICATIONS:\n        raise TypeError(\"'%s' is not a supported specification for color name lookups; supported specifications are: %s.\" % (spec,\n                                                                                                                             ', '.join(SUPPORTED_SPECIFICATIONS)))\n    normalized = name.lower()\n    try:\n        hex_value = globals()['%s_names_to_hex' % spec][normalized]\n    except KeyError:\n        raise ValueError(\"'%s' is not defined as a named color in %s.\" % (name, spec))\n    return hex_value", "response": "Convert a color name to a normalized hexadecimal color value."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a hexadecimal color value to its corresponding normalized color name.", "response": "def hex_to_name(hex_value, spec='css3'):\n    \"\"\"\n    Convert a hexadecimal color value to its corresponding normalized\n    color name, if any such name exists.\n\n    The optional keyword argument ``spec`` determines which\n    specification's list of color names will be used; valid values are\n    ``html4``, ``css2``, ``css21`` and ``css3``, and the default is\n    ``css3``.\n\n    The hexadecimal value will be normalized before being looked up,\n    and when no color name for the value is found in the given\n    specification, ``ValueError`` is raised.\n\n    Examples:\n\n    >>> hex_to_name('#ffffff')\n    'white'\n    >>> hex_to_name('#fff')\n    'white'\n    >>> hex_to_name('#000080')\n    'navy'\n    >>> hex_to_name('#daa520')\n    'goldenrod'\n    >>> hex_to_name('#daa520', spec='html4')\n    Traceback (most recent call last):\n        ...\n    ValueError: '#daa520' has no defined color name in html4.\n    >>> hex_to_name('#daa520', spec='css5')\n    Traceback (most recent call last):\n        ...\n    TypeError: 'css5' is not a supported specification for color name lookups; supported specifications are: html4, css2, css21, css3.\n\n    \"\"\"\n    if spec not in SUPPORTED_SPECIFICATIONS:\n        raise TypeError(\"'%s' is not a supported specification for color name lookups; supported specifications are: %s.\" % (spec,\n                                                                                                                             ', '.join(SUPPORTED_SPECIFICATIONS)))\n    normalized = normalize_hex(hex_value)\n    try:\n        name = globals()['%s_hex_to_names' % spec][normalized]\n    except KeyError:\n        raise ValueError(\"'%s' has no defined color name in %s.\" % (hex_value, spec))\n    return name"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a hexadecimal color value to a 3 - tuple suitable for use in an rgb triplet specifying that color.", "response": "def hex_to_rgb(hex_value):\n    \"\"\"\n    Convert a hexadecimal color value to a 3-tuple of integers\n    suitable for use in an ``rgb()`` triplet specifying that color.\n\n    The hexadecimal value will be normalized before being converted.\n\n    Examples:\n\n    >>> hex_to_rgb('#fff')\n    (255, 255, 255)\n    >>> hex_to_rgb('#000080')\n    (0, 0, 128)\n\n    \"\"\"\n    hex_digits = normalize_hex(hex_value)\n    return tuple([int(s, 16) for s in (hex_digits[1:3], hex_digits[3:5], hex_digits[5:7])])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rgb_to_rgb_percent(rgb_triplet):\n    # In order to maintain precision for common values,\n    # 256 / 2**n is special-cased for values of n\n    # from 0 through 4, as well as 0 itself.\n    specials = {255: '100%', 128: '50%', 64: '25%',\n                 32: '12.5%', 16: '6.25%', 0: '0%'}\n    return tuple([specials.get(d, '%.02f%%' % ((d / 255.0) * 100)) \\\n                  for d in normalize_integer_triplet(rgb_triplet)])", "response": "Converts a 3 - tuple of integers suitable for use in an rgb ( rgb ) rgb tuple to a 3 - tuple of percentages suitable for use in an rgb ( rgb ) rgb tuple."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef color_diff(rgb1, rgb2):\n    import numpy as np\n    from skimage.color import rgb2lab, deltaE_cmc\n\n    rgb1 = np.array(rgb1, dtype=\"float64\").reshape(1, 1, 3) / 255.\n    rgb2 = np.array(rgb2, dtype=\"float64\").reshape(1, 1, 3) / 255.\n    lab1 = rgb2lab(rgb1)\n    lab2 = rgb2lab(rgb2)\n    return deltaE_cmc(lab1, lab2, kL=2, kC=1)[0, 0]", "response": "Calculate distance between two RGB colors."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef closest_color(requested_color):\n    logging.disable(logging.DEBUG)\n    colors = []\n    for key, name in css3_hex_to_names.items():\n        diff = color_diff(hex_to_rgb(key), requested_color)\n        colors.append((diff, name))\n    logging.disable(logging.NOTSET)\n    min_diff, min_color = min(colors)\n\n    return min_color", "response": "Find closest color name for the requested RGB tuple."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef offdiag(args):\n    p = OptionParser(offdiag.__doc__)\n    p.set_beds()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    anchorsfile, = args\n    qbed, sbed, qorder, sorder, is_self = check_beds(anchorsfile, p, opts)\n\n    fp = open(anchorsfile)\n    pf = \"-\".join(anchorsfile.split(\".\")[:2])\n    header = \"Block-id|Napus|Diploid|Napus-chr|Diploid-chr|RBH?\".split(\"|\")\n    print(\"\\t\".join(header))\n    i = -1\n    for row in fp:\n        if row[0] == '#':\n            i += 1\n            continue\n        q, s, score = row.split()\n        rbh = 'no' if score[-1] == 'L' else 'yes'\n        qi, qq = qorder[q]\n        si, ss = sorder[s]\n        oqseqid = qseqid = qq.seqid\n        osseqid = sseqid = ss.seqid\n        sseqid = sseqid.split(\"_\")[0][-3:]\n        if qseqid[0] == 'A':\n            qseqid = qseqid[-3:]       # A09 => A09\n        elif qseqid[0] == 'C':\n            qseqid = 'C0' + qseqid[-1]  # C9 => C09\n        else:\n            continue\n        if qseqid == sseqid or sseqid[-2:] == 'nn':\n            continue\n        block_id = pf + \"-block-{0}\".format(i)\n        print(\"\\t\".join((block_id, q, s, oqseqid, osseqid, rbh)))", "response": "%prog offdiag diploid.napus.1x1.lifted.anchors\n\n    Find gene pairs that are off diagnoal. \"Off diagonal\" are the pairs that are\n    not on the orthologous chromosomes. For example, napus chrA01 and brapa A01."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef diff(args):\n    from jcvi.utils.cbook import SummaryStats\n\n    p = OptionParser(diff.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    simplefile, = args\n    fp = open(simplefile)\n    data = [x.split() for x in fp]\n    spans = []\n    for block_id, ab in groupby(data[1:], key=lambda x: x[0]):\n        a, b = list(ab)\n        aspan, bspan = a[4], b[4]\n        aspan, bspan = int(aspan), int(bspan)\n        spans.append((aspan, bspan))\n    aspans, bspans = zip(*spans)\n    dspans = [b - a for a, b, in spans]\n    s = SummaryStats(dspans)\n    print(\"For a total of {0} blocks:\".format(len(dspans)), file=sys.stderr)\n    print(\"Sum of A: {0}\".format(sum(aspans)), file=sys.stderr)\n    print(\"Sum of B: {0}\".format(sum(bspans)), file=sys.stderr)\n    print(\"Sum of Delta: {0} ({1})\".format(sum(dspans), s), file=sys.stderr)", "response": "%prog diff simplefile\n\n    Calculate difference of pairwise syntenic regions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef estimate_size(accns, bed, order, conservative=True):\n    accns = [order[x] for x in accns]\n    ii, bb = zip(*accns)\n    mini, maxi = min(ii), max(ii)\n    if not conservative: # extend one gene\n        mini -= 1\n        maxi += 1\n    minb = bed[mini]\n    maxb = bed[maxi]\n    assert minb.seqid == maxb.seqid\n    distmode = \"ss\" if conservative else \"ee\"\n    ra = (minb.seqid, minb.start, minb.end, \"+\")\n    rb = (maxb.seqid, maxb.start, maxb.end, \"+\")\n\n    dist, orientation = range_distance(ra, rb, distmode=distmode)\n    assert dist != -1\n    return dist", "response": "Estimate the bp length for the deletion tracks indicated by the gene accns."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef segment(args):\n    from jcvi.formats.base import SetFile\n\n    p = OptionParser(segment.__doc__)\n    p.add_option(\"--chain\", default=1, type=\"int\",\n                 help=\"Allow next N genes to be chained [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    idsfile, bedfile = args\n    bed = Bed(bedfile)\n    order = bed.order\n    ids = SetFile(idsfile)\n    losses = Grouper()\n    skip = opts.chain\n    for i, a in enumerate(bed):\n        a = a.accn\n        for j in xrange(i + 1, i + 1 + skip):\n            if j >= len(bed):\n                break\n            b = bed[j].accn\n            if a in ids:\n                losses.join(a, a)\n            if a in ids and b in ids:\n                losses.join(a, b)\n\n    losses = list(losses)\n    singletons = [x for x in losses if len(x) == 1]\n    segments = [x for x in losses if len(x) > 1]\n    ns, nm, nt = len(singletons), len(segments), len(losses)\n    assert ns + nm == nt\n\n    # Summary for all segments\n    for x in sorted(singletons) + sorted(segments):\n        print(\"\\t\".join(str(x) for x in (\"|\".join(sorted(x)), len(x),\n                        estimate_size(x, bed, order))))\n\n    # Find longest segment stretch\n    if segments:\n        mx, maxsegment = max([(len(x), x) for x in segments])\n        print(\"Longest stretch: run of {0} genes\".format(mx), file=sys.stderr)\n        print(\"  {0}\".format(\"|\".join(sorted(maxsegment))), file=sys.stderr)\n        seg_asize = sum(estimate_size(x, bed, order) for x in segments)\n        seg_bsize = sum(estimate_size(x, bed, order, conservative=False) \\\n                             for x in segments)\n    else:\n        seg_asize = seg_bsize = 0\n\n    sing_asize = sum(estimate_size(x, bed, order) for x in singletons)\n    sing_bsize = sum(estimate_size(x, bed, order, conservative=False) \\\n                           for x in singletons)\n    total_asize = sing_asize + seg_asize\n    total_bsize = sing_bsize + seg_bsize\n    print(\"Singleton ({0}): {1} - {2} bp\".\\\n                         format(ns, sing_asize, sing_bsize), file=sys.stderr)\n    print(\"Segment ({0}): {1} - {2} bp\".\\\n                         format(nm, seg_asize, seg_bsize), file=sys.stderr)\n    print(\"Total ({0}): {1} - {2} bp\".\\\n                         format(nt, total_asize, total_bsize), file=sys.stderr)\n    print(\"Average ({0}): {1} bp\".\\\n                         format(nt, (total_asize + total_bsize) / 2), file=sys.stderr)", "response": "%prog segment loss. ids bedfile"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef gaps(args):\n    from jcvi.formats.base import DictFile\n    from jcvi.apps.base import popen\n    from jcvi.utils.cbook import percentage\n\n    p = OptionParser(gaps.__doc__)\n    p.add_option(\"--bdist\", default=0, type=\"int\",\n                 help=\"Base pair distance [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    idsfile, frfile, gapsbed = args\n    bdist = opts.bdist\n    d =  DictFile(frfile, keypos=1, valuepos=2)\n    bedfile = idsfile + \".bed\"\n    fw = open(bedfile, \"w\")\n    fp = open(idsfile)\n    total = 0\n    for row in fp:\n        id = row.strip()\n        hit = d[id]\n        tag, pos = get_tag(hit, None)\n        seqid, start, end = pos\n        start, end = max(start - bdist, 1), end + bdist\n        print(\"\\t\".join(str(x) for x in (seqid, start - 1, end, id)), file=fw)\n        total += 1\n    fw.close()\n\n    cmd = \"intersectBed -a {0} -b {1} -v | wc -l\".format(bedfile, gapsbed)\n    not_in_gaps = popen(cmd).read()\n    not_in_gaps = int(not_in_gaps)\n    in_gaps = total - not_in_gaps\n    print(\"Ids in gaps: {1}\".\\\n            format(total, percentage(in_gaps, total)), file=sys.stderr)", "response": "%prog gaps idsfile fractionationfile gapsbed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef summary(args):\n    from jcvi.formats.base import DictFile\n    from jcvi.utils.cbook import percentage, Registry\n\n    p = OptionParser(summary.__doc__)\n    p.add_option(\"--extra\", help=\"Cross with extra tsv file [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    frfile, statusfile = args\n    status = DictFile(statusfile)\n    fp = open(frfile)\n    registry = Registry()  # keeps all the tags for any given gene\n    for row in fp:\n        seqid, gene, tag = row.split()\n        if tag == '.':\n            registry[gene].append(\"outside\")\n        else:\n            registry[gene].append(\"inside\")\n            if tag[0] == '[':\n                registry[gene].append(\"no_syntenic_model\")\n                if tag.startswith(\"[S]\"):\n                    registry[gene].append(\"[S]\")\n                    gstatus = status.get(gene, None)\n                    if gstatus == 'complete':\n                        registry[gene].append(\"complete\")\n                    elif gstatus == 'pseudogene':\n                        registry[gene].append(\"pseudogene\")\n                    elif gstatus == 'partial':\n                        registry[gene].append(\"partial\")\n                    else:\n                        registry[gene].append(\"gmap_fail\")\n                elif tag.startswith(\"[NS]\"):\n                    registry[gene].append(\"[NS]\")\n                    if \"random\" in tag or \"Scaffold\" in tag:\n                        registry[gene].append(\"random\")\n                    else:\n                        registry[gene].append(\"real_ns\")\n                elif tag.startswith(\"[NF]\"):\n                    registry[gene].append(\"[NF]\")\n            else:\n                registry[gene].append(\"syntenic_model\")\n\n    inside = registry.count(\"inside\")\n    outside = registry.count(\"outside\")\n    syntenic = registry.count(\"syntenic_model\")\n    non_syntenic = registry.count(\"no_syntenic_model\")\n    s = registry.count(\"[S]\")\n    ns = registry.count(\"[NS]\")\n    nf = registry.count(\"[NF]\")\n    complete = registry.count(\"complete\")\n    pseudogene = registry.count(\"pseudogene\")\n    partial = registry.count(\"partial\")\n    gmap_fail = registry.count(\"gmap_fail\")\n    random = registry.count(\"random\")\n    real_ns = registry.count(\"real_ns\")\n\n    complete_models = registry.get_tag(\"complete\")\n    pseudogenes = registry.get_tag(\"pseudogene\")\n    partial_deletions = registry.get_tag(\"partial\")\n\n    m = \"{0} inside synteny blocks\\n\".format(inside)\n    m += \"{0} outside synteny blocks\\n\".format(outside)\n    m += \"{0} has syntenic gene\\n\".format(syntenic)\n    m += \"{0} lack syntenic gene\\n\".format(non_syntenic)\n    m += \"{0} has sequence match in syntenic location\\n\".format(s)\n    m += \"{0} has sequence match in non-syntenic location\\n\".format(ns)\n    m += \"{0} has sequence match in un-ordered scaffolds\\n\".format(random)\n    m += \"{0} has sequence match in real non-syntenic location\\n\".format(real_ns)\n    m += \"{0} has no sequence match\\n\".format(nf)\n    m += \"{0} syntenic sequence - complete model\\n\".format(percentage(complete, s))\n    m += \"{0} syntenic sequence - partial model\\n\".format(percentage(partial, s))\n    m += \"{0} syntenic sequence - pseudogene\\n\".format(percentage(pseudogene, s))\n    m += \"{0} syntenic sequence - gmap fail\\n\".format(percentage(gmap_fail, s))\n    print(m, file=sys.stderr)\n\n    aa = [\"complete_models\", \"partial_deletions\", \"pseudogenes\"]\n    bb = [complete_models, partial_deletions, pseudogenes]\n    for a, b in zip(aa, bb):\n        fw = open(a, \"w\")\n        print(\"\\n\".join(b), file=fw)\n        fw.close()\n\n    extra = opts.extra\n    if extra:\n        registry.update_from(extra)\n\n    fp.seek(0)\n    fw = open(\"registry\", \"w\")\n    for row in fp:\n        seqid, gene, tag = row.split()\n        ts = registry[gene]\n        print(\"\\t\".join((seqid, gene, tag, \"-\".join(ts))), file=fw)\n    fw.close()\n\n    logging.debug(\"Registry written.\")", "response": "%prog summary diploid. napi. fractionation gmap. status"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef napus(args):\n    from jcvi.utils.cbook import SummaryStats\n\n    p = OptionParser(napus.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    napusbed, brbo, dpnp = args\n    retention = {}\n    fp = open(dpnp)\n    for row in fp:\n        seqid, query, hit = row.split()\n        retention[query] = hit\n\n    order = Bed(napusbed).order\n\n    quartetsfile = \"quartets\"\n    fp = open(brbo)\n    fw = open(quartetsfile, \"w\")\n    AL = \"AN LOST\"\n    CL = \"CN LOST\"\n    for row in fp:\n        br, bo = row.split()\n        if '.' in (br, bo):\n            continue\n        an, cn = retention[br], retention[bo]\n        row = \"\\t\".join((br, bo, an, cn))\n        if '.' in (an, cn):\n            #print row\n            continue\n\n        # label loss candidates\n        antag, anrange = get_tag(an, order)\n        cntag, cnrange = get_tag(cn, order)\n\n        if range_overlap(anrange, cnrange):\n            if (antag, cntag) == (\"NS\", None):\n                row = row + \"\\t{0}|{1}\".format(AL, br)\n            if (antag, cntag) == (None, \"NS\"):\n                row = row + \"\\t{0}|{1}\".format(CL, bo)\n\n        print(row, file=fw)\n    fw.close()\n\n    logging.debug(\"Quartets and gene losses written to `{0}`.\".\\\n                    format(quartetsfile))\n\n    # Parse the quartets file to extract singletons vs.segmental losses\n    fp = open(quartetsfile)\n    fw = open(quartetsfile + \".summary\", \"w\")\n    data = [x.rstrip().split(\"\\t\") for x in fp]\n    skip = 1  # max distance between losses\n\n    g = Grouper()\n    losses = [(len(x) == 5) for x in data]\n    for i, d in enumerate(losses):\n        if not d:\n            continue\n        g.join(i, i)\n        itag = data[i][-1].split(\"|\")[0]\n        for j in xrange(i + 1, i + skip + 1):\n            jtag = data[j][-1].split(\"|\")[0]\n            if j < len(losses) and losses[j] and itag == jtag:\n                g.join(i, j)\n\n    losses = list(g)\n    singletons = [x for x in losses if len(x) == 1]\n    segments = [x for x in losses if len(x) > 1]\n    ns, nm = len(singletons), len(segments)\n    assert len(losses) == ns + nm\n\n    grab_tag = lambda pool, tag: \\\n            [x for x in pool if all(data[z][-1].startswith(tag) for z in x)]\n\n    an_loss_singletons = grab_tag(singletons, AL)\n    cn_loss_singletons = grab_tag(singletons, CL)\n    als, cls = len(an_loss_singletons), len(cn_loss_singletons)\n\n    an_loss_segments = grab_tag(segments, AL)\n    cn_loss_segments = grab_tag(segments, CL)\n    alm, clm = len(an_loss_segments), len(cn_loss_segments)\n    mixed = len(segments) - alm - clm\n    assert mixed == 0\n\n    logging.debug(\"Singletons: {0} (AN LOSS: {1}, CN LOSS: {2})\".\\\n                        format(ns, als, cls))\n    logging.debug(\"Segments: {0} (AN LOSS: {1}, CN LOSS: {2})\".\\\n                        format(nm, alm, clm))\n    print(SummaryStats([len(x) for x in losses]), file=sys.stderr)\n\n    for x in singletons + segments:\n        print(\"### LENGTH =\", len(x), file=fw)\n        for i in x:\n            print(\"\\t\".join(data[i]), file=fw)\n    fw.close()", "response": "%prog napus napus. bed brbo dpnp"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate(args):\n    from jcvi.formats.bed import intersectBed_wao\n\n    p = OptionParser(validate.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    fractionation, cdsbed = args\n    fp = open(fractionation)\n\n    sbed = \"S.bed\"\n    fw = open(sbed, \"w\")\n    for row in fp:\n        a, b, c = row.split()\n        if not c.startswith(\"[S]\"):\n            continue\n\n        tag, (seqid, start, end) = get_tag(c, None)\n        print(\"\\t\".join(str(x) for x in (seqid, start - 1, end, b)), file=fw)\n\n    fw.close()\n\n    pairs = {}\n    for a, b in intersectBed_wao(sbed, cdsbed):\n        if b is None:\n            continue\n        pairs[a.accn] = b.accn\n\n    validated = fractionation + \".validated\"\n    fw = open(validated, \"w\")\n    fp.seek(0)\n    fixed = 0\n    for row in fp:\n        a, b, c = row.split()\n        if b in pairs:\n            assert c.startswith(\"[S]\")\n            c = pairs[b]\n            fixed += 1\n\n        print(\"\\t\".join((a, b, c)), file=fw)\n\n    logging.debug(\"Fixed {0} [S] cases in `{1}`.\".format(fixed, validated))\n    fw.close()", "response": "%prog validate diploid. nation. cds. bed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter(args):\n    from jcvi.formats.fasta import Fasta, SeqIO\n\n    p = OptionParser(filter.__doc__)\n    p.add_option(\"--minsize\", default=2, type=\"int\",\n                 help=\"Minimum cluster size\")\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    fastafiles = args\n    minsize = opts.minsize\n    totalreads = totalassembled = 0\n    fw = must_open(opts.outfile, \"w\")\n    for i, fastafile in enumerate(fastafiles):\n        f = Fasta(fastafile, lazy=True)\n        pf = \"s{0:03d}\".format(i)\n        nreads = nsingletons = nclusters = 0\n        for desc, rec in f.iterdescriptions_ordered():\n            nclusters += 1\n            if desc.startswith(\"singleton\"):\n                nsingletons += 1\n                nreads += 1\n                continue\n            # consensus_for_cluster_0 with 63 sequences\n            name, w, size, seqs = desc.split()\n            assert w == \"with\"\n            size = int(size)\n            nreads += size\n            if size < minsize:\n                continue\n            rec.description = rec.description.split(None, 1)[-1]\n            rec.id = pf + \"_\" + rec.id\n            SeqIO.write(rec, fw, \"fasta\")\n        logging.debug(\"Scanned {0} clusters with {1} reads ..\".\\\n                       format(nclusters, nreads))\n        cclusters, creads = nclusters - nsingletons, nreads - nsingletons\n        logging.debug(\"Saved {0} clusters (min={1}) with {2} reads (avg:{3}) [{4}]\".\\\n                       format(cclusters, minsize, creads, creads / cclusters, pf))\n        totalreads += nreads\n        totalassembled += nreads - nsingletons\n    logging.debug(\"Total assembled: {0}\".\\\n                  format(percentage(totalassembled, totalreads)))", "response": "%prog filter fastafiles > new_fasta"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread complete PEB data from file.", "response": "def read_block(self, block):\n        \"\"\"Read complete PEB data from file.\n        \n        Argument:\n        Obj:block -- Block data is desired for.\n        \"\"\"\n        self.seek(block.file_offset)\n        return self._fhandle.read(block.size)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread LEB data from file", "response": "def read_block_data(self, block):\n        \"\"\"Read LEB data from file\n        \n        Argument:\n        Obj:block -- Block data is desired for.\n        \"\"\"\n        self.seek(block.file_offset + block.ec_hdr.data_offset)\n        buf = self._fhandle.read(block.size - block.ec_hdr.data_offset - block.vid_hdr.data_pad)\n        return buf"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts a list of UBI block objects from a file.", "response": "def extract_blocks(ubi):\n    \"\"\"Get a list of UBI block objects from file\n\n    Arguments:.\n    Obj:ubi    -- UBI object.\n    \n    Returns:\n    Dict -- Of block objects keyed by PEB number.\n    \"\"\"\n\n    blocks = {}\n    ubi.file.seek(ubi.file.start_offset)\n    peb_count = 0\n    cur_offset = 0\n    bad_blocks = []\n\n    # range instead of xrange, as xrange breaks > 4GB end_offset.\n    for i in range(ubi.file.start_offset, ubi.file.end_offset, ubi.file.block_size):\n        try:\n            buf = ubi.file.read(ubi.file.block_size)\n        except Exception as e:\n            if settings.warn_only_block_read_errors:\n                error(extract_blocks, 'Error', 'PEB: %s: %s' % (ubi.first_peb_num + peb_count, str(e)))\n                continue\n            else:\n                error(extract_blocks, 'Fatal', 'PEB: %s: %s' % (ubi.first_peb_num + peb_count, str(e)))\n\n        if buf.startswith(UBI_EC_HDR_MAGIC):\n            blk = description(buf)\n            blk.file_offset = i\n            blk.peb_num = ubi.first_peb_num + peb_count\n            blk.size = ubi.file.block_size\n            blocks[blk.peb_num] = blk\n            peb_count += 1\n            log(extract_blocks, blk)\n            verbose_log(extract_blocks, 'file addr: %s' % (ubi.file.last_read_addr()))\n            ec_hdr_errors = ''\n            vid_hdr_errors = ''\n\n            if blk.ec_hdr.errors:\n                ec_hdr_errors = ','.join(blk.ec_hdr.errors)\n\n            if blk.vid_hdr and blk.vid_hdr.errors:\n                vid_hdr_errors = ','.join(blk.vid_hdr.errors)\n\n            if ec_hdr_errors or vid_hdr_errors:\n                if blk.peb_num not in bad_blocks:\n                    bad_blocks.append(blk.peb_num)\n                    log(extract_blocks, 'PEB: %s has possible issue EC_HDR [%s], VID_HDR [%s]' % (blk.peb_num, ec_hdr_errors, vid_hdr_errors))\n\n            verbose_display(blk)\n\n        else:\n            cur_offset += ubi.file.block_size\n            ubi.first_peb_num = cur_offset/ubi.file.block_size\n            ubi.file.start_offset = cur_offset\n\n    return blocks"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef by_image_seq(blocks, image_seq):\n    return list(filter(lambda block: blocks[block].ec_hdr.image_seq == image_seq, blocks))", "response": "Filter blocks to return only those associated with the provided image_seq number."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsort a list of blocks by Logical Erase Block number.", "response": "def by_leb(blocks):\n    \"\"\"Sort blocks by Logical Erase Block number.\n    \n    Arguments:\n    List:blocks -- List of block objects to sort.\n    \n    Returns:\n    List              -- Indexes of blocks sorted by LEB.\n    \"\"\" \n    slist_len = len(blocks)\n    slist = ['x'] * slist_len\n\n    for block in blocks:\n        if blocks[block].leb_num >= slist_len:\n            add_elements = blocks[block].leb_num - slist_len + 1\n            slist += (['x'] * add_elements)\n            slist_len = len(slist)\n\n        slist[blocks[block].leb_num] = block\n\n    return slist"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsort blocks by volume id", "response": "def by_vol_id(blocks, slist=None):\n    \"\"\"Sort blocks by volume id\n\n    Arguments:\n    Obj:blocks -- List of block objects.\n    List:slist     -- (optional) List of block indexes.\n\n    Return:\n    Dict -- blocks grouped in lists with dict key as volume id.\n    \"\"\"\n\n    vol_blocks = {}\n\n    # sort block by volume\n    # not reliable with multiple partitions (fifo)\n\n    for i in blocks:\n        if slist and i not in slist:\n            continue\n        elif not blocks[i].is_valid:\n            continue\n\n        if blocks[i].vid_hdr.vol_id not in vol_blocks:\n            vol_blocks[blocks[i].vid_hdr.vol_id] = []\n\n        vol_blocks[blocks[i].vid_hdr.vol_id].append(blocks[i].peb_num)\n\n    return vol_blocks"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef by_type(blocks, slist=None):\n\n    layout = []\n    data = []\n    int_vol = []\n    unknown = []\n    \n    for i in blocks:\n        if slist and i not in slist:\n            continue\n\n        if blocks[i].is_vtbl and blocks[i].is_valid:\n            layout.append(i)\n\n        elif blocks[i].is_internal_vol and blocks[i].is_valid:\n            int_vol.append(i)\n\n        elif blocks[i].is_valid:\n            data.append(i)\n\n        else:\n            unknown.append(i)\n\n    return layout, data, int_vol, unknown", "response": "Sort blocks into layout internal volume data or unknown."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extract_files(ubifs, out_path, perms=False):\n    try:\n        inodes = {}\n        bad_blocks = []\n\n        walk.index(ubifs, ubifs.master_node.root_lnum, ubifs.master_node.root_offs, inodes, bad_blocks)\n\n        if len(inodes) < 2:\n            raise Exception('No inodes found')\n\n        for dent in inodes[1]['dent']:\n            extract_dents(ubifs, inodes, dent, out_path, perms)\n\n        if len(bad_blocks):\n            error(extract_files, 'Warning', 'Data may be missing or corrupted, bad blocks, LEB [%s]' % ','.join(map(str, bad_blocks)))\n\n    except Exception as e:\n        error(extract_files, 'Error', '%s' % e)", "response": "Extract UBIFS contents to_path"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_newest(blocks, layout_blocks):\n    layout_temp = list(layout_blocks)\n\n    for i in range(0, len(layout_temp)):\n        for k in range(0, len(layout_blocks)):\n            if blocks[layout_temp[i]].ec_hdr.image_seq != blocks[layout_blocks[k]].ec_hdr.image_seq:\n                continue\n\n            if blocks[layout_temp[i]].leb_num != blocks[layout_blocks[k]].leb_num:\n                continue\n\n            if blocks[layout_temp[i]].vid_hdr.sqnum > blocks[layout_blocks[k]].vid_hdr.sqnum:\n                del layout_blocks[k]\n                break\n\n    return layout_blocks", "response": "Filter out old layout blocks from list of layout blocks"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsort a list of layout blocks into pairs", "response": "def group_pairs(blocks, layout_blocks_list):\n    \"\"\"Sort a list of layout blocks into pairs\n\n    Arguments:\n    List:blocks        -- List of block objects\n    List:layout_blocks -- List of layout block indexes\n\n    Returns:\n    List -- Layout block pair indexes grouped in a list\n    \"\"\"\n\n    image_dict={}\n    for block_id in layout_blocks_list:\n        image_seq=blocks[block_id].ec_hdr.image_seq\n        if image_seq not in image_dict:\n            image_dict[image_seq]=[block_id]\n        else:\n            image_dict[image_seq].append(block_id)\n\n    log(group_pairs, 'Layout blocks found at PEBs: %s' % list(image_dict.values()))\n\n    return list(image_dict.values())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngroup blocks with appropriate layout pairs.", "response": "def associate_blocks(blocks, layout_pairs, start_peb_num):\n    \"\"\"Group block indexes with appropriate layout pairs\n\n    Arguments:\n    List:blocks        -- List of block objects\n    List:layout_pairs  -- List of grouped layout blocks\n    Int:start_peb_num  -- Number of the PEB to start from.\n\n    Returns:\n    List -- Layout block pairs grouped with associated block ranges.\n    \"\"\"\n    seq_blocks = []\n    for layout_pair in layout_pairs:\n        seq_blocks = sort.by_image_seq(blocks, blocks[layout_pair[0]].ec_hdr.image_seq)\n\n        layout_pair.append(seq_blocks)\n\n    return layout_pairs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a list of UBI volume objects from list of blocks.", "response": "def get_volumes(blocks, layout_info):\n    \"\"\"Get a list of UBI volume objects from list of blocks\n\n    Arguments:\n    List:blocks            -- List of layout block objects\n    List:layout_info    -- Layout info (indexes of layout blocks and\n                                        associated data blocks.)\n\n    Returns:\n    Dict -- Of Volume objects by volume name, including any\n            relevant blocks.\n    \"\"\"\n    volumes = {}\n\n    vol_blocks_lists = sort.by_vol_id(blocks, layout_info[2])\n\n    for vol_rec in blocks[layout_info[0]].vtbl_recs:\n        vol_name = vol_rec.name.strip(b'\\x00').decode('utf-8')\n        if vol_rec.rec_index not in vol_blocks_lists:\n            vol_blocks_lists[vol_rec.rec_index] = []\n        volumes[vol_name] = description(vol_rec.rec_index, vol_rec, vol_blocks_lists[vol_rec.rec_index])\n            \n    return volumes"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a key into a node key.", "response": "def parse_key(key):\n    \"\"\"Parse node key\n\n    Arguments:\n    Str:key    -- Hex string literal of node key.\n\n    Returns:\n    Int:key_type   -- Type of key, data, ino, dent, etc.\n    Int:ino_num    -- Inode number.\n    Int:khash      -- Key hash.\n    \"\"\"\n    hkey, lkey = struct.unpack('<II',key[0:UBIFS_SK_LEN])\n    ino_num = hkey & UBIFS_S_KEY_HASH_MASK\n    key_type = lkey >> UBIFS_S_KEY_BLOCK_BITS\n    khash = lkey\n\n    #if key_type < UBIFS_KEY_TYPES_CNT:\n    return {'type':key_type, 'ino_num':ino_num, 'khash': khash}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef decompress(ctype, unc_len, data):\n    if ctype == UBIFS_COMPR_LZO:\n        try:\n            return lzo.decompress(b''.join((b'\\xf0', struct.pack('>I', unc_len), data)))\n        except Exception as e:\n            error(decompress, 'Warn', 'LZO Error: %s' % e)\n    elif ctype == UBIFS_COMPR_ZLIB:\n        try:\n            return zlib.decompress(data, -11)\n        except Exception as e:\n            error(decompress, 'Warn', 'ZLib Error: %s' % e)\n    else:\n        return data", "response": "Decompress data.\n\n    Arguments:\n    Int:ctype    -- Compression type LZO, ZLIB (*currently unused*).\n    Int:unc_len  -- Uncompressed data lenth.\n    Str:data     -- Data to be uncompessed.\n\n    Returns:\n    Uncompressed Data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef index(ubifs, lnum, offset, inodes={}, bad_blocks=[]):\n    try:\n        if len(bad_blocks):\n            if lnum in bad_blocks:\n                return\n\n        ubifs.file.seek((ubifs.leb_size * lnum) + offset)\n        buf = ubifs.file.read(UBIFS_COMMON_HDR_SZ)\n        chdr = nodes.common_hdr(buf)\n        log(index , '%s file addr: %s' % (chdr, ubifs.file.last_read_addr()))\n        verbose_display(chdr)\n        node_buf = ubifs.file.read(chdr.len - UBIFS_COMMON_HDR_SZ)\n        file_offset = ubifs.file.last_read_addr()\n\n    except Exception as e:\n        if str(e) == 'Bad Read Offset Request' and settings.warn_only_block_read_errors:\n            bad_blocks.append(lnum)\n            return\n\n        else:\n            error(index, 'Fatal', 'LEB: %s, UBIFS offset: %s, error: %s' % (lnum, ((ubifs.leb_size * lnum) + offset), e))\n\n    if chdr.node_type == UBIFS_IDX_NODE:\n        try:\n            idxn = nodes.idx_node(node_buf)\n\n        except Exception as e:\n            if settings.warn_only_block_read_errors:\n                error(index, 'Error', 'Problem at file address: %s extracting idx_node: %s' % (file_offset, e))\n                return\n\n            else:\n                error(index, 'Fatal', 'Problem at file address: %s extracting idx_node: %s' % (file_offset, e))\n\n        log(index, '%s file addr: %s' % (idxn, file_offset))\n        verbose_display(idxn)\n        branch_idx = 0\n\n        for branch in idxn.branches:\n            verbose_log(index, '-------------------')\n            log(index, '%s file addr: %s' % (branch, file_offset + UBIFS_IDX_NODE_SZ + (branch_idx * UBIFS_BRANCH_SZ)))\n            verbose_display(branch)\n            index(ubifs, branch.lnum, branch.offs, inodes, bad_blocks)\n            branch_idx += 1\n\n    elif chdr.node_type == UBIFS_INO_NODE:\n        try:\n            inon = nodes.ino_node(node_buf)\n\n        except Exception as e:\n            if settings.warn_only_block_read_errors:\n                error(index, 'Error', 'Problem at file address: %s extracting ino_node: %s' % (file_offset, e))\n                return\n\n            else:\n                error(index, 'Fatal', 'Problem at file address: %s extracting ino_node: %s' % (file_offset, e))\n\n        ino_num = inon.key['ino_num']\n        log(index, '%s file addr: %s, ino num: %s' % (inon, file_offset, ino_num))\n        verbose_display(inon)\n\n        if not ino_num in inodes:\n            inodes[ino_num] = {}\n\n        inodes[ino_num]['ino'] = inon\n\n    elif chdr.node_type == UBIFS_DATA_NODE:\n        try:\n            datn = nodes.data_node(node_buf, (ubifs.leb_size * lnum) + UBIFS_COMMON_HDR_SZ + offset + UBIFS_DATA_NODE_SZ)\n\n        except Exception as e:\n            if settings.warn_only_block_read_errors:\n                error(index, 'Error', 'Problem at file address: %s extracting data_node: %s' % (file_offset, e))\n                return\n\n            else:\n                error(index, 'Fatal', 'Problem at file address: %s extracting data_node: %s' % (file_offset, e))\n\n        ino_num = datn.key['ino_num']\n        log(index, '%s file addr: %s, ino num: %s' % (datn, file_offset, ino_num))\n        verbose_display(datn)\n\n        if not ino_num in inodes:\n            inodes[ino_num] = {}\n\n        if not 'data' in inodes[ino_num]:\n            inodes[ino_num]['data']= []\n\n        inodes[ino_num]['data'].append(datn)\n\n    elif chdr.node_type == UBIFS_DENT_NODE:\n        try:\n            dn = nodes.dent_node(node_buf)\n\n        except Exception as e:\n            if settings.warn_only_block_read_errors:\n                error(index, 'Error', 'Problem at file address: %s extracting dent_node: %s' % (file_offset, e))\n                return\n\n            else:\n                error(index, 'Fatal', 'Problem at file address: %s extracting dent_node: %s' % (file_offset, e))\n\n        ino_num = dn.key['ino_num']\n        log(index, '%s file addr: %s, ino num: %s' % (dn, file_offset, ino_num))\n        verbose_display(dn)\n\n        if not ino_num in inodes:\n            inodes[ino_num] = {}\n\n        if not 'dent' in inodes[ino_num]:\n            inodes[ino_num]['dent']= []\n\n        inodes[ino_num]['dent'].append(dn)", "response": "Walk the index gathering Inode Dir Entry and File nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef guess_leb_size(path):\n\n    f = open(path, 'rb')\n    f.seek(0,2)\n    file_size = f.tell()+1\n    f.seek(0)\n    block_size = None\n\n    for _ in range(0, file_size, FILE_CHUNK_SZ):\n        buf = f.read(FILE_CHUNK_SZ)\n\n        for m in re.finditer(UBIFS_NODE_MAGIC, buf):\n            start = m.start()\n            chdr = nodes.common_hdr(buf[start:start+UBIFS_COMMON_HDR_SZ])\n\n            if chdr and chdr.node_type == UBIFS_SB_NODE:\n                sb_start = start + UBIFS_COMMON_HDR_SZ\n                sb_end = sb_start + UBIFS_SB_NODE_SZ\n\n                if chdr.len != len(buf[sb_start:sb_end]):\n                    f.seek(sb_start)\n                    buf = f.read(UBIFS_SB_NODE_SZ)\n                else:\n                    buf = buf[sb_start:sb_end]\n\n                sbn = nodes.sb_node(buf)\n                block_size = sbn.leb_size\n                f.close()\n                return block_size\n\n    f.close()\n    return block_size", "response": "Guesses LEB size from superblock or node."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef guess_peb_size(path):\n    file_offset = 0\n    offsets = []\n    f = open(path, 'rb')\n    f.seek(0,2)\n    file_size = f.tell()+1\n    f.seek(0)\n\n    for _ in range(0, file_size, FILE_CHUNK_SZ):\n        buf = f.read(FILE_CHUNK_SZ)\n        for m in re.finditer(UBI_EC_HDR_MAGIC, buf):\n            start = m.start()\n\n            if not file_offset:\n                file_offset = start\n                idx = start\n            else:\n                idx = start+file_offset\n\n            offsets.append(idx)\n\n        file_offset += FILE_CHUNK_SZ\n    f.close()\n\n    occurances = {}\n    for i in range(0, len(offsets)):\n        try:\n            diff = offsets[i] - offsets[i-1]\n        except:\n            diff = offsets[i]\n\n        if diff not in occurances:\n            occurances[diff] = 0\n\n        occurances[diff] += 1\n\n    most_frequent = 0\n    block_size = None\n\n    for offset in occurances:\n        if occurances[offset] > most_frequent:\n            most_frequent = occurances[offset]\n            block_size = offset\n\n    return block_size", "response": "Guesses the size of the most likely PEB from the file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nattempt to convert a specified value to an integer", "response": "def convert_to_int(value):\n    \"\"\"Attempts to convert a specified value to an integer\n\n    :param value: Content to be converted into an integer\n    :type value: string or int\n\n    \"\"\"\n    if not value:\n        return None\n\n    # Apart from numbers also accept values that end with px\n    if isinstance(value, str):\n        value = value.strip(' px')\n\n    try:\n        return int(value)\n    except (TypeError, ValueError):\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_oembed_data(oembed_data, data):\n    data.update({\n        'oembed': oembed_data,\n    })\n    _type = oembed_data.get('type')\n    provider_name = oembed_data.get('provider_name')\n    if not _type:\n        return data\n\n    if oembed_data.get('title'):\n        data.update({\n            'title': oembed_data.get('title'),\n        })\n\n    if _type == 'video':\n        try:\n            item = {\n                'width': convert_to_int(oembed_data.get('width')),\n                'height': convert_to_int(oembed_data.get('height'))\n            }\n            if provider_name in ['YouTube', ]:\n                item['src'] = HYPERLINK_PATTERN.search(oembed_data.get('html')).group(0)\n\n            data['videos'].append(item)\n        except Exception:\n            pass\n\n        if oembed_data.get('thumbnail_url'):\n            item = {\n                'width': convert_to_int(oembed_data.get('thumbnail_width')),\n                'height': convert_to_int(oembed_data.get('thumbnail_height')),\n                'src': oembed_data.get('thumbnail_url')\n            }\n\n            data['images'].append(item)\n\n    return data", "response": "Parse OEmbed response data to inject into lassie s response dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves content from the specified url parses it and returns a dictionary of important information about that web page.", "response": "def fetch(self, url, open_graph=None, twitter_card=None, touch_icon=None,\n              favicon=None, all_images=None, parser=None, handle_file_content=None,\n              canonical=None):\n        \"\"\"Retrieves content from the specified url, parses it, and returns\n        a beautifully crafted dictionary of important information about that\n        web page.\n\n        Priority tree is as follows:\n            1. OEmbed\n            2. Open Graph\n            3. Twitter Card\n            4. Other meta content (i.e. description, keywords)\n\n        :param url: URL to send a GET request to\n        :param open_graph: (optional) If ``True``, filters web page content for Open Graph meta tags. The content of these properties have top priority on return values.\n        :type open_graph: bool\n        :param twitter_card: (optional) If ``True``, filters web page content for Twitter Card meta tags\n        :type twitter_card: bool\n        :param touch_icon: (optional) If ``True``, retrieves Apple touch icons and includes them in the response ``images`` array\n        :type touch_icon: bool\n        :param favicon: (optional) If ``True``, retrieves any favicon images and includes them in the response ``images`` array\n        :type favicon: bool\n        :param canonical: (optional) If ``True``, retrieves canonical url from meta tags. Default: False\n        :type canonical: bool\n        :param all_images: (optional) If ``True``, retrieves images inside web pages body and includes them in the response ``images`` array. Default: False\n        :type all_images: bool\n        :param parser: (optional) String reference for the parser that BeautifulSoup will use\n        :type parser: string\n        :param handle_file_content: (optional) If ``True``, lassie will return a generic response when a file is fetched. Default: False\n        :type handle_file_content: bool\n\n        \"\"\"\n\n        # Set params, method params have priority over class params\n        open_graph = merge_settings(open_graph, self.open_graph)\n        twitter_card = merge_settings(twitter_card, self.twitter_card)\n        touch_icon = merge_settings(touch_icon, self.touch_icon)\n        favicon = merge_settings(favicon, self.favicon)\n        canonical = merge_settings(canonical, self.canonical)\n        all_images = merge_settings(all_images, self.all_images)\n        parser = merge_settings(parser, self.parser)\n        handle_file_content = merge_settings(handle_file_content, self.handle_file_content)\n\n        data = {\n            'images': [],\n            'videos': [],\n        }\n\n        has_file_content = False\n        content_type = None\n        if handle_file_content:\n            headers, status_code = self._retrieve_headers(url)\n            content_type = headers.get('Content-Type')\n            has_file_content = content_type and not 'text/html' in content_type\n\n        if has_file_content and content_type:\n            has_image_content = content_type in IMAGE_MIMETYPES\n            if has_image_content:\n                parsed_url = urlparse(url)\n                data['title'] = basename(parsed_url.path.lstrip('/'))  # TODO: if the url doesn't have an extension, maybe we should match it up to the mimetype and append an ext?\n                data['url'] = url\n                data['images'].append({\n                    'type': 'body_image',\n                    'src': url,\n                })\n        else:\n            try:\n                oembed_data, status_code = self._retrieve_oembed_data(url)\n                parse_oembed_data(oembed_data, data)\n            except LassieError:\n                oembed_data = None\n\n            html, status_code = self._retrieve_content(url)\n\n            if not html and not oembed_data:\n                raise LassieError('There was no content to parse.')\n\n            if '<html' not in html:\n                html = re.sub(r'(?:<!DOCTYPE(?:\\s\\w)?>(?:<head>)?)', '<!DOCTYPE html><html>', html)\n\n            soup = BeautifulSoup(clean_text(html), parser)\n\n            self._filter_amp_data(soup, data, url, all_images)\n\n            if open_graph:\n                self._filter_meta_data('open_graph', soup, data, url)\n\n            if twitter_card:\n                self._filter_meta_data('twitter_card', soup, data)\n\n            self._filter_meta_data('generic', soup, data)\n\n            if touch_icon:\n                self._filter_link_tag_data('touch_icon', soup, data, url)\n\n            if favicon:\n                self._filter_link_tag_data('favicon', soup, data, url)\n\n            if canonical:\n                self._filter_link_tag_data('canonical', soup, data, url)\n\n            if all_images:\n                # Maybe filter out 1x1, no \"good\" way to do this if image doesn't supply\n                # width/height.\n                self._find_all_images(soup, data, url)\n\n            # TODO: Find a good place for setting url, title and locale\n            if soup.html.get('lang'):\n                lang = soup.html.get('lang')\n            else:\n                lang = soup.html.get('xml:lang')\n\n            if lang and ('locale' not in data):\n                locale = normalize_locale(lang)\n                if locale:\n                    data['locale'] = locale\n\n            data_url = data.get('url')\n            if not data_url or (data_url in url and len(data_url) < len(url)):\n                data['url'] = url\n\n            if ('title' not in data or not data.get('title')) and hasattr(soup.title, 'string'):\n                data['title'] = soup.title.string\n\n        data['status_code'] = status_code\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decode_mail_header(value, default_charset='us-ascii'):\n    try:\n        headers = decode_header(value)\n    except email.errors.HeaderParseError:\n        return str_decode(str_encode(value, default_charset, 'replace'), default_charset)\n    else:\n        for index, (text, charset) in enumerate(headers):\n            logger.debug(\"Mail header no. {index}: {data} encoding {charset}\".format(\n                index=index,\n                data=str_decode(text, charset or 'utf-8', 'replace'),\n                charset=charset))\n            try:\n                headers[index] = str_decode(text, charset or default_charset,\n                                            'replace')\n            except LookupError:\n                # if the charset is unknown, force default\n                headers[index] = str_decode(text, default_charset, 'replace')\n\n        return ''.join(headers)", "response": "Decode a header value into a unicode string."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves all email addresses from one message header.", "response": "def get_mail_addresses(message, header_name):\n    \"\"\"\n    Retrieve all email addresses from one message header.\n    \"\"\"\n    headers = [h for h in message.get_all(header_name, [])]\n    addresses = email.utils.getaddresses(headers)\n\n    for index, (address_name, address_email) in enumerate(addresses):\n        addresses[index] = {'name': decode_mail_header(address_name),\n                            'email': address_email}\n        logger.debug(\"{} Mail address in message: <{}> {}\".format(\n            header_name.upper(), address_name, address_email))\n    return addresses"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_flags(headers):\n    if len(headers) == 0:\n        return []\n    if sys.version_info[0] == 3:\n        headers = bytes(headers, \"ascii\")\n    return list(imaplib.ParseFlags(headers))", "response": "Copied from Girishramnani s Gmail. py"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a random variable if any otherwise create a new default variable.", "response": "def generate(self, state):\n        \"\"\"Return a random variable if any, otherwise create a new default variable.\"\"\"\n        if self.count >= random.randint(DharmaConst.VARIABLE_MIN, DharmaConst.VARIABLE_MAX):\n            return \"%s%d\" % (self.var, random.randint(1, self.count))\n        var = random.choice(self)\n        prefix = self.eval(var[0], state)\n        suffix = self.eval(var[1], state)\n        self.count += 1\n        element_name = \"%s%d\" % (self.var, self.count)\n        self.default += \"%s%s%s\\n\" % (prefix, element_name, suffix)\n        return element_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_settings(self, settings):\n        logging.debug(\"Using configuration from: %s\", settings.name)\n        exec(compile(settings.read(), settings.name, 'exec'), globals(), locals())", "response": "A lazy way of feeding Dharma with configuration settings."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_xrefs(self, token):\n        out, end = [], 0\n        token = token.replace(\"\\\\n\", \"\\n\")\n        for m in re.finditer(self.xref_registry, token, re.VERBOSE | re.DOTALL):\n            if m.start(0) > end:\n                out.append(String(token[end:m.start(0)], self.current_obj))\n            end = m.end(0)\n            if m.group(\"type\"):\n                xref_type = {\"+\": ValueXRef,\n                             \"!\": VariableXRef,\n                             \"@\": ElementXRef}[m.group(\"type\")]\n                out.append(xref_type(m.group(\"xref\"), self.current_obj))\n            elif m.group(\"uri\") is not None:\n                path = m.group(\"uri\")\n                out.append(MetaURI(path, self.current_obj))\n            elif m.group(\"repeat\") is not None:\n                repeat, separator, nodups = m.group(\"repeat\", \"separator\", \"nodups\")\n                if separator is None:\n                    separator = \"\"\n                if nodups is None:\n                    nodups = \"\"\n                out.append(MetaRepeat(self.parse_xrefs(repeat), separator, nodups, self.current_obj))\n            elif m.group(\"block\") is not None:\n                path = m.group(\"block\")\n                out.append(MetaBlock(path, self.current_obj))\n            elif m.group(\"choices\") is not None:\n                choices = m.group(\"choices\")\n                out.append(MetaChoice(choices, self.current_obj))\n            else:\n                startval, endval = m.group(\"start\", \"end\")\n                out.append(MetaRange(startval, endval, self.current_obj))\n        if end < len(token):\n            out.append(String(token[end:], self.current_obj))\n        return out", "response": "Search token for + value + and!variable! style references. Be careful to not xref a new variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses an assignment variable.", "response": "def parse_assign_variable(self, tokens):\n        \"\"\"\n        Example:\n            tokens\n                dharma.String:      'let ',\n                dharma.ElementXRef: 'GrammarNS:<VarName>',\n                dharma.String:      '= new ',\n                dharma.ValueXRef:   'GrammarNS:<ValueName>'\n        \"\"\"\n        for i, token in enumerate(tokens):\n            if isinstance(token, ElementXRef):\n                variable = token.value\n                break\n        else:\n            logging.error(\"%s: Variable assignment syntax error\", self.id())\n            sys.exit(-1)\n        if variable != self.current_obj.ident:\n            logging.error(\"%s: Variable name mismatch\", self.id())\n            sys.exit(-1)\n        if not isinstance(self.current_obj, DharmaVariable):\n            logging.error(\"%s: Inconsistent object for variable assignment\", self.id())\n            sys.exit(-1)\n        prefix, suffix = tokens[:i], tokens[i + 1:]  # pylint: disable=undefined-loop-variable\n        self.current_obj.append((prefix, suffix))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calculate_leaf_paths(self):\n        reverse_xref = {}\n        leaves = set()\n        for v in self.value.values():\n            if v.leaf:\n                leaves.add(v)\n            for xref in v.value_xref:\n                reverse_xref.setdefault(xref, []).append(v.ident)\n        for leaf in leaves:\n            self.calculate_leaf_path(leaf, reverse_xref)", "response": "Calculate the paths to all leaves of the current entry."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_content(self):\n        # Setup pre-conditions.\n        if not self.variance:\n            logging.error(\"%s: No variance information %s\", self.id(), self.variance)\n            sys.exit(-1)\n\n        for var in self.variable.values():\n            var.clear()\n\n        # Handle variances\n        variances = []\n        for _ in range(random.randint(DharmaConst.VARIANCE_MIN, DharmaConst.VARIANCE_MAX)):\n            var = random.choice(list(self.variance.values()))\n            variances.append(DharmaConst.VARIANCE_TEMPLATE % var.generate(GenState()))\n            variances.append(\"\\n\")\n\n        # Handle variables\n        variables = []\n        for var in self.variable.values():\n            if var.default:\n                variables.append(DharmaConst.VARIANCE_TEMPLATE % var.default)\n                variables.append(\"\\n\")\n\n        # Build content\n        content = \"\".join(chain([self.prefix], variables, variances, [self.suffix]))\n        if self.template:\n            return Template(self.template).safe_substitute(testcase_content=content)\n        return content", "response": "Generates a test case as a string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing provided grammars by parsing them into Python objects.", "response": "def process_grammars(self, grammars):\n        \"\"\"Process provided grammars by parsing them into Python objects.\"\"\"\n        for path in self.default_grammars:\n            grammars.insert(0, open(os.path.relpath(os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                                                                 os.path.normcase(path)))))\n        for fo in grammars:\n            logging.debug(\"Processing grammar content of %s\", fo.name)\n            self.set_namespace(os.path.splitext(os.path.basename(fo.name))[0])\n            for line in fo:\n                self.parse_line(line)\n            self.handle_empty_line()\n        self.resolve_xref()\n        self.calculate_leaf_paths()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntemplates tag that renders the menu.", "response": "def admin_tools_render_menu(context, menu=None):\n    \"\"\"\n    Template tag that renders the menu, it takes an optional ``Menu`` instance\n    as unique argument, if not given, the menu will be retrieved with the\n    ``get_admin_menu`` function.\n    \"\"\"\n    if menu is None:\n        menu = get_admin_menu(context)\n\n    menu.init_with_context(context)\n    has_bookmark_item = False\n    bookmark = None\n    if len([c for c in menu.children if isinstance(c, items.Bookmarks)]) > 0:\n        has_bookmark_item = True\n        url = context['request'].get_full_path()\n        try:\n            bookmark = Bookmark.objects.filter(\n                user=context['request'].user, url=url\n            )[0]\n        except:\n            pass\n\n    context.update({\n        'template': menu.template,\n        'menu': menu,\n        'has_bookmark_item': has_bookmark_item,\n        'bookmark': bookmark,\n        'admin_url': reverse('%s:index' % get_admin_site_name(context)),\n    })\n    return context"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntemplate tag that renders a given menu item.", "response": "def admin_tools_render_menu_item(context, item, index=None):\n    \"\"\"\n    Template tag that renders a given menu item, it takes a ``MenuItem``\n    instance as unique parameter.\n    \"\"\"\n    item.init_with_context(context)\n\n    context.update({\n        'template': item.template,\n        'item': item,\n        'index': index,\n        'selected': item.is_selected(context['request']),\n        'admin_url': reverse('%s:index' % get_admin_site_name(context)),\n    })\n    return context"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef admin_tools_render_menu_css(context, menu=None):\n    if menu is None:\n        menu = get_admin_menu(context)\n\n    context.update({\n        'template': 'admin_tools/menu/css.html',\n        'css_files': menu.Media.css,\n    })\n    return context", "response": "Template tag that renders the menu css files"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the HTML tag that renders the needed css files for the theming app.", "response": "def render_theming_css():\n    \"\"\"\n    Template tag that renders the needed css files for the theming app.\n    \"\"\"\n    css = getattr(settings, 'ADMIN_TOOLS_THEMING_CSS', False)\n    if not css:\n        css = '/'.join(['admin_tools', 'css', 'theming.css'])\n    return mark_safe(\n        '<link rel=\"stylesheet\" type=\"text/css\" media=\"screen\" href=\"%s\" />' %\n        staticfiles_storage.url(css)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_bookmark(request):\n    if request.method == \"POST\":\n        form = BookmarkForm(user=request.user, data=request.POST)\n        if form.is_valid():\n            bookmark = form.save()\n            if not request.is_ajax():\n                messages.success(request, 'Bookmark added')\n                if request.POST.get('next'):\n                    return HttpResponseRedirect(request.POST.get('next'))\n                return HttpResponse('Added')\n            return render_to_response(\n                'admin_tools/menu/remove_bookmark_form.html',\n                {'bookmark': bookmark, 'url': bookmark.url}\n            )\n    else:\n        form = BookmarkForm(user=request.user)\n    return render_to_response(\n        'admin_tools/menu/form.html',\n        {'form': form, 'title': 'Add Bookmark'}\n    )", "response": "This view serves and validates a bookmark form and returns the drop bookmark form to replace the\n    add bookmark form."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_bookmark(request, id):\n    bookmark = get_object_or_404(Bookmark, id=id, user=request.user)\n    if request.method == \"POST\":\n        bookmark.delete()\n        if not request.is_ajax():\n            messages.success(request, 'Bookmark removed')\n            if request.POST.get('next'):\n                return HttpResponseRedirect(request.POST.get('next'))\n            return HttpResponse('Deleted')\n        return render_to_response(\n            'admin_tools/menu/add_bookmark_form.html',\n            {\n                'url': request.POST.get('next'),\n                'title': '**title**'  # replaced on the javascript side\n            }\n        )\n    return render_to_response(\n        'admin_tools/menu/delete_confirm.html',\n        {'bookmark': bookmark, 'title': 'Delete Bookmark'}\n    )", "response": "This view deletes a bookmark."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a string containing the css classes for the module.", "response": "def render_css_classes(self):\n        \"\"\"\n        Return a string containing the css classes for the module.\n\n        >>> mod = DashboardModule(enabled=False, draggable=True,\n        ...                       collapsible=True, deletable=True)\n        >>> mod.render_css_classes()\n        'dashboard-module disabled draggable collapsible deletable'\n        >>> mod.css_classes.append('foo')\n        >>> mod.render_css_classes()\n        'dashboard-module disabled draggable collapsible deletable foo'\n        >>> mod.enabled = True\n        >>> mod.render_css_classes()\n        'dashboard-module draggable collapsible deletable foo'\n        \"\"\"\n        ret = ['dashboard-module']\n        if not self.enabled:\n            ret.append('disabled')\n        if self.draggable:\n            ret.append('draggable')\n        if self.collapsible:\n            ret.append('collapsible')\n        if self.deletable:\n            ret.append('deletable')\n        ret += self.css_classes\n        return ' '.join(ret)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_empty(self):\n        if super(Group, self).is_empty():\n            return True\n        for child in self.children:\n            if not child.is_empty():\n                return False\n        return True", "response": "A method to check if the group of modules is empty."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_dashboard(context, location):\n    if location == 'index':\n        return get_index_dashboard(context)\n    elif location == 'app_index':\n        return get_app_index_dashboard(context)\n    raise ValueError('Invalid dashboard location: \"%s\"' % location)", "response": "Returns the dashboard that matches the given location."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_app_index_dashboard(context):\n    # this is a mess, needs cleanup !\n    app = context['app_list'][0]\n    model_list = []\n    app_label = None\n    app_title = app['name']\n    admin_site = get_admin_site(context=context)\n\n    for model, model_admin in admin_site._registry.items():\n        if app['app_label'] == model._meta.app_label:\n            split = model.__module__.find(model._meta.app_label)\n            app_label = model.__module__[0:split] + model._meta.app_label\n            for m in app['models']:\n                if m['name'] == capfirst(model._meta.verbose_name_plural):\n                    mod = '%s.%s' % (model.__module__, model.__name__)\n                    model_list.append(mod)\n\n    # if an app has registered its own dashboard, use it\n    if app_label is not None and app_label in Registry.registry:\n        return Registry.registry[app_label](app_title, model_list)\n\n    # try to discover a general app_index dashboard (with fallback to the\n    # default dashboard)\n    return _get_dashboard_cls(getattr(\n        settings,\n        'ADMIN_TOOLS_APP_INDEX_DASHBOARD',\n        'admin_tools.dashboard.dashboards.DefaultAppIndexDashboard'\n    ), context)(app_title, model_list)", "response": "Returns the admin dashboard defined by the user or the default one."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_app_content_types(self):\n        # Import this here to silence RemovedInDjango19Warning. See #15\n        from django.contrib.contenttypes.models import ContentType\n\n        return [ContentType.objects.get_for_model(c) for c\n                in self.get_app_model_classes()]", "response": "Return a list of all content types for this app."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntemplate tag that renders the dashboard, it takes two optional arguments: ``location`` The location of the dashboard, it can be 'index' (for the admin index dashboard) or 'app_index' (for the app index dashboard), the default value is 'index'. ``dashboard`` An instance of ``Dashboard``, if not given, the dashboard is retrieved with the ``get_index_dashboard`` or ``get_app_index_dashboard`` functions, depending on the ``location`` argument.", "response": "def admin_tools_render_dashboard(context, location='index', dashboard=None):\n    \"\"\"\n    Template tag that renders the dashboard, it takes two optional arguments:\n\n    ``location``\n        The location of the dashboard, it can be 'index' (for the admin index\n        dashboard) or 'app_index' (for the app index dashboard), the default\n        value is 'index'.\n\n    ``dashboard``\n        An instance of ``Dashboard``, if not given, the dashboard is retrieved\n        with the ``get_index_dashboard`` or ``get_app_index_dashboard``\n        functions, depending on the ``location`` argument.\n    \"\"\"\n    if dashboard is None:\n        dashboard = get_dashboard(context, location)\n\n    dashboard.init_with_context(context)\n    dashboard._prepare_children()\n\n    try:\n        preferences = DashboardPreferences.objects.get(\n            user=context['request'].user,\n            dashboard_id=dashboard.get_id()\n        ).data\n    except DashboardPreferences.DoesNotExist:\n        preferences = '{}'\n        try:\n            DashboardPreferences(\n                user=context['request'].user,\n                dashboard_id=dashboard.get_id(),\n                data=preferences\n            ).save()\n        except IntegrityError:\n            # dashboard already was saved for that (user, dashboard)\n            pass\n\n    context.update({\n        'template': dashboard.template,\n        'dashboard': dashboard,\n        'dashboard_preferences': preferences,\n        'split_at': math.ceil(\n            float(len(dashboard.children)) / float(dashboard.columns)\n        ),\n        'has_disabled_modules': len(\n            [m for m in dashboard.children if not m.enabled]\n        ) > 0,\n        'admin_url': reverse('%s:index' % get_admin_site_name(context)),\n    })\n    return context"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef admin_tools_render_dashboard_module(context, module):\n    module.init_with_context(context)\n    context.update({\n        'template': module.template,\n        'module': module,\n        'admin_url': reverse('%s:index' % get_admin_site_name(context)),\n    })\n    return context", "response": "Template tag that renders a given dashboard module."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef admin_tools_render_dashboard_css(\n        context, location='index', dashboard=None):\n    \"\"\"\n    Template tag that renders the dashboard css files, it takes two optional\n    arguments:\n\n    ``location``\n        The location of the dashboard, it can be 'index' (for the admin index\n        dashboard) or 'app_index' (for the app index dashboard), the default\n        value is 'index'.\n\n    ``dashboard``\n        An instance of ``Dashboard``, if not given, the dashboard is retrieved\n        with the ``get_index_dashboard`` or ``get_app_index_dashboard``\n        functions, depending on the ``location`` argument.\n    \"\"\"\n    if dashboard is None:\n        dashboard = get_dashboard(context, location)\n\n    context.update({\n        'template': 'admin_tools/dashboard/css.html',\n        'css_files': dashboard.Media.css,\n    })\n    return context", "response": "Template tag that renders the dashboard css files."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_selected(self, request):\n        current_url = request.get_full_path()\n        return self.url == current_url or \\\n            len([c for c in self.children if c.is_selected(request)]) > 0", "response": "Helper method that returns True if the menu item is active."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef init_with_context(self, context):\n        items = self._visible_models(context['request'])\n        apps = {}\n        for model, perms in items:\n            if not (perms['change'] or perms.get('view', False)):\n                continue\n            app_label = model._meta.app_label\n            if app_label not in apps:\n                apps[app_label] = {\n                    'title':\n                        django_apps.get_app_config(app_label).verbose_name,\n                    'url': self._get_admin_app_list_url(model, context),\n                    'models': []\n                }\n            apps[app_label]['models'].append({\n                'title': model._meta.verbose_name_plural,\n                'url': self._get_admin_change_url(model, context)\n            })\n\n        for app in sorted(apps.keys()):\n            app_dict = apps[app]\n            item = MenuItem(title=app_dict['title'], url=app_dict['url'])\n            # sort model list alphabetically\n            apps[app]['models'].sort(key=lambda x: x['title'])\n            for model_dict in apps[app]['models']:\n                item.children.append(MenuItem(**model_dict))\n            self.children.append(item)", "response": "Initialize the menu with the given context."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef init_with_context(self, context):\n        items = self._visible_models(context['request'])\n        for model, perms in items:\n            if not (perms['change'] or perms.get('view', False)):\n                continue\n            title = model._meta.verbose_name_plural\n            url = self._get_admin_change_url(model, context)\n            item = MenuItem(title=title, url=url)\n            self.children.append(item)", "response": "Initialize menu items with the given context."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef init_with_context(self, context):\n        from admin_tools.menu.models import Bookmark\n\n        for b in Bookmark.objects.filter(user=context['request'].user):\n            self.children.append(MenuItem(mark_safe(b.title), b.url))\n\n        if not len(self.children):\n            self.enabled = False", "response": "Initialize the item with the given context."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_app_template_dir(app_name):\n    if app_name in _cache:\n        return _cache[app_name]\n    template_dir = None\n    for app in apps.get_app_configs():\n        if app.label == app_name:\n            template_dir = join(app.path, 'templates')\n            break\n    _cache[app_name] = template_dir\n    return template_dir", "response": "Get the template directory for an application."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the absolute paths to template_name in the specified app.", "response": "def get_template_sources(self, template_name, template_dirs=None):\n        \"\"\"\n        Returns the absolute paths to \"template_name\" in the specified app.\n        If the name does not contain an app name (no colon), an empty list\n        is returned.\n        The parent FilesystemLoader.load_template_source() will take care\n        of the actual loading for us.\n        \"\"\"\n        if ':' not in template_name:\n            return []\n        app_name, template_name = template_name.split(\":\", 1)\n        template_dir = get_app_template_dir(app_name)\n        if template_dir:\n            try:\n                from django.template import Origin\n                origin = Origin(\n                    name=join(template_dir, template_name),\n                    template_name=template_name,\n                    loader=self,\n                )\n            except (ImportError, TypeError):\n                origin = join(template_dir, template_name)\n            return [origin]\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef uniquify(value, seen_values):\n    id = 1\n    new_value = value\n    while new_value in seen_values:\n        new_value = \"%s%s\" % (value, id)\n        id += 1\n    seen_values.add(new_value)\n    return new_value", "response": "Adds value to seen_values set ensures it is unique"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of tuples for all available models user can possibly see", "response": "def get_avail_models(request):\n    \"\"\" Returns (model, perm,) for all models user can possibly see \"\"\"\n    items = []\n    admin_site = get_admin_site(request=request)\n\n    for model, model_admin in admin_site._registry.items():\n        perms = model_admin.get_model_perms(request)\n        if True not in perms.values():\n            continue\n        items.append((model, perms,))\n    return items"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filter_models(request, models, exclude):\n    items = get_avail_models(request)\n    included = []\n\n    def full_name(model):\n        return '%s.%s' % (model.__module__, model.__name__)\n\n    # I beleive that that implemented\n    # O(len(patterns)*len(matched_patterns)*len(all_models))\n    # algorythm is fine for model lists because they are small and admin\n    # performance is not a bottleneck. If it is not the case then the code\n    # should be optimized.\n\n    if len(models) == 0:\n        included = items\n    else:\n        for pattern in models:\n            wildcard_models = []\n            for item in items:\n                model, perms = item\n                model_str = full_name(model)\n                if model_str == pattern:\n                    # exact match\n                    included.append(item)\n                elif fnmatch(model_str, pattern) and \\\n                        item not in wildcard_models:\n                    # wildcard match, put item in separate list so it can be\n                    # sorted alphabetically later\n                    wildcard_models.append(item)\n            if wildcard_models:\n                # sort wildcard matches alphabetically before adding them\n                wildcard_models.sort(\n                    key=lambda x: x[0]._meta.verbose_name_plural\n                )\n                included += wildcard_models\n\n    result = included[:]\n    for pattern in exclude:\n        for item in included:\n            model, perms = item\n            if fnmatch(full_name(model), pattern):\n                try:\n                    result.remove(item)\n                except ValueError:  # if the item was already removed skip\n                    pass\n    return result", "response": "Returns all models that match models and are not visible by current user."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_admin_app_list_url(self, model, context):\n        app_label = model._meta.app_label\n        return reverse('%s:app_list' % get_admin_site_name(context),\n                       args=(app_label,))", "response": "Returns the admin change url."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_admin_change_url(self, model, context):\n        app_label = model._meta.app_label\n        return reverse('%s:%s_%s_changelist' % (get_admin_site_name(context),\n                                                app_label,\n                                                model.__name__.lower()))", "response": "Returns the admin change url."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef attempt_connection(self):\n        self.socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)\n        self.socket.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_TTL, 2)\n\n        self.receiver_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)\n        self.receiver_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        self.receiver_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)\n        self.receiver_socket.bind(('', MCAST_PORT))\n        mreq = struct.pack(\"4sl\", socket.inet_aton(MCAST_GRP), socket.INADDR_ANY)\n        self.receiver_socket.setsockopt(socket.IPPROTO_IP, socket.IP_ADD_MEMBERSHIP, mreq)\n\n        if not self.socket or not self.receiver_socket:\n            raise exception.ConnectFailedException()", "response": "Establish a multicast connection for the current class."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses a single frame from the broker.", "response": "def process_frame(self, f, frame_str):\n        \"\"\"\n        :param Frame f: Frame object\n        :param bytes frame_str: Raw frame content\n        \"\"\"\n        frame_type = f.cmd.lower()\n\n        if frame_type in ['disconnect']:\n            return\n\n        if frame_type == 'send':\n            frame_type = 'message'\n            f.cmd = 'MESSAGE'\n\n        if frame_type in ['connected', 'message', 'receipt', 'error', 'heartbeat']:\n            if frame_type == 'message':\n                if f.headers['destination'] not in self.subscriptions.values():\n                    return\n                (f.headers, f.body) = self.notify('before_message', f.headers, f.body)\n            self.notify(frame_type, f.headers, f.body)\n        if 'receipt' in f.headers:\n            receipt_frame = Frame('RECEIPT', {'receipt-id': f.headers['receipt']})\n            lines = convert_frame(receipt_frame)\n            self.send(encode(pack(lines)))\n        log.debug(\"Received frame: %r, headers=%r, body=%r\", f.cmd, f.headers, f.body)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connect(self, username=None, passcode=None, wait=False, headers=None, **keyword_headers):\n        self.transport.start()", "response": "Connect to the naclient"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsubscribes to a specific topic.", "response": "def subscribe(self, destination, id, ack='auto', headers=None, **keyword_headers):\n        \"\"\"\n        :param str destination:\n        :param str id:\n        :param str ack:\n        :param dict headers:\n        :param keyword_headers:\n        \"\"\"\n        self.transport.subscriptions[id] = destination"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisconnecting from the broker.", "response": "def disconnect(self, receipt=None, headers=None, **keyword_headers):\n        \"\"\"\n        :param str receipt:\n        :param dict headers:\n        :param keyword_headers:\n        \"\"\"\n        Protocol12.disconnect(self, receipt, headers, **keyword_headers)\n        self.transport.stop()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a frame to the device.", "response": "def send_frame(self, cmd, headers=None, body=''):\n        \"\"\"\n        :param str cmd:\n        :param dict headers:\n        :param body:\n        \"\"\"\n        if headers is None:\n            headers = {}\n        frame = utils.Frame(cmd, headers, body)\n\n        if cmd == CMD_BEGIN:\n            trans = headers[HDR_TRANSACTION]\n            if trans in self.transactions:\n                self.notify('error', {}, 'Transaction %s already started' % trans)\n            else:\n                self.transactions[trans] = []\n        elif cmd == CMD_COMMIT:\n            trans = headers[HDR_TRANSACTION]\n            if trans not in self.transactions:\n                self.notify('error', {}, 'Transaction %s not started' % trans)\n            else:\n                for f in self.transactions[trans]:\n                    self.transport.transmit(f)\n                del self.transactions[trans]\n        elif cmd == CMD_ABORT:\n            trans = headers['transaction']\n            del self.transactions[trans]\n        else:\n            if 'transaction' in headers:\n                trans = headers['transaction']\n                if trans not in self.transactions:\n                    self.transport.notify('error', {}, 'Transaction %s not started' % trans)\n                    return\n                else:\n                    self.transactions[trans].append(frame)\n            else:\n                self.transport.transmit(frame)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndefault thread creation - used to create threads when the client doesn t want to provide their own thread creation.", "response": "def default_create_thread(callback):\n    \"\"\"\n    Default thread creation - used to create threads when the client doesn't want to provide their\n    own thread creation.\n\n    :param function callback: the callback function provided to threading.Thread\n    \"\"\"\n    thread = threading.Thread(None, callback)\n    thread.daemon = True  # Don't let thread prevent termination\n    thread.start()\n    return thread"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_headers(lines, offset=0):\n    headers = {}\n    for header_line in lines[offset:]:\n        header_match = HEADER_LINE_RE.match(header_line)\n        if header_match:\n            key = header_match.group('key')\n            key = re.sub(r'\\\\.', _unescape_header, key)\n            if key not in headers:\n                value = header_match.group('value')\n                value = re.sub(r'\\\\.', _unescape_header, value)\n                headers[key] = value\n    return headers", "response": "Parse the headers in a STOMP response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a STOMP frame into a Frame object.", "response": "def parse_frame(frame):\n    \"\"\"\n    Parse a STOMP frame into a Frame object.\n\n    :param bytes frame: the frame received from the server (as a byte string)\n\n    :rtype: Frame\n    \"\"\"\n    f = Frame()\n    if frame == b'\\x0a':\n        f.cmd = 'heartbeat'\n        return f\n\n    mat = PREAMBLE_END_RE.search(frame)\n    if mat:\n        preamble_end = mat.start()\n        body_start = mat.end()\n    else:\n        preamble_end = len(frame)\n        body_start = preamble_end\n    preamble = decode(frame[0:preamble_end])\n    preamble_lines = LINE_END_RE.split(preamble)\n    preamble_len = len(preamble_lines)\n    f.body = frame[body_start:]\n\n    # Skip any leading newlines\n    first_line = 0\n    while first_line < preamble_len and len(preamble_lines[first_line]) == 0:\n        first_line += 1\n\n    if first_line >= preamble_len:\n        return None\n\n    # Extract frame type/command\n    f.cmd = preamble_lines[first_line]\n\n    # Put headers into a key/value map\n    f.headers = parse_headers(preamble_lines, first_line + 1)\n\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate what the actual heartbeat settings should be for a given server and client.", "response": "def calculate_heartbeats(shb, chb):\n    \"\"\"\n    Given a heartbeat string from the server, and a heartbeat tuple from the client,\n    calculate what the actual heartbeat settings should be.\n\n    :param (str,str) shb: server heartbeat numbers\n    :param (int,int) chb: client heartbeat numbers\n\n    :rtype: (int,int)\n    \"\"\"\n    (sx, sy) = shb\n    (cx, cy) = chb\n    x = 0\n    y = 0\n    if cx != 0 and sy != '0':\n        x = max(cx, int(sy))\n    if cy != 0 and sx != '0':\n        y = max(cy, int(sx))\n    return x, y"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert_frame(frame, body_encoding=None):\n    lines = []\n\n    body = None\n    if frame.body:\n        if body_encoding:\n            body = encode(frame.body, body_encoding)\n        else:\n            body = encode(frame.body)\n\n        if HDR_CONTENT_LENGTH in frame.headers:\n            frame.headers[HDR_CONTENT_LENGTH] = len(body)\n\n    if frame.cmd:\n        lines.append(encode(frame.cmd))\n        lines.append(ENC_NEWLINE)\n    for key, vals in sorted(frame.headers.items()):\n        if vals is None:\n            continue\n        if type(vals) != tuple:\n            vals = (vals,)\n        for val in vals:\n            lines.append(encode(\"%s:%s\\n\" % (key, val)))\n    lines.append(ENC_NEWLINE)\n    if body:\n        lines.append(body)\n\n    if frame.cmd:\n        lines.append(ENC_NULL)\n    return lines", "response": "Convert a Frame object to a list of lines separated by newlines."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall the protocol disconnect method then stop the transport itself.", "response": "def disconnect(self, receipt=None, headers=None, **keyword_headers):\n        \"\"\"\n        Call the protocol disconnection, and then stop the transport itself.\n\n        :param str receipt: the receipt to use with the disconnect\n        :param dict headers: a map of any additional headers to send with the disconnection\n        :param keyword_headers: any additional headers to send with the disconnection\n        \"\"\"\n        Protocol10.disconnect(self, receipt, headers, **keyword_headers)\n        self.transport.stop()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef disconnect(self, receipt=None, headers=None, **keyword_headers):\n        Protocol11.disconnect(self, receipt, headers, **keyword_headers)\n        self.transport.stop()", "response": "Call the protocol disconnect method then stop the transport itself."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nencode the parameter as a byte string.", "response": "def encode(char_data, encoding='utf-8'):\n    \"\"\"\n    Encode the parameter as a byte string.\n\n    :param char_data: the data to encode\n\n    :rtype: bytes\n    \"\"\"\n    if type(char_data) is str:\n        return char_data.encode(encoding, errors='replace')\n    elif type(char_data) is bytes:\n        return char_data\n    else:\n        raise TypeError('message should be a string or bytes, found %s' % type(char_data))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encode(char_data, encoding='utf-8'):\n    if type(char_data) is unicode:\n        return char_data.encode(encoding, 'replace')\n    else:\n        return char_data", "response": "Encode the parameter as a byte string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall when the connection is established.", "response": "def on_connected(self, headers, body):\n        \"\"\"\n        Once the connection is established, and 'heart-beat' is found in the headers, we calculate the real\n        heartbeat numbers (based on what the server sent and what was specified by the client) - if the heartbeats\n        are not 0, we start up the heartbeat loop accordingly.\n\n        :param dict headers: headers in the connection message\n        :param body: the message body\n        \"\"\"\n        if 'heart-beat' in headers:\n            self.heartbeats = utils.calculate_heartbeats(\n                headers['heart-beat'].replace(' ', '').split(','), self.heartbeats)\n            if self.heartbeats != (0, 0):\n                self.send_sleep = self.heartbeats[0] / 1000\n\n                # by default, receive gets an additional grace of 50%\n                # set a different heart-beat-receive-scale when creating the connection to override that\n                self.receive_sleep = (self.heartbeats[1] / 1000) * self.heart_beat_receive_scale\n                log.debug(\"Setting receive_sleep to %s\", self.receive_sleep)\n\n                # Give grace of receiving the first heartbeat\n                self.received_heartbeat = monotonic() + self.receive_sleep\n\n                self.running = True\n                if self.heartbeat_thread is None:\n                    self.heartbeat_thread = utils.default_create_thread(\n                        self.__heartbeat_loop)\n                    self.heartbeat_thread.name = \"StompHeartbeat%s\" % \\\n                        getattr(self.heartbeat_thread, \"name\", \"Thread\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall when the client sends a new message.", "response": "def on_send(self, frame):\n        \"\"\"\n        Add the heartbeat header to the frame when connecting, and bump\n        next outbound heartbeat timestamp.\n\n        :param Frame frame: the Frame object\n        \"\"\"\n        if frame.cmd == CMD_CONNECT or frame.cmd == CMD_STOMP:\n            if self.heartbeats != (0, 0):\n                frame.headers[HDR_HEARTBEAT] = '%s,%s' % self.heartbeats\n        if self.next_outbound_heartbeat is not None:\n            self.next_outbound_heartbeat = monotonic() + self.send_sleep"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __heartbeat_loop(self):\n        log.info('Starting heartbeat loop')\n        now = monotonic()\n\n        # Setup the initial due time for the outbound heartbeat\n        if self.send_sleep != 0:\n            self.next_outbound_heartbeat = now + self.send_sleep\n\n        while self.running:\n            now = monotonic()\n\n            next_events = []\n            if self.next_outbound_heartbeat is not None:\n                next_events.append(self.next_outbound_heartbeat - now)\n            if self.receive_sleep != 0:\n                t = self.received_heartbeat + self.receive_sleep - now\n                if t > 0:\n                    next_events.append(t)\n            sleep_time = min(next_events)\n            if sleep_time > 0:\n                terminate = self.heartbeat_terminate_event.wait(sleep_time)\n                if terminate:\n                    break\n\n            now = monotonic()\n\n            if not self.transport.is_connected():\n                time.sleep(self.send_sleep)\n                continue\n\n            if self.send_sleep != 0 and now > self.next_outbound_heartbeat:\n                log.debug(\"Sending a heartbeat message at %s\", now)\n                try:\n                    self.transport.transmit(utils.Frame(None, {}, None))\n                except exception.NotConnectedException:\n                    log.debug(\"Lost connection, unable to send heartbeat\")\n                except Exception:\n                    _, e, _ = sys.exc_info()\n                    log.debug(\"Unable to send heartbeat, due to: %s\", e)\n\n            if self.receive_sleep != 0:\n                diff_receive = now - self.received_heartbeat\n\n                if diff_receive > self.receive_sleep:\n                    # heartbeat timeout\n                    log.warning(\"Heartbeat timeout: diff_receive=%s, time=%s, lastrec=%s\",\n                                diff_receive, now, self.received_heartbeat)\n                    self.transport.set_connected(False)\n                    self.transport.disconnect_socket()\n                    self.transport.stop()\n                    for listener in self.transport.listeners.values():\n                        listener.on_heartbeat_timeout()\n        self.heartbeat_thread = None\n        log.info('Heartbeat loop ended')", "response": "This method is called by the heartbeat thread."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalling when a receipt is received from the server.", "response": "def on_receipt(self, headers, body):\n        \"\"\"\n        If the receipt id can be found in the headers, then notify the waiting thread.\n\n        :param dict headers: headers in the message\n        :param body: the message content\n        \"\"\"\n        if 'receipt-id' in headers and headers['receipt-id'] == self.receipt:\n            with self.receipt_condition:\n                self.received = True\n                self.receipt_condition.notify()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwait until we receive a message receipt.", "response": "def wait_on_receipt(self):\n        \"\"\"\n        Wait until we receive a message receipt.\n        \"\"\"\n        with self.receipt_condition:\n            while not self.received:\n                self.receipt_condition.wait()\n        self.received = False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncall when an error occurs in a message.", "response": "def on_error(self, headers, body):\n        \"\"\"\n        Increment the error count. See :py:meth:`ConnectionListener.on_error`\n\n        :param dict headers: headers in the message\n        :param body: the message content\n        \"\"\"\n        if log.isEnabledFor(logging.DEBUG):\n            log.debug(\"received an error %s [%s]\", body, headers)\n        else:\n            log.info(\"received an error %s\", body)\n        self.errors += 1"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef on_connecting(self, host_and_port):\n        log.info(\"connecting %s %s (x %s)\", host_and_port[0], host_and_port[1], self.connections)\n        self.connections += 1", "response": "Increment the connection count."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalling when a new message is sent to the broker.", "response": "def on_send(self, frame):\n        \"\"\"\n        :param Frame frame:\n        \"\"\"\n        print('on_send %s %s %s' % (frame.cmd, frame.headers, frame.body))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send_frame(self, cmd, headers=None, body=''):\n        frame = utils.Frame(cmd, headers, body)\n        self.transport.transmit(frame)", "response": "Encode and send a frame through the underlying transport."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef abort(self, transaction, headers=None, **keyword_headers):\n        assert transaction is not None, \"'transaction' is required\"\n        headers = utils.merge_headers([headers, keyword_headers])\n        headers[HDR_TRANSACTION] = transaction\n        self.send_frame(CMD_ABORT, headers)", "response": "Abort a transaction.\n\n        :param str transaction: the identifier of the transaction\n        :param dict headers: a map of any additional headers the broker requires\n        :param keyword_headers: any additional headers the broker requires"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nacknowledge consumers of a message by id.", "response": "def ack(self, id, transaction=None, receipt=None):\n        \"\"\"\n        Acknowledge 'consumption' of a message by id.\n\n        :param str id: identifier of the message\n        :param str transaction: include the acknowledgement in the specified transaction\n        \"\"\"\n        assert id is not None, \"'id' is required\"\n        headers = {HDR_MESSAGE_ID: id}\n        if transaction:\n            headers[HDR_TRANSACTION] = transaction\n        if receipt:\n            headers[HDR_RECEIPT] = receipt\n        self.send_frame(CMD_ACK, headers)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbegins a transaction. :param str transaction: the identifier for the transaction (optional - if not specified a unique transaction id will be generated) :param dict headers: a map of any additional headers the broker requires :param keyword_headers: any additional headers the broker requires :return: the transaction id :rtype: str", "response": "def begin(self, transaction=None, headers=None, **keyword_headers):\n        \"\"\"\n        Begin a transaction.\n\n        :param str transaction: the identifier for the transaction (optional - if not specified\n            a unique transaction id will be generated)\n        :param dict headers: a map of any additional headers the broker requires\n        :param keyword_headers: any additional headers the broker requires\n\n        :return: the transaction id\n        :rtype: str\n        \"\"\"\n        headers = utils.merge_headers([headers, keyword_headers])\n        if not transaction:\n            transaction = utils.get_uuid()\n        headers[HDR_TRANSACTION] = transaction\n        self.send_frame(CMD_BEGIN, headers)\n        return transaction"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncommit a transaction. :param str transaction: the identifier for the transaction :param dict headers: a map of any additional headers the broker requires :param keyword_headers: any additional headers the broker requires", "response": "def commit(self, transaction=None, headers=None, **keyword_headers):\n        \"\"\"\n        Commit a transaction.\n\n        :param str transaction: the identifier for the transaction\n        :param dict headers: a map of any additional headers the broker requires\n        :param keyword_headers: any additional headers the broker requires\n        \"\"\"\n        assert transaction is not None, \"'transaction' is required\"\n        headers = utils.merge_headers([headers, keyword_headers])\n        headers[HDR_TRANSACTION] = transaction\n        self.send_frame(CMD_COMMIT, headers)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef disconnect(self, receipt=None, headers=None, **keyword_headers):\n        if not self.transport.is_connected():\n            log.debug('Not sending disconnect, already disconnected')\n            return\n        headers = utils.merge_headers([headers, keyword_headers])\n        rec = receipt or utils.get_uuid()\n        headers[HDR_RECEIPT] = rec\n        self.set_receipt(rec, CMD_DISCONNECT)\n        self.send_frame(CMD_DISCONNECT, headers)", "response": "Disconnect from the broker."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a message to a destination.", "response": "def send(self, destination, body, content_type=None, headers=None, **keyword_headers):\n        \"\"\"\n        Send a message to a destination.\n\n        :param str destination: the destination of the message (e.g. queue or topic name)\n        :param body: the content of the message\n        :param str content_type: the content type of the message\n        :param dict headers: a map of any additional headers the broker requires\n        :param keyword_headers: any additional headers the broker requires\n        \"\"\"\n        assert destination is not None, \"'destination' is required\"\n        assert body is not None, \"'body' is required\"\n        headers = utils.merge_headers([headers, keyword_headers])\n        headers[HDR_DESTINATION] = destination\n        if content_type:\n            headers[HDR_CONTENT_TYPE] = content_type\n        if self.auto_content_length and body and HDR_CONTENT_LENGTH not in headers:\n            headers[HDR_CONTENT_LENGTH] = len(body)\n        self.send_frame(CMD_SEND, headers, body)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef subscribe(self, destination, id=None, ack='auto', headers=None, **keyword_headers):\n        assert destination is not None, \"'destination' is required\"\n        headers = utils.merge_headers([headers, keyword_headers])\n        headers[HDR_DESTINATION] = destination\n        if id:\n            headers[HDR_ID] = id\n        headers[HDR_ACK] = ack\n        self.send_frame(CMD_SUBSCRIBE, headers)", "response": "Subscribe to a destination."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unsubscribe(self, destination=None, id=None, headers=None, **keyword_headers):\n        assert id is not None or destination is not None, \"'id' or 'destination' is required\"\n        headers = utils.merge_headers([headers, keyword_headers])\n        if id:\n            headers[HDR_ID] = id\n        if destination:\n            headers[HDR_DESTINATION] = destination\n        self.send_frame(CMD_UNSUBSCRIBE, headers)", "response": "Unsubscribe from a topic or queue."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nencoding and send a frame through the underlying transport", "response": "def send_frame(self, cmd, headers=None, body=''):\n        \"\"\"\n        Encode and send a stomp frame\n        through the underlying transport:\n\n        :param str cmd: the protocol command\n        :param dict headers: a map of headers to include in the frame\n        :param body: the content of the message\n        \"\"\"\n        if cmd != CMD_CONNECT:\n            if headers is None:\n                headers = {}\n            self._escape_headers(headers)\n        frame = utils.Frame(cmd, headers, body)\n        self.transport.transmit(frame)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef connect(self, username=None, passcode=None, wait=False, headers=None, **keyword_headers):\n        cmd = CMD_STOMP\n        headers = utils.merge_headers([headers, keyword_headers])\n        headers[HDR_ACCEPT_VERSION] = self.version\n\n        if self.transport.vhost:\n            headers[HDR_HOST] = self.transport.vhost\n\n        if username is not None:\n            headers[HDR_LOGIN] = username\n\n        if passcode is not None:\n            headers[HDR_PASSCODE] = passcode\n\n        self.send_frame(cmd, headers)\n\n        if wait:\n            self.transport.wait_for_connection()\n            if self.transport.connection_error:\n                raise ConnectFailedException()", "response": "Connect to the broker with the specified username and password."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef nack(self, id, subscription, transaction=None, receipt=None):\n        assert id is not None, \"'id' is required\"\n        assert subscription is not None, \"'subscription' is required\"\n        headers = {HDR_MESSAGE_ID: id, HDR_SUBSCRIPTION: subscription}\n        if transaction:\n            headers[HDR_TRANSACTION] = transaction\n        if receipt:\n            headers[HDR_RECEIPT] = receipt\n        self.send_frame(CMD_NACK, headers)", "response": "Nack a message in the specified subscription"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _escape_headers(self, headers):\n        for key, val in headers.items():\n            try:\n                val = val.replace('\\\\', '\\\\\\\\').replace('\\n', '\\\\n').replace(':', '\\\\c').replace('\\r', '\\\\r')\n            except:\n                pass\n            headers[key] = val", "response": "Escapes the headers in the log file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef nack(self, id, transaction=None, receipt=None):\n        assert id is not None, \"'id' is required\"\n        headers = {HDR_ID: id}\n        if transaction:\n            headers[HDR_TRANSACTION] = transaction\n        if receipt:\n            headers[HDR_RECEIPT] = receipt\n        self.send_frame(CMD_NACK, headers)", "response": "Nack a message in the specified transaction and receipt."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a socket from a list of hosts and ports.", "response": "def get_socket(host, port, timeout=None):\n    \"\"\"\n    Return a socket.\n\n    :param str host: the hostname to connect to\n    :param int port: the port number to connect to\n    :param timeout: if specified, set the socket timeout\n    \"\"\"\n    for res in getaddrinfo(host, port, 0, SOCK_STREAM):\n        af, socktype, proto, canonname, sa = res\n        sock = None\n        try:\n            sock = socket(af, socktype, proto)\n            if timeout is not None:\n                sock.settimeout(timeout)\n            sock.connect(sa)\n            return sock\n\n        except error:\n            if sock is not None:\n                sock.close()\n\n    raise error"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart the connection. This should be called after all listeners have been registered. If this method is not called, no frames will be received by the connection.", "response": "def start(self):\n        \"\"\"\n        Start the connection. This should be called after all\n        listeners have been registered. If this method is not called,\n        no frames will be received by the connection.\n        \"\"\"\n        self.running = True\n        self.attempt_connection()\n        receiver_thread = self.create_thread_fc(self.__receiver_loop)\n        receiver_thread.name = \"StompReceiver%s\" % getattr(receiver_thread, \"name\", \"Thread\")\n        self.notify('connecting')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stop(self):\n        with self.__receiver_thread_exit_condition:\n            while not self.__receiver_thread_exited and self.is_connected():\n                self.__receiver_thread_exit_condition.wait()", "response": "Stop the connection. Performs a clean shutdown by waiting for the\n        receiver thread to exit."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the connected flag.", "response": "def set_connected(self, connected):\n        \"\"\"\n        :param bool connected:\n        \"\"\"\n        with self.__connect_wait_condition:\n            self.connected = connected\n            if connected:\n                self.__connect_wait_condition.notify()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_listener(self, name, listener):\n        with self.__listeners_change_condition:\n            self.listeners[name] = listener", "response": "Set a named listener to use with this connection."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess a single frame from the server.", "response": "def process_frame(self, f, frame_str):\n        \"\"\"\n        :param Frame f: Frame object\n        :param bytes frame_str: raw frame content\n        \"\"\"\n        frame_type = f.cmd.lower()\n        if frame_type in ['connected', 'message', 'receipt', 'error', 'heartbeat']:\n            if frame_type == 'message':\n                (f.headers, f.body) = self.notify('before_message', f.headers, f.body)\n            if log.isEnabledFor(logging.DEBUG):\n                log.debug(\"Received frame: %r, headers=%r, body=%r\", f.cmd, f.headers, f.body)\n            else:\n                log.info(\"Received frame: %r, len(body)=%r\", f.cmd, utils.length(f.body))\n            self.notify(frame_type, f.headers, f.body)\n        else:\n            log.warning(\"Unknown response frame type: '%s' (frame length was %d)\", frame_type, utils.length(frame_str))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef transmit(self, frame):\n        with self.__listeners_change_condition:\n            listeners = sorted(self.listeners.items())\n\n        for (_, listener) in listeners:\n            if not listener:\n                continue\n            try:\n                listener.on_send(frame)\n            except AttributeError:\n                continue\n\n        if frame.cmd == CMD_DISCONNECT and HDR_RECEIPT in frame.headers:\n            self.__disconnect_receipt = frame.headers[HDR_RECEIPT]\n\n        lines = utils.convert_frame(frame)\n        packed_frame = pack(lines)\n\n        if log.isEnabledFor(logging.DEBUG):\n            log.debug(\"Sending frame: %s\", lines)\n        else:\n            log.info(\"Sending frame: %r\", frame.cmd or \"heartbeat\")\n        self.send(packed_frame)", "response": "Convert a frame object to a frame string and transmit to the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwaits until we have established a connection with the server.", "response": "def wait_for_connection(self, timeout=None):\n        \"\"\"\n        Wait until we've established a connection with the server.\n\n        :param float timeout: how long to wait, in seconds\n        \"\"\"\n        if timeout is not None:\n            wait_time = timeout / 10.0\n        else:\n            wait_time = None\n        with self.__connect_wait_condition:\n            while self.running and not self.is_connected() and not self.connection_error:\n                self.__connect_wait_condition.wait(wait_time)\n        if not self.running or not self.is_connected():\n            raise exception.ConnectFailedException()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __receiver_loop(self):\n        log.info(\"Starting receiver loop\")\n        notify_disconnected = True\n        try:\n            while self.running:\n                try:\n                    while self.running:\n                        frames = self.__read()\n\n                        for frame in frames:\n                            f = utils.parse_frame(frame)\n                            if f is None:\n                                continue\n                            if self.__auto_decode:\n                                f.body = decode(f.body)\n                            self.process_frame(f, frame)\n                except exception.ConnectionClosedException:\n                    if self.running:\n                        #\n                        # Clear out any half-received messages after losing connection\n                        #\n                        self.__recvbuf = b''\n                        self.running = False\n                        notify_disconnected = True\n                    break\n                finally:\n                    self.cleanup()\n        finally:\n            with self.__receiver_thread_exit_condition:\n                self.__receiver_thread_exited = True\n                self.__receiver_thread_exit_condition.notifyAll()\n            log.info(\"Receiver loop ended\")\n            self.notify('receiver_loop_completed')\n            if notify_disconnected:\n                self.notify('disconnected')\n            with self.__connect_wait_condition:\n                self.__connect_wait_condition.notifyAll()", "response": "Main loop for receiving incoming data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __read(self):\n        fastbuf = BytesIO()\n        while self.running:\n            try:\n                try:\n                    c = self.receive()\n                except exception.InterruptedException:\n                    log.debug(\"socket read interrupted, restarting\")\n                    continue\n            except Exception:\n                log.debug(\"socket read error\", exc_info=True)\n                c = b''\n            if c is None or len(c) == 0:\n                raise exception.ConnectionClosedException()\n            if c == b'\\x0a' and not self.__recvbuf and not fastbuf.tell():\n                #\n                # EOL to an empty receive buffer: treat as heartbeat.\n                # Note that this may misdetect an optional EOL at end of frame as heartbeat in case the\n                # previous receive() got a complete frame whose NUL at end of frame happened to be the\n                # last byte of that read. But that should be harmless in practice.\n                #\n                fastbuf.close()\n                return [c]\n            fastbuf.write(c)\n            if b'\\x00' in c:\n                #\n                # Possible end of frame\n                #\n                break\n        self.__recvbuf += fastbuf.getvalue()\n        fastbuf.close()\n        result = []\n\n        if self.__recvbuf and self.running:\n            while True:\n                pos = self.__recvbuf.find(b'\\x00')\n\n                if pos >= 0:\n                    frame = self.__recvbuf[0:pos]\n                    preamble_end_match = utils.PREAMBLE_END_RE.search(frame)\n                    if preamble_end_match:\n                        preamble_end = preamble_end_match.start()\n                        content_length_match = BaseTransport.__content_length_re.search(frame[0:preamble_end])\n                        if content_length_match:\n                            content_length = int(content_length_match.group('value'))\n                            content_offset = preamble_end_match.end()\n                            frame_size = content_offset + content_length\n                            if frame_size > len(frame):\n                                #\n                                # Frame contains NUL bytes, need to read more\n                                #\n                                if frame_size < len(self.__recvbuf):\n                                    pos = frame_size\n                                    frame = self.__recvbuf[0:pos]\n                                else:\n                                    #\n                                    # Haven't read enough data yet, exit loop and wait for more to arrive\n                                    #\n                                    break\n                    result.append(frame)\n                    pos += 1\n                    #\n                    # Ignore optional EOLs at end of frame\n                    #\n                    while self.__recvbuf[pos:pos + 1] == b'\\x0a':\n                        pos += 1\n                    self.__recvbuf = self.__recvbuf[pos:]\n                else:\n                    break\n        return result", "response": "Read the next NUL frame from the socket."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_connected(self):\n        try:\n            return self.socket is not None and self.socket.getsockname()[1] != 0 and BaseTransport.is_connected(self)\n        except socket.error:\n            return False", "response": "Returns true if the socket managed by this connection is connected otherwise False."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef disconnect_socket(self):\n        self.running = False\n        if self.socket is not None:\n            if self.__need_ssl():\n                #\n                # Even though we don't want to use the socket, unwrap is the only API method which does a proper SSL\n                # shutdown\n                #\n                try:\n                    self.socket = self.socket.unwrap()\n                except Exception:\n                    #\n                    # unwrap seems flaky on Win with the back-ported ssl mod, so catch any exception and log it\n                    #\n                    _, e, _ = sys.exc_info()\n                    log.warning(e)\n            elif hasattr(socket, 'SHUT_RDWR'):\n                try:\n                    self.socket.shutdown(socket.SHUT_RDWR)\n                except socket.error:\n                    _, e, _ = sys.exc_info()\n                    # ignore when socket already closed\n                    if get_errno(e) != errno.ENOTCONN:\n                        log.warning(\"Unable to issue SHUT_RDWR on socket because of error '%s'\", e)\n\n        #\n        # split this into a separate check, because sometimes the socket is nulled between shutdown and this call\n        #\n        if self.socket is not None:\n            try:\n                self.socket.close()\n            except socket.error:\n                _, e, _ = sys.exc_info()\n                log.warning(\"Unable to close socket because of error '%s'\", e)\n        self.current_host_and_port = None\n        self.socket = None\n        self.notify('disconnected')", "response": "Disconnect the underlying socket connection"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send(self, encoded_frame):\n        if self.socket is not None:\n            try:\n                with self.__socket_semaphore:\n                    self.socket.sendall(encoded_frame)\n            except Exception:\n                _, e, _ = sys.exc_info()\n                log.error(\"Error sending frame\", exc_info=1)\n                raise e\n        else:\n            raise exception.NotConnectedException()", "response": "Sends a frame to the broker."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads and returns a bytes object of the next available tag.", "response": "def receive(self):\n        \"\"\"\n        :rtype: bytes\n        \"\"\"\n        try:\n            return self.socket.recv(self.__recv_bytes)\n        except socket.error:\n            _, e, _ = sys.exc_info()\n            if get_errno(e) in (errno.EAGAIN, errno.EINTR):\n                log.debug(\"socket read interrupted, restarting\")\n                raise exception.InterruptedException()\n            if self.is_connected():\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nattempt to connect to the server.", "response": "def attempt_connection(self):\n        \"\"\"\n        Try connecting to the (host, port) tuples specified at construction time.\n        \"\"\"\n        self.connection_error = False\n        sleep_exp = 1\n        connect_count = 0\n\n        while self.running and self.socket is None and (\n            connect_count < self.__reconnect_attempts_max or \n            self.__reconnect_attempts_max == -1 ):\n            for host_and_port in self.__host_and_ports:\n                try:\n                    log.info(\"Attempting connection to host %s, port %s\", host_and_port[0], host_and_port[1])\n                    self.socket = get_socket(host_and_port[0], host_and_port[1], self.__timeout)\n                    self.__enable_keepalive()\n                    need_ssl = self.__need_ssl(host_and_port)\n\n                    if need_ssl:  # wrap socket\n                        ssl_params = self.get_ssl(host_and_port)\n                        if ssl_params['ca_certs']:\n                            cert_validation = ssl.CERT_REQUIRED\n                        else:\n                            cert_validation = ssl.CERT_NONE\n                        try:\n                            tls_context = ssl.create_default_context(cafile=ssl_params['ca_certs'])\n                        except AttributeError:\n                            tls_context = None\n                        if tls_context:\n                            # Wrap the socket for TLS\n                            certfile = ssl_params['cert_file']\n                            keyfile = ssl_params['key_file']\n                            password = ssl_params.get('password')\n                            if certfile and not keyfile:\n                                keyfile = certfile\n                            if certfile:\n                                tls_context.load_cert_chain(certfile, keyfile, password)\n                            if cert_validation is None or cert_validation == ssl.CERT_NONE:\n                                tls_context.check_hostname = False\n                            tls_context.verify_mode = cert_validation\n                            self.socket = tls_context.wrap_socket(self.socket, server_hostname=host_and_port[0])\n                        else:\n                            # Old-style wrap_socket where we don't have a modern SSLContext (so no SNI)\n                            self.socket = ssl.wrap_socket(\n                                self.socket,\n                                keyfile=ssl_params['key_file'],\n                                certfile=ssl_params['cert_file'],\n                                cert_reqs=cert_validation,\n                                ca_certs=ssl_params['ca_certs'],\n                                ssl_version=ssl_params['ssl_version'])\n\n                    self.socket.settimeout(self.__timeout)\n\n                    if self.blocking is not None:\n                        self.socket.setblocking(self.blocking)\n\n                    #\n                    # Validate server cert\n                    #\n                    if need_ssl and ssl_params['cert_validator']:\n                        cert = self.socket.getpeercert()\n                        (ok, errmsg) = ssl_params['cert_validator'](cert, host_and_port[0])\n                        if not ok:\n                            raise SSLError(\"Server certificate validation failed: %s\", errmsg)\n\n                    self.current_host_and_port = host_and_port\n                    log.info(\"Established connection to host %s, port %s\", host_and_port[0], host_and_port[1])\n                    break\n                except socket.error:\n                    self.socket = None\n                    connect_count += 1\n                    log.warning(\"Could not connect to host %s, port %s\", host_and_port[0], host_and_port[1], exc_info=1)\n\n            if self.socket is None:\n                sleep_duration = (min(self.__reconnect_sleep_max,\n                                      ((self.__reconnect_sleep_initial / (1.0 + self.__reconnect_sleep_increase))\n                                       * math.pow(1.0 + self.__reconnect_sleep_increase, sleep_exp)))\n                                  * (1.0 + random.random() * self.__reconnect_sleep_jitter))\n                sleep_end = monotonic() + sleep_duration\n                log.debug(\"Sleeping for %.1f seconds before attempting reconnect\", sleep_duration)\n                while self.running and monotonic() < sleep_end:\n                    time.sleep(0.2)\n\n                if sleep_duration < self.__reconnect_sleep_max:\n                    sleep_exp += 1\n\n        if not self.socket:\n            raise exception.ConnectFailedException()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset up the SSL connection parameters for the given hosts.", "response": "def set_ssl(self,\n                for_hosts=[],\n                key_file=None,\n                cert_file=None,\n                ca_certs=None,\n                cert_validator=None,\n                ssl_version=DEFAULT_SSL_VERSION,\n                password=None):\n        \"\"\"\n        Sets up SSL configuration for the given hosts. This ensures socket is wrapped in a SSL connection, raising an\n        exception if the SSL module can't be found.\n\n        :param for_hosts: a list of tuples describing hosts this SSL configuration should be applied to\n        :param cert_file: the path to a X509 certificate\n        :param key_file: the path to a X509 key file\n        :param ca_certs: the path to the a file containing CA certificates to validate the server against.\n                         If this is not set, server side certificate validation is not done.\n        :param cert_validator: function which performs extra validation on the client certificate, for example\n                               checking the returned certificate has a commonName attribute equal to the\n                               hostname (to avoid man in the middle attacks).\n                               The signature is: (OK, err_msg) = validation_function(cert, hostname)\n                               where OK is a boolean, and cert is a certificate structure\n                               as returned by ssl.SSLSocket.getpeercert()\n        :param ssl_version: SSL protocol to use for the connection. This should be one of the PROTOCOL_x\n                            constants provided by the ssl module. The default is ssl.PROTOCOL_TLSv1\n        \"\"\"\n        if not ssl:\n            raise Exception(\"SSL connection requested, but SSL library not found\")\n\n        for host_port in for_hosts:\n            self.__ssl_params[host_port] = dict(key_file=key_file,\n                                                cert_file=cert_file,\n                                                ca_certs=ca_certs,\n                                                cert_validator=cert_validator,\n                                                ssl_version=ssl_version,\n                                                password=password)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __need_ssl(self, host_and_port=None):\n        if not host_and_port:\n            host_and_port = self.current_host_and_port\n\n        return host_and_port in self.__ssl_params", "response": "Returns True if the current host needs SSL or False if it does not."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_ssl(self, host_and_port=None):\n        if not host_and_port:\n            host_and_port = self.current_host_and_port\n\n        return self.__ssl_params.get(host_and_port)", "response": "Get the SSL params for the given host and port."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __print_async(self, frame_type, headers, body):\n        if self.__quit:\n            return\n        if self.verbose:\n            self.__sysout(frame_type)\n            for k, v in headers.items():\n                self.__sysout('%s: %s' % (k, v))\n        else:\n            if 'message-id' in headers:\n                self.__sysout('message-id: %s' % headers['message-id'])\n            if 'subscription' in headers:\n                self.__sysout('subscription: %s' % headers['subscription'])\n        if self.prompt != '':\n            self.__sysout('')\n        self.__sysout(body)\n        if not self.__start:\n            self.__sysout(self.prompt, end='')\n        else:\n            self.__start = False\n        self.stdout.flush()", "response": "Utility function to print a message and setup the command prompt\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_message(self, headers, body):\n        self.__sysout('')\n        if 'filename' in headers:\n            content = base64.b64decode(body.encode())\n            if os.path.exists(headers['filename']):\n                fname = '%s.%s' % (headers['filename'], int(time.time()))\n            else:\n                fname = headers['filename']\n            with open(fname, 'wb') as f:\n                f.write(content)\n            self.__print_async(\"MESSAGE\", headers, \"Saved file: %s\" % fname)\n        else:\n            self.__print_async(\"MESSAGE\", headers, body)", "response": "This method is called when a message is received from the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tokenize(text, lang, include_punctuation=False, external_wordlist=False):\n    # Use globals to load CJK tokenizers on demand, so that we can still run\n    # in environments that lack the CJK dependencies\n    global _mecab_tokenize, _jieba_tokenize\n\n    language = langcodes.get(lang)\n    info = get_language_info(language)\n    text = preprocess_text(text, language)\n\n    if info['tokenizer'] == 'mecab':\n        from wordfreq.mecab import mecab_tokenize as _mecab_tokenize\n        # Get just the language code out of the Language object, so we can\n        # use it to select a MeCab dictionary\n        tokens = _mecab_tokenize(text, language.language)\n        if not include_punctuation:\n            tokens = [token for token in tokens if not PUNCT_RE.match(token)]\n    elif info['tokenizer'] == 'jieba':\n        from wordfreq.chinese import jieba_tokenize as _jieba_tokenize\n        tokens = _jieba_tokenize(text, external_wordlist=external_wordlist)\n        if not include_punctuation:\n            tokens = [token for token in tokens if not PUNCT_RE.match(token)]\n    else:\n        # This is the default case where we use the regex tokenizer. First\n        # let's complain a bit if we ended up here because we don't have an\n        # appropriate tokenizer.\n        if info['tokenizer'] != 'regex' and lang not in _WARNED_LANGUAGES:\n            logger.warning(\n                \"The language '{}' is in the '{}' script, which we don't \"\n                \"have a tokenizer for. The results will be bad.\"\n                .format(lang, info['script'])\n            )\n            _WARNED_LANGUAGES.add(lang)\n        tokens = simple_tokenize(text, include_punctuation=include_punctuation)\n\n    return tokens", "response": "Tokenize a text into a CJK word - frequency list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lossy_tokenize(text, lang, include_punctuation=False, external_wordlist=False):\n    global _simplify_chinese\n\n    info = get_language_info(lang)\n    tokens = tokenize(text, lang, include_punctuation, external_wordlist)\n\n    if info['lookup_transliteration'] == 'zh-Hans':\n        from wordfreq.chinese import simplify_chinese as _simplify_chinese\n        tokens = [_simplify_chinese(token) for token in tokens]\n\n    return [smash_numbers(token) for token in tokens]", "response": "Returns a list of tokens for this text in a lossy way."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads a file from a cBpack format that we can store in a dict.", "response": "def read_cBpack(filename):\n    \"\"\"\n    Read a file from an idiosyncratic format that we use for storing\n    approximate word frequencies, called \"cBpack\".\n\n    The cBpack format is as follows:\n\n    - The file on disk is a gzipped file in msgpack format, which decodes to a\n      list whose first element is a header, and whose remaining elements are\n      lists of words.\n\n    - The header is a dictionary with 'format' and 'version' keys that make\n      sure that we're reading the right thing.\n\n    - Each inner list of words corresponds to a particular word frequency,\n      rounded to the nearest centibel -- that is, one tenth of a decibel, or\n      a factor of 10 ** .01.\n\n      0 cB represents a word that occurs with probability 1, so it is the only\n      word in the data (this of course doesn't happen). -200 cB represents a\n      word that occurs once per 100 tokens, -300 cB represents a word that\n      occurs once per 1000 tokens, and so on.\n\n    - The index of each list within the overall list (without the header) is\n      the negative of its frequency in centibels.\n\n    - Each inner list is sorted in alphabetical order.\n\n    As an example, consider a corpus consisting only of the words \"red fish\n    blue fish\". The word \"fish\" occurs as 50% of tokens (-30 cB), while \"red\"\n    and \"blue\" occur as 25% of tokens (-60 cB). The cBpack file of their word\n    frequencies would decode to this:\n\n        [\n            {'format': 'cB', 'version': 1},\n            [], [], [], ...    # 30 empty lists\n            ['fish'],\n            [], [], [], ...    # 29 more empty lists\n            ['blue', 'red']\n        ]\n    \"\"\"\n    with gzip.open(filename, 'rb') as infile:\n        data = msgpack.load(infile, raw=False)\n    header = data[0]\n    if (\n        not isinstance(header, dict) or header.get('format') != 'cB'\n        or header.get('version') != 1\n    ):\n        raise ValueError(\"Unexpected header: %r\" % header)\n    return data[1:]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef available_languages(wordlist='best'):\n    if wordlist == 'best':\n        available = available_languages('small')\n        available.update(available_languages('large'))\n        return available\n    elif wordlist == 'combined':\n        logger.warning(\n            \"The 'combined' wordlists have been renamed to 'small'.\"\n        )\n        wordlist = 'small'\n\n    available = {}\n    for path in DATA_PATH.glob('*.msgpack.gz'):\n        if not path.name.startswith('_'):\n            list_name = path.name.split('.')[0]\n            name, lang = list_name.split('_')\n            if name == wordlist:\n                available[lang] = str(path)\n    return available", "response": "Given a wordlist name return a dictionary of language codes to filenames representing all the languages in which that wordlist is available."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_frequency_list(lang, wordlist='best', match_cutoff=30):\n    available = available_languages(wordlist)\n    best, score = langcodes.best_match(lang, list(available),\n                                       min_score=match_cutoff)\n    if score == 0:\n        raise LookupError(\"No wordlist %r available for language %r\"\n                          % (wordlist, lang))\n\n    if best != lang:\n        logger.warning(\n            \"You asked for word frequencies in language %r. Using the \"\n            \"nearest match, which is %r (%s).\"\n            % (lang, best, langcodes.get(best).language_name('en'))\n        )\n\n    return read_cBpack(available[best])", "response": "Read the raw data from a wordlist file returning it as a list of\n    lists."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a word frequency list as a dictionary mapping tokens to their frequency.", "response": "def get_frequency_dict(lang, wordlist='best', match_cutoff=30):\n    \"\"\"\n    Get a word frequency list as a dictionary, mapping tokens to\n    frequencies as floating-point probabilities.\n    \"\"\"\n    freqs = {}\n    pack = get_frequency_list(lang, wordlist, match_cutoff)\n    for index, bucket in enumerate(pack):\n        freq = cB_to_freq(-index)\n        for word in bucket:\n            freqs[word] = freq\n    return freqs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef word_frequency(word, lang, wordlist='best', minimum=0.):\n    args = (word, lang, wordlist, minimum)\n    try:\n        return _wf_cache[args]\n    except KeyError:\n        if len(_wf_cache) >= CACHE_SIZE:\n            _wf_cache.clear()\n        _wf_cache[args] = _word_frequency(*args)\n        return _wf_cache[args]", "response": "Get the frequency of a word in the language with code lang."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef zipf_frequency(word, lang, wordlist='best', minimum=0.):\n    freq_min = zipf_to_freq(minimum)\n    freq = word_frequency(word, lang, wordlist, freq_min)\n    return round(freq_to_zipf(freq), 2)", "response": "Returns the frequency of word in language with code lang in the order of the word frequency."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a frequency list of length n in descending order of frequency.", "response": "def top_n_list(lang, n, wordlist='best', ascii_only=False):\n    \"\"\"\n    Return a frequency list of length `n` in descending order of frequency.\n    This list contains words from `wordlist`, of the given language.\n    If `ascii_only`, then only ascii words are considered.\n    \"\"\"\n    results = []\n    for word in iter_wordlist(lang, wordlist):\n        if (not ascii_only) or max(word) <= '~':\n            results.append(word)\n            if len(results) >= n:\n                break\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef random_words(lang='en', wordlist='best', nwords=5, bits_per_word=12,\n                 ascii_only=False):\n    \"\"\"\n    Returns a string of random, space separated words.\n\n    These words are of the given language and from the given wordlist.\n    There will be `nwords` words in the string.\n\n    `bits_per_word` determines the amount of entropy provided by each word;\n    when it's higher, this function will choose from a larger list of\n    words, some of which are more rare.\n\n    You can restrict the selection of words to those written in ASCII\n    characters by setting `ascii_only` to True.\n    \"\"\"\n    n_choices = 2 ** bits_per_word\n    choices = top_n_list(lang, n_choices, wordlist, ascii_only=ascii_only)\n    if len(choices) < n_choices:\n        raise ValueError(\n            \"There aren't enough words in the wordlist to provide %d bits of \"\n            \"entropy per word.\" % bits_per_word\n        )\n    return ' '.join([random.choice(choices) for i in range(nwords)])", "response": "Returns a random string of space separated words."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a random string of random space separated ASCII words.", "response": "def random_ascii_words(lang='en', wordlist='best', nwords=5,\n                       bits_per_word=12):\n    \"\"\"\n    Returns a string of random, space separated, ASCII words.\n\n    These words are of the given language and from the given wordlist.\n    There will be `nwords` words in the string.\n\n    `bits_per_word` determines the amount of entropy provided by each word;\n    when it's higher, this function will choose from a larger list of\n    words, some of which are more rare.\n    \"\"\"\n    return random_words(lang, wordlist, nwords, bits_per_word, ascii_only=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef preprocess_text(text, language):\n    # NFC or NFKC normalization, as needed for the language\n    info = get_language_info(language)\n    text = unicodedata.normalize(info['normal_form'], text)\n\n    # Transliteration of multi-script languages\n    if info['transliteration'] is not None:\n        text = transliterate(info['transliteration'], text)\n\n    # Abjad mark removal\n    if info['remove_marks']:\n        text = remove_marks(text)\n\n    # Case folding\n    if info['dotless_i']:\n        text = casefold_with_i_dots(text)\n    else:\n        text = text.casefold()\n\n    # Fixing of diacritics\n    if info['diacritics_under'] == 'commas':\n        text = cedillas_to_commas(text)\n    elif info['diacritics_under'] == 'cedillas':\n        text = commas_to_cedillas(text)\n\n    return text", "response": "This function preprocesses the given text for the given language."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert capital I s and capital dotted \u0130 s to lowercase in the way that s appropriate for Turkish and related languages.", "response": "def casefold_with_i_dots(text):\n    \"\"\"\n    Convert capital I's and capital dotted \u0130's to lowercase in the way\n    that's appropriate for Turkish and related languages, then case-fold\n    the rest of the letters.\n    \"\"\"\n    text = unicodedata.normalize('NFC', text).replace('\u0130', 'i').replace('I', '\u0131')\n    return text.casefold()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a language returns a dictionary of the language - specific information that can be used to parse the text in that language.", "response": "def get_language_info(language):\n    \"\"\"\n    Looks up the things we need to know about how to handle text in a given\n    language. This will return a dictionary with the following fields:\n\n    'script': a BCP 47 script code such as 'Latn', 'Cyrl', 'Hans'...\n\n        Indicates the script that tokens in this language should be in,\n        _after_ our preprocessing. The script for 'zh' is 'Hans', for example,\n        because even if the input is in Traditional Chinese ('Hant'), we\n        convert it to Simplified.\n\n    'tokenizer': 'regex', 'jieba', 'mecab', or None\n\n        Indicates the best way we know to separate tokens in the language.\n\n        'regex' is what will be used for most languages, meaning that we can\n        segment the text with a Unicode-aware regular expression. If a language\n        generally uses spaces to separate words, the regex will work well.\n\n        'jieba' and 'mecab' are tokenizers for specific languages written\n        without spaces.\n\n        A tokenizer of None means we don't have a good way to segment the\n        language. We'll use the regex anyway, but the results will be pretty\n        bad.\n\n    'normal_form': 'NFC' or 'NFKC'\n\n        How \"should\" Unicode be normalized when comparing text in this\n        language? This is not a standard, it's just based on experience.\n        Many languages need NFKC normalization for text comparisons to work\n        properly, but in many European languages, NFKC normalization is\n        excessive and loses information.\n\n    'remove_marks': True or False\n\n        Determines whether marks and decorations, such as vowel points and\n        tatweels, should be removed. True for languages in abjad scripts.\n\n    'dotless_i': True or False\n\n        Is \"\u0131\" the lowercase of \"I\" in this language, as in Turkish?\n\n    'diacritics_under': 'cedillas', 'commas', or None\n\n        Should we convert any diacritics that are under the letters \"s\" and\n        \"t\" in this language? 'cedillas' means we should convert commas to\n        cedillas, and 'commas' means we should convert cedillas to commas.\n\n    'transliteration': 'sr-Latn', 'az-Latn', or None\n\n        Indicates a type of transliteration that we should use for normalizing\n        a multi-script language. 'sr-Latn' means to use Serbian romanization,\n        and 'az-Latn' means to use Azerbaijani romanization.\n\n    'lookup_transliteration': 'zh-Hans' or None\n\n        Indicates a lossy transliteration that should be not be used for output,\n        but should be applied when looking up words in a list. 'zh-Hans' means\n        that we should convert Traditional Chinese characters to Simplified.\n    \"\"\"\n    # The input is probably a string, so parse it into a Language. If it's\n    # already a Language, it will pass through.\n    language = Language.get(language)\n\n    # Assume additional things about the language, such as what script it's in,\n    # using the \"likely subtags\" table\n    language_full = language.maximize()\n\n    # Start the `info` dictionary with default values, including the 'script'\n    # value that we now know from `language_full`.\n    info = {\n        'script': language_full.script,\n        'tokenizer': 'regex',\n        'normal_form': 'NFKC',\n        'remove_marks': False,\n        'dotless_i': False,\n        'diacritics_under': None,\n        'transliteration': None,\n        'lookup_transliteration': None\n    }\n\n    if _language_in_list(language, ['ja', 'ko']):\n        info['tokenizer'] = 'mecab'\n    elif _language_in_list(language, ['zh', 'yue']):\n        info['tokenizer'] = 'jieba'\n    elif info['script'] in SPACELESS_SCRIPTS:\n        info['tokenizer'] = None\n\n    # Cased alphabetic scripts get NFC normal form\n    if info['script'] in ['Latn', 'Grek', 'Cyrl']:\n        info['normal_form'] = 'NFC'\n\n    if info['script'] in ['Arab', 'Hebr']:\n        info['remove_marks'] = True\n\n    if _language_in_list(language, ['tr', 'az', 'kk']):\n        info['dotless_i'] = True\n        info['diacritics_under'] = 'cedillas'\n    elif _language_in_list(language, ['ro']):\n        info['diacritics_under'] = 'commas'\n\n    if _language_in_list(language, ['sr']):\n        info['transliteration'] = 'sr-Latn'\n    elif _language_in_list(language, ['az']):\n        info['transliteration'] = 'az-Latn'\n\n    if language.language == 'zh' and language.script != 'Hant':\n        info['lookup_transliteration'] = 'zh-Hans'\n\n    return info"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind a MeCab dictionary with a given name.", "response": "def find_mecab_dictionary(names):\n    \"\"\"\n    Find a MeCab dictionary with a given name. The dictionary has to be\n    installed separately -- see wordfreq's README for instructions.\n    \"\"\"\n    suggested_pkg = names[0]\n    paths = [\n        os.path.expanduser('~/.local/lib/mecab/dic'),\n        '/var/lib/mecab/dic',\n        '/var/local/lib/mecab/dic',\n        '/usr/lib/mecab/dic',\n        '/usr/local/lib/mecab/dic',\n        '/usr/lib/x86_64-linux-gnu/mecab/dic',\n    ]\n    full_paths = [os.path.join(path, name) for path in paths for name in names]\n    checked_paths = [path for path in full_paths if len(path) <= MAX_PATH_LENGTH]\n    for path in checked_paths:\n        if os.path.exists(path):\n            return path\n\n    error_lines = [\n        \"Couldn't find the MeCab dictionary named %r.\" % suggested_pkg,\n        \"You should download or use your system's package manager to install\",\n        \"the %r package.\" % suggested_pkg,\n        \"\",\n        \"We looked in the following locations:\"\n    ] + [\"\\t%s\" % path for path in checked_paths]\n\n    skipped_paths = [path for path in full_paths if len(path) > MAX_PATH_LENGTH]\n    if skipped_paths:\n        error_lines += [\n            \"We had to skip these paths that are too long for MeCab to find:\",\n        ] + [\"\\t%s\" % path for path in skipped_paths]\n\n    raise OSError('\\n'.join(error_lines))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mecab_tokenize(text, lang):\n    if lang not in MECAB_DICTIONARY_NAMES:\n        raise ValueError(\"Can't run MeCab on language %r\" % lang)\n    if lang not in MECAB_ANALYZERS:\n        MECAB_ANALYZERS[lang] = make_mecab_analyzer(MECAB_DICTIONARY_NAMES[lang])\n\n    analyzer = MECAB_ANALYZERS[lang]\n    text = unicodedata.normalize('NFKC', text.strip())\n    analyzed = analyzer.parse(text)\n    if not analyzed:\n        return []\n    return [line.split('\\t')[0]\n            for line in analyzed.split('\\n')\n            if line != '' and line != 'EOS']", "response": "Tokenize a text string using the Mecab - python3 package."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntransliterates text according to one of the tables above.", "response": "def transliterate(table, text):\n    \"\"\"\n    Transliterate text according to one of the tables above.\n\n    `table` chooses the table. It looks like a language code but comes from a\n    very restricted set:\n\n    - 'sr-Latn' means to convert Serbian, which may be in Cyrillic, into the\n      Latin alphabet.\n    - 'az-Latn' means the same for Azerbaijani Cyrillic to Latn.\n    \"\"\"\n    if table == 'sr-Latn':\n        return text.translate(SR_LATN_TABLE)\n    elif table == 'az-Latn':\n        return text.translate(AZ_LATN_TABLE)\n    else:\n        raise ValueError(\"Unknown transliteration table: {!r}\".format(table))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the exit code based on the Statistics.", "response": "def _update_exit_code_from_stats(cls, statistics: Statistics,\n                                     app: Application):\n        '''Set the current exit code based on the Statistics.'''\n        for error_type in statistics.errors:\n            exit_code = app.ERROR_CODE_MAP.get(error_type)\n            if exit_code:\n                app.update_exit_code(exit_code)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning whether the document is likely to be a Sitemap.", "response": "def is_response(cls, response):\n        '''Return whether the document is likely to be a Sitemap.'''\n        if response.body:\n            if cls.is_file(response.body):\n                return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning whether the file is likely a Sitemap.", "response": "def is_file(cls, file):\n        '''Return whether the file is likely a Sitemap.'''\n        peeked_data = wpull.util.peek_file(file)\n\n        if is_gzip(peeked_data):\n            try:\n                peeked_data = wpull.decompression.gzip_uncompress(\n                    peeked_data, truncated=True\n                )\n            except zlib.error:\n                pass\n\n        peeked_data = wpull.string.printable_bytes(peeked_data)\n\n        if b'<?xml' in peeked_data \\\n           and (b'<sitemapindex' in peeked_data or b'<urlset' in peeked_data):\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef normalize_hostname(hostname):\n    '''Normalizes a hostname so that it is ASCII and valid domain name.'''\n    try:\n        new_hostname = hostname.encode('idna').decode('ascii').lower()\n    except UnicodeError as error:\n        raise UnicodeError('Hostname {} rejected: {}'.format(hostname, error)) from error\n\n    if hostname != new_hostname:\n        # Check for round-trip. May raise UnicodeError\n        new_hostname.encode('idna')\n\n    return new_hostname", "response": "Normalizes a hostname so that it is ASCII and valid domain name."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef normalize_path(path, encoding='utf-8'):\n    '''Normalize a path string.\n\n    Flattens a path by removing dot parts,\n    percent-encodes unacceptable characters and ensures percent-encoding is\n    uppercase.\n    '''\n    if not path.startswith('/'):\n        path = '/' + path\n    path = percent_encode(flatten_path(path, flatten_slashes=True), encoding=encoding)\n    return uppercase_percent_encoding(path)", "response": "Normalize a path string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef normalize_query(text, encoding='utf-8'):\n    '''Normalize a query string.\n\n    Percent-encodes unacceptable characters and ensures percent-encoding is\n    uppercase.\n    '''\n    path = percent_encode_plus(text, encoding=encoding)\n    return uppercase_percent_encoding(path)", "response": "Normalize a query string. Percent - encodes unacceptable characters ensures percent - encoding is uppercase."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef normalize_fragment(text, encoding='utf-8'):\n    '''Normalize a fragment.\n\n    Percent-encodes unacceptable characters and ensures percent-encoding is\n    uppercase.\n    '''\n    path = percent_encode(text, encoding=encoding, encode_set=FRAGMENT_ENCODE_SET)\n    return uppercase_percent_encoding(path)", "response": "Normalize a fragment. Percent - encodes unacceptable characters and ensures percent - encoding is\n    uppercase."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef normalize_username(text, encoding='utf-8'):\n    '''Normalize a username\n\n    Percent-encodes unacceptable characters and ensures percent-encoding is\n    uppercase.\n    '''\n    path = percent_encode(text, encoding=encoding, encode_set=USERNAME_ENCODE_SET)\n    return uppercase_percent_encoding(path)", "response": "Normalize a username by percent - encoding it."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nnormalize a password by percent - encoding it.", "response": "def normalize_password(text, encoding='utf-8'):\n    '''Normalize a password\n\n    Percent-encodes unacceptable characters and ensures percent-encoding is\n    uppercase.\n    '''\n    path = percent_encode(text, encoding=encoding, encode_set=PASSWORD_ENCODE_SET)\n    return uppercase_percent_encoding(path)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef percent_encode_plus(text, encode_set=QUERY_ENCODE_SET,\n                        encoding='utf-8'):\n    '''Percent encode text for query strings.\n\n    Unlike Python's ``quote_plus``, this function accepts a blacklist instead\n    of a whitelist of safe characters.\n    '''\n    if ' ' not in text:\n        return percent_encode(text, encode_set, encoding)\n    else:\n        result = percent_encode(text, encode_set, encoding)\n        return result.replace(' ', '+')", "response": "Percent encode text for query strings."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning whether the given URL schemes are similar.", "response": "def schemes_similar(scheme1, scheme2):\n    '''Return whether URL schemes are similar.\n\n    This function considers the following schemes to be similar:\n\n    * HTTP and HTTPS\n\n    '''\n    if scheme1 == scheme2:\n        return True\n\n    if scheme1 in ('http', 'https') and scheme2 in ('http', 'https'):\n        return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_subdir(base_path, test_path, trailing_slash=False, wildcards=False):\n    '''Return whether the a path is a subpath of another.\n\n    Args:\n        base_path: The base path\n        test_path: The path which we are testing\n        trailing_slash: If True, the trailing slash is treated with importance.\n            For example, ``/images/`` is a directory while ``/images`` is a\n            file.\n        wildcards: If True, globbing wildcards are matched against paths\n    '''\n    if trailing_slash:\n        base_path = base_path.rsplit('/', 1)[0] + '/'\n        test_path = test_path.rsplit('/', 1)[0] + '/'\n    else:\n        if not base_path.endswith('/'):\n            base_path += '/'\n\n        if not test_path.endswith('/'):\n            test_path += '/'\n\n    if wildcards:\n        return fnmatch.fnmatchcase(test_path, base_path)\n    else:\n        return test_path.startswith(base_path)", "response": "Return whether the a path is a subpath of another."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef split_query(qs, keep_blank_values=False):\n    '''Split the query string.\n\n    Note for empty values: If an equal sign (``=``) is present, the value\n    will be an empty string (``''``). Otherwise, the value will be ``None``::\n\n        >>> list(split_query('a=&b', keep_blank_values=True))\n        [('a', ''), ('b', None)]\n\n    No processing is done on the actual values.\n    '''\n    items = []\n    for pair in qs.split('&'):\n        name, delim, value = pair.partition('=')\n\n        if not delim and keep_blank_values:\n            value = None\n\n        if keep_blank_values or value:\n            items.append((name, value))\n\n    return items", "response": "Split the query string into a list of items."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a key - values mapping from a query string.", "response": "def query_to_map(text):\n    '''Return a key-values mapping from a query string.\n\n    Plus symbols are replaced with spaces.\n    '''\n    dict_obj = {}\n\n    for key, value in split_query(text, True):\n        if key not in dict_obj:\n            dict_obj[key] = []\n\n        if value:\n            dict_obj[key].append(value.replace('+', ' '))\n        else:\n            dict_obj[key].append('')\n\n    return query_to_map(text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef urljoin(base_url, url, allow_fragments=True):\n    '''Join URLs like ``urllib.parse.urljoin`` but allow scheme-relative URL.'''\n    if url.startswith('//') and len(url) > 2:\n        scheme = base_url.partition(':')[0]\n        if scheme:\n            return urllib.parse.urljoin(\n                base_url,\n                '{0}:{1}'.format(scheme, url),\n                allow_fragments=allow_fragments\n            )\n\n    return urllib.parse.urljoin(\n        base_url, url, allow_fragments=allow_fragments)", "response": "Join URLs like urllib. parse. urljoin but allow scheme - relative URL."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef flatten_path(path, flatten_slashes=False):\n    '''Flatten an absolute URL path by removing the dot segments.\n\n    :func:`urllib.parse.urljoin` has some support for removing dot segments,\n    but it is conservative and only removes them as needed.\n\n    Arguments:\n        path (str): The URL path.\n        flatten_slashes (bool): If True, consecutive slashes are removed.\n\n    The path returned will always have a leading slash.\n    '''\n    # Based on posixpath.normpath\n\n    # Fast path\n    if not path or path == '/':\n        return '/'\n\n    # Take off leading slash\n    if path[0] == '/':\n        path = path[1:]\n\n    parts = path.split('/')\n    new_parts = collections.deque()\n\n    for part in parts:\n        if part == '.' or (flatten_slashes and not part):\n            continue\n        elif part != '..':\n            new_parts.append(part)\n        elif new_parts:\n            new_parts.pop()\n\n    # If the filename is empty string\n    if flatten_slashes and path.endswith('/') or not len(new_parts):\n        new_parts.append('')\n\n    # Put back leading slash\n    new_parts.appendleft('')\n\n    return '/'.join(new_parts)", "response": "Flatten an absolute URL path by removing the dot segments."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a URL and return a URLInfo.", "response": "def parse(cls, url, default_scheme='http', encoding='utf-8'):\n        '''Parse a URL and return a URLInfo.'''\n        if url is None:\n            return None\n\n        url = url.strip()\n        if frozenset(url) & C0_CONTROL_SET:\n            raise ValueError('URL contains control codes: {}'.format(ascii(url)))\n\n        scheme, sep, remaining = url.partition(':')\n\n        if not scheme:\n            raise ValueError('URL missing scheme: {}'.format(ascii(url)))\n\n        scheme = scheme.lower()\n\n        if not sep and default_scheme:\n            # Likely something like example.com/mystuff\n            remaining = url\n            scheme = default_scheme\n        elif not sep:\n            raise ValueError('URI missing colon: {}'.format(ascii(url)))\n\n        if default_scheme and '.' in scheme or scheme == 'localhost':\n            # Maybe something like example.com:8080/mystuff or\n            # maybe localhost:8080/mystuff\n            remaining = '{}:{}'.format(scheme, remaining)\n            scheme = default_scheme\n\n        info = URLInfo()\n        info.encoding = encoding\n\n        if scheme not in RELATIVE_SCHEME_DEFAULT_PORTS:\n            info.raw = url\n            info.scheme = scheme\n            info.path = remaining\n\n            return info\n\n        if remaining.startswith('//'):\n            remaining = remaining[2:]\n\n        path_index = remaining.find('/')\n        query_index = remaining.find('?')\n        fragment_index = remaining.find('#')\n\n        try:\n            index_tuple = (path_index, query_index, fragment_index)\n            authority_index = min(num for num in index_tuple if num >= 0)\n        except ValueError:\n            authority_index = len(remaining)\n\n        authority = remaining[:authority_index]\n        resource = remaining[authority_index:]\n\n        try:\n            index_tuple = (query_index, fragment_index)\n            path_index = min(num for num in index_tuple if num >= 0)\n        except ValueError:\n            path_index = len(remaining)\n\n        path = remaining[authority_index + 1:path_index] or '/'\n\n        if fragment_index >= 0:\n            query_index = fragment_index\n        else:\n            query_index = len(remaining)\n\n        query = remaining[path_index + 1:query_index]\n        fragment = remaining[query_index + 1:]\n\n        userinfo, host = cls.parse_authority(authority)\n        hostname, port = cls.parse_host(host)\n        username, password = cls.parse_userinfo(userinfo)\n\n        if not hostname:\n            raise ValueError('Hostname is empty: {}'.format(ascii(url)))\n\n        info.raw = url\n        info.scheme = scheme\n        info.authority = authority\n        info.path = normalize_path(path, encoding=encoding)\n        info.query = normalize_query(query, encoding=encoding)\n        info.fragment = normalize_fragment(fragment, encoding=encoding)\n\n        info.userinfo = userinfo\n        info.username = percent_decode(username, encoding=encoding)\n        info.password = percent_decode(password, encoding=encoding)\n\n        info.host = host\n        info.hostname = hostname\n        info.port = port or RELATIVE_SCHEME_DEFAULT_PORTS[scheme]\n\n        info.resource = resource\n\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the authority part and return userinfo and host.", "response": "def parse_authority(cls, authority):\n        '''Parse the authority part and return userinfo and host.'''\n        userinfo, sep, host = authority.partition('@')\n\n        if not sep:\n            return '', userinfo\n        else:\n            return userinfo, host"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_userinfo(cls, userinfo):\n        '''Parse the userinfo and return username and password.'''\n        username, sep, password = userinfo.partition(':')\n\n        return username, password", "response": "Parse the userinfo and return username and password."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the host and return hostname and port.", "response": "def parse_host(cls, host):\n        '''Parse the host and return hostname and port.'''\n        if host.endswith(']'):\n            return cls.parse_hostname(host), None\n        else:\n            hostname, sep, port = host.rpartition(':')\n\n        if sep:\n            port = int(port)\n            if port < 0 or port > 65535:\n                raise ValueError('Port number invalid')\n        else:\n            hostname = port\n            port = None\n\n        return cls.parse_hostname(hostname), port"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_hostname(cls, hostname):\n        '''Parse the hostname and normalize.'''\n        if hostname.startswith('['):\n            return cls.parse_ipv6_hostname(hostname)\n        else:\n            try:\n                new_hostname = normalize_ipv4_address(hostname)\n            except ValueError:\n                # _logger.debug('', exc_info=True)\n                new_hostname = hostname\n\n            new_hostname = normalize_hostname(new_hostname)\n\n            if any(char in new_hostname for char in FORBIDDEN_HOSTNAME_CHARS):\n                raise ValueError('Invalid hostname: {}'\n                                 .format(ascii(hostname)))\n\n            return new_hostname", "response": "Parse the hostname and normalize."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_ipv6_hostname(cls, hostname):\n        '''Parse and normalize a IPv6 address.'''\n        if not hostname.startswith('[') or not hostname.endswith(']'):\n            raise ValueError('Invalid IPv6 address: {}'\n                             .format(ascii(hostname)))\n\n        hostname = ipaddress.IPv6Address(hostname[1:-1]).compressed\n\n        return hostname", "response": "Parse and normalize a IPv6 address."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_dict(self):\n        '''Return a dict of the attributes.'''\n        return dict(\n            raw=self.raw,\n            scheme=self.scheme,\n            authority=self.authority,\n            netloc=self.authority,\n            path=self.path,\n            query=self.query,\n            fragment=self.fragment,\n            userinfo=self.userinfo,\n            username=self.username,\n            password=self.password,\n            host=self.host,\n            hostname=self.hostname,\n            port=self.port,\n            resource=self.resource,\n            url=self.url,\n            encoding=self.encoding,\n        )", "response": "Return a dict of the attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns whether the URL is using the default port.", "response": "def is_port_default(self):\n        '''Return whether the URL is using the default port.'''\n        if self.scheme in RELATIVE_SCHEME_DEFAULT_PORTS:\n            return RELATIVE_SCHEME_DEFAULT_PORTS[self.scheme] == self.port"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the host portion but omit default port if needed.", "response": "def hostname_with_port(self):\n        '''Return the host portion but omit default port if needed.'''\n        default_port = RELATIVE_SCHEME_DEFAULT_PORTS.get(self.scheme)\n        if not default_port:\n            return ''\n\n        assert '[' not in self.hostname\n        assert ']' not in self.hostname\n\n        if self.is_ipv6():\n            hostname = '[{}]'.format(self.hostname)\n        else:\n            hostname = self.hostname\n\n        if default_port != self.port:\n            return '{}:{}'.format(hostname, self.port)\n        else:\n            return hostname"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _new_url_record(cls, request: Request) -> URLRecord:\n        '''Return new empty URLRecord.'''\n        url_record = URLRecord()\n\n        url_record.url = request.url_info.url\n        url_record.status = Status.in_progress\n        url_record.try_count = 0\n        url_record.level = 0\n\n        return url_record", "response": "Return new empty URLRecord."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates streams and commander. Coroutine.", "response": "def _init_stream(self):\n        '''Create streams and commander.\n\n        Coroutine.\n        '''\n        assert not self._control_connection\n        self._control_connection = yield from self._acquire_request_connection(self._request)\n        self._control_stream = ControlStream(self._control_connection)\n        self._commander = Commander(self._control_stream)\n\n        read_callback = functools.partial(self.event_dispatcher.notify, self.Event.control_receive_data)\n        self._control_stream.data_event_dispatcher.add_read_listener(read_callback)\n\n        write_callback = functools.partial(self.event_dispatcher.notify, self.Event.control_send_data)\n        self._control_stream.data_event_dispatcher.add_write_listener(write_callback)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconnects and login. Coroutine.", "response": "def _log_in(self):\n        '''Connect and login.\n\n        Coroutine.\n        '''\n        username = self._request.url_info.username or self._request.username or 'anonymous'\n        password = self._request.url_info.password or self._request.password or '-wpull@'\n\n        cached_login = self._login_table.get(self._control_connection)\n\n        if cached_login and cached_login == (username, password):\n            _logger.debug('Reusing existing login.')\n            return\n\n        try:\n            yield from self._commander.login(username, password)\n        except FTPServerError as error:\n            raise AuthenticationError('Login error: {}'.format(error)) \\\n                from error\n\n        self._login_table[self._control_connection] = (username, password)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting a file or directory listing download.", "response": "def start(self, request: Request) -> Response:\n        '''Start a file or directory listing download.\n\n        Args:\n            request: Request.\n\n        Returns:\n            A Response populated with the initial data connection reply.\n\n        Once the response is received, call :meth:`download`.\n\n        Coroutine.\n        '''\n        if self._session_state != SessionState.ready:\n            raise RuntimeError('Session not ready')\n\n        response = Response()\n\n        yield from self._prepare_fetch(request, response)\n\n        response.file_transfer_size = yield from self._fetch_size(request)\n\n        if request.restart_value:\n            try:\n                yield from self._commander.restart(request.restart_value)\n                response.restart_value = request.restart_value\n            except FTPServerError:\n                _logger.debug('Could not restart file.', exc_info=1)\n\n        yield from self._open_data_stream()\n\n        command = Command('RETR', request.file_path)\n\n        yield from self._begin_stream(command)\n\n        self._session_state = SessionState.file_request_sent\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfetches a file listing. Args: request: Request. Returns: A listing response populated with the initial data connection reply. Once the response is received, call :meth:`download_listing`. Coroutine.", "response": "def start_listing(self, request: Request) -> ListingResponse:\n        '''Fetch a file listing.\n\n        Args:\n            request: Request.\n\n        Returns:\n            A listing response populated with the initial data connection\n            reply.\n\n        Once the response is received, call :meth:`download_listing`.\n\n        Coroutine.\n        '''\n        if self._session_state != SessionState.ready:\n            raise RuntimeError('Session not ready')\n\n        response = ListingResponse()\n\n        yield from self._prepare_fetch(request, response)\n        yield from self._open_data_stream()\n\n        mlsd_command = Command('MLSD', self._request.file_path)\n        list_command = Command('LIST', self._request.file_path)\n\n        try:\n            yield from self._begin_stream(mlsd_command)\n            self._listing_type = 'mlsd'\n        except FTPServerError as error:\n            if error.reply_code in (ReplyCodes.syntax_error_command_unrecognized,\n                                    ReplyCodes.command_not_implemented):\n                self._listing_type = None\n            else:\n                raise\n\n        if not self._listing_type:\n            # This code not in exception handler to avoid incorrect\n            # exception chaining\n            yield from self._begin_stream(list_command)\n            self._listing_type = 'list'\n\n        _logger.debug('Listing type is %s', self._listing_type)\n\n        self._session_state = SessionState.directory_request_sent\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprepares for a fetch. Coroutine.", "response": "def _prepare_fetch(self, request: Request, response: Response):\n        '''Prepare for a fetch.\n\n        Coroutine.\n        '''\n        self._request = request\n        self._response = response\n\n        yield from self._init_stream()\n\n        connection_closed = self._control_connection.closed()\n\n        if connection_closed:\n            self._login_table.pop(self._control_connection, None)\n            yield from self._control_stream.reconnect()\n\n        request.address = self._control_connection.address\n\n        connection_reused = not connection_closed\n        self.event_dispatcher.notify(self.Event.begin_control, request, connection_reused=connection_reused)\n\n        if connection_closed:\n            yield from self._commander.read_welcome_message()\n\n        yield from self._log_in()\n\n        self._response.request = request"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart data stream transfer.", "response": "def _begin_stream(self, command: Command):\n        '''Start data stream transfer.'''\n        begin_reply = yield from self._commander.begin_stream(command)\n\n        self._response.reply = begin_reply\n\n        self.event_dispatcher.notify(self.Event.begin_transfer, self._response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef download(self, file: Optional[IO]=None, rewind: bool=True,\n                 duration_timeout: Optional[float]=None) -> Response:\n        '''Read the response content into file.\n\n        Args:\n            file: A file object or asyncio stream.\n            rewind: Seek the given file back to its original offset after\n                reading is finished.\n            duration_timeout: Maximum time in seconds of which the\n                entire file must be read.\n\n        Returns:\n            A Response populated with the final data connection reply.\n\n        Be sure to call :meth:`start` first.\n\n        Coroutine.\n        '''\n        if self._session_state != SessionState.file_request_sent:\n            raise RuntimeError('File request not sent')\n\n        if rewind and file and hasattr(file, 'seek'):\n            original_offset = file.tell()\n        else:\n            original_offset = None\n\n        if not hasattr(file, 'drain'):\n            self._response.body = file\n\n            if not isinstance(file, Body):\n                self._response.body = Body(file)\n\n        read_future = self._commander.read_stream(file, self._data_stream)\n\n        try:\n            reply = yield from \\\n                asyncio.wait_for(read_future, timeout=duration_timeout)\n        except asyncio.TimeoutError as error:\n            raise DurationTimeout(\n                'Did not finish reading after {} seconds.'\n                .format(duration_timeout)\n            ) from error\n\n        self._response.reply = reply\n\n        if original_offset is not None:\n            file.seek(original_offset)\n\n        self.event_dispatcher.notify(self.Event.end_transfer, self._response)\n\n        self._session_state = SessionState.response_received\n\n        return self._response", "response": "Read the response content into file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndownload the file listing.", "response": "def download_listing(self, file: Optional[IO],\n                         duration_timeout: Optional[float]=None) -> \\\n            ListingResponse:\n        '''Read file listings.\n\n        Args:\n            file: A file object or asyncio stream.\n            duration_timeout: Maximum time in seconds of which the\n                entire file must be read.\n\n        Returns:\n            A Response populated the file listings\n\n        Be sure to call :meth:`start_file_listing` first.\n\n        Coroutine.\n        '''\n        if self._session_state != SessionState.directory_request_sent:\n            raise RuntimeError('File request not sent')\n\n        self._session_state = SessionState.file_request_sent\n\n        yield from self.download(file=file, rewind=False,\n                                 duration_timeout=duration_timeout)\n\n        try:\n            if self._response.body.tell() == 0:\n                listings = ()\n            elif self._listing_type == 'mlsd':\n                self._response.body.seek(0)\n\n                machine_listings = wpull.protocol.ftp.util.parse_machine_listing(\n                    self._response.body.read().decode('utf-8',\n                                                      errors='surrogateescape'),\n                    convert=True, strict=False\n                )\n                listings = list(\n                    wpull.protocol.ftp.util.machine_listings_to_file_entries(\n                        machine_listings\n                    ))\n            else:\n                self._response.body.seek(0)\n\n                file = io.TextIOWrapper(self._response.body, encoding='utf-8',\n                                        errors='surrogateescape')\n\n                listing_parser = ListingParser(file=file)\n\n                listings = list(listing_parser.parse_input())\n\n                _logger.debug('Listing detected as %s', listing_parser.type)\n\n                # We don't want the file to be closed when exiting this function\n                file.detach()\n\n        except (ListingError, ValueError) as error:\n            raise ProtocolError(*error.args) from error\n\n        self._response.files = listings\n\n        self._response.body.seek(0)\n\n        self._session_state = SessionState.response_received\n\n        return self._response"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _open_data_stream(self):\n        '''Open the data stream connection.\n\n        Coroutine.\n        '''\n        @asyncio.coroutine\n        def connection_factory(address: Tuple[int, int]):\n            self._data_connection = yield from self._acquire_connection(address[0], address[1])\n            return self._data_connection\n\n        self._data_stream = yield from self._commander.setup_data_stream(\n            connection_factory\n        )\n\n        self._response.data_address = self._data_connection.address\n\n        read_callback = functools.partial(self.event_dispatcher.notify, self.Event.transfer_receive_data)\n        self._data_stream.data_event_dispatcher.add_read_listener(read_callback)\n\n        write_callback = functools.partial(self.event_dispatcher.notify, self.Event.transfer_send_data)\n        self._data_stream.data_event_dispatcher.add_write_listener(write_callback)", "response": "Open the data stream connection."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _fetch_size(self, request: Request) -> int:\n        '''Return size of file.\n\n        Coroutine.\n        '''\n        try:\n            size = yield from self._commander.size(request.file_path)\n            return size\n        except FTPServerError:\n            return", "response": "Return size of file.\n            Coroutine."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing text for HTTP Refresh URL. Returns None", "response": "def parse_refresh(text):\n    '''Parses text for HTTP Refresh URL.\n\n    Returns:\n        str, None\n    '''\n    match = re.search(r'url\\s*=(.+)', text, re.IGNORECASE)\n\n    if match:\n        url = match.group(1)\n\n        if url.startswith('\"'):\n            url = url.strip('\"')\n        elif url.startswith(\"'\"):\n            url = url.strip(\"'\")\n\n        return clean_link_soup(url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef urljoin_safe(base_url, url, allow_fragments=True):\n    '''urljoin with warning log on error.\n\n    Returns:\n        str, None'''\n    try:\n        return wpull.url.urljoin(\n            base_url, url, allow_fragments=allow_fragments\n        )\n    except ValueError as error:\n        _logger.warning(__(\n            _('Unable to parse URL \u2018{url}\u2019: {error}.'),\n            url=url, error=error\n        ))", "response": "urljoin with warning log on error.\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_likely_inline(link):\n    '''Return whether the link is likely to be inline.'''\n    file_type = mimetypes.guess_type(link, strict=False)[0]\n\n    if file_type:\n        top_level_type, subtype = file_type.split('/', 1)\n\n        return top_level_type in ('image', 'video', 'audio') or subtype == 'javascript'", "response": "Return whether the link is likely to be inline."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_likely_link(text):\n    '''Return whether the text is likely to be a link.\n\n    This function assumes that leading/trailing whitespace has already been\n    removed.\n\n    Returns:\n        bool\n    '''\n    text = text.lower()\n\n    # Check for absolute or relative URLs\n    if (\n        text.startswith('http://')\n        or text.startswith('https://')\n        or text.startswith('ftp://')\n        or text.startswith('/')\n        or text.startswith('//')\n        or text.endswith('/')\n        or text.startswith('../')\n    ):\n        return True\n\n    # Check if it has a alphanumeric file extension and not a decimal number\n    dummy, dot, file_extension = text.rpartition('.')\n\n    if dot and file_extension and len(file_extension) <= 4:\n        file_extension_set = frozenset(file_extension)\n\n        if file_extension_set \\\n           and file_extension_set <= ALPHANUMERIC_CHARS \\\n           and not file_extension_set <= NUMERIC_CHARS:\n            if file_extension in COMMON_TLD:\n                return False\n\n            file_type = mimetypes.guess_type(text, strict=False)[0]\n\n            if file_type:\n                return True\n            else:\n                return False", "response": "Return whether the text is likely to be a link."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning whether the text is likely to cause false positives.", "response": "def is_unlikely_link(text):\n    '''Return whether the text is likely to cause false positives.\n\n    This function assumes that leading/trailing whitespace has already been\n    removed.\n\n    Returns:\n        bool\n    '''\n    # Check for string concatenation in JavaScript\n    if text[:1] in ',;+:' or text[-1:] in '.,;+:':\n        return True\n\n    # Check for unusual characters\n    if re.search(r'''[\\\\$()'\"[\\]{}|<>`]''', text):\n        return True\n\n    if text[:1] == '.' \\\n       and not text.startswith('./') \\\n       and not text.startswith('../'):\n        return True\n\n    if text in ('/', '//'):\n        return True\n\n    if '//' in text and '://' not in text and not text.startswith('//'):\n        return True\n\n    # Forbid strings like mimetypes\n    if text in MIMETYPES:\n        return True\n\n    tag_1, dummy, tag_2 = text.partition('.')\n    if tag_1 in HTML_TAGS and tag_2 != 'html':\n        return True\n\n    # Forbid things where the first part of the path looks like a domain name\n    if FIRST_PART_TLD_PATTERN.match(text):\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef identify_link_type(filename):\n    '''Return link type guessed by filename extension.\n\n    Returns:\n        str: A value from :class:`.item.LinkType`.\n    '''\n    mime_type = mimetypes.guess_type(filename)[0]\n\n    if not mime_type:\n        return\n\n    if mime_type == 'text/css':\n        return LinkType.css\n    elif mime_type == 'application/javascript':\n        return LinkType.javascript\n    elif mime_type == 'text/html' or mime_type.endswith('xml'):\n        return LinkType.html\n    elif mime_type.startswith('video') or \\\n            mime_type.startswith('image') or \\\n            mime_type.startswith('audio') or \\\n            mime_type.endswith('shockwave-flash'):\n        return LinkType.media", "response": "Return link type guessed by filename extension."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef new_encoded_stream(args, stream):\n    '''Return a stream writer.'''\n    if args.ascii_print:\n        return wpull.util.ASCIIStreamWriter(stream)\n    else:\n        return stream", "response": "Return a stream writer."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks and close connection if needed.", "response": "def _check(self):\n        '''Check and close connection if needed.'''\n        _logger.debug('Check if timeout.')\n        self._call_later_handle = None\n\n        if self._touch_time is not None:\n            difference = self._event_loop.time() - self._touch_time\n            _logger.debug('Time difference %s', difference)\n\n            if difference > self._timeout:\n                self._connection.close()\n                self._timed_out = True\n\n        if not self._connection.closed():\n            self._schedule()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef closed(self) -> bool:\n        '''Return whether the connection is closed.'''\n        return not self.writer or not self.reader or self.reader.at_eof()", "response": "Return whether the connection is closed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef connect(self):\n        '''Establish a connection.'''\n        _logger.debug(__('Connecting to {0}.', self._address))\n\n        if self._state != ConnectionState.ready:\n            raise Exception('Closed connection must be reset before reusing.')\n\n        if self._sock:\n            connection_future = asyncio.open_connection(\n                sock=self._sock, **self._connection_kwargs()\n            )\n        else:\n            # TODO: maybe we don't want to ignore flow-info and scope-id?\n            host = self._address[0]\n            port = self._address[1]\n\n            connection_future = asyncio.open_connection(\n                host, port, **self._connection_kwargs()\n            )\n\n        self.reader, self.writer = yield from \\\n            self.run_network_operation(\n                connection_future,\n                wait_timeout=self._connect_timeout,\n                name='Connect')\n\n        if self._timeout is not None:\n            self._close_timer = CloseTimer(self._timeout, self)\n        else:\n            self._close_timer = DummyCloseTimer()\n\n        self._state = ConnectionState.created\n        _logger.debug('Connected.')", "response": "Establish a connection to the Asterisk."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites data to the related object.", "response": "def write(self, data: bytes, drain: bool=True):\n        '''Write data.'''\n        assert self._state == ConnectionState.created, \\\n            'Expect conn created. Got {}.'.format(self._state)\n\n        self.writer.write(data)\n\n        if drain:\n            fut = self.writer.drain()\n\n            if fut:\n                yield from self.run_network_operation(\n                    fut, close_timeout=self._timeout, name='Write')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read(self, amount: int=-1) -> bytes:\n        '''Read data.'''\n        assert self._state == ConnectionState.created, \\\n            'Expect conn created. Got {}.'.format(self._state)\n\n        data = yield from \\\n            self.run_network_operation(\n                self.reader.read(amount),\n                close_timeout=self._timeout,\n                name='Read')\n\n        return data", "response": "Read data from the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread a line of data.", "response": "def readline(self) -> bytes:\n        '''Read a line of data.'''\n        assert self._state == ConnectionState.created, \\\n            'Expect conn created. Got {}.'.format(self._state)\n\n        with self._close_timer.with_timeout():\n            data = yield from \\\n                self.run_network_operation(\n                    self.reader.readline(),\n                    close_timeout=self._timeout,\n                    name='Readline')\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run_network_operation(self, task, wait_timeout=None,\n                              close_timeout=None,\n                              name='Network operation'):\n        '''Run the task and raise appropriate exceptions.\n\n        Coroutine.\n        '''\n        if wait_timeout is not None and close_timeout is not None:\n            raise Exception(\n                'Cannot use wait_timeout and close_timeout at the same time')\n\n        try:\n            if close_timeout is not None:\n                with self._close_timer.with_timeout():\n                    data = yield from task\n\n                if self._close_timer.is_timeout():\n                    raise NetworkTimedOut(\n                        '{name} timed out.'.format(name=name))\n                else:\n                    return data\n            elif wait_timeout is not None:\n                data = yield from asyncio.wait_for(task, wait_timeout)\n                return data\n            else:\n                return (yield from task)\n\n        except asyncio.TimeoutError as error:\n            self.close()\n            raise NetworkTimedOut(\n                '{name} timed out.'.format(name=name)) from error\n        except (tornado.netutil.SSLCertificateError, SSLVerificationError) \\\n                as error:\n            self.close()\n            raise SSLVerificationError(\n                '{name} certificate error: {error}'\n                .format(name=name, error=error)) from error\n        except AttributeError as error:\n            self.close()\n\n            raise NetworkError(\n                '{name} network error: connection closed unexpectedly: {error}'\n                .format(name=name, error=error)) from error\n        except (socket.error, ssl.SSLError, OSError, IOError) as error:\n            self.close()\n            if isinstance(error, NetworkError):\n                raise\n\n            if error.errno == errno.ECONNREFUSED:\n                raise ConnectionRefused(\n                    error.errno, os.strerror(error.errno)) from error\n\n            # XXX: This quality case brought to you by OpenSSL and Python.\n            # Example: _ssl.SSLError: [Errno 1] error:14094418:SSL\n            #          routines:SSL3_READ_BYTES:tlsv1 alert unknown ca\n            error_string = str(error).lower()\n            if 'certificate' in error_string or 'unknown ca' in error_string:\n                raise SSLVerificationError(\n                    '{name} certificate error: {error}'\n                    .format(name=name, error=error)) from error\n\n            else:\n                if error.errno:\n                    raise NetworkError(\n                        error.errno, os.strerror(error.errno)) from error\n                else:\n                    raise NetworkError(\n                        '{name} network error: {error}'\n                        .format(name=name, error=error)) from error", "response": "Run the task and raise appropriate exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef start_tls(self, ssl_context: Union[bool, dict, ssl.SSLContext]=True) \\\n            -> 'SSLConnection':\n        '''Start client TLS on this connection and return SSLConnection.\n\n        Coroutine\n        '''\n        sock = self.writer.get_extra_info('socket')\n        ssl_conn = SSLConnection(\n            self._address,\n            ssl_context=ssl_context,\n            hostname=self._hostname, timeout=self._timeout,\n            connect_timeout=self._connect_timeout, bind_host=self._bind_host,\n            bandwidth_limiter=self._bandwidth_limiter, sock=sock\n        )\n\n        yield from ssl_conn.connect()\n\n        return ssl_conn", "response": "Start client TLS on this connection and return a new SSLConnection."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _verify_cert(self, sock: ssl.SSLSocket):\n        '''Check if certificate matches hostname.'''\n        # Based on tornado.iostream.SSLIOStream\n        # Needed for older OpenSSL (<0.9.8f) versions\n        verify_mode = self._ssl_context.verify_mode\n\n        assert verify_mode in (ssl.CERT_NONE, ssl.CERT_REQUIRED,\n                               ssl.CERT_OPTIONAL), \\\n            'Unknown verify mode {}'.format(verify_mode)\n\n        if verify_mode == ssl.CERT_NONE:\n            return\n\n        cert = sock.getpeercert()\n\n        if not cert and verify_mode == ssl.CERT_OPTIONAL:\n            return\n\n        if not cert:\n            raise SSLVerificationError('No SSL certificate given')\n\n        try:\n            ssl.match_hostname(cert, self._hostname)\n        except ssl.CertificateError as error:\n            raise SSLVerificationError('Invalid SSL certificate') from error", "response": "Check if certificate matches hostname."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove items that are expired or exceed the max size.", "response": "def trim(self):\n        '''Remove items that are expired or exceed the max size.'''\n        now_time = time.time()\n\n        while self._seq and self._seq[0].expire_time < now_time:\n            item = self._seq.popleft()\n            del self._map[item.key]\n\n        if self._max_items:\n            while self._seq and len(self._seq) > self._max_items:\n                item = self._seq.popleft()\n                del self._map[item.key]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstrips session ID from URL path.", "response": "def strip_path_session_id(path):\n    '''Strip session ID from URL path.'''\n    for pattern in SESSION_ID_PATH_PATTERNS:\n        match = pattern.match(path)\n        if match:\n            path = match.group(1) + match.group(3)\n\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rewrite(self, url_info: URLInfo) -> URLInfo:\n        '''Rewrite the given URL.'''\n        if url_info.scheme not in ('http', 'https'):\n            return url_info\n\n        if self._session_id_enabled:\n            url = '{scheme}://{authority}{path}?{query}#{fragment}'.format(\n                scheme=url_info.scheme,\n                authority=url_info.authority,\n                path=strip_path_session_id(url_info.path),\n                query=strip_query_session_id(url_info.query),\n                fragment=url_info.fragment,\n            )\n            url_info = parse_url_or_log(url) or url_info\n\n        if self._hash_fragment_enabled and url_info.fragment.startswith('!'):\n            if url_info.query:\n                url = '{}&_escaped_fragment_={}'.format(url_info.url,\n                                                        url_info.fragment[1:])\n            else:\n                url = '{}?_escaped_fragment_={}'.format(url_info.url,\n                                                        url_info.fragment[1:])\n\n            url_info = parse_url_or_log(url) or url_info\n\n        return url_info", "response": "Rewrite the given URL."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the reply code as a tuple.", "response": "def reply_code_tuple(code: int) -> Tuple[int, int, int]:\n    '''Return the reply code as a tuple.\n\n    Args:\n        code: The reply code.\n\n    Returns:\n        Each item in the tuple is the digit.\n    '''\n    return code // 100, code // 10 % 10, code % 10"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the machine listing from the text.", "response": "def parse_machine_listing(text: str, convert: bool=True, strict: bool=True) -> \\\n        List[dict]:\n    '''Parse machine listing.\n\n    Args:\n        text: The listing.\n        convert: Convert sizes and dates.\n        strict: Method of handling errors. ``True`` will raise\n            ``ValueError``. ``False`` will ignore rows with errors.\n\n    Returns:\n        list: A list of dict of the facts defined in RFC 3659.\n        The key names must be lowercase. The filename uses the key\n        ``name``.\n    '''\n    # TODO: this function should be moved into the 'ls' package\n    listing = []\n\n    for line in text.splitlines(False):\n        facts = line.split(';')\n        row = {}\n        filename = None\n\n        for fact in facts:\n            name, sep, value = fact.partition('=')\n\n            if sep:\n                name = name.strip().lower()\n                value = value.strip().lower()\n\n                if convert:\n                    try:\n                        value = convert_machine_list_value(name, value)\n                    except ValueError:\n                        if strict:\n                            raise\n\n                row[name] = value\n            else:\n                if name[0:1] == ' ':\n                    # Is a filename\n                    filename = name[1:]\n                else:\n                    name = name.strip().lower()\n                    row[name] = ''\n\n        if filename:\n            row['name'] = filename\n            listing.append(row)\n        elif strict:\n            raise ValueError('Missing filename.')\n\n    return listing"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_machine_list_value(name: str, value: str) -> \\\n        Union[datetime.datetime, str, int]:\n    '''Convert sizes and time values.\n\n    Size will be ``int`` while time value will be :class:`datetime.datetime`.\n    '''\n    if name == 'modify':\n        return convert_machine_list_time_val(value)\n    elif name == 'size':\n        return int(value)\n    else:\n        return value", "response": "Convert machine list value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert_machine_list_time_val(text: str) -> datetime.datetime:\n    '''Convert RFC 3659 time-val to datetime objects.'''\n    # TODO: implement fractional seconds\n    text = text[:14]\n\n    if len(text) != 14:\n        raise ValueError('Time value not 14 chars')\n\n    year = int(text[0:4])\n    month = int(text[4:6])\n    day = int(text[6:8])\n    hour = int(text[8:10])\n    minute = int(text[10:12])\n    second = int(text[12:14])\n\n    return datetime.datetime(year, month, day, hour, minute, second,\n                             tzinfo=datetime.timezone.utc)", "response": "Convert RFC 3659 time - val to datetime objects."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef machine_listings_to_file_entries(listings: Iterable[dict]) -> \\\n        Iterable[FileEntry]:\n    '''Convert results from parsing machine listings to FileEntry list.'''\n    for listing in listings:\n        yield FileEntry(\n            listing['name'],\n            type=listing.get('type'),\n            size=listing.get('size'),\n            date=listing.get('modify')\n        )", "response": "Convert results from parsing machine listings to FileEntry list."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _run_producer_wrapper(self):\n        '''Run the producer, if exception, stop engine.'''\n        try:\n            yield from self._producer.process()\n        except Exception as error:\n            if not isinstance(error, StopIteration):\n                # Stop the workers so the producer exception will be handled\n                # when we finally yield from this coroutine\n                _logger.debug('Producer died.', exc_info=True)\n                self.stop()\n            raise\n        else:\n            self.stop()", "response": "Run the producer if exception stop engine."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_cdx(file, encoding='utf8'):\n    '''Iterate CDX file.\n\n    Args:\n        file (str): A file object.\n        encoding (str): The encoding of the file.\n\n    Returns:\n        iterator: Each item is a dict that maps from field key to value.\n    '''\n    with codecs.getreader(encoding)(file) as stream:\n        header_line = stream.readline()\n        separator = header_line[0]\n        field_keys = header_line.strip().split(separator)\n\n        if field_keys.pop(0) != 'CDX':\n            raise ValueError('CDX header not found.')\n\n        for line in stream:\n            yield dict(zip(field_keys, line.strip().split(separator)))", "response": "Iterate CDX file.\n    yields a generator that yields dictionaries of key - > value."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the required fields for the record.", "response": "def set_common_fields(self, warc_type: str, content_type: str):\n        '''Set the required fields for the record.'''\n        self.fields[self.WARC_TYPE] = warc_type\n        self.fields[self.CONTENT_TYPE] = content_type\n        self.fields[self.WARC_DATE] = wpull.util.datetime_str()\n        self.fields[self.WARC_RECORD_ID] = '<{0}>'.format(uuid.uuid4().urn)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind and set the content length.", "response": "def set_content_length(self):\n        '''Find and set the content length.\n\n        .. seealso:: :meth:`compute_checksum`.\n        '''\n        if not self.block_file:\n            self.fields['Content-Length'] = '0'\n            return\n\n        with wpull.util.reset_file_offset(self.block_file):\n            wpull.util.seek_file_end(self.block_file)\n            self.fields['Content-Length'] = str(self.block_file.tell())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute_checksum(self, payload_offset: Optional[int]=None):\n        '''Compute and add the checksum data to the record fields.\n\n        This function also sets the content length.\n        '''\n        if not self.block_file:\n            self.fields['Content-Length'] = '0'\n            return\n\n        block_hasher = hashlib.sha1()\n        payload_hasher = hashlib.sha1()\n\n        with wpull.util.reset_file_offset(self.block_file):\n            if payload_offset is not None:\n                data = self.block_file.read(payload_offset)\n                block_hasher.update(data)\n\n            while True:\n                data = self.block_file.read(4096)\n                if data == b'':\n                    break\n                block_hasher.update(data)\n                payload_hasher.update(data)\n\n            content_length = self.block_file.tell()\n\n        content_hash = block_hasher.digest()\n\n        self.fields['WARC-Block-Digest'] = 'sha1:{0}'.format(\n            base64.b32encode(content_hash).decode()\n        )\n\n        if payload_offset is not None:\n            payload_hash = payload_hasher.digest()\n            self.fields['WARC-Payload-Digest'] = 'sha1:{0}'.format(\n                base64.b32encode(payload_hash).decode()\n            )\n\n        self.fields['Content-Length'] = str(content_length)", "response": "Compute and add the checksum data to the record fields. This function also sets the content length."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_http_header(self) -> Response:\n        '''Return the HTTP header.\n\n        It only attempts to read the first 4 KiB of the payload.\n\n        Returns:\n            Response, None: Returns an instance of\n            :class:`.http.request.Response` or None.\n        '''\n        with wpull.util.reset_file_offset(self.block_file):\n            data = self.block_file.read(4096)\n\n        match = re.match(br'(.*?\\r?\\n\\r?\\n)', data)\n\n        if not match:\n            return\n\n        status_line, dummy, field_str = match.group(1).partition(b'\\n')\n\n        try:\n            version, code, reason = Response.parse_status_line(status_line)\n        except ValueError:\n            return\n\n        response = Response(status_code=code, reason=reason, version=version)\n\n        try:\n            response.fields.parse(field_str, strict=False)\n        except ValueError:\n            return\n\n        return response", "response": "Return the HTTP header."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites the parameters to a file for PhantomJS to read.", "response": "def _write_config(self):\n        '''Write the parameters to a file for PhantomJS to read.'''\n        param_dict = {\n            'url': self._params.url,\n            'snapshot_paths': self._params.snapshot_paths,\n            'wait_time': self._params.wait_time,\n            'num_scrolls': self._params.num_scrolls,\n            'smart_scroll': self._params.smart_scroll,\n            'snapshot': self._params.snapshot,\n            'viewport_width': self._params.viewport_size[0],\n            'viewport_height': self._params.viewport_size[1],\n            'paper_width': self._params.paper_size[0],\n            'paper_height': self._params.paper_size[1],\n            'custom_headers': self._params.custom_headers,\n            'page_settings': self._params.page_settings,\n        }\n\n        if self._params.event_log_filename:\n            param_dict['event_log_filename'] = \\\n                os.path.abspath(self._params.event_log_filename)\n\n        if self._params.action_log_filename:\n            param_dict['action_log_filename'] = \\\n                os.path.abspath(self._params.action_log_filename)\n\n        config_text = json.dumps(param_dict)\n\n        self._config_file.write(config_text.encode('utf-8'))\n\n        # Close it so the phantomjs process can read it on Windows\n        self._config_file.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncleans closed connections. Args: force: Clean connected and idle connections too. Coroutine.", "response": "def clean(self, force: bool=False):\n        '''Clean closed connections.\n\n        Args:\n            force: Clean connected and idle connections too.\n\n        Coroutine.\n        '''\n        with (yield from self._lock):\n            for connection in tuple(self.ready):\n                if force or connection.closed():\n                    connection.close()\n                    self.ready.remove(connection)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef close(self):\n        '''Forcibly close all connections.\n\n        This instance will not be usable after calling this method.\n        '''\n        for connection in self.ready:\n            connection.close()\n\n        for connection in self.busy:\n            connection.close()\n\n        self._closed = True", "response": "Forcibly close all connections."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef acquire(self) -> Connection:\n        '''Register and return a connection.\n\n        Coroutine.\n        '''\n        assert not self._closed\n\n        yield from self._condition.acquire()\n\n        while True:\n            if self.ready:\n                connection = self.ready.pop()\n                break\n            elif len(self.busy) < self.max_connections:\n                connection = self._connection_factory()\n                break\n            else:\n                yield from self._condition.wait()\n\n        self.busy.add(connection)\n        self._condition.release()\n\n        return connection", "response": "Register and return a connection. Coroutine."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef release(self, connection: Connection, reuse: bool=True):\n        '''Unregister a connection.\n\n        Args:\n            connection: Connection instance returned from :meth:`acquire`.\n            reuse: If True, the connection is made available for reuse.\n\n        Coroutine.\n        '''\n        yield from self._condition.acquire()\n        self.busy.remove(connection)\n\n        if reuse:\n            self.ready.add(connection)\n\n        self._condition.notify()\n        self._condition.release()", "response": "Unregister a connection.\n\n        Args:\n            connection: Connection instance returned from :meth:`acquire`.\n            reuse: If True, the connection is made available for reuse.\n\n        Coroutine."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nacquiring an available connection.", "response": "def acquire(self, host: str, port: int, use_ssl: bool=False,\n                host_key: Optional[Any]=None) \\\n            -> Union[Connection, SSLConnection]:\n        '''Return an available connection.\n\n        Args:\n            host: A hostname or IP address.\n            port: Port number.\n            use_ssl: Whether to return a SSL connection.\n            host_key: If provided, it overrides the key used for per-host\n                connection pooling. This is useful for proxies for example.\n\n        Coroutine.\n        '''\n        assert isinstance(port, int), 'Expect int. Got {}'.format(type(port))\n        assert not self._closed\n\n        yield from self._process_no_wait_releases()\n\n        if use_ssl:\n            connection_factory = functools.partial(\n                self._ssl_connection_factory, hostname=host)\n        else:\n            connection_factory = functools.partial(\n                self._connection_factory, hostname=host)\n\n        connection_factory = functools.partial(\n            HappyEyeballsConnection, (host, port), connection_factory,\n            self._resolver, self._happy_eyeballs_table,\n            is_ssl=use_ssl\n        )\n\n        key = host_key or (host, port, use_ssl)\n\n        with (yield from self._host_pools_lock):\n            if key not in self._host_pools:\n                host_pool = self._host_pools[key] = HostPool(\n                    connection_factory,\n                    max_connections=self._max_host_count\n                )\n                self._host_pool_waiters[key] = 1\n            else:\n                host_pool = self._host_pools[key]\n                self._host_pool_waiters[key] += 1\n\n        _logger.debug('Check out %s', key)\n\n        connection = yield from host_pool.acquire()\n        connection.key = key\n\n        # TODO: Verify this assert is always true\n        # assert host_pool.count() <= host_pool.max_connections\n        # assert key in self._host_pools\n        # assert self._host_pools[key] == host_pool\n\n        with (yield from self._host_pools_lock):\n            self._host_pool_waiters[key] -= 1\n\n        return connection"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef release(self, connection: Connection):\n        '''Put a connection back in the pool.\n\n        Coroutine.\n        '''\n        assert not self._closed\n\n        key = connection.key\n        host_pool = self._host_pools[key]\n\n        _logger.debug('Check in %s', key)\n\n        yield from host_pool.release(connection)\n\n        force = self.count() > self._max_count\n        yield from self.clean(force=force)", "response": "Put a connection back in the pool.\n        Coroutine.\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef session(self, host: str, port: int, use_ssl: bool=False):\n        '''Return a context manager that returns a connection.\n\n        Usage::\n\n            session = yield from connection_pool.session('example.com', 80)\n            with session as connection:\n                connection.write(b'blah')\n                connection.close()\n\n        Coroutine.\n        '''\n        connection = yield from self.acquire(host, port, use_ssl)\n\n        @contextlib.contextmanager\n        def context_wrapper():\n            try:\n                yield connection\n            finally:\n                self.no_wait_release(connection)\n\n        return context_wrapper()", "response": "Return a context manager that returns a connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clean(self, force: bool=False):\n        '''Clean all closed connections.\n\n        Args:\n            force: Clean connected and idle connections too.\n\n        Coroutine.\n        '''\n        assert not self._closed\n\n        with (yield from self._host_pools_lock):\n            for key, pool in tuple(self._host_pools.items()):\n                yield from pool.clean(force=force)\n\n                if not self._host_pool_waiters[key] and pool.empty():\n                    del self._host_pools[key]\n                    del self._host_pool_waiters[key]", "response": "Clean all closed connections."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclosing all the connections and clean up.", "response": "def close(self):\n        '''Close all the connections and clean up.\n\n        This instance will not be usable after calling this method.\n        '''\n        for key, pool in tuple(self._host_pools.items()):\n            pool.close()\n\n            del self._host_pools[key]\n            del self._host_pool_waiters[key]\n\n        self._closed = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef count(self) -> int:\n        '''Return number of connections.'''\n        counter = 0\n\n        for pool in self._host_pools.values():\n            counter += pool.count()\n\n        return counter", "response": "Return number of connections."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_preferred(self, preferred_addr, addr_1, addr_2):\n        '''Set the preferred address.'''\n        if addr_1 > addr_2:\n            addr_1, addr_2 = addr_2, addr_1\n\n        self._cache[(addr_1, addr_2)] = preferred_addr", "response": "Set the preferred address."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the preferred address.", "response": "def get_preferred(self, addr_1, addr_2):\n        '''Return the preferred address.'''\n        if addr_1 > addr_2:\n            addr_1, addr_2 = addr_2, addr_1\n\n        return self._cache.get((addr_1, addr_2))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconnecting using happy eyeballs.", "response": "def _connect_dual_stack(self, primary_address, secondary_address):\n        '''Connect using happy eyeballs.'''\n        self._primary_connection = self._connection_factory(primary_address)\n        self._secondary_connection = self._connection_factory(secondary_address)\n\n        @asyncio.coroutine\n        def connect_primary():\n            yield from self._primary_connection.connect()\n            return self._primary_connection\n\n        @asyncio.coroutine\n        def connect_secondary():\n            yield from self._secondary_connection.connect()\n            return self._secondary_connection\n\n        primary_fut = connect_primary()\n        secondary_fut = connect_secondary()\n\n        failed = False\n\n        for fut in asyncio.as_completed((primary_fut, secondary_fut)):\n            if not self._active_connection:\n                try:\n                    self._active_connection = yield from fut\n                except NetworkError:\n                    if not failed:\n                        _logger.debug('Original dual stack exception', exc_info=True)\n                        failed = True\n                    else:\n                        raise\n                else:\n                    _logger.debug('Got first of dual stack.')\n\n            else:\n                @asyncio.coroutine\n                def cleanup():\n                    try:\n                        conn = yield from fut\n                    except NetworkError:\n                        pass\n                    else:\n                        conn.close()\n                    _logger.debug('Closed abandoned connection.')\n\n                asyncio.get_event_loop().create_task(cleanup())\n\n        preferred_host = self._active_connection.host\n\n        self._happy_eyeballs_table.set_preferred(\n            preferred_host, primary_address[0], secondary_address[0])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_preferred_host(self, result: ResolveResult) -> Tuple[str, str]:\n        '''Get preferred host from DNS results.'''\n        host_1 = result.first_ipv4.ip_address if result.first_ipv4 else None\n        host_2 = result.first_ipv6.ip_address if result.first_ipv6 else None\n\n        if not host_2:\n            return host_1, None\n        elif not host_1:\n            return host_2, None\n\n        preferred_host = self._happy_eyeballs_table.get_preferred(\n            host_1, host_2)\n\n        if preferred_host:\n            return preferred_host, None\n        else:\n            return host_1, host_2", "response": "Get preferred host from DNS results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_journals_and_maybe_raise(self):\n        '''Check if any journal files exist and raise an error.'''\n        files = list(glob.glob(self._prefix_filename + '*-wpullinc'))\n\n        if files:\n            raise OSError('WARC file {} is incomplete.'.format(files[0]))", "response": "Check if any journal files exist and raise an error."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate and set as current WARC file.", "response": "def _start_new_warc_file(self, meta=False):\n        '''Create and set as current WARC file.'''\n        if self._params.max_size and not meta and self._params.appending:\n            while True:\n                self._warc_filename = self._generate_warc_filename()\n\n                if os.path.exists(self._warc_filename):\n                    _logger.debug('Skip {0}', self._warc_filename)\n                    self._sequence_num += 1\n                else:\n                    break\n        else:\n            self._warc_filename = self._generate_warc_filename(meta=meta)\n\n        _logger.debug('WARC file at {0}', self._warc_filename)\n\n        if not self._params.appending:\n            wpull.util.truncate_file(self._warc_filename)\n\n        self._warcinfo_record = WARCRecord()\n        self._populate_warcinfo(self._params.extra_fields)\n        self.write_record(self._warcinfo_record)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a suitable WARC filename.", "response": "def _generate_warc_filename(self, meta=False):\n        '''Return a suitable WARC filename.'''\n        if self._params.max_size is None:\n            sequence_name = ''\n        elif meta:\n            sequence_name = '-meta'\n        else:\n            sequence_name = '-{0:05d}'.format(self._sequence_num)\n\n        if self._params.compress:\n            extension = 'warc.gz'\n        else:\n            extension = 'warc'\n\n        return '{0}{1}.{2}'.format(\n            self._prefix_filename, sequence_name, extension\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _start_new_cdx_file(self):\n        '''Create and set current CDX file.'''\n        self._cdx_filename = '{0}.cdx'.format(self._prefix_filename)\n\n        if not self._params.appending:\n            wpull.util.truncate_file(self._cdx_filename)\n            self._write_cdx_header()\n        elif not os.path.exists(self._cdx_filename):\n            self._write_cdx_header()", "response": "Create and set current CDX file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding the metadata to the Warcinfo record.", "response": "def _populate_warcinfo(self, extra_fields=None):\n        '''Add the metadata to the Warcinfo record.'''\n        self._warcinfo_record.set_common_fields(\n            WARCRecord.WARCINFO, WARCRecord.WARC_FIELDS)\n\n        info_fields = NameValueRecord(wrap_width=1024)\n        info_fields['Software'] = self._params.software_string \\\n            or self.DEFAULT_SOFTWARE_STRING\n        info_fields['format'] = 'WARC File Format 1.0'\n        info_fields['conformsTo'] = \\\n            'http://bibnum.bnf.fr/WARC/WARC_ISO_28500_version1_latestdraft.pdf'\n\n        if extra_fields:\n            for name, value in extra_fields:\n                info_fields.add(name, value)\n\n        self._warcinfo_record.block_file = io.BytesIO(\n            bytes(info_fields) + b'\\r\\n')\n        self._warcinfo_record.compute_checksum()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets up the logging file.", "response": "def _setup_log(self):\n        '''Set up the logging file.'''\n        logger = logging.getLogger()\n        formatter = logging.Formatter(\n            '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n        self._log_temp_file = NamedTemporaryFile(\n            prefix='tmp-wpull-warc-',\n            dir=self._params.temp_dir,\n            suffix='.log.gz',\n            delete=False,\n        )\n        self._log_temp_file.close()  # For Windows\n\n        self._log_handler = handler = logging.StreamHandler(\n            io.TextIOWrapper(\n                gzip.GzipFile(\n                    filename=self._log_temp_file.name, mode='wb'\n                ),\n                encoding='utf-8'\n            )\n        )\n\n        logger.setLevel(logging.DEBUG)\n        logger.debug('Wpull needs the root logger level set to DEBUG.')\n\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n        handler.setLevel(logging.INFO)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmoves the file to the move_to directory.", "response": "def _move_file_to_dest_dir(self, filename):\n        '''Move the file to the ``move_to`` directory.'''\n        assert self._params.move_to\n\n        if os.path.isdir(self._params.move_to):\n            _logger.debug('Moved {} to {}.', self._warc_filename,\n                          self._params.move_to)\n            shutil.move(filename, self._params.move_to)\n        else:\n            _logger.error('{} is not a directory; not moving {}.',\n                          self._params.move_to, filename)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the content length and possibly the checksums.", "response": "def set_length_and_maybe_checksums(self, record, payload_offset=None):\n        '''Set the content length and possibly the checksums.'''\n        if self._params.digests:\n            record.compute_checksum(payload_offset)\n        else:\n            record.set_content_length()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_record(self, record):\n        '''Append the record to the WARC file.'''\n        # FIXME: probably not a good idea to modifiy arguments passed to us\n        # TODO: add extra gzip headers that wget uses\n        record.fields['WARC-Warcinfo-ID'] = self._warcinfo_record.fields[\n            WARCRecord.WARC_RECORD_ID]\n\n        _logger.debug('Writing WARC record {0}.',\n                      record.fields['WARC-Type'])\n\n        if self._params.compress:\n            open_func = gzip.GzipFile\n        else:\n            open_func = open\n\n        # Use getsize to get actual file size. Avoid tell() because it may\n        # not be the raw file position.\n        if os.path.exists(self._warc_filename):\n            before_offset = os.path.getsize(self._warc_filename)\n        else:\n            before_offset = 0\n\n        journal_filename = self._warc_filename + '-wpullinc'\n\n        with open(journal_filename, 'w') as file:\n            file.write('wpull-journal-version:1\\n')\n            file.write('offset:{}\\n'.format(before_offset))\n\n        try:\n            with open_func(self._warc_filename, mode='ab') as out_file:\n                for data in record:\n                    out_file.write(data)\n        except (OSError, IOError) as error:\n            _logger.info(\n                _('Rolling back file {filename} to length {length}.'),\n                filename=self._warc_filename, length=before_offset\n            )\n            with open(self._warc_filename, mode='wb') as out_file:\n                out_file.truncate(before_offset)\n\n            raise error\n        finally:\n            os.remove(journal_filename)\n\n        after_offset = os.path.getsize(self._warc_filename)\n\n        if self._cdx_filename:\n            raw_file_offset = before_offset\n            raw_file_record_size = after_offset - before_offset\n\n            self._write_cdx_field(\n                record, raw_file_record_size, raw_file_offset\n            )", "response": "Append the record to the WARC file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclose the WARC file and clean up any logging handlers.", "response": "def close(self):\n        '''Close the WARC file and clean up any logging handlers.'''\n        if self._log_temp_file:\n            self._log_handler.flush()\n\n            logger = logging.getLogger()\n            logger.removeHandler(self._log_handler)\n            self._log_handler.stream.close()\n\n            log_record = WARCRecord()\n            log_record.block_file = gzip.GzipFile(\n                filename=self._log_temp_file.name\n            )\n            log_record.set_common_fields('resource', 'text/plain')\n\n            log_record.fields['WARC-Target-URI'] = \\\n                'urn:X-wpull:log'\n\n            if self._params.max_size is not None:\n                if self._params.move_to is not None:\n                    self._move_file_to_dest_dir(self._warc_filename)\n\n                self._start_new_warc_file(meta=True)\n\n            self.set_length_and_maybe_checksums(log_record)\n            self.write_record(log_record)\n\n            log_record.block_file.close()\n\n            try:\n                os.remove(self._log_temp_file.name)\n            except OSError:\n                _logger.exception('Could not close log temp file.')\n\n            self._log_temp_file = None\n\n            self._log_handler.close()\n            self._log_handler = None\n\n            if self._params.move_to is not None:\n                self._move_file_to_dest_dir(self._warc_filename)\n\n        if self._cdx_filename and self._params.move_to is not None:\n            self._move_file_to_dest_dir(self._cdx_filename)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting the CDX header.", "response": "def _write_cdx_header(self):\n        '''Write the CDX header.\n\n        It writes the fields:\n\n        1. a: original URL\n        2. b: UNIX timestamp\n        3. m: MIME Type from the HTTP Content-type\n        4. s: response code\n        5. k: new style checksum\n        6. S: raw file record size\n        7. V: offset in raw file\n        8. g: filename of raw file\n        9. u: record ID\n        '''\n        with open(self._cdx_filename, mode='a', encoding='utf-8') as out_file:\n            out_file.write(self.CDX_DELIMINATOR)\n            out_file.write(self.CDX_DELIMINATOR.join((\n                'CDX',\n                'a', 'b', 'm', 's',\n                'k', 'S', 'V', 'g',\n                'u'\n            )))\n            out_file.write('\\n')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites the CDX field if needed.", "response": "def _write_cdx_field(self, record, raw_file_record_size, raw_file_offset):\n        '''Write the CDX field if needed.'''\n        if record.fields[WARCRecord.WARC_TYPE] != WARCRecord.RESPONSE \\\n           or not re.match(r'application/http; *msgtype *= *response',\n                           record.fields[WARCRecord.CONTENT_TYPE]):\n            return\n\n        url = record.fields['WARC-Target-URI']\n\n        _logger.debug('Writing CDX record {0}.', url)\n\n        http_header = record.get_http_header()\n\n        if http_header:\n            mime_type = self.parse_mimetype(\n                http_header.fields.get('Content-Type', '')\n            ) or '-'\n            response_code = str(http_header.status_code)\n        else:\n            mime_type = '-'\n            response_code = '-'\n\n        timestamp = str(int(\n            wpull.util.parse_iso8601_str(record.fields[WARCRecord.WARC_DATE])\n        ))\n\n        checksum = record.fields.get('WARC-Payload-Digest', '')\n\n        if checksum.startswith('sha1:'):\n            checksum = checksum.replace('sha1:', '', 1)\n        else:\n            checksum = '-'\n\n        raw_file_record_size_str = str(raw_file_record_size)\n        raw_file_offset_str = str(raw_file_offset)\n        filename = os.path.basename(self._warc_filename)\n        record_id = record.fields[WARCRecord.WARC_RECORD_ID]\n        fields_strs = (\n            url,\n            timestamp,\n            mime_type,\n            response_code,\n            checksum,\n            raw_file_record_size_str,\n            raw_file_offset_str,\n            filename,\n            record_id\n        )\n\n        with open(self._cdx_filename, mode='a', encoding='utf-8') as out_file:\n            out_file.write(self.CDX_DELIMINATOR.join(fields_strs))\n            out_file.write('\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the MIME type from a Content - Type string.", "response": "def parse_mimetype(cls, value):\n        '''Return the MIME type from a Content-Type string.\n\n        Returns:\n            str, None: A string in the form ``type/subtype`` or None.\n        '''\n        match = re.match(r'([a-zA-Z0-9-]+/[a-zA-Z0-9-]+)', value)\n\n        if match:\n            return match.group(1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _new_temp_file(self, hint='warcrecsess'):\n        '''Return new temp file.'''\n        return wpull.body.new_temp_file(\n            directory=self._temp_dir, hint=hint\n        )", "response": "Return new temp file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _record_revisit(self, payload_offset: int):\n        '''Record the revisit if possible.'''\n        fields = self._response_record.fields\n\n        ref_record_id = self._url_table.get_revisit_id(\n            fields['WARC-Target-URI'],\n            fields.get('WARC-Payload-Digest', '').upper().replace('SHA1:', '')\n        )\n\n        if ref_record_id:\n            try:\n                self._response_record.block_file.truncate(payload_offset)\n            except TypeError:\n                self._response_record.block_file.seek(0)\n\n                data = self._response_record.block_file.read(payload_offset)\n\n                self._response_record.block_file.truncate()\n                self._response_record.block_file.seek(0)\n                self._response_record.block_file.write(data)\n\n            self._recorder.set_length_and_maybe_checksums(\n                self._response_record\n            )\n\n            fields[WARCRecord.WARC_TYPE] = WARCRecord.REVISIT\n            fields['WARC-Refers-To'] = ref_record_id\n            fields['WARC-Profile'] = WARCRecord.SAME_PAYLOAD_DIGEST_URI\n            fields['WARC-Truncated'] = 'length'", "response": "Record the revisit if possible."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nincrements the number of files downloaded.", "response": "def increment(self, size: int):\n        '''Increment the number of files downloaded.\n\n        Args:\n            size: The size of the file\n        '''\n        assert size >= 0, size\n\n        self.files += 1\n        self.size += size\n        self.bandwidth_meter.feed(size)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_quota_exceeded(self) -> bool:\n        '''Return whether the quota is exceeded.'''\n\n        if self.quota and self._url_table is not None:\n            return self.size >= self.quota and \\\n                   self._url_table.get_root_url_todo_count() == 0", "response": "Return whether the quota is exceeded."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef increment_error(self, error: Exception):\n        '''Increment the error counter preferring base exceptions.'''\n        _logger.debug('Increment error %s', error)\n\n        for error_class in ERROR_PRIORITIES:\n            if isinstance(error, error_class):\n                self.errors[error_class] += 1\n                return\n\n        self.errors[type(error)] += 1", "response": "Increment the error counter preferring base exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a node to the head.", "response": "def link_head(self, node):\n        '''Add a node to the head. '''\n        assert not node.tail\n        old_head = self.head\n\n        if old_head:\n            assert old_head.tail == self\n            old_head.tail = node\n            node.head = old_head\n\n        node.tail = self\n        self.head = node"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a node to the tail.", "response": "def link_tail(self, node):\n        '''Add a node to the tail.'''\n        assert not node.head\n        old_tail = self.tail\n\n        if old_tail:\n            assert old_tail.head == self\n            old_tail.head = node\n            node.tail = old_tail\n\n        node.head = self\n        self.tail = node"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving this node and link any head or tail.", "response": "def unlink(self):\n        '''Remove this node and link any head or tail.'''\n        old_head = self.head\n        old_tail = self.tail\n\n        self.head = None\n        self.tail = None\n\n        if old_head:\n            old_head.tail = old_tail\n\n        if old_tail:\n            old_tail.head = old_head"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the status line bytes.", "response": "def parse_status_line(self, data):\n        '''Parse the status line bytes.\n\n        Returns:\n            tuple: An tuple representing the method, URI, and\n            version.\n        '''\n        match = re.match(\n            br'([a-zA-Z]+)[ \\t]+([^ \\t]+)[ \\t]+(HTTP/\\d+\\.\\d+)',\n            data\n        )\n        if match:\n            groups = match.groups()\n            if len(groups) == 3:\n                return wpull.string.to_str(\n                    (groups[0], groups[1], groups[2]),\n                    encoding=self.encoding,\n                )\n\n        raise ProtocolError('Error parsing status line.')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmodifying the request to be suitable for HTTP server.", "response": "def prepare_for_send(self, full_url=False):\n        '''Modify the request to be suitable for HTTP server.\n\n        Args:\n            full_url (bool): Use full URL as the URI. By default, only\n                the path of the URL is given to the server.\n        '''\n        assert self.url\n        assert self.method\n        assert self.version\n\n        url_info = self.url_info\n\n        if 'Host' not in self.fields:\n            self.fields['Host'] = url_info.hostname_with_port\n\n        if not full_url:\n            if url_info.query:\n                self.resource_path = '{0}?{1}'.format(url_info.path, url_info.query)\n            else:\n                self.resource_path = url_info.path\n        else:\n            self.resource_path = url_info.url"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_status_line(cls, data):\n        '''Parse the status line bytes.\n\n        Returns:\n            tuple: An tuple representing the version, code, and reason.\n        '''\n        match = re.match(\n            br'(HTTP/\\d+\\.\\d+)[ \\t]+([0-9]{1,3})[ \\t]*([^\\r\\n]*)',\n            data\n        )\n        if match:\n            groups = match.groups()\n            if len(groups) == 3:\n                return wpull.string.to_str(\n                    (groups[0], int(groups[1]), groups[2]),\n                    encoding='latin-1',\n                )\n\n        raise ProtocolError(\n            'Error parsing status line {line}\".'.format(line=ascii(data))\n        )", "response": "Parse the status line bytes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning whether the response is likely to be CSS.", "response": "def is_response(cls, response):\n        '''Return whether the document is likely to be CSS.'''\n        if 'css' in response.fields.get('content-type', '').lower():\n            return True\n\n        if response.body:\n            # Stylesheet mistakenly served as HTML\n            if 'html' in response.fields.get('content-type', '').lower():\n                return cls.is_file(response.body)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn whether the file is likely CSS.", "response": "def is_file(cls, file):\n        '''Return whether the file is likely CSS.'''\n        peeked_data = wpull.string.printable_bytes(\n            wpull.util.peek_file(file)).lower()\n\n        if b'<html' in peeked_data:\n            return VeryFalse\n\n        if re.search(br'@import |color:|background[a-z-]*:|font[a-z-]*:',\n                     peeked_data):\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbegin fetching the next request.", "response": "def start(self):\n        '''Begin fetching the next request.'''\n        self._current_session = session = self._http_client.session()\n\n        request = self.next_request()\n        assert request\n\n        if request.url_info.password or \\\n                request.url_info.hostname_with_port in self._hostnames_with_auth:\n            self._add_basic_auth_header(request)\n\n        response = yield from session.start(request)\n\n        self._process_response(response)\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef download(self, file: Optional[IO[bytes]]=None,\n                 duration_timeout: Optional[float]=None):\n        '''Download content.\n\n        Args:\n            file: An optional file object for the document contents.\n            duration_timeout: Maximum time in seconds of which the\n                entire file must be read.\n\n        Returns:\n            Response: An instance of :class:`.http.request.Response`.\n\n        See :meth:`WebClient.session` for proper usage of this function.\n\n        Coroutine.\n        '''\n        yield from \\\n            self._current_session.download(file, duration_timeout=duration_timeout)", "response": "Download content.\n\n        Args:\n            file: An optional file object for the document contents.\n            duration_timeout: Maximum time in seconds of which the\n                entire file must be read.\n\n        Returns:\n            Response: An instance of :class:`.http.request.Response`.\n\n        See :meth:`WebClient.session` for proper usage of this function.\n\n        Coroutine."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle the response and update the internal state.", "response": "def _process_response(self, response: Response):\n        '''Handle the response and update the internal state.'''\n        _logger.debug('Handling response')\n\n        self._redirect_tracker.load(response)\n\n        if self._redirect_tracker.is_redirect():\n            self._process_redirect()\n            self._loop_type = LoopType.redirect\n        elif response.status_code == http.client.UNAUTHORIZED and self._next_request.password:\n            self._process_authentication(response)\n        else:\n            self._next_request = None\n            self._loop_type = LoopType.normal\n\n        if self._cookie_jar:\n            self._extract_cookies(response)\n\n            if self._next_request:\n                self._add_cookies(self._next_request)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the Redirect Tracker.", "response": "def _process_redirect(self):\n        '''Update the Redirect Tracker.'''\n        _logger.debug('Handling redirect.')\n\n        if self._redirect_tracker.exceeded():\n            raise ProtocolError('Too many redirects.')\n\n        try:\n            url = self._redirect_tracker.next_location()\n\n            if not url:\n                raise ProtocolError('Redirect location missing.')\n\n            if self._redirect_tracker.is_repeat():\n                _logger.debug('Got redirect is repeat.')\n\n                request = self._original_request.copy()\n                request.url = url\n            else:\n                request = self._request_factory(url)\n\n            request.prepare_for_send()\n        except ValueError as error:\n            raise ProtocolError('Invalid redirect location.') from error\n\n        self._next_request = request\n\n        _logger.debug('Updated next redirect request to {0}.'.format(request))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the referrer hostname.", "response": "def _get_cookie_referrer_host(self):\n        '''Return the referrer hostname.'''\n        referer = self._original_request.fields.get('Referer')\n\n        if referer:\n            return URLInfo.parse(referer).hostname\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_cookies(self, request: Request):\n        '''Add the cookie headers to the Request.'''\n        self._cookie_jar.add_cookie_header(\n            request, self._get_cookie_referrer_host()\n        )", "response": "Add the cookie headers to the Request."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the cookie headers from the Response.", "response": "def _extract_cookies(self, response: Response):\n        '''Load the cookie headers from the Response.'''\n        self._cookie_jar.extract_cookies(\n            response, response.request, self._get_cookie_referrer_host()\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a fetch session.", "response": "def session(self, request: Request) -> WebSession:\n        '''Return a fetch session.\n\n        Args:\n            request: The request to be fetched.\n\n        Example usage::\n\n            client = WebClient()\n            session = client.session(Request('http://www.example.com'))\n\n            with session:\n                while not session.done():\n                    request = session.next_request()\n                    print(request)\n\n                    response = yield from session.start()\n                    print(response)\n\n                    if session.done():\n                        with open('myfile.html') as file:\n                            yield from session.download(file)\n                    else:\n                        yield from session.download()\n\n        Returns:\n            WebSession\n        '''\n        return WebSession(\n            request,\n            http_client=self._http_client,\n            redirect_tracker=self._redirect_tracker_factory(),\n            request_factory=self._request_factory,\n            cookie_jar=self._cookie_jar,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns URL string with the path replaced with directory only.", "response": "def to_dir_path_url(url_info: URLInfo) -> str:\n    '''Return URL string with the path replaced with directory only.'''\n    dir_name = posixpath.dirname(url_info.path)\n\n    if not dir_name.endswith('/'):\n        url_template = 'ftp://{}{}/'\n    else:\n        url_template = 'ftp://{}{}'\n\n    return url_template.format(url_info.hostname_with_port, dir_name)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprocess the current request and return the set of items.", "response": "def process(self):\n        '''Process.\n\n        Coroutine.\n        '''\n        self._item_session.request = request = Request(self._item_session.url_record.url)\n        verdict = self._fetch_rule.check_ftp_request(self._item_session)[0]\n\n        if not verdict:\n            self._item_session.skip()\n            return\n\n        self._add_request_password(request)\n\n        dir_name, filename = self._item_session.url_record.url_info.split_path()\n        if self._processor.fetch_params.glob and frozenset(filename) & GLOB_CHARS:\n            request = self._to_directory_request(request)\n            is_file = False\n            self._glob_pattern = urllib.parse.unquote(filename)\n        else:\n            is_file = yield from self._prepare_request_file_vs_dir(request)\n\n            self._file_writer_session.process_request(request)\n\n        wait_time = yield from self._fetch(request, is_file)\n\n        if wait_time:\n            _logger.debug('Sleeping {0}.', wait_time)\n            yield from asyncio.sleep(wait_time)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if file modify request and return whether is a file.", "response": "def _prepare_request_file_vs_dir(self, request: Request) -> bool:\n        '''Check if file, modify request, and return whether is a file.\n\n        Coroutine.\n        '''\n        if self._item_session.url_record.link_type:\n            is_file = self._item_session.url_record.link_type == LinkType.file\n        elif request.url_info.path.endswith('/'):\n            is_file = False\n        else:\n            is_file = 'unknown'\n\n        if is_file == 'unknown':\n            files = yield from self._fetch_parent_path(request)\n\n            if not files:\n                return True\n\n            filename = posixpath.basename(request.file_path)\n\n            for file_entry in files:\n                if file_entry.name == filename:\n                    _logger.debug('Found entry in parent. Type {}',\n                                  file_entry.type)\n                    is_file = file_entry.type != 'dir'\n                    break\n            else:\n                _logger.debug('Did not find entry. Assume file.')\n                return True\n\n            if not is_file:\n                request.url = append_slash_to_path_url(request.url_info)\n                _logger.debug('Request URL changed to {}. Path={}.',\n                              request.url, request.file_path)\n\n        return is_file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _fetch_parent_path(self, request: Request, use_cache: bool=True):\n        '''Fetch parent directory and return list FileEntry.\n\n        Coroutine.\n        '''\n        directory_url = to_dir_path_url(request.url_info)\n\n        if use_cache:\n            if directory_url in self._processor.listing_cache:\n                return self._processor.listing_cache[directory_url]\n\n        directory_request = copy.deepcopy(request)\n        directory_request.url = directory_url\n\n        _logger.debug('Check if URL {} is file with {}.', request.url,\n                      directory_url)\n\n        with self._processor.ftp_client.session() as session:\n            try:\n                yield from session.start_listing(directory_request)\n            except FTPServerError:\n                _logger.debug('Got an error. Assume is file.')\n\n                if use_cache:\n                    self._processor.listing_cache[directory_url] = None\n\n                return\n\n            temp_file = tempfile.NamedTemporaryFile(\n                dir=self._item_session.app_session.root_path,\n                prefix='tmp-wpull-list'\n            )\n\n            with temp_file as file:\n                directory_response = yield from session.download_listing(\n                    file, duration_timeout=self._fetch_rule.duration_timeout)\n\n        if use_cache:\n            self._processor.listing_cache[directory_url] = \\\n                directory_response.files\n\n        return directory_response.files", "response": "Fetch parent directory and return list FileEntry."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _fetch(self, request: Request, is_file: bool):\n        '''Fetch the request\n\n        Coroutine.\n        '''\n        _logger.info(_('Fetching \u2018{url}\u2019.'), url=request.url)\n\n        self._item_session.request = request\n        response = None\n\n        try:\n            with self._processor.ftp_client.session() as session:\n                if is_file:\n                    response = yield from session.start(request)\n                else:\n                    response = yield from session.start_listing(request)\n\n                self._item_session.response = response\n\n                action = self._result_rule.handle_pre_response(\n                    self._item_session\n                )\n\n                if action in (Actions.RETRY, Actions.FINISH):\n                    raise HookPreResponseBreak()\n\n                self._file_writer_session.process_response(response)\n\n                if not response.body:\n                    response.body = Body(\n                        directory=self._item_session.app_session.root_path,\n                        hint='resp_cb')\n\n                duration_timeout = self._fetch_rule.duration_timeout\n\n                if is_file:\n                    yield from session.download(\n                        response.body, duration_timeout=duration_timeout)\n                else:\n                    yield from session.download_listing(\n                        response.body, duration_timeout=duration_timeout)\n\n        except HookPreResponseBreak:\n            if response:\n                response.body.close()\n\n        except REMOTE_ERRORS as error:\n            self._log_error(request, error)\n\n            self._result_rule.handle_error(self._item_session, error)\n\n            wait_time = self._result_rule.get_wait_time(\n                self._item_session, error=error\n            )\n\n            if response:\n                response.body.close()\n\n            return wait_time\n        else:\n            self._log_response(request, response)\n            self._handle_response(request, response)\n\n            wait_time = self._result_rule.get_wait_time(\n                self._item_session\n            )\n\n            if is_file and \\\n                    self._processor.fetch_params.preserve_permissions and \\\n                    hasattr(response.body, 'name'):\n                yield from self._apply_unix_permissions(request, response)\n\n            response.body.close()\n\n            return wait_time", "response": "Fetch the request coroutine."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _add_listing_links(self, response: ListingResponse):\n        '''Add links from file listing response.'''\n        base_url = response.request.url_info.url\n\n        if self._glob_pattern:\n            level = self._item_session.url_record.level\n        else:\n            level = None\n\n        for file_entry in response.files:\n            if self._glob_pattern and \\\n                    not fnmatch.fnmatchcase(file_entry.name, self._glob_pattern):\n                continue\n\n            if file_entry.type == 'dir':\n                linked_url = urljoin_safe(base_url, file_entry.name + '/')\n            elif file_entry.type in ('file', 'symlink', None):\n                if not self._processor.fetch_params.retr_symlinks and \\\n                        file_entry.type == 'symlink':\n                    self._make_symlink(file_entry.name, file_entry.dest)\n                    linked_url = None\n                else:\n                    linked_url = urljoin_safe(base_url, file_entry.name)\n            else:\n                linked_url = None\n\n            if linked_url:\n                linked_url_info = parse_url_or_log(linked_url)\n\n                if linked_url_info:\n                    verdict = self._fetch_rule.check_ftp_request(self._item_session)[0]\n\n                    if verdict:\n                        if linked_url_info.path.endswith('/'):\n                            self._item_session.add_child_url(linked_url_info.url, link_type=LinkType.directory)\n                        else:\n                            self._item_session.add_child_url(linked_url_info.url, link_type=LinkType.file, level=level)", "response": "Add links from file listing response."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking a symlink on the system.", "response": "def _make_symlink(self, link_name: str, link_target: str):\n        '''Make a symlink on the system.'''\n        path = self._file_writer_session.extra_resource_path('dummy')\n\n        if path:\n            dir_path = os.path.dirname(path)\n            symlink_path = os.path.join(dir_path, link_name)\n\n            _logger.debug('symlink {} -> {}', symlink_path, link_target)\n\n            os.symlink(link_target, symlink_path)\n\n            _logger.info(\n                _('Created symbolic link {symlink_path} to target {symlink_target}.'),\n                symlink_path=symlink_path,\n                symlink_target=link_target\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfetches and apply Unix permissions. Coroutine.", "response": "def _apply_unix_permissions(self, request: Request, response: Response):\n        '''Fetch and apply Unix permissions.\n\n        Coroutine.\n        '''\n        files = yield from self._fetch_parent_path(request)\n\n        if not files:\n            return\n\n        filename = posixpath.basename(request.file_path)\n\n        for file_entry in files:\n            if file_entry.name == filename and file_entry.perm:\n                _logger.debug(\n                    'Set chmod {} o{:o}.',\n                    response.body.name, file_entry.perm\n                )\n                os.chmod(response.body.name, file_entry.perm)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build_url_rewriter(cls, session: AppSession):\n        '''Build URL rewriter if needed.'''\n        if session.args.escaped_fragment or session.args.strip_session_id:\n            return session.factory.new(\n                'URLRewriter',\n                hash_fragment=session.args.escaped_fragment,\n                session_id=session.args.strip_session_id\n            )", "response": "Build URL rewriter if needed."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates the URL filter instances.", "response": "def _build_url_filters(cls, session: AppSession):\n        '''Create the URL filter instances.\n\n        Returns:\n            A list of URL filter instances\n        '''\n        args = session.args\n\n        filters = [\n            HTTPSOnlyFilter() if args.https_only else SchemeFilter(),\n            RecursiveFilter(\n                enabled=args.recursive, page_requisites=args.page_requisites\n            ),\n            FollowFTPFilter(follow=args.follow_ftp),\n        ]\n\n        if args.no_parent:\n            filters.append(ParentFilter())\n\n        if args.domains or args.exclude_domains:\n            filters.append(\n                BackwardDomainFilter(args.domains, args.exclude_domains)\n            )\n\n        if args.hostnames or args.exclude_hostnames:\n            filters.append(\n                HostnameFilter(args.hostnames, args.exclude_hostnames)\n            )\n\n        if args.tries:\n            filters.append(TriesFilter(args.tries))\n\n        if args.level and args.recursive or args.page_requisites_level:\n            filters.append(\n                LevelFilter(args.level,\n                            inline_max_depth=args.page_requisites_level)\n            )\n\n        if args.accept_regex or args.reject_regex:\n            filters.append(RegexFilter(args.accept_regex, args.reject_regex))\n\n        if args.include_directories or args.exclude_directories:\n            filters.append(\n                DirectoryFilter(\n                    args.include_directories, args.exclude_directories\n                )\n            )\n\n        if args.accept or args.reject:\n            filters.append(BackwardFilenameFilter(args.accept, args.reject))\n\n        return filters"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build_document_converter(cls, session: AppSession):\n        '''Build the Document Converter.'''\n\n        if not session.args.convert_links:\n            return\n\n        converter = session.factory.new(\n            'BatchDocumentConverter',\n            session.factory['HTMLParser'],\n            session.factory['ElementWalker'],\n            session.factory['URLTable'],\n            backup=session.args.backup_converted\n        )\n\n        return converter", "response": "Build the Document Converter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset up the root logger if needed.", "response": "def _setup_logging(cls, args):\n        '''Set up the root logger if needed.\n\n        The root logger is set the appropriate level so the file and WARC logs\n        work correctly.\n        '''\n        assert (\n            logging.CRITICAL >\n            logging.ERROR >\n            logging.WARNING >\n            logging.INFO >\n            logging.DEBUG >\n            logging.NOTSET\n        )\n        assert (\n            LOG_VERY_QUIET >\n            LOG_QUIET >\n            LOG_NO_VERBOSE >\n            LOG_VERBOSE >\n            LOG_DEBUG\n        )\n        assert args.verbosity\n\n        root_logger = logging.getLogger()\n        current_level = root_logger.getEffectiveLevel()\n        min_level = LOG_VERY_QUIET\n\n        if args.verbosity == LOG_QUIET:\n            min_level = logging.ERROR\n\n        if args.verbosity in (LOG_NO_VERBOSE, LOG_VERBOSE) \\\n                or args.warc_file \\\n                or args.output_file or args.append_output:\n            min_level = logging.INFO\n\n        if args.verbosity == LOG_DEBUG:\n            min_level = logging.DEBUG\n\n        if current_level > min_level:\n            root_logger.setLevel(min_level)\n            root_logger.debug(\n                'Wpull needs the root logger level set to {0}.'\n                    .format(min_level)\n            )\n\n        if current_level <= logging.INFO:\n            logging.captureWarnings(True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets up the file message logger. A file log handler and with a formatter is added to the root logger.", "response": "def _setup_file_logger(cls, session: AppSession, args):\n        '''Set up the file message logger.\n\n        A file log handler and with a formatter is added to the root logger.\n        '''\n        if not (args.output_file or args.append_output):\n            return\n\n        logger = logging.getLogger()\n\n        formatter = logging.Formatter(\n            '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n        if args.output_file:\n            filename = args.output_file\n            mode = 'w'\n        else:\n            filename = args.append_output\n            mode = 'a'\n\n        session.file_log_handler = handler = logging.FileHandler(\n            filename, mode, encoding='utf-8')\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n\n        if args.verbosity == logging.DEBUG:\n            handler.setLevel(logging.DEBUG)\n        else:\n            handler.setLevel(logging.INFO)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding the action log to the WARC file.", "response": "def _add_warc_action_log(self, path, url):\n        '''Add the action log to the WARC file.'''\n        _logger.debug('Adding action log record.')\n\n        actions = []\n        with open(path, 'r', encoding='utf-8', errors='replace') as file:\n            for line in file:\n                actions.append(json.loads(line))\n\n        log_data = json.dumps(\n            {'actions': actions},\n            indent=4,\n        ).encode('utf-8')\n\n        self._action_warc_record = record = WARCRecord()\n        record.set_common_fields('metadata', 'application/json')\n        record.fields['WARC-Target-URI'] = 'urn:X-wpull:snapshot?url={0}' \\\n            .format(wpull.url.percent_encode_query_value(url))\n        record.block_file = io.BytesIO(log_data)\n\n        self._warc_recorder.set_length_and_maybe_checksums(record)\n        self._warc_recorder.write_record(record)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_warc_snapshot(self, filename, url):\n        '''Add the snaphot to the WARC file.'''\n        _logger.debug('Adding snapshot record.')\n\n        extension = os.path.splitext(filename)[1]\n        content_type = {\n            '.pdf': 'application/pdf',\n            '.html': 'text/html',\n            '.png': 'image/png',\n            '.gif': 'image/gif'\n            }[extension]\n\n        record = WARCRecord()\n        record.set_common_fields('resource', content_type)\n        record.fields['WARC-Target-URI'] = 'urn:X-wpull:snapshot?url={0}' \\\n            .format(wpull.url.percent_encode_query_value(url))\n\n        if self._action_warc_record:\n            record.fields['WARC-Concurrent-To'] = \\\n                self._action_warc_record.fields[WARCRecord.WARC_RECORD_ID]\n\n        with open(filename, 'rb') as in_file:\n            record.block_file = in_file\n\n            self._warc_recorder.set_length_and_maybe_checksums(record)\n            self._warc_recorder.write_record(record)", "response": "Add the snaphot to the WARC file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting links from the DOM.", "response": "def _scrape_document(self):\n        '''Extract links from the DOM.'''\n        mock_response = self._new_mock_response(\n            self._response, self._get_temp_path('phantom', '.html')\n        )\n\n        self._item_session.request = self._request\n        self._item_session.response = mock_response\n\n        self._processing_rule.scrape_document(item_session)\n\n        if mock_response.body:\n            mock_response.body.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new mock Response with the content.", "response": "def _new_mock_response(self, response, file_path):\n        '''Return a new mock Response with the content.'''\n        mock_response = copy.copy(response)\n\n        mock_response.body = Body(open(file_path, 'rb'))\n        mock_response.fields = NameValueRecord()\n\n        for name, value in response.fields.get_all():\n            mock_response.fields.add(name, value)\n\n        mock_response.fields['Content-Type'] = 'text/html; charset=\"utf-8\"'\n\n        return mock_response"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build_ssl_context(cls, session: AppSession) -> ssl.SSLContext:\n        '''Create the SSL options.\n\n        The options must be accepted by the `ssl` module.\n        '''\n        args = session.args\n\n        # Logic is based on tornado.netutil.ssl_options_to_context\n        ssl_context = ssl.SSLContext(args.secure_protocol)\n\n        if args.check_certificate:\n            ssl_context.verify_mode = ssl.CERT_REQUIRED\n            cls._load_ca_certs(session)\n            ssl_context.load_verify_locations(session.ca_certs_filename)\n        else:\n            ssl_context.verify_mode = ssl.CERT_NONE\n\n        if args.strong_crypto:\n            ssl_context.options |= ssl.OP_NO_SSLv2\n            ssl_context.options |= ssl.OP_NO_SSLv3  # POODLE\n\n            if hasattr(ssl, 'OP_NO_COMPRESSION'):\n                ssl_context.options |= ssl.OP_NO_COMPRESSION  # CRIME\n            else:\n                _logger.warning(_('Unable to disable TLS compression.'))\n\n        if args.certificate:\n            ssl_context.load_cert_chain(args.certificate, args.private_key)\n\n        if args.edg_file:\n            ssl.RAND_egd(args.edg_file)\n\n        if args.random_file:\n            with open(args.random_file, 'rb') as in_file:\n                # Use 16KB because Wget\n                ssl.RAND_add(in_file.read(15360), 0.0)\n\n        return ssl_context", "response": "Create the SSL options."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _load_ca_certs(cls, session: AppSession, clean: bool=True):\n        '''Load the Certificate Authority certificates.\n        '''\n        args = session.args\n\n        if session.ca_certs_filename:\n            return session.ca_certs_filename\n\n        certs = set()\n\n        if args.use_internal_ca_certs:\n            pem_filename = os.path.join(\n                os.path.dirname(__file__), '..', '..', 'cert', 'ca-bundle.pem'\n            )\n            certs.update(cls._read_pem_file(pem_filename, from_package=True))\n\n        if args.ca_directory:\n            if os.path.isdir(args.ca_directory):\n                for filename in os.listdir(args.ca_directory):\n                    if os.path.isfile(filename):\n                        certs.update(cls._read_pem_file(filename))\n            else:\n                _logger.warning(__(\n                    _('Certificate directory {path} does not exist.'),\n                    path=args.ca_directory\n                ))\n\n        if args.ca_certificate:\n            if os.path.isfile(args.ca_certificate):\n                certs.update(cls._read_pem_file(args.ca_certificate))\n            else:\n                _logger.warning(__(\n                    _('Certificate file {path} does not exist.'),\n                    path=args.ca_certificate\n                ))\n\n        session.ca_certs_filename = certs_filename = tempfile.mkstemp(\n            suffix='.pem', prefix='tmp-wpull-')[1]\n\n        def clean_certs_file():\n            os.remove(certs_filename)\n\n        if clean:\n            atexit.register(clean_certs_file)\n\n        with open(certs_filename, 'w+b') as certs_file:\n            for cert in certs:\n                certs_file.write(cert)\n\n        _logger.debug('CA certs loaded.')", "response": "Load the Certificate Authority certificates."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the PEM file.", "response": "def _read_pem_file(cls, filename, from_package=False):\n        '''Read the PEM file.\n\n        Returns:\n            iterable: An iterable of certificates. The certificate data\n            is :class:`byte`.\n        '''\n        _logger.debug('Reading PEM {0}.'.format(filename))\n\n        if from_package:\n            return wpull.util.filter_pem(wpull.util.get_package_data(filename))\n\n        with open(filename, 'rb') as in_file:\n            return wpull.util.filter_pem(in_file.read())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstarting a HTTP request.", "response": "def start(self, request: Request) -> Response:\n        '''Begin a HTTP request\n\n        Args:\n            request: Request information.\n\n        Returns:\n            A response populated with the HTTP headers.\n\n        Once the headers are received, call :meth:`download`.\n\n        Coroutine.\n        '''\n        if self._session_state != SessionState.ready:\n            raise RuntimeError('Session already started')\n\n        assert not self._request\n        self._request = request\n        _logger.debug(__('Client fetch request {0}.', request))\n\n        connection = yield from self._acquire_request_connection(request)\n        full_url = connection.proxied and not connection.tunneled\n\n        self._stream = stream = self._stream_factory(connection)\n\n        yield from self._stream.reconnect()\n\n        request.address = connection.address\n\n        self.event_dispatcher.notify(self.Event.begin_request, request)\n        write_callback = functools.partial(self.event_dispatcher.notify, self.Event.request_data)\n        stream.data_event_dispatcher.add_write_listener(write_callback)\n\n        yield from stream.write_request(request, full_url=full_url)\n\n        if request.body:\n            assert 'Content-Length' in request.fields\n            length = int(request.fields['Content-Length'])\n            yield from stream.write_body(request.body, length=length)\n\n        stream.data_event_dispatcher.remove_write_listener(write_callback)\n        self.event_dispatcher.notify(self.Event.end_request, request)\n\n        read_callback = functools.partial(self.event_dispatcher.notify, self.Event.response_data)\n        stream.data_event_dispatcher.add_read_listener(read_callback)\n\n        self._response = response = yield from stream.read_response()\n        response.request = request\n\n        self.event_dispatcher.notify(self.Event.begin_response, response)\n\n        self._session_state = SessionState.request_sent\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef download(\n            self,\n            file: Union[IO[bytes], asyncio.StreamWriter, None]=None,\n            raw: bool=False, rewind: bool=True,\n            duration_timeout: Optional[float]=None):\n        '''Read the response content into file.\n\n        Args:\n            file: A file object or asyncio stream.\n            raw: Whether chunked transfer encoding should be included.\n            rewind: Seek the given file back to its original offset after\n                reading is finished.\n            duration_timeout: Maximum time in seconds of which the\n                entire file must be read.\n\n        Be sure to call :meth:`start` first.\n\n        Coroutine.\n        '''\n        if self._session_state != SessionState.request_sent:\n            raise RuntimeError('Request not sent')\n\n        if rewind and file and hasattr(file, 'seek'):\n            original_offset = file.tell()\n        else:\n            original_offset = None\n\n        if not hasattr(file, 'drain'):\n            self._response.body = file\n\n            if not isinstance(file, Body):\n                self._response.body = Body(file)\n\n        read_future = self._stream.read_body(self._request, self._response, file=file, raw=raw)\n\n        try:\n            yield from asyncio.wait_for(read_future, timeout=duration_timeout)\n        except asyncio.TimeoutError as error:\n            raise DurationTimeout(\n                'Did not finish reading after {} seconds.'\n                    .format(duration_timeout)\n            ) from error\n\n        self._session_state = SessionState.response_received\n\n        if original_offset is not None:\n            file.seek(original_offset)\n\n        self.event_dispatcher.notify(self.Event.end_response, self._response)\n        self.recycle()", "response": "Read the response content into file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting an instance recursively to bytes.", "response": "def to_bytes(instance, encoding='utf-8', error='strict'):\n    '''Convert an instance recursively to bytes.'''\n    if isinstance(instance, bytes):\n        return instance\n    elif hasattr(instance, 'encode'):\n        return instance.encode(encoding, error)\n    elif isinstance(instance, list):\n        return list([to_bytes(item, encoding, error) for item in instance])\n    elif isinstance(instance, tuple):\n        return tuple([to_bytes(item, encoding, error) for item in instance])\n    elif isinstance(instance, dict):\n        return dict(\n            [(to_bytes(key, encoding, error), to_bytes(value, encoding, error))\n                for key, value in instance.items()])\n    else:\n        return instance"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_str(instance, encoding='utf-8'):\n    '''Convert an instance recursively to string.'''\n    if isinstance(instance, str):\n        return instance\n    elif hasattr(instance, 'decode'):\n        return instance.decode(encoding)\n    elif isinstance(instance, list):\n        return list([to_str(item, encoding) for item in instance])\n    elif isinstance(instance, tuple):\n        return tuple([to_str(item, encoding) for item in instance])\n    elif isinstance(instance, dict):\n        return dict(\n            [(to_str(key, encoding), to_str(value, encoding))\n                for key, value in instance.items()])\n    else:\n        return instance", "response": "Convert an instance recursively to string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the Python name of the encoder or decoder that corresponds to the given name Returns None if the name is not a valid codec name", "response": "def normalize_codec_name(name):\n    '''Return the Python name of the encoder/decoder\n\n    Returns:\n        str, None\n    '''\n    name = UnicodeDammit.CHARSET_ALIASES.get(name.lower(), name)\n\n    try:\n        return codecs.lookup(name).name\n    except (LookupError, TypeError, ValueError):\n        # TypeError occurs when name contains \\x00 (ValueError in Py3.5)\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef detect_encoding(data, encoding=None, fallback='latin1', is_html=False):\n    '''Detect the character encoding of the data.\n\n    Returns:\n        str: The name of the codec\n\n    Raises:\n        ValueError: The codec could not be detected. This error can only\n        occur if fallback is not a \"lossless\" codec.\n    '''\n    if encoding:\n        encoding = normalize_codec_name(encoding)\n\n    bs4_detector = EncodingDetector(\n        data,\n        override_encodings=(encoding,) if encoding else (),\n        is_html=is_html\n    )\n    candidates = itertools.chain(bs4_detector.encodings, (fallback,))\n\n    for candidate in candidates:\n        if not candidate:\n            continue\n\n        candidate = normalize_codec_name(candidate)\n\n        if not candidate:\n            continue\n\n        if candidate == 'ascii' and fallback != 'ascii':\n            # it's never ascii :)\n            # Falling back on UTF-8/CP-1252/Latin-1 reduces chance of\n            # failure\n            continue\n\n        if try_decoding(data, candidate):\n            return candidate\n\n    raise ValueError('Unable to detect encoding.')", "response": "Detect the character encoding of the data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef try_decoding(data, encoding):\n    '''Return whether the Python codec could decode the data.'''\n    try:\n        data.decode(encoding, 'strict')\n    except UnicodeError:\n        # Data under 16 bytes is very unlikely to be truncated\n        if len(data) > 16:\n            for trim in (1, 2, 3):\n                trimmed_data = data[:-trim]\n                if trimmed_data:\n                    try:\n                        trimmed_data.decode(encoding, 'strict')\n                    except UnicodeError:\n                        continue\n                    else:\n                        return True\n        return False\n    else:\n        return True", "response": "Return whether the Python codec could decode the data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef format_size(num, format_str='{num:.1f} {unit}'):\n    '''Format the file size into a human readable text.\n\n    http://stackoverflow.com/a/1094933/1524507\n    '''\n    for unit in ('B', 'KiB', 'MiB', 'GiB'):\n        if -1024 < num < 1024:\n            return format_str.format(num=num, unit=unit)\n\n        num /= 1024.0\n\n    return format_str.format(num=num, unit='TiB')", "response": "Format the file size into a human readable text."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nescape any control or non - ASCII characters from string.", "response": "def printable_str(text, keep_newlines=False):\n    '''Escape any control or non-ASCII characters from string.\n\n    This function is intended for use with strings from an untrusted\n    source such as writing to a console or writing to logs. It is\n    designed to prevent things like ANSI escape sequences from\n    showing.\n\n    Use :func:`repr` or :func:`ascii` instead for things such as\n    Exception messages.\n    '''\n    if isinstance(text, str):\n        new_text = ascii(text)[1:-1]\n    else:\n        new_text = ascii(text)\n\n    if keep_newlines:\n        new_text = new_text.replace('\\\\r', '\\r').replace('\\\\n', '\\n')\n\n    return new_text"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _print(self, *args):\n        '''Convenience function for the print function.\n\n        This function prints no newline.\n        '''\n        string = ' '.join([str(arg) for arg in args])\n        print(string, end='', file=self._stream)", "response": "Convenience function for the print function. This function prints no newline."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _println(self, *args):\n        '''Convenience function for the print function.'''\n        string = ' '.join([str(arg) for arg in args])\n        print(string, file=self._stream)", "response": "Convenience function for the print function."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints an entire status line including bar and stats.", "response": "def _print_status(self):\n        '''Print an entire status line including bar and stats.'''\n        self._clear_line()\n\n        self._print('  ')\n\n        if self.max_value:\n            self._print_percent()\n            self._print(' ')\n            self._print_bar()\n        else:\n            self._print_throbber()\n\n        self._print(' ')\n\n        if self.measurement == Measurement.bytes:\n            self._print_size_downloaded()\n        else:\n            self._print(self.current_value)\n\n        self._print(' ')\n        self._print_duration()\n        self._print(' ')\n\n        if self.measurement == Measurement.bytes:\n            self._print_speed()\n\n        self._flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints an indefinite progress bar.", "response": "def _print_throbber(self):\n        '''Print an indefinite progress bar.'''\n        self._print('[')\n\n        for position in range(self._bar_width):\n            self._print('O' if position == self._throbber_index else ' ')\n\n        self._print(']')\n\n        self._throbber_index = next(self._throbber_iter)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints a progress bar.", "response": "def _print_bar(self):\n        '''Print a progress bar.'''\n        self._print('[')\n\n        for position in range(self._bar_width):\n            position_fraction = position / (self._bar_width - 1)\n            position_bytes = position_fraction * self.max_value\n\n            if position_bytes < (self.continue_value or 0):\n                self._print('+')\n            elif position_bytes <= (self.continue_value or 0) + self.current_value:\n                self._print('=')\n            else:\n                self._print(' ')\n\n        self._print(']')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _print_duration(self):\n        '''Print the elapsed download time.'''\n        duration = int(time.time() - self._start_time)\n        self._print(datetime.timedelta(seconds=duration))", "response": "Print the elapsed download time."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _print_speed(self):\n        '''Print the current speed.'''\n        if self._bandwidth_meter.num_samples:\n            speed = self._bandwidth_meter.speed()\n\n            if self._human_format:\n                file_size_str = wpull.string.format_size(speed)\n            else:\n                file_size_str = '{:.1f} b'.format(speed * 8)\n\n            speed_str = _('{preformatted_file_size}/s').format(\n                preformatted_file_size=file_size_str\n            )\n        else:\n            speed_str = _('-- B/s')\n\n        self._print(speed_str)", "response": "Print the current speed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _print_percent(self):\n        '''Print how much is done in percentage.'''\n        fraction_done = ((self.continue_value or 0 + self.current_value) /\n                         self.max_value)\n\n        self._print('{fraction_done:.1%}'.format(fraction_done=fraction_done))", "response": "Print how much is done in percentage."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register(self, name: str):\n        '''Register hooks that can be connected.'''\n        if name in self._callbacks:\n            raise ValueError('Hook already registered')\n\n        self._callbacks[name] = None\n\n        if self._event_dispatcher is not None:\n            self._event_dispatcher.register(name)", "response": "Register hooks that can be connected."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connect(self, name, callback):\n        '''Add callback to hook.'''\n        if not self._callbacks[name]:\n            self._callbacks[name] = callback\n        else:\n            raise HookAlreadyConnectedError('Callback hook already connected.')", "response": "Connect a callback to a hook."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_allowed(self, user_agent, url, syntax=GYM2008):\n        if PY_MAJOR_VERSION < 3:\n            # The robot rules are stored internally as Unicode. The two lines \n            # below ensure that the parameters passed to this function are \n            # also Unicode. If those lines were not present and the caller \n            # passed a non-Unicode user agent or URL string to this function,\n            # Python would silently convert it to Unicode before comparing it\n            # to the robot rules. Such conversions use the default encoding \n            # (usually US-ASCII) and if the string couldn't be converted using\n            # that encoding, Python would raise a UnicodeError later on in the\n            # guts of this code which would be confusing. \n            # Converting the strings to Unicode here doesn't make the problem\n            # go away but it does make the conversion explicit so that \n            # failures are easier to understand. \n            if not isinstance(user_agent, unicode):\n                user_agent = user_agent.decode()\n            if not isinstance(url, unicode):\n                url = url.decode()\n        \n        if syntax not in (MK1996, GYM2008):\n            _raise_error(ValueError, \"Syntax must be MK1996 or GYM2008\")\n    \n        for ruleset in self.__rulesets:\n            if ruleset.does_user_agent_match(user_agent):\n                return ruleset.is_url_allowed(url, syntax)\n                \n        return True", "response": "Returns True if the user agent is allowed to visit the URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the crawl delay specified for this user agent.", "response": "def get_crawl_delay(self, user_agent):\n        \"\"\"Returns a float representing the crawl delay specified for this \n        user agent, or None if the crawl delay was unspecified or not a float.\n        \"\"\"\n        # See is_allowed() comment about the explicit unicode conversion.\n        if (PY_MAJOR_VERSION < 3) and (not isinstance(user_agent, unicode)):\n            user_agent = user_agent.decode()\n    \n        for ruleset in self.__rulesets:\n            if ruleset.does_user_agent_match(user_agent):\n                return ruleset.crawl_delay\n                \n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fetch(self, url, timeout=None):\n\n        # ISO-8859-1 is the default encoding for text files per the specs for\n        # HTTP 1.0 (RFC 1945 sec 3.6.1) and HTTP 1.1 (RFC 2616 sec 3.7.1).\n        # ref: http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.7.1\n        encoding = \"iso-8859-1\"\n        content = \"\"\n        expires_header = None\n        content_type_header = None\n        self._response_code = 0\n        self._source_url = url\n\n        if self.user_agent:\n            req = urllib_request.Request(url, None, \n                                         { 'User-Agent' : self.user_agent })\n        else:\n            req = urllib_request.Request(url)\n\n        try:\n            if timeout:\n                f = urllib_request.urlopen(req, timeout=timeout)\n            else:\n                f = urllib_request.urlopen(req)\n\n            content = f.read(MAX_FILESIZE)\n            # As of Python 2.5, f.info() looks like it returns the HTTPMessage\n            # object created during the connection. \n            expires_header = f.info().get(\"expires\")\n            content_type_header = f.info().get(\"Content-Type\")\n            # As of Python 2.4, this file-like object reports the response \n            # code, too. \n            if hasattr(f, \"code\"):\n                self._response_code = f.code\n            else:\n                self._response_code = 200\n            f.close()\n        except urllib_error.URLError:\n            # This is a slightly convoluted way to get the error instance,\n            # but it works under Python 2 & 3. \n            error_instance = sys.exc_info()\n            if len(error_instance) > 1:\n                error_instance = error_instance[1]\n            if hasattr(error_instance, \"code\"):\n                self._response_code = error_instance.code\n                \n        # MK1996 section 3.4 says, \"...robots should take note of Expires \n        # header set by the origin server. If no cache-control directives \n        # are present robots should default to an expiry of 7 days\".\n        \n        # This code is lazy and looks at the Expires header but not \n        # Cache-Control directives.\n        self.expiration_date = None\n        if self._response_code >= 200 and self._response_code < 300:\n            # All's well.\n            if expires_header:\n                self.expiration_date = email_utils.parsedate_tz(expires_header)\n                \n                if self.expiration_date:\n                    # About time zones -- the call to parsedate_tz() returns a\n                    # 10-tuple with the time zone offset in the 10th element. \n                    # There are 3 valid formats for HTTP dates, and one of \n                    # them doesn't contain time zone information. (UTC is \n                    # implied since all HTTP header dates are UTC.) When given\n                    # a date that lacks time zone information, parsedate_tz() \n                    # returns None in the 10th element. mktime_tz() interprets\n                    # None in the 10th (time zone) element to mean that the \n                    # date is *local* time, not UTC. \n                    # Therefore, if the HTTP timestamp lacks time zone info \n                    # and I run that timestamp through parsedate_tz() and pass\n                    # it directly to mktime_tz(), I'll get back a local \n                    # timestamp which isn't what I want. To fix this, I simply\n                    # convert a time zone of None to zero. It's much more \n                    # difficult to explain than to fix. =)\n                    # ref: http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.3.1\n                    if self.expiration_date[9] == None: \n                        self.expiration_date = self.expiration_date[:9] + (0,)\n                \n                    self.expiration_date = email_utils.mktime_tz(self.expiration_date)\n                    if self.use_local_time: \n                        # I have to do a little more converting to get this \n                        # UTC timestamp into localtime.\n                        self.expiration_date = time.mktime(time.gmtime(self.expiration_date)) \n                #else:\n                    # The expires header was garbage.\n\n        if not self.expiration_date: self.expiration_date = self._now() + SEVEN_DAYS\n\n        if (self._response_code >= 200) and (self._response_code < 300):\n            # All's well.\n            media_type, encoding = _parse_content_type_header(content_type_header)\n            # RFC 2616 sec 3.7.1 -- \n            # When no explicit charset parameter is provided by the sender, \n            # media subtypes  of the \"text\" type are defined to have a default\n            # charset value of \"ISO-8859-1\" when received via HTTP.\n            # http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.7.1\n            if not encoding: \n                encoding = \"iso-8859-1\"\n        elif self._response_code in (401, 403):\n            # 401 or 403 ==> Go away or I will taunt you a second time! \n            # (according to MK1996)\n            content = \"User-agent: *\\nDisallow: /\\n\"\n        elif self._response_code == 404:\n            # No robots.txt ==> everyone's welcome\n            content = \"\"\n        else:        \n            # Uh-oh. I punt this up to the caller. \n            _raise_error(urllib_error.URLError, self._response_code)\n\n        if ((PY_MAJOR_VERSION == 2) and isinstance(content, str)) or \\\n           ((PY_MAJOR_VERSION > 2)  and (not isinstance(content, str))):\n            # This ain't Unicode yet! It needs to be.\n            \n            # Unicode decoding errors are another point of failure that I punt \n            # up to the caller.\n            try:\n                content = content.decode(encoding)\n            except UnicodeError:\n                _raise_error(UnicodeError,\n                \"Robots.txt contents are not in the encoding expected (%s).\" % encoding)\n            except (LookupError, ValueError):\n                # LookupError ==> Python doesn't have a decoder for that encoding.\n                # One can also get a ValueError here if the encoding starts with \n                # a dot (ASCII 0x2e). See Python bug 1446043 for details. This \n                # bug was supposedly fixed in Python 2.5.\n                _raise_error(UnicodeError,\n                        \"I don't understand the encoding \\\"%s\\\".\" % encoding)\n        \n        # Now that I've fetched the content and turned it into Unicode, I \n        # can parse it.\n        self.parse(content)", "response": "Attempts to fetch the URL requested which should refer to a robots. txt file e. g. http://example. com"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the passed string as a set of robots. txt rules.", "response": "def parse(self, s):\n        \"\"\"Parses the passed string as a set of robots.txt rules.\"\"\"\n        self._sitemaps = [ ]\n        self.__rulesets = [ ]\n        \n        if (PY_MAJOR_VERSION > 2) and (isinstance(s, bytes) or isinstance(s, bytearray)) or \\\n           (PY_MAJOR_VERSION == 2) and (not isinstance(s, unicode)):            \n            s = s.decode(\"iso-8859-1\")\n    \n        # Normalize newlines.\n        s = _end_of_line_regex.sub(\"\\n\", s)\n        \n        lines = s.split(\"\\n\")\n        \n        previous_line_was_a_user_agent = False\n        current_ruleset = None\n        \n        for line in lines:\n            line = line.strip()\n            \n            if line and line[0] == '#':\n                # \"Lines containing only a comment are discarded completely, \n                # and therefore do not indicate a record boundary.\" (MK1994)\n                pass\n            else:\n                # Remove comments\n                i = line.find(\"#\")\n                if i != -1: line = line[:i]\n        \n                line = line.strip()\n                \n                if not line:\n                    # An empty line indicates the end of a ruleset.\n                    if current_ruleset and current_ruleset.is_not_empty():\n                        self.__rulesets.append(current_ruleset)\n                    \n                    current_ruleset = None\n                    previous_line_was_a_user_agent = False\n                else:\n                    # Each non-empty line falls into one of six categories:\n                    # 1) User-agent: blah blah blah\n                    # 2) Disallow: blah blah blah\n                    # 3) Allow: blah blah blah\n                    # 4) Crawl-delay: blah blah blah\n                    # 5) Sitemap: blah blah blah\n                    # 6) Everything else\n                    # 1 - 5 are interesting and I find them with the regex \n                    # below. Category 6 I discard as directed by the MK1994 \n                    # (\"Unrecognised headers are ignored.\")\n                    # Note that 4 & 5 are specific to GYM2008 syntax, but \n                    # respecting them here is not a problem. They're just \n                    # additional information the the caller is free to ignore.\n                    matches = _directive_regex.findall(line)\n                    \n                    # Categories 1 - 5 produce two matches, #6 produces none.\n                    if matches:\n                        field, data = matches[0]\n                        field = field.lower()\n                        data = _scrub_data(data)\n\n                        # Matching \"useragent\" is a deviation from the \n                        # MK1994/96 which permits only \"user-agent\".\n                        if field in (\"useragent\", \"user-agent\"):\n                            if previous_line_was_a_user_agent:\n                                # Add this UA to the current ruleset \n                                if current_ruleset and data:\n                                    current_ruleset.add_robot_name(data)\n                            else:\n                                # Save the current ruleset and start a new one.\n                                if current_ruleset and current_ruleset.is_not_empty():\n                                    self.__rulesets.append(current_ruleset)\n                                #else:\n                                    # (is_not_empty() == False) ==> malformed \n                                    # robots.txt listed a UA line but provided\n                                    # no name or didn't provide any rules \n                                    # for a named UA.\n                                current_ruleset = _Ruleset()\n                                if data: \n                                    current_ruleset.add_robot_name(data)\n                            \n                            previous_line_was_a_user_agent = True\n                        elif field == \"allow\":\n                            previous_line_was_a_user_agent = False\n                            if current_ruleset:\n                                current_ruleset.add_allow_rule(data)\n                        elif field == \"sitemap\":\n                            previous_line_was_a_user_agent = False\n                            self._sitemaps.append(data)\n                        elif field == \"crawl-delay\":\n                            # Only Yahoo documents the syntax for Crawl-delay.\n                            # ref: http://help.yahoo.com/l/us/yahoo/search/webcrawler/slurp-03.html\n                            previous_line_was_a_user_agent = False\n                            if current_ruleset:\n                                try:\n                                    current_ruleset.crawl_delay = float(data)\n                                except ValueError:\n                                    # Invalid crawl-delay -- ignore.\n                                    pass\n                        else:\n                            # This is a disallow line\n                            previous_line_was_a_user_agent = False\n                            if current_ruleset:\n                                current_ruleset.add_disallow_rule(data)\n\n        if current_ruleset and current_ruleset.is_not_empty():\n            self.__rulesets.append(current_ruleset)\n            \n        # Now that I have all the rulesets, I want to order them in a way \n        # that makes comparisons easier later. Specifically, any ruleset that \n        # contains the default user agent '*' should go at the end of the list\n        # so that I only apply the default as a last resort. According to \n        # MK1994/96, there should only be one ruleset that specifies * as the \n        # user-agent, but you know how these things go.\n        not_defaults = [r for r in self.__rulesets if not r.is_default()]\n        defaults = [r for r in self.__rulesets if r.is_default()]\n\n        self.__rulesets = not_defaults + defaults"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a filename from a URL.", "response": "def url_to_filename(url, index='index.html', alt_char=False):\n    '''Return a filename from a URL.\n\n    Args:\n        url (str): The URL.\n        index (str): If a filename could not be derived from the URL path,\n            use index instead. For example, ``/images/`` will return\n            ``index.html``.\n        alt_char (bool): If True, the character for the query deliminator\n            will be ``@`` intead of ``?``.\n\n    This function does not include the directories and does not sanitize\n    the filename.\n\n    Returns:\n        str\n    '''\n    assert isinstance(url, str), 'Expect str. Got {}.'.format(type(url))\n    url_split_result = urllib.parse.urlsplit(url)\n\n    filename = url_split_result.path.split('/')[-1]\n\n    if not filename:\n        filename = index\n\n    if url_split_result.query:\n        if alt_char:\n            query_delim = '@'\n        else:\n            query_delim = '?'\n\n        filename = '{0}{1}{2}'.format(\n            filename, query_delim, url_split_result.query\n        )\n\n    return filename"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef url_to_dir_parts(url, include_protocol=False, include_hostname=False,\n                     alt_char=False):\n    '''Return a list of directory parts from a URL.\n\n    Args:\n        url (str): The URL.\n        include_protocol (bool): If True, the scheme from the URL will be\n            included.\n        include_hostname (bool): If True, the hostname from the URL will be\n            included.\n        alt_char (bool): If True, the character for the port deliminator\n            will be ``+`` intead of ``:``.\n\n    This function does not include the filename and the paths are not\n    sanitized.\n\n    Returns:\n        list\n    '''\n    assert isinstance(url, str), 'Expect str. Got {}.'.format(type(url))\n    url_split_result = urllib.parse.urlsplit(url)\n\n    parts = []\n\n    if include_protocol:\n        parts.append(url_split_result.scheme)\n\n    if include_hostname:\n        hostname = url_split_result.hostname\n\n        if url_split_result.port:\n            if alt_char:\n                port_delim = '+'\n            else:\n                port_delim = ':'\n\n            hostname = '{0}{1}{2}'.format(\n                hostname, port_delim, url_split_result.port\n            )\n\n        parts.append(hostname)\n\n    for path_part in url_split_result.path.split('/'):\n        if path_part:\n            parts.append(path_part)\n\n    if not url.endswith('/') and parts:\n        parts.pop()\n\n    return parts", "response": "Return a list of directory parts from a URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef safe_filename(filename, os_type='unix', no_control=True, ascii_only=True,\n                  case=None, encoding='utf8', max_length=None):\n    '''Return a safe filename or path part.\n\n    Args:\n        filename (str): The filename or path component.\n        os_type (str): If ``unix``, escape the slash. If ``windows``, escape\n            extra Windows characters.\n        no_control (bool): If True, escape control characters.\n        ascii_only (bool): If True, escape non-ASCII characters.\n        case (str): If ``lower``, lowercase the string. If ``upper``, uppercase\n            the string.\n        encoding (str): The character encoding.\n        max_length (int): The maximum length of the filename.\n\n    This function assumes that `filename` has not already been percent-encoded.\n\n    Returns:\n        str\n    '''\n    assert isinstance(filename, str), \\\n        'Expect str. Got {}.'.format(type(filename))\n\n    if filename in ('.', os.curdir):\n        new_filename = '%2E'\n    elif filename in ('.', os.pardir):\n        new_filename = '%2E%2E'\n    else:\n        unix = os_type == 'unix'\n        windows = os_type == 'windows'\n        encoder_args = (unix, no_control, windows, ascii_only)\n\n        if encoder_args not in _encoder_cache:\n            _encoder_cache[encoder_args] = PercentEncoder(\n                unix=unix, control=no_control, windows=windows,\n                ascii_=ascii_only\n            )\n\n        encoder = _encoder_cache[encoder_args]\n        encoded_filename = filename.encode(encoding)\n        new_filename = encoder.quote(encoded_filename).decode(encoding)\n\n    if os_type == 'windows':\n        if new_filename[-1] in ' .':\n            new_filename = '{0}{1:02X}'.format(\n                new_filename[:-1], new_filename[-1]\n            )\n\n    if max_length and len(new_filename) > max_length:\n        hash_obj = hashlib.sha1(new_filename.encode(encoding))\n        new_length = max(0, max_length - 8)\n        new_filename = '{0}{1}'.format(\n            new_filename[:new_length], hash_obj.hexdigest()[:8]\n        )\n\n    if case == 'lower':\n        new_filename = new_filename.lower()\n    elif case == 'upper':\n        new_filename = new_filename.upper()\n\n    return new_filename", "response": "Return a safe filename or path part."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a directory path free of filenames.", "response": "def anti_clobber_dir_path(dir_path, suffix='.d'):\n    '''Return a directory path free of filenames.\n\n    Args:\n        dir_path (str): A directory path.\n        suffix (str): The suffix to append to the part of the path that is\n             a file.\n\n    Returns:\n        str\n    '''\n    dir_path = os.path.normpath(dir_path)\n    parts = dir_path.split(os.sep)\n\n    for index in range(len(parts)):\n        test_path = os.sep.join(parts[:index + 1])\n\n        if os.path.isfile(test_path):\n            parts[index] += suffix\n\n            return os.sep.join(parts)\n\n    return dir_path"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_content_disposition(text):\n    '''Parse a Content-Disposition header value.'''\n    match = re.search(r'filename\\s*=\\s*(.+)', text, re.IGNORECASE)\n\n    if not match:\n        return\n\n    filename = match.group(1)\n\n    if filename[0] in '\"\\'':\n        match = re.match(r'(.)(.+)(?!\\\\)\\1', filename)\n\n        if match:\n            filename = match.group(2).replace('\\\\\"', '\"')\n\n            return filename\n\n    else:\n        filename = filename.partition(';')[0].strip()\n        return filename", "response": "Parse a Content - Disposition header value."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef safe_filename(self, part):\n        '''Return a safe filename or file part.'''\n        return safe_filename(\n            part,\n            os_type=self._os_type, no_control=self._no_control,\n            ascii_only=self._ascii_only, case=self._case,\n            max_length=self._max_filename_length,\n            )", "response": "Return a safe filename or file part."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints warnings about any enabled hazardous options.", "response": "def _warn_unsafe_options(cls, args):\n        '''Print warnings about any enabled hazardous options.\n\n        This function will print messages complaining about:\n\n        * ``--save-headers``\n        * ``--no-iri``\n        * ``--output-document``\n        * ``--ignore-fatal-errors``\n        '''\n        enabled_options = []\n\n        for option_name in cls.UNSAFE_OPTIONS:\n            if getattr(args, option_name):\n                enabled_options.append(option_name)\n\n        if enabled_options:\n            _logger.warning(__(\n                _('The following unsafe options are enabled: {list}.'),\n                list=enabled_options\n            ))\n            _logger.warning(\n                _('The use of unsafe options may lead to unexpected behavior '\n                  'or file corruption.'))\n\n        if not args.retr_symlinks:\n            _logger.warning(\n                _('The --retr-symlinks=off option is a security risk.')\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints warnings about any options that may be silly.", "response": "def _warn_silly_options(cls, args):\n        '''Print warnings about any options that may be silly.'''\n        if 'page-requisites' in args.span_hosts_allow \\\n                and not args.page_requisites:\n            _logger.warning(\n                _('Spanning hosts is allowed for page requisites, '\n                  'but the page requisites option is not on.')\n            )\n\n        if 'linked-pages' in args.span_hosts_allow \\\n                and not args.recursive:\n            _logger.warning(\n                _('Spanning hosts is allowed for linked pages, '\n                  'but the recursive option is not on.')\n            )\n\n        if args.warc_file and \\\n                (args.http_proxy or args.https_proxy):\n            _logger.warning(_('WARC specifications do not handle proxies.'))\n\n        if (args.password or args.ftp_password or\n                args.http_password or args.proxy_password) and \\\n                args.warc_file:\n            _logger.warning(\n                _('Your password is recorded in the WARC file.'))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef guess_datetime_format(lines: Iterable[str], threshold: int=5) \\\n        -> Tuple[Optional[str], Optional[bool]]:\n    '''Guess whether order of the year, month, day and 12/24 hour.\n\n    Returns:\n        tuple: First item is either str ``ymd``, ``dmy``, ``mdy``\n        or ``None``.\n        Second item is either True for 12-hour time or False for 24-hour time\n        or None.\n    '''\n    time_12_score = 0\n    time_24_score = 0\n    date_ymd_score = 0\n    date_dmy_score = 0\n    date_mdy_score = 0\n\n    for line in lines:\n        line = unicodedata.normalize('NFKD', line).lower()\n\n        if DAY_PERIOD_PATTERN.search(line):\n            time_12_score += 1\n        else:\n            time_24_score += 1\n\n        if ISO_8601_DATE_PATTERN.search(line):\n            date_ymd_score += 1\n\n        if MMM_DD_YY_PATTERN.search(line):\n            date_mdy_score += 1\n\n        match = NN_NN_NNNN_PATTERN.search(line)\n\n        if match:\n            num_1 = int(match.group(1))\n            num_2 = int(match.group(2))\n\n            if num_1 > 12:\n                date_dmy_score += 1\n            elif num_2 > 12:\n                date_mdy_score += 1\n\n        time_score = time_12_score + time_24_score\n        date_score = date_ymd_score + date_dmy_score + date_mdy_score\n        if time_score >= threshold and date_score >= threshold:\n            break\n\n    if date_ymd_score or date_dmy_score or date_mdy_score:\n        top = max([\n            (date_ymd_score, 'ymd'),\n            (date_dmy_score, 'dmy'),\n            (date_mdy_score, 'mdy'),\n            ],\n            key=lambda item: item[0]\n        )\n        date_format = top[1]\n    else:\n        date_format = None\n\n    if time_12_score or time_24_score:\n        day_period = True if time_12_score > time_24_score else False\n    else:\n        day_period = None\n\n    return date_format, day_period", "response": "Guess whether order of the year month day and 24 - hour time is correct."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_datetime(text: str, date_format: str=None,\n                   is_day_period: Optional[bool]=None,\n                   datetime_now: datetime.datetime=None) \\\n        -> Tuple[datetime.datetime, int, int]:\n    '''Parse date/time from a line of text into datetime object.'''\n    datetime_now = datetime_now or \\\n        datetime.datetime.utcnow().replace(tzinfo=datetime.timezone.utc)\n    year = datetime_now.year\n    month = datetime_now.month\n    day = datetime_now.day\n    hour = 0\n    minute = 0\n    second = 0\n    date_ok = False\n    start_index = float('+inf')\n    end_index = float('-inf')\n    ambiguous_year = False\n\n    text = unicodedata.normalize('NFKD', text).lower()\n\n    # Let's do time first\n    match = TIME_PATTERN.search(text)\n\n    if match:\n        hour_str = match.group(1)\n        hour = int(hour_str)\n        minute = int(match.group(2))\n        day_period = match.group(4)\n\n        if match.group(3):\n            second = int(match.group(3))\n\n        if day_period and is_day_period and hour < 13:\n            if day_period.lower() in PM_STRINGS:\n                if hour != 12:\n                    hour += 12\n            elif hour == 12:\n                hour = 0\n\n        start_index = match.start()\n        end_index = match.end()\n\n    # Now try dates\n    if date_format == 'ymd' or not date_format:\n        match = ISO_8601_DATE_PATTERN.search(text)\n        if match:\n            date_ok = True\n            year = int(match.group(1))\n            month = int(match.group(2))\n            day = int(match.group(3))\n\n            start_index = min(start_index, match.start())\n            end_index = max(end_index, match.end())\n\n    if not date_ok and (date_format == 'mdy' or not date_format):\n        match = MMM_DD_YY_PATTERN.search(text)\n        if match:\n            date_ok = True\n            month_str = match.group(1)\n            month = parse_month(month_str)\n            day = int(match.group(2))\n            year_str = match.group(3)\n\n            if year_str and len(year_str) == 4:\n                year = int(year_str)\n            else:\n                ambiguous_year = True\n\n            start_index = min(start_index, match.start())\n            end_index = max(end_index, match.end())\n\n    if not date_ok:\n        match = NN_NN_NNNN_PATTERN.search(text)\n        if match:\n            date_ok = True\n            num_1 = int(match.group(1))\n            num_2 = int(match.group(2))\n            year = int(match.group(3))\n\n            if year < 100:\n                year = y2k(year)\n\n            if date_format == 'mdy' or num_2 > 12:\n                month = num_1\n                day = num_2\n            else:\n                day = num_1\n                month = num_2\n\n            start_index = min(start_index, match.start())\n            end_index = max(end_index, match.end())\n\n    if date_ok:\n        guess_date = datetime.datetime(year, month, day, hour, minute, second,\n                                       tzinfo=datetime.timezone.utc)\n\n        if ambiguous_year and guess_date > datetime_now:\n            # Sometimes year is not shown within 6 months\n            # Year is shown for dates in the future\n            guess_date = guess_date.replace(year=year - 1)\n\n        return guess_date, start_index, end_index\n\n    else:\n        raise ValueError('Failed to parse date from {}'.format(repr(text)))", "response": "Parse a line of text into a datetime object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses month string into integer.", "response": "def parse_month(text: str) -> int:\n    '''Parse month string into integer.'''\n    text = text.lower()\n    try:\n        return MONTH_MAP[text]\n    except KeyError:\n        pass\n\n    try:\n        return MONTH_MAP[text[:3]]\n    except KeyError:\n        pass\n\n    raise ValueError('Month {} not found.'.format(repr(text)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert two digit year to four digit year.", "response": "def y2k(year: int) -> int:\n    '''Convert two digit year to four digit year.'''\n    assert 0 <= year <= 99, 'Not a two digit year {}'.format(year)\n    return year + 1000 if year >= 69 else year + 2000"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_cldr_json(directory, language_codes=DEFAULT_LANGUAGE_CODES,\n                    massage=True):\n    '''Parse CLDR JSON datasets to for date time things.'''\n\n    am_strings = set()\n    pm_strings = set()\n    month_to_int = {}\n\n    for lang in language_codes:\n        path = os.path.join(directory, 'main', lang, 'ca-gregorian.json')\n\n        with open(path) as in_file:\n            doc = json.load(in_file)\n\n        months_dict = doc['main'][lang]['dates']['calendars']['gregorian']['months']['format']['abbreviated']\n        day_periods_dict = doc['main'][lang]['dates']['calendars']['gregorian']['dayPeriods']['format']['abbreviated']\n\n        for month, month_str in months_dict.items():\n            if massage:\n                month_str = unicodedata.normalize('NFKD', month_str).lower().strip('.')\n\n            month_to_int[month_str] = int(month)\n\n        am_str = day_periods_dict['am']\n        pm_str = day_periods_dict['pm']\n\n        if massage:\n            am_str = unicodedata.normalize('NFKD', am_str).lower().strip('.')\n            pm_str = unicodedata.normalize('NFKD', pm_str).lower().strip('.')\n\n        am_strings.add(am_str)\n        pm_strings.add(pm_str)\n\n    print(pprint.pformat(am_strings))\n    print(pprint.pformat(pm_strings))\n    print(pprint.pformat(month_to_int))", "response": "Parse CLDR JSON datasets to for date time things."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nacquiring a connection to a proxy.", "response": "def acquire_proxy(self, host, port, use_ssl=False, host_key=None,\n                      tunnel=True):\n        '''Check out a connection.\n\n        This function is the same as acquire but with extra arguments\n        concerning proxies.\n\n        Coroutine.\n        '''\n        if self._host_filter and not self._host_filter.test(host):\n            connection = yield from \\\n                super().acquire(host, port, use_ssl, host_key)\n\n            return connection\n\n        host_key = host_key or (host, port, use_ssl)\n        proxy_host, proxy_port = self._proxy_address\n\n        connection = yield from super().acquire(\n            proxy_host, proxy_port, self._proxy_ssl, host_key=host_key\n        )\n        connection.proxied = True\n\n        _logger.debug('Request for proxy connection.')\n\n        if connection.closed():\n            _logger.debug('Connecting to proxy.')\n            yield from connection.connect()\n\n            if tunnel:\n                yield from self._establish_tunnel(connection, (host, port))\n\n            if use_ssl:\n                ssl_connection = yield from connection.start_tls(self._ssl_context)\n                ssl_connection.proxied = True\n                ssl_connection.tunneled = True\n\n                self._connection_map[ssl_connection] = connection\n                connection.wrapped_connection = ssl_connection\n\n                return ssl_connection\n\n        if connection.wrapped_connection:\n            ssl_connection = connection.wrapped_connection\n            self._connection_map[ssl_connection] = connection\n            return ssl_connection\n        else:\n            return connection"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _establish_tunnel(self, connection, address):\n        '''Establish a TCP tunnel.\n\n        Coroutine.\n        '''\n        host = '[{}]'.format(address[0]) if ':' in address[0] else address[0]\n        port = address[1]\n        request = RawRequest('CONNECT', '{0}:{1}'.format(host, port))\n\n        self.add_auth_header(request)\n\n        stream = Stream(connection, keep_alive=True)\n\n        _logger.debug('Sending Connect.')\n        yield from stream.write_request(request)\n\n        _logger.debug('Read proxy response.')\n        response = yield from stream.read_response()\n\n        if response.status_code != 200:\n            debug_file = io.BytesIO()\n            _logger.debug('Read proxy response body.')\n            yield from stream.read_body(request, response, file=debug_file)\n\n            debug_file.seek(0)\n            _logger.debug(ascii(debug_file.read()))\n\n        if response.status_code == 200:\n            connection.tunneled = True\n        else:\n            raise NetworkError(\n                'Proxy does not support CONNECT: {} {}'\n                .format(response.status_code,\n                        wpull.string.printable_str(response.reason))\n            )", "response": "Establish a TCP tunnel. Coroutine."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_file(cls, file):\n        '''Return whether the file is likely to be HTML.'''\n        peeked_data = wpull.string.printable_bytes(\n            wpull.util.peek_file(file)).lower()\n\n        if b'<!doctype html' in peeked_data \\\n           or b'<head' in peeked_data \\\n           or b'<title' in peeked_data \\\n           or b'<html' in peeked_data \\\n           or b'<script' in peeked_data \\\n           or b'<table' in peeked_data \\\n           or b'<a href' in peeked_data:\n            return True", "response": "Return whether the file is likely to be HTML."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_http_request(request, referrer_host=None):\n    '''Convert a HTTP request.\n\n    Args:\n        request: An instance of :class:`.http.request.Request`.\n        referrer_host (str): The referrering hostname or IP address.\n\n    Returns:\n        Request: An instance of :class:`urllib.request.Request`\n    '''\n    new_request = urllib.request.Request(\n        request.url_info.url,\n        origin_req_host=referrer_host,\n    )\n\n    for name, value in request.fields.get_all():\n        new_request.add_header(name, value)\n\n    return new_request", "response": "Convert a HTTP request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef info(self):\n        '''Return the header fields as a Message:\n\n        Returns:\n            Message: An instance of :class:`email.message.Message`. If\n            Python 2, returns an instance of :class:`mimetools.Message`.\n        '''\n        if sys.version_info[0] == 2:\n            return mimetools.Message(io.StringIO(str(self._response.fields)))\n        else:\n            return email.message_from_string(str(self._response.fields))", "response": "Return the header fields as a Message."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps ``add_cookie_header``. Args: request: An instance of :class:`.http.request.Request`. referrer_host (str): An hostname or IP address of the referrer URL.", "response": "def add_cookie_header(self, request, referrer_host=None):\n        '''Wrapped ``add_cookie_header``.\n\n        Args:\n            request: An instance of :class:`.http.request.Request`.\n            referrer_host (str): An hostname or IP address of the referrer\n                URL.\n        '''\n        new_request = convert_http_request(request, referrer_host)\n        self._cookie_jar.add_cookie_header(new_request)\n\n        request.fields.clear()\n\n        for name, value in new_request.header_items():\n            request.fields.add(name, value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping ``extract_cookies``. Args: response: An instance of :class:`.http.request.Response`. request: An instance of :class:`.http.request.Request`. referrer_host (str): An hostname or IP address of the referrer URL.", "response": "def extract_cookies(self, response, request, referrer_host=None):\n        '''Wrapped ``extract_cookies``.\n\n        Args:\n            response: An instance of :class:`.http.request.Response`.\n            request: An instance of :class:`.http.request.Request`.\n            referrer_host (str): An hostname or IP address of the referrer\n                URL.\n        '''\n        new_response = HTTPResponseInfoWrapper(response)\n        new_request = convert_http_request(request, referrer_host)\n\n        self._cookie_jar.extract_cookies(new_response, new_request)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef close(self):\n        '''Save the cookie jar if needed.'''\n        if self._save_filename:\n            self._cookie_jar.save(\n                self._save_filename,\n                ignore_discard=self._keep_session_cookies\n            )", "response": "Save the cookie jar if needed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef start(self, use_atexit=True):\n        '''Start the executable.\n\n        Args:\n            use_atexit (bool): If True, the process will automatically be\n                terminated at exit.\n        '''\n        assert not self._process\n\n        _logger.debug('Starting process %s', self._proc_args)\n\n        process_future = asyncio.create_subprocess_exec(\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            *self._proc_args\n        )\n        self._process = yield from process_future\n\n        self._stderr_reader = asyncio.async(self._read_stderr())\n        self._stdout_reader = asyncio.async(self._read_stdout())\n\n        if use_atexit:\n            atexit.register(self.close)", "response": "Start the executable.\n\n        Args:\n            use_atexit (bool): If True, the process will automatically be\n                terminated at exit."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef close(self):\n        '''Terminate or kill the subprocess.\n\n        This function is blocking.\n        '''\n        if not self._process:\n            return\n\n        if self._process.returncode is not None:\n            return\n\n        _logger.debug('Terminate process.')\n\n        try:\n            self._process.terminate()\n        except OSError as error:\n            if error.errno != errno.ESRCH:\n                raise\n\n        for dummy in range(10):\n            if self._process.returncode is not None:\n                return\n\n            time.sleep(0.05)\n\n        _logger.debug('Failed to terminate. Killing.')\n\n        try:\n            self._process.kill()\n        except OSError as error:\n            if error.errno != errno.ESRCH:\n                raise", "response": "Terminate or kill the subprocess."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read_stdout(self):\n        '''Continuously read the stdout for messages.'''\n        try:\n            while self._process.returncode is None:\n                line = yield from self._process.stdout.readline()\n\n                _logger.debug('Read stdout line %s', repr(line))\n\n                if not line:\n                    break\n\n                if self._stdout_callback:\n                    yield from self._stdout_callback(line)\n\n        except Exception:\n            _logger.exception('Unhandled read stdout exception.')\n            raise", "response": "Continuously read the stdout for messages."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _read_stderr(self):\n        '''Continuously read stderr for error messages.'''\n        try:\n            while self._process.returncode is None:\n                line = yield from self._process.stderr.readline()\n\n                if not line:\n                    break\n\n                if self._stderr_callback:\n                    yield from self._stderr_callback(line)\n\n        except Exception:\n            _logger.exception('Unhandled read stderr exception.')\n            raise", "response": "Continuously read stderr for error messages."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading from connection to file.", "response": "def read_file(self, file: Union[IO, asyncio.StreamWriter]=None):\n        '''Read from connection to file.\n\n        Args:\n            file: A file object or a writer stream.\n        '''\n        if file:\n            file_is_async = hasattr(file, 'drain')\n\n        while True:\n            data = yield from self._connection.read(4096)\n\n            if not data:\n                break\n\n            if file:\n                file.write(data)\n\n                if file_is_async:\n                    yield from file.drain()\n\n            self._data_event_dispatcher.notify_read(data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreconnecting the connection if needed.", "response": "def reconnect(self):\n        '''Connected the stream if needed.\n\n        Coroutine.\n        '''\n        if self._connection.closed():\n            self._connection.reset()\n\n            yield from self._connection.connect()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_command(self, command: Command):\n        '''Write a command to the stream.\n\n        Args:\n            command: The command.\n\n        Coroutine.\n        '''\n        _logger.debug('Write command.')\n        data = command.to_bytes()\n        yield from self._connection.write(data)\n        self._data_event_dispatcher.notify_write(data)", "response": "Write a command to the stream."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a reply from the stream.", "response": "def read_reply(self) -> Reply:\n        '''Read a reply from the stream.\n\n        Returns:\n            .ftp.request.Reply: The reply\n\n        Coroutine.\n        '''\n        _logger.debug('Read reply')\n        reply = Reply()\n\n        while True:\n            line = yield from self._connection.readline()\n\n            if line[-1:] != b'\\n':\n                raise NetworkError('Connection closed.')\n\n            self._data_event_dispatcher.notify_read(line)\n            reply.parse(line)\n\n            if reply.code is not None:\n                break\n\n        return reply"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns whether the request can be fetched based on the pool.", "response": "def can_fetch_pool(self, request: Request):\n        '''Return whether the request can be fetched based on the pool.'''\n        url_info = request.url_info\n        user_agent = request.fields.get('User-agent', '')\n\n        if self._robots_txt_pool.has_parser(url_info):\n            return self._robots_txt_pool.can_fetch(url_info, user_agent)\n        else:\n            raise NotInPoolError()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfetching the robots. txt file for the request.", "response": "def fetch_robots_txt(self, request: Request, file=None):\n        '''Fetch the robots.txt file for the request.\n\n        Coroutine.\n        '''\n        url_info = request.url_info\n        url = URLInfo.parse('{0}://{1}/robots.txt'.format(\n            url_info.scheme, url_info.hostname_with_port)).url\n\n        if not file:\n            file = wpull.body.new_temp_file(os.getcwd(), hint='robots')\n\n        with contextlib.closing(file):\n            request = Request(url)\n\n            session = self._web_client.session(request)\n            while not session.done():\n                wpull.util.truncate_file(file.name)\n\n                try:\n                    response = yield from session.start()\n                    yield from session.download(file=file)\n                except ProtocolError:\n                    self._accept_as_blank(url_info)\n\n                    return\n\n            status_code = response.status_code\n\n            if 500 <= status_code <= 599:\n                raise ServerError('Server returned error for robots.txt.')\n\n            if status_code == 200:\n                self._read_content(response, url_info)\n            else:\n                self._accept_as_blank(url_info)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef can_fetch(self, request: Request, file=None) -> bool:\n        '''Return whether the request can fetched.\n\n        Args:\n            request: Request.\n            file: A file object to where the robots.txt contents are written.\n\n        Coroutine.\n        '''\n        try:\n            return self.can_fetch_pool(request)\n        except NotInPoolError:\n            pass\n\n        yield from self.fetch_robots_txt(request, file=file)\n\n        return self.can_fetch_pool(request)", "response": "Return whether the request can be fetched."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read_content(self, response: Response, original_url_info: URLInfo):\n        '''Read response and parse the contents into the pool.'''\n        data = response.body.read(4096)\n        url_info = original_url_info\n\n        try:\n            self._robots_txt_pool.load_robots_txt(url_info, data)\n        except ValueError:\n            _logger.warning(__(\n                _('Failed to parse {url} for robots exclusion rules. '\n                  'Ignoring.'), url_info.url))\n            self._accept_as_blank(url_info)\n        else:\n            _logger.debug(__('Got a good robots.txt for {0}.',\n                             url_info.url))", "response": "Read response and parse the contents into the pool."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmarks the URL as OK in the pool.", "response": "def _accept_as_blank(self, url_info: URLInfo):\n        '''Mark the URL as OK in the pool.'''\n        _logger.debug(__('Got empty robots.txt for {0}.', url_info.url))\n        self._robots_txt_pool.load_robots_txt(url_info, '')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_text_format(self):\n        '''Format as detached DNS information as text.'''\n        return '\\n'.join(itertools.chain(\n            (self.fetch_date.strftime('%Y%m%d%H%M%S'), ),\n            (rr.to_text() for rr in self.resource_records),\n            (),\n        ))", "response": "Format as detached DNS information as text."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rotate(self):\n        '''Move the first address to the last position.'''\n        item = self._address_infos.pop(0)\n        self._address_infos.append(item)", "response": "Move the first address to the last position."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nresolving hostname. Args: host: Hostname. Returns: Resolved IP addresses. Raises: DNSNotFound if the hostname could not be resolved or NetworkError if there was an error connecting to DNS servers. Coroutine.", "response": "def resolve(self, host: str) -> ResolveResult:\n        '''Resolve hostname.\n\n        Args:\n            host: Hostname.\n\n        Returns:\n            Resolved IP addresses.\n\n        Raises:\n            DNSNotFound if the hostname could not be resolved or\n            NetworkError if there was an error connecting to DNS servers.\n\n        Coroutine.\n        '''\n\n        _logger.debug(__('Lookup address {0}.', host))\n\n        try:\n            host = self.hook_dispatcher.call(PluginFunctions.resolve_dns, host\n                                             ) or host\n        except HookDisconnected:\n            pass\n\n        cache_key = (host, self._family)\n\n        if self._cache and cache_key in self._cache:\n            resolve_result = self._cache[cache_key]\n            _logger.debug(__('Return by cache {0}.', resolve_result))\n\n            if self._rotate:\n                resolve_result.rotate()\n\n            return resolve_result\n\n        address_infos = []\n        dns_infos = []\n\n        if not self.dns_python_enabled:\n            families = ()\n        elif self._family == IPFamilyPreference.any:\n            families = (socket.AF_INET, socket.AF_INET6)\n        elif self._family == IPFamilyPreference.ipv4_only:\n            families = (socket.AF_INET, )\n        else:\n            families = (socket.AF_INET6, )\n\n        for family in families:\n            datetime_now = datetime.datetime.utcnow()\n            try:\n                answer = yield from self._query_dns(host, family)\n            except DNSNotFound:\n                continue\n            else:\n                dns_infos.append(DNSInfo(datetime_now, answer.response.answer))\n                address_infos.extend(self._convert_dns_answer(answer))\n\n        if not address_infos:\n            # Maybe the address is defined in hosts file or mDNS\n\n            if self._family == IPFamilyPreference.any:\n                family = socket.AF_UNSPEC\n            elif self._family == IPFamilyPreference.ipv4_only:\n                family = socket.AF_INET\n            else:\n                family = socket.AF_INET6\n\n            results = yield from self._getaddrinfo(host, family)\n            address_infos.extend(self._convert_addrinfo(results))\n\n        _logger.debug(__('Resolved addresses: {0}.', address_infos))\n\n        resolve_result = ResolveResult(address_infos, dns_infos)\n\n        if self._cache:\n            self._cache[cache_key] = resolve_result\n\n        self.event_dispatcher.notify(PluginFunctions.resolve_dns_result, host, resolve_result)\n\n        if self._rotate:\n            resolve_result.shuffle()\n\n        return resolve_result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nquerying DNS using Python.", "response": "def _query_dns(self, host: str, family: int=socket.AF_INET) \\\n            -> dns.resolver.Answer:\n        '''Query DNS using Python.\n\n        Coroutine.\n        '''\n        record_type = {socket.AF_INET: 'A', socket.AF_INET6: 'AAAA'}[family]\n\n        event_loop = asyncio.get_event_loop()\n        query = functools.partial(\n            self._dns_resolver.query, host, record_type,\n            source=self._bind_address)\n\n        try:\n            answer = yield from event_loop.run_in_executor(None, query)\n        except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer) as error:\n            # dnspython doesn't raise an instance with a message, so use the\n            # class name instead.\n            raise DNSNotFound(\n                'DNS resolution failed: {error}'\n                .format(error=wpull.util.get_exception_message(error))\n            ) from error\n        except dns.exception.DNSException as error:\n            raise NetworkError(\n                'DNS resolution error: {error}'\n                .format(error=wpull.util.get_exception_message(error))\n            ) from error\n        else:\n            return answer"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nquery DNS using system resolver.", "response": "def _getaddrinfo(self, host: str, family: int=socket.AF_UNSPEC) \\\n            -> List[tuple]:\n        '''Query DNS using system resolver.\n\n        Coroutine.\n        '''\n        event_loop = asyncio.get_event_loop()\n        query = event_loop.getaddrinfo(host, 0, family=family,\n                                       proto=socket.IPPROTO_TCP)\n\n        if self._timeout:\n            query = asyncio.wait_for(query, self._timeout)\n\n        try:\n            results = yield from query\n        except socket.error as error:\n            if error.errno in (\n                    socket.EAI_FAIL,\n                    socket.EAI_NODATA,\n                    socket.EAI_NONAME):\n                raise DNSNotFound(\n                    'DNS resolution failed: {error}'.format(error=error)\n                ) from error\n            else:\n                raise NetworkError(\n                    'DNS resolution error: {error}'.format(error=error)\n                ) from error\n        except asyncio.TimeoutError as error:\n            raise NetworkError('DNS resolve timed out.') from error\n        else:\n            return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting the DNS answer to address info.", "response": "def _convert_dns_answer(cls, answer: dns.resolver.Answer) \\\n            -> Iterable[AddressInfo]:\n        '''Convert the DNS answer to address info.'''\n        assert answer.rdtype in (dns.rdatatype.A, dns.rdatatype.AAAA)\n\n        if answer.rdtype == dns.rdatatype.A:\n            family = socket.AF_INET\n        else:\n            family = socket.AF_INET6\n\n        for record in answer:\n            ip_address = record.to_text()\n\n            if family == socket.AF_INET6:\n                flow_info, control_id = cls._get_ipv6_info(ip_address)\n            else:\n                flow_info = control_id = None\n\n            yield AddressInfo(ip_address, family, flow_info, control_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts the result list to address info.", "response": "def _convert_addrinfo(cls, results: List[tuple]) -> Iterable[AddressInfo]:\n        '''Convert the result list to address info.'''\n        for result in results:\n            family = result[0]\n            address = result[4]\n            ip_address = address[0]\n\n            if family == socket.AF_INET6:\n                flow_info = address[2]\n                control_id = address[3]\n            else:\n                flow_info = None\n                control_id = None\n\n            yield AddressInfo(ip_address, family, flow_info, control_id)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract the flow info and control id.", "response": "def _get_ipv6_info(cls, ip_address: str) -> tuple:\n        '''Extract the flow info and control id.'''\n        results = socket.getaddrinfo(\n            ip_address, 0, proto=socket.IPPROTO_TCP,\n            flags=socket.AI_NUMERICHOST)\n\n        flow_info = results[0][4][2]\n        control_id = results[0][4][3]\n\n        return flow_info, control_id"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef raise_if_not_match(cls, action: str,\n                           expected_code: Union[int, Sequence[int]],\n                           reply: Reply):\n        '''Raise FTPServerError if not expected reply code.\n\n        Args:\n            action: Label to use in the exception message.\n            expected_code: Expected 3 digit code.\n            reply: Reply from the server.\n        '''\n        if isinstance(expected_code, int):\n            expected_codes = (expected_code,)\n        else:\n            expected_codes = expected_code\n\n        if reply.code not in expected_codes:\n            raise FTPServerError(\n                'Failed action {action}: {reply_code} {reply_text}'\n                .format(action=action, reply_code=reply.code,\n                        reply_text=ascii(reply.text)\n                        ),\n                reply.code\n                )", "response": "Raise FTPServerError if the reply code is not expected."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads the welcome message. Coroutine.", "response": "def read_welcome_message(self):\n        '''Read the welcome message.\n\n        Coroutine.\n        '''\n        reply = yield from self._control_stream.read_reply()\n\n        self.raise_if_not_match(\n            'Server ready', ReplyCodes.service_ready_for_new_user, reply)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef passive_mode(self) -> Tuple[str, int]:\n        '''Enable passive mode.\n\n        Returns:\n            The address (IP address, port) of the passive port.\n\n        Coroutine.\n        '''\n        yield from self._control_stream.write_command(Command('PASV'))\n\n        reply = yield from self._control_stream.read_reply()\n\n        self.raise_if_not_match(\n            'Passive mode', ReplyCodes.entering_passive_mode, reply)\n\n        try:\n            return wpull.protocol.ftp.util.parse_address(reply.text)\n        except ValueError as error:\n            raise ProtocolError(str(error)) from error", "response": "Enable passive mode.\n\n        Returns:\n            The address (IP address, port) of the passive port.\n\n        Coroutine."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate and setup a data stream. This function will set up passive mode and handle the next command.", "response": "def setup_data_stream(\n            self,\n            connection_factory: Callable[[tuple], Connection],\n            data_stream_factory: Callable[[Connection], DataStream]=DataStream) -> \\\n            DataStream:\n        '''Create and setup a data stream.\n\n        This function will set up passive and binary mode and handle\n        connecting to the data connection.\n\n        Args:\n            connection_factory: A coroutine callback that returns a connection\n            data_stream_factory: A callback that returns a data stream\n\n        Coroutine.\n\n        Returns:\n            DataStream\n        '''\n        yield from self._control_stream.write_command(Command('TYPE', 'I'))\n        reply = yield from self._control_stream.read_reply()\n\n        self.raise_if_not_match('Binary mode', ReplyCodes.command_okay, reply)\n\n        address = yield from self.passive_mode()\n\n        connection = yield from connection_factory(address)\n\n        # TODO: unit test for following line for connections that have\n        # the same port over time but within pool cleaning intervals\n        connection.reset()\n\n        yield from connection.connect()\n\n        data_stream = data_stream_factory(connection)\n\n        return data_stream"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef begin_stream(self, command: Command) -> Reply:\n        '''Start sending content on the data stream.\n\n        Args:\n            command: A command that tells the server to send data over the\n            data connection.\n\n        Coroutine.\n\n        Returns:\n            The begin reply.\n        '''\n        yield from self._control_stream.write_command(command)\n        reply = yield from self._control_stream.read_reply()\n\n        self.raise_if_not_match(\n            'Begin stream',\n            (\n                ReplyCodes.file_status_okay_about_to_open_data_connection,\n                ReplyCodes.data_connection_already_open_transfer_starting,\n            ),\n            reply\n        )\n\n        return reply", "response": "Start sending content on the data stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads from the data stream.", "response": "def read_stream(self, file: IO, data_stream: DataStream) -> Reply:\n        '''Read from the data stream.\n\n        Args:\n            file: A destination file object or a stream writer.\n            data_stream: The stream of which to read from.\n\n        Coroutine.\n\n        Returns:\n            Reply: The final reply.\n        '''\n\n        yield from data_stream.read_file(file=file)\n\n        reply = yield from self._control_stream.read_reply()\n\n        self.raise_if_not_match(\n            'End stream',\n            ReplyCodes.closing_data_connection,\n            reply\n        )\n\n        data_stream.close()\n\n        return reply"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget size of a file.", "response": "def size(self, filename: str) -> int:\n        '''Get size of file.\n\n        Coroutine.\n        '''\n        yield from self._control_stream.write_command(Command('SIZE', filename))\n\n        reply = yield from self._control_stream.read_reply()\n\n        self.raise_if_not_match('File size', ReplyCodes.file_status, reply)\n\n        try:\n            return int(reply.text.strip())\n        except ValueError:\n            return"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef restart(self, offset: int):\n        '''Send restart command.\n\n        Coroutine.\n        '''\n        yield from self._control_stream.write_command(Command('REST', str(offset)))\n\n        reply = yield from self._control_stream.read_reply()\n\n        self.raise_if_not_match('Restart', ReplyCodes.requested_file_action_pending_further_information, reply)", "response": "Send restart command. Coroutine."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the version string of youtube - dl.", "response": "def get_version(exe_path='youtube-dl'):\n    '''Get the version string of youtube-dl.'''\n    process = subprocess.Popen(\n        [exe_path, '--version'],\n        stdout=subprocess.PIPE\n    )\n    version_string = process.communicate()[0]\n    version_string = version_string.decode().strip()\n\n    assert ' ' not in version_string, version_string\n\n    return version_string"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the path prefix and output template.", "response": "def _get_output_template(self):\n        '''Return the path prefix and output template.'''\n        path = self._file_writer_session.extra_resource_path('.youtube-dl')\n\n        if not path:\n            self._temp_dir = tempfile.TemporaryDirectory(\n                dir=self._root_path, prefix='tmp-wpull-youtubedl'\n            )\n            path = '{}/tmp'.format(self._temp_dir.name)\n\n        return path, '{}.%(id)s.%(format_id)s.%(ext)s'.format(path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites the JSON metadata to WARC.", "response": "def _write_warc_metadata(self):\n        '''Write the JSON metadata to WARC.\n\n        Uses pywb spec.\n        '''\n        uri = 'metadata://{}{}'.format(self._item_session.url_record.url_info.authority,\n                                       self._item_session.url_record.url_info.resource)\n\n        glob_pattern = self._path_prefix + '*.info.json'\n        filenames = list(glob.glob(glob_pattern))\n\n        if not filenames:\n            _logger.warning(__(\n                _('Could not find external process metadata file: {filename}'),\n                filename=glob_pattern\n            ))\n            return\n\n        for filename in filenames:\n            record = WARCRecord()\n            record.set_common_fields('metadata', 'application/vnd.youtube-dl_formats+json')\n            record.fields['WARC-Target-URI'] = uri\n            record.block_file = open(filename, 'rb')\n\n            self._warc_recorder.set_length_and_maybe_checksums(record)\n            self._warc_recorder.write_record(record)\n\n            record.block_file.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npopulate the visits from the CDX into the URL table.", "response": "def process(self, session: AppSession):\n        '''Populate the visits from the CDX into the URL table.'''\n        if not session.args.warc_dedup:\n            return\n\n        iterable = wpull.warc.format.read_cdx(\n            session.args.warc_dedup,\n            encoding=session.args.local_encoding or 'utf-8'\n        )\n\n        missing_url_msg = _('The URL (\"a\") is missing from the CDX file.')\n        missing_id_msg = _('The record ID (\"u\") is missing from the CDX file.')\n        missing_checksum_msg = \\\n            _('The SHA1 checksum (\"k\") is missing from the CDX file.')\n\n        counter = 0\n\n        def visits():\n            nonlocal counter\n            checked_fields = False\n\n            for record in iterable:\n                if not checked_fields:\n                    if 'a' not in record:\n                        raise ValueError(missing_url_msg)\n                    if 'u' not in record:\n                        raise ValueError(missing_id_msg)\n                    if 'k' not in record:\n                        raise ValueError(missing_checksum_msg)\n\n                    checked_fields = True\n\n                yield record['a'], record['u'], record['k']\n                counter += 1\n\n        url_table = session.factory['URLTable']\n        url_table.add_visits(visits())\n\n        _logger.info(__(\n            gettext.ngettext(\n                'Loaded {num} record from CDX file.',\n                'Loaded {num} records from CDX file.',\n                counter\n            ),\n            num=counter\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if lxml supports the specified encoding. Returns: str None", "response": "def to_lxml_encoding(encoding):\n    '''Check if lxml supports the specified encoding.\n\n    Returns:\n        str, None\n    '''\n    # XXX: Workaround lxml not liking utf-16-le\n    try:\n        lxml.html.HTMLParser(encoding=encoding)\n    except LookupError:\n        encoding = encoding.replace('-', '')\n    else:\n        return encoding\n    try:\n        lxml.html.HTMLParser(encoding=encoding)\n    except LookupError:\n        encoding = encoding.replace('_', '')\n    else:\n        return encoding\n\n    try:\n        lxml.html.HTMLParser(encoding=encoding)\n    except LookupError:\n        pass\n    else:\n        return encoding"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the contents of a file into an iterator of elements.", "response": "def parse_lxml(self, file, encoding=None, target_class=HTMLParserTarget,\n                   parser_type='html'):\n        '''Return an iterator of elements found in the document.\n\n        Args:\n            file: A file object containing the document.\n            encoding (str): The encoding of the document.\n            target_class: A class to be used for target parsing.\n            parser_type (str): The type of parser to use. Accepted values:\n                ``html``, ``xhtml``, ``xml``.\n\n        Returns:\n            iterator: Each item is an element from\n            :mod:`.document.htmlparse.element`\n        '''\n        if encoding:\n            lxml_encoding = to_lxml_encoding(encoding) or 'latin1'\n        else:\n            lxml_encoding = encoding\n\n        elements = []\n\n        callback_func = elements.append\n\n        target = target_class(callback_func)\n\n        if parser_type == 'html':\n            parser = lxml.html.HTMLParser(\n                encoding=lxml_encoding, target=target\n            )\n        elif parser_type == 'xhtml':\n            parser = lxml.html.XHTMLParser(\n                encoding=lxml_encoding, target=target, recover=True\n            )\n        else:\n            parser = lxml.etree.XMLParser(\n                encoding=lxml_encoding, target=target, recover=True\n            )\n\n        if parser_type == 'html':\n            # XXX: Force libxml2 to do full read in case of early \"</html>\"\n            # See https://github.com/chfoo/wpull/issues/104\n            # See https://bugzilla.gnome.org/show_bug.cgi?id=727935\n            for dummy in range(3):\n                parser.feed('<html>'.encode(encoding))\n\n        while True:\n            data = file.read(self.BUFFER_SIZE)\n\n            if not data:\n                break\n\n            parser.feed(data)\n\n            for element in elements:\n                yield element\n\n            del elements[:]\n\n        parser.close()\n\n        for element in elements:\n            yield element"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_doctype(cls, file, encoding=None):\n        '''Get the doctype from the document.\n\n        Returns:\n            str, None\n        '''\n        if encoding:\n            lxml_encoding = to_lxml_encoding(encoding) or 'latin1'\n        else:\n            lxml_encoding = encoding\n\n        try:\n            parser = lxml.etree.XMLParser(encoding=lxml_encoding, recover=True)\n            tree = lxml.etree.parse(\n                io.BytesIO(wpull.util.peek_file(file)), parser=parser\n            )\n            if tree.getroot() is not None:\n                return tree.docinfo.doctype\n        except lxml.etree.LxmlError:\n            pass", "response": "Get the doctype from the document."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetecting the suitable parser type for the document.", "response": "def detect_parser_type(cls, file, encoding=None):\n        '''Get the suitable parser type for the document.\n\n        Returns:\n            str\n        '''\n        is_xml = XMLDetector.is_file(file)\n        doctype = cls.parse_doctype(file, encoding=encoding) or ''\n\n        if not doctype and is_xml:\n            return 'xml'\n\n        if 'XHTML' in doctype:\n            return 'xhtml'\n\n        return 'html'"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new_temp_file(directory=None, hint=''):\n    '''Return a new temporary file.'''\n    return tempfile.NamedTemporaryFile(\n        prefix='tmp-wpull-{0}-'.format(hint), suffix='.tmp', dir=directory)", "response": "Return a new temporary file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef content(self):\n        '''Return the content of the file.\n\n        If this function is invoked, the contents of the entire file is read\n        and cached.\n\n        Returns:\n            ``bytes``: The entire content of the file.\n        '''\n        if not self._content_data:\n            if is_seekable(self.file):\n                with wpull.util.reset_file_offset(self.file):\n                    self._content_data = self.file.read()\n            else:\n                self._content_data = self.file.read()\n\n        return self._content_data", "response": "Return the contents of the file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the size of the file.", "response": "def size(self):\n        '''Return the size of the file.'''\n        try:\n            return os.fstat(self.file.fileno()).st_size\n        except io.UnsupportedOperation:\n            pass\n\n        if is_seekable(self.file):\n            with wpull.util.reset_file_offset(self.file):\n                self.file.seek(0, os.SEEK_END)\n                return self.file.tell()\n\n        raise OSError('Unsupported operation.')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting the body to a dict.", "response": "def to_dict(self):\n        '''Convert the body to a :class:`dict`.\n\n        Returns:\n            dict: The items are:\n\n                * ``filename`` (string, None): The path of the file.\n                * ``length`` (int, None): The size of the file.\n        '''\n        try:\n            name = self.file.name\n        except AttributeError:\n            name = None\n\n        try:\n            size = self.size()\n        except OSError:\n            size = None\n\n        return {\n            'filename': name,\n            'length': size,\n            'content_size': size,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _session(self):\n        # Taken from the session docs.\n        session = self._session_maker()\n        try:\n            yield session\n            session.commit()\n        except:\n            session.rollback()\n            raise\n        finally:\n            session.close()", "response": "Provide a transactional scope around a series of operations."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _apply_pragmas_callback(cls, connection, record):\n        '''Set SQLite pragmas.\n\n        Write-ahead logging, synchronous=NORMAL is used.\n        '''\n        _logger.debug('Setting pragmas.')\n        connection.execute('PRAGMA journal_mode=WAL')\n        connection.execute('PRAGMA synchronous=NORMAL')", "response": "Set SQLite pragmas.\n\n        Write-ahead logging, synchronous=NORMAL is used."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn whether a parser has been created for the URL.", "response": "def has_parser(self, url_info: URLInfo):\n        '''Return whether a parser has been created for the URL.'''\n        key = self.url_info_key(url_info)\n        return key in self._parsers"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef can_fetch(self, url_info: URLInfo, user_agent: str):\n        '''Return whether the URL can be fetched.'''\n        key = self.url_info_key(url_info)\n\n        parser = self._parsers[key]\n        return parser.is_allowed(user_agent, url_info.url)", "response": "Return whether the URL can be fetched."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_robots_txt(self, url_info: URLInfo, text: str):\n        '''Load the robot.txt file.'''\n        key = self.url_info_key(url_info)\n        parser = robotexclusionrulesparser.RobotExclusionRulesParser()\n        parser.parse(text)\n\n        self._parsers[key] = parser", "response": "Load the robot. txt file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate demux document scraper.", "response": "def _build_demux_document_scraper(cls, session: AppSession):\n        '''Create demux document scraper.'''\n        session.factory.new(\n            'DemuxDocumentScraper', cls._build_document_scrapers(session))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _build_document_scrapers(cls, session: AppSession):\n        '''Create the document scrapers.\n\n        Returns:\n            A list of document scrapers\n        '''\n        html_parser = session.factory['HTMLParser']\n        element_walker = session.factory.new('ElementWalker')\n\n        scrapers = [\n            session.factory.new(\n                'HTMLScraper',\n                html_parser,\n                element_walker,\n                followed_tags=session.args.follow_tags,\n                ignored_tags=session.args.ignore_tags,\n                only_relative=session.args.relative,\n                robots=session.args.robots,\n                encoding_override=session.args.remote_encoding,\n            ),\n        ]\n\n        if 'css' in session.args.link_extractors:\n            css_scraper = session.factory.new(\n                'CSSScraper',\n                encoding_override=session.args.remote_encoding,\n            )\n            scrapers.append(css_scraper)\n            element_walker.css_scraper = css_scraper\n\n        if 'javascript' in session.args.link_extractors:\n            javascript_scraper = session.factory.new(\n                'JavaScriptScraper',\n                encoding_override=session.args.remote_encoding,\n            )\n            scrapers.append(javascript_scraper)\n            element_walker.javascript_scraper = javascript_scraper\n\n        if session.args.sitemaps:\n            scrapers.append(session.factory.new(\n                'SitemapScraper', html_parser,\n                encoding_override=session.args.remote_encoding,\n            ))\n\n        return scrapers", "response": "Create the document scrapers.\n            Returns: A list of document scrapers."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate the request factory.", "response": "def _build_request_factory(cls, session: AppSession):\n        '''Create the request factory.\n\n        A request factory is any callable object that returns a\n        :class:`.http.Request`. The callable must accept the same\n        arguments to Request.\n\n        Returns:\n            A callable object\n        '''\n        def request_factory(*args, **kwargs):\n            request = session.factory.class_map['Request'](*args, **kwargs)\n\n            user_agent = session.args.user_agent or session.default_user_agent\n\n            request.fields['User-Agent'] = user_agent\n\n            if session.args.referer:\n                request.fields['Referer'] = session.args.referer\n\n            for header_string in session.args.header:\n                request.fields.parse(header_string)\n\n            if session.args.http_compression:\n                request.fields['Accept-Encoding'] = 'gzip, deflate'\n\n            if session.args.no_cache:\n                request.fields['Cache-Control'] = 'no-cache, must-revalidate'\n                request.fields['Pragma'] = 'no-cache'\n\n            return request\n\n        return request_factory"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate the HTTP client.", "response": "def _build_http_client(cls, session: AppSession):\n        '''Create the HTTP client.\n\n        Returns:\n            Client: An instance of :class:`.http.Client`.\n        '''\n        # TODO:\n        # recorder = self._build_recorder()\n\n        stream_factory = functools.partial(\n            HTTPStream,\n            ignore_length=session.args.ignore_length,\n            keep_alive=session.args.http_keep_alive)\n\n        return session.factory.new(\n            'HTTPClient',\n            connection_pool=session.factory['ConnectionPool'],\n            stream_factory=stream_factory\n         )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _build_cookie_jar(cls, session: AppSession):\n        '''Build the cookie jar'''\n\n        if not session.args.cookies:\n            return\n\n        if session.args.load_cookies or session.args.save_cookies:\n            session.factory.set('CookieJar', BetterMozillaCookieJar)\n\n            cookie_jar = session.factory.new('CookieJar')\n\n            if session.args.load_cookies:\n                cookie_jar.load(session.args.load_cookies, ignore_discard=True)\n        else:\n            cookie_jar = session.factory.new('CookieJar')\n\n        policy = session.factory.new('CookiePolicy', cookie_jar=cookie_jar)\n\n        cookie_jar.set_policy(policy)\n\n        _logger.debug(__('Loaded cookies: {0}', list(cookie_jar)))\n\n        cookie_jar_wrapper = session.factory.new(\n            'CookieJarWrapper',\n            cookie_jar,\n            save_filename=session.args.save_cookies,\n            keep_session_cookies=session.args.keep_session_cookies,\n        )\n\n        return cookie_jar_wrapper", "response": "Build the cookie jar"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process(self, session: AppSession):\n        '''Build MITM proxy server.'''\n        args = session.args\n        if not (args.phantomjs or args.youtube_dl or args.proxy_server):\n            return\n\n        proxy_server = session.factory.new(\n            'HTTPProxyServer',\n            session.factory['HTTPClient'],\n        )\n\n        cookie_jar = session.factory.get('CookieJarWrapper')\n        proxy_coprocessor = session.factory.new(\n            'ProxyCoprocessor',\n            session\n        )\n\n        proxy_socket = tornado.netutil.bind_sockets(\n            session.args.proxy_server_port,\n            address=session.args.proxy_server_address\n        )[0]\n        proxy_port = proxy_socket.getsockname()[1]\n\n        proxy_async_server = yield from asyncio.start_server(proxy_server, sock=proxy_socket)\n\n        session.async_servers.append(proxy_async_server)\n        session.proxy_server_port = proxy_port", "response": "Build MITM proxy server."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates the Processor Processor", "response": "def _build_processor(cls, session: AppSession):\n        '''Create the Processor\n\n        Returns:\n            Processor: An instance of :class:`.processor.BaseProcessor`.\n        '''\n        web_processor = cls._build_web_processor(session)\n        ftp_processor = cls._build_ftp_processor(session)\n        delegate_processor = session.factory.new('Processor')\n\n        delegate_processor.register('http', web_processor)\n        delegate_processor.register('https', web_processor)\n        delegate_processor.register('ftp', ftp_processor)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the post data.", "response": "def _get_post_data(cls, args):\n        '''Return the post data.'''\n        if args.post_data:\n            return args.post_data\n        elif args.post_file:\n            return args.post_file.read()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild robots. txt checker.", "response": "def _build_robots_txt_checker(cls, session: AppSession):\n        '''Build robots.txt checker.'''\n        if session.args.robots:\n            robots_txt_pool = session.factory.new('RobotsTxtPool')\n            robots_txt_checker = session.factory.new(\n                'RobotsTxtChecker',\n                web_client=session.factory['WebClient'],\n                robots_txt_pool=robots_txt_pool\n            )\n\n            return robots_txt_checker"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild proxy server and PhantomJS client. controller coprocessor.", "response": "def _build_phantomjs_coprocessor(cls, session: AppSession, proxy_port: int):\n        '''Build proxy server and PhantomJS client. controller, coprocessor.'''\n        page_settings = {}\n        default_headers = NameValueRecord()\n\n        for header_string in session.args.header:\n            default_headers.parse(header_string)\n\n        # Since we can only pass a one-to-one mapping to PhantomJS,\n        # we put these last since NameValueRecord.items() will use only the\n        # first value added for each key.\n        default_headers.add('Accept-Language', '*')\n\n        if not session.args.http_compression:\n            default_headers.add('Accept-Encoding', 'identity')\n\n        default_headers = dict(default_headers.items())\n\n        if session.args.read_timeout:\n            page_settings['resourceTimeout'] = session.args.read_timeout * 1000\n\n        page_settings['userAgent'] = session.args.user_agent \\\n                                     or session.default_user_agent\n\n        # Test early for executable\n        wpull.driver.phantomjs.get_version(session.args.phantomjs_exe)\n\n        phantomjs_params = PhantomJSParams(\n            wait_time=session.args.phantomjs_wait,\n            num_scrolls=session.args.phantomjs_scroll,\n            smart_scroll=session.args.phantomjs_smart_scroll,\n            snapshot=session.args.phantomjs_snapshot,\n            custom_headers=default_headers,\n            page_settings=page_settings,\n            load_time=session.args.phantomjs_max_time,\n        )\n\n        extra_args = [\n            '--proxy',\n            '{}:{}'.format(session.args.proxy_server_address, proxy_port),\n            '--ignore-ssl-errors=true'\n        ]\n\n        phantomjs_driver_factory = functools.partial(\n            session.factory.class_map['PhantomJSDriver'],\n            exe_path=session.args.phantomjs_exe,\n            extra_args=extra_args,\n        )\n\n        phantomjs_coprocessor = session.factory.new(\n            'PhantomJSCoprocessor',\n            phantomjs_driver_factory,\n            session.factory['ProcessingRule'],\n            phantomjs_params,\n            root_path=session.args.directory_prefix,\n            warc_recorder=session.factory.get('WARCRecorder'),\n        )\n\n        return phantomjs_coprocessor"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild youtube - dl coprocessor.", "response": "def _build_youtube_dl_coprocessor(cls, session: AppSession, proxy_port: int):\n        '''Build youtube-dl coprocessor.'''\n\n        # Test early for executable\n        wpull.processor.coprocessor.youtubedl.get_version(session.args.youtube_dl_exe)\n\n        coprocessor = session.factory.new(\n            'YoutubeDlCoprocessor',\n            session.args.youtube_dl_exe,\n            (session.args.proxy_server_address, proxy_port),\n            root_path=session.args.directory_prefix,\n            user_agent=session.args.user_agent or session.default_user_agent,\n            warc_recorder=session.factory.get('WARCRecorder'),\n            inet_family=session.args.inet_family,\n            # Proxy will always present a invalid MITM cert\n            #check_certificate=session.args.check_certificate\n            check_certificate=False\n        )\n\n        return coprocessor"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nputs the application together.", "response": "def build(self) -> Application:\n        '''Put the application together.\n        '''\n\n        pipelines = self._build_pipelines()\n        self._factory.new('Application', pipelines)\n\n        return self._factory['Application']"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives the hints return whether the document is supported.", "response": "def is_supported(cls, file=None, request=None, response=None,\n                     url_info=None):\n        '''Given the hints, return whether the document is supported.\n\n        Args:\n            file: A file object containing the document.\n            request (:class:`.http.request.Request`): An HTTP request.\n            response (:class:`.http.request.Response`): An HTTP response.\n            url_info (:class:`.url.URLInfo`): A URLInfo.\n\n        Returns:\n            bool: If True, the reader should be able to read it.\n        '''\n        tests = (\n            (response, cls.is_response),\n            (file, cls.is_file),\n            (request, cls.is_request),\n            (url_info, cls.is_url)\n        )\n\n        for instance, method in tests:\n            if instance:\n                try:\n                    result = method(instance)\n                except NotImplementedError:\n                    pass\n                else:\n                    if result:\n                        return True\n                    elif result is VeryFalse:\n                        return VeryFalse"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the links. This function is a convenience function for calling :meth:`iter_text` and returning only the links.", "response": "def iter_links(self, file, encoding=None, context=False):\n        '''Return the links.\n\n        This function is a convenience function for calling :meth:`iter_text`\n        and returning only the links.\n        '''\n        if context:\n            return [item for item in self.iter_text(file, encoding) if item[1]]\n        else:\n            return [item[0] for item in self.iter_text(file, encoding) if item[1]]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlogs the final statistics to the user.", "response": "def _print_stats(cls, stats: Statistics, human_format_speed: bool=True):\n        '''Log the final statistics to the user.'''\n        time_length = datetime.timedelta(\n            seconds=int(stats.stop_time - stats.start_time)\n        )\n        file_size = wpull.string.format_size(stats.size)\n\n        if stats.bandwidth_meter.num_samples:\n            speed = stats.bandwidth_meter.speed()\n\n            if human_format_speed:\n                speed_size_str = wpull.string.format_size(speed)\n            else:\n                speed_size_str = '{:.1f} b'.format(speed * 8)\n        else:\n            speed_size_str = _('-- B')\n\n        _logger.info(_('FINISHED.'))\n        _logger.info(__(\n            _(\n                'Duration: {preformatted_timedelta}. '\n                'Speed: {preformatted_speed_size}/s.'\n            ),\n            preformatted_timedelta=time_length,\n            preformatted_speed_size=speed_size_str,\n        ))\n        _logger.info(__(\n            gettext.ngettext(\n                'Downloaded: {num_files} file, {preformatted_file_size}.',\n                'Downloaded: {num_files} files, {preformatted_file_size}.',\n                stats.files\n            ),\n            num_files=stats.files,\n            preformatted_file_size=file_size\n        ))\n\n        if stats.is_quota_exceeded:\n            _logger.info(_('Download quota exceeded.'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns whether a content body is not expected.", "response": "def is_no_body(request, response, no_content_codes=DEFAULT_NO_CONTENT_CODES):\n    '''Return whether a content body is not expected.'''\n    if 'Content-Length' not in response.fields \\\n            and 'Transfer-Encoding' not in response.fields \\\n            and (\n                response.status_code in no_content_codes\n                or request.method.upper() == 'HEAD'\n            ):\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_request(self, request, full_url=False):\n        '''Send the request's HTTP status line and header fields.\n\n        This class will automatically connect the connection if the\n        connection is closed.\n\n        Coroutine.\n        '''\n        _logger.debug('Sending headers.')\n\n        if hasattr(request, 'prepare_for_send'):\n            request.prepare_for_send(full_url=full_url)\n\n        if self._ignore_length:\n            request.fields['Connection'] = 'close'\n\n        data = request.to_bytes()\n\n        self._data_event_dispatcher.notify_write(data)\n\n        # XXX: Connection lost is raised too early on Python 3.2, 3.3 so\n        # don't flush but check for connection closed on reads\n        yield from self._connection.write(data, drain=False)", "response": "Send the request s HTTP status line and header fields."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_body(self, file, length=None):\n        '''Send the request's content body.\n\n        Coroutine.\n        '''\n        _logger.debug('Sending body.')\n\n        file_is_async = (asyncio.iscoroutine(file.read) or\n                         asyncio.iscoroutinefunction(file.read))\n\n        _logger.debug(__('Body is async: {0}', file_is_async))\n\n        if length is not None:\n            bytes_left = length\n\n        while True:\n            if length is not None:\n                if bytes_left <= 0:\n                    break\n                read_size = min(bytes_left, self._read_size)\n            else:\n                read_size = self._read_size\n\n            if file_is_async:\n                data = yield from file.read(read_size)\n            else:\n                data = file.read(read_size)\n\n            if not data:\n                break\n\n            self._data_event_dispatcher.notify_write(data)\n\n            if bytes_left <= self._read_size:\n                # XXX: Connection lost is raised too early on Python 3.2, 3.3\n                # so don't flush on the last chunk but check for connection\n                # closed on reads\n                drain = False\n            else:\n                drain = True\n\n            yield from self._connection.write(data, drain=drain)\n\n            if length is not None:\n                bytes_left -= len(data)", "response": "Send the request s content body."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading the response s HTTP status line and header fields. Coroutine.", "response": "def read_response(self, response=None):\n        '''Read the response's HTTP status line and header fields.\n\n        Coroutine.\n        '''\n        _logger.debug('Reading header.')\n\n        if response is None:\n            response = Response()\n\n        header_lines = []\n        bytes_read = 0\n\n        while True:\n            try:\n                data = yield from self._connection.readline()\n            except ValueError as error:\n                raise ProtocolError(\n                    'Invalid header: {0}'.format(error)) from error\n\n            self._data_event_dispatcher.notify_read(data)\n\n            if not data.endswith(b'\\n'):\n                raise NetworkError('Connection closed.')\n            elif data in (b'\\r\\n', b'\\n'):\n                break\n\n            header_lines.append(data)\n            assert data.endswith(b'\\n')\n\n            bytes_read += len(data)\n\n            if bytes_read > 32768:\n                raise ProtocolError('Header too big.')\n\n        if not header_lines:\n            raise ProtocolError('No header received.')\n\n        response.parse(b''.join(header_lines))\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the response s content body. Coroutine.", "response": "def read_body(self, request, response, file=None, raw=False):\n        '''Read the response's content body.\n\n        Coroutine.\n        '''\n        if is_no_body(request, response):\n            return\n\n        if not raw:\n            self._setup_decompressor(response)\n\n        read_strategy = self.get_read_strategy(response)\n\n        if self._ignore_length and read_strategy == 'length':\n            read_strategy = 'close'\n\n        if read_strategy == 'chunked':\n            yield from self._read_body_by_chunk(response, file, raw=raw)\n        elif read_strategy == 'length':\n            yield from self._read_body_by_length(response, file)\n        else:\n            yield from self._read_body_until_close(response, file)\n\n        should_close = wpull.protocol.http.util.should_close(\n            request.version, response.fields.get('Connection'))\n\n        if not self._keep_alive or should_close:\n            _logger.debug('Not keep-alive. Closing connection.')\n            self.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_body_until_close(self, response, file):\n        '''Read the response until the connection closes.\n\n        Coroutine.\n        '''\n        _logger.debug('Reading body until close.')\n\n        file_is_async = hasattr(file, 'drain')\n\n        while True:\n            data = yield from self._connection.read(self._read_size)\n\n            if not data:\n                break\n\n            self._data_event_dispatcher.notify_read(data)\n\n            content_data = self._decompress_data(data)\n\n            if file:\n                file.write(content_data)\n\n                if file_is_async:\n                    yield from file.drain()\n\n        content_data = self._flush_decompressor()\n\n        if file:\n            file.write(content_data)\n\n            if file_is_async:\n                yield from file.drain()", "response": "Read the response until the connection closes."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads the connection specified by a length.", "response": "def _read_body_by_length(self, response, file):\n        '''Read the connection specified by a length.\n\n        Coroutine.\n        '''\n        _logger.debug('Reading body by length.')\n\n        file_is_async = hasattr(file, 'drain')\n\n        try:\n            body_size = int(response.fields['Content-Length'])\n\n            if body_size < 0:\n                raise ValueError('Content length cannot be negative.')\n\n        except ValueError as error:\n            _logger.warning(__(\n                _('Invalid content length: {error}'), error=error\n            ))\n\n            yield from self._read_body_until_close(response, file)\n            return\n\n        bytes_left = body_size\n\n        while bytes_left > 0:\n            data = yield from self._connection.read(self._read_size)\n\n            if not data:\n                break\n\n            bytes_left -= len(data)\n\n            if bytes_left < 0:\n                data = data[:bytes_left]\n\n                _logger.warning(_('Content overrun.'))\n                self.close()\n\n            self._data_event_dispatcher.notify_read(data)\n\n            content_data = self._decompress_data(data)\n\n            if file:\n                file.write(content_data)\n\n                if file_is_async:\n                    yield from file.drain()\n\n        if bytes_left > 0:\n            raise NetworkError('Connection closed.')\n\n        content_data = self._flush_decompressor()\n\n        if file and content_data:\n            file.write(content_data)\n\n            if file_is_async:\n                yield from file.drain()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read_body_by_chunk(self, response, file, raw=False):\n        '''Read the connection using chunked transfer encoding.\n\n        Coroutine.\n        '''\n        reader = ChunkedTransferReader(self._connection)\n\n        file_is_async = hasattr(file, 'drain')\n\n        while True:\n            chunk_size, data = yield from reader.read_chunk_header()\n\n            self._data_event_dispatcher.notify_read(data)\n            if raw:\n                file.write(data)\n\n            if not chunk_size:\n                break\n\n            while True:\n                content, data = yield from reader.read_chunk_body()\n\n                self._data_event_dispatcher.notify_read(data)\n\n                if not content:\n                    if raw:\n                        file.write(data)\n\n                    break\n\n                content = self._decompress_data(content)\n\n                if file:\n                    file.write(content)\n\n                    if file_is_async:\n                        yield from file.drain()\n\n        content = self._flush_decompressor()\n\n        if file:\n            file.write(content)\n\n            if file_is_async:\n                yield from file.drain()\n\n        trailer_data = yield from reader.read_trailer()\n\n        self._data_event_dispatcher.notify_read(trailer_data)\n\n        if file and raw:\n            file.write(trailer_data)\n\n            if file_is_async:\n                yield from file.drain()\n\n        response.fields.parse(trailer_data)", "response": "Read the connection using chunked transfer encoding."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the appropriate algorithm of reading response. is chunked length or close.", "response": "def get_read_strategy(cls, response):\n        '''Return the appropriate algorithm of reading response.\n\n        Returns:\n            str: ``chunked``, ``length``, ``close``.\n        '''\n        chunked_match = re.match(\n            r'chunked($|;)',\n            response.fields.get('Transfer-Encoding', '')\n        )\n\n        if chunked_match:\n            return 'chunked'\n        elif 'Content-Length' in response.fields:\n            return 'length'\n        else:\n            return 'close'"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset up the content encoding decompressor.", "response": "def _setup_decompressor(self, response):\n        '''Set up the content encoding decompressor.'''\n        encoding = response.fields.get('Content-Encoding', '').lower()\n\n        if encoding == 'gzip':\n            self._decompressor = wpull.decompression.GzipDecompressor()\n        elif encoding == 'deflate':\n            self._decompressor = wpull.decompression.DeflateDecompressor()\n        else:\n            self._decompressor = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _decompress_data(self, data):\n        '''Decompress the given data and return the uncompressed data.'''\n        if self._decompressor:\n            try:\n                return self._decompressor.decompress(data)\n            except zlib.error as error:\n                raise ProtocolError(\n                    'zlib error: {0}.'.format(error)\n                ) from error\n        else:\n            return data", "response": "Decompress the given data and return the uncompressed data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _flush_decompressor(self):\n        '''Return any data left in the decompressor.'''\n        if self._decompressor:\n            try:\n                return self._decompressor.flush()\n            except zlib.error as error:\n                raise ProtocolError(\n                    'zlib flush error: {0}.'.format(error)\n                ) from error\n        else:\n            return b''", "response": "Return any data left in the decompressor."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef gzip_uncompress(data, truncated=False):\n    '''Uncompress gzip data.\n\n    Args:\n        data (bytes): The gzip data.\n        truncated (bool): If True, the decompressor is not flushed.\n\n    This is a convenience function.\n\n    Returns:\n        bytes: The inflated data.\n\n    Raises:\n        zlib.error\n    '''\n    decompressor = SimpleGzipDecompressor()\n    inflated_data = decompressor.decompress(data)\n\n    if not truncated:\n        inflated_data += decompressor.flush()\n\n    return inflated_data", "response": "Uncompress gzip data.\n\n    Args:\n        data (bytes): The gzip data.\n        truncated (bool): If True, the decompressor is not flushed.\n\n    This is a convenience function.\n\n    Returns:\n        bytes: The inflated data.\n\n    Raises:\n        zlib.error"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef skip(self):\n        '''Mark the item as processed without download.'''\n        _logger.debug(__(_('Skipping \u2018{url}\u2019.'), url=self.url_record.url))\n        self.app_session.factory['URLTable'].check_in(self.url_record.url, Status.skipped)\n\n        self._processed = True", "response": "Mark the item as processed without download."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_status(self, status: Status, increment_try_count: bool=True,\n                   filename: str=None):\n        '''Mark the item with the given status.\n\n        Args:\n            status: a value from :class:`Status`.\n            increment_try_count: if True, increment the ``try_count``\n                value\n        '''\n        url = self.url_record.url\n        assert not self._try_count_incremented, (url, status)\n\n        if increment_try_count:\n            self._try_count_incremented = True\n\n        _logger.debug(__('Marking URL {0} status {1}.', url, status))\n\n        url_result = URLResult()\n        url_result.filename = filename\n\n        self.app_session.factory['URLTable'].check_in(\n            url,\n            status,\n            increment_try_count=increment_try_count,\n            url_result=url_result,\n        )\n\n        self._processed = True", "response": "Mark the item with the given status."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_child_url(self, url: str, inline: bool=False,\n                      link_type: Optional[LinkType]=None,\n                      post_data: Optional[str]=None,\n                      level: Optional[int]=None,\n                      replace: bool=False):\n        '''Add links scraped from the document with automatic values.\n\n        Args:\n            url: A full URL. (It can't be a relative path.)\n            inline: Whether the URL is an embedded object.\n            link_type: Expected link type.\n            post_data: URL encoded form data. The request will be made using\n                POST. (Don't use this to upload files.)\n            level: The child depth of this URL.\n            replace: Whether to replace the existing entry in the database\n                table so it will be redownloaded again.\n\n        This function provides values automatically for:\n\n        * ``inline``\n        * ``level``\n        * ``parent``: The referrering page.\n        * ``root``\n\n        See also :meth:`add_url`.\n        '''\n        url_properties = URLProperties()\n        url_properties.level = self.url_record.level + 1 if level is None else level\n        url_properties.inline_level = (self.url_record.inline_level or 0) + 1 if inline else None\n        url_properties.parent_url = self.url_record.url\n        url_properties.root_url = self.url_record.root_url or self.url_record.url\n        url_properties.link_type = link_type\n        url_data = URLData()\n        url_data.post_data = post_data\n\n        if replace:\n            self.app_session.factory['URLTable'].remove_many([url])\n\n        self.add_url(url, url_properties, url_data)", "response": "Add links scraped from the document with automatic values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a child URLRecord. This function is useful for testing filters before adding to table.", "response": "def child_url_record(self, url: str, inline: bool=False,\n                         link_type: Optional[LinkType]=None,\n                         post_data: Optional[str]=None,\n                         level: Optional[int]=None):\n        '''Return a child URLRecord.\n\n        This function is useful for testing filters before adding to table.\n        '''\n        url_record = URLRecord()\n        url_record.url = url\n        url_record.status = Status.todo\n        url_record.try_count = 0\n        url_record.level = self.url_record.level + 1 if level is None else level\n        url_record.root_url = self.url_record.root_url or self.url_record.url\n        url_record.parent_url = self.url_record.url\n        url_record.inline_level = (self.url_record.inline_level or 0) + 1 if inline else 0\n        url_record.link_type = link_type\n        url_record.post_data = post_data\n\n        return url_record"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the document encoding from a HTTP header.", "response": "def get_heading_encoding(response):\n    '''Return the document encoding from a HTTP header.\n\n    Args:\n        response (Response): An instance of :class:`.http.Response`.\n\n    Returns:\n        ``str``, ``None``: The codec name.\n    '''\n    encoding = wpull.protocol.http.util.parse_charset(\n        response.fields.get('content-type', ''))\n\n    if encoding:\n        return wpull.string.normalize_codec_name(encoding)\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the likely encoding of the response document.", "response": "def detect_response_encoding(response, is_html=False, peek=131072):\n    '''Return the likely encoding of the response document.\n\n    Args:\n        response (Response): An instance of :class:`.http.Response`.\n        is_html (bool): See :func:`.util.detect_encoding`.\n        peek (int): The maximum number of bytes of the document to be analyzed.\n\n    Returns:\n        ``str``, ``None``: The codec name.\n    '''\n    encoding = get_heading_encoding(response)\n\n    encoding = wpull.string.detect_encoding(\n        wpull.util.peek_file(response.body, peek), encoding=encoding, is_html=is_html\n    )\n\n    _logger.debug(__('Got encoding: {0}', encoding))\n\n    return encoding"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef contains(self, url: str):\n        '''Return whether the URL is in the table.'''\n\n        try:\n            self.get_one(url)\n        except NotFound:\n            return False\n        else:\n            return True", "response": "Return whether the URL is in the table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_one(self, url: str,\n                url_properties: Optional[URLProperties]=None,\n                url_data: Optional[URLData]=None):\n        '''Add a single URL to the table.\n\n        Args:\n            url: The URL to be added\n            url_properties: Additional values to be saved\n            url_data: Additional data to be saved\n        '''\n        self.add_many([AddURLInfo(url, url_properties, url_data)])", "response": "Add a single URL to the table."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_in(self, url: str, new_status: Status,\n                 increment_try_count: bool=True,\n                 url_result: Optional[URLResult]=None):\n        '''Update record for processed URL.\n\n        Args:\n            url: The URL.\n            new_status: Update the item status to `new_status`.\n            increment_try_count: Whether to increment the try counter\n                for the URL.\n            url_result: Additional values.\n        '''", "response": "Update the status of the item in the URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stream(self):\n        '''Iterate the file stream.\n\n        Returns:\n            iterator: Each item is a tuple:\n\n            1. None, regex match\n            2. str\n        '''\n        chunk_a = None\n        chunk_b = None\n        chunk_a_index = 0\n        chunk_b_index = 0\n        search_start_index = 0\n\n        while True:\n            chunk_a = chunk_b\n            chunk_a_index = chunk_b_index\n            chunk_b = self._file.read(self._read_size)\n\n            if chunk_a is None:\n                continue\n\n            chunk_b_index = chunk_a_index + len(chunk_a)\n\n            if not chunk_a:\n                break\n\n            current_chunk = chunk_a + chunk_b[:self._overlap_size]\n\n            offset_end = len(chunk_a) + self._overlap_size\n\n            while True:\n                offset_start = search_start_index - chunk_a_index\n                match = self._pattern.search(\n                    current_chunk, offset_start, offset_end)\n\n                if not match:\n                    unmatched_part = chunk_a[offset_start:]\n\n                    if unmatched_part:\n                        yield (None, unmatched_part)\n\n                    search_start_index += len(unmatched_part)\n                    break\n\n                start_index, end_index = match.span(match.lastindex)\n\n                unmatched_part = current_chunk[offset_start:start_index]\n\n                if unmatched_part:\n                    yield (None, unmatched_part)\n\n                yield (match, match.group(match.lastindex))\n\n                search_start_index += len(unmatched_part) + \\\n                    len(match.group(match.lastindex))", "response": "Iterate the file stream."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef notify(self, *args, **kwargs):\n        '''Call all the callback handlers with given arguments.'''\n        for handler in tuple(self.handlers):\n            handler(*args, **kwargs)", "response": "Call all the callback handlers with given arguments."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef new(self, name, *args, **kwargs):\n        '''Create an instance.\n\n        Args:\n            name (str): The name of the class\n            args: The arguments to pass to the class.\n            kwargs: The keyword arguments to pass to the class.\n\n        Returns:\n            instance\n        '''\n        if name in self._instance_map:\n            raise ValueError('Instance {0} is already initialized'\n                             .format(name))\n\n        instance = self._class_map[name](*args, **kwargs)\n        self._instance_map[name] = instance\n        return instance", "response": "Create an instance of the class with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_all_initialized(self):\n        '''Return whether all the instances have been initialized.\n\n        Returns:\n            bool\n        '''\n        return frozenset(self._class_map.keys()) == \\\n            frozenset(self._instance_map.keys())", "response": "Return whether all the class and instance maps have been initialized."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef normalize_name(name, overrides=None):\n    '''Normalize the key name to title case.\n\n    For example, ``normalize_name('content-id')`` will become ``Content-Id``\n\n    Args:\n        name (str): The name to normalize.\n        overrides (set, sequence): A set or sequence containing keys that\n            should be cased to themselves. For example, passing\n            ``set('WARC-Type')`` will normalize any key named \"warc-type\" to\n            ``WARC-Type`` instead of the default ``Warc-Type``.\n\n    Returns:\n        str\n    '''\n\n    normalized_name = name.title()\n\n    if overrides:\n        override_map = dict([(name.title(), name) for name in overrides])\n\n        return override_map.get(normalized_name, normalized_name)\n    else:\n        return normalized_name", "response": "Normalizes the key name to title case."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the most likely line delimiter from the string.", "response": "def guess_line_ending(string):\n    '''Return the most likely line delimiter from the string.'''\n    assert isinstance(string, str), 'Expect str. Got {}'.format(type(string))\n    crlf_count = string.count('\\r\\n')\n    lf_count = string.count('\\n')\n\n    if crlf_count >= lf_count:\n        return '\\r\\n'\n    else:\n        return '\\n'"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unfold_lines(string):\n    '''Join lines that are wrapped.\n\n    Any line that starts with a space or tab is joined to the previous\n    line.\n    '''\n    assert isinstance(string, str), 'Expect str. Got {}'.format(type(string))\n    lines = string.splitlines()\n    line_buffer = io.StringIO()\n\n    for line_number in range(len(lines)):\n        line = lines[line_number]\n        if line and line[0:1] in (' ', '\\t'):\n            line_buffer.write(' ')\n        elif line_number != 0:\n            line_buffer.write('\\r\\n')\n        line_buffer.write(line.strip())\n\n    line_buffer.write('\\r\\n')\n\n    return line_buffer.getvalue()", "response": "Unfold a string into a single segment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse(self, string, strict=True):\n        '''Parse the string or bytes.\n\n        Args:\n            strict (bool): If True, errors will not be ignored\n\n        Raises:\n            :class:`ValueError` if the record is malformed.\n        '''\n        if isinstance(string, bytes):\n            errors = 'strict' if strict else 'replace'\n            string = string.decode(self.encoding, errors=errors)\n\n        if not self.raw:\n            self.raw = string\n        else:\n            self.raw += string\n\n        lines = unfold_lines(string).splitlines()\n        for line in lines:\n            if line:\n                if ':' not in line:\n                    if strict:\n                        raise ValueError('Field missing colon.')\n                    else:\n                        continue\n\n                name, value = line.split(':', 1)\n                name = name.strip()\n                value = value.strip()\n                self.add(name, value)", "response": "Parse the string or bytes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nappend the name - value pair to the record.", "response": "def add(self, name, value):\n        '''Append the name-value pair to the record.'''\n        normalized_name = normalize_name(name, self._normalize_overrides)\n        self._map[normalized_name].append(value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns all the values for given name.", "response": "def get_list(self, name):\n        '''Return all the values for given name.'''\n        normalized_name = normalize_name(name, self._normalize_overrides)\n        return self._map[normalized_name]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an iterator of name - value pairs.", "response": "def get_all(self):\n        '''Return an iterator of name-value pairs.'''\n        for name, values in self._map.items():\n            for value in values:\n                yield (name, value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef recycle(self):\n        '''Clean up and return connections back to the pool.\n\n        Connections should be kept alive if supported.\n        '''\n\n        for connection in self._connections:\n            self._connection_pool.no_wait_release(connection)\n\n        self._connections.clear()", "response": "Clean up and return connections back to the pool."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new session.", "response": "def session(self) -> SessionT:\n        '''Return a new session.'''\n        session = self._session_class()(\n            connection_pool=self._connection_pool,\n        )\n        self.event_dispatcher.notify(self.ClientEvent.new_session, session)\n        return session"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the number of cookies for the given domain.", "response": "def count_cookies(self, domain):\n        '''Return the number of cookies for the given domain.'''\n        cookies = self.cookie_jar._cookies\n\n        if domain in cookies:\n            return sum(\n                [len(cookie) for cookie in cookies[domain].values()]\n            )\n        else:\n            return 0"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cookie_length(self, domain):\n        '''Return approximate length of all cookie key-values for a domain.'''\n        cookies = self.cookie_jar._cookies\n\n        if domain not in cookies:\n            return 0\n\n        length = 0\n\n        for path in cookies[domain]:\n            for name in cookies[domain][path]:\n                cookie = cookies[domain][path][name]\n                length += len(path) + len(name) + len(cookie.value or '')\n\n        return length", "response": "Return approximate length of all cookie key - values for a domain."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef guess_listing_type(lines, threshold=100):\n    '''Guess the style of directory listing.\n\n    Returns:\n        str: ``unix``, ``msdos``, ``nlst``, ``unknown``.\n    '''\n    scores = {\n        'unix': 0,\n        'msdos': 0,\n        'nlst': 0,\n    }\n    for line in lines:\n        if not line:\n            continue\n\n        if re.search(r'---|r--|rw-|rwx', line):\n            scores['unix'] += 1\n\n        if '<DIR>' in line or re.search(r'^.{0,4}\\d\\d', line):\n            scores['msdos'] += 1\n\n        words = line.split(' ', 1)\n\n        if len(words) == 1:\n            scores['nlst'] += 1\n\n        if max(scores.values()) > threshold:\n            break\n\n    top = max(scores.items(), key=lambda item: item[1])\n\n    if top[1]:\n        return top[0]\n    else:\n        return 'unknown'", "response": "Guess the style of directory listing.\n    Returns: str : Unix msdos nlst unknown."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_unix_perm(text):\n    '''Parse a Unix permission string and return integer value.'''\n    # Based on ftp-ls.c symperms\n    if len(text) != 9:\n        return 0\n\n    perms = 0\n\n    for triad_index in range(3):\n        string_index = triad_index * 3\n        perms <<= 3\n\n        if text[string_index] == 'r':\n            perms |= 1 << 2\n\n        if text[string_index + 1] == 'w':\n            perms |= 1 << 1\n\n        if text[string_index + 2] in 'xs':\n            perms |= 1\n\n    return perms", "response": "Parse a Unix permission string and return integer value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_datetime(self, text):\n        '''Parse datetime from line of text.'''\n        return parse_datetime(text, date_format=self.date_format,\n                              is_day_period=self.is_day_period)", "response": "Parse datetime from line of text."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse lines from a MS - DOS format.", "response": "def parse_msdos(self, lines):\n        '''Parse lines from a MS-DOS format.'''\n        for line in lines:\n            fields = line.split(None, 4)\n\n            date_str = fields[0]\n            time_str = fields[1]\n\n            datetime_str = '{} {}'.format(date_str, time_str)\n\n            file_datetime = self.parse_datetime(datetime_str)[0]\n\n            if fields[2] == '<DIR>':\n                file_size = None\n                file_type = 'dir'\n            else:\n                file_size = parse_int(fields[2])\n                file_type = 'file'\n\n            filename = fields[3]\n\n            yield FileEntry(filename, file_type, file_size, file_datetime)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing listings from a Unix ls command format.", "response": "def parse_unix(self, lines):\n        '''Parse listings from a Unix ls command format.'''\n        # This method uses some Filezilla parsing algorithms\n\n        for line in lines:\n            original_line = line\n            fields = line.split(' ')\n            after_perm_index = 0\n\n            # Search for the permissions field by checking the file type\n            for field in fields:\n                after_perm_index += len(field)\n                if not field:\n                    continue\n\n                # If the filesystem goes corrupt, it may show ? instead\n                # but I don't really care in that situation.\n                if field[0] in 'bcdlps-':\n                    if field[0] == 'd':\n                        file_type = 'dir'\n                    elif field[0] == '-':\n                        file_type = 'file'\n                    elif field[0] == 'l':\n                        file_type = 'symlink'\n                    else:\n                        file_type = 'other'\n\n                    perms = parse_unix_perm(field[1:])\n\n                    break\n            else:\n                raise ListingError('Failed to parse file type.')\n\n            line = line[after_perm_index:]\n\n            # We look for the position of the date and use the integer\n            # before it as the file size.\n            # We look for the position of the time and use the text\n            # after it as the filename\n\n            while line:\n                try:\n                    datetime_obj, start_index, end_index = self.parse_datetime(line)\n                except ValueError:\n                    line = line[4:]\n                else:\n                    break\n            else:\n                raise ListingError(\n                    'Could parse a date from {}'.format(repr(original_line)))\n\n            file_size = int(line[:start_index].rstrip().rpartition(' ')[-1])\n\n            filename = line[end_index:].strip()\n\n            if file_type == 'symlink':\n                filename, sep, symlink_dest = filename.partition(' -> ')\n            else:\n                symlink_dest = None\n\n            yield FileEntry(filename, file_type, file_size, datetime_obj,\n                            symlink_dest, perm=perms)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the listings. returns an iterator of FileEntry objects", "response": "def parse_input(self):\n        '''Parse the listings.\n\n        Returns:\n            iter: A iterable of :class:`.ftp.ls.listing.FileEntry`\n        '''\n\n        if self._text:\n            lines = iter(self._text.splitlines())\n        elif self._file:\n            lines = self._file\n        else:\n            lines = ()\n\n        sample_lines = []\n\n        for line in lines:\n            if len(sample_lines) > 100:\n                break\n\n            sample_lines.append(line)\n\n        lines = itertools.chain(sample_lines, lines)\n\n        self.guess_type(sample_lines)\n\n        datetime_format = wpull.protocol.ftp.ls.date.guess_datetime_format(\n            sample_lines)\n\n        self.set_datetime_format(datetime_format)\n\n        return self.parse(lines)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef open_file(cls, filename: str, response: BaseResponse, mode='wb+'):\n        '''Open a file object on to the Response Body.\n\n        Args:\n            filename: The path where the file is to be saved\n            response: Response\n            mode: The file mode\n\n        This function will create the directories if not exist.\n        '''\n        _logger.debug('Saving file to {0}, mode={1}.',\n                      filename, mode)\n\n        dir_path = os.path.dirname(filename)\n        if dir_path and not os.path.exists(dir_path):\n            os.makedirs(dir_path)\n\n        response.body = Body(open(filename, mode))", "response": "Open a file object on to the Response Body."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the Last - Modified timestamp onto the given file.", "response": "def set_timestamp(cls, filename: str, response: HTTPResponse):\n        '''Set the Last-Modified timestamp onto the given file.\n\n        Args:\n            filename: The path of the file\n            response: Response\n        '''\n        last_modified = response.fields.get('Last-Modified')\n\n        if not last_modified:\n            return\n\n        try:\n            last_modified = email.utils.parsedate(last_modified)\n        except ValueError:\n            _logger.exception('Failed to parse date.')\n            return\n\n        last_modified = time.mktime(last_modified)\n\n        os.utime(filename, (time.time(), last_modified))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save_headers(cls, filename: str, response: HTTPResponse):\n        '''Prepend the HTTP response header to the file.\n\n        Args:\n            filename: The path of the file\n            response: Response\n        '''\n        new_filename = filename + '-new'\n\n        with open('wb') as new_file:\n            new_file.write(response.header())\n\n            with wpull.util.reset_file_offset(response.body):\n                response.body.seek(0)\n                shutil.copyfileobj(response.body, new_file)\n\n        os.remove(filename)\n        os.rename(new_filename, filename)", "response": "Prepend the HTTP response header to the file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the appropriate filename from the request.", "response": "def _compute_filename(self, request: BaseRequest):\n        '''Get the appropriate filename from the request.'''\n        path = self._path_namer.get_filename(request.url_info)\n\n        if os.path.isdir(path):\n            path += '.f'\n        else:\n            dir_name, name = os.path.split(path)\n            path = os.path.join(anti_clobber_dir_path(dir_name), name)\n\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _process_file_continue_request(self, request: BaseRequest):\n        '''Modify the request to resume downloading file.'''\n        if os.path.exists(self._filename):\n            size = os.path.getsize(self._filename)\n            request.set_continue(size)\n            self._file_continue_requested = True\n\n            _logger.debug('Continue file from {0}.', size)\n        else:\n            _logger.debug('No file to continue.')", "response": "Modify the request to resume downloading file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _process_file_continue_response(self, response: HTTPResponse):\n        '''Process a partial content response.'''\n        code = response.status_code\n\n        if code == http.client.PARTIAL_CONTENT:\n            self.open_file(self._filename, response, mode='ab+')\n        else:\n            self._raise_cannot_continue_error()", "response": "Process a partial content response."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _process_file_continue_ftp_response(self, response: FTPResponse):\n        '''Process a restarted content response.'''\n        if response.request.restart_value and response.restart_value:\n            self.open_file(self._filename, response, mode='ab+')\n        else:\n            self._raise_cannot_continue_error()", "response": "Process a restarted content response."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nappend an HTML file suffix as needed.", "response": "def _append_filename_extension(self, response: BaseResponse):\n        '''Append an HTML/CSS file suffix as needed.'''\n        if not self._filename:\n            return\n\n        if response.request.url_info.scheme not in ('http', 'https'):\n            return\n\n        if not re.search(r'\\.[hH][tT][mM][lL]?$', self._filename) and \\\n                HTMLReader.is_response(response):\n            self._filename += '.html'\n        elif not re.search(r'\\.[cC][sS][sS]$', self._filename) and \\\n                CSSReader.is_response(response):\n            self._filename += '.css'"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _rename_with_content_disposition(self, response: HTTPResponse):\n        '''Rename using the Content-Disposition header.'''\n        if not self._filename:\n            return\n\n        if response.request.url_info.scheme not in ('http', 'https'):\n            return\n\n        header_value = response.fields.get('Content-Disposition')\n\n        if not header_value:\n            return\n\n        filename = parse_content_disposition(header_value)\n\n        if filename:\n            dir_path = os.path.dirname(self._filename)\n            new_filename = self._path_namer.safe_filename(filename)\n            self._filename = os.path.join(dir_path, new_filename)", "response": "Rename using the Content - Disposition header."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef session(self) -> BaseFileWriterSession:\n        '''Return the File Writer Session.'''\n        return self.session_class(\n            self._path_namer,\n            self._file_continuing,\n            self._headers_included,\n            self._local_timestamping,\n            self._adjust_extension,\n            self._content_disposition,\n            self._trust_server_names,\n        )", "response": "Return the File Writer Session."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a Content - Type string for the document encoding.", "response": "def parse_charset(header_string):\n    '''Parse a \"Content-Type\" string for the document encoding.\n\n    Returns:\n        str, None\n    '''\n    match = re.search(\n        r'''charset[ ]?=[ ]?[\"']?([a-z0-9_-]+)''',\n        header_string,\n        re.IGNORECASE\n    )\n\n    if match:\n        return match.group(1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn whether the connection should be closed.", "response": "def should_close(http_version, connection_field):\n    '''Return whether the connection should be closed.\n\n    Args:\n        http_version (str): The HTTP version string like ``HTTP/1.0``.\n        connection_field (str): The value for the ``Connection`` header.\n    '''\n    connection_field = (connection_field or '').lower()\n\n    if http_version == 'HTTP/1.0':\n        return connection_field.replace('-', '') != 'keepalive'\n    else:\n        return connection_field == 'close'"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nseeks to the end of the file.", "response": "def seek_file_end(file):\n    '''Seek to the end of the file.'''\n    try:\n        file.seek(0, 2)\n    except ValueError:\n        # gzip files don't support seek from end\n        while True:\n            data = file.read(4096)\n            if not data:\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_iso8601_str(string):\n    '''Parse a fixed ISO8601 datetime string.\n\n    .. Note:: This function only parses dates in the format\n       ``%Y-%m-%dT%H:%M:%SZ``. You must use a library like ``dateutils``\n       to properly parse dates and times.\n\n    Returns:\n        float: A UNIX timestamp.\n    '''\n    datetime_obj = datetime.datetime.strptime(string, \"%Y-%m-%dT%H:%M:%SZ\")\n    return int(calendar.timegm(datetime_obj.utctimetuple()))", "response": "Parse a fixed ISO8601 datetime string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the Python version as a string.", "response": "def python_version():\n    '''Return the Python version as a string.'''\n    major, minor, patch = sys.version_info[0:3]\n    return '{0}.{1}.{2}'.format(major, minor, patch)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter_pem(data):\n    '''Processes the bytes for PEM certificates.\n\n    Returns:\n        ``set`` containing each certificate\n    '''\n    assert isinstance(data, bytes), 'Expect bytes. Got {}.'.format(type(data))\n    certs = set()\n    new_list = []\n    in_pem_block = False\n\n    for line in re.split(br'[\\r\\n]+', data):\n        if line == b'-----BEGIN CERTIFICATE-----':\n            assert not in_pem_block\n            in_pem_block = True\n        elif line == b'-----END CERTIFICATE-----':\n            assert in_pem_block\n            in_pem_block = False\n\n            content = b''.join(new_list)\n            content = rewrap_bytes(content)\n\n            certs.add(b'-----BEGIN CERTIFICATE-----\\n' +\n                      content +\n                      b'\\n-----END CERTIFICATE-----\\n')\n\n            new_list = []\n        elif in_pem_block:\n            new_list.append(line)\n\n    return certs", "response": "Processes the bytes for PEM certificates.\n    Returns a set containing each certificate in the PEM format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rewrap_bytes(data):\n    '''Rewrap characters to 70 character width.\n\n    Intended to rewrap base64 content.\n    '''\n    return b'\\n'.join(\n        data[index:index+70] for index in range(0, len(data), 70)\n    )", "response": "Rewrap characters to 70 character width."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the contents of a real file or a zip file.", "response": "def get_package_data(filename, mode='rb'):\n    '''Return the contents of a real file or a zip file.'''\n    if os.path.exists(filename):\n        with open(filename, mode=mode) as in_file:\n            return in_file.read()\n    else:\n        parts = os.path.normpath(filename).split(os.sep)\n\n        for part, index in zip(parts, range(len(parts))):\n            if part.endswith('.zip'):\n                zip_path = os.sep.join(parts[:index + 1])\n                member_path = os.sep.join(parts[index + 1:])\n                break\n\n        if platform.system() == 'Windows':\n            member_path = member_path.replace('\\\\', '/')\n\n        with zipfile.ZipFile(zip_path) as zip_file:\n            return zip_file.read(member_path)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_package_filename(filename, package_dir=None):\n    '''Return the filename of the data file.'''\n    if getattr(sys, 'frozen', False):\n        package_dir = os.path.join(\n            sys._MEIPASS,\n            os.path.basename(os.path.dirname(__file__))\n        )\n    elif not package_dir:\n        package_dir = os.path.dirname(__file__)\n\n    return os.path.join(package_dir, filename)", "response": "Return the filename of the data file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_exception_message(instance):\n    '''Try to get the exception message or the class name.'''\n    args = getattr(instance, 'args', None)\n\n    if args:\n        return str(instance)\n\n    try:\n        return type(instance).__name__\n    except AttributeError:\n        return str(instance)", "response": "Try to get the exception message or the class name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking a value into a quoted XML attribute.", "response": "def quoted_attribute_value(self, value):\n        \"\"\"Make a value into a quoted XML attribute, possibly escaping it.\n\n         Most strings will be quoted using double quotes.\n\n          Bob's Bar -> \"Bob's Bar\"\n\n         If a string contains double quotes, it will be quoted using\n         single quotes.\n\n          Welcome to \"my bar\" -> 'Welcome to \"my bar\"'\n\n         If a string contains both single and double quotes, the\n         double quotes will be escaped, and the string will be quoted\n         using double quotes.\n\n          Welcome to \"Bob's Bar\" -> \"Welcome to &quot;Bob's bar&quot;\n        \"\"\"\n        quote_with = '\"'\n        if '\"' in value:\n            if \"'\" in value:\n                # The string contains both single and double\n                # quotes.  Turn the double quotes into\n                # entities. We quote the double quotes rather than\n                # the single quotes because the entity name is\n                # \"&quot;\" whether this is HTML or XML.  If we\n                # quoted the single quotes, we'd have to decide\n                # between &apos; and &squot;.\n                replace_with = \"&quot;\"\n                value = value.replace('\"', replace_with)\n            else:\n                # There are double quotes but no single quotes.\n                # We can use single quotes to quote the attribute.\n                quote_with = \"'\"\n        return quote_with + value + quote_with"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsubstitutes XML entities for special XML characters.", "response": "def substitute_xml(cls, value, make_quoted_attribute=False):\n        \"\"\"Substitute XML entities for special XML characters.\n\n        :param value: A string to be substituted. The less-than sign\n          will become &lt;, the greater-than sign will become &gt;,\n          and any ampersands will become &amp;. If you want ampersands\n          that appear to be part of an entity definition to be left\n          alone, use substitute_xml_containing_entities() instead.\n\n        :param make_quoted_attribute: If True, then the string will be\n         quoted, as befits an attribute value.\n        \"\"\"\n        # Escape angle brackets and ampersands.\n        value = cls.AMPERSAND_OR_BRACKET.sub(\n            cls._substitute_xml_entity, value)\n\n        if make_quoted_attribute:\n            value = cls.quoted_attribute_value(value)\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsubstitutes XML entities for special XML characters.", "response": "def substitute_xml_containing_entities(\n        cls, value, make_quoted_attribute=False):\n        \"\"\"Substitute XML entities for special XML characters.\n\n        :param value: A string to be substituted. The less-than sign will\n          become &lt;, the greater-than sign will become &gt;, and any\n          ampersands that are not part of an entity defition will\n          become &amp;.\n\n        :param make_quoted_attribute: If True, then the string will be\n         quoted, as befits an attribute value.\n        \"\"\"\n        # Escape angle brackets, and ampersands that aren't part of\n        # entities.\n        value = cls.BARE_AMPERSAND_OR_BRACKET.sub(\n            cls._substitute_xml_entity, value)\n\n        if make_quoted_attribute:\n            value = cls.quoted_attribute_value(value)\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encodings(self):\n        tried = set()\n        for e in self.override_encodings:\n            if self._usable(e, tried):\n                yield e\n\n        # Did the document originally start with a byte-order mark\n        # that indicated its encoding?\n        if self._usable(self.sniffed_encoding, tried):\n            yield self.sniffed_encoding\n\n        # Look within the document for an XML or HTML encoding\n        # declaration.\n        if self.declared_encoding is None:\n            self.declared_encoding = self.find_declared_encoding(\n                self.markup, self.is_html)\n        if self._usable(self.declared_encoding, tried):\n            yield self.declared_encoding\n\n        # Use third-party character set detection to guess at the\n        # encoding.\n        if self.chardet_encoding is None:\n            self.chardet_encoding = chardet_dammit(self.markup)\n        if self._usable(self.chardet_encoding, tried):\n            yield self.chardet_encoding\n\n        # As a last-ditch effort, try utf-8 and windows-1252.\n        for e in ('utf-8', 'windows-1252'):\n            if self._usable(e, tried):\n                yield e", "response": "Yields a number of encodings that might work for this markup."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef strip_byte_order_mark(cls, data):\n        encoding = None\n        if (len(data) >= 4) and (data[:2] == b'\\xfe\\xff') \\\n               and (data[2:4] != '\\x00\\x00'):\n            encoding = 'utf-16be'\n            data = data[2:]\n        elif (len(data) >= 4) and (data[:2] == b'\\xff\\xfe') \\\n                 and (data[2:4] != '\\x00\\x00'):\n            encoding = 'utf-16le'\n            data = data[2:]\n        elif data[:3] == b'\\xef\\xbb\\xbf':\n            encoding = 'utf-8'\n            data = data[3:]\n        elif data[:4] == b'\\x00\\x00\\xfe\\xff':\n            encoding = 'utf-32be'\n            data = data[4:]\n        elif data[:4] == b'\\xff\\xfe\\x00\\x00':\n            encoding = 'utf-32le'\n            data = data[4:]\n        return data, encoding", "response": "Strip a byte - order mark from the data and return the encoding it implies."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a document tries to find its declared encoding.", "response": "def find_declared_encoding(cls, markup, is_html=False, search_entire_document=False):\n        \"\"\"Given a document, tries to find its declared encoding.\n\n        An XML encoding is declared at the beginning of the document.\n\n        An HTML encoding is declared in a <meta> tag, hopefully near the\n        beginning of the document.\n        \"\"\"\n        if search_entire_document:\n            xml_endpos = html_endpos = len(markup)\n        else:\n            xml_endpos = 1024\n            html_endpos = max(2048, int(len(markup) * 0.05))\n\n        declared_encoding = None\n        declared_encoding_match = xml_encoding_re.search(markup, endpos=xml_endpos)\n        if not declared_encoding_match and is_html:\n            declared_encoding_match = html_meta_re.search(markup, endpos=html_endpos)\n        if declared_encoding_match is not None:\n            declared_encoding = declared_encoding_match.groups()[0].decode(\n                'ascii', 'replace')\n        if declared_encoding:\n            return declared_encoding.lower()\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchanges a MS smart quote character to an XML or HTML or an ASCII character.", "response": "def _sub_ms_char(self, match):\n        \"\"\"Changes a MS smart quote character to an XML or HTML\n        entity, or an ASCII character.\"\"\"\n        orig = match.group(1)\n        if self.smart_quotes_to == 'ascii':\n            sub = self.MS_CHARS_TO_ASCII.get(orig).encode()\n        else:\n            sub = self.MS_CHARS.get(orig)\n            if type(sub) == tuple:\n                if self.smart_quotes_to == 'xml':\n                    sub = '&#x'.encode() + sub[1].encode() + ';'.encode()\n                else:\n                    sub = '&'.encode() + sub[0].encode() + ';'.encode()\n            else:\n                sub = sub.encode()\n        return sub"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfix characters from one encoding embedded in some other encoding. Currently the only situation supported is Windows-1252 (or its subset ISO-8859-1), embedded in UTF-8. The input must be a bytestring. If you've already converted the document to Unicode, you're too late. The output is a bytestring in which `embedded_encoding` characters have been converted to their `main_encoding` equivalents.", "response": "def detwingle(cls, in_bytes, main_encoding=\"utf8\",\n                  embedded_encoding=\"windows-1252\"):\n        \"\"\"Fix characters from one encoding embedded in some other encoding.\n\n        Currently the only situation supported is Windows-1252 (or its\n        subset ISO-8859-1), embedded in UTF-8.\n\n        The input must be a bytestring. If you've already converted\n        the document to Unicode, you're too late.\n\n        The output is a bytestring in which `embedded_encoding`\n        characters have been converted to their `main_encoding`\n        equivalents.\n        \"\"\"\n        if embedded_encoding.replace('_', '-').lower() not in (\n            'windows-1252', 'windows_1252'):\n            raise NotImplementedError(\n                \"Windows-1252 and ISO-8859-1 are the only currently supported \"\n                \"embedded encodings.\")\n\n        if main_encoding.lower() not in ('utf8', 'utf-8'):\n            raise NotImplementedError(\n                \"UTF-8 is the only currently supported main encoding.\")\n\n        byte_chunks = []\n\n        chunk_start = 0\n        pos = 0\n        while pos < len(in_bytes):\n            byte = in_bytes[pos]\n            if not isinstance(byte, int):\n                # Python 2.x\n                byte = ord(byte)\n            if (byte >= cls.FIRST_MULTIBYTE_MARKER\n                and byte <= cls.LAST_MULTIBYTE_MARKER):\n                # This is the start of a UTF-8 multibyte character. Skip\n                # to the end.\n                for start, end, size in cls.MULTIBYTE_MARKERS_AND_SIZES:\n                    if byte >= start and byte <= end:\n                        pos += size\n                        break\n            elif byte >= 0x80 and byte in cls.WINDOWS_1252_TO_UTF8:\n                # We found a Windows-1252 character!\n                # Save the string up to this point as a chunk.\n                byte_chunks.append(in_bytes[chunk_start:pos])\n\n                # Now translate the Windows-1252 character into UTF-8\n                # and add it as another, one-byte chunk.\n                byte_chunks.append(cls.WINDOWS_1252_TO_UTF8[byte])\n                pos += 1\n                chunk_start = pos\n            else:\n                # Go on to the next character.\n                pos += 1\n        if chunk_start == 0:\n            # The string is unchanged.\n            return in_bytes\n        else:\n            # Store the final chunk.\n            byte_chunks.append(in_bytes[chunk_start:])\n        return b''.join(byte_chunks)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef scrape_file(self, file, encoding=None, base_url=None):\n        '''Scrape a file for links.\n\n        See :meth:`scrape` for the return value.\n        '''\n        elements = self.iter_elements(file, encoding=encoding)\n\n        link_contexts = set()\n\n        link_infos = self._element_walker.iter_links(elements)\n\n        for link_info in link_infos:\n            element_base_url = base_url\n\n            if link_info.base_link:\n                clean_base_url = clean_link_soup(link_info.base_link)\n\n                if element_base_url and base_url:\n                    element_base_url = urljoin_safe(\n                        base_url, clean_base_url\n                    ) or base_url\n\n            if element_base_url:\n                url = urljoin_safe(\n                    element_base_url,\n                    clean_link_soup(link_info.link),\n                    allow_fragments=False\n                )\n            else:\n                url = clean_link_soup(link_info.link)\n\n            if url:\n                link_contexts.add(LinkContext(\n                    url,\n                    inline=link_info.inline,\n                    linked=link_info.linked,\n                    link_type=link_info.link_type,\n                    extra=link_info\n                ))\n\n        scrape_result = ScrapeResult(link_contexts, encoding)\n        scrape_result['base_url'] = base_url\n        return scrape_result", "response": "Scrape a file for links."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _is_accepted(self, element_tag):\n        '''Return if the link is accepted by the filters.'''\n        element_tag = element_tag.lower()\n\n        if self._ignored_tags is not None \\\n           and element_tag in self._ignored_tags:\n            return False\n\n        if self._followed_tags is not None:\n            return element_tag in self._followed_tags\n        else:\n            return True", "response": "Return if the link is accepted by the filters."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\niterate the document root for links. is an iterable of : class:`LinkedInfo.", "response": "def iter_links(self, elements):\n        '''Iterate the document root for links.\n\n        Returns:\n            iterable: A iterator of :class:`LinkedInfo`.\n        '''\n        for element in elements:\n            if not isinstance(element, Element):\n                continue\n\n            for link_infos in self.iter_links_element(element):\n                yield link_infos"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef iter_links_element(self, element):\n        '''Iterate a HTML element.'''\n        # reference: lxml.html.HtmlMixin.iterlinks()\n        attrib = element.attrib\n        tag = element.tag\n\n        if tag == 'link':\n            iterable = self.iter_links_link_element(element)\n        elif tag == 'meta':\n            iterable = self.iter_links_meta_element(element)\n        elif tag in ('object', 'applet'):\n            iterable = self.iter_links_object_element(element)\n        elif tag == 'param':\n            iterable = self.iter_links_param_element(element)\n        elif tag == 'style':\n            iterable = self.iter_links_style_element(element)\n        elif tag == 'script':\n            iterable = self.iter_links_script_element(element)\n        else:\n            iterable = self.iter_links_plain_element(element)\n\n        # RSS/Atom\n        if tag in ('link', 'url', 'icon'):\n            iterable = itertools.chain(\n                iterable, self.iter_links_element_text(element)\n            )\n\n        for link_info in iterable:\n            yield link_info\n\n        if 'style' in attrib and self.css_scraper:\n            for link in self.css_scraper.scrape_links(attrib['style']):\n                yield LinkInfo(\n                    element=element, tag=element.tag, attrib='style',\n                    link=link,\n                    inline=True, linked=False,\n                    base_link=None,\n                    value_type='css',\n                    link_type=LinkType.media,\n                )", "response": "Iterate a HTML element."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the element text as a link.", "response": "def iter_links_element_text(cls, element):\n        '''Get the element text as a link.'''\n        if element.text:\n            link_type = identify_link_type(element.text)\n            yield LinkInfo(\n                element=element, tag=element.tag, attrib=None,\n                link=element.text,\n                inline=False, linked=True,\n                base_link=None,\n                value_type='plain',\n                link_type=link_type\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef iter_links_link_element(self, element):\n        '''Iterate a ``link`` for URLs.\n\n        This function handles stylesheets and icons in addition to\n        standard scraping rules.\n        '''\n        rel = element.attrib.get('rel', '')\n        stylesheet = 'stylesheet' in rel\n        icon = 'icon' in rel\n        inline = stylesheet or icon\n\n        if stylesheet:\n            link_type = LinkType.css\n        elif icon:\n            link_type = LinkType.media\n        else:\n            link_type = None\n\n        for attrib_name, link in self.iter_links_by_attrib(element):\n            yield LinkInfo(\n                element=element, tag=element.tag, attrib=attrib_name,\n                link=link,\n                inline=inline, linked=not inline,\n                base_link=None,\n                value_type='plain',\n                link_type=link_type\n            )", "response": "Iterate a link for URLs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef iter_links_meta_element(cls, element):\n        '''Iterate the ``meta`` element for links.\n\n        This function handles refresh URLs.\n        '''\n        if element.attrib.get('http-equiv', '').lower() == 'refresh':\n            content_value = element.attrib.get('content')\n\n            if content_value:\n                link = parse_refresh(content_value)\n\n                if link:\n                    yield LinkInfo(\n                        element=element, tag=element.tag, attrib='http-equiv',\n                        link=link,\n                        inline=False, linked=True,\n                        base_link=None,\n                        value_type='refresh',\n                        link_type=None  # treat it as a redirect\n                    )\n        else:\n            for link_info in cls.iter_links_open_graph_meta(element):\n                yield link_info", "response": "Iterate the meta element for links. This function handles refresh URLs."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\niterate object and embed elements.", "response": "def iter_links_object_element(cls, element):\n        '''Iterate ``object`` and ``embed`` elements.\n\n        This function also looks at ``codebase`` and ``archive`` attributes.\n        '''\n        base_link = element.attrib.get('codebase', None)\n\n        if base_link:\n            # lxml returns codebase as inline\n            link_type = element.attrib.get(base_link)\n            yield LinkInfo(\n                element=element, tag=element.tag, attrib='codebase',\n                link=base_link,\n                inline=True, linked=False,\n                base_link=None,\n                value_type='plain',\n                link_type=link_type\n            )\n\n        for attribute in ('code', 'src', 'classid', 'data'):\n            if attribute in element.attrib:\n                link_type = identify_link_type(element.attrib.get(attribute))\n                yield LinkInfo(\n                    element=element, tag=element.tag, attrib=attribute,\n                    link=element.attrib.get(attribute),\n                    inline=True, linked=False,\n                    base_link=base_link,\n                    value_type='plain',\n                    link_type=link_type\n                )\n\n        if 'archive' in element.attrib:\n            for match in re.finditer(r'[^ ]+', element.attrib.get('archive')):\n                value = match.group(0)\n                link_type = identify_link_type(value)\n                yield LinkInfo(\n                    element=element, tag=element.tag, attrib='archive',\n                    link=value,\n                    inline=True, linked=False,\n                    base_link=base_link,\n                    value_type='list',\n                    link_type=link_type\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\niterate a param element.", "response": "def iter_links_param_element(cls, element):\n        '''Iterate a ``param`` element.'''\n        valuetype = element.attrib.get('valuetype', '')\n\n        if valuetype.lower() == 'ref' and 'value' in element.attrib:\n            link_type = identify_link_type(element.attrib.get('value'))\n\n            yield LinkInfo(\n                element=element, tag=element.tag, attrib='value',\n                link=element.attrib.get('value'),\n                inline=True, linked=False,\n                base_link=None,\n                value_type='plain',\n                link_type=link_type\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef iter_links_style_element(self, element):\n        '''Iterate a ``style`` element.'''\n        if self.css_scraper and element.text:\n            link_iter = self.css_scraper.scrape_links(element.text,\n                                                      context=True)\n            for link, context in link_iter:\n                if context == 'import':\n                    link_type = LinkType.css\n                else:\n                    link_type = LinkType.media\n\n                yield LinkInfo(\n                    element=element, tag=element.tag, attrib=None,\n                    link=link,\n                    inline=True, linked=False,\n                    base_link=None,\n                    value_type='css',\n                    link_type=link_type\n                )", "response": "Iterate a style element."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\niterate a script element.", "response": "def iter_links_script_element(self, element):\n        '''Iterate a ``script`` element.'''\n        if self.javascript_scraper and element.text:\n            link_iter = self.javascript_scraper.scrape_links(element.text,\n                                                             context=True)\n\n            for link, context in link_iter:\n                inline = is_likely_inline(link)\n\n                if context is True:\n                    link_type = None\n                else:\n                    link_type = context\n\n                yield LinkInfo(\n                    element=element, tag=element.tag, attrib=None,\n                    link=link,\n                    inline=inline, linked=not inline,\n                    base_link=None,\n                    value_type='script',\n                    link_type=link_type\n                )\n\n        for link in self.iter_links_plain_element(element):\n            yield link"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef iter_links_plain_element(self, element):\n        '''Iterate any element for links using generic rules.'''\n        for attrib_name, link in self.iter_links_by_attrib(element):\n            if attrib_name in self.LINK_ATTRIBUTES:\n                inline = self.is_link_inline(element.tag, attrib_name)\n                linked = self.is_html_link(element.tag, attrib_name)\n            else:\n                inline = is_likely_inline(link)\n                linked = not inline\n\n            link_type = identify_link_type(link)\n\n            yield LinkInfo(\n                element=element, tag=element.tag, attrib=attrib_name,\n                link=link,\n                inline=inline, linked=linked,\n                base_link=None,\n                value_type='plain',\n                link_type=link_type\n            )", "response": "Iterate any element for links using generic rules."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\niterate an element by looking at its attributes for links.", "response": "def iter_links_by_attrib(self, element):\n        '''Iterate an element by looking at its attributes for links.'''\n        for attrib_name in element.attrib.keys():\n            attrib_value = element.attrib.get(attrib_name)\n\n            if attrib_name in self.LINK_ATTRIBUTES:\n                if self.javascript_scraper and \\\n                        attrib_value.lstrip().startswith('javascript:'):\n                    for link in self.iter_links_by_js_attrib(\n                            attrib_name, percent_decode(attrib_value)):\n                        yield link\n                else:\n                    yield attrib_name, attrib_value\n\n            elif self.javascript_scraper and \\\n                    attrib_name[:5] in self.DYNAMIC_ATTRIBUTES:\n                for link in self.iter_links_by_js_attrib(attrib_name,\n                                                         attrib_value):\n                    yield link\n\n            elif attrib_name.startswith('data-'):\n                if is_likely_link(attrib_value) \\\n                        and not is_unlikely_link(attrib_value):\n                    yield attrib_name, attrib_value\n\n            elif attrib_name == 'srcset':\n                items = self.iter_links_by_srcset_attrib(\n                    attrib_name, attrib_value)\n\n                for item in items:\n                    yield item"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef iter_links_by_js_attrib(self, attrib_name, attrib_value):\n        '''Iterate links of a JavaScript pseudo-link attribute.'''\n        links = self.javascript_scraper.scrape_links(attrib_value)\n\n        for link in links:\n            yield attrib_name, link", "response": "Iterate links of a JavaScript pseudo - link attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_link_inline(cls, tag, attribute):\n        '''Return whether the link is likely to be inline object.'''\n        if tag in cls.TAG_ATTRIBUTES \\\n           and attribute in cls.TAG_ATTRIBUTES[tag]:\n            attr_flags = cls.TAG_ATTRIBUTES[tag][attribute]\n            return attr_flags & cls.ATTR_INLINE\n\n        return attribute != 'href'", "response": "Return whether the link is likely to be inline object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_html_link(cls, tag, attribute):\n        '''Return whether the link is likely to be external object.'''\n        if tag in cls.TAG_ATTRIBUTES \\\n           and attribute in cls.TAG_ATTRIBUTES[tag]:\n            attr_flags = cls.TAG_ATTRIBUTES[tag][attribute]\n            return attr_flags & cls.ATTR_HTML\n\n        return attribute == 'href'", "response": "Return whether the link is likely to be external object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef robots_cannot_follow(cls, element):\n        '''Return whether we cannot follow links due to robots.txt directives.\n        '''\n        return (\n            element.tag == 'meta'\n            and element.attrib.get('name', '').lower() == 'robots'\n            and 'nofollow' in element.attrib.get('value', '').lower()\n        )", "response": "Return whether we cannot follow links due to robots. txt directives."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef iter_processed_text(self, file, encoding=None, base_url=None):\n        '''Return the file text and processed absolute links.\n\n        Args:\n            file: A file object containing the document.\n            encoding (str): The encoding of the document.\n            base_url (str): The URL at which the document is located.\n\n        Returns:\n            iterator: Each item is a tuple:\n\n            1. str: The text\n            2. bool: Whether the text a link\n        '''\n        for text, is_link in self.iter_text(file, encoding):\n            if is_link and base_url:\n                new_link = urljoin_safe(base_url, text, allow_fragments=False)\n\n                if new_link:\n                    yield (new_link, is_link)\n                else:\n                    yield (new_link, False)\n            else:\n                yield (text, is_link)", "response": "Return the file text and processed absolute links."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iter_processed_links(self, file, encoding=None, base_url=None, context=False):\n        '''Return the links.\n\n        This function is a convenience function for calling\n        :meth:`iter_processed_text` and returning only the links.\n        '''\n        if context:\n            return [item for item in self.iter_processed_text(file, encoding, base_url) if item[1]]\n        else:\n            return [item[0] for item in self.iter_processed_text(file, encoding, base_url) if item[1]]", "response": "Return the links.\n\n        This function is a convenience function for calling\n        :meth:`iter_processed_text` and returning only the links."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the links. iterator.", "response": "def iter_processed_links(self, file, encoding=None, base_url=None):\n        '''Return the links.\n\n        Returns:\n            iterator: Each item is a str which represents a link.\n        '''\n        for link in self.iter_links(file, encoding):\n            new_link = urljoin_safe(base_url, link, allow_fragments=False)\n            if new_link:\n                yield new_link"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\niterates the scrapers returning the first of the results.", "response": "def scrape(self, request, response, link_type=None):\n        '''Iterate the scrapers, returning the first of the results.'''\n        for scraper in self._document_scrapers:\n            scrape_result = scraper.scrape(request, response, link_type)\n\n            if scrape_result is None:\n                continue\n\n            if scrape_result.link_contexts:\n                return scrape_result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\niterating the scrapers and return a dict of results.", "response": "def scrape_info(self, request, response, link_type=None):\n        '''Iterate the scrapers and return a dict of results.\n\n        Returns:\n            dict: A dict where the keys are the scrapers instances and the\n            values are the results. That is, a mapping from\n            :class:`BaseDocumentScraper` to :class:`ScrapeResult`.\n        '''\n        info = {}\n        for scraper in self._document_scrapers:\n            scrape_result = scraper.scrape(request, response, link_type)\n            info[scraper] = scrape_result\n\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef feed(self, data_len, feed_time=None):\n        '''Update the bandwidth meter.\n\n        Args:\n            data_len (int): The number of bytes transfered since the last\n                call to :func:`feed`.\n            feed_time (float): Current time.\n        '''\n        self._bytes_transferred += data_len\n        self._collected_bytes_transferred += data_len\n\n        time_now = feed_time or time.time()\n        time_diff = time_now - self._last_feed_time\n\n        if time_diff < self._sample_min_time:\n            return\n\n        self._last_feed_time = time.time()\n\n        if data_len == 0 and time_diff >= self._stall_time:\n            self._stalled = True\n            return\n\n        self._samples.append((time_diff, self._collected_bytes_transferred))\n        self._collected_bytes_transferred = 0", "response": "Update the bandwidth meter."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef speed(self):\n        '''Return the current transfer speed.\n\n        Returns:\n            int: The speed in bytes per second.\n        '''\n        if self._stalled:\n            return 0\n\n        time_sum = 0\n        data_len_sum = 0\n\n        for time_diff, data_len in self._samples:\n            time_sum += time_diff\n            data_len_sum += data_len\n\n        if time_sum:\n            return data_len_sum / time_sum\n        else:\n            return 0", "response": "Return the current transfer speed in bytes per second."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check(self):\n        '''Check resource levels.\n\n         Returns:\n            None, ResourceInfo: If None is provided, no levels are exceeded.\n                Otherwise, the first ResourceInfo exceeding limits is returned.\n        '''\n\n        for info in self.get_info():\n            if info.free < info.limit:\n                return info", "response": "Check resource levels.\n         Returns None if no levels are exceeded."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload the response and increment the counter.", "response": "def load(self, response):\n        '''Load the response and increment the counter.\n\n        Args:\n            response (:class:`.http.request.Response`): The response from\n                a previous request.\n        '''\n        self._response = response\n\n        if self.next_location(raw=True):\n            self._num_redirects += 1"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef next_location(self, raw=False):\n        '''Returns the next location.\n\n        Args:\n            raw (bool): If True, the original string contained in the Location\n                field will be returned. Otherwise, the URL will be\n                normalized to a complete URL.\n\n        Returns:\n            str, None: If str, the location. Otherwise, no next location.\n        '''\n        if self._response:\n            location = self._response.fields.get('location')\n\n            if not location or raw:\n                return location\n\n            return wpull.url.urljoin(self._response.request.url_info.url,\n                                     location)", "response": "Returns the next location."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_redirect(self):\n        '''Return whether the response contains a redirect code.'''\n        if self._response:\n            status_code = self._response.status_code\n            return status_code in self._codes \\\n                or status_code in self._repeat_codes", "response": "Return whether the response contains a redirect code."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting all links in URL table.", "response": "def convert_all(self):\n        '''Convert all links in URL table.'''\n        for url_record in self._url_table.get_all():\n            if url_record.status != Status.done:\n                continue\n\n            self.convert_by_record(url_record)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert using given URL Record.", "response": "def convert_by_record(self, url_record):\n        '''Convert using given URL Record.'''\n        filename = url_record.filename\n\n        if not os.path.exists(filename):\n            return\n\n        if url_record.link_type:\n            if url_record.link_type not in ('css', 'html'):\n                return\n            else:\n                link_type = url_record.link_type\n        else:\n            with open(filename, 'rb') as in_file:\n                if HTMLScraper.is_supported(\n                        file=in_file, url_info=url_record.url_info):\n                    link_type = 'html'\n                elif CSSScraper.is_supported(\n                        file=in_file, url_info=url_record.url_info):\n                    link_type = 'css'\n                else:\n                    link_type = None\n\n        _logger.info(__(\n            _('Converting links in file \u2018{filename}\u2019 (type={type}).'),\n            filename=filename, type=link_type\n        ))\n\n        if self._backup_enabled:\n            shutil.copy2(filename, filename + '.orig')\n\n        temp_filename = filename + '-new'\n\n        if link_type == 'css':\n            self._css_converter.convert(\n                filename, temp_filename, base_url=url_record.url)\n        elif link_type == 'html':\n            self._html_converter.convert(\n                filename, temp_filename, base_url=url_record.url)\n        else:\n            raise Exception('Unknown link type.')\n\n        os.remove(filename)\n        os.rename(temp_filename, filename)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_chunk_header(self):\n        '''Read a single chunk's header.\n\n        Returns:\n            tuple: 2-item tuple with the size of the content in the chunk and\n            the raw header byte string.\n\n        Coroutine.\n        '''\n        # _logger.debug('Reading chunk.')\n\n        try:\n            chunk_size_hex = yield from self._connection.readline()\n        except ValueError as error:\n            raise ProtocolError(\n                'Invalid chunk size: {0}'.format(error)) from error\n\n        if not chunk_size_hex.endswith(b'\\n'):\n            raise NetworkError('Connection closed.')\n\n        try:\n            chunk_size = int(chunk_size_hex.split(b';', 1)[0].strip(), 16)\n        except ValueError as error:\n            raise ProtocolError(\n                'Invalid chunk size: {0}'.format(error)) from error\n\n        if chunk_size < 0:\n            raise ProtocolError('Chunk size cannot be negative.')\n\n        self._chunk_size = self._bytes_left = chunk_size\n\n        return chunk_size, chunk_size_hex", "response": "Read a single chunk s header."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_chunk_body(self):\n        '''Read a fragment of a single chunk.\n\n        Call :meth:`read_chunk_header` first.\n\n        Returns:\n            tuple: 2-item tuple with the content data and raw data.\n            First item is empty bytes string when chunk is fully read.\n\n        Coroutine.\n        '''\n        # chunk_size = self._chunk_size\n        bytes_left = self._bytes_left\n\n        # _logger.debug(__('Getting chunk size={0}, remain={1}.',\n        #                 chunk_size, bytes_left))\n\n        if bytes_left > 0:\n            size = min(bytes_left, self._read_size)\n            data = yield from self._connection.read(size)\n\n            self._bytes_left -= len(data)\n\n            return (data, data)\n        elif bytes_left < 0:\n            raise ProtocolError('Chunked-transfer overrun.')\n        elif bytes_left:\n            raise NetworkError('Connection closed.')\n\n        newline_data = yield from self._connection.readline()\n\n        if len(newline_data) > 2:\n            # Should be either CRLF or LF\n            # This could our problem or the server's problem\n            raise ProtocolError('Error reading newline after chunk.')\n\n        self._chunk_size = self._bytes_left = None\n\n        return (b'', newline_data)", "response": "Read a single chunk of a single file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads the HTTP trailer fields.", "response": "def read_trailer(self):\n        '''Read the HTTP trailer fields.\n\n        Returns:\n            bytes: The trailer data.\n\n        Coroutine.\n        '''\n        _logger.debug('Reading chunked trailer.')\n\n        trailer_data_list = []\n\n        while True:\n            trailer_data = yield from self._connection.readline()\n\n            trailer_data_list.append(trailer_data)\n\n            if not trailer_data.strip():\n                break\n\n        return b''.join(trailer_data_list)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a version tuple from a string.", "response": "def get_version_tuple(string):\n    '''Return a version tuple from a string.'''\n    match = re.match(r'(\\d+)\\.(\\d+)\\.?(\\d*)([abc]?)(\\d*)', string)\n    major = int(match.group(1))\n    minor = int(match.group(2))\n    patch = int(match.group(3) or 0)\n    level = RELEASE_LEVEL_MAP.get(match.group(4), 'final')\n    serial = int(match.group(5) or 0)\n\n    return major, minor, patch, level, serial"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read_input_urls(cls, session: AppSession, default_scheme='http'):\n        '''Read the URLs provided by the user.'''\n\n        url_string_iter = session.args.urls or ()\n        # FIXME: url rewriter isn't created yet\n        url_rewriter = session.factory.get('URLRewriter')\n\n        if session.args.input_file:\n            if session.args.force_html:\n                lines = cls._input_file_as_html_links(session)\n            else:\n                lines = cls._input_file_as_lines(session)\n\n            url_string_iter = itertools.chain(url_string_iter, lines)\n\n        base_url = session.args.base\n\n        for url_string in url_string_iter:\n            _logger.debug(__('Parsing URL {0}', url_string))\n\n            if base_url:\n                url_string = wpull.url.urljoin(base_url, url_string)\n\n            try:\n                url_info = wpull.url.URLInfo.parse(\n                    url_string, default_scheme=default_scheme)\n\n                _logger.debug(__('Parsed URL {0}', url_info))\n\n                if url_rewriter:\n                    # TODO: this logic should be a hook\n                    url_info = url_rewriter.rewrite(url_info)\n                    _logger.debug(__('Rewritten URL {0}', url_info))\n\n                yield url_info\n\n            except ValueError as e:\n                _logger.info(__('Invalid URL {0}: {1}', url_string, e))", "response": "Read the URLs provided by the user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread lines from input file and return them.", "response": "def _input_file_as_lines(cls, session: AppSession):\n        '''Read lines from input file and return them.'''\n        if session.args.input_file == sys.stdin:\n            input_file = session.args.input_file\n        else:\n            reader = codecs.getreader(session.args.local_encoding or 'utf-8')\n            input_file = reader(session.args.input_file)\n\n        return input_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads input file as HTML and return the links.", "response": "def _input_file_as_html_links(cls, session: AppSession):\n        '''Read input file as HTML and return the links.'''\n        scrape_result = session.factory['HTMLScraper'].scrape_file(\n            session.args.input_file,\n            encoding=session.args.local_encoding or 'utf-8'\n        )\n\n        for context in scrape_result.link_contexts:\n            yield context.link"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _new_initial_request(self, with_body: bool=True):\n        '''Return a new Request to be passed to the Web Client.'''\n        url_record = self._item_session.url_record\n        url_info = url_record.url_info\n\n        request = self._item_session.app_session.factory['WebClient'].request_factory(url_info.url)\n\n        self._populate_common_request(request)\n\n        if with_body:\n            if url_record.post_data or self._processor.fetch_params.post_data:\n                self._add_post_data(request)\n\n            if self._file_writer_session:\n                request = self._file_writer_session.process_request(request)\n\n        return request", "response": "Return a new Request to be passed to the Web Client."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npopulating the Request with common fields.", "response": "def _populate_common_request(self, request):\n        '''Populate the Request with common fields.'''\n        url_record = self._item_session.url_record\n\n        # Note that referrer may have already been set by the --referer option\n        if url_record.parent_url and not request.fields.get('Referer'):\n            self._add_referrer(request, url_record)\n\n        if self._fetch_rule.http_login:\n            request.username, request.password = self._fetch_rule.http_login"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_referrer(cls, request: Request, url_record: URLRecord):\n        '''Add referrer URL to request.'''\n        # Prohibit leak of referrer from HTTPS to HTTP\n        # rfc7231 section 5.5.2.\n        if url_record.parent_url.startswith('https://') and \\\n                url_record.url_info.scheme == 'http':\n            return\n\n        request.fields['Referer'] = url_record.parent_url", "response": "Add referrer URL to request."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _process_robots(self):\n        '''Process robots.txt.\n\n        Coroutine.\n        '''\n        try:\n            self._item_session.request = request = self._new_initial_request(with_body=False)\n            verdict, reason = (yield from self._should_fetch_reason_with_robots(\n                request))\n        except REMOTE_ERRORS as error:\n            _logger.error(\n                _('Fetching robots.txt for \u2018{url}\u2019 '\n                  'encountered an error: {error}'),\n                url=self._next_url_info.url, error=error\n            )\n            self._result_rule.handle_error(self._item_session, error)\n\n            wait_time = self._result_rule.get_wait_time(\n                self._item_session, error=error\n            )\n\n            if wait_time:\n                _logger.debug('Sleeping {0}.', wait_time)\n                yield from asyncio.sleep(wait_time)\n\n            return False\n        else:\n            _logger.debug('Robots filter verdict {} reason {}', verdict, reason)\n\n            if not verdict:\n                self._item_session.skip()\n                return False\n\n        return True", "response": "Process robots. txt.\nCoroutine.\n        Coroutine.\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _process_loop(self):\n        '''Fetch URL including redirects.\n\n        Coroutine.\n        '''\n        while not self._web_client_session.done():\n            self._item_session.request = self._web_client_session.next_request()\n\n            verdict, reason = self._should_fetch_reason()\n\n            _logger.debug('Filter verdict {} reason {}', verdict, reason)\n\n            if not verdict:\n                self._item_session.skip()\n                break\n\n            exit_early, wait_time = yield from self._fetch_one(cast(Request, self._item_session.request))\n\n            if wait_time:\n                _logger.debug('Sleeping {}', wait_time)\n                yield from asyncio.sleep(wait_time)\n\n            if exit_early:\n                break", "response": "Fetch URL including redirects. Coroutine."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _fetch_one(self, request: Request) -> Tuple[bool, float]:\n        '''Process one of the loop iteration.\n\n        Coroutine.\n\n        Returns:\n            If True, stop processing any future requests.\n        '''\n        _logger.info(_('Fetching \u2018{url}\u2019.'), url=request.url)\n\n        response = None\n\n        try:\n            response = yield from self._web_client_session.start()\n            self._item_session.response = response\n\n            action = self._result_rule.handle_pre_response(self._item_session)\n\n            if action in (Actions.RETRY, Actions.FINISH):\n                raise HookPreResponseBreak()\n\n            self._file_writer_session.process_response(response)\n\n            if not response.body:\n                response.body = Body(\n                    directory=self._item_session.app_session.root_path,\n                    hint='resp_cb'\n                )\n\n            yield from \\\n                self._web_client_session.download(\n                    file=response.body,\n                    duration_timeout=self._fetch_rule.duration_timeout\n                )\n        except HookPreResponseBreak:\n            _logger.debug('Hook pre-response break.')\n            return True, None\n        except REMOTE_ERRORS as error:\n            self._log_error(request, error)\n\n            self._result_rule.handle_error(self._item_session, error)\n            wait_time = self._result_rule.get_wait_time(\n                self._item_session, error=error\n            )\n\n            if request.body:\n                request.body.close()\n\n            if response:\n                response.body.close()\n\n            return True, wait_time\n        else:\n            self._log_response(request, response)\n            action = self._handle_response(request, response)\n            wait_time = self._result_rule.get_wait_time(self._item_session)\n\n            yield from self._run_coprocessors(request, response)\n\n            response.body.close()\n\n            if request.body:\n                request.body.close()\n\n            return action != Actions.NORMAL, wait_time", "response": "Process one of the loop iteration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the next URLInfo to be processed.", "response": "def _next_url_info(self) -> URLInfo:\n        '''Return the next URLInfo to be processed.\n\n        This returns either the original URLInfo or the next URLinfo\n        containing the redirect link.\n        '''\n        if not self._web_client_session:\n            return self._item_session.url_record.url_info\n\n        return self._web_client_session.next_request().url_info"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _should_fetch_reason(self) -> Tuple[bool, str]:\n        '''Return info about whether the URL should be fetched.\n\n        Returns:\n            tuple: A two item tuple:\n\n            1. bool: If True, the URL should be fetched.\n            2. str: A short reason string explaining the verdict.\n        '''\n        is_redirect = False\n\n        if self._strong_redirects:\n            try:\n                is_redirect = self._web_client_session.redirect_tracker\\\n                    .is_redirect()\n            except AttributeError:\n                pass\n\n        return self._fetch_rule.check_subsequent_web_request(\n            self._item_session, is_redirect=is_redirect)", "response": "Return info about whether the URL should be fetched."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _should_fetch_reason_with_robots(self, request: Request) -> Tuple[bool, str]:\n        '''Return info whether the URL should be fetched including checking\n        robots.txt.\n\n        Coroutine.\n        '''\n        result = yield from \\\n            self._fetch_rule.check_initial_web_request(self._item_session, request)\n        return result", "response": "Return info whether the URL should be fetched including checking\n            robots. txt."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _add_post_data(self, request: Request):\n        '''Add data to the payload.'''\n        if self._item_session.url_record.post_data:\n            data = wpull.string.to_bytes(self._item_session.url_record.post_data)\n        else:\n            data = wpull.string.to_bytes(\n                self._processor.fetch_params.post_data\n            )\n\n        request.method = 'POST'\n        request.fields['Content-Type'] = 'application/x-www-form-urlencoded'\n        request.fields['Content-Length'] = str(len(data))\n\n        _logger.debug('Posting with data {0}.', data)\n\n        if not request.body:\n            request.body = Body(io.BytesIO())\n\n        with wpull.util.reset_file_offset(request.body):\n            request.body.write(data)", "response": "Add data to the payload."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing the response. Returns: A value from :class:`.hook.Actions`.", "response": "def _handle_response(self, request: Request, response: Response) -> Actions:\n        '''Process the response.\n\n        Returns:\n            A value from :class:`.hook.Actions`.\n        '''\n        self._item_session.update_record_value(status_code=response.status_code)\n\n        if self._web_client_session.redirect_tracker.is_redirect() or \\\n                self._web_client_session.loop_type() == LoopType.authentication:\n            self._file_writer_session.discard_document(response)\n\n            return self._result_rule.handle_intermediate_response(\n                self._item_session\n            )\n        elif (response.status_code in self._document_codes\n              or self._processor.fetch_params.content_on_error):\n            filename = self._file_writer_session.save_document(response)\n\n            self._processing_rule.scrape_document(self._item_session)\n\n            return self._result_rule.handle_document(\n                self._item_session, filename\n            )\n        elif response.status_code in self._no_document_codes:\n            self._file_writer_session.discard_document(response)\n\n            return self._result_rule.handle_no_document(\n                self._item_session\n            )\n        else:\n            self._file_writer_session.discard_document(response)\n\n            return self._result_rule.handle_document_error(\n                self._item_session\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef int_0_inf(cls, string):\n        '''Convert string to int.\n\n        If ``inf`` is supplied, it returns ``0``.\n        '''\n        if string == 'inf':\n            return 0\n\n        try:\n            value = int(string)\n        except ValueError as error:\n            raise argparse.ArgumentTypeError(error)\n\n        if value < 0:\n            raise argparse.ArgumentTypeError(_('Value must not be negative.'))\n        else:\n            return value", "response": "Convert string to int.\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert string describing size to int.", "response": "def int_bytes(cls, string):\n        '''Convert string describing size to int.'''\n        if string[-1] in ('k', 'm'):\n            value = cls.int_0_inf(string[:-1])\n            unit = string[-1]\n            if unit == 'k':\n                value *= 2 ** 10\n            else:\n                value *= 2 ** 20\n            return value\n        else:\n            return cls.int_0_inf(string)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef comma_list(cls, string):\n        '''Convert a comma separated string to list.'''\n        items = string.split(',')\n        items = list([item.strip() for item in items])\n        return items", "response": "Convert a comma separated string to list."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a comma separated string to CommaChoiceListArgs.", "response": "def comma_choice_list(cls, string):\n        '''Convert a comma separated string to `CommaChoiceListArgs`.'''\n        items = string.split(',')\n        items = CommaChoiceListArgs([item.strip() for item in items])\n        return items"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an iterator of links found in the document.", "response": "def read_links(self, file, encoding=None):\n        '''Return an iterator of links found in the document.\n\n        Args:\n            file: A file object containing the document.\n            encoding (str): The encoding of the document.\n\n        Returns:\n            iterable: str\n        '''\n        return [item[0] for item in self.iter_text(file, encoding) if item[1]]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating the File Writer.", "response": "def _build_file_writer(cls, session: AppSession):\n        '''Create the File Writer.\n\n        Returns:\n            FileWriter: An instance of :class:`.writer.BaseFileWriter`.\n        '''\n        args = session.args\n\n        if args.delete_after:\n            return session.factory.new('FileWriter')  # is a NullWriter\n\n        elif args.output_document:\n            session.factory.class_map['FileWriter'] = SingleDocumentWriter\n            return session.factory.new('FileWriter', args.output_document,\n                                       headers_included=args.save_headers)\n\n        use_dir = (len(args.urls) != 1 or args.page_requisites\n                   or args.recursive)\n\n        if args.use_directories == 'force':\n            use_dir = True\n        elif args.use_directories == 'no':\n            use_dir = False\n\n        os_type = 'windows' if 'windows' in args.restrict_file_names \\\n            else 'unix'\n        ascii_only = 'ascii' in args.restrict_file_names\n        no_control = 'nocontrol' not in args.restrict_file_names\n\n        if 'lower' in args.restrict_file_names:\n            case = 'lower'\n        elif 'upper' in args.restrict_file_names:\n            case = 'upper'\n        else:\n            case = None\n\n        path_namer = session.factory.new(\n            'PathNamer',\n            args.directory_prefix,\n            index=args.default_page,\n            use_dir=use_dir,\n            cut=args.cut_dirs,\n            protocol=args.protocol_directories,\n            hostname=args.host_directories,\n            os_type=os_type,\n            ascii_only=ascii_only,\n            no_control=no_control,\n            case=case,\n            max_filename_length=args.max_filename_length,\n        )\n\n        if args.recursive or args.page_requisites or args.continue_download:\n            if args.clobber_method == 'disable':\n                file_class = OverwriteFileWriter\n            else:\n                file_class = IgnoreFileWriter\n        elif args.timestamping:\n            file_class = TimestampingFileWriter\n        else:\n            file_class = AntiClobberFileWriter\n\n        session.factory.class_map['FileWriter'] = file_class\n\n        return session.factory.new(\n            'FileWriter',\n            path_namer,\n            file_continuing=args.continue_download,\n            headers_included=args.save_headers,\n            local_timestamping=args.use_server_timestamps,\n            adjust_extension=args.adjust_extension,\n            content_disposition=args.content_disposition,\n            trust_server_names=args.trust_server_names,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning the application. This function is blocking.", "response": "def run_sync(self) -> int:\n        '''Run the application.\n\n        This function is blocking.\n\n        Returns:\n            int: The exit status.\n        '''\n        exit_status = asyncio.get_event_loop().run_until_complete(self.run())\n\n        # The following loop close procedure should avoid deadlock while\n        # allowing all callbacks to process before close()\n        asyncio.get_event_loop().stop()\n        asyncio.get_event_loop().run_forever()\n        asyncio.get_event_loop().close()\n        return exit_status"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _update_exit_code_from_error(self, error):\n        '''Set the exit code based on the error type.\n\n        Args:\n            error (:class:`Exception`): An exception instance.\n        '''\n        for error_type, exit_code in self.ERROR_CODE_MAP.items():\n            if isinstance(error, error_type):\n                self.update_exit_code(exit_code)\n                break\n        else:\n            self.update_exit_code(ExitStatus.generic_error)", "response": "Update the exit code based on the error type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_exit_code(self, code: int):\n        '''Set the exit code if it is serious than before.\n\n        Args:\n            code: The exit code.\n        '''\n        if code:\n            if self._exit_code:\n                self._exit_code = min(self._exit_code, code)\n            else:\n                self._exit_code = code", "response": "Set the exit code if it is serious than before."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef consult_robots_txt(self, request: HTTPRequest) -> bool:\n        '''Consult by fetching robots.txt as needed.\n\n        Args:\n            request: The request to be made\n                to get the file.\n\n        Returns:\n            True if can fetch\n\n        Coroutine\n        '''\n        if not self._robots_txt_checker:\n            return True\n\n        result = yield from self._robots_txt_checker.can_fetch(request)\n        return result", "response": "Consult by fetching robots. txt as needed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef consult_filters(self, url_info: URLInfo, url_record: URLRecord, is_redirect: bool=False) \\\n            -> Tuple[bool, str, dict]:\n        '''Consult the URL filter.\n\n        Args:\n            url_record: The URL record.\n            is_redirect: Whether the request is a redirect and it is\n                desired that it spans hosts.\n\n        Returns\n            tuple:\n\n            1. bool: The verdict\n            2. str: A short reason string: nofilters, filters, redirect\n            3. dict: The result from :func:`DemuxURLFilter.test_info`\n        '''\n        if not self._url_filter:\n            return True, 'nofilters', None\n\n        test_info = self._url_filter.test_info(url_info, url_record)\n\n        verdict = test_info['verdict']\n\n        if verdict:\n            reason = 'filters'\n        elif is_redirect and self.is_only_span_hosts_failed(test_info):\n            verdict = True\n            reason = 'redirect'\n        else:\n            reason = 'filters'\n\n        return verdict, reason, test_info", "response": "Consult the URL filter."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconsults the scripting hook. returns bool", "response": "def consult_hook(self, item_session: ItemSession, verdict: bool,\n                     reason: str, test_info: dict):\n        '''Consult the scripting hook.\n\n        Returns:\n            tuple: (bool, str)\n        '''\n        try:\n            reasons = {\n                'filters': test_info['map'],\n                'reason': reason,\n            }\n\n            verdict = self.hook_dispatcher.call(\n                PluginFunctions.accept_url, item_session, verdict, reasons,\n            )\n            reason = 'callback_hook'\n        except HookDisconnected:\n            pass\n\n        return verdict, reason"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck robots. txt URL filters and scripting hook.", "response": "def check_initial_web_request(self, item_session: ItemSession, request: HTTPRequest) -> Tuple[bool, str]:\n        '''Check robots.txt, URL filters, and scripting hook.\n\n        Returns:\n            tuple: (bool, str)\n\n        Coroutine.\n        '''\n        verdict, reason, test_info = self.consult_filters(item_session.request.url_info, item_session.url_record)\n\n        if verdict and self._robots_txt_checker:\n            can_fetch = yield from self.consult_robots_txt(request)\n\n            if not can_fetch:\n                verdict = False\n                reason = 'robotstxt'\n\n        verdict, reason = self.consult_hook(\n            item_session, verdict, reason, test_info\n        )\n\n        return verdict, reason"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking URL filters and scripting hook.", "response": "def check_subsequent_web_request(self, item_session: ItemSession,\n                                     is_redirect: bool=False) -> Tuple[bool, str]:\n        '''Check URL filters and scripting hook.\n\n        Returns:\n            tuple: (bool, str)\n        '''\n        verdict, reason, test_info = self.consult_filters(\n            item_session.request.url_info,\n            item_session.url_record, is_redirect=is_redirect)\n\n        # TODO: provide an option to change this\n        if item_session.is_virtual:\n            verdict = True\n\n        verdict, reason = self.consult_hook(item_session, verdict,\n                                            reason, test_info)\n\n        return verdict, reason"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck URL filters and scripting hook.", "response": "def check_generic_request(self, item_session: ItemSession) -> Tuple[bool, str]:\n        '''Check URL filters and scripting hook.\n\n        Returns:\n            tuple: (bool, str)\n        '''\n        verdict, reason, test_info = self.consult_filters(\n            item_session.request.url_info,\n            item_session.url_record)\n\n        verdict, reason = self.consult_hook(item_session, verdict,\n                                            reason, test_info)\n\n        return verdict, reason"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess a response that is starting.", "response": "def handle_pre_response(self, item_session: ItemSession) -> Actions:\n        '''Process a response that is starting.'''\n        action = self.consult_pre_response_hook(item_session)\n\n        if action == Actions.RETRY:\n            item_session.set_status(Status.skipped)\n        elif action == Actions.FINISH:\n            item_session.set_status(Status.done)\n        elif action == Actions.STOP:\n            raise HookStop('Script requested immediate stop.')\n\n        return action"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess a successful document response.", "response": "def handle_document(self, item_session: ItemSession, filename: str) -> Actions:\n        '''Process a successful document response.\n\n        Returns:\n            A value from :class:`.hook.Actions`.\n        '''\n        self._waiter.reset()\n\n        action = self.handle_response(item_session)\n\n        if action == Actions.NORMAL:\n            self._statistics.increment(item_session.response.body.size())\n            item_session.set_status(Status.done, filename=filename)\n\n        return action"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle_intermediate_response(self, item_session: ItemSession) -> Actions:\n        '''Callback for successful intermediate responses.\n\n        Returns:\n            A value from :class:`.hook.Actions`.\n        '''\n        self._waiter.reset()\n\n        action = self.handle_response(item_session)\n\n        return action", "response": "Callback for successful intermediate responses.\n            A value from : class :. hook. Actions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle_response(self, item_session: ItemSession) -> Actions:\n        '''Generic handler for a response.\n\n        Returns:\n            A value from :class:`.hook.Actions`.\n        '''\n        action = self.consult_response_hook(item_session)\n\n        if action == Actions.RETRY:\n            item_session.set_status(Status.error)\n        elif action == Actions.FINISH:\n            item_session.set_status(Status.done)\n        elif action == Actions.STOP:\n            raise HookStop('Script requested immediate stop.')\n\n        return action", "response": "Generic handler for a response."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_error(self, item_session: ItemSession, error: BaseException) -> Actions:\n        '''Process an error.\n\n        Returns:\n            A value from :class:`.hook.Actions`.\n        '''\n        if not self._ssl_verification and \\\n                isinstance(error, SSLVerificationError):\n            # Change it into a different error since the user doesn't care\n            # about verifying certificates\n            self._statistics.increment_error(ProtocolError())\n        else:\n            self._statistics.increment_error(error)\n\n        self._waiter.increment()\n\n        action = self.consult_error_hook(item_session, error)\n\n        if action == Actions.RETRY:\n            item_session.set_status(Status.error)\n        elif action == Actions.FINISH:\n            item_session.set_status(Status.done)\n        elif action == Actions.STOP:\n            raise HookStop('Script requested immediate stop.')\n        elif self._ssl_verification and isinstance(error, SSLVerificationError):\n            raise\n        elif isinstance(error, ConnectionRefused) and \\\n                not self.retry_connrefused:\n            item_session.set_status(Status.skipped)\n        elif isinstance(error, DNSNotFound) and \\\n                not self.retry_dns_error:\n            item_session.set_status(Status.skipped)\n        else:\n            item_session.set_status(Status.error)\n\n        return action", "response": "Process an error.\n\n        Returns:\n            A value from :class:`.hook.Actions`."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_wait_time(self, item_session: ItemSession, error=None):\n        '''Return the wait time in seconds between requests.'''\n        seconds = self._waiter.get()\n        try:\n            return self.hook_dispatcher.call(PluginFunctions.wait_time, seconds,\n                                             item_session, error)\n        except HookDisconnected:\n            return seconds", "response": "Return the wait time in seconds between requests."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the wait time between requests.", "response": "def plugin_wait_time(seconds: float, item_session: ItemSession, error: Optional[Exception]=None) -> float:\n        '''Return the wait time between requests.\n\n        Args:\n            seconds: The original time in seconds.\n            item_session:\n            error:\n\n        Returns:\n            The time in seconds.\n        '''\n        return seconds"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn scripting action when a response begins.", "response": "def consult_pre_response_hook(self, item_session: ItemSession) -> Actions:\n        '''Return scripting action when a response begins.'''\n        try:\n            return self.hook_dispatcher.call(\n                PluginFunctions.handle_pre_response,\n                item_session\n            )\n        except HookDisconnected:\n            return Actions.NORMAL"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns scripting action when a response ends.", "response": "def consult_response_hook(self, item_session: ItemSession) -> Actions:\n        '''Return scripting action when a response ends.'''\n        try:\n            return self.hook_dispatcher.call(\n                PluginFunctions.handle_response, item_session\n            )\n        except HookDisconnected:\n            return Actions.NORMAL"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef consult_error_hook(self, item_session: ItemSession, error: BaseException):\n        '''Return scripting action when an error occured.'''\n        try:\n            return self.hook_dispatcher.call(\n                PluginFunctions.handle_error, item_session, error)\n        except HookDisconnected:\n            return Actions.NORMAL", "response": "Return scripting action when an error occured."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding additional URLs such as robots. txt favicon. ico.", "response": "def add_extra_urls(self, item_session: ItemSession):\n        '''Add additional URLs such as robots.txt, favicon.ico.'''\n\n        if item_session.url_record.level == 0 and self._sitemaps:\n            extra_url_infos = (\n                self.parse_url(\n                    '{0}://{1}/robots.txt'.format(\n                        item_session.url_record.url_info.scheme,\n                        item_session.url_record.url_info.hostname_with_port)\n                ),\n                self.parse_url(\n                    '{0}://{1}/sitemap.xml'.format(\n                        item_session.url_record.url_info.scheme,\n                        item_session.url_record.url_info.hostname_with_port)\n                )\n            )\n\n            for url_info in extra_url_infos:\n                item_session.add_child_url(url_info.url)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprocessing document for links.", "response": "def scrape_document(self, item_session: ItemSession):\n        '''Process document for links.'''\n        self.event_dispatcher.notify(\n            PluginFunctions.get_urls, item_session\n        )\n\n        if not self._document_scraper:\n            return\n\n        demux_info = self._document_scraper.scrape_info(\n            item_session.request, item_session.response,\n            item_session.url_record.link_type\n        )\n\n        num_inline_urls = 0\n        num_linked_urls = 0\n\n        for scraper, scrape_result in demux_info.items():\n            new_inline, new_linked = self._process_scrape_info(\n                scraper, scrape_result, item_session\n            )\n            num_inline_urls += new_inline\n            num_linked_urls += new_linked\n\n        _logger.debug('Candidate URLs: inline={0} linked={1}',\n                      num_inline_urls, num_linked_urls\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _process_scrape_info(self, scraper: BaseScraper,\n                             scrape_result: ScrapeResult,\n                             item_session: ItemSession):\n        '''Collect the URLs from the scrape info dict.'''\n        if not scrape_result:\n            return 0, 0\n\n        num_inline = 0\n        num_linked = 0\n\n        for link_context in scrape_result.link_contexts:\n            url_info = self.parse_url(link_context.link)\n\n            if not url_info:\n                continue\n\n            url_info = self.rewrite_url(url_info)\n\n            child_url_record = item_session.child_url_record(\n                url_info.url, inline=link_context.inline\n            )\n            if not self._fetch_rule.consult_filters(item_session.request.url_info, child_url_record)[0]:\n                continue\n\n            if link_context.inline:\n                num_inline += 1\n            else:\n                num_linked += 1\n\n            item_session.add_child_url(url_info.url, inline=link_context.inline,\n                                       link_type=link_context.link_type)\n\n        return num_inline, num_linked", "response": "Collect the URLs from the scrape info dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a rewritten URL such as escaped fragment.", "response": "def rewrite_url(self, url_info: URLInfo) -> URLInfo:\n        '''Return a rewritten URL such as escaped fragment.'''\n        if self._url_rewriter:\n            return self._url_rewriter.rewrite(url_info)\n        else:\n            return url_info"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlogging exceptions during a fetch.", "response": "def _log_error(self, request, error):\n        '''Log exceptions during a fetch.'''\n        _logger.error(\n            _('Fetching \u2018{url}\u2019 encountered an error: {error}'),\n            url=request.url, error=error\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a new system to the repo.", "response": "def run_add_system(name, token, org, system, prompt):\n    \"\"\"\n    Adds a new system to the repo.\n    \"\"\"\n    repo = get_repo(token=token, org=org, name=name)\n    try:\n        repo.create_label(name=system.strip(), color=SYSTEM_LABEL_COLOR)\n        click.secho(\"Successfully added new system {}\".format(system), fg=\"green\")\n        if prompt and click.confirm(\"Run update to re-generate the page?\"):\n            run_update(name=name, token=token, org=org)\n    except GithubException as e:\n        if e.status == 422:\n            click.secho(\n                \"Unable to add new system {}, it already exists.\".format(system), fg=\"yellow\")\n            return\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run_remove_system(name, token, org, system, prompt):\n    repo = get_repo(token=token, org=org, name=name)\n    try:\n        label = repo.get_label(name=system.strip())\n        label.delete()\n        click.secho(\"Successfully deleted {}\".format(system), fg=\"green\")\n        if prompt and click.confirm(\"Run update to re-generate the page?\"):\n            run_update(name=name, token=token, org=org)\n    except UnknownObjectException:\n        click.secho(\"Unable to remove system {}, it does not exist.\".format(system), fg=\"yellow\")", "response": "Removes a system from the repo."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_config(repo):\n    files = get_files(repo)\n    config = DEFAULT_CONFIG\n    if \"config.json\" in files:\n        # get the config file, parse JSON and merge it with the default config\n        config_file = repo.get_file_contents('/config.json', ref=\"gh-pages\")\n        try:\n            repo_config = json.loads(config_file.decoded_content.decode(\"utf-8\"))\n            config.update(repo_config)\n        except ValueError:\n            click.secho(\"WARNING: Unable to parse config file. Using defaults.\", fg=\"yellow\")\n    return config", "response": "Get the config for the repo merged with the default config. Returns the default config if no config file is found."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dispatch(self, *args, **kwargs):\n        if not self.registration_allowed():\n            return HttpResponseRedirect(force_text(self.disallowed_url))\n        return super(RegistrationView, self).dispatch(*args, **kwargs)", "response": "Check that user signup is allowed before even bothering to to\n            dispatch."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_activation_key(self, user):\n        return signing.dumps(\n            obj=user.get_username(),\n            salt=REGISTRATION_SALT\n        )", "response": "Generate the activation key which will be emailed to the user."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_email_context(self, activation_key):\n        scheme = 'https' if self.request.is_secure() else 'http'\n        return {\n            'scheme': scheme,\n            'activation_key': activation_key,\n            'expiration_days': settings.ACCOUNT_ACTIVATION_DAYS,\n            'site': get_current_site(self.request)\n        }", "response": "Build the template context used for the activation email."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends the activation email to the user.", "response": "def send_activation_email(self, user):\n        \"\"\"\n        Send the activation email. The activation key is the username,\n        signed using TimestampSigner.\n\n        \"\"\"\n        activation_key = self.get_activation_key(user)\n        context = self.get_email_context(activation_key)\n        context['user'] = user\n        subject = render_to_string(\n            template_name=self.email_subject_template,\n            context=context,\n            request=self.request\n        )\n        # Force subject to a single line to avoid header-injection\n        # issues.\n        subject = ''.join(subject.splitlines())\n        message = render_to_string(\n            template_name=self.email_body_template,\n            context=context,\n            request=self.request\n        )\n        user.email_user(subject, message, settings.DEFAULT_FROM_EMAIL)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nverifies that the activation key is valid and within the activation time window. Returns the username if valid or raising ActivationError if not valid.", "response": "def validate_key(self, activation_key):\n        \"\"\"\n        Verify that the activation key is valid and within the\n        permitted activation time window, returning the username if\n        valid or raising ``ActivationError`` if not.\n\n        \"\"\"\n        try:\n            username = signing.loads(\n                activation_key,\n                salt=REGISTRATION_SALT,\n                max_age=settings.ACCOUNT_ACTIVATION_DAYS * 86400\n            )\n            return username\n        except signing.SignatureExpired:\n            raise ActivationError(\n                self.EXPIRED_MESSAGE,\n                code='expired'\n            )\n        except signing.BadSignature:\n            raise ActivationError(\n                self.INVALID_KEY_MESSAGE,\n                code='invalid_key',\n                params={'activation_key': activation_key}\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlooking up and return the user account if it exists or raise ActivationError", "response": "def get_user(self, username):\n        \"\"\"\n        Given the verified username, look up and return the\n        corresponding user account if it exists, or raising\n        ``ActivationError`` if it doesn't.\n\n        \"\"\"\n        User = get_user_model()\n        try:\n            user = User.objects.get(**{\n                User.USERNAME_FIELD: username,\n            })\n            if user.is_active:\n                raise ActivationError(\n                    self.ALREADY_ACTIVATED_MESSAGE,\n                    code='already_activated'\n                )\n            return user\n        except User.DoesNotExist:\n            raise ActivationError(\n                self.BAD_USERNAME_MESSAGE,\n                code='bad_username'\n            )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef timestamp_with_timezone(dt=None):\n    dt = dt or datetime.now()\n    if timezone is None:\n        return dt.strftime('%Y-%m-%d %H:%M%z')\n    if not dt.tzinfo:\n        tz = timezone.get_current_timezone()\n        if not tz:\n            tz = timezone.utc\n        dt = dt.replace(tzinfo=timezone.get_current_timezone())\n    return dt.strftime(\"%Y-%m-%d %H:%M%z\")", "response": "Return a timestamp with a timezone for the configured locale."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_pos(lang, project_apps=True, django_apps=False, third_party_apps=False):\n\n    paths = []\n\n    # project/locale\n    if settings.SETTINGS_MODULE:\n        parts = settings.SETTINGS_MODULE.split('.')\n    else:\n        # if settings.SETTINGS_MODULE is None, we are probably in \"test\" mode\n        # and override_settings() was used\n        # see: https://code.djangoproject.com/ticket/25911\n        parts = os.environ.get(ENVIRONMENT_VARIABLE).split('.')\n    project = __import__(parts[0], {}, {}, [])\n    abs_project_path = os.path.normpath(os.path.abspath(os.path.dirname(project.__file__)))\n    if project_apps:\n        if os.path.exists(os.path.abspath(os.path.join(os.path.dirname(project.__file__), 'locale'))):\n            paths.append(os.path.abspath(os.path.join(os.path.dirname(project.__file__), 'locale')))\n        if os.path.exists(os.path.abspath(os.path.join(os.path.dirname(project.__file__), '..', 'locale'))):\n            paths.append(os.path.abspath(os.path.join(os.path.dirname(project.__file__), '..', 'locale')))\n\n    case_sensitive_file_system = True\n    tmphandle, tmppath = tempfile.mkstemp()\n    if os.path.exists(tmppath.upper()):\n        # Case insensitive file system.\n        case_sensitive_file_system = False\n\n    # django/locale\n    if django_apps:\n        django_paths = cache.get('rosetta_django_paths')\n        if django_paths is None:\n            django_paths = []\n            for root, dirnames, filename in os.walk(os.path.abspath(os.path.dirname(django.__file__))):\n                if 'locale' in dirnames:\n                    django_paths.append(os.path.join(root, 'locale'))\n                    continue\n            cache.set('rosetta_django_paths', django_paths, 60 * 60)\n        paths = paths + django_paths\n    # settings\n    for localepath in settings.LOCALE_PATHS:\n        if os.path.isdir(localepath):\n            paths.append(localepath)\n\n    # project/app/locale\n    for app_ in apps.get_app_configs():\n        if rosetta_settings.EXCLUDED_APPLICATIONS and app_.name in rosetta_settings.EXCLUDED_APPLICATIONS:\n            continue\n\n        app_path = app_.path\n        # django apps\n        if 'contrib' in app_path and 'django' in app_path and not django_apps:\n            continue\n\n        # third party external\n        if not third_party_apps and abs_project_path not in app_path:\n            continue\n\n        # local apps\n        if not project_apps and abs_project_path in app_path:\n            continue\n\n        if os.path.exists(os.path.abspath(os.path.join(app_path, 'locale'))):\n            paths.append(os.path.abspath(os.path.join(app_path, 'locale')))\n        if os.path.exists(os.path.abspath(os.path.join(app_path, '..', 'locale'))):\n            paths.append(os.path.abspath(os.path.join(app_path, '..', 'locale')))\n\n    ret = set()\n    langs = [lang]\n    if u'-' in lang:\n        _l, _c = map(lambda x: x.lower(), lang.split(u'-', 1))\n        langs += [u'%s_%s' % (_l, _c), u'%s_%s' % (_l, _c.upper()), u'%s_%s' % (_l, _c.capitalize())]\n    elif u'_' in lang:\n        _l, _c = map(lambda x: x.lower(), lang.split(u'_', 1))\n        langs += [u'%s-%s' % (_l, _c), u'%s-%s' % (_l, _c.upper()), u'%s_%s' % (_l, _c.capitalize())]\n\n    paths = map(os.path.normpath, paths)\n    paths = list(set(paths))\n    for path in paths:\n        # Exclude paths\n        if path not in rosetta_settings.ROSETTA_EXCLUDED_PATHS:\n            for lang_ in langs:\n                dirname = os.path.join(path, lang_, 'LC_MESSAGES')\n                for fn in rosetta_settings.POFILENAMES:\n                    filename = os.path.join(dirname, fn)\n                    abs_path = os.path.abspath(filename)\n                    # On case insensitive filesystems (looking at you, MacOS)\n                    # compare the lowercase absolute path of the po file\n                    # to all lowercased paths already collected.\n                    # This is not an issue on sane filesystems\n                    if not case_sensitive_file_system:\n                        if filename.lower() in [p.lower() for p in ret]:\n                            continue\n                    if os.path.isfile(abs_path):\n                        ret.add(abs_path)\n    return list(sorted(ret))", "response": "Find the locale of a gettext catalog for a given language code."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a predicate for determining if a user can access the Rosetta views", "response": "def get_access_control_function():\n    \"\"\"\n    Return a predicate for determining if a user can\n    access the Rosetta views\n    \"\"\"\n    fn_path = getattr(settings, 'ROSETTA_ACCESS_CONTROL_FUNCTION', None)\n    if fn_path is None:\n        return is_superuser_staff_or_in_translators_group\n    # Dynamically load a permissions function\n    perm_module, perm_func = fn_path.rsplit('.', 1)\n    perm_module = importlib.import_module(perm_module)\n    return getattr(perm_module, perm_func)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef language_id(self):\n        # (Formerly known as \"rosetta_i18n_lang_code\")\n        lang_id = self.kwargs['lang_id']\n        if lang_id not in {l[0] for l in rosetta_settings.ROSETTA_LANGUAGES}:\n            raise Http404\n        if not can_translate_language(self.request.user, lang_id):\n            raise Http404\n        return lang_id", "response": "Determine the language id from the url kwargs after validating that lang_id is in ROSETTA_LANGUAGES and if the current user is allowed to translate that language."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninfers and return the path to the. po file to be shown or updated.", "response": "def po_file_path(self):\n        \"\"\"Based on the url kwargs, infer and return the path to the .po file to\n        be shown/updated.\n\n        Throw a 404 if a file isn't found.\n        \"\"\"\n        # This was formerly referred to as 'rosetta_i18n_fn'\n        idx = self.kwargs['idx']\n        idx = int(idx)  # idx matched url re expression; calling int() is safe\n\n        third_party_apps = self.po_filter in ('all', 'third-party')\n        django_apps = self.po_filter in ('all', 'django')\n        project_apps = self.po_filter in ('all', 'project')\n\n        po_paths = find_pos(self.language_id,\n                            project_apps=project_apps,\n                            django_apps=django_apps,\n                            third_party_apps=third_party_apps,\n                            )\n        po_paths.sort(key=get_app_name)\n\n        try:\n            path = po_paths[idx]\n        except IndexError:\n            raise Http404\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef po_file(self):\n        if self.po_file_is_writable:\n            # If we can write changes to file, then we pull it up fresh with\n            # each request.\n            # XXX: brittle; what if this path doesn't exist? Isn't a .po file?\n            po_file = pofile(self.po_file_path,\n                             wrapwidth=rosetta_settings.POFILE_WRAP_WIDTH)\n            for entry in po_file:\n                # Entry is an object representing a single entry in the catalog.\n                # We interate through the *entire catalog*, pasting a hashed\n                # value of the meat of each entry on its side in an attribute\n                # called \"md5hash\".\n                str_to_hash = (\n                    six.text_type(entry.msgid) +\n                    six.text_type(entry.msgstr) +\n                    six.text_type(entry.msgctxt or '')\n                ).encode('utf8')\n                entry.md5hash = hashlib.md5(str_to_hash).hexdigest()\n        else:\n            storage = get_storage(self.request)\n            po_file = storage.get(self.po_file_cache_key, None)\n            if not po_file:\n                po_file = pofile(self.po_file_path)\n                for entry in po_file:\n                    # Entry is an object representing a single entry in the\n                    # catalog. We interate through the entire catalog, pasting\n                    # a hashed value of the meat of each entry on its side in\n                    # an attribute called \"md5hash\".\n                    str_to_hash = (\n                        six.text_type(entry.msgid) +\n                        six.text_type(entry.msgstr) +\n                        six.text_type(entry.msgctxt or '')\n                    ).encode('utf8')\n                    entry.md5hash = hashlib.new('md5', str_to_hash).hexdigest()\n                storage.set(self.po_file_cache_key, po_file)\n        return po_file", "response": "Return the parsed. po file that is currently being translated or viewed."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfixes the submitted translations by filtering carriage returns and pairing newlines at the begging and end of the translated string with the original", "response": "def fix_nls(self, in_, out_):\n        \"\"\"Fixes submitted translations by filtering carriage returns and pairing\n        newlines at the begging and end of the translated string with the original\n        \"\"\"\n        if 0 == len(in_) or 0 == len(out_):\n            return out_\n\n        if \"\\r\" in out_ and \"\\r\" not in in_:\n            out_ = out_.replace(\"\\r\", '')\n\n        if \"\\n\" == in_[0] and \"\\n\" != out_[0]:\n            out_ = \"\\n\" + out_\n        elif \"\\n\" != in_[0] and \"\\n\" == out_[0]:\n            out_ = out_.lstrip()\n        if 0 == len(out_):\n            pass\n        elif \"\\n\" == in_[-1] and \"\\n\" != out_[-1]:\n            out_ = out_ + \"\\n\"\n        elif \"\\n\" != in_[-1] and \"\\n\" == out_[-1]:\n            out_ = out_.rstrip()\n        return out_"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the language id for the reference language.", "response": "def ref_lang(self):\n        \"\"\"Return the language id for the \"reference language\" (the language to\n        be translated *from*, if not English).\n\n        Throw a 404 if it's not in rosetta_settings.ROSETTA_LANGUAGES.\n        \"\"\"\n        ref_lang = self._request_request('ref_lang', 'msgid')\n        if ref_lang != 'msgid':\n            allowed_languages = {l[0] for l in rosetta_settings.ROSETTA_LANGUAGES}\n            if ref_lang not in allowed_languages:\n                raise Http404\n        return ref_lang"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ref_lang_po_file(self):\n        ref_pofile = None\n        if rosetta_settings.ENABLE_REFLANG and self.ref_lang != 'msgid':\n            replacement = '{separator}locale{separator}{ref_lang}'.format(\n                separator=os.sep,\n                ref_lang=self.ref_lang\n            )\n            pattern = '\\{separator}locale\\{separator}[a-z]{{2}}'.format(separator=os.sep)\n            ref_fn = re.sub(pattern, replacement, self.po_file_path,)\n            try:\n                ref_pofile = pofile(ref_fn)\n            except IOError:\n                # there's a syntax error in the PO file and polib can't\n                # open it. Let's just do nothing and thus display msgids.\n                # XXX: :-/\n                pass\n        return ref_pofile", "response": "Return a parsed. po file object for the reference language if one exists otherwise None."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvalidating and return msg_filter from request.", "response": "def msg_filter(self):\n        \"\"\"Validate/return msg_filter from request (e.g. 'fuzzy', 'untranslated'),\n        or a default.\n\n        If a query is also specified in the request, then return None.\n        \"\"\"\n        if self.query:\n            msg_filter = None\n        else:\n            msg_filter = self._request_request('msg_filter', 'all')\n            available_msg_filters = {'untranslated', 'translated', 'fuzzy', 'all'}\n            if msg_filter not in available_msg_filters:\n                msg_filter = 'all'\n        return msg_filter"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_entries(self):\n        if self.query:\n            # Scenario #1: terms matching a search query\n            rx = re.compile(re.escape(self.query), re.IGNORECASE)\n\n            def concat_entry(e):\n                return (six.text_type(e.msgstr) +\n                        six.text_type(e.msgid) +\n                        six.text_type(e.msgctxt) +\n                        six.text_type(e.comment) +\n                        u''.join([o[0] for o in e.occurrences]) +\n                        six.text_type(e.msgid_plural) +\n                        u''.join(e.msgstr_plural.values())\n                        )\n\n            entries = [e_ for e_ in self.po_file\n                       if not e_.obsolete and rx.search(concat_entry(e_))]\n        else:\n            # Scenario #2: filtered list of messages\n            if self.msg_filter == 'untranslated':\n                entries = self.po_file.untranslated_entries()\n            elif self.msg_filter == 'translated':\n                entries = self.po_file.translated_entries()\n            elif self.msg_filter == 'fuzzy':\n                entries = [e_ for e_ in self.po_file.fuzzy_entries()\n                           if not e_.obsolete]\n            else:\n                # (\"all\")\n                entries = [e_ for e_ in self.po_file if not e_.obsolete]\n        return entries", "response": "Return a list of the entries that would be part of the\n        current view ; that is all of the ones from this. po file matching the\n        current query or msg_filter."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert_from_gps_time(gps_time, gps_week=None):\n\n    converted_gps_time = None\n    gps_timestamp = float(gps_time)\n\n    if gps_week != None:\n\n        # image date\n        converted_gps_time = GPS_START + datetime.timedelta(seconds=int(gps_week) *\n                                                            SECS_IN_WEEK + gps_timestamp)\n\n    else:\n        # TAI scale with 1970-01-01 00:00:10 (TAI) epoch\n        os.environ['TZ'] = 'right/UTC'\n\n        # by definition\n        gps_time_as_gps = GPS_START + \\\n            datetime.timedelta(seconds=gps_timestamp)\n\n        # constant offset\n        gps_time_as_tai = gps_time_as_gps + \\\n            datetime.timedelta(seconds=19)\n        tai_epoch_as_tai = datetime.datetime(1970, 1, 1, 0, 0, 10)\n\n        # by definition\n        tai_timestamp = (gps_time_as_tai - tai_epoch_as_tai).total_seconds()\n\n        converted_gps_time = (\n            datetime.datetime.utcfromtimestamp(tai_timestamp))\n\n    # \"right\" timezone is in effect\n    return converted_gps_time", "response": "Convert GPS time in ticks to standard time."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting video duration in seconds", "response": "def get_video_duration(video_file):\n    \"\"\"Get video duration in seconds\"\"\"\n    try:\n        return float(FFProbe(video_file).video[0].duration)\n    except Exception as e:\n        print(\"could not extract duration from video {} due to {}\".format(video_file, e))\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_video_end_time(video_file):\n    if not os.path.isfile(video_file):\n        print(\"Error, video file {} does not exist\".format(video_file))\n        return None\n    try:\n        time_string = FFProbe(video_file).video[0].creation_time\n        try:\n            creation_time = datetime.datetime.strptime(\n                time_string, TIME_FORMAT)\n        except:\n            creation_time = datetime.datetime.strptime(\n                time_string, TIME_FORMAT_2)\n    except:\n        return None\n    return creation_time", "response": "Get video end time in seconds"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_video_start_time(video_file):\n    if not os.path.isfile(video_file):\n        print(\"Error, video file {} does not exist\".format(video_file))\n        return None\n    video_end_time = get_video_end_time(video_file)\n    duration = get_video_duration(video_file)\n    if video_end_time == None or duration == None:\n        return None\n    else:\n        video_start_time = (\n            video_end_time - datetime.timedelta(seconds=duration))\n        return video_start_time", "response": "Get start time in seconds"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupload local images to Mapillary.", "response": "def upload(import_path, verbose=False, skip_subfolders=False, number_threads=None, max_attempts=None, video_import_path=None, dry_run=False,api_version=1.0):\n    '''\n    Upload local images to Mapillary\n    Args:\n        import_path: Directory path to where the images are stored.\n        verbose: Print extra warnings and errors.\n        skip_subfolders: Skip images stored in subdirectories.\n\n    Returns:\n        Images are uploaded to Mapillary and flagged locally as uploaded.\n    '''\n    # sanity check if video file is passed\n    if video_import_path and (not os.path.isdir(video_import_path) and not os.path.isfile(video_import_path)):\n        print(\"Error, video path \" + video_import_path +\n              \" does not exist, exiting...\")\n        sys.exit(1)\n\n    # in case of video processing, adjust the import path\n    if video_import_path:\n        # set sampling path\n        video_sampling_path = \"mapillary_sampled_video_frames\"\n        video_dirname = video_import_path if os.path.isdir(\n            video_import_path) else os.path.dirname(video_import_path)\n        import_path = os.path.join(os.path.abspath(import_path), video_sampling_path) if import_path else os.path.join(\n            os.path.abspath(video_dirname), video_sampling_path)\n\n    # basic check for all\n    if not import_path or not os.path.isdir(import_path):\n        print(\"Error, import directory \" + import_path +\n              \" does not exist, exiting...\")\n        sys.exit(1)\n\n    # get list of file to process\n    total_file_list = uploader.get_total_file_list(\n        import_path, skip_subfolders)\n    upload_file_list = uploader.get_upload_file_list(\n        import_path, skip_subfolders)\n    failed_file_list = uploader.get_failed_upload_file_list(\n        import_path, skip_subfolders)\n    success_file_list = uploader.get_success_upload_file_list(\n        import_path, skip_subfolders)\n    to_finalize_file_list = uploader.get_finalize_file_list(\n        import_path, skip_subfolders)\n\n    if len(success_file_list) == len(total_file_list):\n        print(\"All images have already been uploaded\")\n    else:\n        if len(failed_file_list):\n            upload_failed = raw_input(\n                \"Retry uploading previously failed image uploads? [y/n]: \") if not ipc.is_enabled() else 'y'\n            # if yes, add images to the upload list\n            if upload_failed in [\"y\", \"Y\", \"yes\", \"Yes\"]:\n                upload_file_list.extend(failed_file_list)\n\n        # verify the images in the upload list, they need to have the image\n        # description and certain MAP properties\n        upload_file_list = [\n            f for f in upload_file_list if verify_mapillary_tag(f)]\n\n        if not len(upload_file_list) and not len(to_finalize_file_list):\n            print(\"No images to upload.\")\n            print('Please check if all images contain the required Mapillary metadata. If not, you can use \"mapillary_tools process\" to add them')\n            sys.exit(1)\n\n        if len(upload_file_list):\n            # get upload params for the manual upload images, group them per sequence\n            # and separate direct upload images\n            params = {}\n            list_per_sequence_mapping = {}\n            direct_upload_file_list = []\n            for image in upload_file_list:\n                log_root = uploader.log_rootpath(image)\n                upload_params_path = os.path.join(\n                    log_root, \"upload_params_process.json\")\n                if os.path.isfile(upload_params_path):\n                    with open(upload_params_path, \"rb\") as jf:\n                        params[image] = json.load(\n                            jf, object_hook=uploader.ascii_encode_dict)\n                        sequence = params[image][\"key\"]\n                        if sequence in list_per_sequence_mapping:\n                            list_per_sequence_mapping[sequence].append(image)\n                        else:\n                            list_per_sequence_mapping[sequence] = [image]\n                else:\n                    direct_upload_file_list.append(image)\n\n            # inform how many images are to be uploaded and how many are being skipped\n            # from upload\n\n            print(\"Uploading {} images with valid mapillary tags (Skipping {})\".format(\n                len(upload_file_list), len(total_file_list) - len(upload_file_list)))\n            if api_version==2.0:\n                uploder.uploadfile_list\n            if len(direct_upload_file_list):\n                uploader.upload_file_list_direct(\n                    direct_upload_file_list, number_threads, max_attempts)\n            for idx, sequence in enumerate(list_per_sequence_mapping):\n                uploader.upload_file_list_manual(\n                    list_per_sequence_mapping[sequence], params, idx, number_threads, max_attempts)\n        if len(to_finalize_file_list):\n            params = {}\n            sequences = []\n            for image in to_finalize_file_list:\n                log_root = uploader.log_rootpath(image)\n                upload_params_path = os.path.join(\n                    log_root, \"upload_params_process.json\")\n                if os.path.isfile(upload_params_path):\n                    with open(upload_params_path, \"rb\") as jf:\n                        image_params = json.load(\n                            jf, object_hook=uploader.ascii_encode_dict)\n                        sequence = image_params[\"key\"]\n                        if sequence not in sequences:\n                            params[image] = image_params\n                            sequences.append(sequence)\n            for image in params:\n                uploader.upload_done_file(**params[image])\n            uploader.flag_finalization(to_finalize_file_list)\n\n    uploader.print_summary(upload_file_list)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef format_time(time_string):\n    '''\n    Format time string with invalid time elements in hours/minutes/seconds\n    Format for the timestring needs to be \"%Y_%m_%d_%H_%M_%S\"\n\n    e.g. 2014_03_31_24_10_11 => 2014_04_01_00_10_11\n    '''\n    subseconds = False\n    data = time_string.split(\"_\")\n    hours, minutes, seconds = int(data[3]), int(data[4]), int(data[5])\n    date = datetime.datetime.strptime(\"_\".join(data[:3]), \"%Y_%m_%d\")\n    subsec = 0.0\n    if len(data) == 7:\n        if float(data[6]) != 0:\n            subsec = float(data[6]) / 10**len(data[6])\n            subseconds = True\n    date_time = date + \\\n        datetime.timedelta(hours=hours, minutes=minutes,\n                           seconds=seconds + subsec)\n    return date_time, subseconds", "response": "Format time string with invalid time elements in hours minutes and seconds"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _extract_alternative_fields(self, fields, default=None, field_type=float):\n        '''\n        Extract a value for a list of ordered fields.\n        Return the value of the first existed field in the list\n        '''\n        for field in fields:\n            if field in self.tags:\n                if field_type is float:\n                    value = eval_frac(self.tags[field].values[0])\n                if field_type is str:\n                    value = str(self.tags[field].values)\n                if field_type is int:\n                    value = int(self.tags[field].values[0])\n                return value, field\n        return default, None", "response": "Extract a value for a list of ordered fields."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef exif_name(self):\n        '''\n        Name of file in the form {lat}_{lon}_{ca}_{datetime}_{filename}_{hash}\n        '''\n        mapillary_description = json.loads(self.extract_image_description())\n\n        lat = None\n        lon = None\n        ca = None\n        date_time = None\n\n        if \"MAPLatitude\" in mapillary_description:\n            lat = mapillary_description[\"MAPLatitude\"]\n        if \"MAPLongitude\" in mapillary_description:\n            lon = mapillary_description[\"MAPLongitude\"]\n        if \"MAPCompassHeading\" in mapillary_description:\n            if 'TrueHeading' in mapillary_description[\"MAPCompassHeading\"]:\n                ca = mapillary_description[\"MAPCompassHeading\"]['TrueHeading']\n        if \"MAPCaptureTime\" in mapillary_description:\n            date_time = datetime.datetime.strptime(\n                mapillary_description[\"MAPCaptureTime\"], \"%Y_%m_%d_%H_%M_%S_%f\").strftime(\"%Y-%m-%d-%H-%M-%S-%f\")[:-3]\n\n        filename = '{}_{}_{}_{}_{}'.format(\n            lat, lon, ca, date_time, uuid.uuid4())\n        return filename", "response": "Return the name of the exif file for this image."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting altitude from the EXIF.", "response": "def extract_altitude(self):\n        '''\n        Extract altitude\n        '''\n        altitude_ref = {\n            0: 1,\n            1: -1}\n        fields = ['GPS GPSAltitude', 'EXIF GPS GPSAltitude']\n        refs = ['GPS GPSAltitudeRef', 'EXIF GPS GPSAltitudeRef']\n        altitude, _ = self._extract_alternative_fields(fields, 0, float)\n        ref = 0 if not any([True for x in refs if x in self.tags]) else [\n            self.tags[x].values for x in refs if x in self.tags][0][0]\n        return altitude * altitude_ref[ref]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extract_capture_time(self):\n        '''\n        Extract capture time from EXIF\n        return a datetime object\n        TODO: handle GPS DateTime\n        '''\n        time_string = exif_datetime_fields()[0]\n        capture_time, time_field = self._extract_alternative_fields(\n            time_string, 0, str)\n        if time_field in exif_gps_date_fields()[0]:\n            capture_time = self.extract_gps_time()\n            return capture_time\n\n        if capture_time is 0:\n            # try interpret the filename\n            try:\n                capture_time = datetime.datetime.strptime(os.path.basename(\n                    self.filename)[:-4] + '000', '%Y_%m_%d_%H_%M_%S_%f')\n            except:\n                return None\n        else:\n            capture_time = capture_time.replace(\" \", \"_\")\n            capture_time = capture_time.replace(\":\", \"_\")\n            capture_time = capture_time.replace(\".\", \"_\")\n            capture_time = capture_time.replace(\"-\", \"_\")\n            capture_time = capture_time.replace(\",\", \"_\")\n            capture_time = \"_\".join(\n                [ts for ts in capture_time.split(\"_\") if ts.isdigit()])\n            capture_time, subseconds = format_time(capture_time)\n            sub_sec = \"0\"\n            if not subseconds:\n                sub_sec = self.extract_subsec()\n            capture_time = capture_time + \\\n                datetime.timedelta(seconds=float(\"0.\" + sub_sec))\n\n        return capture_time", "response": "Extract capture time from EXIF\n            return a datetime object"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extract_direction(self):\n        '''\n        Extract image direction (i.e. compass, heading, bearing)\n        '''\n        fields = ['GPS GPSImgDirection',\n                  'EXIF GPS GPSImgDirection',\n                  'GPS GPSTrack',\n                  'EXIF GPS GPSTrack']\n        direction, _ = self._extract_alternative_fields(fields)\n\n        if direction is not None:\n            direction = normalize_bearing(direction, check_hex=True)\n        return direction", "response": "Extract image direction (i.e. compass, heading, bearing)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extract_geo(self):\n        '''\n        Extract geo-related information from exif\n        '''\n        altitude = self.extract_altitude()\n        dop = self.extract_dop()\n        lon, lat = self.extract_lon_lat()\n        d = {}\n        if lon is not None and lat is not None:\n            d['latitude'] = lat\n            d['longitude'] = lon\n        if altitude is not None:\n            d['altitude'] = altitude\n        if dop is not None:\n            d['dop'] = dop\n        return d", "response": "Extract geo - related information from exif\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract timestamp from GPS field.", "response": "def extract_gps_time(self):\n        '''\n        Extract timestamp from GPS field.\n        '''\n        gps_date_field = \"GPS GPSDate\"\n        gps_time_field = \"GPS GPSTimeStamp\"\n        gps_time = 0\n        if gps_date_field in self.tags and gps_time_field in self.tags:\n            date = str(self.tags[gps_date_field].values).split(\":\")\n            if int(date[0]) == 0 or int(date[1]) == 0 or int(date[2]) == 0:\n                return None\n            t = self.tags[gps_time_field]\n            gps_time = datetime.datetime(\n                year=int(date[0]),\n                month=int(date[1]),\n                day=int(date[2]),\n                hour=int(eval_frac(t.values[0])),\n                minute=int(eval_frac(t.values[1])),\n                second=int(eval_frac(t.values[2])),\n            )\n            microseconds = datetime.timedelta(\n                microseconds=int((eval_frac(t.values[2]) % 1) * 1e6))\n            gps_time += microseconds\n        return gps_time"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract a list of exif infos", "response": "def extract_exif(self):\n        '''\n        Extract a list of exif infos\n        '''\n        width, height = self.extract_image_size()\n        make, model = self.extract_make(), self.extract_model()\n        orientation = self.extract_orientation()\n        geo = self.extract_geo()\n        capture = self.extract_capture_time()\n        direction = self.extract_direction()\n        d = {\n            'width': width,\n            'height': height,\n            'orientation': orientation,\n            'direction': direction,\n            'make': make,\n            'model': model,\n            'capture_time': capture\n        }\n        d['gps'] = geo\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extract_image_size(self):\n        '''\n        Extract image height and width\n        '''\n        width, _ = self._extract_alternative_fields(\n            ['Image ImageWidth', 'EXIF ExifImageWidth'], -1, int)\n        height, _ = self._extract_alternative_fields(\n            ['Image ImageLength', 'EXIF ExifImageLength'], -1, int)\n        return width, height", "response": "Extract image height and width from the EXIF image."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extract_firmware(self):\n        '''\n        Extract camera firmware (tag is called 'software' in EXIF)\n        '''\n        fields = ['Image Software']\n        software, _ = self._extract_alternative_fields(\n            fields, default=\"\", field_type=str)\n        return software", "response": "Extract camera firmware (tag is called 'software' in EXIF)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extract_orientation(self):\n        '''\n        Extract image orientation\n        '''\n        fields = ['Image Orientation']\n        orientation, _ = self._extract_alternative_fields(\n            fields, default=1, field_type=int)\n        if orientation not in range(1, 9):\n            return 1\n        return orientation", "response": "Extract image orientation from the ISO."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts the SubSec of the current ISO - 8601 time from the ISO - 8601 data.", "response": "def extract_subsec(self):\n        '''\n        Extract microseconds\n        '''\n        fields = [\n            'Image SubSecTimeOriginal',\n            'EXIF SubSecTimeOriginal',\n            'Image SubSecTimeDigitized',\n            'EXIF SubSecTimeDigitized',\n            'Image SubSecTime',\n            'EXIF SubSecTime'\n        ]\n        sub_sec, _ = self._extract_alternative_fields(\n            fields, default='', field_type=str)\n        return sub_sec"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks existence of a list of fields in exif", "response": "def fields_exist(self, fields):\n        '''\n        Check existence of a list fields in exif\n        '''\n        for rexif in fields:\n            vflag = False\n            for subrexif in rexif:\n                if subrexif in self.tags:\n                    vflag = True\n            if not vflag:\n                print(\"Missing required EXIF tag: {0} for image {1}\".format(\n                    rexif[0], self.filename))\n                return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck existence of required Mapillary tags", "response": "def mapillary_tag_exists(self):\n        '''\n        Check existence of required Mapillary tags\n        '''\n        description_tag = \"Image ImageDescription\"\n        if description_tag not in self.tags:\n            return False\n        for requirement in [\"MAPSequenceUUID\", \"MAPSettingsUserKey\", \"MAPCaptureTime\", \"MAPLongitude\", \"MAPLatitude\"]:\n            if requirement not in self.tags[description_tag].values or json.loads(self.tags[description_tag].values)[requirement] in [\"\", None, \" \"]:\n                return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef query_search_api(min_lat, max_lat, min_lon, max_lon, max_results):\n    '''\n    Send query to the search API and get dict with image data.\n    '''\n\n    # Create URL\n    params = urllib.urlencode(zip(\n        ['client_id', 'bbox', 'per_page'],\n        [CLIENT_ID, ','.join([str(min_lon), str(min_lat), str(\n            max_lon), str(max_lat)]), str(max_results)]\n    ))\n    print(MAPILLARY_API_IM_SEARCH_URL + params)\n\n    # Get data from server, then parse JSON\n    query = urllib2.urlopen(MAPILLARY_API_IM_SEARCH_URL + params).read()\n    query = json.loads(query)['features']\n\n    print(\"Result: {0} images in area.\".format(len(query)))\n    return query", "response": "Send query to the search API and get dict with image data."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndownload images in query result to path.", "response": "def download_images(query, path, size=1024):\n    '''\n    Download images in query result to path.\n\n    Return list of downloaded images with lat,lon.\n    There are four sizes available: 320, 640, 1024 (default), or 2048.\n    '''\n    im_size = \"thumb-{0}.jpg\".format(size)\n    im_list = []\n\n    for im in query:\n        # Use key to create url to download from and filename to save into\n        key = im['properties']['key']\n        url = MAPILLARY_API_IM_RETRIEVE_URL + key + '/' + im_size\n        filename = key + \".jpg\"\n\n        try:\n            # Get image and save to disk\n            image = urllib.URLopener()\n            image.retrieve(url, path + filename)\n\n            # Log filename and GPS location\n            coords = \",\".join(map(str, im['geometry']['coordinates']))\n            im_list.append([filename, coords])\n\n            print(\"Successfully downloaded: {0}\".format(filename))\n        except KeyboardInterrupt:\n            break\n        except Exception as e:\n            print(\"Failed to download: {} due to {}\".format(filename, e))\n\n    return im_list"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_lat_lon_time_from_gpx(gpx_file, local_time=True):\n    '''\n    Read location and time stamps from a track in a GPX file.\n\n    Returns a list of tuples (time, lat, lon).\n\n    GPX stores time in UTC, by default we assume your camera used the local time\n    and convert accordingly.\n    '''\n    with open(gpx_file, 'r') as f:\n        gpx = gpxpy.parse(f)\n\n    points = []\n    if len(gpx.tracks) > 0:\n        for track in gpx.tracks:\n            for segment in track.segments:\n                for point in segment.points:\n                    t = utc_to_localtime(point.time) if local_time else point.time\n                    points.append((t, point.latitude, point.longitude, point.elevation))\n    if len(gpx.waypoints) > 0:\n        for point in gpx.waypoints:\n            t = utc_to_localtime(point.time) if local_time else point.time\n            points.append((t, point.latitude, point.longitude, point.elevation))\n\n    # sort by time just in case\n    points.sort()\n\n    return points", "response": "Reads location and time stamps from a GPX file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_lat_lon_time_from_nmea(nmea_file, local_time=True):\n    '''\n    Read location and time stamps from a track in a NMEA file.\n\n    Returns a list of tuples (time, lat, lon).\n\n    GPX stores time in UTC, by default we assume your camera used the local time\n    and convert accordingly.\n    '''\n    with open(nmea_file, \"r\") as f:\n        lines = f.readlines()\n        lines = [l.rstrip(\"\\n\\r\") for l in lines]\n\n    # Get initial date\n    for l in lines:\n        if \"GPRMC\" in l:\n            data = pynmea2.parse(l)\n            date = data.datetime.date()\n            break\n\n    # Parse GPS trace\n    points = []\n    for l in lines:\n        if \"GPRMC\" in l:\n            data = pynmea2.parse(l)\n            date = data.datetime.date()\n\n        if \"$GPGGA\" in l:\n            data = pynmea2.parse(l)\n            timestamp = datetime.datetime.combine(date, data.timestamp)\n            lat, lon, alt = data.latitude, data.longitude, data.altitude\n            points.append((timestamp, lat, lon, alt))\n\n    points.sort()\n    return points", "response": "Read location and time stamps from a NMEA file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ecef_from_lla(lat, lon, alt):\n    '''\n    Compute ECEF XYZ from latitude, longitude and altitude.\n\n    All using the WGS94 model.\n    Altitude is the distance to the WGS94 ellipsoid.\n    Check results here http://www.oc.nps.edu/oc2902w/coord/llhxyz.htm\n\n    '''\n    a2 = WGS84_a ** 2\n    b2 = WGS84_b ** 2\n    lat = math.radians(lat)\n    lon = math.radians(lon)\n    L = 1.0 / math.sqrt(a2 * math.cos(lat) ** 2 + b2 * math.sin(lat) ** 2)\n    x = (a2 * L + alt) * math.cos(lat) * math.cos(lon)\n    y = (a2 * L + alt) * math.cos(lat) * math.sin(lon)\n    z = (b2 * L + alt) * math.sin(lat)\n    return x, y, z", "response": "Compute ECEF XYZ from latitude longitude and altitude."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gps_distance(latlon_1, latlon_2):\n    '''\n    Distance between two (lat,lon) pairs.\n\n    >>> p1 = (42.1, -11.1)\n    >>> p2 = (42.2, -11.3)\n    >>> 19000 < gps_distance(p1, p2) < 20000\n    True\n    '''\n    x1, y1, z1 = ecef_from_lla(latlon_1[0], latlon_1[1], 0.)\n    x2, y2, z2 = ecef_from_lla(latlon_2[0], latlon_2[1], 0.)\n\n    dis = math.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2 + (z1 - z2) ** 2)\n\n    return dis", "response": "Calculate the distance between two points."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the total distance traveled of a GPS track", "response": "def get_total_distance_traveled(latlon_track):\n    '''\n    Returns the total distance traveled of a GPS track. Used to calculate whether or not the entire sequence was just stationary video\n    Takes a sequence of points as input\n    '''\n    latlon_list = []\n    # Remove timestamps from list\n    for idx, point in enumerate(latlon_track):\n        lat = latlon_track[idx][1]\n        lon = latlon_track[idx][2]\n        alt = latlon_track[idx][3]\n        latlon_list.append([lat, lon, alt])\n\n    total_distance = 0\n    last_position = latlon_list[0]\n    for position in latlon_list:\n        total_distance += gps_distance(last_position, position)\n        last_position = position\n    return total_distance"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert from degrees minutes seconds to decimal degrees.", "response": "def dms_to_decimal(degrees, minutes, seconds, hemisphere):\n    '''\n    Convert from degrees, minutes, seconds to decimal degrees.\n    @author: mprins\n    '''\n    dms = float(degrees) + float(minutes) / 60 + float(seconds) / 3600\n    if hemisphere in \"WwSs\":\n        dms = -1 * dms\n\n    return dms"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decimal_to_dms(value, precision):\n    '''\n    Convert decimal position to degrees, minutes, seconds in a fromat supported by EXIF\n    '''\n    deg = math.floor(value)\n    min = math.floor((value - deg) * 60)\n    sec = math.floor((value - deg - min / 60) * 3600 * precision)\n\n    return ((deg, 1), (min, 1), (sec, precision))", "response": "Convert decimal position to degrees minutes seconds in a fromat supported by EXIF\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting GPS coordinate in GPGGA format to degree minute second", "response": "def gpgga_to_dms(gpgga):\n    '''\n    Convert GPS coordinate in GPGGA format to degree/minute/second\n\n    Reference: http://us.cactii.net/~bb/gps.py\n    '''\n    deg_min, dmin = gpgga.split('.')\n    degrees = int(deg_min[:-2])\n    minutes = float('%s.%s' % (deg_min[-2:], dmin))\n    decimal = degrees + (minutes / 60)\n    return decimal"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the compass bearing from start to end.", "response": "def compute_bearing(start_lat, start_lon, end_lat, end_lon):\n    '''\n    Get the compass bearing from start to end.\n\n    Formula from\n    http://www.movable-type.co.uk/scripts/latlong.html\n    '''\n    # make sure everything is in radians\n    start_lat = math.radians(start_lat)\n    start_lon = math.radians(start_lon)\n    end_lat = math.radians(end_lat)\n    end_lon = math.radians(end_lon)\n\n    dLong = end_lon - start_lon\n\n    dPhi = math.log(math.tan(end_lat / 2.0 + math.pi / 4.0) /\n                    math.tan(start_lat / 2.0 + math.pi / 4.0))\n    if abs(dLong) > math.pi:\n        if dLong > 0.0:\n            dLong = -(2.0 * math.pi - dLong)\n        else:\n            dLong = (2.0 * math.pi + dLong)\n\n    y = math.sin(dLong) * math.cos(end_lat)\n    x = math.cos(start_lat) * math.sin(end_lat) - \\\n        math.sin(start_lat) * math.cos(end_lat) * math.cos(dLong)\n    bearing = (math.degrees(math.atan2(y, x)) + 360.0) % 360.0\n\n    return bearing"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing difference between two bearings", "response": "def diff_bearing(b1, b2):\n    '''\n    Compute difference between two bearings\n    '''\n    d = abs(b2 - b1)\n    d = 360 - d if d > 180 else d\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nnormalizes bearing and convert from hex if necessary", "response": "def normalize_bearing(bearing, check_hex=False):\n    '''\n    Normalize bearing and convert from hex if\n    '''\n    if bearing > 360 and check_hex:\n        # fix negative value wrongly parsed in exifread\n        # -360 degree -> 4294966935 when converting from hex\n        bearing = bin(int(bearing))[2:]\n        bearing = ''.join([str(int(int(a) == 0)) for a in bearing])\n        bearing = -float(int(bearing, 2))\n    bearing %= 360\n    return bearing"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef interpolate_lat_lon(points, t, max_dt=1):\n    '''\n    Return interpolated lat, lon and compass bearing for time t.\n\n    Points is a list of tuples (time, lat, lon, elevation), t a datetime object.\n    '''\n    # find the enclosing points in sorted list\n    if (t <= points[0][0]) or (t >= points[-1][0]):\n        if t <= points[0][0]:\n            dt = abs((points[0][0] - t).total_seconds())\n        else:\n            dt = (t - points[-1][0]).total_seconds()\n        if dt > max_dt:\n            raise ValueError(\n                \"time t not in scope of gpx file by {} seconds\".format(dt))\n        else:\n            print(\n                \"time t not in scope of gpx file by {} seconds, extrapolating...\".format(dt))\n\n        if t < points[0][0]:\n            before = points[0]\n            after = points[1]\n        else:\n            before = points[-2]\n            after = points[-1]\n        bearing = compute_bearing(before[1], before[2], after[1], after[2])\n\n        if t == points[0][0]:\n            x = points[0]\n            return (x[1], x[2], bearing, x[3])\n\n        if t == points[-1][0]:\n            x = points[-1]\n            return (x[1], x[2], bearing, x[3])\n    else:\n        for i, point in enumerate(points):\n            if t < point[0]:\n                if i > 0:\n                    before = points[i - 1]\n                else:\n                    before = points[i]\n                after = points[i]\n                break\n\n    # weight based on time\n    weight = (t - before[0]).total_seconds() / \\\n        (after[0] - before[0]).total_seconds()\n\n    # simple linear interpolation in case points are not the same\n    if before[1] == after[1]:\n        lat = before[1]\n    else:\n        lat = before[1] - weight * before[1] + weight * after[1]\n\n    if before[2] == after[2]:\n        lon = before[2]\n    else:\n        lon = before[2] - weight * before[2] + weight * after[2]\n\n    # camera angle\n    bearing = compute_bearing(before[1], before[2], after[1], after[2])\n\n    # altitude\n    if before[3] is not None:\n        ele = before[3] - weight * before[3] + weight * after[3]\n    else:\n        ele = None\n\n    return lat, lon, bearing, ele", "response": "Interpolate lat lon and compass bearing for time t."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a dict to image description.", "response": "def add_image_description(self, dict):\n        \"\"\"Add a dict to image description.\"\"\"\n        if self._ef is not None:\n            self._ef['0th'][piexif.ImageIFD.ImageDescription] = json.dumps(\n                dict)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding image orientation to image.", "response": "def add_orientation(self, orientation):\n        \"\"\"Add image orientation to image.\"\"\"\n        if not orientation in range(1, 9):\n            print_error(\n                \"Error value for orientation, value must be in range(1,9), setting to default 1\")\n            self._ef['0th'][piexif.ImageIFD.Orientation] = 1\n        else:\n            self._ef['0th'][piexif.ImageIFD.Orientation] = orientation"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_date_time_original(self, date_time, time_format='%Y:%m:%d %H:%M:%S.%f'):\n        try:\n            DateTimeOriginal = date_time.strftime(time_format)[:-3]\n            self._ef['Exif'][piexif.ExifIFD.DateTimeOriginal] = DateTimeOriginal\n        except Exception as e:\n            print_error(\"Error writing DateTimeOriginal, due to \" + str(e))", "response": "Add date time original."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_lat_lon(self, lat, lon, precision=1e7):\n        self._ef[\"GPS\"][piexif.GPSIFD.GPSLatitudeRef] = \"N\" if lat > 0 else \"S\"\n        self._ef[\"GPS\"][piexif.GPSIFD.GPSLongitudeRef] = \"E\" if lon > 0 else \"W\"\n        self._ef[\"GPS\"][piexif.GPSIFD.GPSLongitude] = decimal_to_dms(\n            abs(lon), int(precision))\n        self._ef[\"GPS\"][piexif.GPSIFD.GPSLatitude] = decimal_to_dms(\n            abs(lat), int(precision))", "response": "Add lat lon to gps ( lat in float."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_image_history(self, data):\n        self._ef['0th'][piexif.ImageIFD.ImageHistory] = json.dumps(data)", "response": "Add arbitrary string to ImageHistory tag."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_camera_make_model(self, make, model):\n        ''' Add camera make and model.'''\n        self._ef['0th'][piexif.ImageIFD.Make] = make\n        self._ef['0th'][piexif.ImageIFD.Model] = model", "response": "Add camera make and model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding altitude (pre is the precision).", "response": "def add_altitude(self, altitude, precision=100):\n        \"\"\"Add altitude (pre is the precision).\"\"\"\n        ref = 0 if altitude > 0 else 1\n        self._ef[\"GPS\"][piexif.GPSIFD.GPSAltitude] = (\n            int(abs(altitude) * precision), precision)\n        self._ef[\"GPS\"][piexif.GPSIFD.GPSAltitudeRef] = ref"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsaving exif data to file.", "response": "def write(self, filename=None):\n        \"\"\"Save exif data to file.\"\"\"\n        if filename is None:\n            filename = self._filename\n\n        exif_bytes = piexif.dump(self._ef)\n\n        with open(self._filename, \"rb\") as fin:\n            img = fin.read()\n        try:\n            piexif.insert(exif_bytes, img, filename)\n\n        except IOError:\n            type, value, traceback = sys.exc_info()\n            print >> sys.stderr, \"Error saving file:\", value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef estimate_sub_second_time(files, interval=0.0):\n    '''\n    Estimate the capture time of a sequence with sub-second precision\n    EXIF times are only given up to a second of precision. This function\n    uses the given interval between shots to estimate the time inside that\n    second that each picture was taken.\n    '''\n    if interval <= 0.0:\n        return [exif_time(f) for f in tqdm(files, desc=\"Reading image capture time\")]\n\n    onesecond = datetime.timedelta(seconds=1.0)\n    T = datetime.timedelta(seconds=interval)\n    for i, f in tqdm(enumerate(files), desc=\"Estimating subsecond time\"):\n        m = exif_time(f)\n        if not m:\n            pass\n        if i == 0:\n            smin = m\n            smax = m + onesecond\n        else:\n            m0 = m - T * i\n            smin = max(smin, m0)\n            smax = min(smax, m0 + onesecond)\n    if not smin or not smax:\n        return None\n    if smin > smax:\n        # ERROR LOG\n        print('Interval not compatible with EXIF times')\n        return None\n    else:\n        s = smin + (smax - smin) / 2\n        return [s + T * i for i in range(len(files))]", "response": "Estimate the capture time of a sequence with sub - second precision."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninterpolating time stamps in case of identical timestamps", "response": "def interpolate_timestamp(capture_times):\n    '''\n    Interpolate time stamps in case of identical timestamps\n    '''\n    timestamps = []\n    num_file = len(capture_times)\n\n    time_dict = OrderedDict()\n\n    if num_file < 2:\n        return capture_times\n\n    # trace identical timestamps (always assume capture_times is sorted)\n    time_dict = OrderedDict()\n    for i, t in enumerate(capture_times):\n        if t not in time_dict:\n            time_dict[t] = {\n                \"count\": 0,\n                \"pointer\": 0\n            }\n\n            interval = 0\n            if i != 0:\n                interval = (t - capture_times[i - 1]).total_seconds()\n                time_dict[capture_times[i - 1]][\"interval\"] = interval\n\n        time_dict[t][\"count\"] += 1\n\n    if len(time_dict) >= 2:\n        # set time interval as the last available time interval\n        time_dict[time_dict.keys()[-1]\n                  ][\"interval\"] = time_dict[time_dict.keys()[-2]][\"interval\"]\n    else:\n        # set time interval assuming capture interval is 1 second\n        time_dict[time_dict.keys()[0]][\"interval\"] = time_dict[time_dict.keys()[\n            0]][\"count\"] * 1.\n\n    # interpolate timestamps\n    for t in capture_times:\n        d = time_dict[t]\n        s = datetime.timedelta(\n            seconds=d[\"pointer\"] * d[\"interval\"] / float(d[\"count\"]))\n        updated_time = t + s\n        time_dict[t][\"pointer\"] += 1\n        timestamps.append(updated_time)\n\n    return timestamps"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_ffprobe(path):\n    '''\n    Gets information about a media file\n    TODO: use the class in ffprobe.py - why doesn't it use json output?\n    '''\n    try:\n        with open(os.devnull, 'w') as tempf:\n            subprocess.check_call(\n                ['ffprobe', '-h'], stdout=tempf, stderr=tempf)\n    except Exception as e:\n        raise IOError('ffprobe not found.')\n\n    if not os.path.isfile(path):\n        raise IOError('No such file: ' + path)\n\n    j_str = \"\"\n    try:\n        j_str = subprocess.check_output([\n            'ffprobe',\n            '-v', 'quiet',\n            '-print_format', 'json',\n            '-show_format',\n            '-show_streams',\n            path\n        ])\n    except subprocess.CalledProcessError:\n        pass\n    j_obj = json.loads(j_str)\n\n    return j_obj", "response": "Gets information about a media file using ffprobe."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract the stream from a file using ffmpeg.", "response": "def extract_stream(source, dest, stream_id):\n    '''\n    Get the data out of the file using ffmpeg\n    @param filename: mp4 filename\n    '''\n    if not os.path.isfile(source):\n        raise IOError('No such file: ' + source)\n\n    subprocess.check_output([\n        'ffmpeg',\n        '-i', source,\n        '-y',  # overwrite - potentially dangerous\n        '-nostats',\n        '-loglevel', '0',\n        '-codec', 'copy',\n        '-map', '0:' + str(stream_id),\n        '-f', 'rawvideo',\n        dest,\n    ])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nverify that the given file has the required EXIF fields.", "response": "def verify_exif(filename):\n    '''\n    Check that image file has the required EXIF fields.\n    Incompatible files will be ignored server side.\n    '''\n    # required tags in IFD name convention\n    required_exif = required_fields()\n    exif = ExifRead(filename)\n    required_exif_exist = exif.fields_exist(required_exif)\n    return required_exif_exist"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nverify that the image file has the required Mapillary tag.", "response": "def verify_mapillary_tag(filepath):\n    filepath_keep_original = processing.processed_images_rootpath(filepath)\n    if os.path.isfile(filepath_keep_original):\n        filepath = filepath_keep_original\n    '''\n    Check that image file has the required Mapillary tag\n    '''\n    return ExifRead(filepath).mapillary_tag_exists()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nam this stream labelled as an audio stream?", "response": "def isAudio(self):\n        \"\"\"\n        Is this stream labelled as an audio stream?\n        \"\"\"\n        val=False\n        if self.__dict__['codec_type']:\n            if str(self.__dict__['codec_type']) == 'audio':\n                val=True\n        return val"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nam the stream labelled as a video stream?", "response": "def isVideo(self):\n        \"\"\"\n        Is the stream labelled as a video stream.\n        \"\"\"\n        val=False\n        if self.__dict__['codec_type']:\n            if self.codec_type == 'video':\n                val=True\n        return val"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef isSubtitle(self):\n        val=False\n        if self.__dict__['codec_type']:\n            if str(self.codec_type)=='subtitle':\n                val=True\n        return val", "response": "Is the stream labelled as a subtitle stream?"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef frameSize(self):\n        size=None\n        if self.isVideo():\n            if self.__dict__['width'] and self.__dict__['height']:\n                try:\n                    size=(int(self.__dict__['width']),int(self.__dict__['height']))\n                except Exception as e:\n                    print \"None integer size %s:%s\" %(str(self.__dict__['width']),str(+self.__dict__['height']))\n                    size=(0,0)\n        return size", "response": "Returns the pixel frame size as an integer tuple ( width height"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pixelFormat(self):\n        f=None\n        if self.isVideo():\n            if self.__dict__['pix_fmt']:\n                f=self.__dict__['pix_fmt']\n        return f", "response": "Returns a string representing the pixel format of the video stream. e. g. YUV420p."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the number of frames in this video stream. Returns 0 if not a video stream. Returns 1 if not a audio stream. Returns 0 if not a video stream. Returns 0 if not a video stream. Returns 0 if not a audio stream. Returns 0 if not a video stream.", "response": "def frames(self):\n        \"\"\"\n        Returns the length of a video stream in frames. Returns 0 if not a video stream.\n        \"\"\"\n        f=0\n        if self.isVideo() or self.isAudio():\n            if self.__dict__['nb_frames']:\n                try:\n                    f=int(self.__dict__['nb_frames'])\n                except Exception as e:\n                    print \"None integer frame count\"\n        return f"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef durationSeconds(self):\n        f=0.0\n        if self.isVideo() or self.isAudio():\n            if self.__dict__['duration']:\n                try:\n                    f=float(self.__dict__['duration'])\n                except Exception as e:\n                    print \"None numeric duration\"\n        return f", "response": "Returns the runtime duration of the video stream as a floating point number of seconds."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bitrate(self):\n        b=0\n        if self.__dict__['bit_rate']:\n            try:\n                b=int(self.__dict__['bit_rate'])\n            except Exception as e:\n                print \"None integer bitrate\"\n        return b", "response": "Returns bitrate as an integer in bps\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns upload URL using new upload API", "response": "def get_upload_url(credentials):\n    '''\n    Returns upload URL using new upload API\n    '''\n    request_url = \"https://a.mapillary.com/v3/users/{}/upload_secrets?client_id={}\".format(\n        credentials[\"MAPSettingsUserKey\"], CLIENT_ID)\n    request = urllib2.Request(request_url)\n    request.add_header('Authorization', 'Bearer {}'.format(\n        credentials[\"user_upload_token\"]))\n    try:\n        response = json.loads(urllib2.urlopen(request).read())\n    except requests.exceptions.HTTPError as e:\n        print(\"Error getting upload parameters, upload could not start\")\n        sys.exit(1)\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndisplay a progress bar.", "response": "def progress(count, total, suffix=''):\n    '''\n    Display progress bar\n    sources: https://gist.github.com/vladignatyev/06860ec2040cb497f0f3\n    '''\n    bar_len = 60\n    filled_len = int(round(bar_len * count / float(total)))\n    percents = round(100.0 * count / float(total), 1)\n    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n    sys.stdout.write('[%s] %s%s %s\\r' % (bar, percents, '%', suffix))\n    sys.stdout.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nauthenticates the user by passing the email and password.", "response": "def authenticate_with_email_and_pwd(user_email, user_password):\n    '''\n    Authenticate the user by passing the email and password.\n    This function avoids prompting the command line for user credentials and is useful for calling tools programmatically\n    '''\n    if user_email is None or user_password is None:\n        raise ValueError(\n            'Could not authenticate user. Missing username or password')\n    upload_token = uploader.get_upload_token(user_email, user_password)\n    if not upload_token:\n        print(\"Authentication failed for user name \" +\n              user_name + \", please try again.\")\n        sys.exit(1)\n    user_key = get_user_key(user_name)\n    if not user_key:\n        print(\"User name {} does not exist, please try again or contact Mapillary user support.\".format(\n            user_name))\n        sys.exit(1)\n    user_permission_hash, user_signature_hash = get_user_hashes(\n        user_key, upload_token)\n\n    user_items[\"MAPSettingsUsername\"] = section\n    user_items[\"MAPSettingsUserKey\"] = user_key\n\n    user_items[\"user_upload_token\"] = upload_token\n    user_items[\"user_permission_hash\"] = user_permission_hash\n    user_items[\"user_signature_hash\"] = user_signature_hash\n    return user_items"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef upload_file(filepath, max_attempts, url, permission, signature, key=None, aws_key=None):\n    '''\n    Upload file at filepath.\n\n    '''\n    if max_attempts == None:\n        max_attempts = MAX_ATTEMPTS\n\n    filename = os.path.basename(filepath)\n\n    s3_filename = filename\n    try:\n        s3_filename = ExifRead(filepath).exif_name()\n    except:\n        pass\n\n    filepath_keep_original = processing.processed_images_rootpath(filepath)\n    filepath_in = filepath\n    if os.path.isfile(filepath_keep_original):\n        filepath = filepath_keep_original\n\n    # add S3 'path' if given\n    if key is None:\n        s3_key = s3_filename\n    else:\n        s3_key = key + s3_filename\n\n    parameters = {\"key\": s3_key, \"AWSAccessKeyId\": aws_key, \"acl\": \"private\",\n                  \"policy\": permission, \"signature\": signature, \"Content-Type\": \"image/jpeg\"}\n\n    with open(filepath, \"rb\") as f:\n        encoded_string = f.read()\n\n    data, headers = encode_multipart(\n        parameters, {'file': {'filename': filename, 'content': encoded_string}})\n    if (DRY_RUN == False):\n        displayed_upload_error = False\n        for attempt in range(max_attempts):\n            # Initialize response before each attempt\n            response = None\n            try:\n                request = urllib2.Request(url, data=data, headers=headers)\n                response = urllib2.urlopen(request)\n                if response.getcode() == 204:\n                    create_upload_log(filepath_in, \"upload_success\")\n                    if displayed_upload_error == True:\n                        print(\"Successful upload of {} on attempt {}\".format(\n                            filename, attempt))\n                else:\n                    create_upload_log(filepath_in, \"upload_failed\")\n                break  # attempts\n\n            except urllib2.HTTPError as e:\n                print(\"HTTP error: {} on {}, will attempt upload again for {} more times\".format(\n                    e, filename, max_attempts - attempt - 1))\n                displayed_upload_error = True\n                time.sleep(5)\n            except urllib2.URLError as e:\n                print(\"URL error: {} on {}, will attempt upload again for {} more times\".format(\n                    e, filename, max_attempts - attempt - 1))\n                time.sleep(5)\n            except httplib.HTTPException as e:\n                print(\"HTTP exception: {} on {}, will attempt upload again for {} more times\".format(\n                    e, filename, max_attempts - attempt - 1))\n                time.sleep(5)\n            except OSError as e:\n                print(\"OS error: {} on {}, will attempt upload again for {} more times\".format(\n                    e, filename, max_attempts - attempt - 1))\n                time.sleep(5)\n            except socket.timeout as e:\n                # Specific timeout handling for Python 2.7\n                print(\"Timeout error: {} (retrying), will attempt upload again for {} more times\".format(\n                    filename, max_attempts - attempt - 1))\n            finally:\n                if response is not None:\n                    response.close()\n    else:\n        print('DRY_RUN, Skipping actual image upload. Use this for debug only.')", "response": "Uploads a file at filepath to the specified url."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_key(key, allow_unicode_keys, key_prefix=b''):\n    if allow_unicode_keys:\n        if isinstance(key, six.text_type):\n            key = key.encode('utf8')\n    elif isinstance(key, VALID_STRING_TYPES):\n        try:\n            if isinstance(key, bytes):\n                key = key.decode().encode('ascii')\n            else:\n                key = key.encode('ascii')\n        except (UnicodeEncodeError, UnicodeDecodeError):\n            raise MemcacheIllegalInputError(\"Non-ASCII key: '%r'\" % key)\n\n    key = key_prefix + key\n    parts = key.split()\n\n    if len(key) > 250:\n        raise MemcacheIllegalInputError(\"Key is too long: '%r'\" % key)\n    # second statement catches leading or trailing whitespace\n    elif len(parts) > 1 or parts[0] != key:\n        raise MemcacheIllegalInputError(\"Key contains whitespace: '%r'\" % key)\n    elif b'\\00' in key:\n        raise MemcacheIllegalInputError(\"Key contains null: '%r'\" % key)\n\n    return key", "response": "Checks key and adds key_prefix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading a line of text from the socket and return it.", "response": "def _readline(sock, buf):\n    \"\"\"Read line of text from the socket.\n\n    Read a line of text (delimited by \"\\r\\n\") from the socket, and\n    return that line along with any trailing characters read from the\n    socket.\n\n    Args:\n        sock: Socket object, should be connected.\n        buf: String, zero or more characters, returned from an earlier\n            call to _readline or _readvalue (pass an empty string on the\n            first call).\n\n    Returns:\n      A tuple of (buf, line) where line is the full line read from the\n      socket (minus the \"\\r\\n\" characters) and buf is any trailing\n      characters read after the \"\\r\\n\" was found (which may be an empty\n      string).\n\n    \"\"\"\n    chunks = []\n    last_char = b''\n\n    while True:\n        # We're reading in chunks, so \"\\r\\n\" could appear in one chunk,\n        # or across the boundary of two chunks, so we check for both\n        # cases.\n\n        # This case must appear first, since the buffer could have\n        # later \\r\\n characters in it and we want to get the first \\r\\n.\n        if last_char == b'\\r' and buf[0:1] == b'\\n':\n            # Strip the last character from the last chunk.\n            chunks[-1] = chunks[-1][:-1]\n            return buf[1:], b''.join(chunks)\n        elif buf.find(b'\\r\\n') != -1:\n            before, sep, after = buf.partition(b\"\\r\\n\")\n            chunks.append(before)\n            return after, b''.join(chunks)\n\n        if buf:\n            chunks.append(buf)\n            last_char = buf[-1:]\n\n        buf = _recv(sock, RECV_SIZE)\n        if not buf:\n            raise MemcacheUnexpectedCloseError()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads size bytes from the socket and return them as a tuple of buf and value.", "response": "def _readvalue(sock, buf, size):\n    \"\"\"Read specified amount of bytes from the socket.\n\n    Read size bytes, followed by the \"\\r\\n\" characters, from the socket,\n    and return those bytes and any trailing bytes read after the \"\\r\\n\".\n\n    Args:\n        sock: Socket object, should be connected.\n        buf: String, zero or more characters, returned from an earlier\n            call to _readline or _readvalue (pass an empty string on the\n            first call).\n        size: Integer, number of bytes to read from the socket.\n\n    Returns:\n      A tuple of (buf, value) where value is the bytes read from the\n      socket (there will be exactly size bytes) and buf is trailing\n      characters read after the \"\\r\\n\" following the bytes (but not\n      including the \\r\\n).\n\n    \"\"\"\n    chunks = []\n    rlen = size + 2\n    while rlen - len(buf) > 0:\n        if buf:\n            rlen -= len(buf)\n            chunks.append(buf)\n        buf = _recv(sock, RECV_SIZE)\n        if not buf:\n            raise MemcacheUnexpectedCloseError()\n\n    # Now we need to remove the \\r\\n from the end. There are two cases we care\n    # about: the \\r\\n is all in the last buffer, or only the \\n is in the last\n    # buffer, and we need to remove the \\r from the penultimate buffer.\n\n    if rlen == 1:\n        # replace the last chunk with the same string minus the last character,\n        # which is always '\\r' in this case.\n        chunks[-1] = chunks[-1][:-1]\n    else:\n        # Just remove the \"\\r\\n\" from the latest chunk\n        chunks.append(buf[:rlen - 2])\n\n    return buf[rlen:], b''.join(chunks)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _recv(sock, size):\n    while True:\n        try:\n            return sock.recv(size)\n        except IOError as e:\n            if e.errno != errno.EINTR:\n                raise", "response": "recv from a socket and return a list of the size."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking key and add key_prefix.", "response": "def check_key(self, key):\n        \"\"\"Checks key and add key_prefix.\"\"\"\n        return _check_key(key, allow_unicode_keys=self.allow_unicode_keys,\n                          key_prefix=self.key_prefix)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncloses the connection to memcached if it is open.", "response": "def close(self):\n        \"\"\"Close the connection to memcached, if it is open. The next call to a\n        method that requires a connection will re-open it.\"\"\"\n        if self.sock is not None:\n            try:\n                self.sock.close()\n            except Exception:\n                pass\n            finally:\n                self.sock = None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_many(self, values, expire=0, noreply=None):\n        if noreply is None:\n            noreply = self.default_noreply\n        result = self._store_cmd(b'set', values, expire, noreply)\n        return [k for k, v in six.iteritems(result) if not v]", "response": "A convenience function for setting multiple values in a cache."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef append(self, key, value, expire=0, noreply=None):\n        if noreply is None:\n            noreply = self.default_noreply\n        return self._store_cmd(b'append', {key: value}, expire, noreply)[key]", "response": "The memcached append command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cas(self, key, value, cas, expire=0, noreply=False):\n        return self._store_cmd(b'cas', {key: value}, expire, noreply, cas)[key]", "response": "The memcached cas command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self, key, default=None):\n        return self._fetch_cmd(b'get', [key], False).get(key, default)", "response": "A convenience method for getting a key from the memcached cache."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_many(self, keys, noreply=None):\n        if not keys:\n            return True\n\n        if noreply is None:\n            noreply = self.default_noreply\n\n        cmds = []\n        for key in keys:\n            cmds.append(\n                b'delete ' + self.check_key(key) +\n                (b' noreply' if noreply else b'') +\n                b'\\r\\n')\n        self._misc_cmd(cmds, b'delete', noreply)\n        return True", "response": "A convenience function to delete multiple keys."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef incr(self, key, value, noreply=False):\n        key = self.check_key(key)\n        cmd = b'incr ' + key + b' ' + six.text_type(value).encode('ascii')\n        if noreply:\n            cmd += b' noreply'\n        cmd += b'\\r\\n'\n        results = self._misc_cmd([cmd], b'incr', noreply)\n        if noreply:\n            return None\n        if results[0] == b'NOT_FOUND':\n            return None\n        return int(results[0])", "response": "The memcached incr command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef decr(self, key, value, noreply=False):\n        key = self.check_key(key)\n        cmd = b'decr ' + key + b' ' + six.text_type(value).encode('ascii')\n        if noreply:\n            cmd += b' noreply'\n        cmd += b'\\r\\n'\n        results = self._misc_cmd([cmd], b'decr', noreply)\n        if noreply:\n            return None\n        if results[0] == b'NOT_FOUND':\n            return None\n        return int(results[0])", "response": "The memcached decr command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef touch(self, key, expire=0, noreply=None):\n        if noreply is None:\n            noreply = self.default_noreply\n        key = self.check_key(key)\n        cmd = b'touch ' + key + b' ' + six.text_type(expire).encode('ascii')\n        if noreply:\n            cmd += b' noreply'\n        cmd += b'\\r\\n'\n        results = self._misc_cmd([cmd], b'touch', noreply)\n        if noreply:\n            return True\n        return results[0] == b'TOUCHED'", "response": "The memcached touch command."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stats(self, *args):\n        result = self._fetch_cmd(b'stats', args, False)\n\n        for key, value in six.iteritems(result):\n            converter = STAT_TYPES.get(key, int)\n            try:\n                result[key] = converter(value)\n            except Exception:\n                pass\n\n        return result", "response": "The memcached stats command."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cache_memlimit(self, memlimit):\n\n        self._fetch_cmd(b'cache_memlimit', [str(int(memlimit))], False)\n        return True", "response": "This method is used to set the new memory limit for the memcached cache."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the memcached version string.", "response": "def version(self):\n        \"\"\"\n        The memcached \"version\" command.\n\n        Returns:\n            A string of the memcached version.\n        \"\"\"\n        cmd = b\"version\\r\\n\"\n        results = self._misc_cmd([cmd], b'version', False)\n        before, _, after = results[0].partition(b' ')\n\n        if before != b'VERSION':\n            raise MemcacheUnknownError(\n                \"Received unexpected response: %s\" % results[0])\n        return after"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a list from the input value.", "response": "def _create_list(value, allow_filename=False):\n    \"\"\"Create a list from the input value.\n\n    If the input is a list already, return it.\n    If the input is a comma-separated string, split it.\n\n    \"\"\"\n    if isinstance(value, list):\n        return value\n    elif isinstance(value, string_type):\n        if allow_filename and os.path.isfile(value):\n            with codecs.open(value, 'r', encoding=\"utf-8\") as handle:\n                return handle.read().splitlines()\n        return value.split(',')\n    else:\n        raise ValueError(\"Can't create list for input {}\".format(value))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if a given ncbi assembly level string matches the configured assembly levels.", "response": "def is_compatible_assembly_level(self, ncbi_assembly_level):\n        \"\"\"Check if a given ncbi assembly level string matches the configured assembly levels.\"\"\"\n        configured_ncbi_strings = [self._LEVELS[level] for level in self.assembly_level]\n        return ncbi_assembly_level in configured_ncbi_strings"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitialising configuration from kwargs.", "response": "def from_kwargs(cls, **kwargs):\n        \"\"\"Initialise configuration from kwargs.\"\"\"\n        config = cls()\n        for slot in cls.__slots__:\n\n            if slot.startswith('_'):\n                slot = slot[1:]\n            setattr(config, slot, kwargs.pop(slot, cls.get_default(slot)))\n\n        if kwargs:\n            raise ValueError(\"Unrecognized option(s): {}\".format(kwargs.keys()))\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitialising from argparser Namespace object.", "response": "def from_namespace(cls, namespace):\n        \"\"\"Initialise from argparser Namespace object.\"\"\"\n        config = cls()\n        for slot in cls.__slots__:\n            if slot.startswith('_'):\n                slot = slot[1:]\n            if not hasattr(namespace, slot):\n                continue\n            setattr(config, slot, getattr(namespace, slot))\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the default value of a given category.", "response": "def get_default(cls, category):\n        \"\"\"Get the default value of a given category.\"\"\"\n        value = cls._DEFAULTS[category]\n        if not value or not isinstance(value, list):\n            return value\n        return value[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets all available options for a category.", "response": "def get_choices(cls, category):\n        \"\"\"Get all available options for a category.\"\"\"\n        value = cls._DEFAULTS[category]\n        if not isinstance(value, list):\n            raise ValueError(\"{} does not offer choices\".format(category))\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main():\n    parser = argument_parser(version=__version__)\n    args = parser.parse_args()\n\n    if args.debug:\n        log_level = logging.DEBUG\n    elif args.verbose:\n        log_level = logging.INFO\n    else:\n        log_level = logging.WARNING\n\n    logging.basicConfig(format='%(levelname)s: %(message)s', level=log_level)\n\n    max_retries = args.retries\n    attempts = 0\n    ret = args_download(args)\n    while ret == 75 and attempts < max_retries:\n        attempts += 1\n        logging.error(\n            'Downloading from NCBI failed due to a connection error, retrying. Retries so far: %s',\n            attempts)\n        ret = args_download(args)\n\n    return ret", "response": "Build and parse command line."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(columns=None):\n    if columns is None:\n        columns = _DEFAULT_COLUMNS\n\n    global _METADATA\n    if not _METADATA:\n        _METADATA = MetaData(columns)\n\n    return _METADATA", "response": "Get or create MetaData singleton."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add(self, entry, local_file):\n        row = self.rowClass()\n\n        for key, val in entry.items():\n            if key in self.columns:\n                setattr(row, key, val)\n\n        row.local_filename = os.path.join('.', os.path.relpath(local_file))\n\n        self.rows.append(row)", "response": "Add a metadata row."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting metadata to handle.", "response": "def write(self, handle):\n        \"\"\"Write metadata to handle.\"\"\"\n        handle.write(u\"\\t\".join(self.columns))\n        handle.write(u\"\\n\")\n        for row in self.rows:\n            row.write(handle)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate the argument parser for ncbi - genome - download.", "response": "def argument_parser(version=None):\n    \"\"\"Create the argument parser for ncbi-genome-download.\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('group',\n                        default=NgdConfig.get_default('group'),\n                        help='The NCBI taxonomic group to download (default: %(default)s). '\n                        'A comma-separated list of taxonomic groups is also possible. For example: \"bacteria,viral\"'\n                        'Choose from: {choices}'.format(choices=NgdConfig.get_choices('group')))\n    parser.add_argument('-s', '--section', dest='section',\n                        choices=NgdConfig.get_choices('section'),\n                        default=NgdConfig.get_default('section'),\n                        help='NCBI section to download (default: %(default)s)')\n    parser.add_argument('-F', '--format', dest='file_format',\n                        default=NgdConfig.get_default('file_format'),\n                        help='Which format to download (default: %(default)s).'\n                        'A comma-separated list of formats is also possible. For example: \"fasta,assembly-report\". '\n                        'Choose from: {choices}'.format(choices=NgdConfig.get_choices('file_format')))\n    parser.add_argument('-l', '--assembly-level', dest='assembly_level',\n                        default=NgdConfig.get_default('assembly_level'),\n                        help='Assembly level of genomes to download (default: %(default)s). '\n                        'A comma-separated list of assembly levels is also possible. For example: \"complete,chromosome\". '\n                        'Coose from: {choices}'.format(choices=NgdConfig.get_choices('assembly_level')))\n    parser.add_argument('-g', '--genus', dest='genus',\n                        default=NgdConfig.get_default('genus'),\n                        help='Only download sequences of the provided genus. '\n                        'A comma-seperated list of genera is also possible. For example: '\n                        '\"Streptomyces coelicolor,Escherichia coli\". (default: %(default)s)')\n    parser.add_argument('-T', '--species-taxid', dest='species_taxid',\n                        default=NgdConfig.get_default('species_taxid'),\n                        help='Only download sequences of the provided species NCBI taxonomy ID. '\n                             'A comma-separated list of species taxids is also possible. For example: \"52342,12325\". '\n                             '(default: %(default)s)')\n    parser.add_argument('-t', '--taxid', dest='taxid',\n                        default=NgdConfig.get_default('taxid'),\n                        help='Only download sequences of the provided NCBI taxonomy ID. '\n                             'A comma-separated list of taxids is also possible. For example: \"9606,9685\". '\n                             '(default: %(default)s)')\n    parser.add_argument('-A', '--assembly-accessions', dest='assembly_accessions',\n                        default=NgdConfig.get_default('assembly_accessions'),\n                        help='Only download sequences matching the provided NCBI assembly accession(s). '\n                        'A comma-separated list of accessions is possible, as well as a path to a filename '\n                        'containing one accession per line.')\n    parser.add_argument('-R', '--refseq-category', dest='refseq_category',\n                        choices=NgdConfig.get_choices('refseq_category'),\n                        default=NgdConfig.get_default('refseq_category'),\n                        help='Only download sequences of the provided refseq category (default: %(default)s)')\n    parser.add_argument('-o', '--output-folder', dest='output',\n                        default=NgdConfig.get_default('output'),\n                        help='Create output hierarchy in specified folder (default: %(default)s)')\n    parser.add_argument('-H', '--human-readable', dest='human_readable', action='store_true',\n                        help='Create links in human-readable hierarchy (might fail on Windows)')\n    parser.add_argument('-u', '--uri', dest='uri',\n                        default=NgdConfig.get_default('uri'),\n                        help='NCBI base URI to use (default: %(default)s)')\n    parser.add_argument('-p', '--parallel', dest='parallel', type=int, metavar=\"N\",\n                        default=NgdConfig.get_default('parallel'),\n                        help='Run %(metavar)s downloads in parallel (default: %(default)s)')\n    parser.add_argument('-r', '--retries', dest='retries', type=int, metavar=\"N\",\n                        default=0,\n                        help='Retry download %(metavar)s times when connection to NCBI fails ('\n                             'default: %(default)s)')\n    parser.add_argument('-m', '--metadata-table', type=str,\n                        help='Save tab-delimited file with genome metadata')\n    parser.add_argument('-n', '--dry-run', dest='dry_run', action='store_true',\n                        help=\"Only check which files to download, don't download genome files.\")\n    parser.add_argument('-N', '--no-cache', dest='use_cache', action='store_false',\n                        help=\"Don't cache the assembly summary file in %s.\" % CACHE_DIR)\n    parser.add_argument('-v', '--verbose', action='store_true',\n                        help='increase output verbosity')\n    parser.add_argument('-d', '--debug', action='store_true',\n                        help='print debugging information')\n    parser.add_argument('-V', '--version', action='version', version=version,\n                        help='print version information')\n    parser.add_argument('-M', '--type-material', dest='type_material',\n                        default=NgdConfig.get_default('type_material'),\n                        help='Specifies the relation to type material for the assembly (default: %(default)s). '\n                        '\"any\" will include assemblies with no relation to type material value defined, \"all\" will download only assemblies with a defined value. '\n                        'A comma-separated list of relatons. For example: \"reference,synonym\".  '\n                        'Choose from: {choices} .  '.format(choices=NgdConfig.get_choices('type_material')))\n\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef config_download(config):\n    try:\n        download_candidates = select_candidates(config)\n\n        if len(download_candidates) < 1:\n            logging.error(\"No downloads matched your filter. Please check your options.\")\n            return 1\n\n        if config.dry_run:\n            print(\"Considering the following {} assemblies for download:\".format(len(download_candidates)))\n            for entry, _ in download_candidates:\n                print(entry['assembly_accession'], entry['organism_name'], sep=\"\\t\")\n\n            return 0\n\n        download_jobs = []\n        for entry, group in download_candidates:\n            download_jobs.extend(create_downloadjob(entry, group, config))\n\n        if config.parallel == 1:\n            for dl_job in download_jobs:\n                worker(dl_job)\n        else:  # pragma: no cover\n            # Testing multiprocessing code is annoying\n            pool = Pool(processes=config.parallel)\n            jobs = pool.map_async(worker, download_jobs)\n            try:\n                # 0xFFFF is just \"a really long time\"\n                jobs.get(0xFFFF)\n            except KeyboardInterrupt:\n                # TODO: Actually test this once I figure out how to do this in py.test\n                logging.error(\"Interrupted by user\")\n                return 1\n\n        if config.metadata_table:\n            with codecs.open(config.metadata_table, mode='w', encoding='utf-8') as handle:\n                table = metadata.get()\n                table.write(handle)\n\n    except requests.exceptions.ConnectionError as err:\n        logging.error('Download from NCBI failed: %r', err)\n        # Exit code 75 meas TEMPFAIL in C/C++, so let's stick with that for now.\n        return 75\n    return 0", "response": "Run the actual download from NCBI with parameters in a config object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef select_candidates(config):\n    download_candidates = []\n\n    for group in config.group:\n        summary_file = get_summary(config.section, group, config.uri, config.use_cache)\n        entries = parse_summary(summary_file)\n\n        for entry in filter_entries(entries, config):\n            download_candidates.append((entry, group))\n\n    return download_candidates", "response": "Select candidates to download."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef worker(job):\n    ret = False\n    try:\n        if job.full_url is not None:\n            req = requests.get(job.full_url, stream=True)\n            ret = save_and_check(req, job.local_file, job.expected_checksum)\n            if not ret:\n                return ret\n        ret = create_symlink(job.local_file, job.symlink_path)\n    except KeyboardInterrupt:  # pragma: no cover\n        # TODO: Actually test this once I figure out how to do this in py.test\n        logging.debug(\"Ignoring keyboard interrupt.\")\n\n    return ret", "response": "Run a single download job."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the assembly_summary. txt file from NCBI and return a StringIO object for it.", "response": "def get_summary(section, domain, uri, use_cache):\n    \"\"\"Get the assembly_summary.txt file from NCBI and return a StringIO object for it.\"\"\"\n    logging.debug('Checking for a cached summary file')\n\n    cachefile = \"{section}_{domain}_assembly_summary.txt\".format(section=section, domain=domain)\n    full_cachefile = os.path.join(CACHE_DIR, cachefile)\n    if use_cache and os.path.exists(full_cachefile) and \\\n       datetime.utcnow() - datetime.fromtimestamp(os.path.getmtime(full_cachefile)) < timedelta(days=1):\n        logging.info('Using cached summary.')\n        with codecs.open(full_cachefile, 'r', encoding='utf-8') as fh:\n            return StringIO(fh.read())\n\n    logging.debug('Downloading summary for %r/%r uri: %r', section, domain, uri)\n    url = '{uri}/{section}/{domain}/assembly_summary.txt'.format(\n        section=section, domain=domain, uri=uri)\n    req = requests.get(url)\n\n    if use_cache:\n        try:\n            os.makedirs(CACHE_DIR)\n        except OSError as err:\n            # Errno 17 is \"file exists\", ignore that, otherwise re-raise\n            if err.errno != 17:\n                raise\n\n        with codecs.open(full_cachefile, 'w', encoding='utf-8') as fh:\n            fh.write(req.text)\n\n    return StringIO(req.text)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating download jobs for all file formats from a summary file entry.", "response": "def create_downloadjob(entry, domain, config):\n    \"\"\"Create download jobs for all file formats from a summary file entry.\"\"\"\n    logging.info('Checking record %r', entry['assembly_accession'])\n    full_output_dir = create_dir(entry, config.section, domain, config.output)\n\n    symlink_path = None\n    if config.human_readable:\n        symlink_path = create_readable_dir(entry, config.section, domain, config.output)\n\n    checksums = grab_checksums_file(entry)\n\n    # TODO: Only write this when the checksums file changed\n    with open(os.path.join(full_output_dir, 'MD5SUMS'), 'w') as handle:\n        handle.write(checksums)\n\n    parsed_checksums = parse_checksums(checksums)\n\n    download_jobs = []\n    for fmt in config.file_format:\n        try:\n            if has_file_changed(full_output_dir, parsed_checksums, fmt):\n                download_jobs.append(\n                    download_file_job(entry, full_output_dir, parsed_checksums, fmt, symlink_path))\n            elif need_to_create_symlink(full_output_dir, parsed_checksums, fmt, symlink_path):\n                download_jobs.append(\n                    create_symlink_job(full_output_dir, parsed_checksums, fmt, symlink_path))\n        except ValueError as err:\n            logging.error(err)\n\n    return download_jobs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_dir(entry, section, domain, output):\n    full_output_dir = os.path.join(output, section, domain, entry['assembly_accession'])\n    try:\n        os.makedirs(full_output_dir)\n    except OSError as err:\n        if err.errno == errno.EEXIST and os.path.isdir(full_output_dir):\n            pass\n        else:\n            raise\n\n    return full_output_dir", "response": "Create the output directory for the entry if needed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_readable_dir(entry, section, domain, output):\n    if domain != 'viral':\n        full_output_dir = os.path.join(output, 'human_readable', section, domain,\n                                       get_genus_label(entry),\n                                       get_species_label(entry),\n                                       get_strain_label(entry))\n    else:\n        full_output_dir = os.path.join(output, 'human_readable', section, domain,\n                                       entry['organism_name'].replace(' ', '_'),\n                                       get_strain_label(entry, viral=True))\n\n    try:\n        os.makedirs(full_output_dir)\n    except OSError as err:\n        if err.errno == errno.EEXIST and os.path.isdir(full_output_dir):\n            pass\n        else:\n            raise\n\n    return full_output_dir", "response": "Create the a human - readable directory to link the entry to if needed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef grab_checksums_file(entry):\n    http_url = convert_ftp_url(entry['ftp_path'])\n    full_url = '{}/md5checksums.txt'.format(http_url)\n    req = requests.get(full_url)\n    return req.text", "response": "Grab the checksum file for a given entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_checksums(checksums_string):\n    checksums_list = []\n    for line in checksums_string.split('\\n'):\n        try:\n            # skip empty lines\n            if line == '':\n                continue\n\n            checksum, filename = line.split()\n            # strip leading ./\n            if filename.startswith('./'):\n                filename = filename[2:]\n            checksums_list.append({'checksum': checksum, 'file': filename})\n        except ValueError:\n            logging.debug('Skipping over unexpected checksum line %r', line)\n            continue\n\n    return checksums_list", "response": "Parse a file containing checksums and filenames."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the checksum of a given file has changed.", "response": "def has_file_changed(directory, checksums, filetype='genbank'):\n    \"\"\"Check if the checksum of a given file has changed.\"\"\"\n    pattern = NgdConfig.get_fileending(filetype)\n    filename, expected_checksum = get_name_and_checksum(checksums, pattern)\n    full_filename = os.path.join(directory, filename)\n    # if file doesn't exist, it has changed\n    if not os.path.isfile(full_filename):\n        return True\n\n    actual_checksum = md5sum(full_filename)\n    return expected_checksum != actual_checksum"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if we need to create a symlink for an existing file.", "response": "def need_to_create_symlink(directory, checksums, filetype, symlink_path):\n    \"\"\"Check if we need to create a symlink for an existing file.\"\"\"\n    # If we don't have a symlink path, we don't need to create a symlink\n    if symlink_path is None:\n        return False\n\n    pattern = NgdConfig.get_fileending(filetype)\n    filename, _ = get_name_and_checksum(checksums, pattern)\n    full_filename = os.path.join(directory, filename)\n    symlink_name = os.path.join(symlink_path, filename)\n\n    if os.path.islink(symlink_name):\n        existing_link = os.readlink(symlink_name)\n        if full_filename == existing_link:\n            return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract a full filename and checksum from the checksums list for a file ending in given end.", "response": "def get_name_and_checksum(checksums, end):\n    \"\"\"Extract a full filename and checksum from the checksums list for a file ending in given end.\"\"\"\n    for entry in checksums:\n        if not entry['file'].endswith(end):\n            # wrong file\n            continue\n        # workaround for ..cds_from_genomic.fna.gz and ..rna_from_genomic.fna.gz also\n        # ending in _genomic.fna.gz, causing bogus matches for the plain fasta\n        if '_from_' not in end and '_from_' in entry['file']:\n            # still the wrong file\n            continue\n        filename = entry['file']\n        expected_checksum = entry['checksum']\n        return filename, expected_checksum\n    raise ValueError('No entry for file ending in {!r}'.format(end))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef md5sum(filename):\n    hash_md5 = hashlib.md5()\n    with open(filename, 'rb') as handle:\n        for chunk in iter(lambda: handle.read(4096), b''):\n            hash_md5.update(chunk)\n    return hash_md5.hexdigest()", "response": "Calculate the md5sum of a file and return the hexdigest."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef download_file_job(entry, directory, checksums, filetype='genbank', symlink_path=None):\n    pattern = NgdConfig.get_fileending(filetype)\n    filename, expected_checksum = get_name_and_checksum(checksums, pattern)\n    base_url = convert_ftp_url(entry['ftp_path'])\n    full_url = '{}/{}'.format(base_url, filename)\n    local_file = os.path.join(directory, filename)\n    full_symlink = None\n    if symlink_path is not None:\n        full_symlink = os.path.join(symlink_path, filename)\n\n    # Keep metadata around\n    mtable = metadata.get()\n    mtable.add(entry, local_file)\n\n    return DownloadJob(full_url, local_file, expected_checksum, full_symlink)", "response": "Generate a DownloadJob that actually triggers a file download."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a symlink - creating DownloadJob for an already downloaded file.", "response": "def create_symlink_job(directory, checksums, filetype, symlink_path):\n    \"\"\"Create a symlink-creating DownloadJob for an already downloaded file.\"\"\"\n    pattern = NgdConfig.get_fileending(filetype)\n    filename, _ = get_name_and_checksum(checksums, pattern)\n    local_file = os.path.join(directory, filename)\n    full_symlink = os.path.join(symlink_path, filename)\n    return DownloadJob(None, local_file, None, full_symlink)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave the content of an http response and verify the checksum matches.", "response": "def save_and_check(response, local_file, expected_checksum):\n    \"\"\"Save the content of an http response and verify the checksum matches.\"\"\"\n    with open(local_file, 'wb') as handle:\n        for chunk in response.iter_content(4096):\n            handle.write(chunk)\n\n    actual_checksum = md5sum(local_file)\n    if actual_checksum != expected_checksum:\n        logging.error('Checksum mismatch for %r. Expected %r, got %r',\n                      local_file, expected_checksum, actual_checksum)\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_symlink(local_file, symlink_path):\n    if symlink_path is not None:\n        if os.path.exists(symlink_path) or os.path.lexists(symlink_path):\n            os.unlink(symlink_path)\n        local_file = os.path.normpath(local_file)\n        symlink_path = os.path.normpath(symlink_path)\n        num_dirs_upward = len(os.path.dirname(symlink_path).split(os.sep))\n        local_relative_to_symlink = num_dirs_upward * (os.pardir + os.sep)\n        os.symlink(os.path.join(local_relative_to_symlink, local_file),\n            symlink_path)\n\n    return True", "response": "Create a relative symbolic link if symlink path is given."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_strain_label(entry, viral=False):\n    def get_strain(entry):\n        strain = entry['infraspecific_name']\n        if strain != '':\n            strain = strain.split('=')[-1]\n            return strain\n\n        strain = entry['isolate']\n        if strain != '':\n            return strain\n\n        if len(entry['organism_name'].split(' ')) > 2 and not viral:\n            strain = ' '.join(entry['organism_name'].split(' ')[2:])\n            return strain\n\n        return entry['assembly_accession']\n\n    def cleanup(strain):\n        strain = strain.strip()\n        strain = strain.replace(' ', '_')\n        strain = strain.replace(';', '_')\n        strain = strain.replace('/', '_')\n        strain = strain.replace('\\\\', '_')\n        return strain\n\n    return cleanup(get_strain(entry))", "response": "Try to extract a strain from an assemly summary entry."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_args():\n    desc = 'Perform queries against the NCBI Taxa database'\n    epi = \"\"\"DESCRIPTION:\n    This script lets you find out what TaxIDs to pass to ngd, and will write \n    a simple one-item-per-line file to pass in to it. It utilises the ete3 \n    toolkit, so refer to their site to install the dependency if it's not \n    already satisfied.\n\n    You can query the database using a particular TaxID, or a scientific name. \n    The primary function of the script is to return all the child taxa of the\n    specified parent taxa. If specified with -v verbose flags however, the\n    script will also print out some information about the lineages etc.\n \n    WARNING: This script is still somewhat experimental\n    \"\"\"\n    parser = argparse.ArgumentParser(description=desc, epilog=epi,\n                                     formatter_class=argparse.RawTextHelpFormatter)\n    parser.add_argument('taxid', metavar='taxid', type=str,\n\t\t\thelp='A comma-separated list of TaxIDs and/or taxon names. (e.g. 561,2172)')   \n    parser.add_argument('-v', '--verbose', action='count', default=0,\n\t\t\thelp='Verbose behaviour. Supports 3 levels at present: Off = 0, Info = 1, Verbose = 2. (default: %(default)s)')\n    parser.add_argument('-d', '--database', type=str, default=None,\n\t\t\thelp='NCBI taxonomy database file path. If \"None\", it will be downloaded (default: %(default)s)')\n    parser.add_argument('-u', '--update', action='store_true', default=False,\n                        help='Update the local taxon database before querying. Recommended if not used for a while. (default: %(default)s)')\n    parser.add_argument('-j', '--just-taxids', action='store_true', default=False,\n                        help='Just write out a list of taxids an no other information (default: %(default)s)')\n    parser.add_argument('-i', '--taxon-info', action='store_true', default=False,\n                        help='Just write out rank & lineage info on the provided taxids (default: %(default)s)')\n    parser.add_argument('-o', '--outfile', action='store',\n\t\t\thelp='Output file to store the descendent TaxIDs for the query.')\n    return parser.parse_args()", "response": "Parse command line arguments for the get_child_taxa_of_ncbi_taxonomy command"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting descendent taxa for a given taxid.", "response": "def desc_taxa(taxid, ncbi, outFH, just_taxids=False):\n    \"\"\"Write descendent taxa for taxid\n    \"\"\"\n    # Main feature of the script is to get all taxa within a given group.\t\n    descendent_taxa = ncbi.get_descendant_taxa(taxid)\n    descendent_taxa_names = ncbi.translate_to_names(descendent_taxa)\n\n    if just_taxids:\n        for taxid in descendent_taxa:\n            outFH.write(str(taxid) + '\\n')\n    else:\n        for dtn, dt in zip(descendent_taxa_names, descendent_taxa):\n            x = [str(x) for x in [taxid, dt, dtn]]\n            outFH.write('\\t'.join(x) + '\\n')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite info on taxid", "response": "def taxon_info(taxid, ncbi, outFH):\n    \"\"\"Write info on taxid\n    \"\"\"\n    taxid = int(taxid)\n    tax_name = ncbi.get_taxid_translator([taxid])[taxid]\n    rank = list(ncbi.get_rank([taxid]).values())[0]\n    lineage = ncbi.get_taxid_translator(ncbi.get_lineage(taxid))\n    lineage = ['{}:{}'.format(k,v) for k,v in lineage.items()]\n    lineage = ';'.join(lineage)\n    x = [str(x) for x in [tax_name, taxid, rank, lineage]]\n    outFH.write('\\t'.join(x) + '\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef name2taxid(taxids, ncbi):\n    new_taxids = []\n    for taxid in taxids:\n        try:\n            new_taxids.append(ncbi.get_name_translator([taxid])[taxid][0])\n        except KeyError:\n            try:\n                new_taxids.append(int(taxid))\n            except ValueError:\n                msg = 'Error: cannot convert to taxid: {}'\n                raise ValueError(msg.format(taxid))\n\n    return new_taxids", "response": "Convert taxon names to taxids\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes queries against NCBI Taxa databases", "response": "def main():\n    \"\"\"Make queries against NCBI Taxa databases\n    \"\"\"\n    # Get commandline args\t\t\n    args = get_args()\n\t\n    # Instantiate the ete NCBI taxa object\n    ncbi = NCBITaxa(dbfile=args.database)\n    ## dbfile location\n    if args.verbose > 1:\n        sys.stderr.write('Taxa database is stored at {}\\n'.format(ncbi.dbfile))\n\n    # Update the database if required.\n    if args.update is True:\n        if args.verbose > 1:\n            msg = 'Updating the taxonomy database. This may take several minutes...\\n'\n            sys.stderr.write(msg)\n        ncbi.update_taxonomy_database()\n            \n    # If names were provided in taxid list, convert to taxids\n    args.taxid = args.taxid.replace('\"', '').replace(\"'\", '').split(',')\n    args.taxid = name2taxid(args.taxid, ncbi)\n\n    # Output\n    if args.outfile is None:\n        outFH = sys.stdout\n    else:\n        outFH = open(args.outfile, 'w')\n    ## header\n    if args.taxon_info:\n        outFH.write('\\t'.join(['name', 'taxid', 'rank', 'lineage']) + '\\n')\n    elif not args.just_taxids:\n        outFH.write('\\t'.join(['parent_taxid',\n                               'descendent_taxid',\n                               'descendent_name']) + '\\n')\n    ## body\n    for taxid in args.taxid:\n        if args.taxon_info:\n            taxon_info(taxid, ncbi, outFH)\n        else:\n            desc_taxa(taxid, ncbi,  outFH, args.just_taxids)\n            \n    outFH.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calcLogSumChoiceProbs(Vals, sigma):\n    '''\n    Returns the final optimal value and choice probabilities given the choice\n    specific value functions `Vals`. Probabilities are degenerate if sigma == 0.0.\n    Parameters\n    ----------\n    Vals : [numpy.array]\n        A numpy.array that holds choice specific values at common grid points.\n    sigma : float\n        A number that controls the variance of the taste shocks\n    Returns\n    -------\n    V : [numpy.array]\n        A numpy.array that holds the integrated value function.\n    P : [numpy.array]\n        A numpy.array that holds the discrete choice probabilities\n    '''\n    # Assumes that NaNs have been replaced by -numpy.inf or similar\n    if sigma == 0.0:\n        # We could construct a linear index here and use unravel_index.\n        Pflat = np.argmax(Vals, axis=0)\n\n        V = np.zeros(Vals[0].shape)\n        Probs = np.zeros(Vals.shape)\n        for i in range(Vals.shape[0]):\n            optimalIndices = Pflat == i\n            V[optimalIndices] = Vals[i][optimalIndices]\n            Probs[i][optimalIndices] = 1\n        return V, Probs\n\n    # else we have a taste shock\n    maxV = np.max(Vals, axis=0)\n\n    # calculate maxV+sigma*log(sum_i=1^J exp((V[i]-maxV))/sigma)\n    sumexp = np.sum(np.exp((Vals-maxV)/sigma), axis=0)\n    LogSumV = np.log(sumexp)\n    LogSumV = maxV + sigma*LogSumV\n\n    Probs = np.exp((Vals-LogSumV)/sigma)\n    return LogSumV, Probs", "response": "Calculates the final optimal value and choice probabilities given the choice specific value functionsVals. Probabilities are degenerate if sigma == 0. 0."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the choice probabilities given the choice specific value functions Vals and sigma.", "response": "def calcChoiceProbs(Vals, sigma):\n    '''\n    Returns the choice probabilities given the choice specific value functions\n    `Vals`. Probabilities are degenerate if sigma == 0.0.\n    Parameters\n    ----------\n    Vals : [numpy.array]\n        A numpy.array that holds choice specific values at common grid points.\n    sigma : float\n        A number that controls the variance of the taste shocks\n    Returns\n    -------\n    Probs : [numpy.array]\n        A numpy.array that holds the discrete choice probabilities\n    '''\n\n    # Assumes that NaNs have been replaced by -numpy.inf or similar\n    if sigma == 0.0:\n        # We could construct a linear index here and use unravel_index.\n        Pflat = np.argmax(Vals, axis=0)\n        Probs = np.zeros(Vals.shape)\n        for i in range(Vals.shape[0]):\n            Probs[i][Pflat==i] = 1\n        return Probs\n\n    maxV = np.max(Vals, axis=0)\n    Probs = np.divide(np.exp((Vals-maxV)/sigma), np.sum(np.exp((Vals-maxV)/sigma), axis=0))\n    return Probs"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the optimal value given the choice specific value functions Vals and the variance of the taste shocks.", "response": "def calcLogSum(Vals, sigma):\n    '''\n    Returns the optimal value given the choice specific value functions Vals.\n    Parameters\n    ----------\n    Vals : [numpy.array]\n        A numpy.array that holds choice specific values at common grid points.\n    sigma : float\n        A number that controls the variance of the taste shocks\n    Returns\n    -------\n    V : [numpy.array]\n        A numpy.array that holds the integrated value function.\n    '''\n\n    # Assumes that NaNs have been replaced by -numpy.inf or similar\n    if sigma == 0.0:\n        # We could construct a linear index here and use unravel_index.\n        V = np.amax(Vals, axis=0)\n        return V\n\n    # else we have a taste shock\n    maxV = np.max(Vals, axis=0)\n\n    # calculate maxV+sigma*log(sum_i=1^J exp((V[i]-maxV))/sigma)\n    sumexp = np.sum(np.exp((Vals-maxV)/sigma), axis=0)\n    LogSumV = np.log(sumexp)\n    LogSumV = maxV + sigma*LogSumV\n    return LogSumV"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef derivative(self,x):\n        '''\n        Evaluates the derivative of the interpolated function at the given input.\n\n        Parameters\n        ----------\n        x : np.array or float\n            Real values to be evaluated in the interpolated function.\n\n        Returns\n        -------\n        dydx : np.array or float\n            The interpolated function's first derivative evaluated at x:\n            dydx = f'(x), with the same shape as x.\n        '''\n        z = np.asarray(x)\n        return (self._der(z.flatten())).reshape(z.shape)", "response": "Evaluates the derivative of the interpolated function at the given input."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef eval_with_derivative(self,x):\n        '''\n        Evaluates the interpolated function and its derivative at the given input.\n\n        Parameters\n        ----------\n        x : np.array or float\n            Real values to be evaluated in the interpolated function.\n\n        Returns\n        -------\n        y : np.array or float\n            The interpolated function evaluated at x: y = f(x), with the same\n            shape as x.\n        dydx : np.array or float\n            The interpolated function's first derivative evaluated at x:\n            dydx = f'(x), with the same shape as x.\n        '''\n        z = np.asarray(x)\n        y, dydx = self._evalAndDer(z.flatten())\n        return y.reshape(z.shape), dydx.reshape(z.shape)", "response": "Evaluates the interpolated function and its derivative at the given input."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nevaluating the partial derivative of the interpolated function with respect to x at the given input.", "response": "def derivativeX(self,x,y):\n        '''\n        Evaluates the partial derivative of interpolated function with respect\n        to x (the first argument) at the given input.\n\n        Parameters\n        ----------\n        x : np.array or float\n            Real values to be evaluated in the interpolated function.\n        y : np.array or float\n            Real values to be evaluated in the interpolated function; must be\n            the same size as x.\n\n        Returns\n        -------\n        dfdx : np.array or float\n            The derivative of the interpolated function with respect to x, eval-\n            uated at x,y: dfdx = f_x(x,y), with the same shape as x and y.\n        '''\n        xa = np.asarray(x)\n        ya = np.asarray(y)\n        return (self._derX(xa.flatten(),ya.flatten())).reshape(xa.shape)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nevaluating the partial derivative of the interpolated function with respect to z at the given input.", "response": "def derivativeZ(self,x,y,z):\n        '''\n        Evaluates the partial derivative of the interpolated function with respect\n        to z (the third argument) at the given input.\n\n        Parameters\n        ----------\n        x : np.array or float\n            Real values to be evaluated in the interpolated function.\n        y : np.array or float\n            Real values to be evaluated in the interpolated function; must be\n            the same size as x.\n        z : np.array or float\n            Real values to be evaluated in the interpolated function; must be\n            the same size as x.\n\n        Returns\n        -------\n        dfdz : np.array or float\n            The derivative with respect to z of the interpolated function evaluated\n            at x,y,z: dfdz = f_z(x,y,z), with the same shape as x, y, and z.\n        '''\n        xa = np.asarray(x)\n        ya = np.asarray(y)\n        za = np.asarray(z)\n        return (self._derZ(xa.flatten(),ya.flatten(),za.flatten())).reshape(xa.shape)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nevaluate the partial derivative with respect to w at x y z.", "response": "def derivativeW(self,w,x,y,z):\n        '''\n        Evaluates the partial derivative with respect to w (the first argument)\n        of the interpolated function at the given input.\n\n        Parameters\n        ----------\n        w : np.array or float\n            Real values to be evaluated in the interpolated function.\n        x : np.array or float\n            Real values to be evaluated in the interpolated function; must be\n            the same size as w.\n        y : np.array or float\n            Real values to be evaluated in the interpolated function; must be\n            the same size as w.\n        z : np.array or float\n            Real values to be evaluated in the interpolated function; must be\n            the same size as w.\n\n        Returns\n        -------\n        dfdw : np.array or float\n            The derivative with respect to w of the interpolated function eval-\n            uated at w,x,y,z: dfdw = f_w(w,x,y,z), with the same shape as inputs.\n        '''\n        wa = np.asarray(w)\n        xa = np.asarray(x)\n        ya = np.asarray(y)\n        za = np.asarray(z)\n        return (self._derW(wa.flatten(),xa.flatten(),ya.flatten(),za.flatten())).reshape(wa.shape)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef derivativeY(self,w,x,y,z):\n        '''\n        Evaluates the partial derivative with respect to y (the third argument)\n        of the interpolated function at the given input.\n\n        Parameters\n        ----------\n        w : np.array or float\n            Real values to be evaluated in the interpolated function.\n        x : np.array or float\n            Real values to be evaluated in the interpolated function; must be\n            the same size as w.\n        y : np.array or float\n            Real values to be evaluated in the interpolated function; must be\n            the same size as w.\n        z : np.array or float\n            Real values to be evaluated in the interpolated function; must be\n            the same size as w.\n\n        Returns\n        -------\n        dfdy : np.array or float\n            The derivative with respect to y of the interpolated function eval-\n            uated at w,x,y,z: dfdy = f_y(w,x,y,z), with the same shape as inputs.\n        '''\n        wa = np.asarray(w)\n        xa = np.asarray(x)\n        ya = np.asarray(y)\n        za = np.asarray(z)\n        return (self._derY(wa.flatten(),xa.flatten(),ya.flatten(),za.flatten())).reshape(wa.shape)", "response": "Evaluates the partial derivative with respect to y of the interpolated function at the given input."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef derivative(self,*args):\n        '''\n        Returns the derivative of the function with respect to the first dimension.\n        '''\n        if self.i_dim == 0:\n            return np.ones_like(*args[0])\n        else:\n            return np.zeros_like(*args[0])", "response": "Returns the derivative of the function with respect to the first dimension."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the derivative of the function with respect to the X dimension.", "response": "def derivativeX(self,*args):\n        '''\n        Returns the derivative of the function with respect to the X dimension.\n        This is the first input whenever n_dims < 4 and the second input otherwise.\n        '''\n        if self.n_dims >= 4:\n            j = 1\n        else:\n            j = 0\n        if self.i_dim == j:\n            return np.ones_like(*args[0])\n        else:\n            return np.zeros_like(*args[0])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef derivativeW(self,*args):\n        '''\n        Returns the derivative of the function with respect to the W dimension.\n        This should only exist when n_dims >= 4.\n        '''\n        if self.n_dims >= 4:\n            j = 0\n        else:\n            assert False, \"Derivative with respect to W can't be called when n_dims < 4!\"\n        if self.i_dim == j:\n            return np.ones_like(*args[0])\n        else:\n            return np.zeros_like(*args[0])", "response": "Returns the derivative of the function with respect to the W dimension."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _der(self,*args):\n        '''\n        Evaluate the derivative of the function.  The first input must exist and should be an array.\n        Returns an array of identical shape to args[0] (if it exists).  This is an array of zeros.\n        '''\n        if len(args) > 0:\n            if _isscalar(args[0]):\n                return 0.0\n            else:\n                shape = args[0].shape\n                return np.zeros(shape)\n        else:\n            return 0.0", "response": "Evaluate the derivative of the function."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _evalOrDer(self,x,_eval,_Der):\n        '''\n        Returns the level and/or first derivative of the function at each value in\n        x.  Only called internally by HARKinterpolator1D.eval_and_der (etc).\n\n        Parameters\n        ----------\n        x_list : scalar or np.array\n            Set of points where we want to evlauate the interpolated function and/or its derivative..\n        _eval : boolean\n            Indicator for whether to evalute the level of the interpolated function.\n        _Der : boolean\n            Indicator for whether to evaluate the derivative of the interpolated function.\n\n        Returns\n        -------\n        A list including the level and/or derivative of the interpolated function where requested.\n        '''\n\n\n\n        i      = np.maximum(np.searchsorted(self.x_list[:-1],x),1)\n        alpha  = (x-self.x_list[i-1])/(self.x_list[i]-self.x_list[i-1])\n\n        if _eval:\n                y = (1.-alpha)*self.y_list[i-1] + alpha*self.y_list[i]\n        if _Der:\n                dydx = (self.y_list[i] - self.y_list[i-1])/(self.x_list[i] - self.x_list[i-1])\n\n        if not self.lower_extrap:\n            below_lower_bound = x < self.x_list[0]\n\n            if _eval:\n                y[below_lower_bound] = np.nan\n            if _Der:\n                dydx[below_lower_bound] = np.nan\n\n        if self.decay_extrap:\n            above_upper_bound = x > self.x_list[-1]\n            x_temp = x[above_upper_bound] - self.x_list[-1]\n\n            if _eval:\n                y[above_upper_bound] = self.intercept_limit + \\\n                                       self.slope_limit*x[above_upper_bound] - \\\n                                       self.decay_extrap_A*np.exp(-self.decay_extrap_B*x_temp)\n\n            if _Der:\n                dydx[above_upper_bound] = self.slope_limit + \\\n                                          self.decay_extrap_B*self.decay_extrap_A*\\\n                                          np.exp(-self.decay_extrap_B*x_temp)\n\n        output = []\n        if _eval:\n            output += [y,]\n        if _Der:\n            output += [dydx,]\n\n        return output", "response": "This is a private method that returns the level and derivative of the interpolated function at each value in x."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nevaluating the interpolated function at each value in x.", "response": "def _evaluate(self,x,return_indices = False):\n        '''\n        Returns the level of the interpolated function at each value in x.  Only\n        called internally by HARKinterpolator1D.__call__ (etc).\n        '''\n        return self._evalOrDer(x,True,False)[0]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nevaluating the function at each value in x and return the level and first derivative of the function at each value in x.", "response": "def _evalAndDer(self,x):\n        '''\n        Returns the level and first derivative of the function at each value in\n        x.  Only called internally by HARKinterpolator1D.eval_and_der (etc).\n        '''\n        y,dydx = self._evalOrDer(x,True,True)\n\n        return y,dydx"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the first derivative of the interpolated function at each value in x. Only called internally by HARKinterpolator1D. derivative.", "response": "def _der(self,x):\n        '''\n        Returns the first derivative of the interpolated function at each value\n        in x. Only called internally by HARKinterpolator1D.derivative (etc).\n        '''\n        if _isscalar(x):\n            pos = np.searchsorted(self.x_list,x)\n            if pos == 0:\n                dydx = self.coeffs[0,1]\n            elif (pos < self.n):\n                alpha = (x - self.x_list[pos-1])/(self.x_list[pos] - self.x_list[pos-1])\n                dydx = (self.coeffs[pos,1] + alpha*(2*self.coeffs[pos,2] + alpha*3*self.coeffs[pos,3]))/(self.x_list[pos] - self.x_list[pos-1])\n            else:\n                alpha = x - self.x_list[self.n-1]\n                dydx = self.coeffs[pos,1] - self.coeffs[pos,2]*self.coeffs[pos,3]*np.exp(alpha*self.coeffs[pos,3])\n        else:\n            m = len(x)\n            pos = np.searchsorted(self.x_list,x)\n            dydx = np.zeros(m)\n            if dydx.size > 0:\n                out_bot   = pos == 0\n                out_top   = pos == self.n\n                in_bnds   = np.logical_not(np.logical_or(out_bot, out_top))\n\n                # Do the \"in bounds\" evaluation points\n                i = pos[in_bnds]\n                coeffs_in = self.coeffs[i,:]\n                alpha = (x[in_bnds] - self.x_list[i-1])/(self.x_list[i] - self.x_list[i-1])\n                dydx[in_bnds] = (coeffs_in[:,1] + alpha*(2*coeffs_in[:,2] + alpha*3*coeffs_in[:,3]))/(self.x_list[i] - self.x_list[i-1])\n\n                # Do the \"out of bounds\" evaluation points\n                dydx[out_bot] = self.coeffs[0,1]\n                alpha = x[out_top] - self.x_list[self.n-1]\n                dydx[out_top] = self.coeffs[self.n,1] - self.coeffs[self.n,2]*self.coeffs[self.n,3]*np.exp(alpha*self.coeffs[self.n,3])\n        return dydx"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _evalAndDer(self,x):\n        '''\n        Returns the level and first derivative of the function at each value in\n        x.  Only called internally by HARKinterpolator1D.eval_and_der (etc).\n        '''\n        if _isscalar(x):\n            pos = np.searchsorted(self.x_list,x)\n            if pos == 0:\n                y = self.coeffs[0,0] + self.coeffs[0,1]*(x - self.x_list[0])\n                dydx = self.coeffs[0,1]\n            elif (pos < self.n):\n                alpha = (x - self.x_list[pos-1])/(self.x_list[pos] - self.x_list[pos-1])\n                y = self.coeffs[pos,0] + alpha*(self.coeffs[pos,1] + alpha*(self.coeffs[pos,2] + alpha*self.coeffs[pos,3]))\n                dydx = (self.coeffs[pos,1] + alpha*(2*self.coeffs[pos,2] + alpha*3*self.coeffs[pos,3]))/(self.x_list[pos] - self.x_list[pos-1])\n            else:\n                alpha = x - self.x_list[self.n-1]\n                y = self.coeffs[pos,0] + x*self.coeffs[pos,1] - self.coeffs[pos,2]*np.exp(alpha*self.coeffs[pos,3])\n                dydx = self.coeffs[pos,1] - self.coeffs[pos,2]*self.coeffs[pos,3]*np.exp(alpha*self.coeffs[pos,3])\n        else:\n            m = len(x)\n            pos = np.searchsorted(self.x_list,x)\n            y = np.zeros(m)\n            dydx = np.zeros(m)\n            if y.size > 0:\n                out_bot   = pos == 0\n                out_top   = pos == self.n\n                in_bnds   = np.logical_not(np.logical_or(out_bot, out_top))\n\n                # Do the \"in bounds\" evaluation points\n                i = pos[in_bnds]\n                coeffs_in = self.coeffs[i,:]\n                alpha = (x[in_bnds] - self.x_list[i-1])/(self.x_list[i] - self.x_list[i-1])\n                y[in_bnds] = coeffs_in[:,0] + alpha*(coeffs_in[:,1] + alpha*(coeffs_in[:,2] + alpha*coeffs_in[:,3]))\n                dydx[in_bnds] = (coeffs_in[:,1] + alpha*(2*coeffs_in[:,2] + alpha*3*coeffs_in[:,3]))/(self.x_list[i] - self.x_list[i-1])\n\n                # Do the \"out of bounds\" evaluation points\n                y[out_bot] = self.coeffs[0,0] + self.coeffs[0,1]*(x[out_bot] - self.x_list[0])\n                dydx[out_bot] = self.coeffs[0,1]\n                alpha = x[out_top] - self.x_list[self.n-1]\n                y[out_top] = self.coeffs[self.n,0] + x[out_top]*self.coeffs[self.n,1] - self.coeffs[self.n,2]*np.exp(alpha*self.coeffs[self.n,3])\n                dydx[out_top] = self.coeffs[self.n,1] - self.coeffs[self.n,2]*self.coeffs[self.n,3]*np.exp(alpha*self.coeffs[self.n,3])\n        return y, dydx", "response": "Returns the level and first derivative of the function at each value in\n            x. Only called internally by HARKinterpolator1D. eval_and_der."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _evaluate(self,x,y):\n        '''\n        Returns the level of the interpolated function at each value in x,y.\n        Only called internally by HARKinterpolator2D.__call__ (etc).\n        '''\n        if _isscalar(x):\n            x_pos = max(min(self.xSearchFunc(self.x_list,x),self.x_n-1),1)\n            y_pos = max(min(self.ySearchFunc(self.y_list,y),self.y_n-1),1)\n        else:\n            x_pos = self.xSearchFunc(self.x_list,x)\n            x_pos[x_pos < 1] = 1\n            x_pos[x_pos > self.x_n-1] = self.x_n-1\n            y_pos = self.ySearchFunc(self.y_list,y)\n            y_pos[y_pos < 1] = 1\n            y_pos[y_pos > self.y_n-1] = self.y_n-1\n        alpha = (x - self.x_list[x_pos-1])/(self.x_list[x_pos] - self.x_list[x_pos-1])\n        beta = (y - self.y_list[y_pos-1])/(self.y_list[y_pos] - self.y_list[y_pos-1])\n        f = (\n             (1-alpha)*(1-beta)*self.f_values[x_pos-1,y_pos-1]\n          +  (1-alpha)*beta*self.f_values[x_pos-1,y_pos]\n          +  alpha*(1-beta)*self.f_values[x_pos,y_pos-1]\n          +  alpha*beta*self.f_values[x_pos,y_pos])\n        return f", "response": "Evaluate the interpolated function at each value in x y."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _derX(self,x,y):\n        '''\n        Returns the derivative with respect to x of the interpolated function\n        at each value in x,y. Only called internally by HARKinterpolator2D.derivativeX.\n        '''\n        if _isscalar(x):\n            x_pos = max(min(self.xSearchFunc(self.x_list,x),self.x_n-1),1)\n            y_pos = max(min(self.ySearchFunc(self.y_list,y),self.y_n-1),1)\n        else:\n            x_pos = self.xSearchFunc(self.x_list,x)\n            x_pos[x_pos < 1] = 1\n            x_pos[x_pos > self.x_n-1] = self.x_n-1\n            y_pos = self.ySearchFunc(self.y_list,y)\n            y_pos[y_pos < 1] = 1\n            y_pos[y_pos > self.y_n-1] = self.y_n-1\n        beta = (y - self.y_list[y_pos-1])/(self.y_list[y_pos] - self.y_list[y_pos-1])\n        dfdx = (\n              ((1-beta)*self.f_values[x_pos,y_pos-1]\n            +  beta*self.f_values[x_pos,y_pos]) -\n              ((1-beta)*self.f_values[x_pos-1,y_pos-1]\n            +  beta*self.f_values[x_pos-1,y_pos]))/(self.x_list[x_pos] - self.x_list[x_pos-1])\n        return dfdx", "response": "Returns the derivative with respect to x of the interpolated function\n            at each value in x y. Only called internally by HARKinterpolator2D. derivativeX."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _derY(self,x,y):\n        '''\n        Returns the derivative with respect to y of the interpolated function\n        at each value in x,y. Only called internally by HARKinterpolator2D.derivativeY.\n        '''\n        if _isscalar(x):\n            x_pos = max(min(self.xSearchFunc(self.x_list,x),self.x_n-1),1)\n            y_pos = max(min(self.ySearchFunc(self.y_list,y),self.y_n-1),1)\n        else:\n            x_pos = self.xSearchFunc(self.x_list,x)\n            x_pos[x_pos < 1] = 1\n            x_pos[x_pos > self.x_n-1] = self.x_n-1\n            y_pos = self.ySearchFunc(self.y_list,y)\n            y_pos[y_pos < 1] = 1\n            y_pos[y_pos > self.y_n-1] = self.y_n-1\n        alpha = (x - self.x_list[x_pos-1])/(self.x_list[x_pos] - self.x_list[x_pos-1])\n        dfdy = (\n              ((1-alpha)*self.f_values[x_pos-1,y_pos]\n            +  alpha*self.f_values[x_pos,y_pos]) -\n              ((1-alpha)*self.f_values[x_pos-1,y_pos-1]\n            +  alpha*self.f_values[x_pos,y_pos-1]))/(self.y_list[y_pos] - self.y_list[y_pos-1])\n        return dfdy", "response": "Returns the derivative with respect to y of the interpolated function\n            at each value in x y. Only called internally by HARKinterpolator2D. derivativeY."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the derivative with respect to x at each value in x y z. Only called internally by HARKinterpolator3D. derivativeX.", "response": "def _derX(self,x,y,z):\n        '''\n        Returns the derivative with respect to x of the interpolated function\n        at each value in x,y,z. Only called internally by HARKinterpolator3D.derivativeX.\n        '''\n        if _isscalar(x):\n            x_pos = max(min(self.xSearchFunc(self.x_list,x),self.x_n-1),1)\n            y_pos = max(min(self.ySearchFunc(self.y_list,y),self.y_n-1),1)\n            z_pos = max(min(self.zSearchFunc(self.z_list,z),self.z_n-1),1)\n        else:\n            x_pos = self.xSearchFunc(self.x_list,x)\n            x_pos[x_pos < 1] = 1\n            x_pos[x_pos > self.x_n-1] = self.x_n-1\n            y_pos = self.ySearchFunc(self.y_list,y)\n            y_pos[y_pos < 1] = 1\n            y_pos[y_pos > self.y_n-1] = self.y_n-1\n            z_pos = self.zSearchFunc(self.z_list,z)\n            z_pos[z_pos < 1] = 1\n            z_pos[z_pos > self.z_n-1] = self.z_n-1\n        beta = (y - self.y_list[y_pos-1])/(self.y_list[y_pos] - self.y_list[y_pos-1])\n        gamma = (z - self.z_list[z_pos-1])/(self.z_list[z_pos] - self.z_list[z_pos-1])\n        dfdx = (\n           (  (1-beta)*(1-gamma)*self.f_values[x_pos,y_pos-1,z_pos-1]\n           +  (1-beta)*gamma*self.f_values[x_pos,y_pos-1,z_pos]\n           +  beta*(1-gamma)*self.f_values[x_pos,y_pos,z_pos-1]\n           +  beta*gamma*self.f_values[x_pos,y_pos,z_pos]) -\n           (  (1-beta)*(1-gamma)*self.f_values[x_pos-1,y_pos-1,z_pos-1]\n           +  (1-beta)*gamma*self.f_values[x_pos-1,y_pos-1,z_pos]\n           +  beta*(1-gamma)*self.f_values[x_pos-1,y_pos,z_pos-1]\n           +  beta*gamma*self.f_values[x_pos-1,y_pos,z_pos]))/(self.x_list[x_pos] - self.x_list[x_pos-1])\n        return dfdx"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nevaluate the interpolated function at each value in x y z.", "response": "def _evaluate(self,w,x,y,z):\n        '''\n        Returns the level of the interpolated function at each value in x,y,z.\n        Only called internally by HARKinterpolator4D.__call__ (etc).\n        '''\n        if _isscalar(w):\n            w_pos = max(min(self.wSearchFunc(self.w_list,w),self.w_n-1),1)\n            x_pos = max(min(self.xSearchFunc(self.x_list,x),self.x_n-1),1)\n            y_pos = max(min(self.ySearchFunc(self.y_list,y),self.y_n-1),1)\n            z_pos = max(min(self.zSearchFunc(self.z_list,z),self.z_n-1),1)\n        else:\n            w_pos = self.wSearchFunc(self.w_list,w)\n            w_pos[w_pos < 1] = 1\n            w_pos[w_pos > self.w_n-1] = self.w_n-1\n            x_pos = self.xSearchFunc(self.x_list,x)\n            x_pos[x_pos < 1] = 1\n            x_pos[x_pos > self.x_n-1] = self.x_n-1\n            y_pos = self.ySearchFunc(self.y_list,y)\n            y_pos[y_pos < 1] = 1\n            y_pos[y_pos > self.y_n-1] = self.y_n-1\n            z_pos = self.zSearchFunc(self.z_list,z)\n            z_pos[z_pos < 1] = 1\n            z_pos[z_pos > self.z_n-1] = self.z_n-1\n        i = w_pos # for convenience\n        j = x_pos\n        k = y_pos\n        l = z_pos\n        alpha = (w - self.w_list[i-1])/(self.w_list[i] - self.w_list[i-1])\n        beta = (x - self.x_list[j-1])/(self.x_list[j] - self.x_list[j-1])\n        gamma = (y - self.y_list[k-1])/(self.y_list[k] - self.y_list[k-1])\n        delta = (z - self.z_list[l-1])/(self.z_list[l] - self.z_list[l-1])\n        f = (\n             (1-alpha)*((1-beta)*((1-gamma)*(1-delta)*self.f_values[i-1,j-1,k-1,l-1]\n           + (1-gamma)*delta*self.f_values[i-1,j-1,k-1,l]\n           + gamma*(1-delta)*self.f_values[i-1,j-1,k,l-1]\n           + gamma*delta*self.f_values[i-1,j-1,k,l])\n           + beta*((1-gamma)*(1-delta)*self.f_values[i-1,j,k-1,l-1]\n           + (1-gamma)*delta*self.f_values[i-1,j,k-1,l]\n           + gamma*(1-delta)*self.f_values[i-1,j,k,l-1]\n           + gamma*delta*self.f_values[i-1,j,k,l]))\n           + alpha*((1-beta)*((1-gamma)*(1-delta)*self.f_values[i,j-1,k-1,l-1]\n           + (1-gamma)*delta*self.f_values[i,j-1,k-1,l]\n           + gamma*(1-delta)*self.f_values[i,j-1,k,l-1]\n           + gamma*delta*self.f_values[i,j-1,k,l])\n           + beta*((1-gamma)*(1-delta)*self.f_values[i,j,k-1,l-1]\n           + (1-gamma)*delta*self.f_values[i,j,k-1,l]\n           + gamma*(1-delta)*self.f_values[i,j,k,l-1]\n           + gamma*delta*self.f_values[i,j,k,l])))\n        return f"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nevaluate the function at each value in x as the minimum among all of the functions.", "response": "def _evaluate(self,x):\n        '''\n        Returns the level of the function at each value in x as the minimum among\n        all of the functions.  Only called internally by HARKinterpolator1D.__call__.\n        '''\n        if _isscalar(x):\n            y = np.nanmin([f(x) for f in self.functions])\n        else:\n            m = len(x)\n            fx = np.zeros((m,self.funcCount))\n            for j in range(self.funcCount):\n                fx[:,j] = self.functions[j](x)\n            y = np.nanmin(fx,axis=1)\n        return y"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _evalAndDer(self,x):\n        '''\n        Returns the level and first derivative of the function at each value in\n        x.  Only called internally by HARKinterpolator1D.eval_and_der.\n        '''\n        m = len(x)\n        fx = np.zeros((m,self.funcCount))\n        for j in range(self.funcCount):\n            fx[:,j] = self.functions[j](x)\n        fx[np.isnan(fx)] = np.inf\n        i = np.argmin(fx,axis=1)\n        y = fx[np.arange(m),i]\n        dydx = np.zeros_like(y)\n        for j in range(self.funcCount):\n            c = i == j\n            dydx[c] = self.functions[j].derivative(x[c])\n        return y,dydx", "response": "Evaluate and derivative of the function at each value in\n        x."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the first derivative of the function with respect to X at each value in x and y.", "response": "def _derX(self,x,y):\n        '''\n        Returns the first derivative of the function with respect to X at each\n        value in (x,y).  Only called internally by HARKinterpolator2D._derX.\n        '''\n        m = len(x)\n        temp = np.zeros((m,self.funcCount))\n        for j in range(self.funcCount):\n            temp[:,j] = self.functions[j](x,y)\n        temp[np.isnan(temp)] = np.inf\n        i = np.argmin(temp,axis=1)\n        dfdx = np.zeros_like(x)\n        for j in range(self.funcCount):\n            c = i == j\n            dfdx[c] = self.functions[j].derivativeX(x[c],y[c])\n        return dfdx"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the first derivative of the function with respect to Y at each value in x y. Only called internally by HARKinterpolator2D. _derY.", "response": "def _derY(self,x,y):\n        '''\n        Returns the first derivative of the function with respect to Y at each\n        value in (x,y).  Only called internally by HARKinterpolator2D._derY.\n        '''\n        m = len(x)\n        temp = np.zeros((m,self.funcCount))\n        for j in range(self.funcCount):\n            temp[:,j] = self.functions[j](x,y)\n        temp[np.isnan(temp)] = np.inf\n        i = np.argmin(temp,axis=1)\n        y = temp[np.arange(m),i]\n        dfdy = np.zeros_like(x)\n        for j in range(self.funcCount):\n            c = i == j\n            dfdy[c] = self.functions[j].derivativeY(x[c],y[c])\n        return dfdy"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the first derivative of the function with respect to Z at each value in ( x y z. Only called internally by HARKinterpolator3D. _derZ.", "response": "def _derZ(self,x,y,z):\n        '''\n        Returns the first derivative of the function with respect to Z at each\n        value in (x,y,z).  Only called internally by HARKinterpolator3D._derZ.\n        '''\n        m = len(x)\n        temp = np.zeros((m,self.funcCount))\n        for j in range(self.funcCount):\n            temp[:,j] = self.functions[j](x,y,z)\n        temp[np.isnan(temp)] = np.inf\n        i = np.argmin(temp,axis=1)\n        y = temp[np.arange(m),i]\n        dfdz = np.zeros_like(x)\n        for j in range(self.funcCount):\n            c = i == j\n            dfdz[c] = self.functions[j].derivativeZ(x[c],y[c],z[c])\n        return dfdz"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nevaluating the first derivative of the function at given state space points.", "response": "def derivativeX(self,x,y):\n        '''\n        Evaluate the first derivative with respect to x of the function at given\n        state space points.\n\n        Parameters\n        ----------\n        x : np.array\n             First input values.\n        y : np.array\n             Second input values; should be of same shape as x.\n\n        Returns\n        -------\n        dfdx_out : np.array\n            First derivative of function with respect to the first input,\n            evaluated at (x,y), of same shape as inputs.\n        '''\n        xShift = self.lowerBound(y)\n        dfdx_out = self.func.derivativeX(x-xShift,y)\n        return dfdx_out"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef derivativeY(self,x,y):\n        '''\n        Evaluate the first derivative with respect to y of the function at given\n        state space points.\n\n        Parameters\n        ----------\n        x : np.array\n             First input values.\n        y : np.array\n             Second input values; should be of same shape as x.\n\n        Returns\n        -------\n        dfdy_out : np.array\n            First derivative of function with respect to the second input,\n            evaluated at (x,y), of same shape as inputs.\n        '''\n        xShift,xShiftDer = self.lowerBound.eval_with_derivative(y)\n        dfdy_out = self.func.derivativeY(x-xShift,y) - xShiftDer*self.func.derivativeX(x-xShift,y)\n        return dfdy_out", "response": "Evaluate the first derivative with respect to y of the function at given state space points."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nevaluate the first derivative of the function at given state space points.", "response": "def derivativeZ(self,x,y,z):\n        '''\n        Evaluate the first derivative with respect to z of the function at given\n        state space points.\n\n        Parameters\n        ----------\n        x : np.array\n             First input values.\n        y : np.array\n             Second input values; should be of same shape as x.\n        z : np.array\n             Third input values; should be of same shape as x.\n\n        Returns\n        -------\n        dfdz_out : np.array\n            First derivative of function with respect to the third input,\n            evaluated at (x,y,z), of same shape as inputs.\n        '''\n        xShift = self.lowerBound(y)\n        dfdz_out = self.func.derivativeZ(x-xShift,y,z)\n        return dfdz_out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _derX(self,x,y):\n        '''\n        Returns the derivative with respect to x of the interpolated function\n        at each value in x,y. Only called internally by HARKinterpolator2D.derivativeX.\n        '''\n        if _isscalar(x):\n            y_pos = max(min(np.searchsorted(self.y_list,y),self.y_n-1),1)\n            alpha = (y - self.y_list[y_pos-1])/(self.y_list[y_pos] - self.y_list[y_pos-1])\n            dfdx = (1-alpha)*self.xInterpolators[y_pos-1]._der(x) + alpha*self.xInterpolators[y_pos]._der(x)\n        else:\n            m = len(x)\n            y_pos = np.searchsorted(self.y_list,y)\n            y_pos[y_pos > self.y_n-1] = self.y_n-1\n            y_pos[y_pos < 1] = 1\n            dfdx = np.zeros(m) + np.nan\n            if y.size > 0:\n                for i in range(1,self.y_n):\n                    c = y_pos == i\n                    if np.any(c):\n                        alpha = (y[c] - self.y_list[i-1])/(self.y_list[i] - self.y_list[i-1])\n                        dfdx[c] = (1-alpha)*self.xInterpolators[i-1]._der(x[c]) + alpha*self.xInterpolators[i]._der(x[c])\n        return dfdx", "response": "Returns the derivative with respect to x of the interpolated function\n            at each value in x y. Only called internally by HARKinterpolator2D. derivativeX."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _evaluate(self,x,y,z):\n        '''\n        Returns the level of the interpolated function at each value in x,y,z.\n        Only called internally by HARKinterpolator3D.__call__ (etc).\n        '''\n        if _isscalar(x):\n            y_pos = max(min(np.searchsorted(self.y_list,y),self.y_n-1),1)\n            z_pos = max(min(np.searchsorted(self.z_list,z),self.z_n-1),1)\n            alpha = (y - self.y_list[y_pos-1])/(self.y_list[y_pos] - self.y_list[y_pos-1])\n            beta = (z - self.z_list[z_pos-1])/(self.z_list[z_pos] - self.z_list[z_pos-1])\n            f = ((1-alpha)*(1-beta)*self.xInterpolators[y_pos-1][z_pos-1](x)\n              + (1-alpha)*beta*self.xInterpolators[y_pos-1][z_pos](x)\n              + alpha*(1-beta)*self.xInterpolators[y_pos][z_pos-1](x)\n              + alpha*beta*self.xInterpolators[y_pos][z_pos](x))\n        else:\n            m = len(x)\n            y_pos = np.searchsorted(self.y_list,y)\n            y_pos[y_pos > self.y_n-1] = self.y_n-1\n            y_pos[y_pos < 1] = 1\n            z_pos = np.searchsorted(self.z_list,z)\n            z_pos[z_pos > self.z_n-1] = self.z_n-1\n            z_pos[z_pos < 1] = 1\n            f = np.zeros(m) + np.nan\n            for i in range(1,self.y_n):\n                for j in range(1,self.z_n):\n                    c = np.logical_and(i == y_pos, j == z_pos)\n                    if np.any(c):\n                        alpha = (y[c] - self.y_list[i-1])/(self.y_list[i] - self.y_list[i-1])\n                        beta = (z[c] - self.z_list[j-1])/(self.z_list[j] - self.z_list[j-1])\n                        f[c] = (\n                          (1-alpha)*(1-beta)*self.xInterpolators[i-1][j-1](x[c])\n                          + (1-alpha)*beta*self.xInterpolators[i-1][j](x[c])\n                          + alpha*(1-beta)*self.xInterpolators[i][j-1](x[c])\n                          + alpha*beta*self.xInterpolators[i][j](x[c]))\n        return f", "response": "Evaluate the interpolated function at each value in x y z."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nevaluates the interpolated function at each value in w x y z.", "response": "def _evaluate(self,w,x,y,z):\n        '''\n        Returns the level of the interpolated function at each value in w,x,y,z.\n        Only called internally by HARKinterpolator4D.__call__ (etc).\n        '''\n        if _isscalar(w):\n            x_pos = max(min(np.searchsorted(self.x_list,x),self.x_n-1),1)\n            y_pos = max(min(np.searchsorted(self.y_list,y),self.y_n-1),1)\n            z_pos = max(min(np.searchsorted(self.z_list,z),self.z_n-1),1)\n            alpha = (x - self.x_list[x_pos-1])/(self.x_list[x_pos] - self.x_list[x_pos-1])\n            beta = (y - self.y_list[y_pos-1])/(self.y_list[y_pos] - self.y_list[y_pos-1])\n            gamma = (z - self.z_list[z_pos-1])/(self.z_list[z_pos] - self.z_list[z_pos-1])\n            f = (\n                (1-alpha)*(1-beta)*(1-gamma)*self.wInterpolators[x_pos-1][y_pos-1][z_pos-1](w)\n              + (1-alpha)*(1-beta)*gamma*self.wInterpolators[x_pos-1][y_pos-1][z_pos](w)\n              + (1-alpha)*beta*(1-gamma)*self.wInterpolators[x_pos-1][y_pos][z_pos-1](w)\n              + (1-alpha)*beta*gamma*self.wInterpolators[x_pos-1][y_pos][z_pos](w)\n              + alpha*(1-beta)*(1-gamma)*self.wInterpolators[x_pos][y_pos-1][z_pos-1](w)\n              + alpha*(1-beta)*gamma*self.wInterpolators[x_pos][y_pos-1][z_pos](w)\n              + alpha*beta*(1-gamma)*self.wInterpolators[x_pos][y_pos][z_pos-1](w)\n              + alpha*beta*gamma*self.wInterpolators[x_pos][y_pos][z_pos](w))\n        else:\n            m = len(x)\n            x_pos = np.searchsorted(self.x_list,x)\n            x_pos[x_pos > self.x_n-1] = self.x_n-1\n            y_pos = np.searchsorted(self.y_list,y)\n            y_pos[y_pos > self.y_n-1] = self.y_n-1\n            y_pos[y_pos < 1] = 1\n            z_pos = np.searchsorted(self.z_list,z)\n            z_pos[z_pos > self.z_n-1] = self.z_n-1\n            z_pos[z_pos < 1] = 1\n            f = np.zeros(m) + np.nan\n            for i in range(1,self.x_n):\n                for j in range(1,self.y_n):\n                    for k in range(1,self.z_n):\n                        c = np.logical_and(np.logical_and(i == x_pos, j == y_pos),k == z_pos)\n                        if np.any(c):\n                            alpha = (x[c] - self.x_list[i-1])/(self.x_list[i] - self.x_list[i-1])\n                            beta = (y[c] - self.y_list[j-1])/(self.y_list[j] - self.y_list[j-1])\n                            gamma = (z[c] - self.z_list[k-1])/(self.z_list[k] - self.z_list[k-1])\n                            f[c] = (\n                                (1-alpha)*(1-beta)*(1-gamma)*self.wInterpolators[i-1][j-1][k-1](w[c])\n                              + (1-alpha)*(1-beta)*gamma*self.wInterpolators[i-1][j-1][k](w[c])\n                              + (1-alpha)*beta*(1-gamma)*self.wInterpolators[i-1][j][k-1](w[c])\n                              + (1-alpha)*beta*gamma*self.wInterpolators[i-1][j][k](w[c])\n                              + alpha*(1-beta)*(1-gamma)*self.wInterpolators[i][j-1][k-1](w[c])\n                              + alpha*(1-beta)*gamma*self.wInterpolators[i][j-1][k](w[c])\n                              + alpha*beta*(1-gamma)*self.wInterpolators[i][j][k-1](w[c])\n                              + alpha*beta*gamma*self.wInterpolators[i][j][k](w[c]))\n        return f"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _derY(self,x,y,z):\n        '''\n        Returns the derivative with respect to y of the interpolated function\n        at each value in x,y,z. Only called internally by HARKinterpolator3D.derivativeY.\n        '''\n        if _isscalar(x):\n            z_pos = max(min(np.searchsorted(self.z_list,z),self.z_n-1),1)\n            alpha = (z - self.z_list[z_pos-1])/(self.z_list[z_pos] - self.z_list[z_pos-1])\n            dfdy = (1-alpha)*self.xyInterpolators[z_pos-1].derivativeY(x,y) + alpha*self.xyInterpolators[z_pos].derivativeY(x,y)\n        else:\n            m = len(x)\n            z_pos = np.searchsorted(self.z_list,z)\n            z_pos[z_pos > self.z_n-1] = self.z_n-1\n            z_pos[z_pos < 1] = 1\n            dfdy = np.zeros(m) + np.nan\n            if x.size > 0:\n                for i in range(1,self.z_n):\n                    c = z_pos == i\n                    if np.any(c):\n                        alpha = (z[c] - self.z_list[i-1])/(self.z_list[i] - self.z_list[i-1])\n                        dfdy[c] = (1-alpha)*self.xyInterpolators[i-1].derivativeY(x[c],y[c]) + alpha*self.xyInterpolators[i].derivativeY(x[c],y[c])\n        return dfdy", "response": "Returns the derivative with respect to y of the interpolated function\n            at each value in x y z. Only called internally by HARKinterpolator3D. derivativeY."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the polarity attribute of the interpolation of the system of equations .", "response": "def updatePolarity(self):\n        '''\n        Fills in the polarity attribute of the interpolation, determining whether\n        the \"plus\" (True) or \"minus\" (False) solution of the system of equations\n        should be used for each sector.  Needs to be called in __init__.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        # Grab a point known to be inside each sector: the midway point between\n        # the lower left and upper right vertex of each sector\n        x_temp = 0.5*(self.x_values[0:(self.x_n-1),0:(self.y_n-1)] + self.x_values[1:self.x_n,1:self.y_n])\n        y_temp = 0.5*(self.y_values[0:(self.x_n-1),0:(self.y_n-1)] + self.y_values[1:self.x_n,1:self.y_n])\n        size = (self.x_n-1)*(self.y_n-1)\n        x_temp = np.reshape(x_temp,size)\n        y_temp = np.reshape(y_temp,size)\n        y_pos = np.tile(np.arange(0,self.y_n-1),self.x_n-1)\n        x_pos = np.reshape(np.tile(np.arange(0,self.x_n-1),(self.y_n-1,1)).transpose(),size)\n\n        # Set the polarity of all sectors to \"plus\", then test each sector\n        self.polarity = np.ones((self.x_n-1,self.y_n-1),dtype=bool)\n        alpha, beta = self.findCoords(x_temp,y_temp,x_pos,y_pos)\n        polarity = np.logical_and(\n            np.logical_and(alpha > 0, alpha < 1),\n            np.logical_and(beta > 0, beta < 1))\n\n        # Update polarity: if (alpha,beta) not in the unit square, then that\n        # sector must use the \"minus\" solution instead\n        self.polarity = np.reshape(polarity,(self.x_n-1,self.y_n-1))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef findSector(self,x,y):\n        '''\n        Finds the quadrilateral \"sector\" for each (x,y) point in the input.\n        Only called as a subroutine of _evaluate().\n\n        Parameters\n        ----------\n        x : np.array\n            Values whose sector should be found.\n        y : np.array\n            Values whose sector should be found.  Should be same size as x.\n\n        Returns\n        -------\n        x_pos : np.array\n            Sector x-coordinates for each point of the input, of the same size.\n        y_pos : np.array\n            Sector y-coordinates for each point of the input, of the same size.\n        '''\n        # Initialize the sector guess\n        m = x.size\n        x_pos_guess = (np.ones(m)*self.x_n/2).astype(int)\n        y_pos_guess = (np.ones(m)*self.y_n/2).astype(int)\n\n        # Define a function that checks whether a set of points violates a linear\n        # boundary defined by (x_bound_1,y_bound_1) and (x_bound_2,y_bound_2),\n        # where the latter is *COUNTER CLOCKWISE* from the former.  Returns\n        # 1 if the point is outside the boundary and 0 otherwise.\n        violationCheck = lambda x_check,y_check,x_bound_1,y_bound_1,x_bound_2,y_bound_2 : (\n            (y_bound_2 - y_bound_1)*x_check - (x_bound_2 - x_bound_1)*y_check > x_bound_1*y_bound_2 - y_bound_1*x_bound_2 ) + 0\n\n        # Identify the correct sector for each point to be evaluated\n        these = np.ones(m,dtype=bool)\n        max_loops = self.x_n + self.y_n\n        loops = 0\n        while np.any(these) and loops < max_loops:\n            # Get coordinates for the four vertices: (xA,yA),...,(xD,yD)\n            x_temp = x[these]\n            y_temp = y[these]\n            xA = self.x_values[x_pos_guess[these],y_pos_guess[these]]\n            xB = self.x_values[x_pos_guess[these]+1,y_pos_guess[these]]\n            xC = self.x_values[x_pos_guess[these],y_pos_guess[these]+1]\n            xD = self.x_values[x_pos_guess[these]+1,y_pos_guess[these]+1]\n            yA = self.y_values[x_pos_guess[these],y_pos_guess[these]]\n            yB = self.y_values[x_pos_guess[these]+1,y_pos_guess[these]]\n            yC = self.y_values[x_pos_guess[these],y_pos_guess[these]+1]\n            yD = self.y_values[x_pos_guess[these]+1,y_pos_guess[these]+1]\n\n            # Check the \"bounding box\" for the sector: is this guess plausible?\n            move_down = (y_temp < np.minimum(yA,yB)) + 0\n            move_right = (x_temp > np.maximum(xB,xD)) + 0\n            move_up = (y_temp > np.maximum(yC,yD)) + 0\n            move_left = (x_temp < np.minimum(xA,xC)) + 0\n\n            # Check which boundaries are violated (and thus where to look next)\n            c = (move_down + move_right + move_up + move_left) == 0\n            move_down[c] = violationCheck(x_temp[c],y_temp[c],xA[c],yA[c],xB[c],yB[c])\n            move_right[c] = violationCheck(x_temp[c],y_temp[c],xB[c],yB[c],xD[c],yD[c])\n            move_up[c] = violationCheck(x_temp[c],y_temp[c],xD[c],yD[c],xC[c],yC[c])\n            move_left[c] = violationCheck(x_temp[c],y_temp[c],xC[c],yC[c],xA[c],yA[c])\n\n            # Update the sector guess based on the violations\n            x_pos_next = x_pos_guess[these] - move_left + move_right\n            x_pos_next[x_pos_next < 0] = 0\n            x_pos_next[x_pos_next > (self.x_n-2)] = self.x_n-2\n            y_pos_next = y_pos_guess[these] - move_down + move_up\n            y_pos_next[y_pos_next < 0] = 0\n            y_pos_next[y_pos_next > (self.y_n-2)] = self.y_n-2\n\n            # Check which sectors have not changed, and mark them as complete\n            no_move = np.array(np.logical_and(x_pos_guess[these] == x_pos_next, y_pos_guess[these] == y_pos_next))\n            x_pos_guess[these] = x_pos_next\n            y_pos_guess[these] = y_pos_next\n            temp = these.nonzero()\n            these[temp[0][no_move]] = False\n\n            # Move to the next iteration of the search\n            loops += 1\n\n        # Return the output\n        x_pos = x_pos_guess\n        y_pos = y_pos_guess\n        return x_pos, y_pos", "response": "Find the quadrilateral sector for each point in the input."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef findCoords(self,x,y,x_pos,y_pos):\n        '''\n        Calculates the relative coordinates (alpha,beta) for each point (x,y),\n        given the sectors (x_pos,y_pos) in which they reside.  Only called as\n        a subroutine of __call__().\n\n        Parameters\n        ----------\n        x : np.array\n            Values whose sector should be found.\n        y : np.array\n            Values whose sector should be found.  Should be same size as x.\n        x_pos : np.array\n            Sector x-coordinates for each point in (x,y), of the same size.\n        y_pos : np.array\n            Sector y-coordinates for each point in (x,y), of the same size.\n\n        Returns\n        -------\n        alpha : np.array\n            Relative \"horizontal\" position of the input in their respective sectors.\n        beta : np.array\n            Relative \"vertical\" position of the input in their respective sectors.\n        '''\n        # Calculate relative coordinates in the sector for each point\n        xA = self.x_values[x_pos,y_pos]\n        xB = self.x_values[x_pos+1,y_pos]\n        xC = self.x_values[x_pos,y_pos+1]\n        xD = self.x_values[x_pos+1,y_pos+1]\n        yA = self.y_values[x_pos,y_pos]\n        yB = self.y_values[x_pos+1,y_pos]\n        yC = self.y_values[x_pos,y_pos+1]\n        yD = self.y_values[x_pos+1,y_pos+1]\n        polarity = 2.0*self.polarity[x_pos,y_pos] - 1.0\n        a = xA\n        b = (xB-xA)\n        c = (xC-xA)\n        d = (xA-xB-xC+xD)\n        e = yA\n        f = (yB-yA)\n        g = (yC-yA)\n        h = (yA-yB-yC+yD)\n        denom = (d*g-h*c)\n        mu = (h*b-d*f)/denom\n        tau = (h*(a-x) - d*(e-y))/denom\n        zeta = a - x + c*tau\n        eta = b + c*mu + d*tau\n        theta = d*mu\n        alpha = (-eta + polarity*np.sqrt(eta**2.0 - 4.0*zeta*theta))/(2.0*theta)\n        beta = mu*alpha + tau\n\n        # Alternate method if there are sectors that are \"too regular\"\n        z = np.logical_or(np.isnan(alpha),np.isnan(beta))   # These points weren't able to identify coordinates\n        if np.any(z):\n            these = np.isclose(f/b,(yD-yC)/(xD-xC)) # iso-beta lines have equal slope\n            if np.any(these):\n                kappa     = f[these]/b[these]\n                int_bot   = yA[these] - kappa*xA[these]\n                int_top   = yC[these] - kappa*xC[these]\n                int_these = y[these] - kappa*x[these]\n                beta_temp = (int_these-int_bot)/(int_top-int_bot)\n                x_left    = beta_temp*xC[these] + (1.0-beta_temp)*xA[these]\n                x_right   = beta_temp*xD[these] + (1.0-beta_temp)*xB[these]\n                alpha_temp= (x[these]-x_left)/(x_right-x_left)\n                beta[these]  = beta_temp\n                alpha[these] = alpha_temp\n\n            #print(np.sum(np.isclose(g/c,(yD-yB)/(xD-xB))))\n\n        return alpha, beta", "response": "This method calculates the relative coordinates of the input sectors and the sectors in which they reside."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _evaluate(self,x,y):\n        '''\n        Returns the level of the interpolated function at each value in x,y.\n        Only called internally by HARKinterpolator2D.__call__ (etc).\n        '''\n        x_pos, y_pos = self.findSector(x,y)\n        alpha, beta = self.findCoords(x,y,x_pos,y_pos)\n\n        # Calculate the function at each point using bilinear interpolation\n        f = (\n             (1-alpha)*(1-beta)*self.f_values[x_pos,y_pos]\n          +  (1-alpha)*beta*self.f_values[x_pos,y_pos+1]\n          +  alpha*(1-beta)*self.f_values[x_pos+1,y_pos]\n          +  alpha*beta*self.f_values[x_pos+1,y_pos+1])\n        return f", "response": "Evaluate the interpolated function at each value in x y."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _derX(self,x,y):\n        '''\n        Returns the derivative with respect to x of the interpolated function\n        at each value in x,y. Only called internally by HARKinterpolator2D.derivativeX.\n        '''\n        x_pos, y_pos = self.findSector(x,y)\n        alpha, beta = self.findCoords(x,y,x_pos,y_pos)\n\n        # Get four corners data for each point\n        xA = self.x_values[x_pos,y_pos]\n        xB = self.x_values[x_pos+1,y_pos]\n        xC = self.x_values[x_pos,y_pos+1]\n        xD = self.x_values[x_pos+1,y_pos+1]\n        yA = self.y_values[x_pos,y_pos]\n        yB = self.y_values[x_pos+1,y_pos]\n        yC = self.y_values[x_pos,y_pos+1]\n        yD = self.y_values[x_pos+1,y_pos+1]\n        fA = self.f_values[x_pos,y_pos]\n        fB = self.f_values[x_pos+1,y_pos]\n        fC = self.f_values[x_pos,y_pos+1]\n        fD = self.f_values[x_pos+1,y_pos+1]\n\n        # Calculate components of the alpha,beta --> x,y delta translation matrix\n        alpha_x = (1-beta)*(xB-xA) + beta*(xD-xC)\n        alpha_y = (1-beta)*(yB-yA) + beta*(yD-yC)\n        beta_x  = (1-alpha)*(xC-xA) + alpha*(xD-xB)\n        beta_y  = (1-alpha)*(yC-yA) + alpha*(yD-yB)\n\n        # Invert the delta translation matrix into x,y --> alpha,beta\n        det = alpha_x*beta_y - beta_x*alpha_y\n        x_alpha = beta_y/det\n        x_beta  = -alpha_y/det\n\n        # Calculate the derivative of f w.r.t. alpha and beta\n        dfda = (1-beta)*(fB-fA) + beta*(fD-fC)\n        dfdb = (1-alpha)*(fC-fA) + alpha*(fD-fB)\n\n        # Calculate the derivative with respect to x (and return it)\n        dfdx = x_alpha*dfda + x_beta*dfdb\n        return dfdx", "response": "Returns the derivative of the interpolated function at each value in x y."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the derivative of the interpolated function at each value in x y. Only called internally by HARKinterpolator2D. derivativeY.", "response": "def _derY(self,x,y):\n        '''\n        Returns the derivative with respect to y of the interpolated function\n        at each value in x,y. Only called internally by HARKinterpolator2D.derivativeX.\n        '''\n        x_pos, y_pos = self.findSector(x,y)\n        alpha, beta = self.findCoords(x,y,x_pos,y_pos)\n\n        # Get four corners data for each point\n        xA = self.x_values[x_pos,y_pos]\n        xB = self.x_values[x_pos+1,y_pos]\n        xC = self.x_values[x_pos,y_pos+1]\n        xD = self.x_values[x_pos+1,y_pos+1]\n        yA = self.y_values[x_pos,y_pos]\n        yB = self.y_values[x_pos+1,y_pos]\n        yC = self.y_values[x_pos,y_pos+1]\n        yD = self.y_values[x_pos+1,y_pos+1]\n        fA = self.f_values[x_pos,y_pos]\n        fB = self.f_values[x_pos+1,y_pos]\n        fC = self.f_values[x_pos,y_pos+1]\n        fD = self.f_values[x_pos+1,y_pos+1]\n\n        # Calculate components of the alpha,beta --> x,y delta translation matrix\n        alpha_x = (1-beta)*(xB-xA) + beta*(xD-xC)\n        alpha_y = (1-beta)*(yB-yA) + beta*(yD-yC)\n        beta_x  = (1-alpha)*(xC-xA) + alpha*(xD-xB)\n        beta_y  = (1-alpha)*(yC-yA) + alpha*(yD-yB)\n\n        # Invert the delta translation matrix into x,y --> alpha,beta\n        det = alpha_x*beta_y - beta_x*alpha_y\n        y_alpha = -beta_x/det\n        y_beta  = alpha_x/det\n\n        # Calculate the derivative of f w.r.t. alpha and beta\n        dfda = (1-beta)*(fB-fA) + beta*(fD-fC)\n        dfdb = (1-alpha)*(fC-fA) + alpha*(fD-fB)\n\n        # Calculate the derivative with respect to x (and return it)\n        dfdy = y_alpha*dfda + y_beta*dfdb\n        return dfdy"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsolving the dynamic model for one agent type.", "response": "def solveAgent(agent,verbose):\n    '''\n    Solve the dynamic model for one agent type.  This function iterates on \"cycles\"\n    of an agent's model either a given number of times or until solution convergence\n    if an infinite horizon model is used (with agent.cycles = 0).\n\n    Parameters\n    ----------\n    agent : AgentType\n        The microeconomic AgentType whose dynamic problem is to be solved.\n    verbose : boolean\n        If True, solution progress is printed to screen (when cycles != 1).\n\n    Returns\n    -------\n    solution : [Solution]\n        A list of solutions to the one period problems that the agent will\n        encounter in his \"lifetime\".  Returns in reverse chronological order.\n    '''\n    # Record the flow of time when the Agent began the process, and make sure time is flowing backwards\n    original_time_flow = agent.time_flow\n    agent.timeRev()\n\n    # Check to see whether this is an (in)finite horizon problem\n    cycles_left      = agent.cycles\n    infinite_horizon = cycles_left == 0\n\n    # Initialize the solution, which includes the terminal solution if it's not a pseudo-terminal period\n    solution = []\n    if not agent.pseudo_terminal:\n        solution.append(deepcopy(agent.solution_terminal))\n\n    # Initialize the process, then loop over cycles\n    solution_last    = agent.solution_terminal\n    go               = True\n    completed_cycles = 0\n    max_cycles       = 5000 # escape clause\n    if verbose:\n        t_last = clock()\n    while go:\n        # Solve a cycle of the model, recording it if horizon is finite\n        solution_cycle = solveOneCycle(agent,solution_last)\n        if not infinite_horizon:\n            solution += solution_cycle\n\n        # Check for termination: identical solutions across cycle iterations or run out of cycles\n        solution_now = solution_cycle[-1]\n        if infinite_horizon:\n            if completed_cycles > 0:\n                solution_distance = solution_now.distance(solution_last)\n                go = (solution_distance > agent.tolerance and completed_cycles < max_cycles)\n            else: # Assume solution does not converge after only one cycle\n                solution_distance = 100.0\n                go = True\n        else:\n            cycles_left += -1\n            go = cycles_left > 0\n\n        # Update the \"last period solution\"\n        solution_last = solution_now\n        completed_cycles += 1\n\n        # Display progress if requested\n        if verbose:\n            t_now = clock()\n            if infinite_horizon:\n                print('Finished cycle #' + str(completed_cycles) + ' in ' + str(t_now-t_last) +\\\n                ' seconds, solution distance = ' + str(solution_distance))\n            else:\n                print('Finished cycle #' + str(completed_cycles) + ' of ' + str(agent.cycles) +\\\n                ' in ' + str(t_now-t_last) + ' seconds.')\n            t_last = t_now\n\n    # Record the last cycle if horizon is infinite (solution is still empty!)\n    if infinite_horizon:\n        solution = solution_cycle # PseudoTerminal=False impossible for infinite horizon\n\n    # Restore the direction of time to its original orientation, then return the solution\n    if original_time_flow:\n        agent.timeFwd()\n    return solution"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef solveOneCycle(agent,solution_last):\n    '''\n    Solve one \"cycle\" of the dynamic model for one agent type.  This function\n    iterates over the periods within an agent's cycle, updating the time-varying\n    parameters and passing them to the single period solver(s).\n\n    Parameters\n    ----------\n    agent : AgentType\n        The microeconomic AgentType whose dynamic problem is to be solved.\n    solution_last : Solution\n        A representation of the solution of the period that comes after the\n        end of the sequence of one period problems.  This might be the term-\n        inal period solution, a \"pseudo terminal\" solution, or simply the\n        solution to the earliest period from the succeeding cycle.\n\n    Returns\n    -------\n    solution_cycle : [Solution]\n        A list of one period solutions for one \"cycle\" of the AgentType's\n        microeconomic model.  Returns in reverse chronological order.\n    '''\n    # Calculate number of periods per cycle, defaults to 1 if all variables are time invariant\n    if len(agent.time_vary) > 0:\n        name = agent.time_vary[0]\n        T    = len(eval('agent.' + name))\n    else:\n        T = 1\n\n    # Check whether the same solution method is used in all periods\n    always_same_solver = 'solveOnePeriod' not in agent.time_vary\n    if always_same_solver:\n        solveOnePeriod = agent.solveOnePeriod\n        these_args     = getArgNames(solveOnePeriod)\n\n    # Construct a dictionary to be passed to the solver\n    time_inv_string = ''\n    for name in agent.time_inv:\n        time_inv_string += ' \\'' + name + '\\' : agent.' +name + ','\n    time_vary_string = ''\n    for name in agent.time_vary:\n        time_vary_string += ' \\'' + name + '\\' : None,'\n    solve_dict = eval('{' + time_inv_string + time_vary_string + '}')\n\n    # Initialize the solution for this cycle, then iterate on periods\n    solution_cycle = []\n    solution_next  = solution_last\n    for t in range(T):\n        # Update which single period solver to use (if it depends on time)\n        if not always_same_solver:\n            solveOnePeriod = agent.solveOnePeriod[t]\n            these_args = getArgNames(solveOnePeriod)\n\n        # Update time-varying single period inputs\n        for name in agent.time_vary:\n            if name in these_args:\n                solve_dict[name] = eval('agent.' + name + '[t]')\n        solve_dict['solution_next'] = solution_next\n\n        # Make a temporary dictionary for this period\n        temp_dict = {name: solve_dict[name] for name in these_args}\n\n        # Solve one period, add it to the solution, and move to the next period\n        solution_t = solveOnePeriod(**temp_dict)\n        solution_cycle.append(solution_t)\n        solution_next = solution_t\n\n    # Return the list of per-period solutions\n    return solution_cycle", "response": "This function solves one cycle of the dynamic model for one agent type."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef copy_module(target_path, my_directory_full_path, my_module):\n    '''\n    Helper function for copy_module_to_local(). Provides the actual copy\n    functionality, with highly cautious safeguards against copying over\n    important things.\n\n    Parameters\n    ----------\n    target_path : string\n        String, file path to target location\n\n    my_directory_full_path: string\n        String, full pathname to this file's directory\n\n    my_module : string\n        String, name of the module to copy\n\n    Returns\n    -------\n    none\n    '''\n\n    if target_path == 'q' or target_path == 'Q':\n        print(\"Goodbye!\")\n        return\n    elif target_path == os.path.expanduser(\"~\") or os.path.normpath(target_path) == os.path.expanduser(\"~\"):\n        print(\"You have indicated that the target location is \"+target_path+\" -- that is, you want to wipe out your home directory with the contents of \"+my_module+\". My programming does not allow me to do that.\\n\\nGoodbye!\")\n        return\n    elif os.path.exists(target_path):\n        print(\"There is already a file or directory at the location \"+target_path+\". For safety reasons this code does not overwrite existing files.\\nPlease remove the file at \"+target_path+\" and try again.\")\n        return\n    else:\n        user_input = input(\"\"\"You have indicated you want to copy module:\\n    \"\"\"+ my_module\n                           + \"\"\"\\nto:\\n    \"\"\"+ target_path +\"\"\"\\nIs that correct? Please indicate: y / [n]\\n\\n\"\"\")\n        if user_input == 'y' or user_input == 'Y':\n            #print(\"copy_tree(\",my_directory_full_path,\",\", target_path,\")\")\n            copy_tree(my_directory_full_path, target_path)\n        else:\n            print(\"Goodbye!\")\n            return", "response": "This function is a helper function for copy_module_to_local. It will copy the module to the local file system."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef assignParameters(self,**kwds):\n        '''\n        Assign an arbitrary number of attributes to this agent.\n\n        Parameters\n        ----------\n        **kwds : keyword arguments\n            Any number of keyword arguments of the form key=value.  Each value\n            will be assigned to the attribute named in self.\n\n        Returns\n        -------\n        none\n        '''\n        for key in kwds:\n            setattr(self,key,kwds[key])", "response": "Assign an arbitrary number of attributes to this agent."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the average of an attribute of this instance. Returns NaN if no such attribute.", "response": "def getAvg(self,varname,**kwds):\n        '''\n        Calculates the average of an attribute of this instance.  Returns NaN if no such attribute.\n\n        Parameters\n        ----------\n        varname : string\n            The name of the attribute whose average is to be calculated.  This attribute must be an\n            np.array or other class compatible with np.mean.\n\n        Returns\n        -------\n        avg : float or np.array\n            The average of this attribute.  Might be an array if the axis keyword is passed.\n        '''\n        if hasattr(self,varname):\n            return np.mean(getattr(self,varname),**kwds)\n        else:\n            return np.nan"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef timeFlip(self):\n        '''\n        Reverse the flow of time for this instance.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        for name in self.time_vary:\n            exec('self.' + name + '.reverse()')\n        self.time_flow = not self.time_flow", "response": "Reverse the flow of time for this instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd any number of parameters to time_vary for this instance.", "response": "def addToTimeVary(self,*params):\n        '''\n        Adds any number of parameters to time_vary for this instance.\n\n        Parameters\n        ----------\n        params : string\n            Any number of strings naming attributes to be added to time_vary\n\n        Returns\n        -------\n        None\n        '''\n        for param in params:\n            if param not in self.time_vary:\n                self.time_vary.append(param)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef addToTimeInv(self,*params):\n        '''\n        Adds any number of parameters to time_inv for this instance.\n\n        Parameters\n        ----------\n        params : string\n            Any number of strings naming attributes to be added to time_inv\n\n        Returns\n        -------\n        None\n        '''\n        for param in params:\n            if param not in self.time_inv:\n                self.time_inv.append(param)", "response": "Adds any number of parameters to time_inv for this instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delFromTimeVary(self,*params):\n        '''\n        Removes any number of parameters from time_vary for this instance.\n\n        Parameters\n        ----------\n        params : string\n            Any number of strings naming attributes to be removed from time_vary\n\n        Returns\n        -------\n        None\n        '''\n        for param in params:\n            if param in self.time_vary:\n                self.time_vary.remove(param)", "response": "Removes any number of parameters from time_vary for this instance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delFromTimeInv(self,*params):\n        '''\n        Removes any number of parameters from time_inv for this instance.\n\n        Parameters\n        ----------\n        params : string\n            Any number of strings naming attributes to be removed from time_inv\n\n        Returns\n        -------\n        None\n        '''\n        for param in params:\n            if param in self.time_inv:\n                self.time_inv.remove(param)", "response": "Removes any number of parameters from time_inv for this instance."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsolving the model for this instance of an agent type by backward induction.", "response": "def solve(self,verbose=False):\n        '''\n        Solve the model for this instance of an agent type by backward induction.\n        Loops through the sequence of one period problems, passing the solution\n        from period t+1 to the problem for period t.\n\n        Parameters\n        ----------\n        verbose : boolean\n            If True, solution progress is printed to screen.\n\n        Returns\n        -------\n        none\n        '''\n\n        # Ignore floating point \"errors\". Numpy calls it \"errors\", but really it's excep-\n        # tions with well-defined answers such as 1.0/0.0 that is np.inf, -1.0/0.0 that is\n        # -np.inf, np.inf/np.inf is np.nan and so on.\n        with np.errstate(divide='ignore', over='ignore', under='ignore', invalid='ignore'):\n            self.preSolve() # Do pre-solution stuff\n            self.solution = solveAgent(self,verbose) # Solve the model by backward induction\n            if self.time_flow: # Put the solution in chronological order if this instance's time flow runs that way\n                self.solution.reverse()\n            self.addToTimeVary('solution') # Add solution to the list of time-varying attributes\n            self.postSolve()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize the internal simulation for this AgentType.", "response": "def initializeSim(self):\n        '''\n        Prepares this AgentType for a new simulation.  Resets the internal random number generator,\n        makes initial states for all agents (using simBirth), clears histories of tracked variables.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        self.resetRNG()\n        self.t_sim = 0\n        all_agents = np.ones(self.AgentCount,dtype=bool)\n        blank_array = np.zeros(self.AgentCount)\n        for var_name in self.poststate_vars:\n            exec('self.' + var_name + ' = copy(blank_array)')\n        self.t_age = np.zeros(self.AgentCount,dtype=int)   # Number of periods since agent entry\n        self.t_cycle = np.zeros(self.AgentCount,dtype=int) # Which cycle period each agent is on\n        self.simBirth(all_agents)\n        self.clearHistory()\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsimulates one period for this type.", "response": "def simOnePeriod(self):\n        '''\n        Simulates one period for this type.  Calls the methods getMortality(), getShocks() or\n        readShocks, getStates(), getControls(), and getPostStates().  These should be defined for\n        AgentType subclasses, except getMortality (define its components simDeath and simBirth\n        instead) and readShocks.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        self.getMortality()  # Replace some agents with \"newborns\"\n        if self.read_shocks: # If shock histories have been pre-specified, use those\n            self.readShocks()\n        else:                # Otherwise, draw shocks as usual according to subclass-specific method\n            self.getShocks()\n        self.getStates()     # Determine each agent's state at decision time\n        self.getControls()   # Determine each agent's choice or control variables based on states\n        self.getPostStates() # Determine each agent's post-decision / end-of-period states using states and controls\n\n        # Advance time for all agents\n        self.t_age = self.t_age + 1 # Age all consumers by one period\n        self.t_cycle = self.t_cycle + 1 # Age all consumers within their cycle\n        self.t_cycle[self.t_cycle == self.T_cycle] = 0"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking a pre - specified history of shocks for the simulation.", "response": "def makeShockHistory(self):\n        '''\n        Makes a pre-specified history of shocks for the simulation.  Shock variables should be named\n        in self.shock_vars, a list of strings that is subclass-specific.  This method runs a subset\n        of the standard simulation loop by simulating only mortality and shocks; each variable named\n        in shock_vars is stored in a T_sim x AgentCount array in an attribute of self named X_hist.\n        Automatically sets self.read_shocks to True so that these pre-specified shocks are used for\n        all subsequent calls to simulate().\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        # Make sure time is flowing forward and re-initialize the simulation\n        orig_time = self.time_flow\n        self.timeFwd()\n        self.initializeSim()\n\n        # Make blank history arrays for each shock variable\n        for var_name in self.shock_vars:\n            setattr(self,var_name+'_hist',np.zeros((self.T_sim,self.AgentCount))+np.nan)\n\n        # Make and store the history of shocks for each period\n        for t in range(self.T_sim):\n            self.getMortality()\n            self.getShocks()\n            for var_name in self.shock_vars:\n                exec('self.' + var_name + '_hist[self.t_sim,:] = self.' + var_name)\n            self.t_sim += 1\n            self.t_age = self.t_age + 1 # Age all consumers by one period\n            self.t_cycle = self.t_cycle + 1 # Age all consumers within their cycle\n            self.t_cycle[self.t_cycle == self.T_cycle] = 0 # Resetting to zero for those who have reached the end\n\n        # Restore the flow of time and flag that shocks can be read rather than simulated\n        self.read_shocks = True\n        if not orig_time:\n            self.timeRev()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef simDeath(self):\n        '''\n        Determines which agents in the current population \"die\" or should be replaced.  Takes no\n        inputs, returns a Boolean array of size self.AgentCount, which has True for agents who die\n        and False for those that survive. Returns all False by default, must be overwritten by a\n        subclass to have replacement events.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        who_dies : np.array\n            Boolean array of size self.AgentCount indicating which agents die and are replaced.\n        '''\n        print('AgentType subclass must define method simDeath!')\n        who_dies = np.ones(self.AgentCount,dtype=bool)\n        return who_dies", "response": "Returns a Boolean array of size self. AgentCount indicating which agents in the current population die and should be replaced."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef readShocks(self):\n        '''\n        Reads values of shock variables for the current period from history arrays.  For each var-\n        iable X named in self.shock_vars, this attribute of self is set to self.X_hist[self.t_sim,:].\n\n        This method is only ever called if self.read_shocks is True.  This can be achieved by using\n        the method makeShockHistory() (or manually after storing a \"handcrafted\" shock history).\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        for var_name in self.shock_vars:\n            setattr(self,var_name,getattr(self,var_name+'_hist')[self.t_sim,:])", "response": "Reads values of all iable X variables for the current period from history arrays."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsimulate this agent type for a given number of periods.", "response": "def simulate(self,sim_periods=None):\n        '''\n        Simulates this agent type for a given number of periods (defaults to self.T_sim if no input).\n        Records histories of attributes named in self.track_vars in attributes named varname_hist.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        # Ignore floating point \"errors\". Numpy calls it \"errors\", but really it's excep-\n        # tions with well-defined answers such as 1.0/0.0 that is np.inf, -1.0/0.0 that is\n        # -np.inf, np.inf/np.inf is np.nan and so on.\n        with np.errstate(divide='ignore', over='ignore', under='ignore', invalid='ignore'):\n            orig_time = self.time_flow\n            self.timeFwd()\n            if sim_periods is None:\n                sim_periods = self.T_sim\n\n            for t in range(sim_periods):\n                self.simOnePeriod()\n                for var_name in self.track_vars:\n                    exec('self.' + var_name + '_hist[self.t_sim,:] = self.' + var_name)\n                self.t_sim += 1\n\n            if not orig_time:\n                self.timeRev()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsolving the microeconomic problem for all AgentTypes in this market.", "response": "def solveAgents(self):\n        '''\n        Solves the microeconomic problem for all AgentTypes in this market.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        #for this_type in self.agents:\n        #    this_type.solve()\n        try:\n            multiThreadCommands(self.agents,['solve()'])\n        except Exception as err:\n            if self.print_parallel_error_once:\n                # Set flag to False so this is only printed once.\n                self.print_parallel_error_once = False\n                print(\"**** WARNING: could not execute multiThreadCommands in HARK.core.Market.solveAgents(), so using the serial version instead. This will likely be slower. The multiTreadCommands() functions failed with the following error:\", '\\n    ', sys.exc_info()[0], ':', err) #sys.exc_info()[0])\n            multiThreadCommandsFake(self.agents,['solve()'])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsolve the market by finding a dynamic rule that governs the aggregate dynamics and then performing a loop until the dynamic rule converges.", "response": "def solve(self):\n        '''\n        \"Solves\" the market by finding a \"dynamic rule\" that governs the aggregate\n        market state such that when agents believe in these dynamics, their actions\n        collectively generate the same dynamic rule.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        go              = True\n        max_loops       = self.max_loops # Failsafe against infinite solution loop\n        completed_loops = 0\n        old_dynamics    = None\n\n        while go: # Loop until the dynamic process converges or we hit the loop cap\n            self.solveAgents()     # Solve each AgentType's micro problem\n            self.makeHistory()     # \"Run\" the model while tracking aggregate variables\n            new_dynamics = self.updateDynamics() # Find a new aggregate dynamic rule\n\n            # Check to see if the dynamic rule has converged (if this is not the first loop)\n            if completed_loops > 0:\n                distance = new_dynamics.distance(old_dynamics)\n            else:\n                distance = 1000000.0\n\n            # Move to the next loop if the terminal conditions are not met\n            old_dynamics     = new_dynamics\n            completed_loops += 1\n            go               = distance >= self.tolerance and completed_loops < max_loops\n\n        self.dynamics = new_dynamics"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reap(self):\n        '''\n        Collects attributes named in reap_vars from each AgentType in the market,\n        storing them in respectively named attributes of self.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        for var_name in self.reap_vars:\n            harvest = []\n            for this_type in self.agents:\n                harvest.append(getattr(this_type,var_name))\n            setattr(self,var_name,harvest)", "response": "Reaps the attributes of the current object from each AgentType in the market and stores them in respectively named attributes of self."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the attributes of all agents in this agent to each AgentType in the market storing them in respectively named attributes.", "response": "def sow(self):\n        '''\n        Distributes attrributes named in sow_vars from self to each AgentType\n        in the market, storing them in respectively named attributes.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        for var_name in self.sow_vars:\n            this_seed = getattr(self,var_name)\n            for this_type in self.agents:\n                setattr(this_type,var_name,this_seed)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mill(self):\n        '''\n        Processes the variables collected from agents using the function millRule,\n        storing the results in attributes named in aggr_sow.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        # Make a dictionary of inputs for the millRule\n        reap_vars_string = ''\n        for name in self.reap_vars:\n            reap_vars_string += ' \\'' + name + '\\' : self.' + name + ','\n        const_vars_string = ''\n        for name in self.const_vars:\n            const_vars_string += ' \\'' + name + '\\' : self.' + name + ','\n        mill_dict = eval('{' + reap_vars_string + const_vars_string + '}')\n\n        # Run the millRule and store its output in self\n        product = self.millRule(**mill_dict)\n        for j in range(len(self.sow_vars)):\n            this_var = self.sow_vars[j]\n            this_product = getattr(product,this_var)\n            setattr(self,this_var,this_product)", "response": "This method processes the variables collected from agents using the function millRule and stores the results in attributes named in aggr_sow."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reset(self):\n        '''\n        Reset the state of the market (attributes in sow_vars, etc) to some\n        user-defined initial state, and erase the histories of tracked variables.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        for var_name in self.track_vars: # Reset the history of tracked variables\n            setattr(self,var_name + '_hist',[])\n        for var_name in self.sow_vars: # Set the sow variables to their initial levels\n            initial_val = getattr(self,var_name + '_init')\n            setattr(self,var_name,initial_val)\n        for this_type in self.agents: # Reset each AgentType in the market\n            this_type.reset()", "response": "Reset the state of the market to some specific state and erase the histories of tracked variables."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstore the current value of each variable X in an annova attribute named X_hist.", "response": "def store(self):\n        '''\n        Record the current value of each variable X named in track_vars in an\n        attribute named X_hist.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        for var_name in self.track_vars:\n            value_now = getattr(self,var_name)\n            getattr(self,var_name + '_hist').append(value_now)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns a loop of sow cultivate reap mill act_T times tracking the evolution of variables X named in track_vars in attributes named X_hist.", "response": "def makeHistory(self):\n        '''\n        Runs a loop of sow-->cultivate-->reap-->mill act_T times, tracking the\n        evolution of variables X named in track_vars in attributes named X_hist.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        self.reset() # Initialize the state of the market\n        for t in range(self.act_T):\n            self.sow()       # Distribute aggregated information/state to agents\n            self.cultivate() # Agents take action\n            self.reap()      # Collect individual data from agents\n            self.mill()      # Process individual data into aggregate data\n            self.store()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating a new dynamic rule using the history of variables in track_vars and distributes this rule to AgentTypes in agents.", "response": "def updateDynamics(self):\n        '''\n        Calculates a new \"aggregate dynamic rule\" using the history of variables\n        named in track_vars, and distributes this rule to AgentTypes in agents.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        dynamics : instance\n            The new \"aggregate dynamic rule\" that agents believe in and act on.\n            Should have attributes named in dyn_vars.\n        '''\n        # Make a dictionary of inputs for the dynamics calculator\n        history_vars_string = ''\n        arg_names = list(getArgNames(self.calcDynamics))\n        if 'self' in arg_names:\n            arg_names.remove('self')\n        for name in arg_names:\n            history_vars_string += ' \\'' + name + '\\' : self.' + name + '_hist,'\n        update_dict = eval('{' + history_vars_string + '}')\n\n        # Calculate a new dynamic rule and distribute it to the agents in agent_list\n        dynamics = self.calcDynamics(**update_dict) # User-defined dynamics calculator\n        for var_name in self.dyn_vars:\n            this_obj = getattr(dynamics,var_name)\n            for this_type in self.agents:\n                setattr(this_type,var_name,this_obj)\n        return dynamics"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getArgNames(function):\n    '''\n    Returns a list of strings naming all of the arguments for the passed function.\n\n    Parameters\n    ----------\n    function : function\n        A function whose argument names are wanted.\n\n    Returns\n    -------\n    argNames : [string]\n        The names of the arguments of function.\n    '''\n    argCount = function.__code__.co_argcount\n    argNames = function.__code__.co_varnames[:argCount]\n    return argNames", "response": "Returns a list of strings naming all of the arguments of the passed function."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef CRRAutility(c, gam):\n    '''\n    Evaluates constant relative risk aversion (CRRA) utility of consumption c\n    given risk aversion parameter gam.\n\n    Parameters\n    ----------\n    c : float\n        Consumption value\n    gam : float\n        Risk aversion\n\n    Returns\n    -------\n    (unnamed) : float\n        Utility\n\n    Tests\n    -----\n    Test a value which should pass:\n    >>> c, gamma = 1.0, 2.0    # Set two values at once with Python syntax\n    >>> utility(c=c, gam=gamma)\n    -1.0\n    '''\n    if gam == 1:\n        return np.log(c)\n    else:\n        return( c**(1.0 - gam) / (1.0 - gam) )", "response": "Evaluates constant relative risk aversion utility of consumption c\n    given risk aversion parameter gam."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nevaluates the inverse of the CRRA utility function at a given utility level u.", "response": "def CRRAutility_inv(u, gam):\n    '''\n    Evaluates the inverse of the CRRA utility function (with risk aversion para-\n    meter gam) at a given utility level u.\n\n    Parameters\n    ----------\n    u : float\n        Utility value\n    gam : float\n        Risk aversion\n\n    Returns\n    -------\n    (unnamed) : float\n        Consumption corresponding to given utility value\n    '''\n    if gam == 1:\n        return np.exp(u)\n    else:\n        return( ((1.0-gam)*u)**(1/(1.0-gam)) )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef CRRAutility_invP(u, gam):\n    '''\n    Evaluates the derivative of the inverse of the CRRA utility function (with\n    risk aversion parameter gam) at a given utility level u.\n\n    Parameters\n    ----------\n    u : float\n        Utility value\n    gam : float\n        Risk aversion\n\n    Returns\n    -------\n    (unnamed) : float\n        Marginal consumption corresponding to given utility value\n    '''\n    if gam == 1:\n        return np.exp(u)\n    else:\n        return( ((1.0-gam)*u)**(gam/(1.0-gam)) )", "response": "Evaluates the derivative of the inverse of the CRRA utility function at a given utility level u and the given risk aversion parameter gam."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef approxLognormal(N, mu=0.0, sigma=1.0, tail_N=0, tail_bound=[0.02,0.98], tail_order=np.e):\n    '''\n    Construct a discrete approximation to a lognormal distribution with underlying\n    normal distribution N(mu,sigma).  Makes an equiprobable distribution by\n    default, but user can optionally request augmented tails with exponentially\n    sized point masses.  This can improve solution accuracy in some models.\n\n    Parameters\n    ----------\n    N: int\n        Number of discrete points in the \"main part\" of the approximation.\n    mu: float\n        Mean of underlying normal distribution.\n    sigma: float\n        Standard deviation of underlying normal distribution.\n    tail_N: int\n        Number of points in each \"tail part\" of the approximation; 0 = no tail.\n    tail_bound: [float]\n        CDF boundaries of the tails vs main portion; tail_bound[0] is the lower\n        tail bound, tail_bound[1] is the upper tail bound.  Inoperative when\n        tail_N = 0.  Can make \"one tailed\" approximations with 0.0 or 1.0.\n    tail_order: float\n        Factor by which consecutive point masses in a \"tail part\" differ in\n        probability.  Should be >= 1 for sensible spacing.\n\n    Returns\n    -------\n    pmf: np.ndarray\n        Probabilities for discrete probability mass function.\n    X: np.ndarray\n        Discrete values in probability mass function.\n\n    Written by Luca Gerotto\n    Based on Matab function \"setup_workspace.m,\" from Chris Carroll's\n      [Solution Methods for Microeconomic Dynamic Optimization Problems]\n      (http://www.econ2.jhu.edu/people/ccarroll/solvingmicrodsops/) toolkit.\n    Latest update: 11 February 2017 by Matthew N. White\n    '''\n    # Find the CDF boundaries of each segment\n    if sigma > 0.0:\n        if tail_N > 0:\n            lo_cut     = tail_bound[0]\n            hi_cut     = tail_bound[1]\n        else:\n            lo_cut     = 0.0\n            hi_cut     = 1.0\n        inner_size     = hi_cut - lo_cut\n        inner_CDF_vals = [lo_cut + x*N**(-1.0)*inner_size for x in range(1, N)]\n        if inner_size < 1.0:\n            scale      = 1.0/tail_order\n            mag        = (1.0-scale**tail_N)/(1.0-scale)\n        lower_CDF_vals = [0.0]\n        if lo_cut > 0.0:\n            for x in range(tail_N-1,-1,-1):\n                lower_CDF_vals.append(lower_CDF_vals[-1] + lo_cut*scale**x/mag)\n        upper_CDF_vals  = [hi_cut]\n        if hi_cut < 1.0:\n            for x in range(tail_N):\n                upper_CDF_vals.append(upper_CDF_vals[-1] + (1.0-hi_cut)*scale**x/mag)\n        CDF_vals       = lower_CDF_vals + inner_CDF_vals + upper_CDF_vals\n        temp_cutoffs   = list(stats.lognorm.ppf(CDF_vals[1:-1], s=sigma, loc=0, scale=np.exp(mu)))\n        cutoffs        = [0] + temp_cutoffs + [np.inf]\n        CDF_vals       = np.array(CDF_vals)\n\n        # Construct the discrete approximation by finding the average value within each segment.\n        # This codeblock ignores warnings because it throws a \"divide by zero encountered in log\"\n        # warning due to computing erf(infty) at the tail boundary.  This is irrelevant and\n        # apparently freaks new users out.\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            K              = CDF_vals.size-1 # number of points in approximation\n            pmf            = CDF_vals[1:(K+1)] - CDF_vals[0:K]\n            X              = np.zeros(K)\n            for i in range(K):\n                zBot  = cutoffs[i]\n                zTop = cutoffs[i+1]\n                tempBot = (mu+sigma**2-np.log(zBot))/(np.sqrt(2)*sigma)\n                tempTop = (mu+sigma**2-np.log(zTop))/(np.sqrt(2)*sigma)\n                if tempBot <= 4:\n                    X[i] = -0.5*np.exp(mu+(sigma**2)*0.5)*(erf(tempTop) - erf(tempBot))/pmf[i]\n                else:\n                    X[i] = -0.5*np.exp(mu+(sigma**2)*0.5)*(erfc(tempBot) - erfc(tempTop))/pmf[i]\n\n    else:\n        pmf = np.ones(N)/N\n        X   = np.exp(mu)*np.ones(N)\n    return [pmf, X]", "response": "Construct a discrete approximation to a lognormal distribution with underlying normal distribution N mu and sigma."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating a discrete approximation to a mean one lognormal distribution.", "response": "def approxMeanOneLognormal(N, sigma=1.0, **kwargs):\n    '''\n    Calculate a discrete approximation to a mean one lognormal distribution.\n    Based on function approxLognormal; see that function's documentation for\n    further notes.\n\n    Parameters\n    ----------\n    N : int\n        Size of discrete space vector to be returned.\n    sigma : float\n        standard deviation associated with underlying normal probability distribution.\n\n    Returns\n    -------\n    X : np.array\n        Discrete points for discrete probability mass function.\n    pmf : np.array\n        Probability associated with each point in X.\n\n    Written by Nathan M. Palmer\n    Based on Matab function \"setup_shocks.m,\" from Chris Carroll's\n      [Solution Methods for Microeconomic Dynamic Optimization Problems]\n      (http://www.econ2.jhu.edu/people/ccarroll/solvingmicrodsops/) toolkit.\n    Latest update: 01 May 2015\n    '''\n    mu_adj = - 0.5*sigma**2;\n    pmf,X = approxLognormal(N=N, mu=mu_adj, sigma=sigma, **kwargs)\n    return [pmf,X]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates a discrete approximation to the beta distribution.", "response": "def approxBeta(N,a=1.0,b=1.0):\n    '''\n    Calculate a discrete approximation to the beta distribution.  May be quite\n    slow, as it uses a rudimentary numeric integration method to generate the\n    discrete approximation.\n\n    Parameters\n    ----------\n    N : int\n        Size of discrete space vector to be returned.\n    a : float\n        First shape parameter (sometimes called alpha).\n    b : float\n        Second shape parameter (sometimes called beta).\n\n    Returns\n    -------\n    X : np.array\n        Discrete points for discrete probability mass function.\n    pmf : np.array\n        Probability associated with each point in X.\n    '''\n    P    = 1000\n    vals = np.reshape(stats.beta.ppf(np.linspace(0.0,1.0,N*P),a,b),(N,P))\n    X    = np.mean(vals,axis=1)\n    pmf  = np.ones(N)/float(N)\n    return( [pmf, X] )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake a discrete approximation to a uniform distribution given its bottom and top limits and number of points.", "response": "def approxUniform(N,bot=0.0,top=1.0):\n    '''\n    Makes a discrete approximation to a uniform distribution, given its bottom\n    and top limits and number of points.\n\n    Parameters\n    ----------\n    N : int\n        The number of points in the discrete approximation\n    bot : float\n        The bottom of the uniform distribution\n    top : float\n        The top of the uniform distribution\n\n    Returns\n    -------\n    (unnamed) : np.array\n        An equiprobable discrete approximation to the uniform distribution.\n    '''\n    pmf = np.ones(N)/float(N)\n    center = (top+bot)/2.0\n    width = (top-bot)/2.0\n    X = center + width*np.linspace(-(N-1.0)/2.0,(N-1.0)/2.0,N)/(N/2.0)\n    return [pmf,X]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef makeMarkovApproxToNormal(x_grid,mu,sigma,K=351,bound=3.5):\n    '''\n    Creates an approximation to a normal distribution with mean mu and standard\n    deviation sigma, returning a stochastic vector called p_vec, corresponding\n    to values in x_grid.  If a RV is distributed x~N(mu,sigma), then the expectation\n    of a continuous function f() is E[f(x)] = numpy.dot(p_vec,f(x_grid)).\n\n    Parameters\n    ----------\n    x_grid: numpy.array\n        A sorted 1D array of floats representing discrete values that a normally\n        distributed RV could take on.\n    mu: float\n        Mean of the normal distribution to be approximated.\n    sigma: float\n        Standard deviation of the normal distribution to be approximated.\n    K: int\n        Number of points in the normal distribution to sample.\n    bound: float\n        Truncation bound of the normal distribution, as +/- bound*sigma.\n\n    Returns\n    -------\n    p_vec: numpy.array\n        A stochastic vector with probability weights for each x in x_grid.\n    '''\n    x_n = x_grid.size     # Number of points in the outcome grid\n    lower_bound = -bound  # Lower bound of normal draws to consider, in SD\n    upper_bound = bound   # Upper bound of normal draws to consider, in SD\n    raw_sample = np.linspace(lower_bound,upper_bound,K) # Evenly spaced draws between bounds\n    f_weights = stats.norm.pdf(raw_sample) # Relative probability of each draw\n    sample = mu + sigma*raw_sample # Adjusted bounds, given mean and stdev\n    w_vec = np.zeros(x_n) # A vector of outcome weights\n\n    # Find the relative position of each of the draws\n    sample_pos = np.searchsorted(x_grid,sample)\n    sample_pos[sample_pos < 1] = 1\n    sample_pos[sample_pos > x_n-1] = x_n-1\n\n    # Make arrays of the x_grid point directly above and below each draw\n    bot = x_grid[sample_pos-1]\n    top = x_grid[sample_pos]\n    alpha = (sample-bot)/(top-bot)\n\n    # Keep the weights (alpha) in bounds\n    alpha_clipped = np.clip(alpha,0.,1.)\n\n    # Loop through each x_grid point and add up the probability that each nearby\n    # draw contributes to it (accounting for distance)\n    for j in range(1,x_n):\n        c = sample_pos == j\n        w_vec[j-1] = w_vec[j-1] + np.dot(f_weights[c],1.0-alpha_clipped[c])\n        w_vec[j] = w_vec[j] + np.dot(f_weights[c],alpha_clipped[c])\n\n    # Reweight the probabilities so they sum to 1\n    W = np.sum(w_vec)\n    p_vec = w_vec/W\n\n    # Check for obvious errors, and return p_vec\n    assert (np.all(p_vec>=0.)) and (np.all(p_vec<=1.)) and (np.isclose(np.sum(p_vec),1.))\n    return p_vec", "response": "This function creates an approximation to a normal distribution with mean mu and standard deviation sigma returning a stochastic vector called p_vec corresponding to the values in x_grid."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef makeMarkovApproxToNormalByMonteCarlo(x_grid,mu,sigma,N_draws = 10000):\n    '''\n    Creates an approximation to a normal distribution with mean mu and standard\n    deviation sigma, by Monte Carlo.\n    Returns a stochastic vector called p_vec, corresponding\n    to values in x_grid.  If a RV is distributed x~N(mu,sigma), then the expectation\n    of a continuous function f() is E[f(x)] = numpy.dot(p_vec,f(x_grid)).\n\n    Parameters\n    ----------\n    x_grid: numpy.array\n        A sorted 1D array of floats representing discrete values that a normally\n        distributed RV could take on.\n    mu: float\n        Mean of the normal distribution to be approximated.\n    sigma: float\n        Standard deviation of the normal distribution to be approximated.\n    N_draws: int\n        Number of draws to use in Monte Carlo.\n\n    Returns\n    -------\n    p_vec: numpy.array\n        A stochastic vector with probability weights for each x in x_grid.\n    '''\n\n    # Take random draws from the desired normal distribution\n    random_draws = np.random.normal(loc = mu, scale = sigma, size = N_draws)\n\n    # Compute the distance between the draws and points in x_grid\n    distance = np.abs(x_grid[:,np.newaxis] - random_draws[np.newaxis,:])\n\n    # Find the indices of the points in x_grid that are closest to the draws\n    distance_minimizing_index = np.argmin(distance,axis=0)\n\n    # For each point in x_grid, the approximate probability of that point is the number\n    # of Monte Carlo draws that are closest to that point\n    p_vec = np.zeros_like(x_grid)\n    for p_index,p in enumerate(p_vec):\n        p_vec[p_index] = np.sum(distance_minimizing_index==p_index) / N_draws\n\n    # Check for obvious errors, and return p_vec\n    assert (np.all(p_vec>=0.)) and (np.all(p_vec<=1.)) and (np.isclose(np.sum(p_vec)),1.)\n    return p_vec", "response": "This function creates an approximation to a normal distribution with mean mu and standard deviation sigma by Monte Carlo draws."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef makeTauchenAR1(N, sigma=1.0, rho=0.9, bound=3.0):\n    '''\n    Function to return a discretized version of an AR1 process.\n    See http://www.fperri.net/TEACHING/macrotheory08/numerical.pdf for details\n\n    Parameters\n    ----------\n    N: int\n        Size of discretized grid\n    sigma: float\n        Standard deviation of the error term\n    rho: float\n        AR1 coefficient\n    bound: float\n        The highest (lowest) grid point will be bound (-bound) multiplied by the unconditional\n        standard deviation of the process\n\n    Returns\n    -------\n    y: np.array\n        Grid points on which the discretized process takes values\n    trans_matrix: np.array\n        Markov transition array for the discretized process\n\n    Written by Edmund S. Crawley\n    Latest update: 27 October 2017\n    '''\n    yN = bound*sigma/((1-rho**2)**0.5)\n    y = np.linspace(-yN,yN,N)\n    d = y[1]-y[0]\n    trans_matrix = np.ones((N,N))\n    for j in range(N):\n        for k_1 in range(N-2):\n            k=k_1+1\n            trans_matrix[j,k] = stats.norm.cdf((y[k] + d/2.0 - rho*y[j])/sigma) - stats.norm.cdf((y[k] - d/2.0 - rho*y[j])/sigma)\n        trans_matrix[j,0] = stats.norm.cdf((y[0] + d/2.0 - rho*y[j])/sigma)\n        trans_matrix[j,N-1] = 1.0 - stats.norm.cdf((y[N-1] - d/2.0 - rho*y[j])/sigma)\n\n    return y, trans_matrix", "response": "Function to return a discretized version of an AR1 process."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a discrete outcome of x with probability p to an existing distribution and returns the new distribution and the overall mean of the other outcomes.", "response": "def addDiscreteOutcomeConstantMean(distribution, x, p, sort = False):\n    '''\n    Adds a discrete outcome of x with probability p to an existing distribution,\n    holding constant the relative probabilities of other outcomes and overall mean.\n\n    Parameters\n    ----------\n    distribution : [np.array]\n        Two element list containing a list of probabilities and a list of outcomes.\n    x : float\n        The new value to be added to the distribution.\n    p : float\n        The probability of the discrete outcome x occuring.\n    sort: bool\n        Whether or not to sort X before returning it\n\n    Returns\n    -------\n    X : np.array\n        Discrete points for discrete probability mass function.\n    pmf : np.array\n        Probability associated with each point in X.\n\n    Written by Matthew N. White\n    Latest update: 08 December 2015 by David Low\n    '''\n    X   = np.append(x,distribution[1]*(1-p*x)/(1-p))\n    pmf = np.append(p,distribution[0]*(1-p))\n\n    if sort:\n        indices = np.argsort(X)\n        X       = X[indices]\n        pmf     = pmf[indices]\n\n    return([pmf,X])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef combineIndepDstns(*distributions):\n    '''\n    Given n lists (or tuples) whose elements represent n independent, discrete\n    probability spaces (probabilities and values), construct a joint pmf over\n    all combinations of these independent points.  Can take multivariate discrete\n    distributions as inputs.\n\n    Parameters\n    ----------\n    distributions : [np.array]\n        Arbitrary number of distributions (pmfs).  Each pmf is a list or tuple.\n        For each pmf, the first vector is probabilities and all subsequent vectors\n        are values.  For each pmf, this should be true:\n        len(X_pmf[0]) == len(X_pmf[j]) for j in range(1,len(distributions))\n\n    Returns\n    -------\n    List of arrays, consisting of:\n\n    P_out: np.array\n        Probability associated with each point in X_out.\n\n    X_out: np.array (as many as in *distributions)\n        Discrete points for the joint discrete probability mass function.\n\n    Written by Nathan Palmer\n    Latest update: 5 July August 2017 by Matthew N White\n    '''\n    # Very quick and incomplete parameter check:\n    for dist in distributions:\n        assert len(dist[0]) == len(dist[-1]), \"len(dist[0]) != len(dist[-1])\"\n\n    # Get information on the distributions\n    dist_lengths = ()\n    dist_dims = ()\n    for dist in distributions:\n        dist_lengths += (len(dist[0]),)\n        dist_dims += (len(dist)-1,)\n    number_of_distributions = len(distributions)\n\n    # Initialize lists we will use\n    X_out  = []\n    P_temp = []\n\n    # Now loop through the distributions, tiling and flattening as necessary.\n    for dd,dist in enumerate(distributions):\n\n        # The shape we want before we tile\n        dist_newshape = (1,) * dd + (len(dist[0]),) + \\\n                        (1,) * (number_of_distributions - dd)\n\n        # The tiling we want to do\n        dist_tiles    = dist_lengths[:dd] + (1,) + dist_lengths[dd+1:]\n\n        # Now we are ready to tile.\n        # We don't use the np.meshgrid commands, because they do not\n        # easily support non-symmetric grids.\n\n        # First deal with probabilities\n        Pmesh  = np.tile(dist[0].reshape(dist_newshape),dist_tiles) # Tiling\n        flatP  = Pmesh.ravel() # Flatten the tiled arrays\n        P_temp += [flatP,] #Add the flattened arrays to the output lists\n\n        # Then loop through each value variable\n        for n in range(1,dist_dims[dd]+1):\n            Xmesh  = np.tile(dist[n].reshape(dist_newshape),dist_tiles)\n            flatX  = Xmesh.ravel()\n            X_out  += [flatX,]\n\n    # We're done getting the flattened X_out arrays we wanted.\n    # However, we have a bunch of flattened P_temp arrays, and just want one\n    # probability array. So get the probability array, P_out, here.\n    P_out = np.prod(np.array(P_temp),axis=0)\n\n    assert np.isclose(np.sum(P_out),1),'Probabilities do not sum to 1!'\n    return [P_out,] + X_out", "response": "Given n lists or tuples containing n independent discrete probability spaces and probabilities and values construct a joint discrete probability mass function over the given distributions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef makeGridExpMult(ming, maxg, ng, timestonest=20):\n    '''\n    Make a multi-exponentially spaced grid.\n\n    Parameters\n    ----------\n    ming : float\n        Minimum value of the grid\n    maxg : float\n        Maximum value of the grid\n    ng : int\n        The number of grid points\n    timestonest : int\n        the number of times to nest the exponentiation\n\n    Returns\n    -------\n    points : np.array\n        A multi-exponentially spaced grid\n\n    Original Matab code can be found in Chris Carroll's\n    [Solution Methods for Microeconomic Dynamic Optimization Problems]\n    (http://www.econ2.jhu.edu/people/ccarroll/solvingmicrodsops/) toolkit.\n    Latest update: 01 May 2015\n    '''\n    if timestonest > 0:\n        Lming = ming\n        Lmaxg = maxg\n        for j in range(timestonest):\n            Lming = np.log(Lming + 1)\n            Lmaxg = np.log(Lmaxg + 1)\n        Lgrid = np.linspace(Lming,Lmaxg,ng)\n        grid = Lgrid\n        for j in range(timestonest):\n            grid = np.exp(grid) - 1\n    else:\n        Lming = np.log(ming)\n        Lmaxg = np.log(maxg)\n        Lstep = (Lmaxg - Lming)/(ng - 1)\n        Lgrid = np.arange(Lming,Lmaxg+0.000001,Lstep)\n        grid = np.exp(Lgrid)\n    return(grid)", "response": "This function creates a multi - exponentially spaced grid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate a weighted average of simulated data.", "response": "def calcWeightedAvg(data,weights):\n    '''\n    Generates a weighted average of simulated data.  The Nth row of data is averaged\n    and then weighted by the Nth element of weights in an aggregate average.\n\n    Parameters\n    ----------\n    data : numpy.array\n        An array of data with N rows of J floats\n    weights : numpy.array\n        A length N array of weights for the N rows of data.\n\n    Returns\n    -------\n    weighted_sum : float\n        The weighted sum of the data.\n    '''\n    data_avg = np.mean(data,axis=1)\n    weighted_sum = np.dot(data_avg,weights)\n    return weighted_sum"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getPercentiles(data,weights=None,percentiles=[0.5],presorted=False):\n    '''\n    Calculates the requested percentiles of (weighted) data.  Median by default.\n\n    Parameters\n    ----------\n    data : numpy.array\n        A 1D array of float data.\n    weights : np.array\n        A weighting vector for the data.\n    percentiles : [float]\n        A list of percentiles to calculate for the data.  Each element should\n        be in (0,1).\n    presorted : boolean\n        Indicator for whether data has already been sorted.\n\n    Returns\n    -------\n    pctl_out : numpy.array\n        The requested percentiles of the data.\n    '''\n    if weights is None: # Set equiprobable weights if none were passed\n        weights = np.ones(data.size)/float(data.size)\n\n    if presorted: # Sort the data if it is not already\n        data_sorted = data\n        weights_sorted = weights\n    else:\n        order = np.argsort(data)\n        data_sorted = data[order]\n        weights_sorted = weights[order]\n\n    cum_dist = np.cumsum(weights_sorted)/np.sum(weights_sorted) # cumulative probability distribution\n\n    # Calculate the requested percentiles by interpolating the data over the\n    # cumulative distribution, then evaluating at the percentile values\n    inv_CDF = interp1d(cum_dist,data_sorted,bounds_error=False,assume_sorted=True)\n    pctl_out = inv_CDF(percentiles)\n    return pctl_out", "response": "Calculates the requested percentiles of the data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the Lorenz curve at the requested percentiles of the data.", "response": "def getLorenzShares(data,weights=None,percentiles=[0.5],presorted=False):\n    '''\n    Calculates the Lorenz curve at the requested percentiles of (weighted) data.\n    Median by default.\n\n    Parameters\n    ----------\n    data : numpy.array\n        A 1D array of float data.\n    weights : numpy.array\n        A weighting vector for the data.\n    percentiles : [float]\n        A list of percentiles to calculate for the data.  Each element should\n        be in (0,1).\n    presorted : boolean\n        Indicator for whether data has already been sorted.\n\n    Returns\n    -------\n    lorenz_out : numpy.array\n        The requested Lorenz curve points of the data.\n    '''\n    if weights is None: # Set equiprobable weights if none were given\n        weights = np.ones(data.size)\n\n    if presorted: # Sort the data if it is not already\n        data_sorted = data\n        weights_sorted = weights\n    else:\n        order = np.argsort(data)\n        data_sorted = data[order]\n        weights_sorted = weights[order]\n\n    cum_dist = np.cumsum(weights_sorted)/np.sum(weights_sorted) # cumulative probability distribution\n    temp = data_sorted*weights_sorted\n    cum_data = np.cumsum(temp)/sum(temp) # cumulative ownership shares\n\n    # Calculate the requested Lorenz shares by interpolating the cumulative ownership\n    # shares over the cumulative distribution, then evaluating at requested points\n    lorenzFunc = interp1d(cum_dist,cum_data,bounds_error=False,assume_sorted=True)\n    lorenz_out = lorenzFunc(percentiles)\n    return lorenz_out"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the average of data that falls within the cutoff percentiles of a reference variable.", "response": "def calcSubpopAvg(data,reference,cutoffs,weights=None):\n    '''\n    Calculates the average of (weighted) data between cutoff percentiles of a\n    reference variable.\n\n    Parameters\n    ----------\n    data : numpy.array\n        A 1D array of float data.\n    reference : numpy.array\n        A 1D array of float data of the same length as data.\n    cutoffs : [(float,float)]\n        A list of doubles with the lower and upper percentile bounds (should be\n        in [0,1]).\n    weights : numpy.array\n        A weighting vector for the data.\n\n    Returns\n    -------\n    slice_avg\n        The (weighted) average of data that falls within the cutoff percentiles\n        of reference.\n\n    '''\n    if weights is None: # Set equiprobable weights if none were given\n        weights = np.ones(data.size)\n\n    # Sort the data and generate a cumulative distribution\n    order = np.argsort(reference)\n    data_sorted = data[order]\n    weights_sorted = weights[order]\n    cum_dist = np.cumsum(weights_sorted)/np.sum(weights_sorted)\n\n    # For each set of cutoffs, calculate the average of data that falls within\n    # the cutoff percentiles of reference\n    slice_avg = []\n    for j in range(len(cutoffs)):\n        bot = np.searchsorted(cum_dist,cutoffs[j][0])\n        top = np.searchsorted(cum_dist,cutoffs[j][1])\n        slice_avg.append(np.sum(data_sorted[bot:top]*weights_sorted[bot:top])/\n                         np.sum(weights_sorted[bot:top]))\n    return slice_avg"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms a non - parametric Nadaraya - Watson 1D kernel regression on given data.", "response": "def kernelRegression(x,y,bot=None,top=None,N=500,h=None):\n    '''\n    Performs a non-parametric Nadaraya-Watson 1D kernel regression on given data\n    with optionally specified range, number of points, and kernel bandwidth.\n\n    Parameters\n    ----------\n    x : np.array\n        The independent variable in the kernel regression.\n    y : np.array\n        The dependent variable in the kernel regression.\n    bot : float\n        Minimum value of interest in the regression; defaults to min(x).\n    top : float\n        Maximum value of interest in the regression; defaults to max(y).\n    N : int\n        Number of points to compute.\n    h : float\n        The bandwidth of the (Epanechnikov) kernel. To-do: GENERALIZE.\n\n    Returns\n    -------\n    regression : LinearInterp\n        A piecewise locally linear kernel regression: y = f(x).\n    '''\n    # Fix omitted inputs\n    if bot is None:\n        bot = np.min(x)\n    if top is None:\n        top = np.max(x)\n    if h is None:\n        h = 2.0*(top - bot)/float(N) # This is an arbitrary default\n\n    # Construct a local linear approximation\n    x_vec = np.linspace(bot,top,num=N)\n    y_vec = np.zeros_like(x_vec) + np.nan\n    for j in range(N):\n        x_here = x_vec[j]\n        weights = epanechnikovKernel(x,x_here,h)\n        y_vec[j] = np.dot(weights,y)/np.sum(weights)\n    regression = interp1d(x_vec,y_vec,bounds_error=False,assume_sorted=True)\n    return regression"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nevaluating the Epanechnikov kernel at each value of x", "response": "def epanechnikovKernel(x,ref_x,h=1.0):\n    '''\n    The Epanechnikov kernel.\n\n    Parameters\n    ----------\n    x : np.array\n        Values at which to evaluate the kernel\n    x_ref : float\n        The reference point\n    h : float\n        Kernel bandwidth\n\n    Returns\n    -------\n    out : np.array\n        Kernel values at each value of x\n    '''\n    u          = (x-ref_x)/h   # Normalize distance by bandwidth\n    these      = np.abs(u) <= 1.0 # Kernel = 0 outside [-1,1]\n    out        = np.zeros_like(x)   # Initialize kernel output\n    out[these] = 0.75*(1.0-u[these]**2.0) # Evaluate kernel\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plotFuncs(functions,bottom,top,N=1000,legend_kwds = None):\n    '''\n    Plots 1D function(s) over a given range.\n\n    Parameters\n    ----------\n    functions : [function] or function\n        A single function, or a list of functions, to be plotted.\n    bottom : float\n        The lower limit of the domain to be plotted.\n    top : float\n        The upper limit of the domain to be plotted.\n    N : int\n        Number of points in the domain to evaluate.\n    legend_kwds: None, or dictionary\n        If not None, the keyword dictionary to pass to plt.legend\n\n    Returns\n    -------\n    none\n    '''\n    if type(functions)==list:\n        function_list = functions\n    else:\n        function_list = [functions]\n\n    for function in function_list:\n        x = np.linspace(bottom,top,N,endpoint=True)\n        y = function(x)\n        plt.plot(x,y)\n    plt.xlim([bottom, top])\n    if legend_kwds is not None:\n        plt.legend(**legend_kwds)\n    plt.show()", "response": "Plots a list of functions over a given range."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plotFuncsDer(functions,bottom,top,N=1000,legend_kwds = None):\n    '''\n    Plots the first derivative of 1D function(s) over a given range.\n\n    Parameters\n    ----------\n    function : function\n        A function or list of functions, the derivatives of which are to be plotted.\n    bottom : float\n        The lower limit of the domain to be plotted.\n    top : float\n        The upper limit of the domain to be plotted.\n    N : int\n        Number of points in the domain to evaluate.\n    legend_kwds: None, or dictionary\n        If not None, the keyword dictionary to pass to plt.legend\n\n    Returns\n    -------\n    none\n    '''\n    if type(functions)==list:\n        function_list = functions\n    else:\n        function_list = [functions]\n\n    step = (top-bottom)/N\n    for function in function_list:\n        x = np.arange(bottom,top,step)\n        y = function.derivative(x)\n        plt.plot(x,y)\n    plt.xlim([bottom, top])\n    if legend_kwds is not None:\n        plt.legend(**legend_kwds)\n    plt.show()", "response": "Plots the first derivative of 1D functions over a given range."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake a descriptive statistics and macroeconomic data file.", "response": "def makeStickyEdataFile(Economy,ignore_periods,description='',filename=None,save_data=False,calc_micro_stats=True,meas_err_base=None):\n    '''\n    Makes descriptive statistics and macroeconomic data file. Behaves slightly\n    differently for heterogeneous agents vs representative agent models.\n\n    Parameters\n    ----------\n    Economy : Market or AgentType\n        A representation of the model economy.  For heterogeneous agents specifications,\n        this will be an instance of a subclass of Market.  For representative agent\n        specifications, this will be an instance of an AgentType subclass.\n    ignore_periods : int\n        Number of periods at the start of the simulation to throw out.\n    description : str\n        Description of the economy that is prepended on the output string.\n    filename : str\n        Name of the output log file, if any; .txt will be appended automatically.\n    save_data : bool\n        When True, save simulation data to filename + 'Data.txt' for use in Stata.\n    calc_micro_stats : bool\n        When True, calculate microeconomic statistics like in Table 2 of the\n        paper draft.\n    meas_err_base : float or None\n        Base value of measurement error standard deviation, which will be adjusted.\n        When None (default), value is calculated as stdev(DeltaLogC).\n\n    Returns\n    -------\n    None\n    '''\n    # Extract time series data from the economy\n    if hasattr(Economy,'agents'): # If this is a heterogeneous agent specification...\n        if len(Economy.agents) > 1:\n            pLvlAll_hist = np.concatenate([this_type.pLvlTrue_hist for this_type in Economy.agents],axis=1)\n            aLvlAll_hist = np.concatenate([this_type.aLvlNow_hist for this_type in Economy.agents],axis=1)\n            cLvlAll_hist = np.concatenate([this_type.cLvlNow_hist for this_type in Economy.agents],axis=1)\n            yLvlAll_hist = np.concatenate([this_type.yLvlNow_hist for this_type in Economy.agents],axis=1)\n        else: # Don't duplicate the data unless necessary (with one type, concatenating is useless)\n            pLvlAll_hist = Economy.agents[0].pLvlTrue_hist\n            aLvlAll_hist = Economy.agents[0].aLvlNow_hist\n            cLvlAll_hist = Economy.agents[0].cLvlNow_hist\n            yLvlAll_hist = Economy.agents[0].yLvlNow_hist\n        # PermShkAggHist needs to be shifted one period forward\n        PlvlAgg_hist = np.cumprod(np.concatenate(([1.0],Economy.PermShkAggHist[:-1]),axis=0))\n        AlvlAgg_hist = np.mean(aLvlAll_hist,axis=1) # Level of aggregate assets\n        AnrmAgg_hist = AlvlAgg_hist/PlvlAgg_hist # Normalized level of aggregate assets\n        ClvlAgg_hist = np.mean(cLvlAll_hist,axis=1) # Level of aggregate consumption\n        CnrmAgg_hist = ClvlAgg_hist/PlvlAgg_hist # Normalized level of aggregate consumption\n        YlvlAgg_hist = np.mean(yLvlAll_hist,axis=1) # Level of aggregate income\n        YnrmAgg_hist = YlvlAgg_hist/PlvlAgg_hist # Normalized level of aggregate income\n\n        if calc_micro_stats: # Only calculate stats if requested.  This is a memory hog with many simulated periods\n            micro_stat_periods = int((Economy.agents[0].T_sim-ignore_periods)*0.1)\n            not_newborns = (np.concatenate([this_type.t_age_hist[(ignore_periods+1):(ignore_periods+micro_stat_periods),:] for this_type in Economy.agents],axis=1) > 1).flatten()\n            Logc = np.log(cLvlAll_hist[ignore_periods:(ignore_periods+micro_stat_periods),:])\n            DeltaLogc = (Logc[1:] - Logc[0:-1]).flatten()\n            DeltaLogc_trimmed = DeltaLogc[not_newborns]\n            Loga = np.log(aLvlAll_hist[ignore_periods:(ignore_periods+micro_stat_periods),:])\n            DeltaLoga = (Loga[1:] - Loga[0:-1]).flatten()\n            DeltaLoga_trimmed = DeltaLoga[not_newborns]\n            Logp = np.log(pLvlAll_hist[ignore_periods:(ignore_periods+micro_stat_periods),:])\n            DeltaLogp = (Logp[1:] - Logp[0:-1]).flatten()\n            DeltaLogp_trimmed = DeltaLogp[not_newborns]\n            Logy = np.log(yLvlAll_hist[ignore_periods:(ignore_periods+micro_stat_periods),:])\n            Logy_trimmed = Logy\n            Logy_trimmed[np.isinf(Logy)] = np.nan\n            birth_events = np.concatenate([this_type.t_age_hist == 1 for this_type in Economy.agents],axis=1)\n            vBirth = calcValueAtBirth(cLvlAll_hist[ignore_periods:,:],birth_events[ignore_periods:,:],PlvlAgg_hist[ignore_periods:],Economy.MrkvNow_hist[ignore_periods:],Economy.agents[0].DiscFac,Economy.agents[0].CRRA)\n\n        BigTheta_hist = Economy.TranShkAggHist\n        if hasattr(Economy,'MrkvNow'):\n            Mrkv_hist = Economy.MrkvNow_hist\n            if ~hasattr(Economy,'Rfree'): # If this is a markov DSGE specification...\n                # Find the expected interest rate - approximate by assuming growth = expected growth\n                ExpectedGrowth_hist = Economy.PermGroFacAgg[Mrkv_hist]\n                ExpectedKLRatio_hist = AnrmAgg_hist/ExpectedGrowth_hist\n                ExpectedR_hist = Economy.Rfunc(ExpectedKLRatio_hist)\n\n    else: # If this is a representative agent specification...\n        PlvlAgg_hist = Economy.pLvlTrue_hist.flatten()\n        ClvlAgg_hist = Economy.cLvlNow_hist.flatten()\n        CnrmAgg_hist = ClvlAgg_hist/PlvlAgg_hist.flatten()\n        YnrmAgg_hist = Economy.yNrmTrue_hist.flatten()\n        YlvlAgg_hist = YnrmAgg_hist*PlvlAgg_hist.flatten()\n        AlvlAgg_hist = Economy.aLvlNow_hist.flatten()\n        AnrmAgg_hist = AlvlAgg_hist/PlvlAgg_hist.flatten()\n        BigTheta_hist = Economy.TranShkNow_hist.flatten()\n        if hasattr(Economy,'MrkvNow'):\n            Mrkv_hist = Economy.MrkvNow_hist\n\n    # Process aggregate data into forms used by regressions\n    LogC = np.log(ClvlAgg_hist[ignore_periods:])\n    LogA = np.log(AlvlAgg_hist[ignore_periods:])\n    LogY = np.log(YlvlAgg_hist[ignore_periods:])\n    DeltaLogC = LogC[1:] - LogC[0:-1]\n    DeltaLogA = LogA[1:] - LogA[0:-1]\n    DeltaLogY = LogY[1:] - LogY[0:-1]\n    A = AnrmAgg_hist[(ignore_periods+1):] # This is a relabeling for the regression code\n    BigTheta = BigTheta_hist[(ignore_periods+1):]\n    if hasattr(Economy,'MrkvNow'):\n        Mrkv = Mrkv_hist[(ignore_periods+1):] # This is a relabeling for the regression code\n        if ~hasattr(Economy,'Rfree') and hasattr(Economy,'agents'): # If this is a markov DSGE specification...\n            R = ExpectedR_hist[(ignore_periods+1):]\n    Delta8LogC = (np.log(ClvlAgg_hist[8:]) - np.log(ClvlAgg_hist[:-8]))[(ignore_periods-7):]\n    Delta8LogY = (np.log(YlvlAgg_hist[8:]) - np.log(YlvlAgg_hist[:-8]))[(ignore_periods-7):]\n\n    # Add measurement error to LogC\n    if meas_err_base is None:\n        meas_err_base = np.std(DeltaLogC)\n    sigma_meas_err = meas_err_base*0.375 # This approximately matches the change in IV vs OLS in U.S. empirical coefficients\n    np.random.seed(10)\n    Measurement_Error = sigma_meas_err*np.random.normal(0.,1.,LogC.size)\n    LogC_me = LogC + Measurement_Error\n    DeltaLogC_me = LogC_me[1:] - LogC_me[0:-1]\n\n    # Apply measurement error to long delta LogC\n    LogC_long = np.log(ClvlAgg_hist)\n    LogC_long_me = LogC_long + sigma_meas_err*np.random.normal(0.,1.,LogC_long.size)\n    Delta8LogC_me = (LogC_long_me[8:] - LogC_long_me[:-8])[(ignore_periods-7):]\n\n    # Make summary statistics for the results file\n    csv_output_string = str(np.mean(AnrmAgg_hist[ignore_periods:])) +\",\"+ str(np.mean(CnrmAgg_hist[ignore_periods:]))+ \",\"+str(np.std(np.log(AnrmAgg_hist[ignore_periods:])))+ \",\"+str(np.std(DeltaLogC))+ \",\"+str(np.std(DeltaLogY)) +\",\"+ str(np.std(DeltaLogA))\n    if hasattr(Economy,'agents') and calc_micro_stats: # This block only runs for heterogeneous agents specifications\n        csv_output_string += \",\"+str(np.mean(np.std(Loga,axis=1)))+ \",\"+str(np.mean(np.std(Logc,axis=1))) + \",\"+str(np.mean(np.std(Logp,axis=1))) +\",\"+ str(np.mean(np.nanstd(Logy_trimmed,axis=1))) +\",\"+ str(np.std(DeltaLoga_trimmed))+\",\"+ str(np.std(DeltaLogc_trimmed))+ \",\"+str(np.std(DeltaLogp_trimmed))\n\n    # Save the results to a logfile if requested\n    if filename is not None:\n        with open(results_dir + filename + 'Results.csv','w') as f:\n            f.write(csv_output_string)\n            f.close()\n        if calc_micro_stats and hasattr(Economy,'agents'):\n            with open(results_dir + filename + 'BirthValue.csv','w') as f:\n                my_writer = csv.writer(f, delimiter = ',')\n                my_writer.writerow(vBirth)\n                f.close()\n\n        if save_data:\n            DataArray = (np.vstack((np.arange(DeltaLogC.size),DeltaLogC_me,DeltaLogC,DeltaLogY,A,BigTheta,Delta8LogC,Delta8LogY,Delta8LogC_me,Measurement_Error[1:]))).transpose()\n            VarNames = ['time_period','DeltaLogC_me','DeltaLogC','DeltaLogY','A','BigTheta','Delta8LogC','Delta8LogY','Delta8LogC_me','Measurement_Error']\n            if hasattr(Economy,'MrkvNow'):\n                DataArray = np.hstack((DataArray,np.reshape(Mrkv,(Mrkv.size,1))))\n                VarNames.append('MrkvState')\n            if hasattr(Economy,'MrkvNow') & ~hasattr(Economy,'Rfree') and hasattr(Economy,'agents'):\n                DataArray = np.hstack((DataArray,np.reshape(R,(R.size,1))))\n                VarNames.append('R')\n            with open(results_dir + filename + 'Data.txt','w') as f:\n                my_writer = csv.writer(f, delimiter = '\\t')\n                my_writer.writerow(VarNames)\n                for i in range(DataArray.shape[0]):\n                    my_writer.writerow(DataArray[i,:])\n                f.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef runStickyEregressions(infile_name,interval_size,meas_err,sticky,all_specs):\n    '''\n    Runs regressions for the main tables of the StickyC paper and produces a LaTeX\n    table with results for one \"panel\".\n\n    Parameters\n    ----------\n    infile_name : str\n        Name of tab-delimited text file with simulation data.  Assumed to be in\n        the results directory, and was almost surely generated by makeStickyEdataFile\n        unless we resort to fabricating simulated data.  THAT'S A JOKE, FUTURE REFEREES.\n    interval_size : int\n        Number of periods in each sub-interval.\n    meas_err : bool\n        Indicator for whether to add measurement error to DeltaLogC.\n    sticky : bool\n        Indicator for whether these results used sticky expectations.\n    all_specs : bool\n        Indicator for whether this panel should include all specifications or\n        just the OLS on lagged consumption growth.\n\n    Returns\n    -------\n    panel_text : str\n        String with one panel's worth of LaTeX input.\n    '''\n    # Read in the data from the infile\n    with open(results_dir + infile_name + '.txt') as f:\n        my_reader = csv.reader(f, delimiter='\\t')\n        all_data = list(my_reader)\n\n    # Unpack the data into numpy arrays\n    obs = len(all_data) - 1\n    DeltaLogC_me = np.zeros(obs)\n    DeltaLogC = np.zeros(obs)\n    DeltaLogY = np.zeros(obs)\n    A = np.zeros(obs)\n    BigTheta = np.zeros(obs)\n    Delta8LogC = np.zeros(obs)\n    Delta8LogY = np.zeros(obs)\n    Delta8LogC_me = np.zeros(obs)\n    Measurement_Error = np.zeros(obs)\n    Mrkv_hist = np.zeros(obs,dtype=int)\n    R = np.zeros(obs)\n\n    has_mrkv = 'MrkvState' in all_data[0]\n    has_R = 'R' in all_data[0]\n    for i in range(obs):\n        j = i+1\n        DeltaLogC_me[i] = float(all_data[j][1])\n        DeltaLogC[i] = float(all_data[j][2])\n        DeltaLogY[i] = float(all_data[j][3])\n        A[i] = float(all_data[j][4])\n        BigTheta[i] = float(all_data[j][5])\n        Delta8LogC[i] = float(all_data[j][6])\n        Delta8LogY[i] = float(all_data[j][7])\n        Delta8LogC_me[i] = float(all_data[j][8])\n        Measurement_Error[i] = float(all_data[j][9])\n        if has_mrkv:\n            Mrkv_hist[i] = int(float(all_data[j][10]))\n        if has_R:\n            R[i] = float(all_data[j][11])\n\n    # Determine how many subsample intervals to run (and initialize array of coefficients)\n    N = DeltaLogC.size // interval_size\n    CoeffsArray = np.zeros((N,7)) # Order: DeltaLogC_OLS, DeltaLogC_IV, DeltaLogY_IV, A_OLS, DeltaLogC_HR, DeltaLogY_HR, A_HR\n    StdErrArray = np.zeros((N,7)) # Same order as above\n    RsqArray = np.zeros((N,5))\n    PvalArray = np.zeros((N,5))\n    OIDarray = np.zeros((N,5)) + np.nan\n    InstrRsqVec = np.zeros(N)\n\n    # Loop through subsample intervals, running various regressions\n    for n in range(N):\n        # Select the data subsample\n        start = n*interval_size\n        end = (n+1)*interval_size\n        if meas_err:\n            DeltaLogC_n = DeltaLogC_me[start:end]\n            Delta8LogC_n = Delta8LogC_me[start:end]\n        else:\n            DeltaLogC_n = DeltaLogC[start:end]\n            Delta8LogC_n = Delta8LogC[start:end]\n        DeltaLogY_n = DeltaLogY[start:end]\n        A_n = A[start:end]\n        Delta8LogY_n = Delta8LogY[start:end]\n\n        # Run OLS on log consumption\n        mod = sm.OLS(DeltaLogC_n[1:],sm.add_constant(DeltaLogC_n[0:-1]))\n        res = mod.fit()\n        CoeffsArray[n,0] = res._results.params[1]\n        StdErrArray[n,0] = res._results.HC0_se[1]\n        RsqArray[n,0] = res._results.rsquared_adj\n        PvalArray[n,0] = res._results.f_pvalue\n\n        # Define instruments for IV regressions\n        temp = np.transpose(np.vstack([DeltaLogC_n[1:-3],DeltaLogC_n[:-4],DeltaLogY_n[1:-3],DeltaLogY_n[:-4],A_n[1:-3],A_n[:-4],Delta8LogC_n[1:-3],Delta8LogY_n[1:-3]]))\n        instruments = sm.add_constant(temp) # With measurement error\n\n        # Run IV on log consumption\n        mod = sm.OLS(DeltaLogC_n[3:-1],instruments)\n        res = mod.fit()\n        DeltaLogC_predict = res.predict()\n        mod_2ndStage = sm.OLS(DeltaLogC_n[4:],sm.add_constant(DeltaLogC_predict))\n        res_2ndStage = mod_2ndStage.fit()\n        mod_IV = smsrg.IV2SLS(DeltaLogC_n[4:], sm.add_constant(DeltaLogC_n[3:-1]),instruments)\n        res_IV = mod_IV.fit()\n        CoeffsArray[n,1] = res_IV._results.params[1]\n        StdErrArray[n,1] = res_IV.bse[1]\n        RsqArray[n,1] = res_2ndStage._results.rsquared_adj\n        PvalArray[n,1] = res._results.f_pvalue\n\n        # Run IV on log income\n        mod = sm.OLS(DeltaLogY_n[4:],instruments)\n        res = mod.fit()\n        DeltaLogY_predict = res.predict()\n        mod_2ndStage = sm.OLS(DeltaLogC_n[4:],sm.add_constant(DeltaLogY_predict))\n        res_2ndStage = mod_2ndStage.fit()\n        mod_IV = smsrg.IV2SLS(DeltaLogC_n[4:], sm.add_constant(DeltaLogY_n[4:]),instruments)\n        res_IV = mod_IV.fit()\n        CoeffsArray[n,2] = res_IV._results.params[1]\n        StdErrArray[n,2] = res_IV.bse[1]\n        RsqArray[n,2] = res_2ndStage._results.rsquared_adj\n        PvalArray[n,2] = res._results.f_pvalue\n\n        # Run IV on assets\n        mod = sm.OLS(A_n[3:-1],instruments)\n        res = mod.fit()\n        A_predict = res.predict()\n        mod_2ndStage = sm.OLS(DeltaLogC_n[4:],sm.add_constant(A_predict))\n        res_2ndStage = mod_2ndStage.fit()\n        mod_IV = smsrg.IV2SLS(DeltaLogC_n[4:], sm.add_constant(A_n[3:-1]),instruments)\n        res_IV = mod_IV.fit()\n        CoeffsArray[n,3] = res_IV._results.params[1]\n        StdErrArray[n,3] = res_IV.bse[1]\n        RsqArray[n,3] = res_2ndStage._results.rsquared_adj\n        PvalArray[n,3] = res._results.f_pvalue\n\n        # Run horserace IV\n        regressors = sm.add_constant(np.transpose(np.array([DeltaLogC_predict,DeltaLogY_predict,A_predict])))\n        mod_2ndStage = sm.OLS(DeltaLogC_n[4:],regressors)\n        res_2ndStage = mod_2ndStage.fit()\n        mod_IV = smsrg.IV2SLS(DeltaLogC_n[4:], sm.add_constant(np.transpose(np.array([DeltaLogC_n[3:-1],DeltaLogY_n[4:],A_n[3:-1]]))),instruments)\n        res_IV = mod_IV.fit()\n        CoeffsArray[n,4] = res_IV._results.params[1]\n        CoeffsArray[n,5] = res_IV._results.params[2]\n        CoeffsArray[n,6] = res_IV._results.params[3]\n        StdErrArray[n,4] = res_IV._results.bse[1]\n        StdErrArray[n,5] = res_IV._results.bse[2]\n        StdErrArray[n,6] = res_IV._results.bse[3]\n        RsqArray[n,4] = res_2ndStage._results.rsquared_adj\n        PvalArray[n,4] = np.nan    #Need to put in KP stat here, may have to do this in Stata\n\n        # Regress Delta C_{t+1} on instruments\n        mod = sm.OLS(DeltaLogC_n[4:],instruments)\n        res = mod.fit()\n        InstrRsqVec[n] = res._results.rsquared_adj\n\n    # Count the number of times we reach significance in each variable\n    t_stat_array = CoeffsArray/StdErrArray\n    C_successes_95 = np.sum(t_stat_array[:,4] > 1.96)\n    Y_successes_95 = np.sum(t_stat_array[:,5] > 1.96)\n\n    sigma_meas_err = np.std(Measurement_Error)\n\n    N_out = [C_successes_95,Y_successes_95,N,np.mean(InstrRsqVec),sigma_meas_err**2]\n\n    # Make results table and return it\n    panel_text = makeResultsPanel(Coeffs=np.mean(CoeffsArray,axis=0),\n                     StdErrs=np.mean(StdErrArray,axis=0),\n                     Rsq=np.mean(RsqArray,axis=0),\n                     Pvals=np.mean(PvalArray,axis=0),\n                     OID=np.mean(OIDarray,axis=0),\n                     Counts=N_out,\n                     meas_err=meas_err,\n                     sticky=sticky,\n                     all_specs=all_specs)\n    return panel_text", "response": "Runs regressions for the main tables of the StickyC paper and produces a LaTeX\n    table with results for one panel."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef runStickyEregressionsInStata(infile_name,interval_size,meas_err,sticky,all_specs,stata_exe):\n    '''\n    Runs regressions for the main tables of the StickyC paper in Stata and produces a\n    LaTeX table with results for one \"panel\". Running in Stata allows production of\n    the KP-statistic, for which there is currently no command in statsmodels.api.\n\n    Parameters\n    ----------\n    infile_name : str\n        Name of tab-delimited text file with simulation data.  Assumed to be in\n        the results directory, and was almost surely generated by makeStickyEdataFile\n        unless we resort to fabricating simulated data.  THAT'S A JOKE, FUTURE REFEREES.\n    interval_size : int\n        Number of periods in each regression sample (or interval).\n    meas_err : bool\n        Indicator for whether to add measurement error to DeltaLogC.\n    sticky : bool\n        Indicator for whether these results used sticky expectations.\n    all_specs : bool\n        Indicator for whether this panel should include all specifications or\n        just the OLS on lagged consumption growth.\n    stata_exe : str\n        Absolute location where the Stata executable can be found on the computer\n        running this code.  Usually set at the top of StickyEparams.py.\n\n    Returns\n    -------\n    panel_text : str\n        String with one panel's worth of LaTeX input.\n    '''\n    dofile = \"StickyETimeSeries.do\"\n    infile_name_full = os.path.abspath(results_dir + infile_name + \".txt\")\n    temp_name_full = os.path.abspath(results_dir + \"temp.txt\")\n    if meas_err:\n        meas_err_stata = 1\n    else:\n        meas_err_stata = 0\n\n    # Define the command to run the Stata do file\n    cmd = [stata_exe, \"do\", dofile, infile_name_full, temp_name_full, str(interval_size), str(meas_err_stata)]\n\n    # Run Stata do-file\n    stata_status = subprocess.call(cmd,shell = 'true')\n    if stata_status!=0:\n        raise ValueError('Stata code could not run. Check the stata_exe in StickyEparams.py')\n    stata_output = pd.read_csv(temp_name_full, sep=',',header=0)\n\n    # Make results table and return it\n    panel_text = makeResultsPanel(Coeffs=stata_output.CoeffsArray,\n                     StdErrs=stata_output.StdErrArray,\n                     Rsq=stata_output.RsqArray,\n                     Pvals=stata_output.PvalArray,\n                     OID=stata_output.OIDarray,\n                     Counts=stata_output.ExtraInfo,\n                     meas_err=meas_err,\n                     sticky=sticky,\n                     all_specs=all_specs)\n    return panel_text", "response": "Runs regressions for the main tables of the StickyC paper in Stata and produces a LaTeX table with results for one panel."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the expected value of being born in each Markov state using the realizations of consumption level history for many consumers.", "response": "def calcValueAtBirth(cLvlHist,BirthBool,PlvlHist,MrkvHist,DiscFac,CRRA):\n    '''\n    Calculate expected value of being born in each Markov state using the realizations\n    of consumption for a history of many consumers.  The histories should already be\n    trimmed of the \"burn in\" periods.\n\n    Parameters\n    ----------\n    cLvlHist : np.array\n        TxN array of consumption level history for many agents across many periods.\n        Agents who die are replaced by newborms.\n    BirthBool : np.array\n        TxN boolean array indicating when agents are born, replacing one who died.\n    PlvlHist : np.array\n        T length vector of aggregate permanent productivity levels.\n    MrkvHist : np.array\n        T length vector of integers for the Markov index in each period.\n    DiscFac : float\n        Intertemporal discount factor.\n    CRRA : float\n        Coefficient of relative risk aversion.\n\n    Returns\n    -------\n    vAtBirth : np.array\n        J length vector of average lifetime value at birth by Markov state.\n    '''\n    J = np.max(MrkvHist) + 1 # Number of Markov states\n    T = MrkvHist.size        # Length of simulation\n    I = cLvlHist.shape[1]    # Number of agent indices in histories\n    u = lambda c : CRRAutility(c,gam=CRRA)\n\n    # Initialize an array to hold each agent's lifetime utility\n    BirthsByPeriod = np.sum(BirthBool,axis=1)\n    BirthsByState = np.zeros(J,dtype=int)\n    for j in range(J):\n        these = MrkvHist == j\n        BirthsByState[j] = np.sum(BirthsByPeriod[these])\n    N = np.max(BirthsByState) # Array must hold this many agents per row at least\n    vArray = np.zeros((J,N)) + np.nan\n    n = np.zeros(J,dtype=int)\n\n    # Loop through each agent index\n    DiscVec = DiscFac**np.arange(T)\n    for i in range(I):\n        birth_t = np.where(BirthBool[:,i])[0]\n        # Loop through each agent who lived and died in this index\n        for k in range(birth_t.size-1): # Last birth event has no death, so ignore\n            # Get lifespan of this agent and circumstances at birth\n            t0 = birth_t[k]\n            t1 = birth_t[k+1]\n            span = t1-t0\n            j = MrkvHist[t0]\n            # Calculate discounted flow of utility for this agent and store it\n            cVec = cLvlHist[t0:t1,i]/PlvlHist[t0]\n            uVec = u(cVec)\n            v = np.dot(DiscVec[:span],uVec)\n            vArray[j,n[j]] = v\n            n[j] += 1\n\n    # Calculate expected value at birth by state and return it\n    vAtBirth = np.nanmean(vArray,axis=1)\n    return vAtBirth"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake one panel of simulated results table. A panel has all results with/out measurement error for the sticky or frictionless version, and might have only one specification (OLS on lagged consumption growth) or many specifications. Parameters ---------- Coeffs : np.array Array with 7 entries with regression coefficients. StdErrs : np.array Array with 7 entries with coefficient standard errors. Rsq : np.array Array with 5 entries with R^2 values. Pvals : np.array Array with 5 entries with P values (for the first stage). OID : np.array Array with 5 entries with overidentification statistics. Counts : [int] List with 3 elements: [C_successes,Y_successes,N_intervals]. meas_err : bool Indicator for whether this panel used measurement error. sticky : bool Indicator for whether this panel used sticky expectations. all_specs : bool Indicator for whether this panel should include all specifications or just the OLS on lagged consumption growth. Returns ------- output : str Text string with one panel of LaTeX input.", "response": "def makeResultsPanel(Coeffs,StdErrs,Rsq,Pvals,OID,Counts,meas_err,sticky,all_specs):\n    '''\n    Make one panel of simulated results table.  A panel has all results with/out\n    measurement error for the sticky or frictionless version, and might have only\n    one specification (OLS on lagged consumption growth) or many specifications.\n\n    Parameters\n    ----------\n    Coeffs : np.array\n        Array with 7 entries with regression coefficients.\n    StdErrs : np.array\n        Array with 7 entries with coefficient standard errors.\n    Rsq : np.array\n        Array with 5 entries with R^2 values.\n    Pvals : np.array\n        Array with 5 entries with P values (for the first stage).\n    OID : np.array\n        Array with 5 entries with overidentification statistics.\n    Counts : [int]\n        List with 3 elements: [C_successes,Y_successes,N_intervals].\n    meas_err : bool\n        Indicator for whether this panel used measurement error.\n    sticky : bool\n        Indicator for whether this panel used sticky expectations.\n    all_specs : bool\n        Indicator for whether this panel should include all specifications or\n        just the OLS on lagged consumption growth.\n\n    Returns\n    -------\n    output : str\n        Text string with one panel of LaTeX input.\n    '''\n    # Define Delta log C text and expectations text\n    if meas_err:\n        DeltaLogC = '$\\Delta \\log \\mathbf{C}_{t}^*$'\n        DeltaLogC1 = '$\\Delta \\log \\mathbf{C}_{t+1}^*$'\n        MeasErr = ' (with measurement error $\\mathbf{C}_{t}^* =\\mathbf{C}_{t}\\\\times \\\\xi_t$);'\n    else:\n        DeltaLogC = '$\\Delta \\log \\mathbf{C}_{t}$'\n        DeltaLogC1 = '$\\Delta \\log \\mathbf{C}_{t+1}$'\n        MeasErr = ' (no measurement error)'\n\n    if sticky:\n        Expectations = 'Sticky'\n    else:\n        Expectations = 'Frictionless'\n\n    DeltaLogY1 = '$\\Delta \\log \\mathbf{Y}_{t+1}$'\n    A_t = '$A_{t}$'\n\n\n    # Define significance symbols\n    if Counts[2] > 1:\n        sig_symb = '\\\\bullet '\n    else:\n        sig_symb = '*'\n    def sigFunc(coeff,stderr):\n        z_stat = np.abs(coeff/stderr)\n        cuts = np.array([1.645,1.96,2.576])\n        N = np.sum(z_stat > cuts)\n        if N > 0:\n            sig_text = '^{' + N*sig_symb + '}'\n        else:\n            sig_text = ''\n        return sig_text\n\n    # Make the memo text\n    memo = ''\n    if all_specs:\n        memo += '\\\\\\\\ \\multicolumn{6}{l}{Memo: For instruments $\\mathbf{Z}_{t}$, ' + DeltaLogC + ' $= \\mathbf{Z}_{t} \\zeta, ~\\\\bar{R}^{2}=$ ' + mystr1(Counts[3])\n        if meas_err:\n            memo += '; ~$\\\\var(\\\\log(\\\\xi_t))=$ ' + mystr3(Counts[4])\n        memo += ' }  \\n'\n\n    # Make the top of the panel\n    output = '\\\\\\\\ \\\\midrule \\multicolumn{6}{l}{' + Expectations + ' : ' + DeltaLogC1 + MeasErr + '} \\n'\n    output += '\\\\\\\\ \\multicolumn{1}{c}{' + DeltaLogC + '} & \\multicolumn{1}{c}{' + DeltaLogY1 +'} & \\multicolumn{1}{c}{'+A_t+'} & & & \\n'\n\n    # OLS on just lagged consumption growth\n    output += '\\\\\\\\ ' + mystr1(Coeffs[0]) + sigFunc(Coeffs[0],StdErrs[0]) + ' & & & OLS & ' + mystr1(Rsq[0]) + ' & ' + mystr1(np.nan) + '\\n'\n    output += '\\\\\\\\ (' + mystr1(StdErrs[0]) + ') & & & & & \\n'\n\n    # Add the rest of the specifications if requested\n    if all_specs:\n        # IV on lagged consumption growth\n        output += '\\\\\\\\ ' + mystr1(Coeffs[1]) + sigFunc(Coeffs[1],StdErrs[1]) + ' & & & IV & ' + mystr1(Rsq[1]) + ' & ' + mystr1(Pvals[1]) + '\\n'\n        output += '\\\\\\\\ (' + mystr1(StdErrs[1]) + ') & & & & &' + mystr1(OID[1]) + '\\n'\n\n        # IV on expected income growth\n        output += '\\\\\\\\ & ' + mystr1(Coeffs[2]) + sigFunc(Coeffs[2],StdErrs[2]) + ' & & IV & ' + mystr1(Rsq[2]) + ' & ' + mystr1(Pvals[2]) + '\\n'\n        output += '\\\\\\\\ & (' + mystr1(StdErrs[2]) + ') & & & &' + mystr1(OID[2]) + '\\n'\n\n        # IV on aggregate assets\n        output += '\\\\\\\\ & & ' + mystr2(Coeffs[3]) + sigFunc(Coeffs[3],StdErrs[3]) + ' & IV & ' + mystr1(Rsq[3]) + ' & ' + mystr1(Pvals[3]) + '\\n'\n        output += '\\\\\\\\ & & (' + mystr2(StdErrs[3]) + ') & & &' + mystr1(OID[3]) + '\\n'\n\n        # Horse race\n        output += '\\\\\\\\ ' + mystr1(Coeffs[4]) + sigFunc(Coeffs[4],StdErrs[4]) + ' & ' + mystr1(Coeffs[5]) + sigFunc(Coeffs[5],StdErrs[5]) + ' & ' + mystr2(Coeffs[6]) + sigFunc(Coeffs[6],StdErrs[6]) + ' & IV & ' + mystr1(Rsq[4]) + ' & ' + mystr1(Pvals[4]) + '\\n'\n        output += '\\\\\\\\ (' + mystr1(StdErrs[4]) + ') & (' + mystr1(StdErrs[5]) + ') & (' + mystr2(StdErrs[6]) + ') & & & ' + mystr1(OID[4]) + '\\n'\n    output += memo\n\n    if Counts[0] is not None and Counts[2] > 1 and False:\n        output += '\\\\\\\\ \\multicolumn{6}{c}{Horserace coefficient on ' + DeltaLogC + ' significant at 95\\% level for ' + str(Counts[0]) + ' of ' + str(Counts[2]) + ' subintervals.} \\n'\n        output += '\\\\\\\\ \\multicolumn{6}{c}{Horserace coefficient on $\\mathbb{E}[\\Delta \\log \\mathbf{Y}_{t+1}]$ significant at 95\\% level for ' + str(Counts[1]) + ' of ' + str(Counts[2]) + ' subintervals.} \\n'\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef makeResultsTable(caption,panels,counts,filename,label):\n    '''\n    Make a time series regression results table by piecing together one or more panels.\n    Saves a tex file to disk in the tables directory.\n\n    Parameters\n    ----------\n    caption : str or None\n        Text to apply at the start of the table as a title.  If None, the table\n        environment is not invoked and the output is formatted for slides.\n    panels : [str]\n        List of strings with one or more panels, usually made by makeResultsPanel.\n    counts : int\n        List of two integers: [interval_length, interval_count]\n    filename : str\n        Name of the file in which to save output (in the ./Tables/ directory).\n    label : str\n        LaTeX \\label, for internal reference in the paper text.\n\n    Returns\n    -------\n    None\n    '''\n    if caption is not None:\n        note_size = '\\\\footnotesize'\n    else:\n        note_size = '\\\\tiny'\n    note = '\\\\multicolumn{6}{p{0.95\\\\textwidth}}{' + note_size + ' \\\\textbf{Notes:} '\n    if counts[1] > 1:\n        note += 'Reported statistics are the average values for ' + str(counts[1]) + ' samples of ' + str(counts[0]) + ' simulated quarters each.  '\n        note += 'Bullets indicate that the average sample coefficient divided by average sample standard error is outside of the inner 90\\%, 95\\%, and 99\\% of the standard normal distribution.  '\n    else:\n        note += 'Reported statistics are for a single simulation of ' + str(counts[0]) + ' quarters.  '\n        note += 'Stars indicate statistical significance at the 90\\%, 95\\%, and 99\\% levels, respectively.  '\n    note += 'Instruments $\\\\textbf{Z}_t = \\\\{\\Delta \\log \\mathbf{C}_{t-2}, \\Delta \\log \\mathbf{C}_{t-3}, \\Delta \\log \\mathbf{Y}_{t-2}, \\Delta \\log \\mathbf{Y}_{t-3}, A_{t-2}, A_{t-3}, \\Delta_8 \\log \\mathbf{C}_{t-2}, \\Delta_8 \\log \\mathbf{Y}_{t-2}   \\\\}$.'\n    note += '}'\n\n    if caption is not None:\n        output = '\\\\begin{minipage}{\\\\textwidth}\\n'\n        output += '\\\\begin{table} \\caption{' + caption + '} \\\\label{' + label + '} \\n'\n        output += '  \\\\centerline{$ \\Delta \\log \\mathbf{C}_{t+1} = \\\\varsigma + \\chi \\Delta \\log \\mathbf{C}_t + \\eta \\mathbb{E}_t[\\Delta \\log \\mathbf{Y}_{t+1}] + \\\\alpha A_t + \\epsilon_{t+1} $}\\n'\n    else:\n        output = '\\\\begin{center} \\n'\n        output += '$ \\Delta \\log \\mathbf{C}_{t+1} = \\\\varsigma + \\chi \\Delta \\log \\mathbf{C}_t + \\eta \\mathbb{E}_t[\\Delta \\log \\mathbf{Y}_{t+1}] + \\\\alpha A_t + \\epsilon_{t+1} $ \\\\\\\\  \\n'\n    output += '\\\\begin{tabular}{d{4}d{4}d{5}cd{4}c}\\n \\\\toprule \\n'\n    output += '\\multicolumn{3}{c}{Expectations : Dep Var} & OLS &  \\multicolumn{1}{c}{2${}^{\\\\text{nd}}$ Stage}  &  \\multicolumn{1}{c}{KP $p$-val} \\n'\n    output += '\\\\\\\\ \\multicolumn{3}{c}{Independent Variables} & or IV & \\multicolumn{1}{c}{$\\\\bar{R}^{2} $} & \\multicolumn{1}{c}{Hansen J $p$-val} \\n'\n\n    for panel in panels:\n        output += panel\n\n    output += '\\\\\\\\ \\\\bottomrule \\n ' + note + '\\n'\n    output += '\\end{tabular}\\n'\n\n    if caption is not None:\n        output += '\\end{table}\\n'\n        output += '\\end{minipage}\\n'\n    else:\n        output += '\\end{center}\\n'\n\n    with open(tables_dir + filename + '.tex','w') as f:\n        f.write(output)\n        f.close()", "response": "This function creates a time series regression results table by piecing together one or more panels and one or more files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef makeParameterTable(filename, params):\n    '''\n    Makes the parameter table for the paper, saving it to a tex file in the tables folder.\n    Also makes two partial parameter tables for the slides.\n\n    Parameters\n    ----------\n\n    filename : str\n        Name of the file in which to save output (in the tables directory).\n        Suffix .tex is automatically added.\n    params :\n        Object containing the parameter values.\n\n    Returns\n    -------\n    None\n    '''\n    # Calibrated macroeconomic parameters\n    macro_panel = \"\\multicolumn{3}{c}{\\\\textbf{Macroeconomic Parameters} }  \\n\"\n    macro_panel += \"\\\\\\\\ $\\\\kapShare$ & \" + \"{:.2f}\".format(params.CapShare) + \" & Capital's Share of Income   \\n\"\n    macro_panel += \"\\\\\\\\ $\\\\daleth$ & \" + \"{:.2f}\".format(params.DeprFacAnn) + \"^{1/4} & Depreciation Factor   \\n\"\n    macro_panel += \"\\\\\\\\ $\\sigma_{\\Theta}^{2}$ & \"+ \"{:.5f}\".format(params.TranShkAggVar) +\" & Variance Aggregate Transitory Shocks \\n\"\n    macro_panel += \"\\\\\\\\ $\\sigma_{\\Psi}^{2}$ & \"+ \"{:.5f}\".format(params.PermShkAggVar) +\" & Variance Aggregate Permanent Shocks \\n\"\n\n    # Steady state values\n    SS_panel = \"\\multicolumn{3}{c}{ \\\\textbf{Steady State of Perfect Foresight DSGE Model} } \\\\  \\n\"\n    SS_panel += \"\\\\\\\\ \\multicolumn{3}{c}{ $(\\\\sigma_{\\\\Psi}=\\\\sigma_{\\\\Theta}=\\\\sigma_{\\\\psi}=\\\\sigma_{\\\\theta}=\\wp=\\\\PDies=0$, $\\\\Phi_t = 1)$} \\\\  \\n\"\n    SS_panel += \"\\\\\\\\ $\\\\breve{K}/\\\\breve{K}^{\\\\kapShare}$ & \" + \"{:.1f}\".format(params.KYratioSS) + \" & SS Capital to Output Ratio  \\n\"\n    SS_panel += \"\\\\\\\\ $\\\\breve{K}$ & \" + \"{:.2f}\".format(params.KSS) + \" & SS Capital to Labor Productivity Ratio ($=12^{1/(1-\\\\kapShare)}$) \\n\"\n    SS_panel += \"\\\\\\\\ $\\\\breve{\\\\Wage}$ &  \" + \"{:.2f}\".format(params.wRteSS) + \" & SS Wage Rate ($=(1-\\\\kapShare)\\\\breve{K}^{\\\\kapShare}$) \\n\"\n    SS_panel += \"\\\\\\\\ $\\\\breve{\\\\mathsf{r}}$ & \" + \"{:.2f}\".format(params.rFreeSS) + \" & SS Interest Rate ($=\\\\kapShare \\\\breve{K}^{\\\\kapShare-1}$) \\n\"\n    SS_panel += \"\\\\\\\\ $\\\\breve{\\\\Rprod}$ & \" + \"{:.3f}\".format(params.RfreeSS) + \"& SS Between-Period Return Factor ($=\\\\daleth + \\\\breve{\\\\mathsf{r}}$) \\n\"\n\n    # Calibrated preference parameters\n    pref_panel = \"\\multicolumn{3}{c}{ \\\\textbf{Preference Parameters} }  \\n\"\n    pref_panel += \"\\\\\\\\ $\\\\rho$ & \"+ \"{:.0f}\".format(params.CRRA) +\". & Coefficient of Relative Risk Aversion \\n\"\n    pref_panel += \"\\\\\\\\ $\\\\beta_{SOE}$ &  \" + \"{:.3f}\".format(params.DiscFacSOE) +\" & SOE Discount Factor \\n\" #($=0.99 \\\\cdot \\\\PLives / (\\\\breve{\\\\mathcal{R}} \\\\Ex [\\\\pmb{\\\\psi}^{-\\CRRA}])$)\\n\"\n    pref_panel += \"\\\\\\\\ $\\\\beta_{DSGE}$ &  \" + \"{:.3f}\".format(params.DiscFacDSGE) +\" & HA-DSGE Discount Factor ($=\\\\breve{\\\\Rprod}^{-1}$) \\n\"\n    pref_panel += \"\\\\\\\\ $\\Pi$                    & \" + \"{:.2f}\".format(params.UpdatePrb) +\"  & Probability of Updating Expectations (if Sticky) \\n\"\n\n    # Idiosyncratic shock parameters\n    idio_panel = \"\\multicolumn{3}{c}{ \\\\textbf{Idiosyncratic Shock Parameters} }  \\n\"\n    idio_panel += \"\\\\\\\\ $\\sigma_{\\\\theta}^{2}$    & \" + \"{:.3f}\".format(params.TranShkVar) +\"     & Variance Idiosyncratic Tran Shocks (=$4 \\\\times$ Annual) \\n\"\n    idio_panel += \"\\\\\\\\ $\\sigma_{\\psi}^{2}$      &\" + \"{:.3f}\".format(params.PermShkVar) +\"      & Variance Idiosyncratic Perm Shocks (=$\\\\frac{1}{4} \\\\times$ Annual) \\n\"\n    idio_panel += \"\\\\\\\\ $\\wp$                    & \" + \"{:.3f}\".format(params.UnempPrb) +\"  & Probability of Unemployment Spell \\n\"\n    idio_panel += \"\\\\\\\\ $\\PDies$             & \" + \"{:.3f}\".format(params.DiePrb) +\"  & Probability of Mortality \\n\"\n\n    # Make full parameter table for paper\n    paper_output = \"\\provideboolean{Slides} \\setboolean{Slides}{false}  \\n\"\n\n    paper_output += \"\\\\begin{minipage}{\\\\textwidth}\\n\"\n    paper_output += \"  \\\\begin{table}\\n\"\n    paper_output += \"    \\\\caption{Calibration}\\label{table:calibration}\\n\"\n\n    paper_output += \"\\\\begin{tabular}{cd{5}l}  \\n\"\n    paper_output += \"\\\\\\\\ \\\\toprule  \\n\"\n    paper_output += macro_panel\n    paper_output += \"\\\\\\\\ \\\\midrule  \\n\"\n    paper_output += SS_panel\n    paper_output += \"\\\\\\\\ \\\\midrule  \\n\"\n    paper_output += pref_panel\n    paper_output += \"\\\\\\\\ \\\\midrule  \\n\"\n    paper_output += idio_panel\n    paper_output += \"\\\\\\\\ \\\\bottomrule  \\n\"\n    paper_output += \"\\end{tabular}\\n\"\n    paper_output += \"\\end{table}\\n\"\n    paper_output += \"\\end{minipage}\\n\"\n    paper_output += \"\\ifthenelse{\\\\boolean{StandAlone}}{\\end{document}}{}    \\n\"\n    with open(tables_dir + filename + '.tex','w') as f:\n        f.write(paper_output)\n        f.close()\n\n    # Make two partial parameter tables for the slides\n    slides1_output = \"\\\\begin{center}\\label{table:calibration1}  \\n\"\n    slides1_output += \"\\\\begin{tabular}{cd{5}l}  \\n\"\n    slides1_output += \"\\\\\\\\ \\\\toprule  \\n\"\n    slides1_output += macro_panel\n    slides1_output += \"\\\\\\\\ \\\\midrule  \\n\"\n    slides1_output += SS_panel\n    slides1_output += \"\\\\\\\\ \\\\bottomrule  \\n\"\n    slides1_output += \"\\end{tabular}  \\n\"\n    slides1_output += \"\\end{center}  \\n\"\n    with open(tables_dir + filename + '_1.tex','w') as f:\n        f.write(slides1_output)\n        f.close()\n\n    slides2_output = \"\\\\begin{center}\\label{table:calibration2}  \\n\"\n    slides2_output += \"\\\\begin{tabular}{cd{5}l}  \\n\"\n    slides2_output += \"\\\\\\\\ \\\\toprule  \\n\"\n    slides2_output += pref_panel\n    slides2_output += \"\\\\\\\\ \\\\midrule  \\n\"\n    slides2_output += idio_panel\n    slides2_output += \"\\\\\\\\ \\\\bottomrule  \\n\"\n    slides2_output += \"\\end{tabular}  \\n\"\n    slides2_output += \"\\end{center}  \\n\"\n    with open(tables_dir + filename + '_2.tex','w') as f:\n        f.write(slides2_output)\n        f.close()", "response": "Makes the parameter table for the paper saving it to a tex file in the tables folder."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking the equilibrium statistics table for the paper, saving it as a tex file in the tables folder. Also makes a version for the slides that doesn't use the table environment, nor include the note at bottom. Parameters ---------- out_filename : str Name of the file in which to save output (in the tables directory). Suffix .tex appended automatically. four_in_files: [str] A list with four csv files. 0) SOE frictionless 1) SOE Sticky 2) DSGE frictionless 3) DSGE sticky CRRA : float Coefficient of relative risk aversion Returns ------- None", "response": "def makeEquilibriumTable(out_filename, four_in_files, CRRA):\n    '''\n    Make the equilibrium statistics table for the paper, saving it as a tex file\n    in the tables folder.  Also makes a version for the slides that doesn't use\n    the table environment, nor include the note at bottom.\n\n    Parameters\n    ----------\n\n    out_filename : str\n        Name of the file in which to save output (in the tables directory).\n        Suffix .tex appended automatically.\n    four_in_files: [str]\n        A list with four csv files. 0) SOE frictionless 1) SOE Sticky 2) DSGE frictionless 3) DSGE sticky\n    CRRA : float\n        Coefficient of relative risk aversion\n\n    Returns\n    -------\n    None\n    '''\n    # Read in statistics from the four files\n    SOEfrictionless = np.genfromtxt(results_dir + four_in_files[0] + 'Results.csv', delimiter=',')\n    SOEsticky = np.genfromtxt(results_dir + four_in_files[1] + 'Results.csv', delimiter=',')\n    DSGEfrictionless = np.genfromtxt(results_dir + four_in_files[2] + 'Results.csv', delimiter=',')\n    DSGEsticky = np.genfromtxt(results_dir + four_in_files[3] + 'Results.csv', delimiter=',')\n\n    # Read in value at birth from the four files\n    vBirth_SOE_F = np.genfromtxt(results_dir + four_in_files[0] + 'BirthValue.csv', delimiter=',')\n    vBirth_SOE_S = np.genfromtxt(results_dir + four_in_files[1] + 'BirthValue.csv', delimiter=',')\n    vBirth_DSGE_F = np.genfromtxt(results_dir + four_in_files[2] + 'BirthValue.csv', delimiter=',')\n    vBirth_DSGE_S = np.genfromtxt(results_dir + four_in_files[3] + 'BirthValue.csv', delimiter=',')\n\n    # Calculate the cost of stickiness in the SOE and DSGE models\n    StickyCost_SOE = np.mean(1. - (vBirth_SOE_S/vBirth_SOE_F)**(1./(1.-CRRA)))\n    StickyCost_DSGE = np.mean(1. - (vBirth_DSGE_S/vBirth_DSGE_F)**(1./(1.-CRRA)))\n\n    paper_top = \"\\\\begin{minipage}{\\\\textwidth}\\n\"\n    paper_top += \"    \\\\begin{table}  \\n\"\n    paper_top += \"\\caption{Equilibrium Statistics}  \\n\"\n    paper_top += \"\\label{table:Eqbm}  \\n\"\n    paper_top += \"\\\\newsavebox{\\EqbmBox}  \\n\"\n    paper_top += \"\\sbox{\\EqbmBox}{  \\n\"\n    paper_top += \"\\\\newcommand{\\EqDir}{\\TablesDir/Eqbm}  \\n\"\n\n    slides_top = '\\\\begin{center} \\n'\n\n    main_table = \"\\\\begin{tabular}{lllcccc}  \\n\"\n    main_table += \"\\\\toprule \\n\"\n    main_table += \"&&& \\multicolumn{2}{c}{SOE Model} & \\multicolumn{2}{c}{HA-DSGE Model}   \\n\"\n    main_table += \"\\\\\\\\ %\\cline{4-5}   \\n\"\n    main_table += \"   &&& \\multicolumn{1}{c}{Frictionless} & \\multicolumn{1}{c}{Sticky} & \\multicolumn{1}{c}{Frictionless} & \\multicolumn{1}{c}{Sticky}  \\n\"\n    main_table += \"\\\\\\\\ \\\\midrule   \\n\"\n    main_table += \"  \\multicolumn{3}{l}{Means}  \\n\"\n    main_table += \"%\\\\\\\\  & & $M$  \\n\"\n    main_table += \"%\\\\\\\\  & & $K$  \\n\"\n    main_table += \"\\\\\\\\  & & $A$ & {:.2f}\".format(SOEfrictionless[0]) +\" &{:.2f}\".format(SOEsticky[0]) +\" & {:.2f}\".format(DSGEfrictionless[0]) +\" & {:.2f}\".format(DSGEsticky[0]) +\"   \\n\"\n    main_table += \"\\\\\\\\  & & $C$ & {:.2f}\".format(SOEfrictionless[1]) +\" &{:.2f}\".format(SOEsticky[1]) +\" & {:.2f}\".format(DSGEfrictionless[1]) +\" & {:.2f}\".format(DSGEsticky[1]) +\"   \\n\"\n    main_table += \"\\\\\\\\ \\\\midrule  \\n\"\n    main_table += \"  \\multicolumn{3}{l}{Standard Deviations}  \\n\"\n    main_table += \"\\\\\\\\ &    \\multicolumn{4}{l}{Aggregate Time Series (`Macro')}  \\n\"\n    main_table += \"%\\\\  & & $\\Delta \\log \\mathbf{M}$   \\n\"\n    main_table += \"\\\\\\\\ & & $\\log A $         & {:.3f}\".format(SOEfrictionless[2]) +\" & {:.3f}\".format(SOEsticky[2]) +\" & {:.3f}\".format(DSGEfrictionless[2]) +\" & {:.3f}\".format(DSGEsticky[2]) +\" \\n\"\n    main_table += \"\\\\\\\\ & & $\\Delta \\log \\\\CLevBF $  & {:.3f}\".format(SOEfrictionless[3]) +\" & {:.3f}\".format(SOEsticky[3]) +\" & {:.3f}\".format(DSGEfrictionless[3]) +\" & {:.3f}\".format(DSGEsticky[3]) +\" \\n\"\n    main_table += \"\\\\\\\\ & & $\\Delta \\log \\\\YLevBF $  & {:.3f}\".format(SOEfrictionless[4]) +\" & {:.3f}\".format(SOEsticky[4]) +\" & {:.3f}\".format(DSGEfrictionless[4]) +\" & {:.3f}\".format(DSGEsticky[4]) +\" \\n\"\n    main_table += \"\\\\\\\\ &   \\multicolumn{3}{l}{Individual Cross Sectional (`Micro')}  \\n\"\n    main_table += \"\\\\\\\\ & & $\\log \\\\aLevBF $  & {:.3f}\".format(SOEfrictionless[6]) +\" & {:.3f}\".format(SOEsticky[6]) +\" & {:.3f}\".format(DSGEfrictionless[6]) +\" & {:.3f}\".format(DSGEsticky[6]) +\" \\n\"\n    main_table += \"\\\\\\\\ & & $\\log \\\\cLevBF $  & {:.3f}\".format(SOEfrictionless[7]) +\" & {:.3f}\".format(SOEsticky[7]) +\" & {:.3f}\".format(DSGEfrictionless[7]) +\" & {:.3f}\".format(DSGEsticky[7]) +\" \\n\"\n    main_table += \"\\\\\\\\ & & $\\log p $  & {:.3f}\".format(SOEfrictionless[8]) +\" & {:.3f}\".format(SOEsticky[8]) +\" & {:.3f}\".format(DSGEfrictionless[8]) +\" & {:.3f}\".format(DSGEsticky[8]) +\" \\n\"\n    main_table += \"\\\\\\\\ & & $\\log \\\\yLevBF | \\\\yLevBF > 0 $  & {:.3f}\".format(SOEfrictionless[9]) +\" & {:.3f}\".format(SOEsticky[9]) +\" & {:.3f}\".format(DSGEfrictionless[9]) +\" & {:.3f}\".format(DSGEsticky[9]) +\" \\n\"\n    main_table += \"\\\\\\\\ & & $\\Delta \\log \\\\cLevBF $  & {:.3f}\".format(SOEfrictionless[11]) +\" & {:.3f}\".format(SOEsticky[11]) +\" & {:.3f}\".format(DSGEfrictionless[11]) +\" & {:.3f}\".format(DSGEsticky[11]) +\" \\n\"\n    main_table += \"  \\n\"\n    main_table += \"  \\n\"\n    main_table += \"\\\\\\\\ \\\\midrule \\multicolumn{3}{l}{Cost of Stickiness}  \\n\"\n    main_table += \" & \\multicolumn{2}{c}{\" + mystr2(StickyCost_SOE) + \"}  \\n\"\n    main_table += \" & \\multicolumn{2}{c}{\" + mystr2(StickyCost_DSGE) + \"}  \\n\"\n    main_table += \"\\\\\\\\ \\\\bottomrule  \\n\"\n    main_table += \" \\end{tabular}   \\n\"\n\n    paper_bot = \" } \\n \"\n    paper_bot += \"\\\\usebox{\\EqbmBox}  \\n\"\n    paper_bot += \"\\ifthenelse{\\\\boolean{StandAlone}}{\\\\newlength\\TableWidth}{}  \\n\"\n    paper_bot += \"\\settowidth\\TableWidth{\\\\usebox{\\EqbmBox}} % Calculate width of table so notes will match  \\n\"\n    paper_bot += \"\\medskip\\medskip \\\\vspace{0.0cm} \\parbox{\\TableWidth}{\\\\footnotesize\\n\"\n    paper_bot += \"\\\\textbf{Notes}: The cost of stickiness is calculated as the proportion by which the permanent income of a newborn frictionless consumer would need to be reduced in order to achieve the same reduction of expected value associated with forcing them to become a sticky expectations consumer.}  \\n\"\n    paper_bot += \"\\end{table}\\n\"\n    paper_bot += \"\\end{minipage}\\n\"\n    paper_bot += \"\\ifthenelse{\\\\boolean{StandAlone}}{\\end{document}}{}  \\n\"\n\n    slides_bot = '\\\\end{center} \\n'\n\n    paper_output = paper_top + main_table + paper_bot\n    with open(tables_dir + out_filename + '.tex','w') as f:\n        f.write(paper_output)\n        f.close()\n\n    slides_output = slides_top + main_table + slides_bot\n    with open(tables_dir + out_filename + 'Slides.tex','w') as f:\n        f.write(slides_output)\n        f.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extractSampleMicroData(Economy, num_periods, AgentCount, ignore_periods):\n    '''\n    Extracts sample micro data to be used in micro (cross section) regression.\n\n    Parameters\n    ----------\n\n    Economy : Economy\n        An economy (with one AgentType) for for which history has already been calculated.\n    num_periods : int\n        Number of periods to be stored (should be less than the number of periods calculated)\n    AgentCouunt : int\n        Number of agent histories to be stored (should be less than the AgentCount property of the agent)\n    ignore_periods : int\n        Number of periods at the beginning of the history to be discarded\n\n    Returns\n    -------\n    micro_data : np.array\n        Array with rows 1) logc_diff 2) log_trans_shk 3) top_assets\n    '''\n    # First pull out economy common data.\n    # Note indexing on Economy tracked vars is one ahead of agent.\n    agg_trans_shk_matrix = deepcopy(Economy.TranShkAggNow_hist[ignore_periods:ignore_periods+num_periods])\n    wRte_matrix = deepcopy(Economy.wRteNow_hist[ignore_periods:ignore_periods+num_periods])\n\n    # Now pull out agent data\n    agent = Economy.agents[0]\n    c_matrix = deepcopy(agent.cLvlNow_hist[(ignore_periods+1):ignore_periods+num_periods+1,0:AgentCount])\n    y_matrix = deepcopy(agent.yLvlNow_hist[(ignore_periods+1):ignore_periods+num_periods+1,0:AgentCount])\n    total_trans_shk_matrix = deepcopy(agent.TranShkNow_hist[(ignore_periods+1):ignore_periods+num_periods+1,0:AgentCount])\n    trans_shk_matrix = total_trans_shk_matrix/(np.array(agg_trans_shk_matrix)*np.array(wRte_matrix))[:,None]\n    a_matrix = deepcopy(agent.aLvlNow_hist[(ignore_periods+1):ignore_periods+num_periods+1,0:AgentCount])\n    pLvlTrue_matrix = deepcopy(agent.pLvlTrue_hist[(ignore_periods+1):ignore_periods+num_periods+1,0:AgentCount])\n    a_matrix_nrm = a_matrix/pLvlTrue_matrix\n    age_matrix = deepcopy(agent.t_age_hist[(ignore_periods+1):ignore_periods+num_periods+1,0:AgentCount])\n\n    # Put nan's in so that we do not regress over periods where agents die\n    newborn = age_matrix == 1\n    c_matrix[newborn] = np.nan\n    c_matrix[0:0,:] = np.nan\n    y_matrix[newborn] = np.nan\n    y_matrix[0,0:] = np.nan\n    y_matrix[trans_shk_matrix==0.0] = np.nan\n    c_matrix[trans_shk_matrix==0.0] = np.nan\n    trans_shk_matrix[trans_shk_matrix==0.0] = np.nan\n\n    top_assets = a_matrix_nrm > np.transpose(np.tile(np.percentile(a_matrix_nrm,1,axis=1),(np.shape(a_matrix_nrm)[1],1)))\n    logc_diff = np.log(c_matrix[1:,:])-np.log(c_matrix[:-1,:])\n    logy_diff = np.log(y_matrix[1:,:])-np.log(y_matrix[:-1,:])\n    logc_diff = logc_diff.flatten('F')\n    logy_diff = logy_diff.flatten('F')\n    log_trans_shk = np.log(trans_shk_matrix[1:,:].flatten('F'))\n    top_assets = top_assets[1:,:].flatten('F')\n\n    # Put nan's in where they exist in logc_diff\n    log_trans_shk = log_trans_shk + logc_diff*0.0\n    top_assets = top_assets + logc_diff*0.0\n\n    return np.stack((logc_diff,log_trans_shk,top_assets),1)", "response": "Extracts sample micro data from an economy."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef makeMicroRegressionTable(out_filename, micro_data):\n    '''\n    Make the micro regression or (cross section regression) table for the paper, saving\n    it to a tex file in the tables folder.  Also makes two partial tables for the slides.\n\n    Parameters\n    ----------\n\n    out_filename : str\n        Name of the file in which to save output (in the tables directory).\n        The suffix .tex is automatically added.\n    micro_data : [np.array]\n        A list of two np.array's each containing micro data array as returned by extractSampleMicroData\n\n    Returns\n    -------\n    None\n    '''\n    coeffs = np.zeros((6,2)) + np.nan\n    stdevs = np.zeros((6,2)) +np.nan\n    r_sq = np.zeros((4,2)) +np.nan\n    obs = np.zeros((4,2)) +np.nan\n    for i in range(2):\n        this_micro_data = micro_data[i]\n        logc_diff = this_micro_data[:,0]\n        log_trans_shk = this_micro_data[:,1]\n        top_assets = this_micro_data[:,2]\n\n        #Lagged consumption regression\n        mod = sm.OLS(logc_diff[1:],sm.add_constant(np.transpose(np.vstack([logc_diff[0:-1]]))), missing='drop')\n        res = mod.fit()\n        coeffs[0,i] = res._results.params[1]\n        stdevs[0,i] = res._results.HC0_se[1]\n        r_sq[0,i] = res._results.rsquared_adj\n        obs[0,i] = res.nobs\n\n        #Expected income regression\n        mod = sm.OLS(logc_diff[1:],sm.add_constant(np.transpose(np.vstack([-log_trans_shk[0:-1]]))), missing='drop')\n        res = mod.fit()\n        coeffs[1,i] = res._results.params[1]\n        stdevs[1,i] = res._results.HC0_se[1]\n        r_sq[1,i] = res._results.rsquared_adj\n        obs[1,i] = res.nobs\n\n        #Assets regression\n        mod = sm.OLS(logc_diff[1:],sm.add_constant(np.transpose(np.vstack([top_assets[0:-1]]))), missing='drop')\n        res = mod.fit()\n        coeffs[2,i] = res._results.params[1]\n        stdevs[2,i] = res._results.HC0_se[1]\n        r_sq[2,i] = res._results.rsquared_adj\n        obs[2,i] = res.nobs\n\n        #Horeserace regression\n        mod = sm.OLS(logc_diff[1:],sm.add_constant(np.transpose(np.vstack([logc_diff[0:-1],-log_trans_shk[0:-1],top_assets[0:-1]]))), missing='drop')\n        res = mod.fit()\n        coeffs[3,i] = res._results.params[1]\n        stdevs[3,i] = res._results.HC0_se[1]\n        coeffs[4,i] = res._results.params[2]\n        stdevs[4,i] = res._results.HC0_se[2]\n        coeffs[5,i] = res._results.params[3]\n        stdevs[5,i] = res._results.HC0_se[3]\n        r_sq[3,i] = res._results.rsquared_adj\n        obs[3,i] = res.nobs\n\n    paper_top = \"\\\\begin{minipage}{\\TableWidth}\\n\"\n    paper_top += \"  \\\\begin{table}\\n\"\n    paper_top += \"    \\\\caption{Micro Consumption Regression on Simulated Data} \\\\label{table:CGrowCross}\\n\"\n    paper_top += \"    \\\\begin{eqnarray} \\n\"\n    paper_top += \"\\\\CGrowCross    \\\\nonumber %\\\\\\CGrowCrossBar \\\\nonumber \\n\"\n    paper_top += \"    \\end{eqnarray}\\n\"\n\n    slides_top = '\\\\begin{center} \\n'\n\n    header = \"\\\\begin{tabular}{cd{4}d{4}d{5}ccc}  \\n\"\n    header += \"\\\\toprule  \\n\"\n    header += \"Model of     &                                &                                &                                 &                                       &                 \\\\\\\\  \\n\"\n    header += \"Expectations & \\multicolumn{1}{c}{$ \\chi $} & \\multicolumn{1}{c}{$ \\eta $} & \\multicolumn{1}{c}{$ \\\\alpha $} & \\multicolumn{1}{c}{$\\\\bar{R}^{2}$} &                   \\n\"\n\n    F_panel = \"\\\\\\\\ \\\\midrule \\n \\multicolumn{2}{l}{Frictionless}  \\n\"\n    F_panel += \"\\\\\\\\ &  {:.3f}\".format(coeffs[0,0]) +\"  &        &        & {:.3f}\".format(r_sq[0,0]) +\" &  \"+ \" %NotOnSlide   \\n\"\n    F_panel += \"\\\\\\\\ &  \\multicolumn{1}{c}{(--)}  &        &        &  &  \"+ \" %NotOnSlide   \\n\"\n    F_panel += \"\\\\\\\\ &    &    {:.3f}\".format(coeffs[1,0]) +\"    &        & {:.3f}\".format(r_sq[1,0])  +\" &  \"+\" %NotOnSlide   \\n\"\n    F_panel += \"\\\\\\\\ &    &  \\multicolumn{1}{c}{(--)}  &        &  &  \"+ \" %NotOnSlide   \\n\"\n    F_panel += \"\\\\\\\\ &    &        &     {:.3f}\".format(coeffs[2,0]) +\"   & {:.3f}\".format(r_sq[2,0]) +\" &  \" +\" %NotOnSlide   \\n\"\n    F_panel += \"\\\\\\\\ &    &       &  \\multicolumn{1}{c}{(--)}  &  &  \"+ \" %NotOnSlide   \\n\"\n    F_panel += \"\\\\\\\\ &  {:.3f}\".format(coeffs[3,0]) +\"  &    {:.3f}\".format(coeffs[4,0]) +\"    &     {:.3f}\".format(coeffs[5,0]) +\"   & {:.3f}\".format(r_sq[3,0]) +\" &    \\n\"\n    F_panel += \"\\\\\\\\ & \\multicolumn{1}{c}{(--)} &  \\multicolumn{1}{c}{(--)} &  \\multicolumn{1}{c}{(--)}  &  &  \"+ \" %NotOnSlide   \\n\"\n\n    S_panel = \"\\\\\\\\ \\\\midrule \\n \\multicolumn{2}{l}{Sticky}  \\n\"\n    S_panel += \"\\\\\\\\ &  {:.3f}\".format(coeffs[0,1]) +\"  &        &        & {:.3f}\".format(r_sq[0,1]) +\" &   %NotOnSlide   \\n\"\n    S_panel += \"\\\\\\\\ &  \\multicolumn{1}{c}{(--)}  &        &        &  &  \"+ \" %NotOnSlide   \\n\"\n    S_panel += \"\\\\\\\\ &    &    {:.3f}\".format(coeffs[1,1]) +\"    &        & {:.3f}\".format(r_sq[1,1]) +\" &   %NotOnSlide   \\n\"\n    S_panel += \"\\\\\\\\ &    &  \\multicolumn{1}{c}{(--)}  &        &  &  \"+ \" %NotOnSlide   \\n\"\n    S_panel += \"\\\\\\\\ &    &        &     {:.3f}\".format(coeffs[2,1]) +\"   & {:.3f}\".format(r_sq[2,1]) +\" &   %NotOnSlide   \\n\"\n    S_panel += \"\\\\\\\\ &    &       &  \\multicolumn{1}{c}{(--)}  &  &  \"+ \" %NotOnSlide   \\n\"\n    S_panel += \"\\\\\\\\ &  {:.3f}\".format(coeffs[3,1]) +\"  &    {:.3f}\".format(coeffs[4,1]) +\"    &     {:.3f}\".format(coeffs[5,1]) +\"   & {:.3f}\".format(r_sq[3,1]) +\" &    \\n\"\n    S_panel += \"\\\\\\\\ & \\multicolumn{1}{c}{(--)} &  \\multicolumn{1}{c}{(--)} &  \\multicolumn{1}{c}{(--)}  &  &  \"+ \" %NotOnSlide   \\n\"\n\n    paper_bot = \"  \\\\\\\\ \\\\bottomrule \\\\\\\\\\n\"\n    paper_bot += \"  \\multicolumn{5}{p{0.9\\\\textwidth}}{\\\\footnotesize \\\\textbf{Notes}: $\\\\mathbb{E}_{t,i}$ is the expectation from the perspective of person $i$ in period $t$; $\\\\bar{a}$ is a dummy variable indicating that agent $i$ is in the top 99 percent of the normalized $a$ distribution.  Simulated sample size is large enough such that standard errors are effectively zero.  Sample is restricted to households with positive income in period $t$. The notation ``(---)'' indicates that standard errors are close to zero, given the very large simulated sample size.}\\n\"\n    paper_bot += \"\\end{tabular}  \\n\"\n    paper_bot += \"\\end{table}\\n\"\n    paper_bot += \"\\end{minipage}\\n\"\n    paper_bot += \"\\ifthenelse{\\\\boolean{StandAlone}}{\\end{document}}{} \\n\"\n\n    slides_bot = \"\\\\\\\\ \\\\bottomrule  \\n\"\n    slides_bot += \"\\end{tabular}  \\n\"\n    slides_bot += '\\\\end{center} \\n'\n\n    # Make table for paper\n    paper_output = paper_top + header + F_panel + S_panel + paper_bot\n    with open(tables_dir + out_filename + '.tex','w') as f:\n        f.write(paper_output)\n        f.close()\n\n    # Make tables for slides\n    slidesF_output = slides_top + header + F_panel + slides_bot\n    with open(tables_dir + out_filename + '_SlidesF.tex','w') as f:\n        f.write(slidesF_output)\n        f.close()\n\n    slidesS_output = slides_top + header + S_panel + slides_bot\n    with open(tables_dir + out_filename + '_SlidesS.tex','w') as f:\n        f.write(slidesS_output)\n        f.close()", "response": "This function creates a micro regression table for the paper saving it to a tex file in the tables folder."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsolve a single period of a consumption - saving model with preference shocks and marginal utility shifters.", "response": "def solveConsPrefShock(solution_next,IncomeDstn,PrefShkDstn,\n                       LivPrb,DiscFac,CRRA,Rfree,PermGroFac,BoroCnstArt,\n                       aXtraGrid,vFuncBool,CubicBool):\n    '''\n    Solves a single period of a consumption-saving model with preference shocks\n    to marginal utility.  Problem is solved using the method of endogenous gridpoints.\n\n    Parameters\n    ----------\n    solution_next : ConsumerSolution\n        The solution to the succeeding one period problem.\n    IncomeDstn : [np.array]\n        A list containing three arrays of floats, representing a discrete\n        approximation to the income process between the period being solved\n        and the one immediately following (in solution_next). Order: event\n        probabilities, permanent shocks, transitory shocks.\n    PrefShkDstn : [np.array]\n        Discrete distribution of the multiplicative utility shifter.  Order:\n        probabilities, preference shocks.\n    LivPrb : float\n        Survival probability; likelihood of being alive at the beginning of\n        the succeeding period.\n    DiscFac : float\n        Intertemporal discount factor for future utility.\n    CRRA : float\n        Coefficient of relative risk aversion.\n    Rfree : float\n        Risk free interest factor on end-of-period assets.\n    PermGroGac : float\n        Expected permanent income growth factor at the end of this period.\n    BoroCnstArt: float or None\n        Borrowing constraint for the minimum allowable assets to end the\n        period with.  If it is less than the natural borrowing constraint,\n        then it is irrelevant; BoroCnstArt=None indicates no artificial bor-\n        rowing constraint.\n    aXtraGrid: np.array\n        Array of \"extra\" end-of-period asset values-- assets above the\n        absolute minimum acceptable level.\n    vFuncBool: boolean\n        An indicator for whether the value function should be computed and\n        included in the reported solution.\n    CubicBool: boolean\n        An indicator for whether the solver should use cubic or linear inter-\n        polation.\n\n    Returns\n    -------\n    solution: ConsumerSolution\n        The solution to the single period consumption-saving problem.  Includes\n        a consumption function cFunc (using linear splines), a marginal value\n        function vPfunc, a minimum acceptable level of normalized market re-\n        sources mNrmMin, normalized human wealth hNrm, and bounding MPCs MPCmin\n        and MPCmax.  It might also have a value function vFunc.  The consumption\n        function is defined over normalized market resources and the preference\n        shock, c = cFunc(m,PrefShk), but the (marginal) value function is defined\n        unconditionally on the shock, just before it is revealed.\n    '''\n    solver = ConsPrefShockSolver(solution_next,IncomeDstn,PrefShkDstn,LivPrb,\n                             DiscFac,CRRA,Rfree,PermGroFac,BoroCnstArt,aXtraGrid,\n                             vFuncBool,CubicBool)\n    solver.prepareToSolve()\n    solution = solver.solve()\n    return solution"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef solveConsKinkyPref(solution_next,IncomeDstn,PrefShkDstn,\n                       LivPrb,DiscFac,CRRA,Rboro,Rsave,PermGroFac,BoroCnstArt,\n                       aXtraGrid,vFuncBool,CubicBool):\n    '''\n    Solves a single period of a consumption-saving model with preference shocks\n    to marginal utility and a different interest rate on saving vs borrowing.\n    Problem is solved using the method of endogenous gridpoints.\n\n    Parameters\n    ----------\n    solution_next : ConsumerSolution\n        The solution to the succeeding one period problem.\n    IncomeDstn : [np.array]\n        A list containing three arrays of floats, representing a discrete\n        approximation to the income process between the period being solved\n        and the one immediately following (in solution_next). Order: event\n        probabilities, permanent shocks, transitory shocks.\n    PrefShkDstn : [np.array]\n        Discrete distribution of the multiplicative utility shifter.  Order:\n        probabilities, preference shocks.\n    LivPrb : float\n        Survival probability; likelihood of being alive at the beginning of\n        the succeeding period.\n    DiscFac : float\n        Intertemporal discount factor for future utility.\n    CRRA : float\n        Coefficient of relative risk aversion.\n    Rboro: float\n        Interest factor on assets between this period and the succeeding\n        period when assets are negative.\n    Rsave: float\n        Interest factor on assets between this period and the succeeding\n        period when assets are positive.\n    PermGroGac : float\n        Expected permanent income growth factor at the end of this period.\n    BoroCnstArt: float or None\n        Borrowing constraint for the minimum allowable assets to end the\n        period with.  If it is less than the natural borrowing constraint,\n        then it is irrelevant; BoroCnstArt=None indicates no artificial bor-\n        rowing constraint.\n    aXtraGrid: np.array\n        Array of \"extra\" end-of-period asset values-- assets above the\n        absolute minimum acceptable level.\n    vFuncBool: boolean\n        An indicator for whether the value function should be computed and\n        included in the reported solution.\n    CubicBool: boolean\n        An indicator for whether the solver should use cubic or linear inter-\n        polation.\n\n    Returns\n    -------\n    solution: ConsumerSolution\n        The solution to the single period consumption-saving problem.  Includes\n        a consumption function cFunc (using linear splines), a marginal value\n        function vPfunc, a minimum acceptable level of normalized market re-\n        sources mNrmMin, normalized human wealth hNrm, and bounding MPCs MPCmin\n        and MPCmax.  It might also have a value function vFunc.  The consumption\n        function is defined over normalized market resources and the preference\n        shock, c = cFunc(m,PrefShk), but the (marginal) value function is defined\n        unconditionally on the shock, just before it is revealed.\n    '''\n    solver = ConsKinkyPrefSolver(solution_next,IncomeDstn,PrefShkDstn,LivPrb,\n                             DiscFac,CRRA,Rboro,Rsave,PermGroFac,BoroCnstArt,\n                             aXtraGrid,vFuncBool,CubicBool)\n    solver.prepareToSolve()\n    solution = solver.solve()\n    return solution", "response": "Solve a single period of a consumption - saving model with preference shocks and a different interest rate on saving vs borrowing."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef updatePrefShockProcess(self):\n        '''\n        Make a discrete preference shock structure for each period in the cycle\n        for this agent type, storing them as attributes of self for use in the\n        solution (and other methods).\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        time_orig = self.time_flow\n        self.timeFwd()\n\n        PrefShkDstn = [] # discrete distributions of preference shocks\n        for t in range(len(self.PrefShkStd)):\n            PrefShkStd = self.PrefShkStd[t]\n            PrefShkDstn.append(approxMeanOneLognormal(N=self.PrefShkCount,\n                                                      sigma=PrefShkStd,tail_N=self.PrefShk_tail_N))\n\n        # Store the preference shocks in self (time-varying) and restore time flow\n        self.PrefShkDstn = PrefShkDstn\n        self.addToTimeVary('PrefShkDstn')\n        if not time_orig:\n            self.timeRev()", "response": "Updates the attributes of self for each period in the cycle\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getShocks(self):\n        '''\n        Gets permanent and transitory income shocks for this period as well as preference shocks.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        IndShockConsumerType.getShocks(self) # Get permanent and transitory income shocks\n        PrefShkNow = np.zeros(self.AgentCount) # Initialize shock array\n        for t in range(self.T_cycle):\n            these = t == self.t_cycle\n            N = np.sum(these)\n            if N > 0:\n                PrefShkNow[these] = self.RNG.permutation(approxMeanOneLognormal(N,sigma=self.PrefShkStd[t])[1])\n        self.PrefShkNow = PrefShkNow", "response": "Gets the permanent and transitory income shocks for this period as well as preference shocks."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getControls(self):\n        '''\n        Calculates consumption for each consumer of this type using the consumption functions.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        cNrmNow = np.zeros(self.AgentCount) + np.nan\n        for t in range(self.T_cycle):\n            these = t == self.t_cycle\n            cNrmNow[these] = self.solution[t].cFunc(self.mNrmNow[these],self.PrefShkNow[these])\n        self.cNrmNow = cNrmNow\n        return None", "response": "Calculates consumption for each consumer of this type using the consumption functions. Returns None if there are no consumers in this type."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds endogenous interpolation points for each asset point and each discrete preference shock. Parameters ---------- EndOfPrdvP : np.array Array of end-of-period marginal values. aNrmNow : np.array Array of end-of-period asset values that yield the marginal values in EndOfPrdvP. Returns ------- c_for_interpolation : np.array Consumption points for interpolation. m_for_interpolation : np.array Corresponding market resource points for interpolation.", "response": "def getPointsForInterpolation(self,EndOfPrdvP,aNrmNow):\n        '''\n        Find endogenous interpolation points for each asset point and each\n        discrete preference shock.\n\n        Parameters\n        ----------\n        EndOfPrdvP : np.array\n            Array of end-of-period marginal values.\n        aNrmNow : np.array\n            Array of end-of-period asset values that yield the marginal values\n            in EndOfPrdvP.\n\n        Returns\n        -------\n        c_for_interpolation : np.array\n            Consumption points for interpolation.\n        m_for_interpolation : np.array\n            Corresponding market resource points for interpolation.\n        '''\n        c_base       = self.uPinv(EndOfPrdvP)\n        PrefShkCount = self.PrefShkVals.size\n        PrefShk_temp = np.tile(np.reshape(self.PrefShkVals**(1.0/self.CRRA),(PrefShkCount,1)),\n                               (1,c_base.size))\n        self.cNrmNow = np.tile(c_base,(PrefShkCount,1))*PrefShk_temp\n        self.mNrmNow = self.cNrmNow + np.tile(aNrmNow,(PrefShkCount,1))\n\n        # Add the bottom point to the c and m arrays\n        m_for_interpolation = np.concatenate((self.BoroCnstNat*np.ones((PrefShkCount,1)),\n                                              self.mNrmNow),axis=1)\n        c_for_interpolation = np.concatenate((np.zeros((PrefShkCount,1)),self.cNrmNow),axis=1)\n        return c_for_interpolation,m_for_interpolation"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmake a basic solution object with a consumption function and marginal value function (unconditional on the preference shock). Parameters ---------- cNrm : np.array Consumption points for interpolation. mNrm : np.array Corresponding market resource points for interpolation. interpolator : function A function that constructs and returns a consumption function. Returns ------- solution_now : ConsumerSolution The solution to this period's consumption-saving problem, with a consumption function, marginal value function, and minimum m.", "response": "def usePointsForInterpolation(self,cNrm,mNrm,interpolator):\n        '''\n        Make a basic solution object with a consumption function and marginal\n        value function (unconditional on the preference shock).\n\n        Parameters\n        ----------\n        cNrm : np.array\n            Consumption points for interpolation.\n        mNrm : np.array\n            Corresponding market resource points for interpolation.\n        interpolator : function\n            A function that constructs and returns a consumption function.\n\n        Returns\n        -------\n        solution_now : ConsumerSolution\n            The solution to this period's consumption-saving problem, with a\n            consumption function, marginal value function, and minimum m.\n        '''\n        # Make the preference-shock specific consumption functions\n        PrefShkCount = self.PrefShkVals.size\n        cFunc_list   = []\n        for j in range(PrefShkCount):\n            MPCmin_j         = self.MPCminNow*self.PrefShkVals[j]**(1.0/self.CRRA)\n            cFunc_this_shock = LowerEnvelope(LinearInterp(mNrm[j,:],cNrm[j,:],\n                                             intercept_limit=self.hNrmNow*MPCmin_j,\n                                             slope_limit=MPCmin_j),self.cFuncNowCnst)\n            cFunc_list.append(cFunc_this_shock)\n\n        # Combine the list of consumption functions into a single interpolation\n        cFuncNow = LinearInterpOnInterp1D(cFunc_list,self.PrefShkVals)\n\n        # Make the ex ante marginal value function (before the preference shock)\n        m_grid = self.aXtraGrid + self.mNrmMinNow\n        vP_vec = np.zeros_like(m_grid)\n        for j in range(PrefShkCount): # numeric integration over the preference shock\n            vP_vec += self.uP(cFunc_list[j](m_grid))*self.PrefShkPrbs[j]*self.PrefShkVals[j]\n        vPnvrs_vec = self.uPinv(vP_vec)\n        vPfuncNow  = MargValueFunc(LinearInterp(m_grid,vPnvrs_vec),self.CRRA)\n\n        # Store the results in a solution object and return it\n        solution_now = ConsumerSolution(cFunc=cFuncNow, vPfunc=vPfuncNow, mNrmMin=self.mNrmMinNow)\n        return solution_now"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking the beginning-of-period value function (unconditional on the shock). Parameters ---------- solution : ConsumerSolution The solution to this single period problem, which must include the consumption function. Returns ------- vFuncNow : ValueFunc A representation of the value function for this period, defined over normalized market resources m: v = vFuncNow(m).", "response": "def makevFunc(self,solution):\n        '''\n        Make the beginning-of-period value function (unconditional on the shock).\n\n        Parameters\n        ----------\n        solution : ConsumerSolution\n            The solution to this single period problem, which must include the\n            consumption function.\n\n        Returns\n        -------\n        vFuncNow : ValueFunc\n            A representation of the value function for this period, defined over\n            normalized market resources m: v = vFuncNow(m).\n        '''\n        # Compute expected value and marginal value on a grid of market resources,\n        # accounting for all of the discrete preference shocks\n        PrefShkCount = self.PrefShkVals.size\n        mNrm_temp   = self.mNrmMinNow + self.aXtraGrid\n        vNrmNow     = np.zeros_like(mNrm_temp)\n        vPnow       = np.zeros_like(mNrm_temp)\n        for j in range(PrefShkCount):\n            this_shock  = self.PrefShkVals[j]\n            this_prob   = self.PrefShkPrbs[j]\n            cNrmNow     = solution.cFunc(mNrm_temp,this_shock*np.ones_like(mNrm_temp))\n            aNrmNow     = mNrm_temp - cNrmNow\n            vNrmNow    += this_prob*(this_shock*self.u(cNrmNow) + self.EndOfPrdvFunc(aNrmNow))\n            vPnow      += this_prob*this_shock*self.uP(cNrmNow)\n\n        # Construct the beginning-of-period value function\n        vNvrs        = self.uinv(vNrmNow) # value transformed through inverse utility\n        vNvrsP       = vPnow*self.uinvP(vNrmNow)\n        mNrm_temp    = np.insert(mNrm_temp,0,self.mNrmMinNow)\n        vNvrs        = np.insert(vNvrs,0,0.0)\n        vNvrsP       = np.insert(vNvrsP,0,self.MPCmaxEff**(-self.CRRA/(1.0-self.CRRA)))\n        MPCminNvrs   = self.MPCminNow**(-self.CRRA/(1.0-self.CRRA))\n        vNvrsFuncNow = CubicInterp(mNrm_temp,vNvrs,vNvrsP,MPCminNvrs*self.hNrmNow,MPCminNvrs)\n        vFuncNow     = ValueFunc(vNvrsFuncNow,self.CRRA)\n        return vFuncNow"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef simBirth(self,which_agents):\n        '''\n        Makes new consumers for the given indices.  Slightly extends base method by also setting\n        pLvlErrNow = 1.0 for new agents, indicating that they correctly perceive their productivity.\n\n        Parameters\n        ----------\n        which_agents : np.array(Bool)\n            Boolean array of size self.AgentCount indicating which agents should be \"born\".\n\n        Returns\n        -------\n        None\n        '''\n        AggShockConsumerType.simBirth(self,which_agents)\n        if hasattr(self,'pLvlErrNow'):\n            self.pLvlErrNow[which_agents] = 1.0\n        else:\n            self.pLvlErrNow = np.ones(self.AgentCount)", "response": "Makes new consumers for the given indices."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of updaters that can be used to update the log entries for the current period.", "response": "def getUpdaters(self):\n        '''\n        Determine which agents update this period vs which don't.  Fills in the\n        attributes update and dont as boolean arrays of size AgentCount.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        how_many_update = int(round(self.UpdatePrb*self.AgentCount))\n        base_bool = np.zeros(self.AgentCount,dtype=bool)\n        base_bool[0:how_many_update] = True\n        self.update = self.RNG.permutation(base_bool)\n        self.dont = np.logical_not(self.update)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the permanent and transitory shocks for the current period and aggregate ones.", "response": "def getShocks(self):\n        '''\n        Gets permanent and transitory shocks (combining idiosyncratic and aggregate shocks), but\n        only consumers who update their macroeconomic beliefs this period incorporate all pre-\n        viously unnoticed aggregate permanent shocks.  Agents correctly observe the level of all\n        real variables (market resources, consumption, assets, etc), but misperceive the aggregate\n        productivity level.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        # The strange syntax here is so that both StickyEconsumerType and StickyEmarkovConsumerType\n        # run the getShocks method of their first superclass: AggShockConsumerType and\n        # AggShockMarkovConsumerType respectively.  This will be simplified in Python 3.\n        super(self.__class__,self).getShocks() # Get permanent and transitory combined shocks\n        newborns = self.t_age == 0\n        self.TranShkNow[newborns] = self.TranShkAggNow*self.wRteNow # Turn off idiosyncratic shocks for newborns\n        self.PermShkNow[newborns] = self.PermShkAggNow\n        self.getUpdaters() # Randomly draw which agents will update their beliefs\n\n        # Calculate innovation to the productivity level perception error\n        pLvlErrNew = self.getpLvlError()\n        self.pLvlErrNow *= pLvlErrNew # Perception error accumulation\n\n        # Calculate (mis)perceptions of the permanent shock\n        PermShkPcvd = self.PermShkNow/pLvlErrNew\n        PermShkPcvd[self.update] *= self.pLvlErrNow[self.update] # Updaters see the true permanent shock and all missed news\n        self.pLvlErrNow[self.update] = 1.0\n        self.PermShkNow = PermShkPcvd"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getStates(self):\n        '''\n        Gets simulated consumers pLvl and mNrm for this period, but with the alteration that these\n        represent perceived rather than actual values.  Also calculates mLvlTrue, the true level of\n        market resources that the individual has on hand.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        # Update consumers' perception of their permanent income level\n        pLvlPrev = self.pLvlNow\n        self.pLvlNow = pLvlPrev*self.PermShkNow # Perceived permanent income level (only correct if macro state is observed this period)\n        self.PlvlAggNow *= self.PermShkAggNow # Updated aggregate permanent productivity level\n        self.pLvlTrue = self.pLvlNow*self.pLvlErrNow\n\n        # Calculate what the consumers perceive their normalized market resources to be\n        RfreeNow = self.getRfree()\n        bLvlNow = RfreeNow*self.aLvlNow # This is the true level\n\n        yLvlNow = self.pLvlTrue*self.TranShkNow # This is true income level\n        mLvlTrueNow = bLvlNow + yLvlNow # This is true market resource level\n        mNrmPcvdNow = mLvlTrueNow/self.pLvlNow # This is perceived normalized resources\n        self.mNrmNow = mNrmPcvdNow\n        self.mLvlTrueNow = mLvlTrueNow\n        self.yLvlNow = yLvlNow", "response": "Calculates simulated consumers pLvl and mNrm for this period."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getPostStates(self):\n        '''\n        Slightly extends the base version of this method by recalculating aLvlNow to account for the\n        consumer's (potential) misperception about their productivity level.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        AggShockConsumerType.getPostStates(self)\n        self.cLvlNow = self.cNrmNow*self.pLvlNow # True consumption level\n        self.aLvlNow = self.mLvlTrueNow - self.cLvlNow # True asset level\n        self.aNrmNow = self.aLvlNow/self.pLvlNow", "response": "This method is slowly extends the base version of AggShockConsumerType. getPostStates and updates the consumption level and asset level of the consumer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of all the updaters that have been updated and dont.", "response": "def getUpdaters(self):\n        '''\n        Determine which agents update this period vs which don't.  Fills in the\n        attributes update and dont as boolean arrays of size AgentCount.  This\n        version also updates perceptions of the Markov state.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        StickyEconsumerType.getUpdaters(self)\n        # Only updaters change their perception of the Markov state\n        if hasattr(self,'MrkvNowPcvd'):\n            self.MrkvNowPcvd[self.update] = self.MrkvNow\n        else: # This only triggers in the first simulated period\n            self.MrkvNowPcvd = np.ones(self.AgentCount,dtype=int)*self.MrkvNow"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate and returns the misperception of this period s shocks.", "response": "def getpLvlError(self):\n        '''\n        Calculates and returns the misperception of this period's shocks.  Updaters\n        have no misperception this period, while those who don't update don't see\n        the value of the aggregate permanent shock and thus base their belief about\n        aggregate growth on the last Markov state that they actually observed,\n        which is stored in MrkvNowPcvd.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        pLvlErr : np.array\n            Array of size AgentCount with this period's (new) misperception.\n        '''\n        pLvlErr = np.ones(self.AgentCount)\n        pLvlErr[self.dont] = self.PermShkAggNow/self.PermGroFacAgg[self.MrkvNowPcvd[self.dont]]\n        return pLvlErr"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef simBirth(self,which_agents):\n        '''\n        Makes new consumers for the given indices.  Slightly extends base method by also setting\n        pLvlTrue = 1.0 in the very first simulated period.\n\n        Parameters\n        ----------\n        which_agents : np.array(Bool)\n            Boolean array of size self.AgentCount indicating which agents should be \"born\".\n\n        Returns\n        -------\n        None\n        '''\n        super(self.__class__,self).simBirth(which_agents)\n        if self.t_sim == 0: # Make sure that pLvlTrue and aLvlNow exist\n            self.pLvlTrue = np.ones(self.AgentCount)\n            self.aLvlNow = self.aNrmNow*self.pLvlTrue", "response": "Makes new consumers for the given indices."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the updated values of the normalized market resources and permanent income level.", "response": "def getStates(self):\n        '''\n        Calculates updated values of normalized market resources and permanent income level.\n        Makes both perceived and true values.  The representative consumer will act on the\n        basis of his *perceived* normalized market resources.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        # Calculate perceived and true productivity level\n        aLvlPrev = self.aLvlNow\n        self.pLvlTrue = self.pLvlTrue*self.PermShkNow\n        self.pLvlNow = self.getpLvlPcvd()\n\n        # Calculate true values of variables\n        self.kNrmTrue = aLvlPrev/self.pLvlTrue\n        self.yNrmTrue = self.kNrmTrue**self.CapShare*self.TranShkNow**(1.-self.CapShare) - self.kNrmTrue*self.DeprFac\n        self.Rfree = 1. + self.CapShare*self.kNrmTrue**(self.CapShare-1.)*self.TranShkNow**(1.-self.CapShare) - self.DeprFac\n        self.wRte  = (1.-self.CapShare)*self.kNrmTrue**self.CapShare*self.TranShkNow**(-self.CapShare)\n        self.mNrmTrue = self.Rfree*self.kNrmTrue + self.wRte*self.TranShkNow\n        self.mLvlTrue = self.mNrmTrue*self.pLvlTrue\n\n        # Calculate perception of normalized market resources\n        self.mNrmNow = self.mLvlTrue/self.pLvlNow"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getPostStates(self):\n        '''\n        Slightly extends the base version of this method by recalculating aLvlNow to account for the\n        consumer's (potential) misperception about their productivity level.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        RepAgentConsumerType.getPostStates(self)\n        self.aLvlNow = self.mLvlTrue - self.cLvlNow # This is true\n        self.aNrmNow = self.aLvlNow/self.pLvlTrue", "response": "This method is slowly extends the base version of this method by recalculating aLvlNow to account for the potential misperception about their productivity level."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the average perceived productivity level for all representative agents.", "response": "def getpLvlPcvd(self):\n        '''\n        Finds the representative agent's (average) perceived productivity level.\n        Average perception of productivity gets UpdatePrb weight on the true level,\n        for those that update, and (1-UpdatePrb) weight on the previous average\n        perception times expected aggregate growth, for those that don't update.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        pLvlPcvd : np.array\n            Size 1 array with average perception of productivity level.\n        '''\n        pLvlPcvd = self.UpdatePrb*self.pLvlTrue + (1.0-self.UpdatePrb)*(self.pLvlNow*self.PermGroFac[self.t_cycle[0]-1])\n        return pLvlPcvd"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake new consumers for the given indices.", "response": "def simBirth(self,which_agents):\n        '''\n        Makes new consumers for the given indices.  Slightly extends base method by also setting\n        pLvlTrue = 1.0 in the very first simulated period, as well as initializing the perception\n        of aggregate productivity for each Markov state.  The representative agent begins with\n        the correct perception of the Markov state.\n\n        Parameters\n        ----------\n        which_agents : np.array(Bool)\n            Boolean array of size self.AgentCount indicating which agents should be \"born\".\n\n        Returns\n        -------\n        None\n        '''\n        if which_agents==np.array([True]):\n            RepAgentMarkovConsumerType.simBirth(self,which_agents)\n            if self.t_sim == 0: # Initialize perception distribution for Markov state\n                self.pLvlTrue = np.ones(self.AgentCount)\n                self.aLvlNow = self.aNrmNow*self.pLvlTrue\n                StateCount = self.MrkvArray.shape[0]\n                self.pLvlNow = np.ones(StateCount) # Perceived productivity level by Markov state\n                self.MrkvPcvd = np.zeros(StateCount) # Distribution of perceived Markov state\n                self.MrkvPcvd[self.MrkvNow[0]] = 1.0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getpLvlPcvd(self):\n        '''\n        Finds the representative agent's (average) perceived productivity level\n        for each Markov state, as well as the distribution of the representative\n        agent's perception of the Markov state.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        pLvlPcvd : np.array\n            Array with average perception of productivity level by Markov state.\n        '''\n        StateCount = self.MrkvArray.shape[0]\n        t = self.t_cycle[0]\n        i = self.MrkvNow[0]\n\n        dont_mass = self.MrkvPcvd*(1.-self.UpdatePrb) # pmf of non-updaters\n        update_mass = np.zeros(StateCount)\n        update_mass[i] = self.UpdatePrb # pmf of updaters\n\n        dont_pLvlPcvd = self.pLvlNow*self.PermGroFac[t-1] # those that don't update think pLvl grows at PermGroFac for last observed state\n        update_pLvlPcvd = np.zeros(StateCount)\n        update_pLvlPcvd[i] = self.pLvlTrue # those that update see the true pLvl\n\n        # Combine updaters and non-updaters to get average pLvl perception by Markov state\n        self.MrkvPcvd = dont_mass + update_mass # Total mass of agent in each state\n        pLvlPcvd = (dont_mass*dont_pLvlPcvd + update_mass*update_pLvlPcvd)/self.MrkvPcvd\n        pLvlPcvd[self.MrkvPcvd==0.] = 1.0 # Fix division by zero problem when MrkvPcvd[i]=0\n        return pLvlPcvd", "response": "Returns the average perceived productivity level for each Markov state."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the consumption for the representative agent using the consumption functions.", "response": "def getControls(self):\n        '''\n        Calculates consumption for the representative agent using the consumption functions.\n        Takes the weighted average of cLvl across perceived Markov states.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        StateCount = self.MrkvArray.shape[0]\n        t = self.t_cycle[0]\n\n        cNrmNow = np.zeros(StateCount) # Array of chosen cNrm by Markov state\n        for i in range(StateCount):\n            cNrmNow[i] = self.solution[t].cFunc[i](self.mNrmNow[i])\n        self.cNrmNow = cNrmNow\n        self.cLvlNow = np.dot(cNrmNow*self.pLvlNow,self.MrkvPcvd)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef millRule(self,aLvlNow,pLvlTrue):\n        '''\n        Function to calculate the capital to labor ratio, interest factor, and\n        wage rate based on each agent's current state.  Just calls calcRandW()\n        and adds the Markov state index.\n\n        See documentation for calcRandW for more information.\n        '''\n        MrkvNow = self.MrkvNow_hist[self.Shk_idx]\n        temp =  self.calcRandW(aLvlNow,pLvlTrue)\n        temp(MrkvNow = MrkvNow)\n\n        # Overwrite MaggNow, wRteNow, and RfreeNow if requested\n        if self.overwrite_hist:\n            t = self.Shk_idx-1\n            temp(MaggNow = self.MaggNow_overwrite[t])\n            temp(wRteNow = self.wRteNow_overwrite[t])\n            temp(RfreeNow = self.RfreeNow_overwrite[t])\n\n        return temp", "response": "Function to calculate the capital to labor ratio interest factor and wage rate based on each agent s current state."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef multiThreadCommandsFake(agent_list,command_list,num_jobs=None):\n    '''\n    Executes the list of commands in command_list for each AgentType in agent_list\n    in an ordinary, single-threaded loop.  Each command should be a method of\n    that AgentType subclass.  This function exists so as to easily disable\n    multithreading, as it uses the same syntax as multithreadCommands.\n\n    Parameters\n    ----------\n    agent_list : [AgentType]\n        A list of instances of AgentType on which the commands will be run.\n    command_list : [string]\n        A list of commands to run for each AgentType.\n    num_jobs : None\n        Dummy input to match syntax of multiThreadCommands.  Does nothing.\n\n    Returns\n    -------\n    none\n    '''\n    for agent in agent_list:\n        for command in command_list:\n            exec('agent.' + command)", "response": "This function is used to fake the multiThreadCommands method of each AgentType in command_list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef multiThreadCommands(agent_list,command_list,num_jobs=None):\n    '''\n    Executes the list of commands in command_list for each AgentType in agent_list\n    using a multithreaded system. Each command should be a method of that AgentType subclass.\n\n    Parameters\n    ----------\n    agent_list : [AgentType]\n        A list of instances of AgentType on which the commands will be run.\n    command_list : [string]\n        A list of commands to run for each AgentType in agent_list.\n\n    Returns\n    -------\n    None\n    '''\n    if len(agent_list) == 1:\n        multiThreadCommandsFake(agent_list,command_list)\n        return None\n\n    # Default umber of parallel jobs is the smaller of number of AgentTypes in\n    # the input and the number of available cores.\n    if num_jobs is None:\n        num_jobs = min(len(agent_list),multiprocessing.cpu_count())\n\n    # Send each command in command_list to each of the types in agent_list to be run\n    agent_list_out = Parallel(n_jobs=num_jobs)(delayed(runCommands)(*args) for args in zip(agent_list, len(agent_list)*[command_list]))\n\n    # Replace the original types with the output from the parallel call\n    for j in range(len(agent_list)):\n        agent_list[j] = agent_list_out[j]", "response": "Executes the list of commands in command_list for each AgentType in agent_list using a multithreaded system."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parallelNelderMead(objFunc,guess,perturb=None,P=1,ftol=0.000001,xtol=0.00000001,maxiter=np.inf,maxeval=np.inf,r_param=1.0,e_param=1.0,c_param=0.5,s_param=0.5,maxcores=None,name=None,resume=False,savefreq=None,verbose=1):\n    '''\n    A parallel implementation of the Nelder-Mead minimization algorithm, as\n    described in Lee and Wiswall.  For long optimization procedures, it can\n    save progress between iterations and resume later.\n\n    Parameters\n    ----------\n    objFunc : function\n        The objective function to be minimized. Takes a single 1D array as input.\n    guess : np.array\n        Initial starting point for the simplex, representing an input for objFunc.\n    perturb : np.array\n        Perturbation vector for the simplex, of the same length as an input to\n        objFunc.  If perturb[j] is non-zero, a simplex point will be created\n        that perturbs the j-th element of guess by perturb[j]; if it is zero,\n        then the j-th parameter of objFunc will not be optimized over.  By\n        default, guess=None, indicating that all parameters should be optimized,\n        with an initial perturbation of 0.1*guess.\n    P : int\n        Degree of parallelization: the number of vertices of the simplex to try\n        to update on each iteration of the process.\n    ftol : float\n        Absolute tolerance of the objective function for convergence.  If suc-\n        cessive iterations return minimum function values that differ by less\n        than ftol, the process terminates successfully.\n    xtol : float\n        Absolute tolerance of the input values for convergence.  If the maximum\n        distance between the current minimum point and the worst point in the\n        simplex is less than xtol, then the process terminates successfully.\n    maxiter : int\n        Maximum number of Nelder-Mead iterations; reaching iters=maxiter is\n        reported as an \"unsuccessful\" minimization.\n    maxeval : int\n        Maximum number of evaluations of objFunc (across all processes); reaching\n        evals=maxeval is reported as an \"unsuccessful\" minimization.\n    r_param: float\n        Parameter indicating magnitude of the reflection point calculation.\n    e_param: float\n        Parameter indicating magnitude of the expansion point calculation.\n    c_param: float\n        Parameter indicating magnitude of the contraction point calculation.\n    s_param: float\n        Parameter indicating magnitude of the shrink calculation.\n    maxcores : int\n        The maximum number of CPU cores that the optimization should use,\n        regardless of the size of the problem.\n    name : string\n        A filename for (optionally) saving the progress of the Nelder-Mead search,\n        and for resuming a previous search (when resume=True).  Useful for long\n        searches that could potentially be interrupted by computer down time.\n    resume : boolean\n        An indicator for whether the search should resume from earlier progress.\n        When True, the process will load a progress file named in input name.\n    savefreq : int\n        When not None, search progress will be saved to name.txt every savefreq\n        iterations, to be loaded later with resume=True).\n    verbose : int\n        Indicator for the verbosity of the optimization routine.  Higher values\n        generate more text output; verbose=0 produces no text output.\n\n    Returns\n    -------\n    min_point : np.array\n        The input that minimizes objFunc, as found by the minimization.\n    fmin : float\n        The minimum of objFunc; fmin = objFunc(min_point).\n    '''\n    # If this is a resumed search, load the data\n    if resume:\n        simplex, fvals, iters, evals = loadNelderMeadData(name)\n        dim_count = fvals.size - 1\n        N = dim_count+1 # Number of points in simplex\n        K = simplex.shape[1] # Total number of parameters\n    # Otherwise, construct the initial simplex and array of function values\n    else:\n        if perturb is None: # Default: perturb each parameter by 10%\n            perturb = 0.1*guess\n            guess[guess == 0] = 0.1\n        params_to_opt = np.where(perturb != 0)[0] # Indices of which parameters to optimize\n        dim_count = params_to_opt.size # Number of parameters to search over\n        N = dim_count+1 # Number of points in simplex\n        K = guess.size # Total number of parameters\n        simplex = np.tile(guess,(N,1))\n        for j in range(dim_count): # Perturb each parameter to optimize by the specified distance\n            simplex[j+1,params_to_opt[j]] = simplex[j+1,params_to_opt[j]] + perturb[params_to_opt[j]]\n        # Initialize a few\n        fvals = np.zeros(dim_count+1) + np.nan\n        iters = 0\n        evals = 0\n\n    # Create the pool of worker processes\n    cpu_cores = multiprocessing.cpu_count() # Total number of available CPU cores\n    cores_to_use = min(cpu_cores,dim_count)\n    if maxcores is not None: # Cap the number of cores if desired\n        cores_to_use = min(cores_to_use,maxcores)\n    parallel = Parallel(n_jobs=cores_to_use)\n\n    # Begin a new Nelder-Mead search\n    if not resume:\n        temp_simplex = list(simplex) # Evaluate the initial simplex\n        fvals = np.array(parallel(delayed(objFunc)(params) for params in temp_simplex))\n        evals += N\n        # Reorder the initial simplex\n        order = np.argsort(fvals)\n        fvals = fvals[order]\n        simplex = simplex[order,:]\n        fmin = fvals[0]\n        f_dist = np.abs(fmin - fvals[-1])\n        x_dist = np.max(np.sqrt(np.sum(simplex**2.0 - np.tile(simplex[0,:],(N,1))**2.0,axis=1)))\n        if verbose > 0:\n            print('Evaluated the initial simplex: fmin=' + str(fmin) + ', f_dist=' + str(f_dist) + ', x_dist=' + str(x_dist))\n    else: # Resume an existing search that was cut short\n        if verbose > 0:\n            print('Resuming search after ' + str(iters) + ' iterations and ' + str(evals) + ' function evaluations.')\n\n    # Initialize some inputs for the multithreader\n    j_list = list(range(N-P,N))\n    opt_params= [r_param,c_param,e_param]\n\n    # Run the Nelder-Mead algorithm until a terminal condition is met\n    go = True\n    while go:\n        t_start = clock()\n        iters += 1\n        if verbose > 0:\n            print('Beginning iteration #' + str(iters) + ' now.')\n\n        # Update the P worst points of the simplex\n        output = parallel(delayed(parallelNelderMeadWorker)(objFunc,simplex,fvals,j,P,opt_params) for j in j_list)\n        new_subsimplex = np.zeros((P,K)) + np.nan\n        new_vals = np.zeros(P) + np.nan\n        new_evals = 0\n        for i in range(P):\n            new_subsimplex[i,:] = output[i][0]\n            new_vals[i] = output[i][1]\n            new_evals += output[i][2]\n        evals += new_evals\n\n        # Check whether any updates actually happened\n        old_subsimplex = simplex[(N-P):N,:]\n        if np.max(np.abs(new_subsimplex - old_subsimplex)) == 0:\n            if verbose > 0:\n                print('Updated the simplex, but must perform a shrink step.')\n            # If every attempted update was unsuccessful, must shrink the simplex\n            simplex = s_param*np.tile(simplex[0,:],(N,1)) + (1.0-s_param)*simplex\n            temp_simplex = list(simplex[1:N,:])\n            fvals = np.array([fvals[0]] + parallel(delayed(objFunc)(params) for params in temp_simplex))\n            new_evals += N-1\n            evals += N-1\n        else:\n            if verbose > 0:\n                print('Updated the simplex successfully.')\n            # Otherwise, update the simplex with the new results\n            simplex[(N-P):N,:] = new_subsimplex\n            fvals[(N-P):N] = new_vals\n\n        # Reorder the simplex from best to worst\n        order = np.argsort(fvals)\n        fvals = fvals[order]\n        simplex = simplex[order,:]\n        fmin = fvals[0]\n        f_dist = np.abs(fmin - fvals[-1])\n        x_dist = np.max(np.sqrt(np.sum(simplex**2.0 - np.tile(simplex[0,:],(N,1))**2.0,axis=1)))\n        t_end = clock()\n        if verbose > 0:\n            t_iter = t_end - t_start\n            print('Finished iteration #' + str(iters) +' with ' + str(new_evals) + ' evaluations (' + str(evals) + ' cumulative) in ' + str(t_iter) + ' seconds.')\n            print('Simplex status: fmin=' + str(fmin) + ', f_dist=' + str(f_dist) + ', x_dist=' + str(x_dist))\n\n        # Check for terminal conditions\n        if iters >= maxiter:\n            go = False\n            print('Maximum iterations reached, terminating unsuccessfully.')\n        if evals >= maxeval:\n            go = False\n            print('Maximum evaluations reached, terminating unsuccessfully.')\n        if f_dist < ftol:\n            go = False\n            print('Function tolerance reached, terminating successfully.')\n        if x_dist < xtol:\n            go = False\n            print('Parameter tolerance reached, terminating successfully.')\n\n        # Save the progress of the estimation if desired\n        if savefreq is not None:\n            if (iters % savefreq) == 0:\n                 saveNelderMeadData(name, simplex, fvals, iters, evals)\n                 if verbose > 0:\n                     print('Saved search progress in ' + name + '.txt')\n\n    # Return the results\n    xopt = simplex[0,:]\n    return xopt, fmin", "response": "This function is used to run a parallel Nelder - Mead minimization algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave the progress of a parallel Nelder - Mead search in a text file.", "response": "def saveNelderMeadData(name, simplex, fvals, iters, evals):\n    '''\n    Stores the progress of a parallel Nelder-Mead search in a text file so that\n    it can be resumed later (after manual termination or a crash).\n\n    Parameters\n    ----------\n    name : string\n        Name of the txt file in which to store search progress.\n    simplex : np.array\n        The current state of the simplex of parameter guesses.\n    fvals : np.array\n        The objective function value at each row of simplex.\n    iters : int\n        The number of completed Nelder-Mead iterations.\n    evals : int\n        The cumulative number of function evaluations in the search process.\n\n    Returns\n    -------\n    none\n    '''\n    f = open(name + '.txt','w')\n    my_writer = csv.writer(f,delimiter=' ')\n    my_writer.writerow(simplex.shape)\n    my_writer.writerow([iters, evals])\n    my_writer.writerow(simplex.flatten())\n    my_writer.writerow(fvals)\n    f.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread the progress of a parallel Nelder - Mead search from a text file.", "response": "def loadNelderMeadData(name):\n    '''\n    Reads the progress of a parallel Nelder-Mead search from a text file, as\n    created by saveNelderMeadData().\n\n    Parameters\n    ----------\n    name : string\n        Name of the txt file from which to read search progress.\n\n    Returns\n    -------\n    simplex : np.array\n        The current state of the simplex of parameter guesses.\n    fvals : np.array\n        The objective function value at each row of simplex.\n    iters : int\n        The number of completed Nelder-Mead iterations.\n    evals : int\n        The cumulative number of function evaluations in the search process.\n    '''\n    f = open(name + '.txt','rb')\n    my_reader = csv.reader(f,delimiter=' ')\n    my_shape_txt = next(my_reader)\n    shape0 = int(my_shape_txt[0])\n    shape1 = int(my_shape_txt[1])\n    my_nums_txt = next(my_reader)\n    iters = int(my_nums_txt[0])\n    evals = int(my_nums_txt[1])\n    simplex_flat = np.array(next(my_reader),dtype=float)\n    simplex = np.reshape(simplex_flat,(shape0,shape1))\n    fvals = np.array(next(my_reader),dtype=float)\n    f.close()\n\n    return simplex, fvals, iters, evals"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nminimize the objective function using the Nelder - Mead simplex algorithm.", "response": "def minimizeNelderMead(objectiveFunction, parameter_guess, verbose=False, **kwargs):\n    '''\n    Minimizes the objective function using the Nelder-Mead simplex algorithm,\n    starting from an initial parameter guess.\n\n    Parameters\n    ----------\n    objectiveFunction : function\n        The function to be minimized.  It should take only a single argument, which\n        should be a list representing the parameters to be estimated.\n    parameter_guess : [float]\n        A starting point for the Nelder-Mead algorithm, which must be a valid\n        input for objectiveFunction.\n    verbose : boolean\n        A flag for the amount of output to print.\n\n    Returns\n    -------\n    xopt : [float]\n        The values that minimize objectiveFunction.\n    '''\n\n    # Execute the minimization, starting from the given parameter guess\n    t0 = time() # Time the process\n    OUTPUT = fmin(objectiveFunction, parameter_guess, full_output=1, maxiter=1000, disp=verbose, **kwargs)\n    t1 = time()\n\n    # Extract values from optimization output:\n    xopt = OUTPUT[0]        # Parameters that minimize function.\n    fopt = OUTPUT[1]        # Value of function at minimum: ``fopt = func(xopt)``.\n    optiter = OUTPUT[2]     # Number of iterations performed.\n    funcalls = OUTPUT[3]    # Number of function calls made.\n    warnflag = OUTPUT[4]    # warnflag : int\n                            #   1 : Maximum number of function evaluations made.\n                            #   2 : Maximum number of iterations reached.\n    # Check that optimization succeeded:\n    if warnflag != 0:\n        warnings.warn(\"Minimization failed! xopt=\" + str(xopt) + ', fopt=' + str(fopt) +\n                      ', optiter=' + str(optiter) +', funcalls=' + str(funcalls) +\n                      ', warnflag=' + str(warnflag))\n\n    # Display and return the results:\n    if verbose:\n        print(\"Time to estimate is \" + str(t1-t0) +  \" seconds.\")\n    return xopt"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nminimize the objective function using a derivative - free Powell algorithm.", "response": "def minimizePowell(objectiveFunction, parameter_guess, verbose=False):\n    '''\n    Minimizes the objective function using a derivative-free Powell algorithm,\n    starting from an initial parameter guess.\n\n    Parameters\n    ----------\n    objectiveFunction : function\n        The function to be minimized.  It should take only a single argument, which\n        should be a list representing the parameters to be estimated.\n    parameter_guess : [float]\n        A starting point for the Powell algorithm, which must be a valid\n        input for objectiveFunction.\n    verbose : boolean\n        A flag for the amount of output to print.\n\n    Returns\n    -------\n    xopt : [float]\n        The values that minimize objectiveFunction.\n    '''\n\n    # Execute the minimization, starting from the given parameter guess\n    t0 = time() # Time the process\n    OUTPUT = fmin_powell(objectiveFunction, parameter_guess, full_output=1, maxiter=1000, disp=verbose)\n    t1 = time()\n\n    # Extract values from optimization output:\n    xopt = OUTPUT[0]        # Parameters that minimize function.\n    fopt = OUTPUT[1]        # Value of function at minimum: ``fopt = func(xopt)``.\n    direc = OUTPUT[2]\n    optiter = OUTPUT[3]     # Number of iterations performed.\n    funcalls = OUTPUT[4]    # Number of function calls made.\n    warnflag = OUTPUT[5]    # warnflag : int\n                            #   1 : Maximum number of function evaluations made.\n                            #   2 : Maximum number of iterations reached.\n    # Check that optimization succeeded:\n    if warnflag != 0:\n        warnings.warn(\"Minimization failed! xopt=\" + str(xopt) + ', fopt=' + str(fopt) + ', direc=' + str(direc) + ', optiter=' + str(optiter) +', funcalls=' + str(funcalls) +', warnflag=' + str(warnflag))\n\n    # Display and return the results:\n    if verbose:\n        print(\"Time to estimate is \" + str(t1-t0) +  \" seconds.\")\n    return xopt"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsampling rows from the input array of data generating a new data array with equal number of rows.", "response": "def bootstrapSampleFromData(data,weights=None,seed=0):\n    '''\n    Samples rows from the input array of data, generating a new data array with\n    an equal number of rows (records).  Rows are drawn with equal probability\n    by default, but probabilities can be specified with weights (must sum to 1).\n\n    Parameters\n    ----------\n    data : np.array\n        An array of data, with each row representing a record.\n    weights : np.array\n        A weighting array with length equal to data.shape[0].\n    seed : int\n        A seed for the random number generator.\n\n    Returns\n    -------\n    new_data : np.array\n        A resampled version of input data.\n    '''\n    # Set up the random number generator\n    RNG = np.random.RandomState(seed)\n    N = data.shape[0]\n\n    # Set up weights\n    if weights is not None:\n        cutoffs = np.cumsum(weights)\n    else:\n        cutoffs = np.linspace(0,1,N)\n\n    # Draw random indices\n    indices = np.searchsorted(cutoffs,RNG.uniform(size=N))\n\n    # Create a bootstrapped sample\n    new_data = deepcopy(data[indices,])\n    return new_data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating what consumption market resources and marginal propensity to consume and MPC next.", "response": "def findNextPoint(DiscFac,Rfree,CRRA,PermGroFacCmp,UnempPrb,Rnrm,Beth,cNext,mNext,MPCnext,PFMPC):\n    '''\n    Calculates what consumption, market resources, and the marginal propensity\n    to consume must have been in the previous period given model parameters and\n    values of market resources, consumption, and MPC today.\n\n    Parameters\n    ----------\n    DiscFac : float\n        Intertemporal discount factor on future utility.\n    Rfree : float\n        Risk free interest factor on end-of-period assets.\n    PermGroFacCmp : float\n        Permanent income growth factor, compensated for the possibility of\n        permanent unemployment.\n    UnempPrb : float\n        Probability of becoming permanently unemployed.\n    Rnrm : float\n        Interest factor normalized by compensated permanent income growth factor.\n    Beth : float\n        Composite effective discount factor for reverse shooting solution; defined \n        in appendix \"Numerical Solution/The Consumption Function\" in TBS \n        lecture notes\n    cNext : float\n        Normalized consumption in the succeeding period.\n    mNext : float\n        Normalized market resources in the succeeding period.\n    MPCnext : float\n        The marginal propensity to consume in the succeeding period.\n    PFMPC : float\n        The perfect foresight MPC; also the MPC when permanently unemployed.\n\n    Returns\n    -------\n    mNow : float\n        Normalized market resources this period.\n    cNow : float\n        Normalized consumption this period.\n    MPCnow : float\n        Marginal propensity to consume this period.\n    '''\n    uPP = lambda x : utilityPP(x,gam=CRRA)\n    cNow = PermGroFacCmp*(DiscFac*Rfree)**(-1.0/CRRA)*cNext*(1 + UnempPrb*((cNext/(PFMPC*(mNext-1.0)))**CRRA-1.0))**(-1.0/CRRA)\n    mNow = (PermGroFacCmp/Rfree)*(mNext - 1.0) + cNow\n    cUNext = PFMPC*(mNow-cNow)*Rnrm\n    # See TBS Appendix \"E.1 The Consumption Function\" \n    natural = Beth*Rnrm*(1.0/uPP(cNow))*((1.0-UnempPrb)*uPP(cNext)*MPCnext + UnempPrb*uPP(cUNext)*PFMPC) # Convenience variable\n    MPCnow = natural / (natural + 1)\n    return mNow, cNow, MPCnow"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef addToStableArmPoints(solution_next,DiscFac,Rfree,CRRA,PermGroFacCmp,UnempPrb,PFMPC,Rnrm,Beth,mLowerBnd,mUpperBnd):\n    '''\n    Adds a one point to the bottom and top of the list of stable arm points if\n    the bounding levels of mLowerBnd (lower) and mUpperBnd (upper) have not yet\n    been met by a stable arm point in mNrm_list.  This acts as the \"one period\n    solver\" / solveOnePeriod in the tractable buffer stock model.\n\n    Parameters\n    ----------\n    solution_next : TractableConsumerSolution\n        The solution object from the previous iteration of the backshooting\n        procedure.  Not the \"next period\" solution per se.\n    DiscFac : float\n        Intertemporal discount factor on future utility.\n    Rfree : float\n        Risk free interest factor on end-of-period assets.\n    CRRA : float\n        Coefficient of relative risk aversion.\n    PermGroFacCmp : float\n        Permanent income growth factor, compensated for the possibility of\n        permanent unemployment.\n    UnempPrb : float\n        Probability of becoming permanently unemployed.\n    PFMPC : float\n        The perfect foresight MPC; also the MPC when permanently unemployed.\n    Rnrm : float\n        Interest factor normalized by compensated permanent income growth factor.\n    Beth : float\n        Damned if I know.\n    mLowerBnd : float\n        Lower bound on market resources for the backshooting process.  If\n        min(solution_next.mNrm_list) < mLowerBnd, no new bottom point is found.\n    mUpperBnd : float\n        Upper bound on market resources for the backshooting process.  If\n        max(solution_next.mNrm_list) > mUpperBnd, no new top point is found.\n\n    Returns:\n    ---------\n    solution_now : TractableConsumerSolution\n        A new solution object with new points added to the top and bottom.  If\n        no new points were added, then the backshooting process is about to end.\n    '''\n    # Unpack the lists of Euler points\n    mNrm_list = copy(solution_next.mNrm_list)\n    cNrm_list = copy(solution_next.cNrm_list)\n    MPC_list = copy(solution_next.MPC_list)\n\n    # Check whether to add a stable arm point to the top\n    mNext = mNrm_list[-1]\n    if mNext < mUpperBnd:\n        # Get the rest of the data for the previous top point\n        cNext = solution_next.cNrm_list[-1]\n        MPCNext = solution_next.MPC_list[-1]\n\n        # Calculate employed levels of c, m, and MPC from next period's values\n        mNow, cNow, MPCnow = findNextPoint(DiscFac,Rfree,CRRA,PermGroFacCmp,UnempPrb,Rnrm,Beth,cNext,mNext,MPCNext,PFMPC)\n\n        # Add this point to the top of the stable arm list\n        mNrm_list.append(mNow)\n        cNrm_list.append(cNow)\n        MPC_list.append(MPCnow)\n\n    # Check whether to add a stable arm point to the bottom\n    mNext = mNrm_list[0]\n    if mNext > mLowerBnd:\n        # Get the rest of the data for the previous bottom point\n        cNext = solution_next.cNrm_list[0]\n        MPCNext = solution_next.MPC_list[0]\n\n        # Calculate employed levels of c, m, and MPC from next period's values\n        mNow, cNow, MPCnow = findNextPoint(DiscFac,Rfree,CRRA,PermGroFacCmp,UnempPrb,Rnrm,Beth,cNext,mNext,MPCNext,PFMPC)\n\n        # Add this point to the top of the stable arm list\n        mNrm_list.insert(0,mNow)\n        cNrm_list.insert(0,cNow)\n        MPC_list.insert(0,MPCnow)\n\n    # Construct and return this period's solution\n    solution_now = TractableConsumerSolution(mNrm_list=mNrm_list, cNrm_list=cNrm_list, MPC_list=MPC_list)\n    solution_now.PointCount = len(mNrm_list)\n    return solution_now", "response": "This function adds a new stable arm point to the bottom and bottom of the tractable buffer stock model. The new points are added to the bottom and bottom of the tractable buffer stock model. The new points are added to the bottom and bottom of the tractable buffer stock model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef preSolve(self):\n        '''\n        Calculates all of the solution objects that can be obtained before con-\n        ducting the backshooting routine, including the target levels, the per-\n        fect foresight solution, (marginal) consumption at m=0, and the small\n        perturbations around the steady state.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        # Define utility functions\n        uPP = lambda x : utilityPP(x,gam=self.CRRA)\n        uPPP = lambda x : utilityPPP(x,gam=self.CRRA)\n        uPPPP = lambda x : utilityPPPP(x,gam=self.CRRA)\n\n        # Define some useful constants from model primitives\n        self.PermGroFacCmp = self.PermGroFac/(1.0-self.UnempPrb) #\"uncertainty compensated\" wage growth factor\n        self.Rnrm = self.Rfree/self.PermGroFacCmp # net interest factor (Rfree normalized by wage growth)\n        self.PFMPC= 1.0-(self.Rfree**(-1.0))*(self.Rfree*self.DiscFac)**(1.0/self.CRRA) # MPC for a perfect forsight consumer\n        self.Beth = self.Rnrm*self.DiscFac*self.PermGroFacCmp**(1.0-self.CRRA)\n\n        # Verify that this consumer is impatient\n        PatFacGrowth = (self.Rfree*self.DiscFac)**(1.0/self.CRRA)/self.PermGroFacCmp\n        PatFacReturn = (self.Rfree*self.DiscFac)**(1.0/self.CRRA)/self.Rfree\n        if PatFacReturn >= 1.0:\n            raise Exception(\"Employed consumer not return impatient, cannot solve!\")\n        if PatFacGrowth >= 1.0:\n            raise Exception(\"Employed consumer not growth impatient, cannot solve!\")\n\n        # Find target money and consumption\n        # See TBS Appendix \"B.2 A Target Always Exists When Human Wealth Is Infinite\"\n        Pi = (1+(PatFacGrowth**(-self.CRRA)-1.0)/self.UnempPrb)**(1/self.CRRA)\n        self.h = (1.0/(1.0-self.PermGroFac/self.Rfree))\n        zeta = self.Rnrm*self.PFMPC*Pi # See TBS Appendix \"C The Exact Formula for target m\"\n        self.mTarg = 1.0+(self.Rfree/(self.PermGroFacCmp+zeta*self.PermGroFacCmp-self.Rfree))\n        self.cTarg = (1.0-self.Rnrm**(-1.0))*self.mTarg+self.Rnrm**(-1.0)\n        mTargU = (self.mTarg - self.cTarg)*self.Rnrm\n        cTargU = mTargU*self.PFMPC\n        self.SSperturbance = self.mTarg*0.1\n\n        # Find the MPC, MMPC, and MMMPC at the target\n        mpcTargFixedPointFunc = lambda k : k*uPP(self.cTarg) - self.Beth*((1.0-self.UnempPrb)*(1.0-k)*k*self.Rnrm*uPP(self.cTarg)+self.PFMPC*self.UnempPrb*(1.0-k)*self.Rnrm*uPP(cTargU))\n        self.MPCtarg = newton(mpcTargFixedPointFunc,0)\n        mmpcTargFixedPointFunc = lambda kk : kk*uPP(self.cTarg) + self.MPCtarg**2.0*uPPP(self.cTarg) - self.Beth*(-(1.0 - self.UnempPrb)*self.MPCtarg*kk*self.Rnrm*uPP(self.cTarg)+(1.0-self.UnempPrb)*(1.0 - self.MPCtarg)**2.0*kk*self.Rnrm**2.0*uPP(self.cTarg)-self.PFMPC*self.UnempPrb*kk*self.Rnrm*uPP(cTargU)+(1.0-self.UnempPrb)*(1.0-self.MPCtarg)**2.0*self.MPCtarg**2.0*self.Rnrm**2.0*uPPP(self.cTarg)+self.PFMPC**2.0*self.UnempPrb*(1.0-self.MPCtarg)**2.0*self.Rnrm**2.0*uPPP(cTargU))\n        self.MMPCtarg = newton(mmpcTargFixedPointFunc,0)\n        mmmpcTargFixedPointFunc = lambda kkk : kkk * uPP(self.cTarg) + 3 * self.MPCtarg * self.MMPCtarg * uPPP(self.cTarg) + self.MPCtarg**3 * uPPPP(self.cTarg) - self.Beth * (-(1 - self.UnempPrb) * self.MPCtarg * kkk * self.Rnrm * uPP(self.cTarg) - 3 * (1 - self.UnempPrb) * (1 - self.MPCtarg) * self.MMPCtarg**2 * self.Rnrm**2 * uPP(self.cTarg) + (1 - self.UnempPrb) * (1 - self.MPCtarg)**3 * kkk * self.Rnrm**3 * uPP(self.cTarg) - self.PFMPC * self.UnempPrb * kkk * self.Rnrm * uPP(cTargU) - 3 * (1 - self.UnempPrb) * (1 - self.MPCtarg) * self.MPCtarg**2 * self.MMPCtarg * self.Rnrm**2 * uPPP(self.cTarg) + 3 * (1 - self.UnempPrb) * (1 - self.MPCtarg)**3 * self.MPCtarg * self.MMPCtarg * self.Rnrm**3 * uPPP(self.cTarg) - 3 * self.PFMPC**2 * self.UnempPrb * (1 - self.MPCtarg) * self.MMPCtarg * self.Rnrm**2 * uPPP(cTargU) + (1 - self.UnempPrb) * (1 - self.MPCtarg)**3 * self.MPCtarg**3 * self.Rnrm**3 * uPPPP(self.cTarg) + self.PFMPC**3 * self.UnempPrb * (1 - self.MPCtarg)**3 * self.Rnrm**3 * uPPPP(cTargU))\n        self.MMMPCtarg = newton(mmmpcTargFixedPointFunc,0)\n\n        # Find the MPC at m=0\n        f_temp = lambda k : self.Beth*self.Rnrm*self.UnempPrb*(self.PFMPC*self.Rnrm*((1.0-k)/k))**(-self.CRRA-1.0)*self.PFMPC\n        mpcAtZeroFixedPointFunc = lambda k : k - f_temp(k)/(1 + f_temp(k))\n        #self.MPCmax = newton(mpcAtZeroFixedPointFunc,0.5)\n        self.MPCmax = brentq(mpcAtZeroFixedPointFunc,self.PFMPC,0.99,xtol=0.00000001,rtol=0.00000001)\n\n        # Make the initial list of Euler points: target and perturbation to either side\n        mNrm_list = [self.mTarg-self.SSperturbance, self.mTarg, self.mTarg+self.SSperturbance]\n        c_perturb_lo = self.cTarg - self.SSperturbance*self.MPCtarg + 0.5*self.SSperturbance**2.0*self.MMPCtarg - (1.0/6.0)*self.SSperturbance**3.0*self.MMMPCtarg\n        c_perturb_hi = self.cTarg + self.SSperturbance*self.MPCtarg + 0.5*self.SSperturbance**2.0*self.MMPCtarg + (1.0/6.0)*self.SSperturbance**3.0*self.MMMPCtarg\n        cNrm_list = [c_perturb_lo, self.cTarg, c_perturb_hi]\n        MPC_perturb_lo = self.MPCtarg - self.SSperturbance*self.MMPCtarg + 0.5*self.SSperturbance**2.0*self.MMMPCtarg\n        MPC_perturb_hi = self.MPCtarg + self.SSperturbance*self.MMPCtarg + 0.5*self.SSperturbance**2.0*self.MMMPCtarg\n        MPC_list = [MPC_perturb_lo, self.MPCtarg, MPC_perturb_hi]\n\n        # Set bounds for money (stable arm construction stops when these are exceeded)\n        self.mLowerBnd = 1.0\n        self.mUpperBnd = 2.0*self.mTarg\n\n        # Make the terminal period solution\n        solution_terminal = TractableConsumerSolution(mNrm_list=mNrm_list,cNrm_list=cNrm_list,MPC_list=MPC_list)\n        self.solution_terminal = solution_terminal\n\n        # Make two linear steady state functions\n        self.cSSfunc = lambda m : m*((self.Rnrm*self.PFMPC*Pi)/(1.0+self.Rnrm*self.PFMPC*Pi))\n        self.mSSfunc = lambda m : (self.PermGroFacCmp/self.Rfree)+(1.0-self.PermGroFacCmp/self.Rfree)*m", "response": "This method is called before the solution is solved."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking new consumers for the given indices.", "response": "def simBirth(self,which_agents):\n        '''\n        Makes new consumers for the given indices.  Initialized variables include aNrm, as\n        well as time variables t_age and t_cycle.  Normalized assets are drawn from a lognormal\n        distributions given by aLvlInitMean and aLvlInitStd.\n\n        Parameters\n        ----------\n        which_agents : np.array(Bool)\n            Boolean array of size self.AgentCount indicating which agents should be \"born\".\n\n        Returns\n        -------\n        None\n        '''\n        # Get and store states for newly born agents\n        N = np.sum(which_agents) # Number of new consumers to make\n        self.aLvlNow[which_agents] = drawLognormal(N,mu=self.aLvlInitMean,sigma=self.aLvlInitStd,seed=self.RNG.randint(0,2**31-1))\n        self.eStateNow[which_agents] = 1.0 # Agents are born employed\n        self.t_age[which_agents]   = 0 # How many periods since each agent was born\n        self.t_cycle[which_agents] = 0 # Which period of the cycle each agent is currently in\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef simDeath(self):\n        '''\n        Trivial function that returns boolean array of all False, as there is no death.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        which_agents : np.array(bool)\n            Boolean array of size AgentCount indicating which agents die.\n        '''\n        # Nobody dies in this model\n        which_agents = np.zeros(self.AgentCount,dtype=bool)\n        return which_agents", "response": "Trivial function that returns boolean array of all False as there is no death."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining which agents switch from employment to unemployment.", "response": "def getShocks(self):\n        '''\n        Determine which agents switch from employment to unemployment.  All unemployed agents remain\n        unemployed until death.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        employed = self.eStateNow == 1.0\n        N = int(np.sum(employed))\n        newly_unemployed = drawBernoulli(N,p=self.UnempPrb,seed=self.RNG.randint(0,2**31-1))\n        self.eStateNow[employed] = 1.0 - newly_unemployed"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates market resources for all agents in this period.", "response": "def getStates(self):\n        '''\n        Calculate market resources for all agents this period.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        self.bLvlNow = self.Rfree*self.aLvlNow\n        self.mLvlNow = self.bLvlNow + self.eStateNow"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates consumption for each agent this period.", "response": "def getControls(self):\n        '''\n        Calculate consumption for each agent this period.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        employed = self.eStateNow == 1.0\n        unemployed = np.logical_not(employed)\n        cLvlNow = np.zeros(self.AgentCount)\n        cLvlNow[employed] = self.solution[0].cFunc(self.mLvlNow[employed])\n        cLvlNow[unemployed] = self.solution[0].cFunc_U(self.mLvlNow[unemployed])\n        self.cLvlNow = cLvlNow"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsolves the one period problem for a consumer with shocks to permanent and transitory income as well as medical need shocks.", "response": "def solveConsMedShock(solution_next,IncomeDstn,MedShkDstn,LivPrb,DiscFac,CRRA,CRRAmed,Rfree,MedPrice,\n                 pLvlNextFunc,BoroCnstArt,aXtraGrid,pLvlGrid,vFuncBool,CubicBool):\n    '''\n    Solve the one period problem for a consumer with shocks to permanent and\n    transitory income as well as medical need shocks (as multiplicative shifters\n    for utility from a second medical care good).\n\n    Parameters\n    ----------\n    solution_next : ConsumerSolution\n        The solution to next period's one period problem.\n    IncomeDstn : [np.array]\n        A list containing three arrays of floats, representing a discrete\n        approximation to the income process between the period being solved\n        and the one immediately following (in solution_next). Order: event\n        probabilities, permanent shocks, transitory shocks.\n    MedShkDstn : [np.array]\n        Discrete distribution of the multiplicative utility shifter for med-\n        ical care. Order: probabilities, preference shocks.\n    LivPrb : float\n        Survival probability; likelihood of being alive at the beginning of\n        the succeeding period.\n    DiscFac : float\n        Intertemporal discount factor for future utility.\n    CRRA : float\n        Coefficient of relative risk aversion for composite consumption.\n    CRRAmed : float\n        Coefficient of relative risk aversion for medical care.\n    Rfree : float\n        Risk free interest factor on end-of-period assets.\n    MedPrice : float\n        Price of unit of medical care relative to unit of consumption.\n    pLvlNextFunc : float\n        Expected permanent income next period as a function of current pLvl.\n    PrstIncCorr : float\n        Correlation of permanent income from period to period.\n    BoroCnstArt: float or None\n        Borrowing constraint for the minimum allowable assets to end the\n        period with.\n    aXtraGrid: np.array\n        Array of \"extra\" end-of-period (normalized) asset values-- assets\n        above the absolute minimum acceptable level.\n    pLvlGrid: np.array\n        Array of permanent income levels at which to solve the problem.\n    vFuncBool: boolean\n        An indicator for whether the value function should be computed and\n        included in the reported solution.\n    CubicBool: boolean\n        An indicator for whether the solver should use cubic or linear inter-\n        polation.\n\n    Returns\n    -------\n    solution : ConsumerSolution\n        Solution to this period's problem, including a consumption function,\n        medical spending function, and marginal value function.  The former two\n        are defined over (mLvl,pLvl,MedShk), while the latter is defined only\n        on (mLvl,pLvl), with MedShk integrated out.\n    '''\n    solver = ConsMedShockSolver(solution_next,IncomeDstn,MedShkDstn,LivPrb,DiscFac,CRRA,CRRAmed,Rfree,\n                MedPrice,pLvlNextFunc,BoroCnstArt,aXtraGrid,pLvlGrid,vFuncBool,CubicBool)\n    solver.prepareToSolve()       # Do some preparatory work\n    solution_now = solver.solve() # Solve the period\n    return solution_now"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nevaluates the derivative of consumption and medical care with respect to market resources at given levels of market resources permanent income levels and medical need shocks at given levels of market resources permanent income levels and medical need shocks.", "response": "def derivativeX(self,mLvl,pLvl,MedShk):\n        '''\n        Evaluate the derivative of consumption and medical care with respect to\n        market resources at given levels of market resources, permanent income,\n        and medical need shocks.\n\n        Parameters\n        ----------\n        mLvl : np.array\n            Market resource levels.\n        pLvl : np.array\n            Permanent income levels; should be same size as mLvl.\n        MedShk : np.array\n            Medical need shocks; should be same size as mLvl.\n\n        Returns\n        -------\n        dcdm : np.array\n            Derivative of consumption with respect to market resources for each\n            point in (xLvl,MedShk).\n        dMeddm : np.array\n            Derivative of medical care with respect to market resources for each\n            point in (xLvl,MedShk).\n        '''\n        xLvl = self.xFunc(mLvl,pLvl,MedShk)\n        dxdm = self.xFunc.derivativeX(mLvl,pLvl,MedShk)\n        dcdx = self.cFunc.derivativeX(xLvl,MedShk)\n        dcdm = dxdm*dcdx\n        dMeddm = (dxdm - dcdm)/self.MedPrice\n        return dcdm,dMeddm"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nevaluate the derivative of consumption and medical need shock at given levels of market resources permanent income levels and medical need shocks at given levels of market resources permanent income levels and medical need shocks at given levels of market resources permanent income levels and medical need shocks.", "response": "def derivativeZ(self,mLvl,pLvl,MedShk):\n        '''\n        Evaluate the derivative of consumption and medical care with respect to\n        medical need shock at given levels of market resources, permanent income,\n        and medical need shocks.\n\n        Parameters\n        ----------\n        mLvl : np.array\n            Market resource levels.\n        pLvl : np.array\n            Permanent income levels; should be same size as mLvl.\n        MedShk : np.array\n            Medical need shocks; should be same size as mLvl.\n\n        Returns\n        -------\n        dcdShk : np.array\n            Derivative of consumption with respect to medical need for each\n            point in (xLvl,MedShk).\n        dMeddShk : np.array\n            Derivative of medical care with respect to medical need for each\n            point in (xLvl,MedShk).\n        '''\n        xLvl = self.xFunc(mLvl,pLvl,MedShk)\n        dxdShk = self.xFunc.derivativeZ(mLvl,pLvl,MedShk)\n        dcdx = self.cFunc.derivativeX(xLvl,MedShk)\n        dcdShk = dxdShk*dcdx + self.cFunc.derivativeY(xLvl,MedShk)\n        dMeddShk = (dxdShk - dcdShk)/self.MedPrice\n        return dcdShk,dMeddShk"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(self):\n        '''\n        Update the income process, the assets grid, the permanent income grid,\n        the medical shock distribution, and the terminal solution.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        self.updateIncomeProcess()\n        self.updateAssetsGrid()\n        self.updatepLvlNextFunc()\n        self.updatepLvlGrid()\n        self.updateMedShockProcess()\n        self.updateSolutionTerminal()", "response": "Updates the income process assets grid medical shock distribution and terminal solution."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the attribute MedShkDstn and add it to time_vary.", "response": "def updateMedShockProcess(self):\n        '''\n        Constructs discrete distributions of medical preference shocks for each\n        period in the cycle.  Distributions are saved as attribute MedShkDstn,\n        which is added to time_vary.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        MedShkDstn = [] # empty list for medical shock distribution each period\n        for t in range(self.T_cycle):\n            MedShkAvgNow  = self.MedShkAvg[t] # get shock distribution parameters\n            MedShkStdNow  = self.MedShkStd[t]\n            MedShkDstnNow = approxLognormal(mu=np.log(MedShkAvgNow)-0.5*MedShkStdNow**2,\\\n                            sigma=MedShkStdNow,N=self.MedShkCount, tail_N=self.MedShkCountTail,\n                            tail_bound=[0,0.9])\n            MedShkDstnNow = addDiscreteOutcomeConstantMean(MedShkDstnNow,0.0,0.0,sort=True) # add point at zero with no probability\n            MedShkDstn.append(MedShkDstnNow)\n        self.MedShkDstn = MedShkDstn\n        self.addToTimeVary('MedShkDstn')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the terminal period solution for this type.", "response": "def updateSolutionTerminal(self):\n        '''\n        Update the terminal period solution for this type.  Similar to other models,\n        optimal behavior involves spending all available market resources; however,\n        the agent must split his resources between consumption and medical care.\n\n        Parameters\n        ----------\n        None\n\n        Returns:\n        --------\n        None\n        '''\n        # Take last period data, whichever way time is flowing\n        if self.time_flow:\n            MedPrice = self.MedPrice[-1]\n            MedShkVals = self.MedShkDstn[-1][1]\n            MedShkPrbs = self.MedShkDstn[-1][0]\n        else:\n            MedPrice = self.MedPrice[0]\n            MedShkVals = self.MedShkDstn[0][1]\n            MedShkPrbs = self.MedShkDstn[0][0]\n\n        # Initialize grids of medical need shocks, market resources, and optimal consumption\n        MedShkGrid = MedShkVals\n        xLvlMin = np.min(self.aXtraGrid)*np.min(self.pLvlGrid)\n        xLvlMax = np.max(self.aXtraGrid)*np.max(self.pLvlGrid)\n        xLvlGrid = makeGridExpMult(xLvlMin, xLvlMax, 3*self.aXtraGrid.size, 8)\n        trivial_grid = np.array([0.0,1.0]) # Trivial grid\n\n        # Make the policy functions for the terminal period\n        xFunc_terminal = TrilinearInterp(np.array([[[0.0,0.0],[0.0,0.0]],[[1.0,1.0],[1.0,1.0]]]),\\\n                         trivial_grid,trivial_grid,trivial_grid)\n        policyFunc_terminal = MedShockPolicyFunc(xFunc_terminal,xLvlGrid,MedShkGrid,MedPrice,\n                              self.CRRA,self.CRRAmed,xLvlCubicBool=self.CubicBool)\n        cFunc_terminal = cThruXfunc(xFunc_terminal,policyFunc_terminal.cFunc)\n        MedFunc_terminal = MedThruXfunc(xFunc_terminal,policyFunc_terminal.cFunc,MedPrice)\n\n        # Calculate optimal consumption on a grid of market resources and medical shocks\n        mLvlGrid = xLvlGrid\n        mLvlGrid_tiled = np.tile(np.reshape(mLvlGrid,(mLvlGrid.size,1)),(1,MedShkGrid.size))\n        pLvlGrid_tiled = np.ones_like(mLvlGrid_tiled) # permanent income irrelevant in terminal period\n        MedShkGrid_tiled = np.tile(np.reshape(MedShkVals,(1,MedShkGrid.size)),(mLvlGrid.size,1))\n        cLvlGrid,MedGrid = policyFunc_terminal(mLvlGrid_tiled,pLvlGrid_tiled,MedShkGrid_tiled)\n\n        # Integrate marginal value across shocks to get expected marginal value\n        vPgrid = cLvlGrid**(-self.CRRA)\n        vPgrid[np.isinf(vPgrid)] = 0.0 # correct for issue at bottom edges\n        PrbGrid = np.tile(np.reshape(MedShkPrbs,(1,MedShkGrid.size)),(mLvlGrid.size,1))\n        vP_expected = np.sum(vPgrid*PrbGrid,axis=1)\n\n        # Construct the marginal (marginal) value function for the terminal period\n        vPnvrs = vP_expected**(-1.0/self.CRRA)\n        vPnvrs[0] = 0.0\n        vPnvrsFunc = BilinearInterp(np.tile(np.reshape(vPnvrs,(vPnvrs.size,1)),\n                     (1,trivial_grid.size)),mLvlGrid,trivial_grid)\n        vPfunc_terminal = MargValueFunc2D(vPnvrsFunc,self.CRRA)\n        vPPfunc_terminal = MargMargValueFunc2D(vPnvrsFunc,self.CRRA)\n\n        # Integrate value across shocks to get expected value\n        vGrid = utility(cLvlGrid,gam=self.CRRA) + MedShkGrid_tiled*utility(MedGrid,gam=self.CRRAmed)\n        vGrid[:,0] = utility(cLvlGrid[:,0],gam=self.CRRA) # correct for issue when MedShk=0\n        vGrid[np.isinf(vGrid)] = 0.0 # correct for issue at bottom edges\n        v_expected = np.sum(vGrid*PrbGrid,axis=1)\n\n        # Construct the value function for the terminal period\n        vNvrs = utility_inv(v_expected,gam=self.CRRA)\n        vNvrs[0] = 0.0\n        vNvrsP = vP_expected*utility_invP(v_expected,gam=self.CRRA) # NEED TO FIGURE OUT MPC MAX IN THIS MODEL\n        vNvrsP[0] = 0.0\n        tempFunc = CubicInterp(mLvlGrid,vNvrs,vNvrsP)\n        vNvrsFunc = LinearInterpOnInterp1D([tempFunc,tempFunc],trivial_grid)\n        vFunc_terminal = ValueFunc2D(vNvrsFunc,self.CRRA)\n\n        # Make the terminal period solution\n        self.solution_terminal.cFunc = cFunc_terminal\n        self.solution_terminal.MedFunc = MedFunc_terminal\n        self.solution_terminal.policyFunc = policyFunc_terminal\n        self.solution_terminal.vPfunc = vPfunc_terminal\n        self.solution_terminal.vFunc = vFunc_terminal\n        self.solution_terminal.vPPfunc = vPPfunc_terminal\n        self.solution_terminal.hNrm = 0.0 # Don't track normalized human wealth\n        self.solution_terminal.hLvl = lambda p : np.zeros_like(p) # But do track absolute human wealth by permanent income\n        self.solution_terminal.mLvlMin = lambda p : np.zeros_like(p)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the grid of permanent income levels.", "response": "def updatepLvlGrid(self):\n        '''\n        Update the grid of permanent income levels.  Currently only works for\n        infinite horizon models (cycles=0) and lifecycle models (cycles=1).  Not\n        clear what to do about cycles>1.  Identical to version in persistent\n        shocks model, but pLvl=0 is manually added to the grid (because there is\n        no closed form lower-bounding cFunc for pLvl=0).\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        # Run basic version of this method\n        PersistentShockConsumerType.updatepLvlGrid(self)\n        for j in range(len(self.pLvlGrid)): # Then add 0 to the bottom of each pLvlGrid\n            this_grid = self.pLvlGrid[j]\n            self.pLvlGrid[j] = np.insert(this_grid,0,0.0001)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the permanent and transitory income shocks for this period as well as medical need shocks and the price of medical care.", "response": "def getShocks(self):\n        '''\n        Gets permanent and transitory income shocks for this period as well as medical need shocks\n        and the price of medical care.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        PersistentShockConsumerType.getShocks(self) # Get permanent and transitory income shocks\n        MedShkNow = np.zeros(self.AgentCount) # Initialize medical shock array\n        MedPriceNow = np.zeros(self.AgentCount) # Initialize relative price array\n        for t in range(self.T_cycle):\n            these = t == self.t_cycle\n            N = np.sum(these)\n            if N > 0:\n                MedShkAvg = self.MedShkAvg[t]\n                MedShkStd = self.MedShkStd[t]\n                MedPrice  = self.MedPrice[t]\n                MedShkNow[these] = self.RNG.permutation(approxLognormal(N,mu=np.log(MedShkAvg)-0.5*MedShkStd**2,sigma=MedShkStd)[1])\n                MedPriceNow[these] = MedPrice\n        self.MedShkNow = MedShkNow\n        self.MedPriceNow = MedPriceNow"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating consumption and medical care for each consumer of this type using the consumption and medical care functions. Returns None if there is no such consumer.", "response": "def getControls(self):\n        '''\n        Calculates consumption and medical care for each consumer of this type using the consumption\n        and medical care functions.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        cLvlNow = np.zeros(self.AgentCount) + np.nan\n        MedNow  = np.zeros(self.AgentCount) + np.nan\n        for t in range(self.T_cycle):\n            these = t == self.t_cycle\n            cLvlNow[these], MedNow[these] = self.solution[t].policyFunc(self.mLvlNow[these],self.pLvlNow[these],self.MedShkNow[these])\n        self.cLvlNow = cLvlNow\n        self.MedNow  = MedNow\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the end - of - period assets for each consumer of this type.", "response": "def getPostStates(self):\n        '''\n        Calculates end-of-period assets for each consumer of this type.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        self.aLvlNow = self.mLvlNow - self.cLvlNow - self.MedPriceNow*self.MedNow\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nunpacks some of the inputs (and calculates simple objects based on them), storing the results in self for use by other methods. These include: income shocks and probabilities, medical shocks and probabilities, next period's marginal value function (etc), the probability of getting the worst income shock next period, the patience factor, human wealth, and the bounding MPCs. Parameters ---------- solution_next : ConsumerSolution The solution to next period's one period problem. IncomeDstn : [np.array] A list containing three arrays of floats, representing a discrete approximation to the income process between the period being solved and the one immediately following (in solution_next). Order: event probabilities, permanent shocks, transitory shocks. LivPrb : float Survival probability; likelihood of being alive at the beginning of the succeeding period. DiscFac : float Intertemporal discount factor for future utility. Returns ------- None", "response": "def setAndUpdateValues(self,solution_next,IncomeDstn,LivPrb,DiscFac):\n        '''\n        Unpacks some of the inputs (and calculates simple objects based on them),\n        storing the results in self for use by other methods.  These include:\n        income shocks and probabilities, medical shocks and probabilities, next\n        period's marginal value function (etc), the probability of getting the\n        worst income shock next period, the patience factor, human wealth, and\n        the bounding MPCs.\n\n        Parameters\n        ----------\n        solution_next : ConsumerSolution\n            The solution to next period's one period problem.\n        IncomeDstn : [np.array]\n            A list containing three arrays of floats, representing a discrete\n            approximation to the income process between the period being solved\n            and the one immediately following (in solution_next). Order: event\n            probabilities, permanent shocks, transitory shocks.\n        LivPrb : float\n            Survival probability; likelihood of being alive at the beginning of\n            the succeeding period.\n        DiscFac : float\n            Intertemporal discount factor for future utility.\n\n        Returns\n        -------\n        None\n        '''\n        # Run basic version of this method\n        ConsGenIncProcessSolver.setAndUpdateValues(self,self.solution_next,self.IncomeDstn,\n                                                     self.LivPrb,self.DiscFac)\n\n        # Also unpack the medical shock distribution\n        self.MedShkPrbs = self.MedShkDstn[0]\n        self.MedShkVals = self.MedShkDstn[1]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndefine the marginal utility function for this period and saves them as attributes of self for other methods .", "response": "def defUtilityFuncs(self):\n        '''\n        Defines CRRA utility function for this period (and its derivatives,\n        and their inverses), saving them as attributes of self for other methods\n        to use.  Extends version from ConsIndShock models by also defining inverse\n        marginal utility function over medical care.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        ConsGenIncProcessSolver.defUtilityFuncs(self) # Do basic version\n        self.uMedPinv = lambda Med : utilityP_inv(Med,gam=self.CRRAmed)\n        self.uMed     = lambda Med : utility(Med,gam=self.CRRAmed)\n        self.uMedPP   = lambda Med : utilityPP(Med,gam=self.CRRAmed)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef defBoroCnst(self,BoroCnstArt):\n        '''\n        Defines the constrained portion of the consumption function as cFuncNowCnst,\n        an attribute of self.  Uses the artificial and natural borrowing constraints.\n\n        Parameters\n        ----------\n        BoroCnstArt : float or None\n            Borrowing constraint for the minimum allowable (normalized) assets\n            to end the period with.  If it is less than the natural borrowing\n            constraint at a particular permanent income level, then it is irrelevant;\n            BoroCnstArt=None indicates no artificial borrowing constraint.\n\n        Returns\n        -------\n        None\n        '''\n        # Find minimum allowable end-of-period assets at each permanent income level\n        PermIncMinNext = self.PermShkMinNext*self.pLvlNextFunc(self.pLvlGrid)\n        IncLvlMinNext  = PermIncMinNext*self.TranShkMinNext\n        aLvlMin = (self.solution_next.mLvlMin(PermIncMinNext) - IncLvlMinNext)/self.Rfree\n\n        # Make a function for the natural borrowing constraint by permanent income\n        BoroCnstNat = LinearInterp(np.insert(self.pLvlGrid,0,0.0),np.insert(aLvlMin,0,0.0))\n        self.BoroCnstNat = BoroCnstNat\n\n        # Define the minimum allowable level of market resources by permanent income\n        if self.BoroCnstArt is not None:\n            BoroCnstArt = LinearInterp([0.0,1.0],[0.0,self.BoroCnstArt])\n            self.mLvlMinNow = UpperEnvelope(BoroCnstNat,BoroCnstArt)\n        else:\n            self.mLvlMinNow = BoroCnstNat\n\n        # Make the constrained total spending function: spend all market resources\n        trivial_grid = np.array([0.0,1.0]) # Trivial grid\n        spendAllFunc = TrilinearInterp(np.array([[[0.0,0.0],[0.0,0.0]],[[1.0,1.0],[1.0,1.0]]]),\\\n                       trivial_grid,trivial_grid,trivial_grid)\n        self.xFuncNowCnst = VariableLowerBoundFunc3D(spendAllFunc,self.mLvlMinNow)\n\n        self.mNrmMinNow = 0.0 # Needs to exist so as not to break when solution is created\n        self.MPCmaxEff  = 0.0", "response": "Define the constrained portion of the consumption function as cFuncNowCnst and self. mLvlMinNow."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind endogenous interpolation points (x,m) for the expenditure function. Parameters ---------- EndOfPrdvP : np.array Array of end-of-period marginal values. aLvlNow : np.array Array of end-of-period asset values that yield the marginal values in EndOfPrdvP. Returns ------- x_for_interpolation : np.array Total expenditure points for interpolation. m_for_interpolation : np.array Corresponding market resource points for interpolation. p_for_interpolation : np.array Corresponding permanent income points for interpolation.", "response": "def getPointsForInterpolation(self,EndOfPrdvP,aLvlNow):\n        '''\n        Finds endogenous interpolation points (x,m) for the expenditure function.\n\n        Parameters\n        ----------\n        EndOfPrdvP : np.array\n            Array of end-of-period marginal values.\n        aLvlNow : np.array\n            Array of end-of-period asset values that yield the marginal values\n            in EndOfPrdvP.\n\n        Returns\n        -------\n        x_for_interpolation : np.array\n            Total expenditure points for interpolation.\n        m_for_interpolation : np.array\n            Corresponding market resource points for interpolation.\n        p_for_interpolation : np.array\n            Corresponding permanent income points for interpolation.\n        '''\n        # Get size of each state dimension\n        mCount      = aLvlNow.shape[1]\n        pCount      = aLvlNow.shape[0]\n        MedCount = self.MedShkVals.size\n\n        # Calculate endogenous gridpoints and controls\n        cLvlNow = np.tile(np.reshape(self.uPinv(EndOfPrdvP),(1,pCount,mCount)),(MedCount,1,1))\n        MedBaseNow = np.tile(np.reshape(self.uMedPinv(self.MedPrice*EndOfPrdvP),(1,pCount,mCount)),\n                             (MedCount,1,1))\n        MedShkVals_tiled = np.tile(np.reshape(self.MedShkVals**(1.0/self.CRRAmed),(MedCount,1,1)),\n                                   (1,pCount,mCount))\n        MedLvlNow = MedShkVals_tiled*MedBaseNow\n        aLvlNow_tiled = np.tile(np.reshape(aLvlNow,(1,pCount,mCount)),(MedCount,1,1))\n        xLvlNow = cLvlNow + self.MedPrice*MedLvlNow\n        mLvlNow = xLvlNow + aLvlNow_tiled\n\n        # Limiting consumption is zero as m approaches the natural borrowing constraint\n        x_for_interpolation = np.concatenate((np.zeros((MedCount,pCount,1)),xLvlNow),axis=-1)\n        temp = np.tile(self.BoroCnstNat(np.reshape(self.pLvlGrid,(1,self.pLvlGrid.size,1))),\n                       (MedCount,1,1))\n        m_for_interpolation = np.concatenate((temp,mLvlNow),axis=-1)\n\n        # Make a 3D array of permanent income for interpolation\n        p_for_interpolation = np.tile(np.reshape(self.pLvlGrid,(1,pCount,1)),(MedCount,1,mCount+1))\n\n        # Store for use by cubic interpolator\n        self.cLvlNow = cLvlNow\n        self.MedLvlNow = MedLvlNow\n        self.MedShkVals_tiled = np.tile(np.reshape(self.MedShkVals,(MedCount,1,1)),(1,pCount,mCount))\n\n        return x_for_interpolation, m_for_interpolation, p_for_interpolation"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a ConsumerSolution for this period with a consumption function and marginal value function and minimum m.", "response": "def usePointsForInterpolation(self,xLvl,mLvl,pLvl,MedShk,interpolator):\n        '''\n        Constructs a basic solution for this period, including the consumption\n        function and marginal value function.\n\n        Parameters\n        ----------\n        xLvl : np.array\n            Total expenditure points for interpolation.\n        mLvl : np.array\n            Corresponding market resource points for interpolation.\n        pLvl : np.array\n            Corresponding permanent income level points for interpolation.\n        MedShk : np.array\n            Corresponding medical need shocks for interpolation.\n        interpolator : function\n            A function that constructs and returns a consumption function.\n\n        Returns\n        -------\n        solution_now : ConsumerSolution\n            The solution to this period's consumption-saving problem, with a\n            consumption function, marginal value function, and minimum m.\n        '''\n        # Construct the unconstrained total expenditure function\n        xFuncNowUnc = interpolator(mLvl,pLvl,MedShk,xLvl)\n        xFuncNowCnst = self.xFuncNowCnst\n        xFuncNow = LowerEnvelope3D(xFuncNowUnc,xFuncNowCnst)\n\n        # Transform the expenditure function into policy functions for consumption and medical care\n        aug_factor = 2\n        xLvlGrid = makeGridExpMult(np.min(xLvl),np.max(xLvl),aug_factor*self.aXtraGrid.size,8)\n        policyFuncNow = MedShockPolicyFunc(xFuncNow,xLvlGrid,self.MedShkVals,self.MedPrice,\n                        self.CRRA,self.CRRAmed,xLvlCubicBool=self.CubicBool)\n        cFuncNow = cThruXfunc(xFuncNow,policyFuncNow.cFunc)\n        MedFuncNow = MedThruXfunc(xFuncNow,policyFuncNow.cFunc,self.MedPrice)\n\n        # Make the marginal value function (and the value function if vFuncBool=True)\n        vFuncNow, vPfuncNow = self.makevAndvPfuncs(policyFuncNow)\n\n        # Pack up the solution and return it\n        solution_now = ConsumerSolution(cFunc=cFuncNow, vFunc=vFuncNow, vPfunc=vPfuncNow,\\\n                                        mNrmMin=self.mNrmMinNow)\n        solution_now.MedFunc = MedFuncNow\n        solution_now.policyFunc = policyFuncNow\n        return solution_now"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef makevAndvPfuncs(self,policyFunc):\n        '''\n        Constructs the marginal value function for this period.\n\n        Parameters\n        ----------\n        policyFunc : function\n            Consumption and medical care function for this period, defined over\n            market resources, permanent income level, and the medical need shock.\n\n        Returns\n        -------\n        vFunc : function\n            Value function for this period, defined over market resources and\n            permanent income.\n        vPfunc : function\n            Marginal value (of market resources) function for this period, defined\n            over market resources and permanent income.\n        '''\n        # Get state dimension sizes\n        mCount   = self.aXtraGrid.size\n        pCount   = self.pLvlGrid.size\n        MedCount = self.MedShkVals.size\n\n        # Make temporary grids to evaluate the consumption function\n        temp_grid  = np.tile(np.reshape(self.aXtraGrid,(mCount,1,1)),(1,pCount,MedCount))\n        aMinGrid   = np.tile(np.reshape(self.mLvlMinNow(self.pLvlGrid),(1,pCount,1)),\n                             (mCount,1,MedCount))\n        pGrid      = np.tile(np.reshape(self.pLvlGrid,(1,pCount,1)),(mCount,1,MedCount))\n        mGrid      = temp_grid*pGrid + aMinGrid\n        if self.pLvlGrid[0] == 0:\n            mGrid[:,0,:] = np.tile(np.reshape(self.aXtraGrid,(mCount,1)),(1,MedCount))\n        MedShkGrid = np.tile(np.reshape(self.MedShkVals,(1,1,MedCount)),(mCount,pCount,1))\n        probsGrid  = np.tile(np.reshape(self.MedShkPrbs,(1,1,MedCount)),(mCount,pCount,1))\n\n        # Get optimal consumption (and medical care) for each state\n        cGrid,MedGrid = policyFunc(mGrid,pGrid,MedShkGrid)\n\n        # Calculate expected value by \"integrating\" across medical shocks\n        if self.vFuncBool:\n            MedGrid = np.maximum(MedGrid,1e-100) # interpolation error sometimes makes Med < 0 (barely)\n            aGrid = np.maximum(mGrid - cGrid - self.MedPrice*MedGrid, aMinGrid) # interpolation error sometimes makes tiny violations\n            vGrid = self.u(cGrid) + MedShkGrid*self.uMed(MedGrid) + self.EndOfPrdvFunc(aGrid,pGrid)\n            vNow  = np.sum(vGrid*probsGrid,axis=2)\n\n        # Calculate expected marginal value by \"integrating\" across medical shocks\n        vPgrid = self.uP(cGrid)\n        vPnow  = np.sum(vPgrid*probsGrid,axis=2)\n\n        # Add vPnvrs=0 at m=mLvlMin to close it off at the bottom (and vNvrs=0)\n        mGrid_small = np.concatenate((np.reshape(self.mLvlMinNow(self.pLvlGrid),(1,pCount)),mGrid[:,:,0]))\n        vPnvrsNow   = np.concatenate((np.zeros((1,pCount)),self.uPinv(vPnow)))\n        if self.vFuncBool:\n            vNvrsNow  = np.concatenate((np.zeros((1,pCount)),self.uinv(vNow)),axis=0)\n            vNvrsPnow = vPnow*self.uinvP(vNow)\n            vNvrsPnow = np.concatenate((np.zeros((1,pCount)),vNvrsPnow),axis=0)\n\n        # Construct the pseudo-inverse value and marginal value functions over mLvl,pLvl\n        vPnvrsFunc_by_pLvl = []\n        vNvrsFunc_by_pLvl = []\n        for j in range(pCount): # Make a pseudo inverse marginal value function for each pLvl\n            pLvl = self.pLvlGrid[j]\n            m_temp = mGrid_small[:,j] - self.mLvlMinNow(pLvl)\n            vPnvrs_temp = vPnvrsNow[:,j]\n            vPnvrsFunc_by_pLvl.append(LinearInterp(m_temp,vPnvrs_temp))\n            if self.vFuncBool:\n                vNvrs_temp  = vNvrsNow[:,j]\n                vNvrsP_temp = vNvrsPnow[:,j]\n                vNvrsFunc_by_pLvl.append(CubicInterp(m_temp,vNvrs_temp,vNvrsP_temp))\n        vPnvrsFuncBase = LinearInterpOnInterp1D(vPnvrsFunc_by_pLvl,self.pLvlGrid)\n        vPnvrsFunc = VariableLowerBoundFunc2D(vPnvrsFuncBase,self.mLvlMinNow) # adjust for the lower bound of mLvl\n        if self.vFuncBool:\n            vNvrsFuncBase  = LinearInterpOnInterp1D(vNvrsFunc_by_pLvl,self.pLvlGrid)\n            vNvrsFunc = VariableLowerBoundFunc2D(vNvrsFuncBase,self.mLvlMinNow) # adjust for the lower bound of mLvl\n\n        # \"Re-curve\" the (marginal) value function\n        vPfunc = MargValueFunc2D(vPnvrsFunc,self.CRRA)\n        if self.vFuncBool:\n            vFunc = ValueFunc2D(vNvrsFunc,self.CRRA)\n        else:\n            vFunc = NullFunc()\n\n        return vFunc, vPfunc", "response": "Constructs the marginal value function for this period and the value function for the current state."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef makeLinearxFunc(self,mLvl,pLvl,MedShk,xLvl):\n        '''\n        Constructs the (unconstrained) expenditure function for this period using\n        bilinear interpolation (over permanent income and the medical shock) among\n        an array of linear interpolations over market resources.\n\n        Parameters\n        ----------\n        mLvl : np.array\n            Corresponding market resource points for interpolation.\n        pLvl : np.array\n            Corresponding permanent income level points for interpolation.\n        MedShk : np.array\n            Corresponding medical need shocks for interpolation.\n        xLvl : np.array\n            Expenditure points for interpolation, corresponding to those in mLvl,\n            pLvl, and MedShk.\n\n        Returns\n        -------\n        xFuncUnc : BilinearInterpOnInterp1D\n            Unconstrained total expenditure function for this period.\n        '''\n        # Get state dimensions\n        pCount = mLvl.shape[1]\n        MedCount = mLvl.shape[0]\n\n        # Loop over each permanent income level and medical shock and make a linear xFunc\n        xFunc_by_pLvl_and_MedShk = [] # Initialize the empty list of lists of 1D xFuncs\n        for i in range(pCount):\n            temp_list = []\n            pLvl_i = pLvl[0,i,0]\n            mLvlMin_i = self.BoroCnstNat(pLvl_i)\n            for j in range(MedCount):\n                m_temp = mLvl[j,i,:] - mLvlMin_i\n                x_temp = xLvl[j,i,:]\n                temp_list.append(LinearInterp(m_temp,x_temp))\n            xFunc_by_pLvl_and_MedShk.append(deepcopy(temp_list))\n\n        # Combine the nested list of linear xFuncs into a single function\n        pLvl_temp = pLvl[0,:,0]\n        MedShk_temp = MedShk[:,0,0]\n        xFuncUncBase = BilinearInterpOnInterp1D(xFunc_by_pLvl_and_MedShk,pLvl_temp,MedShk_temp)\n        xFuncUnc = VariableLowerBoundFunc3D(xFuncUncBase,self.BoroCnstNat)\n        return xFuncUnc", "response": "Constructs a linear expenditure function for a given set of permanent income levels and medical shocks."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconstructing the unconstrained total expenditure function for this period using cubic interpolation over market resource points and medical shocks.", "response": "def makeCubicxFunc(self,mLvl,pLvl,MedShk,xLvl):\n        '''\n        Constructs the (unconstrained) expenditure function for this period using\n        bilinear interpolation (over permanent income and the medical shock) among\n        an array of cubic interpolations over market resources.\n\n        Parameters\n        ----------\n        mLvl : np.array\n            Corresponding market resource points for interpolation.\n        pLvl : np.array\n            Corresponding permanent income level points for interpolation.\n        MedShk : np.array\n            Corresponding medical need shocks for interpolation.\n        xLvl : np.array\n            Expenditure points for interpolation, corresponding to those in mLvl,\n            pLvl, and MedShk.\n\n        Returns\n        -------\n        xFuncUnc : BilinearInterpOnInterp1D\n            Unconstrained total expenditure function for this period.\n        '''\n        # Get state dimensions\n        pCount = mLvl.shape[1]\n        MedCount = mLvl.shape[0]\n\n        # Calculate the MPC and MPM at each gridpoint\n        EndOfPrdvPP = self.DiscFacEff*self.Rfree*self.Rfree*np.sum(self.vPPfuncNext(self.mLvlNext,\\\n                      self.pLvlNext)*self.ShkPrbs_temp,axis=0)\n        EndOfPrdvPP = np.tile(np.reshape(EndOfPrdvPP,(1,pCount,EndOfPrdvPP.shape[1])),(MedCount,1,1))\n        dcda        = EndOfPrdvPP/self.uPP(np.array(self.cLvlNow))\n        dMedda      = EndOfPrdvPP/(self.MedShkVals_tiled*self.uMedPP(self.MedLvlNow))\n        dMedda[0,:,:] = 0.0 # dMedda goes crazy when MedShk=0\n        MPC         = dcda/(1.0 + dcda + self.MedPrice*dMedda)\n        MPM         = dMedda/(1.0 + dcda + self.MedPrice*dMedda)\n\n        # Convert to marginal propensity to spend\n        MPX = MPC + self.MedPrice*MPM\n        MPX = np.concatenate((np.reshape(MPX[:,:,0],(MedCount,pCount,1)),MPX),axis=2) # NEED TO CALCULATE MPM AT NATURAL BORROWING CONSTRAINT\n        MPX[0,:,0] = self.MPCmaxNow\n\n        # Loop over each permanent income level and medical shock and make a cubic xFunc\n        xFunc_by_pLvl_and_MedShk = [] # Initialize the empty list of lists of 1D xFuncs\n        for i in range(pCount):\n            temp_list = []\n            pLvl_i = pLvl[0,i,0]\n            mLvlMin_i = self.BoroCnstNat(pLvl_i)\n            for j in range(MedCount):\n                m_temp = mLvl[j,i,:] - mLvlMin_i\n                x_temp = xLvl[j,i,:]\n                MPX_temp = MPX[j,i,:]\n                temp_list.append(CubicInterp(m_temp,x_temp,MPX_temp))\n            xFunc_by_pLvl_and_MedShk.append(deepcopy(temp_list))\n\n        # Combine the nested list of cubic xFuncs into a single function\n        pLvl_temp = pLvl[0,:,0]\n        MedShk_temp = MedShk[:,0,0]\n        xFuncUncBase = BilinearInterpOnInterp1D(xFunc_by_pLvl_and_MedShk,pLvl_temp,MedShk_temp)\n        xFuncUnc = VariableLowerBoundFunc3D(xFuncUncBase,self.BoroCnstNat)\n        return xFuncUnc"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive end - of - period assets and end - of - period marginal values construct a basic solution for this period.", "response": "def makeBasicSolution(self,EndOfPrdvP,aLvl,interpolator):\n        '''\n        Given end of period assets and end of period marginal value, construct\n        the basic solution for this period.\n\n        Parameters\n        ----------\n        EndOfPrdvP : np.array\n            Array of end-of-period marginal values.\n        aLvl : np.array\n            Array of end-of-period asset values that yield the marginal values\n            in EndOfPrdvP.\n        interpolator : function\n            A function that constructs and returns a consumption function.\n\n        Returns\n        -------\n        solution_now : ConsumerSolution\n            The solution to this period's consumption-saving problem, with a\n            consumption function, marginal value function, and minimum m.\n        '''\n        xLvl,mLvl,pLvl = self.getPointsForInterpolation(EndOfPrdvP,aLvl)\n        MedShk_temp    = np.tile(np.reshape(self.MedShkVals,(self.MedShkVals.size,1,1)),\\\n                         (1,mLvl.shape[1],mLvl.shape[2]))\n        solution_now   = self.usePointsForInterpolation(xLvl,mLvl,pLvl,MedShk_temp,interpolator)\n        return solution_now"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef solve(self):\n        '''\n        Solves a one period consumption saving problem with risky income and\n        shocks to medical need.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        solution : ConsumerSolution\n            The solution to the one period problem, including a consumption\n            function, medical spending function ( both defined over market re-\n            sources, permanent income, and medical shock), a marginal value func-\n            tion (defined over market resources and permanent income), and human\n            wealth as a function of permanent income.\n        '''\n        aLvl,trash  = self.prepareToCalcEndOfPrdvP()\n        EndOfPrdvP = self.calcEndOfPrdvP()\n        if self.vFuncBool:\n            self.makeEndOfPrdvFunc(EndOfPrdvP)\n        if self.CubicBool:\n            interpolator = self.makeCubicxFunc\n        else:\n            interpolator = self.makeLinearxFunc\n        solution   = self.makeBasicSolution(EndOfPrdvP,aLvl,interpolator)\n        solution   = self.addMPCandHumanWealth(solution)\n        if self.CubicBool:\n            solution = self.addvPPfunc(solution)\n        return solution", "response": "Solves a one period consumption saving problem with risky income and medical shock and medical need."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef solveConsMarkov(solution_next,IncomeDstn,LivPrb,DiscFac,CRRA,Rfree,PermGroFac,\n                                 MrkvArray,BoroCnstArt,aXtraGrid,vFuncBool,CubicBool):\n    '''\n    Solves a single period consumption-saving problem with risky income and\n    stochastic transitions between discrete states, in a Markov fashion.  Has\n    identical inputs as solveConsIndShock, except for a discrete\n    Markov transitionrule MrkvArray.  Markov states can differ in their interest\n    factor, permanent growth factor, and income distribution, so the inputs Rfree,\n    PermGroFac, and IncomeDstn are arrays or lists specifying those values in each\n    (succeeding) Markov state.\n\n    Parameters\n    ----------\n    solution_next : ConsumerSolution\n        The solution to next period's one period problem.\n    IncomeDstn_list : [[np.array]]\n        A length N list of income distributions in each succeeding Markov\n        state.  Each income distribution contains three arrays of floats,\n        representing a discrete approximation to the income process at the\n        beginning of the succeeding period. Order: event probabilities,\n        permanent shocks, transitory shocks.\n    LivPrb : float\n        Survival probability; likelihood of being alive at the beginning of\n        the succeeding period.\n    DiscFac : float\n        Intertemporal discount factor for future utility.\n    CRRA : float\n        Coefficient of relative risk aversion.\n    Rfree_list : np.array\n        Risk free interest factor on end-of-period assets for each Markov\n        state in the succeeding period.\n    PermGroGac_list : float\n        Expected permanent income growth factor at the end of this period\n        for each Markov state in the succeeding period.\n    MrkvArray : numpy.array\n        An NxN array representing a Markov transition matrix between discrete\n        states.  The i,j-th element of MrkvArray is the probability of\n        moving from state i in period t to state j in period t+1.\n    BoroCnstArt: float or None\n        Borrowing constraint for the minimum allowable assets to end the\n        period with.  If it is less than the natural borrowing constraint,\n        then it is irrelevant; BoroCnstArt=None indicates no artificial bor-\n        rowing constraint.\n    aXtraGrid: np.array\n        Array of \"extra\" end-of-period asset values-- assets above the\n        absolute minimum acceptable level.\n    vFuncBool: boolean\n        An indicator for whether the value function should be computed and\n        included in the reported solution.\n    CubicBool: boolean\n        An indicator for whether the solver should use cubic or linear inter-\n        polation.\n\n    Returns\n    -------\n    solution : ConsumerSolution\n        The solution to the single period consumption-saving problem. Includes\n        a consumption function cFunc (using cubic or linear splines), a marg-\n        inal value function vPfunc, a minimum acceptable level of normalized\n        market resources mNrmMin, normalized human wealth hNrm, and bounding\n        MPCs MPCmin and MPCmax.  It might also have a value function vFunc\n        and marginal marginal value function vPPfunc.  All of these attributes\n        are lists or arrays, with elements corresponding to the current\n        Markov state.  E.g. solution.cFunc[0] is the consumption function\n        when in the i=0 Markov state this period.\n    '''\n    solver = ConsMarkovSolver(solution_next,IncomeDstn,LivPrb,DiscFac,CRRA,Rfree,\n                              PermGroFac,MrkvArray,BoroCnstArt,aXtraGrid,vFuncBool,CubicBool)\n    solution_now = solver.solve()\n    return solution_now", "response": "Solves a single period consumption - saving problem with risky income and income and transitionrule MrkvArray between discrete states."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsolves the one period problem of the consumption - saving model with a Markov state.", "response": "def solve(self):\n        '''\n        Solve the one period problem of the consumption-saving model with a Markov state.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        solution : ConsumerSolution\n            The solution to the single period consumption-saving problem. Includes\n            a consumption function cFunc (using cubic or linear splines), a marg-\n            inal value function vPfunc, a minimum acceptable level of normalized\n            market resources mNrmMin, normalized human wealth hNrm, and bounding\n            MPCs MPCmin and MPCmax.  It might also have a value function vFunc\n            and marginal marginal value function vPPfunc.  All of these attributes\n            are lists or arrays, with elements corresponding to the current\n            Markov state.  E.g. solution.cFunc[0] is the consumption function\n            when in the i=0 Markov state this period.\n        '''\n        # Find the natural borrowing constraint in each current state\n        self.defBoundary()\n\n        # Initialize end-of-period (marginal) value functions\n        self.EndOfPrdvFunc_list  = []\n        self.EndOfPrdvPfunc_list = []\n        self.ExIncNextAll        = np.zeros(self.StateCount) + np.nan # expected income conditional on the next state\n        self.WorstIncPrbAll      = np.zeros(self.StateCount) + np.nan # probability of getting the worst income shock in each next period state\n\n        # Loop through each next-period-state and calculate the end-of-period\n        # (marginal) value function\n        for j in range(self.StateCount):\n            # Condition values on next period's state (and record a couple for later use)\n            self.conditionOnState(j)\n            self.ExIncNextAll[j]   = np.dot(self.ShkPrbsNext,self.PermShkValsNext*self.TranShkValsNext)\n            self.WorstIncPrbAll[j] = self.WorstIncPrb\n\n            # Construct the end-of-period marginal value function conditional\n            # on next period's state and add it to the list of value functions\n            EndOfPrdvPfunc_cond = self.makeEndOfPrdvPfuncCond()\n            self.EndOfPrdvPfunc_list.append(EndOfPrdvPfunc_cond)\n\n            # Construct the end-of-period value functional conditional on next\n            # period's state and add it to the list of value functions\n            if self.vFuncBool:\n                EndOfPrdvFunc_cond = self.makeEndOfPrdvFuncCond()\n                self.EndOfPrdvFunc_list.append(EndOfPrdvFunc_cond)\n\n        # EndOfPrdvP_cond is EndOfPrdvP conditional on *next* period's state.\n        # Take expectations to get EndOfPrdvP conditional on *this* period's state.\n        self.calcEndOfPrdvP()\n\n        # Calculate the bounding MPCs and PDV of human wealth for each state\n        self.calcHumWealthAndBoundingMPCs()\n\n        # Find consumption and market resources corresponding to each end-of-period\n        # assets point for each state (and add an additional point at the lower bound)\n        aNrm = np.asarray(self.aXtraGrid)[np.newaxis,:] + np.array(self.BoroCnstNat_list)[:,np.newaxis]\n        self.getPointsForInterpolation(self.EndOfPrdvP,aNrm)\n        cNrm = np.hstack((np.zeros((self.StateCount,1)),self.cNrmNow))\n        mNrm = np.hstack((np.reshape(self.mNrmMin_list,(self.StateCount,1)),self.mNrmNow))\n\n        # Package and return the solution for this period\n        self.BoroCnstNat = self.BoroCnstNat_list\n        solution = self.makeSolution(cNrm,mNrm)\n        return solution"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef defBoundary(self):\n        '''\n        Find the borrowing constraint for each current state and save it as an\n        attribute of self for use by other methods.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        self.BoroCnstNatAll          = np.zeros(self.StateCount) + np.nan\n        # Find the natural borrowing constraint conditional on next period's state\n        for j in range(self.StateCount):\n            PermShkMinNext         = np.min(self.IncomeDstn_list[j][1])\n            TranShkMinNext         = np.min(self.IncomeDstn_list[j][2])\n            self.BoroCnstNatAll[j] = (self.solution_next.mNrmMin[j] - TranShkMinNext)*\\\n                                     (self.PermGroFac_list[j]*PermShkMinNext)/self.Rfree_list[j]\n\n        self.BoroCnstNat_list   = np.zeros(self.StateCount) + np.nan\n        self.mNrmMin_list       = np.zeros(self.StateCount) + np.nan\n        self.BoroCnstDependency = np.zeros((self.StateCount,self.StateCount)) + np.nan\n        # The natural borrowing constraint in each current state is the *highest*\n        # among next-state-conditional natural borrowing constraints that could\n        # occur from this current state.\n        for i in range(self.StateCount):\n            possible_next_states         = self.MrkvArray[i,:] > 0\n            self.BoroCnstNat_list[i]     = np.max(self.BoroCnstNatAll[possible_next_states])\n\n            # Explicitly handle the \"None\" case:            \n            if self.BoroCnstArt is None:\n                self.mNrmMin_list[i]         = self.BoroCnstNat_list[i]\n            else:\n                self.mNrmMin_list[i]         = np.max([self.BoroCnstNat_list[i],self.BoroCnstArt])\n            self.BoroCnstDependency[i,:] = self.BoroCnstNat_list[i] == self.BoroCnstNatAll", "response": "This method finds the natural borrowing constraint for each current state and saves it as an attribute of self for use by other methods."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef conditionOnState(self,state_index):\n        '''\n        Temporarily assume that a particular Markov state will occur in the\n        succeeding period, and condition solver attributes on this assumption.\n        Allows the solver to construct the future-state-conditional marginal\n        value function (etc) for that future state.\n\n        Parameters\n        ----------\n        state_index : int\n            Index of the future Markov state to condition on.\n\n        Returns\n        -------\n        none\n        '''\n        # Set future-state-conditional values as attributes of self\n        self.IncomeDstn     = self.IncomeDstn_list[state_index]\n        self.Rfree          = self.Rfree_list[state_index]\n        self.PermGroFac     = self.PermGroFac_list[state_index]\n        self.vPfuncNext     = self.solution_next.vPfunc[state_index]\n        self.mNrmMinNow     = self.mNrmMin_list[state_index]\n        self.BoroCnstNat    = self.BoroCnstNatAll[state_index]\n        self.setAndUpdateValues(self.solution_next,self.IncomeDstn,self.LivPrb,self.DiscFac)\n        self.DiscFacEff     = self.DiscFac # survival probability LivPrb represents probability from\n                                           # *current* state, so DiscFacEff is just DiscFac for now\n\n        # These lines have to come after setAndUpdateValues to override the definitions there\n        self.vPfuncNext = self.solution_next.vPfunc[state_index]\n        if self.CubicBool:\n            self.vPPfuncNext= self.solution_next.vPPfunc[state_index]\n        if self.vFuncBool:\n            self.vFuncNext  = self.solution_next.vFunc[state_index]", "response": "This function is called by the state machine to determine if a particular Markov state is occurring in a particular state."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calcEndOfPrdvPP(self):\n        '''\n        Calculates end-of-period marginal marginal value using a pre-defined\n        array of next period market resources in self.mNrmNext.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        EndOfPrdvPP : np.array\n            End-of-period marginal marginal value of assets at each value in\n            the grid of assets.\n        '''\n        EndOfPrdvPP = self.DiscFacEff*self.Rfree*self.Rfree*self.PermGroFac**(-self.CRRA-1.0)*\\\n                      np.sum(self.PermShkVals_temp**(-self.CRRA-1.0)*self.vPPfuncNext(self.mNrmNext)\n                      *self.ShkPrbs_temp,axis=0)\n        return EndOfPrdvPP", "response": "Calculates the end - of - period marginal value using a pre - defined array of next period market resources in self. mNrmNext."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef makeEndOfPrdvFuncCond(self):\n        '''\n        Construct the end-of-period value function conditional on next period's\n        state.  NOTE: It might be possible to eliminate this method and replace\n        it with ConsIndShockSolver.makeEndOfPrdvFunc, but the self.X_cond\n        variables must be renamed.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        EndofPrdvFunc_cond : ValueFunc\n            The end-of-period value function conditional on a particular state\n            occuring in the next period.\n        '''\n        VLvlNext               = (self.PermShkVals_temp**(1.0-self.CRRA)*\n                                  self.PermGroFac**(1.0-self.CRRA))*self.vFuncNext(self.mNrmNext)\n        EndOfPrdv_cond         = self.DiscFacEff*np.sum(VLvlNext*self.ShkPrbs_temp,axis=0)\n        EndOfPrdvNvrs_cond     = self.uinv(EndOfPrdv_cond)\n        EndOfPrdvNvrsP_cond    = self.EndOfPrdvP_cond*self.uinvP(EndOfPrdv_cond)\n        EndOfPrdvNvrs_cond     = np.insert(EndOfPrdvNvrs_cond,0,0.0)\n        EndOfPrdvNvrsP_cond    = np.insert(EndOfPrdvNvrsP_cond,0,EndOfPrdvNvrsP_cond[0])\n        aNrm_temp              = np.insert(self.aNrm_cond,0,self.BoroCnstNat)\n        EndOfPrdvNvrsFunc_cond = CubicInterp(aNrm_temp,EndOfPrdvNvrs_cond,EndOfPrdvNvrsP_cond)\n        EndofPrdvFunc_cond     = ValueFunc(EndOfPrdvNvrsFunc_cond,self.CRRA)\n        return EndofPrdvFunc_cond", "response": "Construct the end - of - period value function conditional on next period s state."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconstruct the end - of - period marginal value function conditional on a particular resource.", "response": "def makeEndOfPrdvPfuncCond(self):\n        '''\n        Construct the end-of-period marginal value function conditional on next\n        period's state.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        EndofPrdvPfunc_cond : MargValueFunc\n            The end-of-period marginal value function conditional on a particular\n            state occuring in the succeeding period.\n        '''\n        # Get data to construct the end-of-period marginal value function (conditional on next state)\n        self.aNrm_cond      = self.prepareToCalcEndOfPrdvP()\n        self.EndOfPrdvP_cond= self.calcEndOfPrdvPcond()\n        EndOfPrdvPnvrs_cond = self.uPinv(self.EndOfPrdvP_cond) # \"decurved\" marginal value\n        if self.CubicBool:\n            EndOfPrdvPP_cond = self.calcEndOfPrdvPP()\n            EndOfPrdvPnvrsP_cond = EndOfPrdvPP_cond*self.uPinvP(self.EndOfPrdvP_cond) # \"decurved\" marginal marginal value\n\n        # Construct the end-of-period marginal value function conditional on the next state.\n        if self.CubicBool:\n            EndOfPrdvPnvrsFunc_cond = CubicInterp(self.aNrm_cond,EndOfPrdvPnvrs_cond,\n                                                  EndOfPrdvPnvrsP_cond,lower_extrap=True)\n        else:\n            EndOfPrdvPnvrsFunc_cond = LinearInterp(self.aNrm_cond,EndOfPrdvPnvrs_cond,\n                                                   lower_extrap=True)\n        EndofPrdvPfunc_cond = MargValueFunc(EndOfPrdvPnvrsFunc_cond,self.CRRA) # \"recurve\" the interpolated marginal value function\n        return EndofPrdvPfunc_cond"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calcEndOfPrdvP(self):\n        '''\n        Calculates end of period marginal value (and marginal marginal) value\n        at each aXtra gridpoint for each current state, unconditional on the\n        future Markov state (i.e. weighting conditional end-of-period marginal\n        value by transition probabilities).\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        # Find unique values of minimum acceptable end-of-period assets (and the\n        # current period states for which they apply).\n        aNrmMin_unique, state_inverse = np.unique(self.BoroCnstNat_list,return_inverse=True)\n        self.possible_transitions     = self.MrkvArray > 0\n\n        # Calculate end-of-period marginal value (and marg marg value) at each\n        # asset gridpoint for each current period state\n        EndOfPrdvP                    = np.zeros((self.StateCount,self.aXtraGrid.size))\n        EndOfPrdvPP                   = np.zeros((self.StateCount,self.aXtraGrid.size))\n        for k in range(aNrmMin_unique.size):\n            aNrmMin       = aNrmMin_unique[k]   # minimum assets for this pass\n            which_states  = state_inverse == k  # the states for which this minimum applies\n            aGrid         = aNrmMin + self.aXtraGrid # assets grid for this pass\n            EndOfPrdvP_all  = np.zeros((self.StateCount,self.aXtraGrid.size))\n            EndOfPrdvPP_all = np.zeros((self.StateCount,self.aXtraGrid.size))\n            for j in range(self.StateCount):\n                if np.any(np.logical_and(self.possible_transitions[:,j],which_states)): # only consider a future state if one of the relevant states could transition to it\n                    EndOfPrdvP_all[j,:] = self.EndOfPrdvPfunc_list[j](aGrid)\n                    if self.CubicBool: # Add conditional end-of-period (marginal) marginal value to the arrays\n                        EndOfPrdvPP_all[j,:] = self.EndOfPrdvPfunc_list[j].derivative(aGrid)\n            # Weight conditional marginal (marginal) values by transition probs\n            # to get unconditional marginal (marginal) value at each gridpoint.\n            EndOfPrdvP_temp = np.dot(self.MrkvArray,EndOfPrdvP_all)\n            EndOfPrdvP[which_states,:] = EndOfPrdvP_temp[which_states,:] # only take the states for which this asset minimum applies\n            if self.CubicBool:\n                EndOfPrdvPP_temp = np.dot(self.MrkvArray,EndOfPrdvPP_all)\n                EndOfPrdvPP[which_states,:] = EndOfPrdvPP_temp[which_states,:]\n\n        # Store the results as attributes of self, scaling end of period marginal value by survival probability from each current state\n        LivPrb_tiled = np.tile(np.reshape(self.LivPrb,(self.StateCount,1)),(1,self.aXtraGrid.size))\n        self.EndOfPrdvP = LivPrb_tiled*EndOfPrdvP\n        if self.CubicBool:\n            self.EndOfPrdvPP = LivPrb_tiled*EndOfPrdvPP", "response": "Calculates the end - of - period marginal value at each current period state and unconditional on the markov state."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the human wealth and the maximum and minimum MPC for each current period state.", "response": "def calcHumWealthAndBoundingMPCs(self):\n        '''\n        Calculates human wealth and the maximum and minimum MPC for each current\n        period state, then stores them as attributes of self for use by other methods.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        # Upper bound on MPC at lower m-bound\n        WorstIncPrb_array = self.BoroCnstDependency*np.tile(np.reshape(self.WorstIncPrbAll,\n                            (1,self.StateCount)),(self.StateCount,1))\n        temp_array        = self.MrkvArray*WorstIncPrb_array\n        WorstIncPrbNow    = np.sum(temp_array,axis=1) # Probability of getting the \"worst\" income shock and transition from each current state\n        ExMPCmaxNext      = (np.dot(temp_array,self.Rfree_list**(1.0-self.CRRA)*\n                            self.solution_next.MPCmax**(-self.CRRA))/WorstIncPrbNow)**\\\n                            (-1.0/self.CRRA)\n        DiscFacEff_temp   = self.DiscFac*self.LivPrb\n        self.MPCmaxNow    = 1.0/(1.0 + ((DiscFacEff_temp*WorstIncPrbNow)**\n                            (1.0/self.CRRA))/ExMPCmaxNext)\n        self.MPCmaxEff    = self.MPCmaxNow\n        self.MPCmaxEff[self.BoroCnstNat_list < self.mNrmMin_list] = 1.0\n        # State-conditional PDV of human wealth\n        hNrmPlusIncNext   = self.ExIncNextAll + self.solution_next.hNrm\n        self.hNrmNow      = np.dot(self.MrkvArray,(self.PermGroFac_list/self.Rfree_list)*\n                            hNrmPlusIncNext)\n        # Lower bound on MPC as m gets arbitrarily large\n        temp              = (DiscFacEff_temp*np.dot(self.MrkvArray,self.solution_next.MPCmin**\n                            (-self.CRRA)*self.Rfree_list**(1.0-self.CRRA)))**(1.0/self.CRRA)\n        self.MPCminNow    = 1.0/(1.0 + temp)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing an object representing the solution to this period s problem.", "response": "def makeSolution(self,cNrm,mNrm):\n        '''\n        Construct an object representing the solution to this period's problem.\n\n        Parameters\n        ----------\n        cNrm : np.array\n            Array of normalized consumption values for interpolation.  Each row\n            corresponds to a Markov state for this period.\n        mNrm : np.array\n            Array of normalized market resource values for interpolation.  Each\n            row corresponds to a Markov state for this period.\n\n        Returns\n        -------\n        solution : ConsumerSolution\n            The solution to the single period consumption-saving problem. Includes\n            a consumption function cFunc (using cubic or linear splines), a marg-\n            inal value function vPfunc, a minimum acceptable level of normalized\n            market resources mNrmMin, normalized human wealth hNrm, and bounding\n            MPCs MPCmin and MPCmax.  It might also have a value function vFunc\n            and marginal marginal value function vPPfunc.  All of these attributes\n            are lists or arrays, with elements corresponding to the current\n            Markov state.  E.g. solution.cFunc[0] is the consumption function\n            when in the i=0 Markov state this period.\n        '''\n        solution = ConsumerSolution() # An empty solution to which we'll add state-conditional solutions\n        # Calculate the MPC at each market resource gridpoint in each state (if desired)\n        if self.CubicBool:\n            dcda          = self.EndOfPrdvPP/self.uPP(np.array(self.cNrmNow))\n            MPC           = dcda/(dcda+1.0)\n            self.MPC_temp = np.hstack((np.reshape(self.MPCmaxNow,(self.StateCount,1)),MPC))\n            interpfunc    = self.makeCubiccFunc\n        else:\n            interpfunc    = self.makeLinearcFunc\n\n        # Loop through each current period state and add its solution to the overall solution\n        for i in range(self.StateCount):\n            # Set current-period-conditional human wealth and MPC bounds\n            self.hNrmNow_j   = self.hNrmNow[i]\n            self.MPCminNow_j = self.MPCminNow[i]\n            if self.CubicBool:\n                self.MPC_temp_j  = self.MPC_temp[i,:]\n\n            # Construct the consumption function by combining the constrained and unconstrained portions\n            self.cFuncNowCnst = LinearInterp([self.mNrmMin_list[i], self.mNrmMin_list[i]+1.0],\n                                             [0.0,1.0])\n            cFuncNowUnc       = interpfunc(mNrm[i,:],cNrm[i,:])\n            cFuncNow          = LowerEnvelope(cFuncNowUnc,self.cFuncNowCnst)\n\n            # Make the marginal value function and pack up the current-state-conditional solution\n            vPfuncNow     = MargValueFunc(cFuncNow,self.CRRA)\n            solution_cond = ConsumerSolution(cFunc=cFuncNow, vPfunc=vPfuncNow,\n                                             mNrmMin=self.mNrmMinNow)\n            if self.CubicBool: # Add the state-conditional marginal marginal value function (if desired)\n                solution_cond = self.addvPPfunc(solution_cond)\n\n            # Add the current-state-conditional solution to the overall period solution\n            solution.appendSolution(solution_cond)\n\n        # Add the lower bounds of market resources, MPC limits, human resources,\n        # and the value functions to the overall solution\n        solution.mNrmMin = self.mNrmMin_list\n        solution         = self.addMPCandHumanWealth(solution)\n        if self.vFuncBool:\n            vFuncNow = self.makevFunc(solution)\n            solution.vFunc = vFuncNow\n\n        # Return the overall solution to this period\n        return solution"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef makeLinearcFunc(self,mNrm,cNrm):\n        '''\n        Make a linear interpolation to represent the (unconstrained) consumption\n        function conditional on the current period state.\n\n        Parameters\n        ----------\n        mNrm : np.array\n            Array of normalized market resource values for interpolation.\n        cNrm : np.array\n            Array of normalized consumption values for interpolation.\n\n        Returns\n        -------\n        cFuncUnc: an instance of HARK.interpolation.LinearInterp\n        '''\n        cFuncUnc = LinearInterp(mNrm,cNrm,self.MPCminNow_j*self.hNrmNow_j,self.MPCminNow_j)\n        return cFuncUnc", "response": "This method creates a linear interpolation to represent the unconstrained consumption\n        function conditional on the current period state."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes a cubic interpolation to represent the (unconstrained) consumption function conditional on the current period state. Parameters ---------- mNrm : np.array Array of normalized market resource values for interpolation. cNrm : np.array Array of normalized consumption values for interpolation. Returns ------- cFuncUnc: an instance of HARK.interpolation.CubicInterp", "response": "def makeCubiccFunc(self,mNrm,cNrm):\n        '''\n        Make a cubic interpolation to represent the (unconstrained) consumption\n        function conditional on the current period state.\n\n        Parameters\n        ----------\n        mNrm : np.array\n            Array of normalized market resource values for interpolation.\n        cNrm : np.array\n            Array of normalized consumption values for interpolation.\n\n        Returns\n        -------\n        cFuncUnc: an instance of HARK.interpolation.CubicInterp\n        '''\n        cFuncUnc = CubicInterp(mNrm,cNrm,self.MPC_temp_j,self.MPCminNow_j*self.hNrmNow_j,\n                               self.MPCminNow_j)\n        return cFuncUnc"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef makevFunc(self,solution):\n        '''\n        Construct the value function for each current state.\n\n        Parameters\n        ----------\n        solution : ConsumerSolution\n            The solution to the single period consumption-saving problem. Must\n            have a consumption function cFunc (using cubic or linear splines) as\n            a list with elements corresponding to the current Markov state.  E.g.\n            solution.cFunc[0] is the consumption function when in the i=0 Markov\n            state this period.\n\n        Returns\n        -------\n        vFuncNow : [ValueFunc]\n            A list of value functions (defined over normalized market resources\n            m) for each current period Markov state.\n        '''\n        vFuncNow = [] # Initialize an empty list of value functions\n        # Loop over each current period state and construct the value function\n        for i in range(self.StateCount):\n            # Make state-conditional grids of market resources and consumption\n            mNrmMin       = self.mNrmMin_list[i]\n            mGrid         = mNrmMin + self.aXtraGrid\n            cGrid         = solution.cFunc[i](mGrid)\n            aGrid         = mGrid - cGrid\n\n            # Calculate end-of-period value at each gridpoint\n            EndOfPrdv_all   = np.zeros((self.StateCount,self.aXtraGrid.size))\n            for j in range(self.StateCount):\n                if self.possible_transitions[i,j]:\n                    EndOfPrdv_all[j,:] = self.EndOfPrdvFunc_list[j](aGrid)\n            EndOfPrdv     = np.dot(self.MrkvArray[i,:],EndOfPrdv_all)\n\n            # Calculate (normalized) value and marginal value at each gridpoint\n            vNrmNow       = self.u(cGrid) + EndOfPrdv\n            vPnow         = self.uP(cGrid)\n\n            # Make a \"decurved\" value function with the inverse utility function\n            vNvrs        = self.uinv(vNrmNow) # value transformed through inverse utility\n            vNvrsP       = vPnow*self.uinvP(vNrmNow)\n            mNrm_temp    = np.insert(mGrid,0,mNrmMin) # add the lower bound\n            vNvrs        = np.insert(vNvrs,0,0.0)\n            vNvrsP       = np.insert(vNvrsP,0,self.MPCmaxEff[i]**(-self.CRRA/(1.0-self.CRRA)))\n            MPCminNvrs   = self.MPCminNow[i]**(-self.CRRA/(1.0-self.CRRA))\n            vNvrsFunc_i  = CubicInterp(mNrm_temp,vNvrs,vNvrsP,MPCminNvrs*self.hNrmNow[i],MPCminNvrs)\n\n            # \"Recurve\" the decurved value function and add it to the list\n            vFunc_i     = ValueFunc(vNvrsFunc_i,self.CRRA)\n            vFuncNow.append(vFunc_i)\n        return vFuncNow", "response": "Constructs the value function for each current period consumption - saving problem."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef checkMarkovInputs(self):\n        '''\n        Many parameters used by MarkovConsumerType are arrays.  Make sure those arrays are the\n        right shape.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        StateCount = self.MrkvArray[0].shape[0]\n\n        # Check that arrays are the right shape\n        assert self.Rfree.shape      == (StateCount,),'Rfree not the right shape!'\n\n        # Check that arrays in lists are the right shape\n        for MrkvArray_t in self.MrkvArray:\n            assert MrkvArray_t.shape  == (StateCount,StateCount),'MrkvArray not the right shape!'\n        for LivPrb_t in self.LivPrb:\n            assert LivPrb_t.shape == (StateCount,),'Array in LivPrb is not the right shape!'\n        for PermGroFac_t in self.LivPrb:\n            assert PermGroFac_t.shape == (StateCount,),'Array in PermGroFac is not the right shape!'\n\n        # Now check the income distribution.\n        # Note IncomeDstn is (potentially) time-varying, so it is in time_vary.\n        # Therefore it is a list, and each element of that list responds to the income distribution\n        # at a particular point in time.  Each income distribution at a point in time should itself\n        # be a list, with each element corresponding to the income distribution\n        # conditional on a particular Markov state.\n        for IncomeDstn_t in self.IncomeDstn:\n            assert len(IncomeDstn_t) == StateCount,'List in IncomeDstn is not the right length!'", "response": "Check that the Markov consumer type and Markov state inputs are the right shape."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef updateSolutionTerminal(self):\n        '''\n        Update the terminal period solution.  This method should be run when a\n        new AgentType is created or when CRRA changes.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        IndShockConsumerType.updateSolutionTerminal(self)\n\n        # Make replicated terminal period solution: consume all resources, no human wealth, minimum m is 0\n        StateCount = self.MrkvArray[0].shape[0]\n        self.solution_terminal.cFunc   = StateCount*[self.cFunc_terminal_]\n        self.solution_terminal.vFunc   = StateCount*[self.solution_terminal.vFunc]\n        self.solution_terminal.vPfunc  = StateCount*[self.solution_terminal.vPfunc]\n        self.solution_terminal.vPPfunc = StateCount*[self.solution_terminal.vPPfunc]\n        self.solution_terminal.mNrmMin = np.zeros(StateCount)\n        self.solution_terminal.hRto    = np.zeros(StateCount)\n        self.solution_terminal.MPCmax  = np.ones(StateCount)\n        self.solution_terminal.MPCmin  = np.ones(StateCount)", "response": "This method is called when the new AgentType is created or the CRRA changes. This method is called when the new AgentType is created or the CRRA changes. This method is called when the new AgentType is created or the CRRA changes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef simBirth(self,which_agents):\n        '''\n        Makes new Markov consumer by drawing initial normalized assets, permanent income levels, and\n        discrete states. Calls IndShockConsumerType.simBirth, then draws from initial Markov distribution.\n\n        Parameters\n        ----------\n        which_agents : np.array(Bool)\n            Boolean array of size self.AgentCount indicating which agents should be \"born\".\n\n        Returns\n        -------\n        None\n        '''\n        IndShockConsumerType.simBirth(self,which_agents) # Get initial assets and permanent income\n        if not self.global_markov:  #Markov state is not changed if it is set at the global level\n            N = np.sum(which_agents)\n            base_draws = drawUniform(N,seed=self.RNG.randint(0,2**31-1))\n            Cutoffs = np.cumsum(np.array(self.MrkvPrbsInit))\n            self.MrkvNow[which_agents] = np.searchsorted(Cutoffs,base_draws).astype(int)", "response": "Makes new Markov consumer by drawing initial normalized assets permanent income levels and discrete states. Calls IndShockConsumerType. simBirth."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getShocks(self):\n        '''\n        Gets new Markov states and permanent and transitory income shocks for this period.  Samples\n        from IncomeDstn for each period-state in the cycle.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        # Get new Markov states for each agent\n        if self.global_markov:\n            base_draws = np.ones(self.AgentCount)*drawUniform(1,seed=self.RNG.randint(0,2**31-1))\n        else:\n            base_draws = self.RNG.permutation(np.arange(self.AgentCount,dtype=float)/self.AgentCount + 1.0/(2*self.AgentCount))\n        newborn = self.t_age == 0 # Don't change Markov state for those who were just born (unless global_markov)\n        MrkvPrev = self.MrkvNow\n        MrkvNow = np.zeros(self.AgentCount,dtype=int)\n        for t in range(self.T_cycle):\n            Cutoffs = np.cumsum(self.MrkvArray[t],axis=1)\n            for j in range(self.MrkvArray[t].shape[0]):\n                these = np.logical_and(self.t_cycle == t,MrkvPrev == j)\n                MrkvNow[these] = np.searchsorted(Cutoffs[j,:],base_draws[these]).astype(int)\n        if not self.global_markov:\n                MrkvNow[newborn] = MrkvPrev[newborn]\n        self.MrkvNow = MrkvNow.astype(int)\n\n        # Now get income shocks for each consumer, by cycle-time and discrete state\n        PermShkNow = np.zeros(self.AgentCount) # Initialize shock arrays\n        TranShkNow = np.zeros(self.AgentCount)\n        for t in range(self.T_cycle):\n            for j in range(self.MrkvArray[t].shape[0]):\n                these = np.logical_and(t == self.t_cycle, j == MrkvNow)\n                N = np.sum(these)\n                if N > 0:\n                    IncomeDstnNow    = self.IncomeDstn[t-1][j] # set current income distribution\n                    PermGroFacNow    = self.PermGroFac[t-1][j] # and permanent growth factor\n                    Indices          = np.arange(IncomeDstnNow[0].size) # just a list of integers\n                    # Get random draws of income shocks from the discrete distribution\n                    EventDraws       = drawDiscrete(N,X=Indices,P=IncomeDstnNow[0],exact_match=False,seed=self.RNG.randint(0,2**31-1))\n                    PermShkNow[these] = IncomeDstnNow[1][EventDraws]*PermGroFacNow # permanent \"shock\" includes expected growth\n                    TranShkNow[these] = IncomeDstnNow[2][EventDraws]\n        newborn = self.t_age == 0\n        PermShkNow[newborn] = 1.0\n        TranShkNow[newborn] = 1.0\n        self.PermShkNow = PermShkNow\n        self.TranShkNow = TranShkNow", "response": "Gets new Markov states permanent and transitory income shocks for each consumer in the cycle."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef readShocks(self):\n        '''\n        A slight modification of AgentType.readShocks that makes sure that MrkvNow is int, not float.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        IndShockConsumerType.readShocks(self)\n        self.MrkvNow = self.MrkvNow.astype(int)", "response": "A slight modification of AgentType. readShocks that makes sure that MrkvNow is int not float."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getControls(self):\n        '''\n        Calculates consumption for each consumer of this type using the consumption functions.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        cNrmNow = np.zeros(self.AgentCount) + np.nan\n        for t in range(self.T_cycle):\n            for j in range(self.MrkvArray[t].shape[0]):\n                these = np.logical_and(t == self.t_cycle, j == self.MrkvNow)\n                cNrmNow[these] = self.solution[t].cFunc[j](self.mNrmNow[these])\n        self.cNrmNow = cNrmNow\n        return None", "response": "Calculates consumption for each consumer of this type using the consumption functions. Returns None if there are no consumers in this type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef solveConsRepAgent(solution_next,DiscFac,CRRA,IncomeDstn,CapShare,DeprFac,PermGroFac,aXtraGrid):\n    '''\n    Solve one period of the simple representative agent consumption-saving model.\n\n    Parameters\n    ----------\n    solution_next : ConsumerSolution\n        Solution to the next period's problem (i.e. previous iteration).\n    DiscFac : float\n        Intertemporal discount factor for future utility.\n    CRRA : float\n        Coefficient of relative risk aversion.\n    IncomeDstn : [np.array]\n        A list containing three arrays of floats, representing a discrete\n        approximation to the income process between the period being solved\n        and the one immediately following (in solution_next). Order: event\n        probabilities, permanent shocks, transitory shocks.\n    CapShare : float\n        Capital's share of income in Cobb-Douglas production function.\n    DeprFac : float\n        Depreciation rate of capital.\n    PermGroFac : float\n        Expected permanent income growth factor at the end of this period.\n    aXtraGrid : np.array\n        Array of \"extra\" end-of-period asset values-- assets above the\n        absolute minimum acceptable level.  In this model, the minimum acceptable\n        level is always zero.\n\n    Returns\n    -------\n    solution_now : ConsumerSolution\n        Solution to this period's problem (new iteration).\n    '''\n    # Unpack next period's solution and the income distribution\n    vPfuncNext      = solution_next.vPfunc\n    ShkPrbsNext     = IncomeDstn[0]\n    PermShkValsNext = IncomeDstn[1]\n    TranShkValsNext = IncomeDstn[2]\n\n    # Make tiled versions of end-of-period assets, shocks, and probabilities\n    aNrmNow     = aXtraGrid\n    aNrmCount   = aNrmNow.size\n    ShkCount    = ShkPrbsNext.size\n    aNrm_tiled  = np.tile(np.reshape(aNrmNow,(aNrmCount,1)),(1,ShkCount))\n\n    # Tile arrays of the income shocks and put them into useful shapes\n    PermShkVals_tiled = np.tile(np.reshape(PermShkValsNext,(1,ShkCount)),(aNrmCount,1))\n    TranShkVals_tiled = np.tile(np.reshape(TranShkValsNext,(1,ShkCount)),(aNrmCount,1))\n    ShkPrbs_tiled     = np.tile(np.reshape(ShkPrbsNext,(1,ShkCount)),(aNrmCount,1))\n\n    # Calculate next period's capital-to-permanent-labor ratio under each combination\n    # of end-of-period assets and shock realization\n    kNrmNext = aNrm_tiled/(PermGroFac*PermShkVals_tiled)\n\n    # Calculate next period's market resources\n    KtoLnext  = kNrmNext/TranShkVals_tiled\n    RfreeNext = 1. - DeprFac + CapShare*KtoLnext**(CapShare-1.)\n    wRteNext  = (1.-CapShare)*KtoLnext**CapShare\n    mNrmNext  = RfreeNext*kNrmNext + wRteNext*TranShkVals_tiled\n\n    # Calculate end-of-period marginal value of assets for the RA\n    vPnext = vPfuncNext(mNrmNext)\n    EndOfPrdvP = DiscFac*np.sum(RfreeNext*(PermGroFac*PermShkVals_tiled)**(-CRRA)*vPnext*ShkPrbs_tiled,axis=1)\n\n    # Invert the first order condition to get consumption, then find endogenous gridpoints\n    cNrmNow = EndOfPrdvP**(-1./CRRA)\n    mNrmNow = aNrmNow + cNrmNow\n\n    # Construct the consumption function and the marginal value function\n    cFuncNow  = LinearInterp(np.insert(mNrmNow,0,0.0),np.insert(cNrmNow,0,0.0))\n    vPfuncNow = MargValueFunc(cFuncNow,CRRA)\n\n    # Construct and return the solution for this period\n    solution_now = ConsumerSolution(cFunc=cFuncNow,vPfunc=vPfuncNow)\n    return solution_now", "response": "Solve one period of the simple representative agent consumption - saving model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsolve one period of the simple representative agent consumption - saving model. This version supports a discrete Markov process.", "response": "def solveConsRepAgentMarkov(solution_next,MrkvArray,DiscFac,CRRA,IncomeDstn,CapShare,DeprFac,PermGroFac,aXtraGrid):\n    '''\n    Solve one period of the simple representative agent consumption-saving model.\n    This version supports a discrete Markov process.\n\n    Parameters\n    ----------\n    solution_next : ConsumerSolution\n        Solution to the next period's problem (i.e. previous iteration).\n    MrkvArray : np.array\n        Markov transition array between this period and next period.\n    DiscFac : float\n        Intertemporal discount factor for future utility.\n    CRRA : float\n        Coefficient of relative risk aversion.\n    IncomeDstn : [[np.array]]\n        A list of lists containing three arrays of floats, representing a discrete\n        approximation to the income process between the period being solved\n        and the one immediately following (in solution_next). Order: event\n        probabilities, permanent shocks, transitory shocks.\n    CapShare : float\n        Capital's share of income in Cobb-Douglas production function.\n    DeprFac : float\n        Depreciation rate of capital.\n    PermGroFac : [float]\n        Expected permanent income growth factor for each state we could be in\n        next period.\n    aXtraGrid : np.array\n        Array of \"extra\" end-of-period asset values-- assets above the\n        absolute minimum acceptable level.  In this model, the minimum acceptable\n        level is always zero.\n\n    Returns\n    -------\n    solution_now : ConsumerSolution\n        Solution to this period's problem (new iteration).\n    '''\n    # Define basic objects\n    StateCount  = MrkvArray.shape[0]\n    aNrmNow     = aXtraGrid\n    aNrmCount   = aNrmNow.size\n    EndOfPrdvP_cond = np.zeros((StateCount,aNrmCount)) + np.nan\n\n    # Loop over *next period* states, calculating conditional EndOfPrdvP\n    for j in range(StateCount):\n        # Define next-period-state conditional objects\n        vPfuncNext  = solution_next.vPfunc[j]\n        ShkPrbsNext     = IncomeDstn[j][0]\n        PermShkValsNext = IncomeDstn[j][1]\n        TranShkValsNext = IncomeDstn[j][2]\n\n        # Make tiled versions of end-of-period assets, shocks, and probabilities\n        ShkCount    = ShkPrbsNext.size\n        aNrm_tiled  = np.tile(np.reshape(aNrmNow,(aNrmCount,1)),(1,ShkCount))\n\n        # Tile arrays of the income shocks and put them into useful shapes\n        PermShkVals_tiled = np.tile(np.reshape(PermShkValsNext,(1,ShkCount)),(aNrmCount,1))\n        TranShkVals_tiled = np.tile(np.reshape(TranShkValsNext,(1,ShkCount)),(aNrmCount,1))\n        ShkPrbs_tiled     = np.tile(np.reshape(ShkPrbsNext,(1,ShkCount)),(aNrmCount,1))\n\n        # Calculate next period's capital-to-permanent-labor ratio under each combination\n        # of end-of-period assets and shock realization\n        kNrmNext = aNrm_tiled/(PermGroFac[j]*PermShkVals_tiled)\n\n        # Calculate next period's market resources\n        KtoLnext  = kNrmNext/TranShkVals_tiled\n        RfreeNext = 1. - DeprFac + CapShare*KtoLnext**(CapShare-1.)\n        wRteNext  = (1.-CapShare)*KtoLnext**CapShare\n        mNrmNext  = RfreeNext*kNrmNext + wRteNext*TranShkVals_tiled\n\n        # Calculate end-of-period marginal value of assets for the RA\n        vPnext = vPfuncNext(mNrmNext)\n        EndOfPrdvP_cond[j,:] = DiscFac*np.sum(RfreeNext*(PermGroFac[j]*PermShkVals_tiled)**(-CRRA)*vPnext*ShkPrbs_tiled,axis=1)\n\n    # Apply the Markov transition matrix to get unconditional end-of-period marginal value\n    EndOfPrdvP = np.dot(MrkvArray,EndOfPrdvP_cond)\n\n    # Construct the consumption function and marginal value function for each discrete state\n    cFuncNow_list = []\n    vPfuncNow_list = []\n    for i in range(StateCount):\n        # Invert the first order condition to get consumption, then find endogenous gridpoints\n        cNrmNow = EndOfPrdvP[i,:]**(-1./CRRA)\n        mNrmNow = aNrmNow + cNrmNow\n\n        # Construct the consumption function and the marginal value function\n        cFuncNow_list.append(LinearInterp(np.insert(mNrmNow,0,0.0),np.insert(cNrmNow,0,0.0)))\n        vPfuncNow_list.append(MargValueFunc(cFuncNow_list[-1],CRRA))\n\n    # Construct and return the solution for this period\n    solution_now = ConsumerSolution(cFunc=cFuncNow_list,vPfunc=vPfuncNow_list)\n    return solution_now"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates updated values of normalized market resources and permanent income level and Tran income level.", "response": "def getStates(self):\n        '''\n        Calculates updated values of normalized market resources and permanent income level.\n        Uses pLvlNow, aNrmNow, PermShkNow, TranShkNow.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        pLvlPrev = self.pLvlNow\n        aNrmPrev = self.aNrmNow\n\n        # Calculate new states: normalized market resources and permanent income level\n        self.pLvlNow = pLvlPrev*self.PermShkNow # Same as in IndShockConsType\n        self.kNrmNow = aNrmPrev/self.PermShkNow\n        self.yNrmNow = self.kNrmNow**self.CapShare*self.TranShkNow**(1.-self.CapShare)\n        self.Rfree = 1. + self.CapShare*self.kNrmNow**(self.CapShare-1.)*self.TranShkNow**(1.-self.CapShare) - self.DeprFac\n        self.wRte  = (1.-self.CapShare)*self.kNrmNow**self.CapShare*self.TranShkNow**(-self.CapShare)\n        self.mNrmNow = self.Rfree*self.kNrmNow + self.wRte*self.TranShkNow"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef updateSolutionTerminal(self):\n        '''\n        Update the terminal period solution.  This method should be run when a\n        new AgentType is created or when CRRA changes.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        RepAgentConsumerType.updateSolutionTerminal(self)\n\n        # Make replicated terminal period solution\n        StateCount = self.MrkvArray.shape[0]\n        self.solution_terminal.cFunc   = StateCount*[self.cFunc_terminal_]\n        self.solution_terminal.vPfunc  = StateCount*[self.solution_terminal.vPfunc]\n        self.solution_terminal.mNrmMin = StateCount*[self.solution_terminal.mNrmMin]", "response": "Update the terminal period solution. This method is called when the new AgentType is created and CRRA changes are made."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndrawing a new Markov state and income shocks for the representative agent.", "response": "def getShocks(self):\n        '''\n        Draws a new Markov state and income shocks for the representative agent.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        cutoffs = np.cumsum(self.MrkvArray[self.MrkvNow,:])\n        MrkvDraw = drawUniform(N=1,seed=self.RNG.randint(0,2**31-1))\n        self.MrkvNow = np.searchsorted(cutoffs,MrkvDraw)\n\n        t = self.t_cycle[0]\n        i = self.MrkvNow[0]\n        IncomeDstnNow    = self.IncomeDstn[t-1][i] # set current income distribution\n        PermGroFacNow    = self.PermGroFac[t-1][i] # and permanent growth factor\n        Indices          = np.arange(IncomeDstnNow[0].size) # just a list of integers\n        # Get random draws of income shocks from the discrete distribution\n        EventDraw        = drawDiscrete(N=1,X=Indices,P=IncomeDstnNow[0],exact_match=False,seed=self.RNG.randint(0,2**31-1))\n        PermShkNow = IncomeDstnNow[1][EventDraw]*PermGroFacNow # permanent \"shock\" includes expected growth\n        TranShkNow = IncomeDstnNow[2][EventDraw]\n        self.PermShkNow = np.array(PermShkNow)\n        self.TranShkNow = np.array(TranShkNow)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the consumption for the representative agent using the consumption functions.", "response": "def getControls(self):\n        '''\n        Calculates consumption for the representative agent using the consumption functions.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        t = self.t_cycle[0]\n        i = self.MrkvNow[0]\n        self.cNrmNow = self.solution[t].cFunc[i](self.mNrmNow)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsolving one period of a consumption-saving problem with idiosyncratic and aggregate shocks (transitory and permanent). This is a basic solver that can't handle cubic splines, nor can it calculate a value function. Parameters ---------- solution_next : ConsumerSolution The solution to the succeeding one period problem. IncomeDstn : [np.array] A list containing five arrays of floats, representing a discrete approximation to the income process between the period being solved and the one immediately following (in solution_next). Order: event probabilities, idisyncratic permanent shocks, idiosyncratic transitory shocks, aggregate permanent shocks, aggregate transitory shocks. LivPrb : float Survival probability; likelihood of being alive at the beginning of the succeeding period. DiscFac : float Intertemporal discount factor for future utility. CRRA : float Coefficient of relative risk aversion. PermGroFac : float Expected permanent income growth factor at the end of this period. PermGroFacAgg : float Expected aggregate productivity growth factor. aXtraGrid : np.array Array of \"extra\" end-of-period asset values-- assets above the absolute minimum acceptable level. BoroCnstArt : float Artificial borrowing constraint; minimum allowable end-of-period asset-to- permanent-income ratio. Unlike other models, this *can't* be None. Mgrid : np.array A grid of aggregate market resourses to permanent income in the economy. AFunc : function Aggregate savings as a function of aggregate market resources. Rfunc : function The net interest factor on assets as a function of capital ratio k. wFunc : function The wage rate for labor as a function of capital-to-labor ratio k. DeprFac : float Capital Depreciation Rate Returns ------- solution_now : ConsumerSolution The solution to the single period consumption-saving problem. Includes a consumption function cFunc (linear interpolation over linear interpola- tions) and marginal value function vPfunc.", "response": "def solveConsAggShock(solution_next,IncomeDstn,LivPrb,DiscFac,CRRA,PermGroFac,\n                      PermGroFacAgg,aXtraGrid,BoroCnstArt,Mgrid,AFunc,Rfunc,wFunc,DeprFac):\n    '''\n    Solve one period of a consumption-saving problem with idiosyncratic and\n    aggregate shocks (transitory and permanent).  This is a basic solver that\n    can't handle cubic splines, nor can it calculate a value function.\n\n    Parameters\n    ----------\n    solution_next : ConsumerSolution\n        The solution to the succeeding one period problem.\n    IncomeDstn : [np.array]\n        A list containing five arrays of floats, representing a discrete\n        approximation to the income process between the period being solved\n        and the one immediately following (in solution_next). Order: event\n        probabilities, idisyncratic permanent shocks, idiosyncratic transitory\n        shocks, aggregate permanent shocks, aggregate transitory shocks.\n    LivPrb : float\n        Survival probability; likelihood of being alive at the beginning of\n        the succeeding period.\n    DiscFac : float\n        Intertemporal discount factor for future utility.\n    CRRA : float\n        Coefficient of relative risk aversion.\n    PermGroFac : float\n        Expected permanent income growth factor at the end of this period.\n    PermGroFacAgg : float\n        Expected aggregate productivity growth factor.\n    aXtraGrid : np.array\n        Array of \"extra\" end-of-period asset values-- assets above the\n        absolute minimum acceptable level.\n    BoroCnstArt : float\n        Artificial borrowing constraint; minimum allowable end-of-period asset-to-\n        permanent-income ratio.  Unlike other models, this *can't* be None.\n    Mgrid : np.array\n        A grid of aggregate market resourses to permanent income in the economy.\n    AFunc : function\n        Aggregate savings as a function of aggregate market resources.\n    Rfunc : function\n        The net interest factor on assets as a function of capital ratio k.\n    wFunc : function\n        The wage rate for labor as a function of capital-to-labor ratio k.\n    DeprFac : float\n        Capital Depreciation Rate\n\n    Returns\n    -------\n    solution_now : ConsumerSolution\n        The solution to the single period consumption-saving problem.  Includes\n        a consumption function cFunc (linear interpolation over linear interpola-\n        tions) and marginal value function vPfunc.\n    '''\n    # Unpack next period's solution\n    vPfuncNext = solution_next.vPfunc\n    mNrmMinNext = solution_next.mNrmMin\n\n    # Unpack the income shocks\n    ShkPrbsNext  = IncomeDstn[0]\n    PermShkValsNext = IncomeDstn[1]\n    TranShkValsNext = IncomeDstn[2]\n    PermShkAggValsNext = IncomeDstn[3]\n    TranShkAggValsNext = IncomeDstn[4]\n    ShkCount = ShkPrbsNext.size\n\n    # Make the grid of end-of-period asset values, and a tiled version\n    aNrmNow = aXtraGrid\n    aCount = aNrmNow.size\n    Mcount = Mgrid.size\n    aXtra_tiled = np.tile(np.reshape(aNrmNow,(1,aCount,1)),(Mcount,1,ShkCount))\n\n    # Make tiled versions of the income shocks\n    # Dimension order: Mnow, aNow, Shk\n    ShkPrbsNext_tiled = np.tile(np.reshape(ShkPrbsNext,(1,1,ShkCount)),(Mcount,aCount,1))\n    PermShkValsNext_tiled = np.tile(np.reshape(PermShkValsNext,(1,1,ShkCount)),(Mcount,aCount,1))\n    TranShkValsNext_tiled = np.tile(np.reshape(TranShkValsNext,(1,1,ShkCount)),(Mcount,aCount,1))\n    PermShkAggValsNext_tiled = np.tile(np.reshape(PermShkAggValsNext,(1,1,ShkCount)),(Mcount,aCount,1))\n    TranShkAggValsNext_tiled = np.tile(np.reshape(TranShkAggValsNext,(1,1,ShkCount)),(Mcount,aCount,1))\n\n    # Calculate returns to capital and labor in the next period\n    AaggNow_tiled = np.tile(np.reshape(AFunc(Mgrid),(Mcount,1,1)),(1,aCount,ShkCount))\n    kNext_array = AaggNow_tiled/(PermGroFacAgg*PermShkAggValsNext_tiled) # Next period's aggregate capital to labor ratio\n    kNextEff_array = kNext_array/TranShkAggValsNext_tiled # Same thing, but account for *transitory* shock\n    R_array = Rfunc(kNextEff_array) # Interest factor on aggregate assets\n    Reff_array = R_array/LivPrb # Effective interest factor on individual assets *for survivors*\n    wEff_array = wFunc(kNextEff_array)*TranShkAggValsNext_tiled # Effective wage rate (accounts for labor supply)\n    PermShkTotal_array = PermGroFac*PermGroFacAgg*PermShkValsNext_tiled*PermShkAggValsNext_tiled # total / combined permanent shock\n    Mnext_array = kNext_array*R_array + wEff_array # next period's aggregate market resources\n\n    # Find the natural borrowing constraint for each value of M in the Mgrid.\n    # There is likely a faster way to do this, but someone needs to do the math:\n    # is aNrmMin determined by getting the worst shock of all four types?\n    aNrmMin_candidates = PermGroFac*PermGroFacAgg*PermShkValsNext_tiled[:,0,:]*PermShkAggValsNext_tiled[:,0,:]/Reff_array[:,0,:]*\\\n                         (mNrmMinNext(Mnext_array[:,0,:]) - wEff_array[:,0,:]*TranShkValsNext_tiled[:,0,:])\n    aNrmMin_vec = np.max(aNrmMin_candidates,axis=1)\n    BoroCnstNat_vec = aNrmMin_vec\n    aNrmMin_tiled = np.tile(np.reshape(aNrmMin_vec,(Mcount,1,1)),(1,aCount,ShkCount))\n    aNrmNow_tiled = aNrmMin_tiled + aXtra_tiled\n\n    # Calculate market resources next period (and a constant array of capital-to-labor ratio)\n    mNrmNext_array = Reff_array*aNrmNow_tiled/PermShkTotal_array + TranShkValsNext_tiled*wEff_array\n\n    # Find marginal value next period at every income shock realization and every aggregate market resource gridpoint\n    vPnext_array = Reff_array*PermShkTotal_array**(-CRRA)*vPfuncNext(mNrmNext_array,Mnext_array)\n\n    # Calculate expectated marginal value at the end of the period at every asset gridpoint\n    EndOfPrdvP = DiscFac*LivPrb*np.sum(vPnext_array*ShkPrbsNext_tiled,axis=2)\n\n    # Calculate optimal consumption from each asset gridpoint\n    cNrmNow = EndOfPrdvP**(-1.0/CRRA)\n    mNrmNow = aNrmNow_tiled[:,:,0] + cNrmNow\n\n    # Loop through the values in Mgrid and make a linear consumption function for each\n    cFuncBaseByM_list = []\n    for j in range(Mcount):\n        c_temp = np.insert(cNrmNow[j,:],0,0.0) # Add point at bottom\n        m_temp = np.insert(mNrmNow[j,:] - BoroCnstNat_vec[j],0,0.0)\n        cFuncBaseByM_list.append(LinearInterp(m_temp,c_temp))\n        # Add the M-specific consumption function to the list\n\n    # Construct the overall unconstrained consumption function by combining the M-specific functions\n    BoroCnstNat = LinearInterp(np.insert(Mgrid,0,0.0),np.insert(BoroCnstNat_vec,0,0.0))\n    cFuncBase = LinearInterpOnInterp1D(cFuncBaseByM_list,Mgrid)\n    cFuncUnc  = VariableLowerBoundFunc2D(cFuncBase,BoroCnstNat)\n\n    # Make the constrained consumption function and combine it with the unconstrained component\n    cFuncCnst = BilinearInterp(np.array([[0.0,0.0],[1.0,1.0]]),\n                                           np.array([BoroCnstArt,BoroCnstArt+1.0]),np.array([0.0,1.0]))\n    cFuncNow = LowerEnvelope2D(cFuncUnc,cFuncCnst)\n\n    # Make the minimum m function as the greater of the natural and artificial constraints\n    mNrmMinNow = UpperEnvelope(BoroCnstNat,ConstantFunction(BoroCnstArt))\n\n    # Construct the marginal value function using the envelope condition\n    vPfuncNow = MargValueFunc2D(cFuncNow,CRRA)\n\n    # Pack up and return the solution\n    solution_now = ConsumerSolution(cFunc=cFuncNow,vPfunc=vPfuncNow,mNrmMin=mNrmMinNow)\n    return solution_now"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsolve one period of a consumption-saving problem with idiosyncratic and aggregate shocks (transitory and permanent). Moreover, the macroeconomic state follows a Markov process that determines the income distribution and aggregate permanent growth factor. This is a basic solver that can't handle cubic splines, nor can it calculate a value function. Parameters ---------- solution_next : ConsumerSolution The solution to the succeeding one period problem. IncomeDstn : [[np.array]] A list of lists, each containing five arrays of floats, representing a discrete approximation to the income process between the period being solved and the one immediately following (in solution_next). Order: event probabilities, idisyncratic permanent shocks, idiosyncratic transitory shocks, aggregate permanent shocks, aggregate transitory shocks. LivPrb : float Survival probability; likelihood of being alive at the beginning of the succeeding period. DiscFac : float Intertemporal discount factor for future utility. CRRA : float Coefficient of relative risk aversion. MrkvArray : np.array Markov transition matrix between discrete macroeconomic states. MrkvArray[i,j] is probability of being in state j next period conditional on being in state i this period. PermGroFac : float Expected permanent income growth factor at the end of this period, for the *individual*'s productivity. PermGroFacAgg : [float] Expected aggregate productivity growth in each Markov macro state. aXtraGrid : np.array Array of \"extra\" end-of-period asset values-- assets above the absolute minimum acceptable level. BoroCnstArt : float Artificial borrowing constraint; minimum allowable end-of-period asset-to- permanent-income ratio. Unlike other models, this *can't* be None. Mgrid : np.array A grid of aggregate market resourses to permanent income in the economy. AFunc : [function] Aggregate savings as a function of aggregate market resources, for each Markov macro state. Rfunc : function The net interest factor on assets as a function of capital ratio k. wFunc : function The wage rate for labor as a function of capital-to-labor ratio k. DeprFac : float Capital Depreciation Rate Returns ------- solution_now : ConsumerSolution The solution to the single period consumption-saving problem. Includes a consumption function cFunc (linear interpolation over linear interpola- tions) and marginal value function vPfunc.", "response": "def solveConsAggMarkov(solution_next,IncomeDstn,LivPrb,DiscFac,CRRA,MrkvArray,\n                       PermGroFac,PermGroFacAgg,aXtraGrid,BoroCnstArt,Mgrid,\n                       AFunc,Rfunc,wFunc,DeprFac):\n    '''\n    Solve one period of a consumption-saving problem with idiosyncratic and\n    aggregate shocks (transitory and permanent).  Moreover, the macroeconomic\n    state follows a Markov process that determines the income distribution and\n    aggregate permanent growth factor. This is a basic solver that can't handle\n    cubic splines, nor can it calculate a value function.\n\n    Parameters\n    ----------\n    solution_next : ConsumerSolution\n        The solution to the succeeding one period problem.\n    IncomeDstn : [[np.array]]\n        A list of lists, each containing five arrays of floats, representing a\n        discrete approximation to the income process between the period being\n        solved and the one immediately following (in solution_next). Order: event\n        probabilities, idisyncratic permanent shocks, idiosyncratic transitory\n        shocks, aggregate permanent shocks, aggregate transitory shocks.\n    LivPrb : float\n        Survival probability; likelihood of being alive at the beginning of\n        the succeeding period.\n    DiscFac : float\n        Intertemporal discount factor for future utility.\n    CRRA : float\n        Coefficient of relative risk aversion.\n    MrkvArray : np.array\n        Markov transition matrix between discrete macroeconomic states.\n        MrkvArray[i,j] is probability of being in state j next period conditional\n        on being in state i this period.\n    PermGroFac : float\n        Expected permanent income growth factor at the end of this period,\n        for the *individual*'s productivity.\n    PermGroFacAgg : [float]\n        Expected aggregate productivity growth in each Markov macro state.\n    aXtraGrid : np.array\n        Array of \"extra\" end-of-period asset values-- assets above the\n        absolute minimum acceptable level.\n    BoroCnstArt : float\n        Artificial borrowing constraint; minimum allowable end-of-period asset-to-\n        permanent-income ratio.  Unlike other models, this *can't* be None.\n    Mgrid : np.array\n        A grid of aggregate market resourses to permanent income in the economy.\n    AFunc : [function]\n        Aggregate savings as a function of aggregate market resources, for each\n        Markov macro state.\n    Rfunc : function\n        The net interest factor on assets as a function of capital ratio k.\n    wFunc : function\n        The wage rate for labor as a function of capital-to-labor ratio k.\n    DeprFac : float\n        Capital Depreciation Rate\n\n    Returns\n    -------\n    solution_now : ConsumerSolution\n        The solution to the single period consumption-saving problem.  Includes\n        a consumption function cFunc (linear interpolation over linear interpola-\n        tions) and marginal value function vPfunc.\n    '''\n    # Get sizes of grids\n    aCount = aXtraGrid.size\n    Mcount = Mgrid.size\n    StateCount = MrkvArray.shape[0]\n\n    # Loop through next period's states, assuming we reach each one at a time.\n    # Construct EndOfPrdvP_cond functions for each state.\n    EndOfPrdvPfunc_cond = []\n    BoroCnstNat_cond = []\n    for j in range(StateCount):\n        # Unpack next period's solution\n        vPfuncNext = solution_next.vPfunc[j]\n        mNrmMinNext = solution_next.mNrmMin[j]\n\n        # Unpack the income shocks\n        ShkPrbsNext  = IncomeDstn[j][0]\n        PermShkValsNext = IncomeDstn[j][1]\n        TranShkValsNext = IncomeDstn[j][2]\n        PermShkAggValsNext = IncomeDstn[j][3]\n        TranShkAggValsNext = IncomeDstn[j][4]\n        ShkCount = ShkPrbsNext.size\n        aXtra_tiled = np.tile(np.reshape(aXtraGrid,(1,aCount,1)),(Mcount,1,ShkCount))\n\n        # Make tiled versions of the income shocks\n        # Dimension order: Mnow, aNow, Shk\n        ShkPrbsNext_tiled = np.tile(np.reshape(ShkPrbsNext,(1,1,ShkCount)),(Mcount,aCount,1))\n        PermShkValsNext_tiled = np.tile(np.reshape(PermShkValsNext,(1,1,ShkCount)),(Mcount,aCount,1))\n        TranShkValsNext_tiled = np.tile(np.reshape(TranShkValsNext,(1,1,ShkCount)),(Mcount,aCount,1))\n        PermShkAggValsNext_tiled = np.tile(np.reshape(PermShkAggValsNext,(1,1,ShkCount)),(Mcount,aCount,1))\n        TranShkAggValsNext_tiled = np.tile(np.reshape(TranShkAggValsNext,(1,1,ShkCount)),(Mcount,aCount,1))\n\n        # Make a tiled grid of end-of-period aggregate assets.  These lines use\n        # next prd state j's aggregate saving rule to get a relevant set of Aagg,\n        # which will be used to make an interpolated EndOfPrdvP_cond function.\n        # After constructing these functions, we will use the aggregate saving\n        # rule for *current* state i to get values of Aagg at which to evaluate\n        # these conditional marginal value functions.  In the strange, maybe even\n        # impossible case where the aggregate saving rules differ wildly across\n        # macro states *and* there is \"anti-persistence\", so that the macro state\n        # is very likely to change each period, then this procedure will lead to\n        # an inaccurate solution because the grid of Aagg values on which the\n        # conditional marginal value functions are constructed is not relevant\n        # to the values at which it will actually be evaluated.\n        AaggGrid = AFunc[j](Mgrid)\n        AaggNow_tiled = np.tile(np.reshape(AaggGrid,(Mcount,1,1)),(1,aCount,ShkCount))\n\n        # Calculate returns to capital and labor in the next period\n        kNext_array = AaggNow_tiled/(PermGroFacAgg[j]*PermShkAggValsNext_tiled) # Next period's aggregate capital to labor ratio\n        kNextEff_array = kNext_array/TranShkAggValsNext_tiled # Same thing, but account for *transitory* shock\n        R_array = Rfunc(kNextEff_array) # Interest factor on aggregate assets\n        Reff_array = R_array/LivPrb # Effective interest factor on individual assets *for survivors*\n        wEff_array = wFunc(kNextEff_array)*TranShkAggValsNext_tiled # Effective wage rate (accounts for labor supply)\n        PermShkTotal_array = PermGroFac*PermGroFacAgg[j]*PermShkValsNext_tiled*PermShkAggValsNext_tiled # total / combined permanent shock\n        Mnext_array = kNext_array*R_array + wEff_array # next period's aggregate market resources\n\n        # Find the natural borrowing constraint for each value of M in the Mgrid.\n        # There is likely a faster way to do this, but someone needs to do the math:\n        # is aNrmMin determined by getting the worst shock of all four types?\n        aNrmMin_candidates = PermGroFac*PermGroFacAgg[j]*PermShkValsNext_tiled[:,0,:]*PermShkAggValsNext_tiled[:,0,:]/Reff_array[:,0,:]*\\\n                             (mNrmMinNext(Mnext_array[:,0,:]) - wEff_array[:,0,:]*TranShkValsNext_tiled[:,0,:])\n        aNrmMin_vec = np.max(aNrmMin_candidates,axis=1)\n        BoroCnstNat_vec = aNrmMin_vec\n        aNrmMin_tiled = np.tile(np.reshape(aNrmMin_vec,(Mcount,1,1)),(1,aCount,ShkCount))\n        aNrmNow_tiled = aNrmMin_tiled + aXtra_tiled\n\n        # Calculate market resources next period (and a constant array of capital-to-labor ratio)\n        mNrmNext_array = Reff_array*aNrmNow_tiled/PermShkTotal_array + TranShkValsNext_tiled*wEff_array\n\n        # Find marginal value next period at every income shock realization and every aggregate market resource gridpoint\n        vPnext_array = Reff_array*PermShkTotal_array**(-CRRA)*vPfuncNext(mNrmNext_array,Mnext_array)\n\n        # Calculate expectated marginal value at the end of the period at every asset gridpoint\n        EndOfPrdvP = DiscFac*LivPrb*np.sum(vPnext_array*ShkPrbsNext_tiled,axis=2)\n\n        # Make the conditional end-of-period marginal value function\n        BoroCnstNat = LinearInterp(np.insert(AaggGrid,0,0.0),np.insert(BoroCnstNat_vec,0,0.0))\n        EndOfPrdvPnvrs = np.concatenate((np.zeros((Mcount,1)),EndOfPrdvP**(-1./CRRA)),axis=1)\n        EndOfPrdvPnvrsFunc_base = BilinearInterp(np.transpose(EndOfPrdvPnvrs),np.insert(aXtraGrid,0,0.0),AaggGrid)\n        EndOfPrdvPnvrsFunc = VariableLowerBoundFunc2D(EndOfPrdvPnvrsFunc_base,BoroCnstNat)\n        EndOfPrdvPfunc_cond.append(MargValueFunc2D(EndOfPrdvPnvrsFunc,CRRA))\n        BoroCnstNat_cond.append(BoroCnstNat)\n\n    # Prepare some objects that are the same across all current states\n    aXtra_tiled = np.tile(np.reshape(aXtraGrid,(1,aCount)),(Mcount,1))\n    cFuncCnst = BilinearInterp(np.array([[0.0,0.0],[1.0,1.0]]),np.array([BoroCnstArt,BoroCnstArt+1.0]),np.array([0.0,1.0]))\n\n    # Now loop through *this* period's discrete states, calculating end-of-period\n    # marginal value (weighting across state transitions), then construct consumption\n    # and marginal value function for each state.\n    cFuncNow = []\n    vPfuncNow = []\n    mNrmMinNow = []\n    for i in range(StateCount):\n        # Find natural borrowing constraint for this state by Aagg\n        AaggNow = AFunc[i](Mgrid)\n        aNrmMin_candidates = np.zeros((StateCount,Mcount)) + np.nan\n        for j in range(StateCount):\n            if MrkvArray[i,j] > 0.: # Irrelevant if transition is impossible\n                aNrmMin_candidates[j,:] = BoroCnstNat_cond[j](AaggNow)\n        aNrmMin_vec = np.nanmax(aNrmMin_candidates,axis=0)\n        BoroCnstNat_vec = aNrmMin_vec\n\n        # Make tiled grids of aNrm and Aagg\n        aNrmMin_tiled = np.tile(np.reshape(aNrmMin_vec,(Mcount,1)),(1,aCount))\n        aNrmNow_tiled = aNrmMin_tiled + aXtra_tiled\n        AaggNow_tiled = np.tile(np.reshape(AaggNow,(Mcount,1)),(1,aCount))\n\n        # Loop through feasible transitions and calculate end-of-period marginal value\n        EndOfPrdvP = np.zeros((Mcount,aCount))\n        for j in range(StateCount):\n            if MrkvArray[i,j] > 0.:\n                temp = EndOfPrdvPfunc_cond[j](aNrmNow_tiled,AaggNow_tiled)\n                EndOfPrdvP += MrkvArray[i,j]*temp\n\n        # Calculate consumption and the endogenous mNrm gridpoints for this state\n        cNrmNow = EndOfPrdvP**(-1./CRRA)\n        mNrmNow = aNrmNow_tiled + cNrmNow\n\n        # Loop through the values in Mgrid and make a piecewise linear consumption function for each\n        cFuncBaseByM_list = []\n        for n in range(Mcount):\n            c_temp = np.insert(cNrmNow[n,:],0,0.0) # Add point at bottom\n            m_temp = np.insert(mNrmNow[n,:] - BoroCnstNat_vec[n],0,0.0)\n            cFuncBaseByM_list.append(LinearInterp(m_temp,c_temp))\n            # Add the M-specific consumption function to the list\n\n        # Construct the unconstrained consumption function by combining the M-specific functions\n        BoroCnstNat = LinearInterp(np.insert(Mgrid,0,0.0),np.insert(BoroCnstNat_vec,0,0.0))\n        cFuncBase = LinearInterpOnInterp1D(cFuncBaseByM_list,Mgrid)\n        cFuncUnc  = VariableLowerBoundFunc2D(cFuncBase,BoroCnstNat)\n\n        # Combine the constrained consumption function with unconstrained component\n        cFuncNow.append(LowerEnvelope2D(cFuncUnc,cFuncCnst))\n\n        # Make the minimum m function as the greater of the natural and artificial constraints\n        mNrmMinNow.append(UpperEnvelope(BoroCnstNat,ConstantFunction(BoroCnstArt)))\n\n        # Construct the marginal value function using the envelope condition\n        vPfuncNow.append(MargValueFunc2D(cFuncNow[-1],CRRA))\n\n    # Pack up and return the solution\n    solution_now = ConsumerSolution(cFunc=cFuncNow,vPfunc=vPfuncNow,mNrmMin=mNrmMinNow)\n    return solution_now"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresets the current state of the current set of time intervals to the initial state of the current set of agent count.", "response": "def reset(self):\n        '''\n        Initialize this type for a new simulated history of K/L ratio.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        self.initializeSim()\n        self.aLvlNow = self.kInit*np.ones(self.AgentCount) # Start simulation near SS\n        self.aNrmNow = self.aLvlNow/self.pLvlNow"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the terminal period solution for an aggregate shock consumer.", "response": "def updateSolutionTerminal(self):\n        '''\n        Updates the terminal period solution for an aggregate shock consumer.\n        Only fills in the consumption function and marginal value function.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        cFunc_terminal  = BilinearInterp(np.array([[0.0,0.0],[1.0,1.0]]),np.array([0.0,1.0]),np.array([0.0,1.0]))\n        vPfunc_terminal = MargValueFunc2D(cFunc_terminal,self.CRRA)\n        mNrmMin_terminal = ConstantFunction(0)\n        self.solution_terminal = ConsumerSolution(cFunc=cFunc_terminal,vPfunc=vPfunc_terminal,mNrmMin=mNrmMin_terminal)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getEconomyData(self,Economy):\n        '''\n        Imports economy-determined objects into self from a Market.\n        Instances of AggShockConsumerType \"live\" in some macroeconomy that has\n        attributes relevant to their microeconomic model, like the relationship\n        between the capital-to-labor ratio and the interest and wage rates; this\n        method imports those attributes from an \"economy\" object and makes them\n        attributes of the ConsumerType.\n\n        Parameters\n        ----------\n        Economy : Market\n            The \"macroeconomy\" in which this instance \"lives\".  Might be of the\n            subclass CobbDouglasEconomy, which has methods to generate the\n            relevant attributes.\n\n        Returns\n        -------\n        None\n        '''\n        self.T_sim = Economy.act_T                          # Need to be able to track as many periods as economy runs\n        self.kInit = Economy.kSS                            # Initialize simulation assets to steady state\n        self.aNrmInitMean = np.log(0.00000001)              # Initialize newborn assets to nearly zero\n        self.Mgrid = Economy.MSS*self.MgridBase             # Aggregate market resources grid adjusted around SS capital ratio\n        self.AFunc = Economy.AFunc                          # Next period's aggregate savings function\n        self.Rfunc = Economy.Rfunc                          # Interest factor as function of capital ratio\n        self.wFunc = Economy.wFunc                          # Wage rate as function of capital ratio\n        self.DeprFac = Economy.DeprFac                      # Rate of capital depreciation\n        self.PermGroFacAgg = Economy.PermGroFacAgg          # Aggregate permanent productivity growth\n        self.addAggShkDstn(Economy.AggShkDstn)              # Combine idiosyncratic and aggregate shocks into one dstn\n        self.addToTimeInv('Mgrid','AFunc','Rfunc', 'wFunc','DeprFac','PermGroFacAgg')", "response": "This method creates the basic attributes of the object self and populates the internal state of the internal state of the object with the values of the attributes of the object self and the attributes of the object self."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef addAggShkDstn(self,AggShkDstn):\n        '''\n        Updates attribute IncomeDstn by combining idiosyncratic shocks with aggregate shocks.\n\n        Parameters\n        ----------\n        AggShkDstn : [np.array]\n            Aggregate productivity shock distribution.  First element is proba-\n            bilities, second element is agg permanent shocks, third element is\n            agg transitory shocks.\n\n        Returns\n        -------\n        None\n        '''\n        if len(self.IncomeDstn[0]) > 3:\n            self.IncomeDstn = self.IncomeDstnWithoutAggShocks\n        else:\n            self.IncomeDstnWithoutAggShocks = self.IncomeDstn\n        self.IncomeDstn = [combineIndepDstns(self.IncomeDstn[t],AggShkDstn) for t in range(self.T_cycle)]", "response": "Adds an aggregate productivity shock distribution to the attribute IncomeDstn."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef simBirth(self,which_agents):\n        '''\n        Makes new consumers for the given indices.  Initialized variables include aNrm and pLvl, as\n        well as time variables t_age and t_cycle.  Normalized assets and permanent income levels\n        are drawn from lognormal distributions given by aNrmInitMean and aNrmInitStd (etc).\n\n        Parameters\n        ----------\n        which_agents : np.array(Bool)\n            Boolean array of size self.AgentCount indicating which agents should be \"born\".\n\n        Returns\n        -------\n        None\n        '''\n        IndShockConsumerType.simBirth(self,which_agents)\n        if hasattr(self,'aLvlNow'):\n            self.aLvlNow[which_agents] = self.aNrmNow[which_agents]*self.pLvlNow[which_agents]\n        else:\n            self.aLvlNow = self.aNrmNow*self.pLvlNow", "response": "Makes new consumers for the given indices."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an array of size self. AgentCount with self. RfreeNow in every entry.", "response": "def getRfree(self):\n        '''\n        Returns an array of size self.AgentCount with self.RfreeNow in every entry.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        RfreeNow : np.array\n             Array of size self.AgentCount with risk free interest rate for each agent.\n        '''\n        RfreeNow = self.RfreeNow*np.ones(self.AgentCount)\n        return RfreeNow"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the effective permanent and transitory shocks for this period.", "response": "def getShocks(self):\n        '''\n        Finds the effective permanent and transitory shocks this period by combining the aggregate\n        and idiosyncratic shocks of each type.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        IndShockConsumerType.getShocks(self) # Update idiosyncratic shocks\n        self.TranShkNow = self.TranShkNow*self.TranShkAggNow*self.wRteNow\n        self.PermShkNow = self.PermShkNow*self.PermShkAggNow"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef addAggShkDstn(self,AggShkDstn):\n        '''\n        Variation on AggShockConsumerType.addAggShkDstn that handles the Markov\n        state. AggShkDstn is a list of aggregate productivity shock distributions\n        for each Markov state.\n        '''\n        if len(self.IncomeDstn[0][0]) > 3:\n            self.IncomeDstn = self.IncomeDstnWithoutAggShocks\n        else:\n            self.IncomeDstnWithoutAggShocks = self.IncomeDstn\n\n        IncomeDstnOut = []\n        N = self.MrkvArray.shape[0]\n        for t in range(self.T_cycle):\n            IncomeDstnOut.append([combineIndepDstns(self.IncomeDstn[t][n],AggShkDstn[n]) for n in range(N)])\n        self.IncomeDstn = IncomeDstnOut", "response": "Adds an aggregate productivity shock distributions to the IncomeDstn attribute of the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef updateSolutionTerminal(self):\n        '''\n        Update the terminal period solution.  This method should be run when a\n        new AgentType is created or when CRRA changes.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        AggShockConsumerType.updateSolutionTerminal(self)\n\n        # Make replicated terminal period solution\n        StateCount = self.MrkvArray.shape[0]\n        self.solution_terminal.cFunc   = StateCount*[self.solution_terminal.cFunc]\n        self.solution_terminal.vPfunc  = StateCount*[self.solution_terminal.vPfunc]\n        self.solution_terminal.mNrmMin = StateCount*[self.solution_terminal.mNrmMin]", "response": "Update the terminal period solution. This method should be run when new AgentType is created and CRRA changes are made."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of all permanent and transitory income shocks for this period.", "response": "def getShocks(self):\n        '''\n        Gets permanent and transitory income shocks for this period.  Samples from IncomeDstn for\n        each period in the cycle.  This is a copy-paste from IndShockConsumerType, with the\n        addition of the Markov macroeconomic state.  Unfortunately, the getShocks method for\n        MarkovConsumerType cannot be used, as that method assumes that MrkvNow is a vector\n        with a value for each agent, not just a single int.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        PermShkNow = np.zeros(self.AgentCount) # Initialize shock arrays\n        TranShkNow = np.zeros(self.AgentCount)\n        newborn = self.t_age == 0\n        for t in range(self.T_cycle):\n            these = t == self.t_cycle\n            N = np.sum(these)\n            if N > 0:\n                IncomeDstnNow    = self.IncomeDstn[t-1][self.MrkvNow] # set current income distribution\n                PermGroFacNow    = self.PermGroFac[t-1]               # and permanent growth factor\n                Indices          = np.arange(IncomeDstnNow[0].size) # just a list of integers\n                # Get random draws of income shocks from the discrete distribution\n                EventDraws       = drawDiscrete(N,X=Indices,P=IncomeDstnNow[0],exact_match=True,seed=self.RNG.randint(0,2**31-1))\n                PermShkNow[these] = IncomeDstnNow[1][EventDraws]*PermGroFacNow # permanent \"shock\" includes expected growth\n                TranShkNow[these] = IncomeDstnNow[2][EventDraws]\n\n        # That procedure used the *last* period in the sequence for newborns, but that's not right\n        # Redraw shocks for newborns, using the *first* period in the sequence.  Approximation.\n        N = np.sum(newborn)\n        if N > 0:\n            these = newborn\n            IncomeDstnNow    = self.IncomeDstn[0][self.MrkvNow]  # set current income distribution\n            PermGroFacNow    = self.PermGroFac[0]                # and permanent growth factor\n            Indices          = np.arange(IncomeDstnNow[0].size) # just a list of integers\n            # Get random draws of income shocks from the discrete distribution\n            EventDraws       = drawDiscrete(N,X=Indices,P=IncomeDstnNow[0],exact_match=False,seed=self.RNG.randint(0,2**31-1))\n            PermShkNow[these] = IncomeDstnNow[1][EventDraws]*PermGroFacNow # permanent \"shock\" includes expected growth\n            TranShkNow[these] = IncomeDstnNow[2][EventDraws]\n#        PermShkNow[newborn] = 1.0\n#        TranShkNow[newborn] = 1.0\n\n        # Store the shocks in self\n        self.EmpNow = np.ones(self.AgentCount,dtype=bool)\n        self.EmpNow[TranShkNow == self.IncUnemp] = False\n        self.TranShkNow = TranShkNow*self.TranShkAggNow*self.wRteNow\n        self.PermShkNow = PermShkNow*self.PermShkAggNow"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getControls(self):\n        '''\n        Calculates consumption for each consumer of this type using the consumption functions.\n        For this AgentType class, MrkvNow is the same for all consumers.  However, in an\n        extension with \"macroeconomic inattention\", consumers might misperceive the state\n        and thus act as if they are in different states.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        cNrmNow = np.zeros(self.AgentCount) + np.nan\n        MPCnow = np.zeros(self.AgentCount) + np.nan\n        MaggNow = self.getMaggNow()\n        MrkvNow = self.getMrkvNow()\n\n        StateCount = self.MrkvArray.shape[0]\n        MrkvBoolArray = np.zeros((StateCount,self.AgentCount),dtype=bool)\n        for i in range(StateCount):\n            MrkvBoolArray[i,:] = i == MrkvNow\n\n        for t in range(self.T_cycle):\n            these = t == self.t_cycle\n            for i in range(StateCount):\n                those = np.logical_and(these,MrkvBoolArray[i,:])\n                cNrmNow[those] = self.solution[t].cFunc[i](self.mNrmNow[those],MaggNow[those])\n                MPCnow[those]  = self.solution[t].cFunc[i].derivativeX(self.mNrmNow[those],MaggNow[those]) # Marginal propensity to consume\n        self.cNrmNow = cNrmNow\n        self.MPCnow = MPCnow\n        return None", "response": "Calculates consumption for each consumer of this type using the consumption functions. This function calculates consumption for each consumer of this type using the derivative X function of the consumption functions."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates the attributes TranShkAggDstn PermShkAggDstn and AggShkDstn.", "response": "def makeAggShkDstn(self):\n        '''\n        Creates the attributes TranShkAggDstn, PermShkAggDstn, and AggShkDstn.\n        Draws on attributes TranShkAggStd, PermShkAddStd, TranShkAggCount, PermShkAggCount.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        self.TranShkAggDstn = approxMeanOneLognormal(sigma=self.TranShkAggStd,N=self.TranShkAggCount)\n        self.PermShkAggDstn = approxMeanOneLognormal(sigma=self.PermShkAggStd,N=self.PermShkAggCount)\n        self.AggShkDstn = combineIndepDstns(self.PermShkAggDstn,self.TranShkAggDstn)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the interest factor and wage rate this period using each agent s aggregate capital and permanent income shocks to get the aggregate capital ratio.", "response": "def calcRandW(self,aLvlNow,pLvlNow):\n        '''\n        Calculates the interest factor and wage rate this period using each agent's\n        capital stock to get the aggregate capital ratio.\n\n        Parameters\n        ----------\n        aLvlNow : [np.array]\n            Agents' current end-of-period assets.  Elements of the list correspond\n            to types in the economy, entries within arrays to agents of that type.\n\n        Returns\n        -------\n        AggVarsNow : CobbDouglasAggVars\n            An object containing the aggregate variables for the upcoming period:\n            capital-to-labor ratio, interest factor, (normalized) wage rate,\n            aggregate permanent and transitory shocks.\n        '''\n        # Calculate aggregate savings\n        AaggPrev = np.mean(np.array(aLvlNow))/np.mean(pLvlNow) # End-of-period savings from last period\n        # Calculate aggregate capital this period\n        AggregateK = np.mean(np.array(aLvlNow)) # ...becomes capital today\n        # This version uses end-of-period assets and\n        # permanent income to calculate aggregate capital, unlike the Mathematica\n        # version, which first applies the idiosyncratic permanent income shocks\n        # and then aggregates.  Obviously this is mathematically equivalent.\n\n        # Get this period's aggregate shocks\n        PermShkAggNow = self.PermShkAggHist[self.Shk_idx]\n        TranShkAggNow = self.TranShkAggHist[self.Shk_idx]\n        self.Shk_idx += 1\n\n        AggregateL = np.mean(pLvlNow)*PermShkAggNow\n\n        # Calculate the interest factor and wage rate this period\n        KtoLnow = AggregateK/AggregateL\n        self.KtoYnow = KtoLnow**(1.0-self.CapShare)\n        RfreeNow = self.Rfunc(KtoLnow/TranShkAggNow)\n        wRteNow  = self.wFunc(KtoLnow/TranShkAggNow)\n        MaggNow  = KtoLnow*RfreeNow + wRteNow*TranShkAggNow\n        self.KtoLnow = KtoLnow   # Need to store this as it is a sow variable\n\n        # Package the results into an object and return it\n        AggVarsNow = CobbDouglasAggVars(MaggNow,AaggPrev,KtoLnow,RfreeNow,wRteNow,PermShkAggNow,TranShkAggNow)\n        return AggVarsNow"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating a new aggregate savings rule based on the history of the aggregate savings and aggregate market resources for an economy.", "response": "def calcAFunc(self,MaggNow,AaggNow):\n        '''\n        Calculate a new aggregate savings rule based on the history\n        of the aggregate savings and aggregate market resources from a simulation.\n\n        Parameters\n        ----------\n        MaggNow : [float]\n            List of the history of the simulated aggregate market resources for an economy.\n        AaggNow : [float]\n            List of the history of the simulated aggregate savings for an economy.\n\n        Returns\n        -------\n        (unnamed) : CapDynamicRule\n            Object containing a new savings rule\n        '''\n        verbose = self.verbose\n        discard_periods = self.T_discard # Throw out the first T periods to allow the simulation to approach the SS\n        update_weight = 1. - self.DampingFac  # Proportional weight to put on new function vs old function parameters\n        total_periods = len(MaggNow)\n\n        # Regress the log savings against log market resources\n        logAagg   = np.log(AaggNow[discard_periods:total_periods])\n        logMagg = np.log(MaggNow[discard_periods-1:total_periods-1])\n        slope, intercept, r_value, p_value, std_err = stats.linregress(logMagg,logAagg)\n\n        # Make a new aggregate savings rule by combining the new regression parameters\n        # with the previous guess\n        intercept = update_weight*intercept + (1.0-update_weight)*self.intercept_prev\n        slope = update_weight*slope + (1.0-update_weight)*self.slope_prev\n        AFunc = AggregateSavingRule(intercept,slope) # Make a new next-period capital function\n\n        # Save the new values as \"previous\" values for the next iteration\n        self.intercept_prev = intercept\n        self.slope_prev = slope\n\n        # Plot aggregate resources vs aggregate savings for this run and print the new parameters\n        if verbose:\n            print('intercept=' + str(intercept) + ', slope=' + str(slope) + ', r-sq=' + str(r_value**2))\n            #plot_start = discard_periods\n            #plt.plot(logMagg[plot_start:],logAagg[plot_start:],'.k')\n            #plt.show()\n\n        return AggShocksDynamicRule(AFunc)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef makeAggShkHist(self):\n        '''\n        Make simulated histories of aggregate transitory and permanent shocks. Histories are of\n        length self.act_T, for use in the general equilibrium simulation.  This replicates the same\n        method for CobbDouglasEconomy; future version should create parent class.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        sim_periods = self.act_T\n        Events      = np.arange(self.AggShkDstn[0].size) # just a list of integers\n        EventDraws  = drawDiscrete(N=sim_periods,P=self.AggShkDstn[0],X=Events,seed=0)\n        PermShkAggHist = self.AggShkDstn[1][EventDraws]\n        TranShkAggHist = self.AggShkDstn[2][EventDraws]\n\n        # Store the histories\n        self.PermShkAggHist = PermShkAggHist\n        self.TranShkAggHist = TranShkAggHist", "response": "Make simulated histories of aggregate transitory and permanent shocks."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns aggregate state variables and shocks for this period.", "response": "def getAggShocks(self):\n        '''\n        Returns aggregate state variables and shocks for this period.  The capital-to-labor ratio\n        is irrelevant and thus treated as constant, and the wage and interest rates are also\n        constant.  However, aggregate shocks are assigned from a prespecified history.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        AggVarsNow : CobbDouglasAggVars\n            Aggregate state and shock variables for this period.\n        '''\n        # Get this period's aggregate shocks\n        PermShkAggNow = self.PermShkAggHist[self.Shk_idx]\n        TranShkAggNow = self.TranShkAggHist[self.Shk_idx]\n        self.Shk_idx += 1\n\n        # Factor prices are constant\n        RfreeNow = self.Rfunc(1.0/PermShkAggNow)\n        wRteNow  = self.wFunc(1.0/PermShkAggNow)\n\n        # Aggregates are irrelavent\n        AaggNow = 1.0\n        MaggNow = 1.0\n        KtoLnow = 1.0/PermShkAggNow\n\n        # Package the results into an object and return it\n        AggVarsNow = CobbDouglasAggVars(MaggNow,AaggNow,KtoLnow,RfreeNow,wRteNow,PermShkAggNow,TranShkAggNow)\n        return AggVarsNow"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getPermGroFacAggLR(self):\n        '''\n        Calculates and returns the long run permanent income growth factor.  This\n        is the average growth factor in self.PermGroFacAgg, weighted by the long\n        run distribution of Markov states (as determined by self.MrkvArray).\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        PermGroFacAggLR : float\n            Long run aggregate permanent income growth factor\n        '''\n        # Find the long run distribution of Markov states\n        w, v = np.linalg.eig(np.transpose(self.MrkvArray))\n        idx = (np.abs(w-1.0)).argmin()\n        x = v[:,idx].astype(float)\n        LR_dstn = (x/np.sum(x))\n\n        # Return the weighted average of aggregate permanent income growth factors\n        PermGroFacAggLR = np.dot(LR_dstn,np.array(self.PermGroFacAgg))\n        return PermGroFacAggLR", "response": "Calculates and returns the long run aggregate permanent income growth factor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef makeAggShkDstn(self):\n        '''\n        Creates the attributes TranShkAggDstn, PermShkAggDstn, and AggShkDstn.\n        Draws on attributes TranShkAggStd, PermShkAddStd, TranShkAggCount, PermShkAggCount.\n        This version accounts for the Markov macroeconomic state.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        TranShkAggDstn = []\n        PermShkAggDstn = []\n        AggShkDstn = []\n        StateCount = self.MrkvArray.shape[0]\n\n        for i in range(StateCount):\n            TranShkAggDstn.append(approxMeanOneLognormal(sigma=self.TranShkAggStd[i],N=self.TranShkAggCount))\n            PermShkAggDstn.append(approxMeanOneLognormal(sigma=self.PermShkAggStd[i],N=self.PermShkAggCount))\n            AggShkDstn.append(combineIndepDstns(PermShkAggDstn[-1],TranShkAggDstn[-1]))\n\n        self.TranShkAggDstn = TranShkAggDstn\n        self.PermShkAggDstn = PermShkAggDstn\n        self.AggShkDstn = AggShkDstn", "response": "Creates the attributes TranShkAggDstn PermShkAggDstn and AggShkDstn."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef makeAggShkHist(self):\n        '''\n        Make simulated histories of aggregate transitory and permanent shocks.\n        Histories are of length self.act_T, for use in the general equilibrium\n        simulation.  Draws on history of aggregate Markov states generated by\n        internal call to makeMrkvHist().\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        self.makeMrkvHist() # Make a (pseudo)random sequence of Markov states\n        sim_periods = self.act_T\n\n        # For each Markov state in each simulated period, draw the aggregate shocks\n        # that would occur in that state in that period\n        StateCount = self.MrkvArray.shape[0]\n        PermShkAggHistAll = np.zeros((StateCount,sim_periods))\n        TranShkAggHistAll = np.zeros((StateCount,sim_periods))\n        for i in range(StateCount):\n            Events      = np.arange(self.AggShkDstn[i][0].size) # just a list of integers\n            EventDraws  = drawDiscrete(N=sim_periods,P=self.AggShkDstn[i][0],X=Events,seed=0)\n            PermShkAggHistAll[i,:] = self.AggShkDstn[i][1][EventDraws]\n            TranShkAggHistAll[i,:] = self.AggShkDstn[i][2][EventDraws]\n\n        # Select the actual history of aggregate shocks based on the sequence\n        # of Markov states that the economy experiences\n        PermShkAggHist = np.zeros(sim_periods)\n        TranShkAggHist = np.zeros(sim_periods)\n        for i in range(StateCount):\n            these = i == self.MrkvNow_hist\n            PermShkAggHist[these] = PermShkAggHistAll[i,these]*self.PermGroFacAgg[i]\n            TranShkAggHist[these] = TranShkAggHistAll[i,these]\n\n        # Store the histories\n        self.PermShkAggHist = PermShkAggHist\n        self.TranShkAggHist = TranShkAggHist", "response": "Creates simulated and permanent shocks of aggregate transitory and permanent shocks."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef makeMrkvHist(self):\n        '''\n        Makes a history of macroeconomic Markov states, stored in the attribute\n        MrkvNow_hist.  This version ensures that each state is reached a sufficient\n        number of times to have a valid sample for calcDynamics to produce a good\n        dynamic rule.  It will sometimes cause act_T to be increased beyond its\n        initially specified level.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        if hasattr(self,'loops_max'):\n            loops_max = self.loops_max\n        else: # Maximum number of loops; final act_T never exceeds act_T*loops_max\n            loops_max   = 10\n\n        state_T_min = 50 # Choose minimum number of periods in each state for a valid Markov sequence\n        logit_scale = 0.2 # Scaling factor on logit choice shocks when jumping to a new state\n        # Values close to zero make the most underrepresented states very likely to visit, while\n        # large values of logit_scale make any state very likely to be jumped to.\n\n        # Reset act_T to the level actually specified by the user\n        if hasattr(self,'act_T_orig'):\n            act_T = self.act_T_orig\n        else: # Or store it for the first time\n            self.act_T_orig = self.act_T\n            act_T = self.act_T\n\n        # Find the long run distribution of Markov states\n        w, v = np.linalg.eig(np.transpose(self.MrkvArray))\n        idx = (np.abs(w-1.0)).argmin()\n        x = v[:,idx].astype(float)\n        LR_dstn = (x/np.sum(x))\n\n        # Initialize the Markov history and set up transitions\n        MrkvNow_hist = np.zeros(self.act_T_orig,dtype=int)\n        cutoffs = np.cumsum(self.MrkvArray,axis=1)\n        loops = 0\n        go = True\n        MrkvNow = self.MrkvNow_init\n        t = 0\n        StateCount = self.MrkvArray.shape[0]\n\n        # Add histories until each state has been visited at least state_T_min times\n        while go:\n            draws = drawUniform(N=self.act_T_orig,seed=loops)\n            for s in range(draws.size): # Add act_T_orig more periods\n                MrkvNow_hist[t] = MrkvNow\n                MrkvNow = np.searchsorted(cutoffs[MrkvNow,:],draws[s])\n                t += 1\n\n            # Calculate the empirical distribution\n            state_T = np.zeros(StateCount)\n            for i in range(StateCount):\n                state_T[i] = np.sum(MrkvNow_hist==i)\n\n            # Check whether each state has been visited state_T_min times\n            if np.all(state_T >= state_T_min):\n                go = False # If so, terminate the loop\n                continue\n\n            # Choose an underrepresented state to \"jump\" to\n            if np.any(state_T == 0): # If any states have *never* been visited, randomly choose one of those\n                never_visited = np.where(np.array(state_T == 0))[0]\n                MrkvNow = np.random.choice(never_visited)\n            else: # Otherwise, use logit choice probabilities to visit an underrepresented state\n                emp_dstn   = state_T/act_T\n                ratios     = LR_dstn/emp_dstn\n                ratios_adj = ratios - np.max(ratios)\n                ratios_exp = np.exp(ratios_adj/logit_scale)\n                ratios_sum = np.sum(ratios_exp)\n                jump_probs = ratios_exp/ratios_sum\n                cum_probs  = np.cumsum(jump_probs)\n                MrkvNow    = np.searchsorted(cum_probs,draws[-1])\n\n            loops += 1\n            # Make the Markov state history longer by act_T_orig periods\n            if loops >= loops_max:\n                go = False\n                print('makeMrkvHist reached maximum number of loops without generating a valid sequence!')\n            else:\n                MrkvNow_new = np.zeros(self.act_T_orig,dtype=int)\n                MrkvNow_hist = np.concatenate((MrkvNow_hist,MrkvNow_new))\n                act_T += self.act_T_orig\n\n        # Store the results as attributes of self\n        self.MrkvNow_hist = MrkvNow_hist\n        self.act_T = act_T", "response": "Makes a history of macroeconomic Markov states stored in the attribute\n        MrkvNow_hist."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfunction to calculate the capital to labor ratio interest factor and wage rate based on each agent s current state.", "response": "def millRule(self,aLvlNow,pLvlNow):\n        '''\n        Function to calculate the capital to labor ratio, interest factor, and\n        wage rate based on each agent's current state.  Just calls calcRandW()\n        and adds the Markov state index.\n\n        See documentation for calcRandW for more information.\n        '''\n        MrkvNow = self.MrkvNow_hist[self.Shk_idx]\n        temp =  self.calcRandW(aLvlNow,pLvlNow)\n        temp(MrkvNow = MrkvNow)\n        return temp"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating a new aggregate savings rule based on the history of the aggregate savings and aggregate market resources from a simulation.", "response": "def calcAFunc(self,MaggNow,AaggNow):\n        '''\n        Calculate a new aggregate savings rule based on the history of the\n        aggregate savings and aggregate market resources from a simulation.\n        Calculates an aggregate saving rule for each macroeconomic Markov state.\n\n        Parameters\n        ----------\n        MaggNow : [float]\n            List of the history of the simulated  aggregate market resources for an economy.\n        AaggNow : [float]\n            List of the history of the simulated  aggregate savings for an economy.\n\n        Returns\n        -------\n        (unnamed) : CapDynamicRule\n            Object containing new saving rules for each Markov state.\n        '''\n        verbose = self.verbose\n        discard_periods = self.T_discard # Throw out the first T periods to allow the simulation to approach the SS\n        update_weight = 1. - self.DampingFac  # Proportional weight to put on new function vs old function parameters\n        total_periods = len(MaggNow)\n\n        # Trim the histories of M_t and A_t and convert them to logs\n        logAagg   = np.log(AaggNow[discard_periods:total_periods])\n        logMagg   = np.log(MaggNow[discard_periods-1:total_periods-1])\n        MrkvHist  = self.MrkvNow_hist[discard_periods-1:total_periods-1]\n\n        # For each Markov state, regress A_t on M_t and update the saving rule\n        AFunc_list = []\n        rSq_list = []\n        for i in range(self.MrkvArray.shape[0]):\n            these = i == MrkvHist\n            slope, intercept, r_value, p_value, std_err = stats.linregress(logMagg[these],logAagg[these])\n            #if verbose:\n            #    plt.plot(logMagg[these],logAagg[these],'.')\n\n            # Make a new aggregate savings rule by combining the new regression parameters\n            # with the previous guess\n            intercept = update_weight*intercept + (1.0-update_weight)*self.intercept_prev[i]\n            slope = update_weight*slope + (1.0-update_weight)*self.slope_prev[i]\n            AFunc_list.append(AggregateSavingRule(intercept,slope)) # Make a new next-period capital function\n            rSq_list.append(r_value**2)\n\n            # Save the new values as \"previous\" values for the next iteration\n            self.intercept_prev[i] = intercept\n            self.slope_prev[i] = slope\n\n        # Plot aggregate resources vs aggregate savings for this run and print the new parameters\n        if verbose:\n            print('intercept=' + str(self.intercept_prev) + ', slope=' + str(self.slope_prev) + ', r-sq=' + str(rSq_list))\n            #plt.show()\n\n        return AggShocksDynamicRule(AFunc_list)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getKYratioDifference(Economy,param_name,param_count,center,spread,dist_type):\n    '''\n    Finds the difference between simulated and target capital to income ratio in an economy when\n    a given parameter has heterogeneity according to some distribution.\n\n    Parameters\n    ----------\n    Economy : cstwMPCmarket\n        An object representing the entire economy, containing the various AgentTypes as an attribute.\n    param_name : string\n        The name of the parameter of interest that varies across the population.\n    param_count : int\n        The number of different values the parameter of interest will take on.\n    center : float\n        A measure of centrality for the distribution of the parameter of interest.\n    spread : float\n        A measure of spread or diffusion for the distribution of the parameter of interest.\n    dist_type : string\n        The type of distribution to be used.  Can be \"lognormal\" or \"uniform\" (can expand).\n\n    Returns\n    -------\n    diff : float\n        Difference between simulated and target capital to income ratio for this economy.\n    '''\n    Economy(LorenzBool = False, ManyStatsBool = False) # Make sure we're not wasting time calculating stuff\n    Economy.distributeParams(param_name,param_count,center,spread,dist_type) # Distribute parameters\n    Economy.solve()\n    diff = Economy.calcKYratioDifference()\n    print('getKYratioDifference tried center = ' + str(center) + ' and got ' + str(diff))\n    return diff", "response": "Finds the difference between simulated and target capital to income ratio in an economy when a given parameter has heterogeneity according to some distribution."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef findLorenzDistanceAtTargetKY(Economy,param_name,param_count,center_range,spread,dist_type):\n    '''\n    Finds the sum of squared distances between simulated and target Lorenz points in an economy when\n    a given parameter has heterogeneity according to some distribution.  The class of distribution\n    and a measure of spread are given as inputs, but the measure of centrality such that the capital\n    to income ratio matches the target ratio must be found.\n\n    Parameters\n    ----------\n    Economy : cstwMPCmarket\n        An object representing the entire economy, containing the various AgentTypes as an attribute.\n    param_name : string\n        The name of the parameter of interest that varies across the population.\n    param_count : int\n        The number of different values the parameter of interest will take on.\n    center_range : [float,float]\n        Bounding values for a measure of centrality for the distribution of the parameter of interest.\n    spread : float\n        A measure of spread or diffusion for the distribution of the parameter of interest.\n    dist_type : string\n        The type of distribution to be used.  Can be \"lognormal\" or \"uniform\" (can expand).\n\n    Returns\n    -------\n    dist : float\n        Sum of squared distances between simulated and target Lorenz points for this economy (sqrt).\n    '''\n    # Define the function to search for the correct value of center, then find its zero\n    intermediateObjective = lambda center : getKYratioDifference(Economy = Economy,\n                                                                 param_name = param_name,\n                                                                 param_count = param_count,\n                                                                 center = center,\n                                                                 spread = spread,\n                                                                 dist_type = dist_type)\n    optimal_center = brentq(intermediateObjective,center_range[0],center_range[1],xtol=10**(-6))\n    Economy.center_save = optimal_center\n\n    # Get the sum of squared Lorenz distances given the correct distribution of the parameter\n    Economy(LorenzBool = True) # Make sure we actually calculate simulated Lorenz points\n    Economy.distributeParams(param_name,param_count,optimal_center,spread,dist_type) # Distribute parameters\n    Economy.solveAgents()\n    Economy.makeHistory()\n    dist = Economy.calcLorenzDistance()\n    Economy(LorenzBool = False)\n    print ('findLorenzDistanceAtTargetKY tried spread = ' + str(spread) + ' and got ' + str(dist))\n    return dist", "response": "This function finds the sum of squared distances between simulated and target Lorenz points in an economy when a given parameter has heterogeneity."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the steady state proportions of each age given a survival probability sequence LivPrb.", "response": "def calcStationaryAgeDstn(LivPrb,terminal_period):\n    '''\n    Calculates the steady state proportions of each age given survival probability sequence LivPrb.\n    Assumes that agents who die are replaced by a newborn agent with t_age=0.\n\n    Parameters\n    ----------\n    LivPrb : [float]\n        Sequence of survival probabilities in ordinary chronological order.  Has length T_cycle.\n    terminal_period : bool\n        Indicator for whether a terminal period follows the last period in the cycle (with LivPrb=0).\n\n    Returns\n    -------\n    AgeDstn : np.array\n        Stationary distribution of age.  Stochastic vector with frequencies of each age.\n    '''\n    T = len(LivPrb)\n    if terminal_period:\n        MrkvArray = np.zeros((T+1,T+1))\n        top = T\n    else:\n        MrkvArray = np.zeros((T,T))\n        top = T-1\n\n    for t in range(top):\n        MrkvArray[t,0] = 1.0 - LivPrb[t]\n        MrkvArray[t,t+1] = LivPrb[t]\n    MrkvArray[t+1,0] = 1.0\n\n    w, v = np.linalg.eig(np.transpose(MrkvArray))\n    idx = (np.abs(w-1.0)).argmin()\n    x = v[:,idx].astype(float)\n    AgeDstn = (x/np.sum(x))\n    return AgeDstn"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef updateIncomeProcess(self):\n        '''\n        An alternative method for constructing the income process in the infinite horizon model.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        if self.cycles == 0:\n            tax_rate = (self.IncUnemp*self.UnempPrb)/((1.0-self.UnempPrb)*self.IndL)\n            TranShkDstn     = deepcopy(approxMeanOneLognormal(self.TranShkCount,sigma=self.TranShkStd[0],tail_N=0))\n            TranShkDstn[0]  = np.insert(TranShkDstn[0]*(1.0-self.UnempPrb),0,self.UnempPrb)\n            TranShkDstn[1]  = np.insert(TranShkDstn[1]*(1.0-tax_rate)*self.IndL,0,self.IncUnemp)\n            PermShkDstn     = approxMeanOneLognormal(self.PermShkCount,sigma=self.PermShkStd[0],tail_N=0)\n            self.IncomeDstn = [combineIndepDstns(PermShkDstn,TranShkDstn)]\n            self.TranShkDstn = TranShkDstn\n            self.PermShkDstn = PermShkDstn\n            self.addToTimeVary('IncomeDstn')\n        else: # Do the usual method if this is the lifecycle model\n            EstimationAgentClass.updateIncomeProcess(self)", "response": "Update the income process of the income process."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef millRule(self,aLvlNow,pLvlNow,MPCnow,TranShkNow,EmpNow,t_age,LorenzBool,ManyStatsBool):\n        '''\n        The millRule for this class simply calls the method calcStats.\n        '''\n        self.calcStats(aLvlNow,pLvlNow,MPCnow,TranShkNow,EmpNow,t_age,LorenzBool,ManyStatsBool)\n        if self.AggShockBool:\n            return self.calcRandW(aLvlNow,pLvlNow)\n        else: # These variables are tracked but not created in no-agg-shocks specifications\n            self.MaggNow = 0.0\n            self.AaggNow = 0.0", "response": "This method calculates the millRule for a specific class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calcStats(self,aLvlNow,pLvlNow,MPCnow,TranShkNow,EmpNow,t_age,LorenzBool,ManyStatsBool):\n        '''\n        Calculate various statistics about the current population in the economy.\n\n        Parameters\n        ----------\n        aLvlNow : [np.array]\n            Arrays with end-of-period assets, listed by each ConsumerType in self.agents.\n        pLvlNow : [np.array]\n            Arrays with permanent income levels, listed by each ConsumerType in self.agents.\n        MPCnow : [np.array]\n            Arrays with marginal propensity to consume, listed by each ConsumerType in self.agents.\n        TranShkNow : [np.array]\n            Arrays with transitory income shocks, listed by each ConsumerType in self.agents.\n        EmpNow : [np.array]\n            Arrays with employment states: True if employed, False otherwise.\n        t_age : [np.array]\n            Arrays with periods elapsed since model entry, listed by each ConsumerType in self.agents.\n        LorenzBool: bool\n            Indicator for whether the Lorenz target points should be calculated.  Usually False,\n            only True when DiscFac has been identified for a particular nabla.\n        ManyStatsBool: bool\n            Indicator for whether a lot of statistics for tables should be calculated. Usually False,\n            only True when parameters have been estimated and we want values for tables.\n\n        Returns\n        -------\n        None\n        '''\n        # Combine inputs into single arrays\n        aLvl = np.hstack(aLvlNow)\n        pLvl = np.hstack(pLvlNow)\n        age  = np.hstack(t_age)\n        TranShk = np.hstack(TranShkNow)\n        Emp = np.hstack(EmpNow)\n\n        # Calculate the capital to income ratio in the economy\n        CohortWeight = self.PopGroFac**(-age)\n        CapAgg = np.sum(aLvl*CohortWeight)\n        IncAgg = np.sum(pLvl*TranShk*CohortWeight)\n        KtoYnow = CapAgg/IncAgg\n        self.KtoYnow = KtoYnow\n\n        # Store Lorenz data if requested\n        self.LorenzLong = np.nan\n        if LorenzBool:\n            order = np.argsort(aLvl)\n            aLvl = aLvl[order]\n            CohortWeight = CohortWeight[order]\n            wealth_shares = getLorenzShares(aLvl,weights=CohortWeight,percentiles=self.LorenzPercentiles,presorted=True)\n            self.Lorenz = wealth_shares\n            if ManyStatsBool:\n                self.LorenzLong = getLorenzShares(aLvl,weights=CohortWeight,percentiles=np.arange(0.01,1.0,0.01),presorted=True)\n        else:\n            self.Lorenz = np.nan # Store nothing if we don't want Lorenz data\n\n        # Calculate a whole bunch of statistics if requested\n        if ManyStatsBool:\n            # Reshape other inputs\n            MPC  = np.hstack(MPCnow)\n\n            # Sort other data items if aLvl and CohortWeight were sorted\n            if LorenzBool:\n                pLvl = pLvl[order]\n                MPC  = MPC[order]\n                TranShk = TranShk[order]\n                age = age[order]\n                Emp = Emp[order]\n            aNrm = aLvl/pLvl # Normalized assets (wealth ratio)\n            IncLvl = TranShk*pLvl # Labor income this period\n\n            # Calculate overall population MPC and by subpopulations\n            MPCannual = 1.0 - (1.0 - MPC)**4\n            self.MPCall = np.sum(MPCannual*CohortWeight)/np.sum(CohortWeight)\n            employed =  Emp\n            unemployed = np.logical_not(employed)\n            if self.T_retire > 0: # Adjust for the lifecycle model, where agents might be retired instead\n                unemployed = np.logical_and(unemployed,age < self.T_retire)\n                employed   = np.logical_and(employed,age < self.T_retire)\n                retired    = age >= self.T_retire\n            else:\n                retired    = np.zeros_like(unemployed,dtype=bool)\n            self.MPCunemployed = np.sum(MPCannual[unemployed]*CohortWeight[unemployed])/np.sum(CohortWeight[unemployed])\n            self.MPCemployed   = np.sum(MPCannual[employed]*CohortWeight[employed])/np.sum(CohortWeight[employed])\n            self.MPCretired    = np.sum(MPCannual[retired]*CohortWeight[retired])/np.sum(CohortWeight[retired])\n            self.MPCbyWealthRatio = calcSubpopAvg(MPCannual,aNrm,self.cutoffs,CohortWeight)\n            self.MPCbyIncome      = calcSubpopAvg(MPCannual,IncLvl,self.cutoffs,CohortWeight)\n\n            # Calculate the wealth quintile distribution of \"hand to mouth\" consumers\n            quintile_cuts = getPercentiles(aLvl,weights=CohortWeight,percentiles=[0.2, 0.4, 0.6, 0.8])\n            wealth_quintiles = np.ones(aLvl.size,dtype=int)\n            wealth_quintiles[aLvl > quintile_cuts[0]] = 2\n            wealth_quintiles[aLvl > quintile_cuts[1]] = 3\n            wealth_quintiles[aLvl > quintile_cuts[2]] = 4\n            wealth_quintiles[aLvl > quintile_cuts[3]] = 5\n            MPC_cutoff = getPercentiles(MPCannual,weights=CohortWeight,percentiles=[2.0/3.0]) # Looking at consumers with MPCs in the top 1/3\n            these = MPCannual > MPC_cutoff\n            in_top_third_MPC = wealth_quintiles[these]\n            temp_weights = CohortWeight[these]\n            hand_to_mouth_total = np.sum(temp_weights)\n            hand_to_mouth_pct = []\n            for q in range(1,6):\n                hand_to_mouth_pct.append(np.sum(temp_weights[in_top_third_MPC == q])/hand_to_mouth_total)\n            self.HandToMouthPct = np.array(hand_to_mouth_pct)\n\n        else: # If we don't want these stats, just put empty values in history\n            self.MPCall = np.nan\n            self.MPCunemployed = np.nan\n            self.MPCemployed = np.nan\n            self.MPCretired = np.nan\n            self.MPCbyWealthRatio = np.nan\n            self.MPCbyIncome = np.nan\n            self.HandToMouthPct = np.nan", "response": "Calculates various statistics about the current population in the economy."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef distributeParams(self,param_name,param_count,center,spread,dist_type):\n        '''\n        Distributes heterogeneous values of one parameter to the AgentTypes in self.agents.\n\n        Parameters\n        ----------\n        param_name : string\n            Name of the parameter to be assigned.\n        param_count : int\n            Number of different values the parameter will take on.\n        center : float\n            A measure of centrality for the distribution of the parameter.\n        spread : float\n            A measure of spread or diffusion for the distribution of the parameter.\n        dist_type : string\n            The type of distribution to be used.  Can be \"lognormal\" or \"uniform\" (can expand).\n\n        Returns\n        -------\n        None\n        '''\n        # Get a list of discrete values for the parameter\n        if dist_type == 'uniform':\n            # If uniform, center is middle of distribution, spread is distance to either edge\n            param_dist = approxUniform(N=param_count,bot=center-spread,top=center+spread)\n        elif dist_type == 'lognormal':\n            # If lognormal, center is the mean and spread is the standard deviation (in log)\n            tail_N = 3\n            param_dist = approxLognormal(N=param_count-tail_N,mu=np.log(center)-0.5*spread**2,sigma=spread,tail_N=tail_N,tail_bound=[0.0,0.9], tail_order=np.e)\n\n        # Distribute the parameters to the various types, assigning consecutive types the same\n        # value if there are more types than values\n        replication_factor = len(self.agents) // param_count \n            # Note: the double division is intenger division in Python 3 and 2.7, this makes it explicit\n        j = 0\n        b = 0\n        while j < len(self.agents):\n            for n in range(replication_factor):\n                self.agents[j](AgentCount = int(self.Population*param_dist[0][b]*self.TypeWeight[n]))\n                exec('self.agents[j](' + param_name + '= param_dist[1][b])')\n                j += 1\n            b += 1", "response": "Distributes heterogeneous values of one parameter to the AgentTypes in self. agents."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calcKYratioDifference(self):\n        '''\n        Returns the difference between the simulated capital to income ratio and the target ratio.\n        Can only be run after solving all AgentTypes and running makeHistory.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        diff : float\n            Difference between simulated and target capital to income ratio.\n        '''\n        # Ignore the first X periods to allow economy to stabilize from initial conditions\n        KYratioSim = np.mean(np.array(self.KtoYnow_hist)[self.ignore_periods:])\n        diff = KYratioSim - self.KYratioTarget\n        return diff", "response": "Calculates the difference between the simulated capital to income ratio and the target capital to income ratio."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calcLorenzDistance(self):\n        '''\n        Returns the sum of squared differences between simulated and target Lorenz points.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        dist : float\n            Sum of squared distances between simulated and target Lorenz points (sqrt)\n        '''\n        LorenzSim = np.mean(np.array(self.Lorenz_hist)[self.ignore_periods:,:],axis=0)\n        dist = np.sqrt(np.sum((100*(LorenzSim - self.LorenzTarget))**2))\n        self.LorenzDistance = dist\n        return dist", "response": "Calculates the sum of squared differences between simulated and target Lorenz points and stores the result in self. LorenzDistance."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndisplays the many statistics for the current species.", "response": "def showManyStats(self,spec_name=None):\n        '''\n        Calculates the \"many statistics\" by averaging histories across simulated periods.  Displays\n        the results as text and saves them to files if spec_name is not None.\n\n        Parameters\n        ----------\n        spec_name : string\n            A name or label for the current specification.\n\n        Returns\n        -------\n        None\n        '''\n        # Calculate MPC overall and by subpopulations\n        MPCall = np.mean(self.MPCall_hist[self.ignore_periods:])\n        MPCemployed = np.mean(self.MPCemployed_hist[self.ignore_periods:])\n        MPCunemployed = np.mean(self.MPCunemployed_hist[self.ignore_periods:])\n        MPCretired = np.mean(self.MPCretired_hist[self.ignore_periods:])\n        MPCbyIncome = np.mean(np.array(self.MPCbyIncome_hist)[self.ignore_periods:,:],axis=0)\n        MPCbyWealthRatio = np.mean(np.array(self.MPCbyWealthRatio_hist)[self.ignore_periods:,:],axis=0)\n        HandToMouthPct = np.mean(np.array(self.HandToMouthPct_hist)[self.ignore_periods:,:],axis=0)\n\n        LorenzSim = np.hstack((np.array(0.0),np.mean(np.array(self.LorenzLong_hist)[self.ignore_periods:,:],axis=0),np.array(1.0)))\n        LorenzAxis = np.arange(101,dtype=float)\n        plt.plot(LorenzAxis,self.LorenzData,'-k',linewidth=1.5)\n        plt.plot(LorenzAxis,LorenzSim,'--k',linewidth=1.5)\n        plt.xlabel('Income percentile',fontsize=12)\n        plt.ylabel('Cumulative wealth share',fontsize=12)\n        plt.ylim([-0.02,1.0])\n        plt.show()\n\n        # Make a string of results to display\n        results_string = 'Estimate is center=' + str(self.center_estimate) + ', spread=' + str(self.spread_estimate) + '\\n'\n        results_string += 'Lorenz distance is ' + str(self.LorenzDistance) + '\\n'\n        results_string += 'Average MPC for all consumers is ' + mystr(MPCall) + '\\n'\n        results_string += 'Average MPC in the top percentile of W/Y is ' + mystr(MPCbyWealthRatio[0]) + '\\n'\n        results_string += 'Average MPC in the top decile of W/Y is ' + mystr(MPCbyWealthRatio[1]) + '\\n'\n        results_string += 'Average MPC in the top quintile of W/Y is ' + mystr(MPCbyWealthRatio[2]) + '\\n'\n        results_string += 'Average MPC in the second quintile of W/Y is ' + mystr(MPCbyWealthRatio[3]) + '\\n'\n        results_string += 'Average MPC in the middle quintile of W/Y is ' + mystr(MPCbyWealthRatio[4]) + '\\n'\n        results_string += 'Average MPC in the fourth quintile of W/Y is ' + mystr(MPCbyWealthRatio[5]) + '\\n'\n        results_string += 'Average MPC in the bottom quintile of W/Y is ' + mystr(MPCbyWealthRatio[6]) + '\\n'\n        results_string += 'Average MPC in the top percentile of y is ' + mystr(MPCbyIncome[0]) + '\\n'\n        results_string += 'Average MPC in the top decile of y is ' + mystr(MPCbyIncome[1]) + '\\n'\n        results_string += 'Average MPC in the top quintile of y is ' + mystr(MPCbyIncome[2]) + '\\n'\n        results_string += 'Average MPC in the second quintile of y is ' + mystr(MPCbyIncome[3]) + '\\n'\n        results_string += 'Average MPC in the middle quintile of y is ' + mystr(MPCbyIncome[4]) + '\\n'\n        results_string += 'Average MPC in the fourth quintile of y is ' + mystr(MPCbyIncome[5]) + '\\n'\n        results_string += 'Average MPC in the bottom quintile of y is ' + mystr(MPCbyIncome[6]) + '\\n'\n        results_string += 'Average MPC for the employed is ' + mystr(MPCemployed) + '\\n'\n        results_string += 'Average MPC for the unemployed is ' + mystr(MPCunemployed) + '\\n'\n        results_string += 'Average MPC for the retired is ' + mystr(MPCretired) + '\\n'\n        results_string += 'Of the population with the 1/3 highest MPCs...' + '\\n'\n        results_string += mystr(HandToMouthPct[0]*100) + '% are in the bottom wealth quintile,' + '\\n'\n        results_string += mystr(HandToMouthPct[1]*100) + '% are in the second wealth quintile,' + '\\n'\n        results_string += mystr(HandToMouthPct[2]*100) + '% are in the third wealth quintile,' + '\\n'\n        results_string += mystr(HandToMouthPct[3]*100) + '% are in the fourth wealth quintile,' + '\\n'\n        results_string += 'and ' + mystr(HandToMouthPct[4]*100) + '% are in the top wealth quintile.' + '\\n'\n        print(results_string)\n\n        # Save results to disk\n        if spec_name is not None:\n            with open(self.my_file_path  + '/Results/' + spec_name + 'Results.txt','w') as f:\n                f.write(results_string)\n                f.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef solveConsGenIncProcess(solution_next,IncomeDstn,LivPrb,DiscFac,CRRA,Rfree,pLvlNextFunc,\n                                BoroCnstArt,aXtraGrid,pLvlGrid,vFuncBool,CubicBool):\n    '''\n    Solves the one period problem of a consumer who experiences persistent and\n    transitory shocks to his income.  Unlike in ConsIndShock, consumers do not\n    necessarily have expected persistent income growth that is constant with respect\n    to their current level of pLvl.  Instead, they have a function that translates\n    current pLvl into expected next period pLvl (subject to shocks).\n\n    Parameters\n    ----------\n    solution_next : ConsumerSolution\n        The solution to next period's one period problem.\n    IncomeDstn : [np.array]\n        A list containing three arrays of floats, representing a discrete\n        approximation to the income process between the period being solved\n        and the one immediately following (in solution_next). Order: event\n        probabilities, persistent shocks, transitory shocks.\n    LivPrb : float\n        Survival probability; likelihood of being alive at the beginning of\n        the succeeding period.\n    DiscFac : float\n        Intertemporal discount factor for future utility.\n    CRRA : float\n        Coefficient of relative risk aversion.\n    Rfree : float\n        Risk free interest factor on end-of-period assets.\n    pLvlNextFunc : float\n        Expected persistent income next period as a function of current pLvl.\n    BoroCnstArt: float or None\n        Borrowing constraint for the minimum allowable assets to end the\n        period with.  Currently ignored, with BoroCnstArt=0 used implicitly.\n    aXtraGrid: np.array\n        Array of \"extra\" end-of-period (normalized) asset values-- assets\n        above the absolute minimum acceptable level.\n    pLvlGrid: np.array\n        Array of persistent income levels at which to solve the problem.\n    vFuncBool: boolean\n        An indicator for whether the value function should be computed and\n        included in the reported solution.\n    CubicBool: boolean\n        An indicator for whether the solver should use cubic or linear interpolation.\n\n    Returns\n    -------\n    solution : ConsumerSolution\n            The solution to the one period problem, including a consumption\n            function (defined over market resources and persistent income), a\n            marginal value function, bounding MPCs, and normalized human wealth.\n    '''\n    solver = ConsGenIncProcessSolver(solution_next,IncomeDstn,LivPrb,DiscFac,CRRA,Rfree,\n                            pLvlNextFunc,BoroCnstArt,aXtraGrid,pLvlGrid,vFuncBool,CubicBool)\n    solver.prepareToSolve()       # Do some preparatory work\n    solution_now = solver.solve() # Solve the period\n    return solution_now", "response": "Solves a single consumer with a discrete income process between the next period and the current income growth."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef derivativeX(self,m,p):\n        '''\n        Evaluate the first derivative with respect to market resources of the\n        marginal value function at given levels of market resources m and per-\n        manent income p.\n\n        Parameters\n        ----------\n        m : float or np.array\n            Market resources whose value is to be calcuated.\n        p : float or np.array\n            Persistent income levels whose value is to be calculated.\n\n        Returns\n        -------\n        vPP : float or np.array\n            Marginal marginal value of market resources when beginning this period\n            with market resources m and persistent income p; has same size as inputs\n            m and p.\n        '''\n        c = self.cFunc(m,p)\n        MPC = self.cFunc.derivativeX(m,p)\n        return MPC*utilityPP(c,gam=self.CRRA)", "response": "Evaluate the first derivative with respect to market resources of the marginal value function at given levels of market resources m and persistent income levels p."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setAndUpdateValues(self,solution_next,IncomeDstn,LivPrb,DiscFac):\n        '''\n        Unpacks some of the inputs (and calculates simple objects based on them),\n        storing the results in self for use by other methods.  These include:\n        income shocks and probabilities, next period's marginal value function\n        (etc), the probability of getting the worst income shock next period,\n        the patience factor, human wealth, and the bounding MPCs.  Human wealth\n        is stored as a function of persistent income.\n\n        Parameters\n        ----------\n        solution_next : ConsumerSolution\n            The solution to next period's one period problem.\n        IncomeDstn : [np.array]\n            A list containing three arrays of floats, representing a discrete\n            approximation to the income process between the period being solved\n            and the one immediately following (in solution_next). Order: event\n            probabilities, persistent shocks, transitory shocks.\n        LivPrb : float\n            Survival probability; likelihood of being alive at the beginning of\n            the succeeding period.\n        DiscFac : float\n            Intertemporal discount factor for future utility.\n\n        Returns\n        -------\n        None\n        '''\n        # Run basic version of this method\n        ConsIndShockSetup.setAndUpdateValues(self,solution_next,IncomeDstn,LivPrb,DiscFac)\n        self.mLvlMinNext = solution_next.mLvlMin\n\n        # Replace normalized human wealth (scalar) with human wealth level as function of persistent income\n        self.hNrmNow = 0.0\n        pLvlCount    = self.pLvlGrid.size\n        IncShkCount  = self.PermShkValsNext.size\n        pLvlNext  = np.tile(self.pLvlNextFunc(self.pLvlGrid),(IncShkCount,1))*np.tile(self.PermShkValsNext,(pLvlCount,1)).transpose()\n        hLvlGrid     = 1.0/self.Rfree*np.sum((np.tile(self.TranShkValsNext,(pLvlCount,1)).transpose()*pLvlNext + solution_next.hLvl(pLvlNext))*np.tile(self.ShkPrbsNext,(pLvlCount,1)).transpose(),axis=0)\n        self.hLvlNow = LinearInterp(np.insert(self.pLvlGrid,0,0.0),np.insert(hLvlGrid,0,0.0))", "response": "This method sets the internal state of the object based on the given solution_next and returns the object representation of the object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndefine the constrained portion of the consumption function as cFuncNowCnst and an attribute of self. BoroCnstNat and self. BoroCnstArt.", "response": "def defBoroCnst(self,BoroCnstArt):\n        '''\n        Defines the constrained portion of the consumption function as cFuncNowCnst,\n        an attribute of self.\n\n        Parameters\n        ----------\n        BoroCnstArt : float or None\n            Borrowing constraint for the minimum allowable assets to end the\n            period with.  If it is less than the natural borrowing constraint,\n            then it is irrelevant; BoroCnstArt=None indicates no artificial bor-\n            rowing constraint.\n\n        Returns\n        -------\n        None\n        '''\n        # Make temporary grids of income shocks and next period income values\n        ShkCount = self.TranShkValsNext.size\n        pLvlCount = self.pLvlGrid.size\n        PermShkVals_temp = np.tile(np.reshape(self.PermShkValsNext,(1,ShkCount)),(pLvlCount,1))\n        TranShkVals_temp = np.tile(np.reshape(self.TranShkValsNext,(1,ShkCount)),(pLvlCount,1))\n        pLvlNext_temp = np.tile(np.reshape(self.pLvlNextFunc(self.pLvlGrid),(pLvlCount,1)),(1,ShkCount))*PermShkVals_temp\n\n        # Find the natural borrowing constraint for each persistent income level\n        aLvlMin_candidates = (self.mLvlMinNext(pLvlNext_temp) - TranShkVals_temp*pLvlNext_temp)/self.Rfree\n        aLvlMinNow = np.max(aLvlMin_candidates,axis=1)\n        self.BoroCnstNat = LinearInterp(np.insert(self.pLvlGrid,0,0.0),np.insert(aLvlMinNow,0,0.0))\n\n        # Define the minimum allowable mLvl by pLvl as the greater of the natural and artificial borrowing constraints\n        if self.BoroCnstArt is not None:\n            self.BoroCnstArt = LinearInterp(np.array([0.0,1.0]),np.array([0.0,self.BoroCnstArt]))\n            self.mLvlMinNow = UpperEnvelope(self.BoroCnstArt,self.BoroCnstNat)\n        else:\n            self.mLvlMinNow = self.BoroCnstNat\n\n        # Define the constrained consumption function as \"consume all\" shifted by mLvlMin\n        cFuncNowCnstBase = BilinearInterp(np.array([[0.,0.],[1.,1.]]),np.array([0.0,1.0]),np.array([0.0,1.0]))\n        self.cFuncNowCnst = VariableLowerBoundFunc2D(cFuncNowCnstBase,self.mLvlMinNow)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprepare to calculate the end - of - period marginal value for the agent.", "response": "def prepareToCalcEndOfPrdvP(self):\n        '''\n        Prepare to calculate end-of-period marginal value by creating an array\n        of market resources that the agent could have next period, considering\n        the grid of end-of-period normalized assets, the grid of persistent income\n        levels, and the distribution of shocks he might experience next period.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        aLvlNow : np.array\n            2D array of end-of-period assets; also stored as attribute of self.\n        pLvlNow : np.array\n            2D array of persistent income levels this period.\n        '''\n        ShkCount    = self.TranShkValsNext.size\n        pLvlCount   = self.pLvlGrid.size\n        aNrmCount   = self.aXtraGrid.size\n        pLvlNow     = np.tile(self.pLvlGrid,(aNrmCount,1)).transpose()\n        aLvlNow     = np.tile(self.aXtraGrid,(pLvlCount,1))*pLvlNow + self.BoroCnstNat(pLvlNow)\n        pLvlNow_tiled = np.tile(pLvlNow,(ShkCount,1,1))\n        aLvlNow_tiled = np.tile(aLvlNow,(ShkCount,1,1)) # shape = (ShkCount,pLvlCount,aNrmCount)\n        if self.pLvlGrid[0] == 0.0:  # aLvl turns out badly if pLvl is 0 at bottom\n            aLvlNow[0,:] = self.aXtraGrid\n            aLvlNow_tiled[:,0,:] = np.tile(self.aXtraGrid,(ShkCount,1))\n\n        # Tile arrays of the income shocks and put them into useful shapes\n        PermShkVals_tiled = np.transpose(np.tile(self.PermShkValsNext,(aNrmCount,pLvlCount,1)),(2,1,0))\n        TranShkVals_tiled = np.transpose(np.tile(self.TranShkValsNext,(aNrmCount,pLvlCount,1)),(2,1,0))\n        ShkPrbs_tiled     = np.transpose(np.tile(self.ShkPrbsNext,(aNrmCount,pLvlCount,1)),(2,1,0))\n\n        # Get cash on hand next period\n        pLvlNext = self.pLvlNextFunc(pLvlNow_tiled)*PermShkVals_tiled\n        mLvlNext = self.Rfree*aLvlNow_tiled + pLvlNext*TranShkVals_tiled\n\n        # Store and report the results\n        self.ShkPrbs_temp      = ShkPrbs_tiled\n        self.pLvlNext          = pLvlNext\n        self.mLvlNext          = mLvlNext\n        self.aLvlNow           = aLvlNow\n        return aLvlNow, pLvlNow"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calcEndOfPrdvP(self):\n        '''\n        Calculates end-of-period marginal value of assets at each state space\n        point in aLvlNow x pLvlNow. Does so by taking a weighted sum of next\n        period marginal values across income shocks (in preconstructed grids\n        self.mLvlNext x self.pLvlNext).\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        EndOfPrdVP : np.array\n            A 2D array of end-of-period marginal value of assets.\n        '''\n        EndOfPrdvP  = self.DiscFacEff*self.Rfree*np.sum(self.vPfuncNext(self.mLvlNext,self.pLvlNext)*self.ShkPrbs_temp,axis=0)\n        return EndOfPrdvP", "response": "Calculates the end - of - period marginal value of assets at each state space at each income shock at each state space\n        point in aLvlNow x pLvlNow."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef makeEndOfPrdvFunc(self,EndOfPrdvP):\n        '''\n        Construct the end-of-period value function for this period, storing it\n        as an attribute of self for use by other methods.\n\n        Parameters\n        ----------\n        EndOfPrdvP : np.array\n            Array of end-of-period marginal value of assets corresponding to the\n            asset values in self.aLvlNow x self.pLvlGrid.\n\n        Returns\n        -------\n        none\n        '''\n        vLvlNext            = self.vFuncNext(self.mLvlNext,self.pLvlNext) # value in many possible future states\n        EndOfPrdv           = self.DiscFacEff*np.sum(vLvlNext*self.ShkPrbs_temp,axis=0) # expected value, averaging across states\n        EndOfPrdvNvrs       = self.uinv(EndOfPrdv) # value transformed through inverse utility\n        EndOfPrdvNvrsP      = EndOfPrdvP*self.uinvP(EndOfPrdv)\n\n        # Add points at mLvl=zero\n        EndOfPrdvNvrs      = np.concatenate((np.zeros((self.pLvlGrid.size,1)),EndOfPrdvNvrs),axis=1)\n        if hasattr(self,'MedShkDstn'):\n            EndOfPrdvNvrsP = np.concatenate((np.zeros((self.pLvlGrid.size,1)),EndOfPrdvNvrsP),axis=1)\n        else:\n            EndOfPrdvNvrsP = np.concatenate((np.reshape(EndOfPrdvNvrsP[:,0],(self.pLvlGrid.size,1)),EndOfPrdvNvrsP),axis=1) # This is a very good approximation, vNvrsPP = 0 at the asset minimum\n        aLvl_temp          = np.concatenate((np.reshape(self.BoroCnstNat(self.pLvlGrid),(self.pLvlGrid.size,1)),self.aLvlNow),axis=1)\n\n        # Make an end-of-period value function for each persistent income level in the grid\n        EndOfPrdvNvrsFunc_list = []\n        for p in range(self.pLvlGrid.size):\n            EndOfPrdvNvrsFunc_list.append(CubicInterp(aLvl_temp[p,:]-self.BoroCnstNat(self.pLvlGrid[p]),EndOfPrdvNvrs[p,:],EndOfPrdvNvrsP[p,:]))\n        EndOfPrdvNvrsFuncBase = LinearInterpOnInterp1D(EndOfPrdvNvrsFunc_list,self.pLvlGrid)\n\n        # Re-adjust the combined end-of-period value function to account for the natural borrowing constraint shifter\n        EndOfPrdvNvrsFunc     = VariableLowerBoundFunc2D(EndOfPrdvNvrsFuncBase,self.BoroCnstNat)\n        self.EndOfPrdvFunc    = ValueFunc2D(EndOfPrdvNvrsFunc,self.CRRA)", "response": "Constructs the end - of - period value function for this period storing it\n        EndOfPrdvP as an attribute of self for use by other methods. This function is called by the base class makeEndOfPrdvFunc and the base class makeEndOfPrdvP are called to create the end - of - period value function for this period."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding endogenous interpolation points (c,m) for the consumption function. Parameters ---------- EndOfPrdvP : np.array Array of end-of-period marginal values. aLvlNow : np.array Array of end-of-period asset values that yield the marginal values in EndOfPrdvP. Returns ------- c_for_interpolation : np.array Consumption points for interpolation. m_for_interpolation : np.array Corresponding market resource points for interpolation.", "response": "def getPointsForInterpolation(self,EndOfPrdvP,aLvlNow):\n        '''\n        Finds endogenous interpolation points (c,m) for the consumption function.\n\n        Parameters\n        ----------\n        EndOfPrdvP : np.array\n            Array of end-of-period marginal values.\n        aLvlNow : np.array\n            Array of end-of-period asset values that yield the marginal values\n            in EndOfPrdvP.\n\n        Returns\n        -------\n        c_for_interpolation : np.array\n            Consumption points for interpolation.\n        m_for_interpolation : np.array\n            Corresponding market resource points for interpolation.\n        '''\n        cLvlNow = self.uPinv(EndOfPrdvP)\n        mLvlNow = cLvlNow + aLvlNow\n\n        # Limiting consumption is zero as m approaches mNrmMin\n        c_for_interpolation = np.concatenate((np.zeros((self.pLvlGrid.size,1)),cLvlNow),axis=-1)\n        m_for_interpolation = np.concatenate((self.BoroCnstNat(np.reshape(self.pLvlGrid,(self.pLvlGrid.size,1))),mLvlNow),axis=-1)\n\n        # Limiting consumption is MPCmin*mLvl as p approaches 0\n        m_temp = np.reshape(m_for_interpolation[0,:],(1,m_for_interpolation.shape[1]))\n        m_for_interpolation = np.concatenate((m_temp,m_for_interpolation),axis=0)\n        c_for_interpolation = np.concatenate((self.MPCminNow*m_temp,c_for_interpolation),axis=0)\n\n        return c_for_interpolation, m_for_interpolation"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef usePointsForInterpolation(self,cLvl,mLvl,pLvl,interpolator):\n        '''\n        Constructs a basic solution for this period, including the consumption\n        function and marginal value function.\n\n        Parameters\n        ----------\n        cLvl : np.array\n            Consumption points for interpolation.\n        mLvl : np.array\n            Corresponding market resource points for interpolation.\n        pLvl : np.array\n            Corresponding persistent income level points for interpolation.\n        interpolator : function\n            A function that constructs and returns a consumption function.\n\n        Returns\n        -------\n        solution_now : ConsumerSolution\n            The solution to this period's consumption-saving problem, with a\n            consumption function, marginal value function, and minimum m.\n        '''\n        # Construct the unconstrained consumption function\n        cFuncNowUnc = interpolator(mLvl,pLvl,cLvl)\n\n        # Combine the constrained and unconstrained functions into the true consumption function\n        cFuncNow = LowerEnvelope2D(cFuncNowUnc,self.cFuncNowCnst)\n\n        # Make the marginal value function\n        vPfuncNow = self.makevPfunc(cFuncNow)\n\n        # Pack up the solution and return it\n        solution_now = ConsumerSolution(cFunc=cFuncNow, vPfunc=vPfuncNow, mNrmMin=0.0)\n        return solution_now", "response": "Constructs a basic solution for this period including the consumption function and marginal value function and minimum m."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef makevFunc(self,solution):\n        '''\n        Creates the value function for this period, defined over market resources\n        m and persistent income p.  self must have the attribute EndOfPrdvFunc in\n        order to execute.\n\n        Parameters\n        ----------\n        solution : ConsumerSolution\n            The solution to this single period problem, which must include the\n            consumption function.\n\n        Returns\n        -------\n        vFuncNow : ValueFunc\n            A representation of the value function for this period, defined over\n            market resources m and persistent income p: v = vFuncNow(m,p).\n        '''\n        mSize = self.aXtraGrid.size\n        pSize = self.pLvlGrid.size\n\n        # Compute expected value and marginal value on a grid of market resources\n        pLvl_temp   = np.tile(self.pLvlGrid,(mSize,1)) # Tile pLvl across m values\n        mLvl_temp   = np.tile(self.mLvlMinNow(self.pLvlGrid),(mSize,1)) + np.tile(np.reshape(self.aXtraGrid,(mSize,1)),(1,pSize))*pLvl_temp\n        cLvlNow     = solution.cFunc(mLvl_temp,pLvl_temp)\n        aLvlNow     = mLvl_temp - cLvlNow\n        vNow        = self.u(cLvlNow) + self.EndOfPrdvFunc(aLvlNow,pLvl_temp)\n        vPnow       = self.uP(cLvlNow)\n\n        # Calculate pseudo-inverse value and its first derivative (wrt mLvl)\n        vNvrs        = self.uinv(vNow) # value transformed through inverse utility\n        vNvrsP       = vPnow*self.uinvP(vNow)\n\n        # Add data at the lower bound of m\n        mLvl_temp    = np.concatenate((np.reshape(self.mLvlMinNow(self.pLvlGrid),(1,pSize)),mLvl_temp),axis=0)\n        vNvrs        = np.concatenate((np.zeros((1,pSize)),vNvrs),axis=0)\n        vNvrsP       = np.concatenate((np.reshape(vNvrsP[0,:],(1,vNvrsP.shape[1])),vNvrsP),axis=0)\n\n        # Add data at the lower bound of p\n        MPCminNvrs   = self.MPCminNow**(-self.CRRA/(1.0-self.CRRA))\n        m_temp = np.reshape(mLvl_temp[:,0],(mSize+1,1))\n        mLvl_temp    = np.concatenate((m_temp,mLvl_temp),axis=1)\n        vNvrs        = np.concatenate((MPCminNvrs*m_temp,vNvrs),axis=1)\n        vNvrsP       = np.concatenate((MPCminNvrs*np.ones((mSize+1,1)),vNvrsP),axis=1)\n\n        # Construct the pseudo-inverse value function\n        vNvrsFunc_list = []\n        for j in range(pSize+1):\n            pLvl = np.insert(self.pLvlGrid,0,0.0)[j]\n            vNvrsFunc_list.append(CubicInterp(mLvl_temp[:,j]-self.mLvlMinNow(pLvl),vNvrs[:,j],vNvrsP[:,j],MPCminNvrs*self.hLvlNow(pLvl),MPCminNvrs))\n        vNvrsFuncBase = LinearInterpOnInterp1D(vNvrsFunc_list,np.insert(self.pLvlGrid,0,0.0)) # Value function \"shifted\"\n        vNvrsFuncNow  = VariableLowerBoundFunc2D(vNvrsFuncBase,self.mLvlMinNow)\n\n        # \"Re-curve\" the pseudo-inverse value function into the value function\n        vFuncNow     = ValueFunc2D(vNvrsFuncNow,self.CRRA)\n        return vFuncNow", "response": "Creates the value function for this single period."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef makeBasicSolution(self,EndOfPrdvP,aLvl,pLvl,interpolator):\n        '''\n        Given end of period assets and end of period marginal value, construct\n        the basic solution for this period.\n\n        Parameters\n        ----------\n        EndOfPrdvP : np.array\n            Array of end-of-period marginal values.\n        aLvl : np.array\n            Array of end-of-period asset values that yield the marginal values\n            in EndOfPrdvP.\n        pLvl : np.array\n            Array of persistent income levels that yield the marginal values\n            in EndOfPrdvP (corresponding pointwise to aLvl).\n        interpolator : function\n            A function that constructs and returns a consumption function.\n\n        Returns\n        -------\n        solution_now : ConsumerSolution\n            The solution to this period's consumption-saving problem, with a\n            consumption function, marginal value function, and minimum m.\n        '''\n        cLvl,mLvl    = self.getPointsForInterpolation(EndOfPrdvP,aLvl)\n        pLvl_temp    = np.concatenate((np.reshape(self.pLvlGrid,(self.pLvlGrid.size,1)),pLvl),axis=-1)\n        pLvl_temp    = np.concatenate((np.zeros((1,mLvl.shape[1])),pLvl_temp))\n        solution_now = self.usePointsForInterpolation(cLvl,mLvl,pLvl_temp,interpolator)\n        return solution_now", "response": "Given end - of - period assets and end - of - period marginal values construct a basic solution for this period."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef makeLinearcFunc(self,mLvl,pLvl,cLvl):\n        '''\n        Makes a quasi-bilinear interpolation to represent the (unconstrained)\n        consumption function.\n\n        Parameters\n        ----------\n        mLvl : np.array\n            Market resource points for interpolation.\n        pLvl : np.array\n            Persistent income level points for interpolation.\n        cLvl : np.array\n            Consumption points for interpolation.\n\n        Returns\n        -------\n        cFuncUnc : LinearInterp\n            The unconstrained consumption function for this period.\n        '''\n        cFunc_by_pLvl_list = [] # list of consumption functions for each pLvl\n        for j in range(pLvl.shape[0]):\n            pLvl_j = pLvl[j,0]\n            m_temp = mLvl[j,:] - self.BoroCnstNat(pLvl_j)\n            c_temp = cLvl[j,:] # Make a linear consumption function for this pLvl\n            if pLvl_j > 0:\n                cFunc_by_pLvl_list.append(LinearInterp(m_temp,c_temp,lower_extrap=True,slope_limit=self.MPCminNow,intercept_limit=self.MPCminNow*self.hLvlNow(pLvl_j)))\n            else:\n                cFunc_by_pLvl_list.append(LinearInterp(m_temp,c_temp,lower_extrap=True))\n        pLvl_list = pLvl[:,0]\n        cFuncUncBase = LinearInterpOnInterp1D(cFunc_by_pLvl_list,pLvl_list) # Combine all linear cFuncs\n        cFuncUnc     = VariableLowerBoundFunc2D(cFuncUncBase,self.BoroCnstNat) # Re-adjust for natural borrowing constraint (as lower bound)\n        return cFuncUnc", "response": "Makes a quasi - bilinear interpolation to represent the unconstrained consumption function for this period."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef makeCubiccFunc(self,mLvl,pLvl,cLvl):\n        '''\n        Makes a quasi-cubic spline interpolation of the unconstrained consumption\n        function for this period.  Function is cubic splines with respect to mLvl,\n        but linear in pLvl.\n\n        Parameters\n        ----------\n        mLvl : np.array\n            Market resource points for interpolation.\n        pLvl : np.array\n            Persistent income level points for interpolation.\n        cLvl : np.array\n            Consumption points for interpolation.\n\n        Returns\n        -------\n        cFuncUnc : CubicInterp\n            The unconstrained consumption function for this period.\n        '''\n        # Calculate the MPC at each gridpoint\n        EndOfPrdvPP = self.DiscFacEff*self.Rfree*self.Rfree*np.sum(self.vPPfuncNext(self.mLvlNext,self.pLvlNext)*self.ShkPrbs_temp,axis=0)\n        dcda        = EndOfPrdvPP/self.uPP(np.array(cLvl[1:,1:]))\n        MPC         = dcda/(dcda+1.)\n        MPC         = np.concatenate((np.reshape(MPC[:,0],(MPC.shape[0],1)),MPC),axis=1) # Stick an extra MPC value at bottom; MPCmax doesn't work\n        MPC         = np.concatenate((self.MPCminNow*np.ones((1,self.aXtraGrid.size+1)),MPC),axis=0)\n\n        # Make cubic consumption function with respect to mLvl for each persistent income level\n        cFunc_by_pLvl_list = [] # list of consumption functions for each pLvl\n        for j in range(pLvl.shape[0]):\n            pLvl_j = pLvl[j,0]\n            m_temp = mLvl[j,:] - self.BoroCnstNat(pLvl_j)\n            c_temp = cLvl[j,:] # Make a cubic consumption function for this pLvl\n            MPC_temp = MPC[j,:]\n            if pLvl_j > 0:\n                cFunc_by_pLvl_list.append(CubicInterp(m_temp,c_temp,MPC_temp,lower_extrap=True,slope_limit=self.MPCminNow,intercept_limit=self.MPCminNow*self.hLvlNow(pLvl_j)))\n            else: # When pLvl=0, cFunc is linear\n                cFunc_by_pLvl_list.append(LinearInterp(m_temp,c_temp,lower_extrap=True))\n        pLvl_list = pLvl[:,0]\n        cFuncUncBase = LinearInterpOnInterp1D(cFunc_by_pLvl_list,pLvl_list) # Combine all linear cFuncs\n        cFuncUnc     = VariableLowerBoundFunc2D(cFuncUncBase,self.BoroCnstNat) # Re-adjust for lower bound of natural borrowing constraint\n        return cFuncUnc", "response": "Makes a cubic interpolation of the unconstrained consumption function for this period."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntaking a solution and add human wealth and the bounding MPCs to it.", "response": "def addMPCandHumanWealth(self,solution):\n        '''\n        Take a solution and add human wealth and the bounding MPCs to it.\n\n        Parameters\n        ----------\n        solution : ConsumerSolution\n            The solution to this period's consumption-saving problem.\n\n        Returns:\n        ----------\n        solution : ConsumerSolution\n            The solution to this period's consumption-saving problem, but now\n            with human wealth and the bounding MPCs.\n        '''\n        solution.hNrm   = 0.0 # Can't have None or setAndUpdateValues breaks, should fix\n        solution.hLvl   = self.hLvlNow\n        solution.mLvlMin= self.mLvlMinNow\n        solution.MPCmin = self.MPCminNow\n        solution.MPCmax = 0.0 # MPCmax is actually a function in this model\n        return solution"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef addvPPfunc(self,solution):\n        '''\n        Adds the marginal marginal value function to an existing solution, so\n        that the next solver can evaluate vPP and thus use cubic interpolation.\n\n        Parameters\n        ----------\n        solution : ConsumerSolution\n            The solution to this single period problem, which must include the\n            consumption function.\n\n        Returns\n        -------\n        solution : ConsumerSolution\n            The same solution passed as input, but with the marginal marginal\n            value function for this period added as the attribute vPPfunc.\n        '''\n        vPPfuncNow        = MargMargValueFunc2D(solution.cFunc,self.CRRA)\n        solution.vPPfunc  = vPPfuncNow\n        return solution", "response": "Adds the marginal value function to an existing solution so that the next solver can evaluate vPP."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef solve(self):\n        '''\n        Solves a one period consumption saving problem with risky income, with\n        persistent income explicitly tracked as a state variable.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        solution : ConsumerSolution\n            The solution to the one period problem, including a consumption\n            function (defined over market resources and persistent income), a\n            marginal value function, bounding MPCs, and human wealth as a func-\n            tion of persistent income.  Might also include a value function and\n            marginal marginal value function, depending on options selected.\n        '''\n        aLvl,pLvl  = self.prepareToCalcEndOfPrdvP()\n        EndOfPrdvP = self.calcEndOfPrdvP()\n        if self.vFuncBool:\n            self.makeEndOfPrdvFunc(EndOfPrdvP)\n        if self.CubicBool:\n            interpolator = self.makeCubiccFunc\n        else:\n            interpolator = self.makeLinearcFunc\n        solution   = self.makeBasicSolution(EndOfPrdvP,aLvl,pLvl,interpolator)\n        solution   = self.addMPCandHumanWealth(solution)\n        if self.vFuncBool:\n            solution.vFunc = self.makevFunc(solution)\n        if self.CubicBool:\n            solution = self.addvPPfunc(solution)\n        return solution", "response": "Solves a one period consumption saving problem with risky income with persistent income explicitly tracked as a state variable."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef updateSolutionTerminal(self):\n        '''\n        Update the terminal period solution.  This method should be run when a\n        new AgentType is created or when CRRA changes.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        self.solution_terminal.vFunc = ValueFunc2D(self.cFunc_terminal_,self.CRRA)\n        self.solution_terminal.vPfunc = MargValueFunc2D(self.cFunc_terminal_,self.CRRA)\n        self.solution_terminal.vPPfunc = MargMargValueFunc2D(self.cFunc_terminal_,self.CRRA)\n        self.solution_terminal.hNrm = 0.0 # Don't track normalized human wealth\n        self.solution_terminal.hLvl = lambda p : np.zeros_like(p) # But do track absolute human wealth by persistent income\n        self.solution_terminal.mLvlMin = lambda p : np.zeros_like(p)", "response": "Update the terminal period solution."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef installRetirementFunc(self):\n        '''\n        Installs a special pLvlNextFunc representing retirement in the correct\n        element of self.pLvlNextFunc.  Draws on the attributes T_retire and\n        pLvlNextFuncRet.  If T_retire is zero or pLvlNextFuncRet does not\n        exist, this method does nothing.  Should only be called from within the\n        method updatepLvlNextFunc, which ensures that time is flowing forward.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        if (not hasattr(self,'pLvlNextFuncRet')) or self.T_retire == 0:\n            return\n        t = self.T_retire\n        self.pLvlNextFunc[t] = self.pLvlNextFuncRet", "response": "Installs a special pLvlNextFunc representing retirement in the correct\n        element of self. pLvlNextFunc."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef updatepLvlGrid(self):\n        '''\n        Update the grid of persistent income levels.  Currently only works for\n        infinite horizon models (cycles=0) and lifecycle models (cycles=1).  Not\n        clear what to do about cycles>1 because the distribution of persistent\n        income will be different within a period depending on how many cycles\n        have elapsed.  This method uses a simulation approach to generate the\n        pLvlGrid at each period of the cycle, drawing on the initial distribution\n        of persistent income, the pLvlNextFuncs, and the attribute pLvlPctiles.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        orig_time = self.time_flow\n        self.timeFwd()\n        LivPrbAll = np.array(self.LivPrb)\n\n        # Simulate the distribution of persistent income levels by t_cycle in a lifecycle model\n        if self.cycles == 1:\n            pLvlNow = drawLognormal(self.AgentCount,mu=self.pLvlInitMean,sigma=self.pLvlInitStd,seed=31382)\n            pLvlGrid = [] # empty list of time-varying persistent income grids\n            # Calculate distribution of persistent income in each period of lifecycle\n            for t in range(len(self.PermShkStd)):\n                if t > 0:\n                    PermShkNow = drawDiscrete(N=self.AgentCount,P=self.PermShkDstn[t-1][0],X=self.PermShkDstn[t-1][1],exact_match=False,seed=t)\n                    pLvlNow = self.pLvlNextFunc[t-1](pLvlNow)*PermShkNow\n                pLvlGrid.append(getPercentiles(pLvlNow,percentiles=self.pLvlPctiles))\n\n        # Calculate \"stationary\" distribution in infinite horizon (might vary across periods of cycle)\n        elif self.cycles == 0:\n            T_long = 1000 # Number of periods to simulate to get to \"stationary\" distribution\n            pLvlNow = drawLognormal(self.AgentCount,mu=self.pLvlInitMean,sigma=self.pLvlInitStd,seed=31382)\n            t_cycle = np.zeros(self.AgentCount,dtype=int)\n            for t in range(T_long):\n                LivPrb = LivPrbAll[t_cycle] # Determine who dies and replace them with newborns\n                draws = drawUniform(self.AgentCount,seed=t)\n                who_dies = draws > LivPrb\n                pLvlNow[who_dies] = drawLognormal(np.sum(who_dies),mu=self.pLvlInitMean,sigma=self.pLvlInitStd,seed=t+92615)\n                t_cycle[who_dies] = 0\n                for j in range(self.T_cycle): # Update persistent income\n                    these = t_cycle == j\n                    PermShkTemp = drawDiscrete(N=np.sum(these),P=self.PermShkDstn[j][0],X=self.PermShkDstn[j][1],exact_match=False,seed=t+13*j)\n                    pLvlNow[these] = self.pLvlNextFunc[j](pLvlNow[these])*PermShkTemp\n                t_cycle = t_cycle + 1\n                t_cycle[t_cycle == self.T_cycle] = 0\n\n            # We now have a \"long run stationary distribution\", extract percentiles\n            pLvlGrid = [] # empty list of time-varying persistent income grids\n            for t in range(self.T_cycle):\n                these = t_cycle == t\n                pLvlGrid.append(getPercentiles(pLvlNow[these],percentiles=self.pLvlPctiles))\n\n        # Throw an error if cycles>1\n        else:\n            assert False, \"Can only handle cycles=0 or cycles=1!\"\n\n        # Store the result and add attribute to time_vary\n        self.pLvlGrid = pLvlGrid\n        self.addToTimeVary('pLvlGrid')\n        if not orig_time:\n            self.timeRev()", "response": "Updates the grid of persistent income levels for the given set of income in each cycle in the lifecycle model. This method is used to update the grid of persistent income levels for the given set of income in each cycle in the model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmake new consumers for the given indices.", "response": "def simBirth(self,which_agents):\n        '''\n        Makes new consumers for the given indices.  Initialized variables include aNrm and pLvl, as\n        well as time variables t_age and t_cycle.  Normalized assets and persistent income levels\n        are drawn from lognormal distributions given by aNrmInitMean and aNrmInitStd (etc).\n\n        Parameters\n        ----------\n        which_agents : np.array(Bool)\n            Boolean array of size self.AgentCount indicating which agents should be \"born\".\n\n        Returns\n        -------\n        None\n        '''\n        # Get and store states for newly born agents\n        N = np.sum(which_agents) # Number of new consumers to make\n        aNrmNow_new = drawLognormal(N,mu=self.aNrmInitMean,sigma=self.aNrmInitStd,seed=self.RNG.randint(0,2**31-1))\n        self.pLvlNow[which_agents] = drawLognormal(N,mu=self.pLvlInitMean,sigma=self.pLvlInitStd,seed=self.RNG.randint(0,2**31-1))\n        self.aLvlNow[which_agents] = aNrmNow_new*self.pLvlNow[which_agents]\n        self.t_age[which_agents]   = 0 # How many periods since each agent was born\n        self.t_cycle[which_agents] = 0"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getStates(self):\n        '''\n        Calculates updated values of normalized market resources and persistent income level for each\n        agent.  Uses pLvlNow, aLvlNow, PermShkNow, TranShkNow.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        aLvlPrev = self.aLvlNow\n        RfreeNow = self.getRfree()\n\n        # Calculate new states: normalized market resources and persistent income level\n        pLvlNow = np.zeros_like(aLvlPrev)\n        for t in range(self.T_cycle):\n            these = t == self.t_cycle\n            pLvlNow[these] = self.pLvlNextFunc[t-1](self.pLvlNow[these])*self.PermShkNow[these]\n        self.pLvlNow = pLvlNow                  # Updated persistent income level\n        self.bLvlNow = RfreeNow*aLvlPrev        # Bank balances before labor income\n        self.mLvlNow = self.bLvlNow + self.TranShkNow*self.pLvlNow", "response": "Calculates updated values of normalized market resources and persistent income level for each agent in the set of income levels. Uses pLvlNow PermShkNow TranShkNow."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating consumption for each consumer of this type using the consumption functions.", "response": "def getControls(self):\n        '''\n        Calculates consumption for each consumer of this type using the consumption functions.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        cLvlNow = np.zeros(self.AgentCount) + np.nan\n        MPCnow = np.zeros(self.AgentCount) + np.nan\n        for t in range(self.T_cycle):\n            these = t == self.t_cycle\n            cLvlNow[these] = self.solution[t].cFunc(self.mLvlNow[these],self.pLvlNow[these])\n            MPCnow[these]  = self.solution[t].cFunc.derivativeX(self.mLvlNow[these],self.pLvlNow[these])\n        self.cLvlNow = cLvlNow\n        self.MPCnow  = MPCnow"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef updatepLvlNextFunc(self):\n        '''\n        A method that creates the pLvlNextFunc attribute as a sequence of\n        linear functions, indicating constant expected permanent income growth\n        across permanent income levels.  Draws on the attribute PermGroFac, and\n        installs a special retirement function when it exists.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        orig_time = self.time_flow\n        self.timeFwd()\n\n        pLvlNextFunc = []\n        for t in range(self.T_cycle):\n            pLvlNextFunc.append(LinearInterp(np.array([0.,1.]),np.array([0.,self.PermGroFac[t]])))\n\n        self.pLvlNextFunc = pLvlNextFunc\n        self.addToTimeVary('pLvlNextFunc')\n        if not orig_time:\n            self.timeRev()", "response": "A method that creates the pLvlNextFunc attribute of the object. This method creates the linear functions indicating constant expected permanent income growth across permanent income levels."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef drawMeanOneLognormal(N, sigma=1.0, seed=0):\n    '''\n    Generate arrays of mean one lognormal draws. The sigma input can be a number\n    or list-like.  If a number, output is a length N array of draws from the\n    lognormal distribution with standard deviation sigma. If a list, output is\n    a length T list whose t-th entry is a length N array of draws from the\n    lognormal with standard deviation sigma[t].\n\n    Parameters\n    ----------\n    N : int\n        Number of draws in each row.\n    sigma : float or [float]\n        One or more standard deviations. Number of elements T in sigma\n        determines number of rows of output.\n    seed : int\n        Seed for random number generator.\n\n    Returns:\n    ------------\n    draws : np.array or [np.array]\n        T-length list of arrays of mean one lognormal draws each of size N, or\n        a single array of size N (if sigma is a scalar).\n    '''\n    # Set up the RNG\n    RNG = np.random.RandomState(seed)\n\n    if isinstance(sigma,float): # Return a single array of length N\n        mu = -0.5*sigma**2\n        draws = RNG.lognormal(mean=mu, sigma=sigma, size=N)\n    else: # Set up empty list to populate, then loop and populate list with draws\n        draws=[]\n        for sig in sigma:\n            mu = -0.5*(sig**2)\n            draws.append(RNG.lognormal(mean=mu, sigma=sig, size=N))\n    return draws", "response": "Generates arrays of mean one lognormal draws. The sigma input can be a number or a list - like."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef drawLognormal(N,mu=0.0,sigma=1.0,seed=0):\n    '''\n    Generate arrays of mean one lognormal draws. The sigma input can be a number\n    or list-like.  If a number, output is a length N array of draws from the\n    lognormal distribution with standard deviation sigma. If a list, output is\n    a length T list whose t-th entry is a length N array of draws from the\n    lognormal with standard deviation sigma[t].\n\n    Parameters\n    ----------\n    N : int\n        Number of draws in each row.\n    mu : float or [float]\n        One or more means.  Number of elements T in mu determines number\n        of rows of output.\n    sigma : float or [float]\n        One or more standard deviations. Number of elements T in sigma\n        determines number of rows of output.\n    seed : int\n        Seed for random number generator.\n\n    Returns:\n    ------------\n    draws : np.array or [np.array]\n        T-length list of arrays of mean one lognormal draws each of size N, or\n        a single array of size N (if sigma is a scalar).\n    '''\n    # Set up the RNG\n    RNG = np.random.RandomState(seed)\n\n    if isinstance(sigma,float): # Return a single array of length N\n        if sigma == 0:\n            draws = np.exp(mu)*np.ones(N)\n        else:\n            draws = RNG.lognormal(mean=mu, sigma=sigma, size=N)\n    else: # Set up empty list to populate, then loop and populate list with draws\n        draws=[]\n        for j in range(len(sigma)):\n            if sigma[j] == 0:\n                draws.append(np.exp(mu[j])*np.ones(N))\n            else:\n                draws.append(RNG.lognormal(mean=mu[j], sigma=sigma[j], size=N))\n    return draws", "response": "Generates arrays of mean one lognormal draws. The sigma input can be a number or a list - like."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating arrays of normal draws.", "response": "def drawNormal(N, mu=0.0, sigma=1.0, seed=0):\n    '''\n    Generate arrays of normal draws.  The mu and sigma inputs can be numbers or\n    list-likes.  If a number, output is a length N array of draws from the normal\n    distribution with mean mu and standard deviation sigma. If a list, output is\n    a length T list whose t-th entry is a length N array with draws from the\n    normal distribution with mean mu[t] and standard deviation sigma[t].\n\n    Parameters\n    ----------\n    N : int\n        Number of draws in each row.\n    mu : float or [float]\n        One or more means.  Number of elements T in mu determines number of rows\n        of output.\n    sigma : float or [float]\n        One or more standard deviations. Number of elements T in sigma\n        determines number of rows of output.\n    seed : int\n        Seed for random number generator.\n\n    Returns\n    -------\n    draws : np.array or [np.array]\n        T-length list of arrays of normal draws each of size N, or a single array\n        of size N (if sigma is a scalar).\n    '''\n    # Set up the RNG\n    RNG = np.random.RandomState(seed)\n\n    if isinstance(sigma,float): # Return a single array of length N\n        draws = sigma*RNG.randn(N) + mu\n    else: # Set up empty list to populate, then loop and populate list with draws\n        draws=[]\n        for t in range(len(sigma)):\n            draws.append(sigma[t]*RNG.randn(N) + mu[t])\n    return draws"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating arrays of Weibull draws from the given number of elements.", "response": "def drawWeibull(N, scale=1.0, shape=1.0,  seed=0):\n    '''\n    Generate arrays of Weibull draws.  The scale and shape inputs can be\n    numbers or list-likes.  If a number, output is a length N array of draws from\n    the Weibull distribution with the given scale and shape. If a list, output\n    is a length T list whose t-th entry is a length N array with draws from the\n    Weibull distribution with scale scale[t] and shape shape[t].\n\n    Note: When shape=1, the Weibull distribution is simply the exponential dist.\n\n    Mean: scale*Gamma(1 + 1/shape)\n\n    Parameters\n    ----------\n    N : int\n        Number of draws in each row.\n    scale : float or [float]\n        One or more scales.  Number of elements T in scale determines number of\n        rows of output.\n    shape : float or [float]\n        One or more shape parameters. Number of elements T in scale\n        determines number of rows of output.\n    seed : int\n        Seed for random number generator.\n\n    Returns:\n    ------------\n    draws : np.array or [np.array]\n        T-length list of arrays of Weibull draws each of size N, or a single\n        array of size N (if sigma is a scalar).\n    '''\n    # Set up the RNG\n    RNG = np.random.RandomState(seed)\n\n    if scale == 1:\n        scale = float(scale)\n    if isinstance(scale,float): # Return a single array of length N\n        draws = scale*(-np.log(1.0-RNG.rand(N)))**(1.0/shape)\n    else: # Set up empty list to populate, then loop and populate list with draws\n        draws=[]\n        for t in range(len(scale)):\n            draws.append(scale[t]*(-np.log(1.0-RNG.rand(N)))**(1.0/shape[t]))\n    return draws"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates arrays of uniform draws on the chosen size.", "response": "def drawUniform(N, bot=0.0, top=1.0, seed=0):\n    '''\n    Generate arrays of uniform draws.  The bot and top inputs can be numbers or\n    list-likes.  If a number, output is a length N array of draws from the\n    uniform distribution on [bot,top]. If a list, output is a length T list\n    whose t-th entry is a length N array with draws from the uniform distribution\n    on [bot[t],top[t]].\n\n    Parameters\n    ----------\n    N : int\n        Number of draws in each row.\n    bot : float or [float]\n        One or more bottom values.  Number of elements T in mu determines number\n        of rows of output.\n    top : float or [float]\n        One or more top values. Number of elements T in top determines number of\n        rows of output.\n    seed : int\n        Seed for random number generator.\n\n    Returns\n    -------\n    draws : np.array or [np.array]\n        T-length list of arrays of uniform draws each of size N, or a single\n        array of size N (if sigma is a scalar).\n    '''\n    # Set up the RNG\n    RNG = np.random.RandomState(seed)\n\n    if isinstance(bot,float) or isinstance(bot,int): # Return a single array of size N\n        draws = bot + (top - bot)*RNG.rand(N)\n    else: # Set up empty list to populate, then loop and populate list with draws\n        draws=[]\n        for t in range(len(bot)):\n            draws.append(bot[t] + (top[t] - bot[t])*RNG.rand(N))\n    return draws"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating arrays of booleans drawn from a simple Bernoulli distribution.", "response": "def drawBernoulli(N,p=0.5,seed=0):\n    '''\n    Generates arrays of booleans drawn from a simple Bernoulli distribution.\n    The input p can be a float or a list-like of floats; its length T determines\n    the number of entries in the output.  The t-th entry of the output is an\n    array of N booleans which are True with probability p[t] and False otherwise.\n\n    Arguments\n    ---------\n    N : int\n        Number of draws in each row.\n    p : float or [float]\n        Probability or probabilities of the event occurring (True).\n    seed : int\n        Seed for random number generator.\n\n    Returns\n    -------\n    draws : np.array or [np.array]\n        T-length list of arrays of Bernoulli draws each of size N, or a single\n        array of size N (if sigma is a scalar).\n    '''\n    # Set up the RNG\n    RNG = np.random.RandomState(seed)\n\n    if isinstance(p,float):# Return a single array of size N\n        draws = RNG.uniform(size=N) < p\n    else: # Set up empty list to populate, then loop and populate list with draws:\n        draws=[]\n        for t in range(len(p)):\n            draws.append(RNG.uniform(size=N) < p[t])\n    return draws"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsimulate N draws from a discrete distribution with probabilities P and outcomes X.", "response": "def drawDiscrete(N,P=[1.0],X=[0.0],exact_match=False,seed=0):\n    '''\n    Simulates N draws from a discrete distribution with probabilities P and outcomes X.\n\n    Parameters\n    ----------\n    P : np.array\n        A list of probabilities of outcomes.\n    X : np.array\n        A list of discrete outcomes.\n    N : int\n        Number of draws to simulate.\n    exact_match : boolean\n        Whether the draws should \"exactly\" match the discrete distribution (as\n        closely as possible given finite draws).  When True, returned draws are\n        a random permutation of the N-length list that best fits the discrete\n        distribution.  When False (default), each draw is independent from the\n        others and the result could deviate from the input.\n    seed : int\n        Seed for random number generator.\n\n    Returns\n    -------\n    draws : np.array\n        An array draws from the discrete distribution; each element is a value in X.\n    '''\n    # Set up the RNG\n    RNG = np.random.RandomState(seed)\n\n    if exact_match:\n        events = np.arange(P.size) # just a list of integers\n        cutoffs = np.round(np.cumsum(P)*N).astype(int) # cutoff points between discrete outcomes\n        top = 0\n        # Make a list of event indices that closely matches the discrete distribution\n        event_list        = []\n        for j in range(events.size):\n            bot = top\n            top = cutoffs[j]\n            event_list += (top-bot)*[events[j]]\n        # Randomly permute the event indices and store the corresponding results\n        event_draws = RNG.permutation(event_list)\n        draws = X[event_draws]\n    else:\n        # Generate a cumulative distribution\n        base_draws = RNG.uniform(size=N)\n        cum_dist = np.cumsum(P)\n\n        # Convert the basic uniform draws into discrete draws\n        indices = cum_dist.searchsorted(base_draws)\n        draws = np.asarray(X)[indices]\n    return draws"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef solveFashion(solution_next,DiscFac,conformUtilityFunc,punk_utility,jock_utility,switchcost_J2P,switchcost_P2J,pGrid,pEvolution,pref_shock_mag):\n    '''\n    Solves a single period of the fashion victim model.\n\n    Parameters\n    ----------\n    solution_next: FashionSolution\n        A representation of the solution to the subsequent period's problem.\n    DiscFac: float\n        The intertemporal discount factor.\n    conformUtilityFunc: function\n        Utility as a function of the proportion of the population who wears the\n        same style as the agent.\n    punk_utility: float\n        Direct utility from wearing the punk style this period.\n    jock_utility: float\n        Direct utility from wearing the jock style this period.\n    switchcost_J2P: float\n        Utility cost of switching from jock to punk this period.\n    switchcost_P2J: float\n        Utility cost of switching from punk to jock this period.\n    pGrid: np.array\n        1D array of \"proportion of punks\" states spanning [0,1], representing\n        the fraction of agents *currently* wearing punk style.\n    pEvolution: np.array\n        2D array representing the distribution of next period's \"proportion of\n        punks\".  The pEvolution[i,:] contains equiprobable values of p for next\n        period if p = pGrid[i] today.\n    pref_shock_mag: float\n        Standard deviation of T1EV preference shocks over style.\n\n    Returns\n    -------\n    solution_now: FashionSolution\n        A representation of the solution to this period's problem.\n    '''\n    # Unpack next period's solution\n    VfuncPunkNext = solution_next.VfuncPunk\n    VfuncJockNext = solution_next.VfuncJock\n\n    # Calculate end-of-period expected value for each style at points on the pGrid\n    EndOfPrdVpunk = DiscFac*np.mean(VfuncPunkNext(pEvolution),axis=1)\n    EndOfPrdVjock = DiscFac*np.mean(VfuncJockNext(pEvolution),axis=1)\n\n    # Get current period utility flow from each style (without switching cost)\n    Upunk = punk_utility + conformUtilityFunc(pGrid)\n    Ujock = jock_utility + conformUtilityFunc(1.0 - pGrid)\n\n    # Calculate choice-conditional value for each combination of current and next styles (at each)\n    V_J2J = Ujock                  + EndOfPrdVjock\n    V_J2P = Upunk - switchcost_J2P + EndOfPrdVpunk\n    V_P2J = Ujock - switchcost_P2J + EndOfPrdVjock\n    V_P2P = Upunk                  + EndOfPrdVpunk\n\n    # Calculate the beginning-of-period expected value of each p-state when punk\n    Vboth_P = np.vstack((V_P2J,V_P2P))\n    Vbest_P = np.max(Vboth_P,axis=0)\n    Vnorm_P = Vboth_P - np.tile(np.reshape(Vbest_P,(1,pGrid.size)),(2,1))\n    ExpVnorm_P = np.exp(Vnorm_P/pref_shock_mag)\n    SumExpVnorm_P = np.sum(ExpVnorm_P,axis=0)\n    V_P = np.log(SumExpVnorm_P)*pref_shock_mag + Vbest_P\n    switch_P = ExpVnorm_P[0,:]/SumExpVnorm_P\n\n    # Calculate the beginning-of-period expected value of each p-state when jock\n    Vboth_J = np.vstack((V_J2J,V_J2P))\n    Vbest_J = np.max(Vboth_J,axis=0)\n    Vnorm_J = Vboth_J - np.tile(np.reshape(Vbest_J,(1,pGrid.size)),(2,1))\n    ExpVnorm_J = np.exp(Vnorm_J/pref_shock_mag)\n    SumExpVnorm_J = np.sum(ExpVnorm_J,axis=0)\n    V_J = np.log(SumExpVnorm_J)*pref_shock_mag + Vbest_J\n    switch_J = ExpVnorm_J[1,:]/SumExpVnorm_J\n\n    # Make value and policy functions for each style\n    VfuncPunkNow = LinearInterp(pGrid,V_P)\n    VfuncJockNow = LinearInterp(pGrid,V_J)\n    switchFuncPunkNow = LinearInterp(pGrid,switch_P)\n    switchFuncJockNow = LinearInterp(pGrid,switch_J)\n\n    # Make and return this period's solution\n    solution_now = FashionSolution(VfuncJock=VfuncJockNow,\n                                   VfuncPunk=VfuncPunkNow,\n                                   switchFuncJock=switchFuncJockNow,\n                                   switchFuncPunk=switchFuncPunkNow)\n    return solution_now", "response": "Solves a single period of the fashion victim model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the proportion of punks in the population given data from each type of the current market.", "response": "def calcPunkProp(sNow):\n    '''\n    Calculates the proportion of punks in the population, given data from each type.\n\n    Parameters\n    ----------\n    pNow : [np.array]\n        List of arrays of binary data, representing the fashion choice of each\n        agent in each type of this market (0=jock, 1=punk).\n    pop_size : [int]\n        List with the number of agents of each type in the market.  Unused.\n    '''\n    sNowX = np.asarray(sNow).flatten()\n    pNow  = np.mean(sNowX)\n    return FashionMarketInfo(pNow)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calcFashionEvoFunc(pNow):\n    '''\n    Calculates a new approximate dynamic rule for the evolution of the proportion\n    of punks as a linear function and a \"shock width\".\n\n    Parameters\n    ----------\n    pNow : [float]\n        List describing the history of the proportion of punks in the population.\n\n    Returns\n    -------\n    (unnamed) : FashionEvoFunc\n        A new rule for the evolution of the population punk proportion, based on\n        the history in input pNow.\n    '''\n    pNowX = np.array(pNow)\n    T = pNowX.size\n    p_t   = pNowX[100:(T-1)]\n    p_tp1 = pNowX[101:T]\n    pNextSlope, pNextIntercept, trash1, trash2, trash3 = stats.linregress(p_t,p_tp1)\n    pPopExp  = pNextIntercept + pNextSlope*p_t\n    pPopErrSq= (pPopExp - p_tp1)**2\n    pNextStd  = np.sqrt(np.mean(pPopErrSq))\n    print(str(pNextIntercept) + ', ' + str(pNextSlope) + ', ' + str(pNextStd))\n    return FashionEvoFunc(pNextIntercept,pNextSlope,2*pNextStd)", "response": "Calculates a new approximate dynamic rule for the evolution of the proportion of punks as a linear function and a shock width."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef updateEvolution(self):\n        '''\n        Updates the \"population punk proportion\" evolution array.  Fasion victims\n        believe that the proportion of punks in the subsequent period is a linear\n        function of the proportion of punks this period, subject to a uniform\n        shock.  Given attributes of self pNextIntercept, pNextSlope, pNextCount,\n        pNextWidth, and pGrid, this method generates a new array for the attri-\n        bute pEvolution, representing a discrete approximation of next period\n        states for each current period state in pGrid.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        self.pEvolution = np.zeros((self.pCount,self.pNextCount))\n        for j in range(self.pCount):\n            pNow = self.pGrid[j]\n            pNextMean = self.pNextIntercept + self.pNextSlope*pNow\n            dist = approxUniform(N=self.pNextCount,bot=pNextMean-self.pNextWidth,top=pNextMean+self.pNextWidth)[1]\n            self.pEvolution[j,:] = dist", "response": "Updates the population punk proportion evolution array."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self):\n        '''\n        Updates the non-primitive objects needed for solution, using primitive\n        attributes.  This includes defining a \"utility from conformity\" function\n        conformUtilityFunc, a grid of population punk proportions, and an array\n        of future punk proportions (for each value in the grid).  Results are\n        stored as attributes of self.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        self.conformUtilityFunc = lambda x : stats.beta.pdf(x,self.uParamA,self.uParamB)\n        self.pGrid = np.linspace(0.0001,0.9999,self.pCount)\n        self.updateEvolution()", "response": "Updates the non - primitive objects needed for solution using primitive\n        attributes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nresetting this agent type to prepare it for a new simulation run.", "response": "def reset(self):\n        '''\n        Resets this agent type to prepare it for a new simulation run.  This\n        includes resetting the random number generator and initializing the style\n        of each agent of this type.\n        '''\n        self.resetRNG()\n        sNow = np.zeros(self.pop_size)\n        Shk  = self.RNG.rand(self.pop_size)\n        sNow[Shk < self.p_init] = 1\n        self.sNow = sNow"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nunpacking the behavioral and value functions for more parsimonious access.", "response": "def postSolve(self):\n        '''\n        Unpack the behavioral and value functions for more parsimonious access.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        self.switchFuncPunk = self.solution[0].switchFuncPunk\n        self.switchFuncJock = self.solution[0].switchFuncJock\n        self.VfuncPunk      = self.solution[0].VfuncPunk\n        self.VfuncJock      = self.solution[0].VfuncJock"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef simOnePrd(self):\n        '''\n        Simulate one period of the fashion victom model for this type.  Each\n        agent receives an idiosyncratic preference shock and chooses whether to\n        change styles (using the optimal decision rule).\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        pNow    = self.pNow\n        sPrev   = self.sNow\n        J2Pprob = self.switchFuncJock(pNow)\n        P2Jprob = self.switchFuncPunk(pNow)\n        Shks    = self.RNG.rand(self.pop_size)\n        J2P     = np.logical_and(sPrev == 0,Shks < J2Pprob)\n        P2J     = np.logical_and(sPrev == 1,Shks < P2Jprob)\n        sNow    = copy(sPrev)\n        sNow[J2P] = 1\n        sNow[P2J] = 0\n        self.sNow = sNow", "response": "Simulate one period of the fashion victom model for this type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the set of segments that are part of the current segment.", "response": "def calcSegments(x, v):\n    \"\"\"\n    Find index vectors `rise` and `fall` such that `rise` holds the indeces `i`\n    such that x[i+1]>x[i] and `fall` holds indeces `j` such that either\n    - x[j+1] < x[j] or,\n    - x[j]>x[j-1] and v[j]<v[j-1].\n\n    The vectors are essential to the DCEGM algorithm, as they definite the\n    relevant intervals to be used to construct the upper envelope of potential\n    solutions to the (necessary) first order conditions.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        array of points where `v` is evaluated\n    v : np.ndarray\n        array of values of some function of `x`\n\n    Returns\n    -------\n    rise : np.ndarray\n        see description above\n    fall : np.ndarray\n        see description above\n    \"\"\"\n    # NOTE: assumes that the first segment is in fact increasing (forced in EGM\n    # by augmentation with the constrained segment).\n    # elements in common grid g\n\n    # Identify index intervals of falling and rising regions\n    # We need these to construct the upper envelope because we need to discard\n    # solutions from the inverted Euler equations that do not represent optimal\n    # choices (the FOCs are only necessary in these models).\n    #\n    # `fall` is a vector of indeces that represent the first elements in all\n    # of the falling segments (the curve can potentially fold several times)\n    fall = np.empty(0, dtype=int) # initialize with empty and then add the last point below while-loop\n\n    rise = np.array([0]) # Initialize such thatthe lowest point is the first grid point\n    i = 1 # Initialize\n    while i <= len(x) - 2:\n        # Check if the next (`ip1` stands for i plus 1) grid point is below the\n        # current one, such that the line is folding back.\n        ip1_falls = x[i+1] < x[i] # true if grid decreases on index increment\n        i_rose = x[i] > x[i-1] # true if grid decreases on index decrement\n        val_fell = v[i] < v[i-1] # true if value rises on index decrement\n\n        if (ip1_falls and i_rose) or (val_fell and i_rose):\n\n            # we are in a region where the endogenous grid is decreasing or\n            # the value function rises by stepping back in the grid.\n            fall = np.append(fall, i) # add the index to the vector\n\n            # We now iterate from the current index onwards until we find point\n            # where resources rises again. Unfortunately, we need to check\n            # each points, as there can be multiple spells of falling endogenous\n            # grids, so we cannot use bisection or some other fast algorithm.\n            k = i\n            while x[k+1] < x[k]:\n                k = k + 1\n            # k now holds either the next index the starts a new rising\n            # region, or it holds the length of M, `m_len`.\n\n            rise = np.append(rise, k)\n\n            # Set the index to the point where resources again is rising\n            i = k\n\n        i = i + 1\n\n    # Add the last index for convenience (then all segments are complete, as\n    # len(fall) == len(rise), and we can form them by range(rise[j], fall[j]+1).\n    fall = np.append(fall, len(v)-1)\n\n    return rise, fall"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calcMultilineEnvelope(M, C, V_T, commonM):\n    m_len = len(commonM)\n    rise, fall = calcSegments(M, V_T)\n\n    num_kinks = len(fall) # number of kinks / falling EGM grids\n\n    # Use these segments to sequentially find upper envelopes. commonVARNAME\n    # means the VARNAME evaluated on the common grid with a cloumn for each kink\n    # discovered in calcSegments. This means that commonVARNAME is a matrix\n    # common grid length-by-number of segments to consider. In the end, we'll\n    # use nanargmax over the columns to pick out the best (transformed) values.\n    # This is why we fill the arrays with np.nan's.\n    commonV_T = np.empty((m_len, num_kinks))\n    commonV_T[:] = np.nan\n    commonC = np.empty((m_len, num_kinks))\n    commonC[:] = np.nan\n\n    # Now, loop over all segments as defined by the \"kinks\" or the combination\n    # of \"rise\" and \"fall\" indeces. These (rise[j], fall[j]) pairs define regions\n    for j in range(num_kinks):\n        # Find points in the common grid that are in the range of the points in\n        # the interval defined by (rise[j], fall[j]).\n        below = M[rise[j]] >= commonM # boolean array of bad indeces below\n        above = M[fall[j]] <= commonM # boolen array of bad indeces above\n        in_range = below + above == 0 # pick out elements that are neither\n\n        # create range of indeces in the input arrays\n        idxs = range(rise[j], fall[j]+1)\n        # grab ressource values at the relevant indeces\n        m_idx_j = M[idxs]\n\n        # based in in_range, find the relevant ressource values to interpolate\n        m_eval = commonM[in_range]\n\n        # re-interpolate to common grid\n        commonV_T[in_range,j] = LinearInterp(m_idx_j, V_T[idxs], lower_extrap=True)(m_eval)\n        commonC[in_range,j]  = LinearInterp(m_idx_j, C[idxs], lower_extrap=True)(m_eval) # Interpolat econsumption also. May not be nesserary\n    # for each row in the commonV_T matrix, see if all entries are np.nan. This\n    # would mean that we have no valid value here, so we want to use this boolean\n    # vector to filter out irrelevant entries of commonV_T.\n    row_all_nan = np.array([np.all(np.isnan(row)) for row in commonV_T])\n    # Now take the max of all these line segments.\n    idx_max = np.zeros(commonM.size, dtype = int)\n    idx_max[row_all_nan == False] = np.nanargmax(commonV_T[row_all_nan == False], axis=1)\n\n    # prefix with upper for variable that are \"upper enveloped\"\n    upperV_T = np.zeros(m_len)\n\n    # Set the non-nan rows to the maximum over columns\n    upperV_T[row_all_nan == False] = np.nanmax(commonV_T[row_all_nan == False, :], axis=1)\n    # Set the rest to nan\n    upperV_T[row_all_nan] = np.nan\n\n    # Add the zero point in the bottom\n    if np.isnan(upperV_T[0]):\n        # in transformed space space, utility of zero-consumption (-inf) is 0.0\n        upperV_T[0] = 0.0\n        # commonM[0] is typically 0, so this is safe, but maybe it should be 0.0\n        commonC[0]  = commonM[0]\n\n    # Extrapolate if NaNs are introduced due to the common grid\n    # going outside all the sub-line segments\n    IsNaN = np.isnan(upperV_T)\n    upperV_T[IsNaN] = LinearInterp(commonM[IsNaN == False], upperV_T[IsNaN == False])(commonM[IsNaN])\n\n\n    LastBeforeNaN = np.append(np.diff(IsNaN)>0, 0)\n    LastId = LastBeforeNaN*idx_max # Find last id-number\n    idx_max[IsNaN] = LastId[IsNaN]\n    # Linear index used to get optimal consumption based on \"id\"  from max\n    ncols = commonC.shape[1]\n    rowidx = np.cumsum(ncols*np.ones(len(commonM), dtype=int))-ncols\n    idx_linear = np.unravel_index(rowidx+idx_max, commonC.shape)\n    upperC = commonC[idx_linear]\n    upperC[IsNaN] = LinearInterp(commonM[IsNaN==0], upperC[IsNaN==0])(commonM[IsNaN])\n\n    # TODO calculate cross points of line segments to get the true vertical drops\n\n    upperM = commonM.copy() # anticipate this TODO\n\n    return upperM, upperC, upperV_T", "response": "This function calculates the optimal envelope for a multi - line set of market resources and consumption levels."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef solvePerfForesight(solution_next,DiscFac,LivPrb,CRRA,Rfree,PermGroFac):\n    '''\n    Solves a single period consumption-saving problem for a consumer with perfect foresight.\n\n    Parameters\n    ----------\n    solution_next : ConsumerSolution\n        The solution to next period's one period problem.\n    DiscFac : float\n        Intertemporal discount factor for future utility.\n    LivPrb : float\n        Survival probability; likelihood of being alive at the beginning of\n        the succeeding period.\n    CRRA : float\n        Coefficient of relative risk aversion.\n    Rfree : float\n        Risk free interest factor on end-of-period assets.\n    PermGroFac : float\n        Expected permanent income growth factor at the end of this period.\n\n    Returns\n    -------\n    solution : ConsumerSolution\n            The solution to this period's problem.\n    '''\n    solver = ConsPerfForesightSolver(solution_next,DiscFac,LivPrb,CRRA,Rfree,PermGroFac)\n    solution = solver.solve()\n    return solution", "response": "Solves a single period consumption - saving problem for a consumer with perfect foresight."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsolving a single period consumption - saving problem with a single period consumption - saving problem with permanent and transitory shocks.", "response": "def solveConsIndShock(solution_next,IncomeDstn,LivPrb,DiscFac,CRRA,Rfree,PermGroFac,\n                                BoroCnstArt,aXtraGrid,vFuncBool,CubicBool):\n    '''\n    Solves a single period consumption-saving problem with CRRA utility and risky\n    income (subject to permanent and transitory shocks).  Can generate a value\n    function if requested; consumption function can be linear or cubic splines.\n\n    Parameters\n    ----------\n    solution_next : ConsumerSolution\n        The solution to next period's one period problem.\n    IncomeDstn : [np.array]\n        A list containing three arrays of floats, representing a discrete\n        approximation to the income process between the period being solved\n        and the one immediately following (in solution_next). Order: event\n        probabilities, permanent shocks, transitory shocks.\n    LivPrb : float\n        Survival probability; likelihood of being alive at the beginning of\n        the succeeding period.\n    DiscFac : float\n        Intertemporal discount factor for future utility.\n    CRRA : float\n        Coefficient of relative risk aversion.\n    Rfree : float\n        Risk free interest factor on end-of-period assets.\n    PermGroFac : float\n        Expected permanent income growth factor at the end of this period.\n    BoroCnstArt: float or None\n        Borrowing constraint for the minimum allowable assets to end the\n        period with.  If it is less than the natural borrowing constraint,\n        then it is irrelevant; BoroCnstArt=None indicates no artificial bor-\n        rowing constraint.\n    aXtraGrid: np.array\n        Array of \"extra\" end-of-period asset values-- assets above the\n        absolute minimum acceptable level.\n    vFuncBool: boolean\n        An indicator for whether the value function should be computed and\n        included in the reported solution.\n    CubicBool: boolean\n        Indicator for whether the solver should use cubic or linear interpolation.\n\n    Returns\n    -------\n    solution_now : ConsumerSolution\n        The solution to the single period consumption-saving problem.  Includes\n        a consumption function cFunc (using cubic or linear splines), a marginal\n        value function vPfunc, a minimum acceptable level of normalized market\n        resources mNrmMin, normalized human wealth hNrm, and bounding MPCs MPCmin\n        and MPCmax.  It might also have a value function vFunc and marginal mar-\n        ginal value function vPPfunc.\n    '''\n    # Use the basic solver if user doesn't want cubic splines or the value function\n    if (not CubicBool) and (not vFuncBool):\n        solver = ConsIndShockSolverBasic(solution_next,IncomeDstn,LivPrb,DiscFac,CRRA,\n                                                  Rfree,PermGroFac,BoroCnstArt,aXtraGrid,vFuncBool,\n                                                  CubicBool)\n    else: # Use the \"advanced\" solver if either is requested\n        solver = ConsIndShockSolver(solution_next,IncomeDstn,LivPrb,DiscFac,CRRA,Rfree,\n                                             PermGroFac,BoroCnstArt,aXtraGrid,vFuncBool,CubicBool)\n    solver.prepareToSolve()       # Do some preparatory work\n    solution_now = solver.solve() # Solve the period\n    return solution_now"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a list of discrete approximations to the income process for each life period, from end of life to beginning of life. Permanent shocks are mean one lognormally distributed with standard deviation PermShkStd[t] during the working life, and degenerate at 1 in the retirement period. Transitory shocks are mean one lognormally distributed with a point mass at IncUnemp with probability UnempPrb while working; they are mean one with a point mass at IncUnempRet with probability UnempPrbRet. Retirement occurs after t=T_retire periods of working. Note 1: All time in this function runs forward, from t=0 to t=T Note 2: All parameters are passed as attributes of the input parameters. Parameters (passed as attributes of the input parameters) ---------- PermShkStd : [float] List of standard deviations in log permanent income uncertainty during the agent's life. PermShkCount : int The number of approximation points to be used in the discrete approxima- tion to the permanent income shock distribution. TranShkStd : [float] List of standard deviations in log transitory income uncertainty during the agent's life. TranShkCount : int The number of approximation points to be used in the discrete approxima- tion to the permanent income shock distribution. UnempPrb : float The probability of becoming unemployed during the working period. UnempPrbRet : float The probability of not receiving typical retirement income when retired. T_retire : int The index value for the final working period in the agent's life. If T_retire <= 0 then there is no retirement. IncUnemp : float Transitory income received when unemployed. IncUnempRet : float Transitory income received while \"unemployed\" when retired. T_cycle : int Total number of non-terminal periods in the consumer's sequence of periods. Returns ------- IncomeDstn : [[np.array]] A list with T_cycle elements, each of which is a list of three arrays representing a discrete approximation to the income process in a period. Order: probabilities, permanent shocks, transitory shocks. PermShkDstn : [[np.array]] A list with T_cycle elements, each of which is a list of two arrays representing a discrete approximation to the permanent income shocks. TranShkDstn : [[np.array]] A list with T_cycle elements, each of which is a list of two arrays representing a discrete approximation to the transitory income shocks.", "response": "def constructLognormalIncomeProcessUnemployment(parameters):\n    '''\n    Generates a list of discrete approximations to the income process for each\n    life period, from end of life to beginning of life.  Permanent shocks are mean\n    one lognormally distributed with standard deviation PermShkStd[t] during the\n    working life, and degenerate at 1 in the retirement period.  Transitory shocks\n    are mean one lognormally distributed with a point mass at IncUnemp with\n    probability UnempPrb while working; they are mean one with a point mass at\n    IncUnempRet with probability UnempPrbRet.  Retirement occurs\n    after t=T_retire periods of working.\n\n    Note 1: All time in this function runs forward, from t=0 to t=T\n\n    Note 2: All parameters are passed as attributes of the input parameters.\n\n    Parameters (passed as attributes of the input parameters)\n    ----------\n    PermShkStd : [float]\n        List of standard deviations in log permanent income uncertainty during\n        the agent's life.\n    PermShkCount : int\n        The number of approximation points to be used in the discrete approxima-\n        tion to the permanent income shock distribution.\n    TranShkStd : [float]\n        List of standard deviations in log transitory income uncertainty during\n        the agent's life.\n    TranShkCount : int\n        The number of approximation points to be used in the discrete approxima-\n        tion to the permanent income shock distribution.\n    UnempPrb : float\n        The probability of becoming unemployed during the working period.\n    UnempPrbRet : float\n        The probability of not receiving typical retirement income when retired.\n    T_retire : int\n        The index value for the final working period in the agent's life.\n        If T_retire <= 0 then there is no retirement.\n    IncUnemp : float\n        Transitory income received when unemployed.\n    IncUnempRet : float\n        Transitory income received while \"unemployed\" when retired.\n    T_cycle :  int\n        Total number of non-terminal periods in the consumer's sequence of periods.\n\n    Returns\n    -------\n    IncomeDstn :  [[np.array]]\n        A list with T_cycle elements, each of which is a list of three arrays\n        representing a discrete approximation to the income process in a period.\n        Order: probabilities, permanent shocks, transitory shocks.\n    PermShkDstn : [[np.array]]\n        A list with T_cycle elements, each of which is a list of two arrays\n        representing a discrete approximation to the permanent income shocks.\n    TranShkDstn : [[np.array]]\n        A list with T_cycle elements, each of which is a list of two arrays\n        representing a discrete approximation to the transitory income shocks.\n    '''\n    # Unpack the parameters from the input\n    PermShkStd    = parameters.PermShkStd\n    PermShkCount  = parameters.PermShkCount\n    TranShkStd    = parameters.TranShkStd\n    TranShkCount  = parameters.TranShkCount\n    T_cycle       = parameters.T_cycle\n    T_retire      = parameters.T_retire\n    UnempPrb      = parameters.UnempPrb\n    IncUnemp      = parameters.IncUnemp\n    UnempPrbRet   = parameters.UnempPrbRet\n    IncUnempRet   = parameters.IncUnempRet\n\n    IncomeDstn    = [] # Discrete approximations to income process in each period\n    PermShkDstn   = [] # Discrete approximations to permanent income shocks\n    TranShkDstn   = [] # Discrete approximations to transitory income shocks\n\n    # Fill out a simple discrete RV for retirement, with value 1.0 (mean of shocks)\n    # in normal times; value 0.0 in \"unemployment\" times with small prob.\n    if T_retire > 0:\n        if UnempPrbRet > 0:\n            PermShkValsRet  = np.array([1.0, 1.0])    # Permanent income is deterministic in retirement (2 states for temp income shocks)\n            TranShkValsRet  = np.array([IncUnempRet,\n                                        (1.0-UnempPrbRet*IncUnempRet)/(1.0-UnempPrbRet)])\n            ShkPrbsRet      = np.array([UnempPrbRet, 1.0-UnempPrbRet])\n        else:\n            PermShkValsRet  = np.array([1.0])\n            TranShkValsRet  = np.array([1.0])\n            ShkPrbsRet      = np.array([1.0])\n        IncomeDstnRet = [ShkPrbsRet,PermShkValsRet,TranShkValsRet]\n\n    # Loop to fill in the list of IncomeDstn random variables.\n    for t in range(T_cycle): # Iterate over all periods, counting forward\n\n        if T_retire > 0 and t >= T_retire:\n            # Then we are in the \"retirement period\" and add a retirement income object.\n            IncomeDstn.append(deepcopy(IncomeDstnRet))\n            PermShkDstn.append([np.array([1.0]),np.array([1.0])])\n            TranShkDstn.append([ShkPrbsRet,TranShkValsRet])\n        else:\n            # We are in the \"working life\" periods.\n            TranShkDstn_t    = approxMeanOneLognormal(N=TranShkCount, sigma=TranShkStd[t], tail_N=0)\n            if UnempPrb > 0:\n                TranShkDstn_t = addDiscreteOutcomeConstantMean(TranShkDstn_t, p=UnempPrb, x=IncUnemp)\n            PermShkDstn_t    = approxMeanOneLognormal(N=PermShkCount, sigma=PermShkStd[t], tail_N=0)\n            IncomeDstn.append(combineIndepDstns(PermShkDstn_t,TranShkDstn_t)) # mix the independent distributions\n            PermShkDstn.append(PermShkDstn_t)\n            TranShkDstn.append(TranShkDstn_t)\n    return IncomeDstn, PermShkDstn, TranShkDstn"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply a flat income tax rate to all income states during the working income tree.", "response": "def applyFlatIncomeTax(IncomeDstn,tax_rate,T_retire,unemployed_indices=[],transitory_index=2):\n    '''\n    Applies a flat income tax rate to all employed income states during the working\n    period of life (those before T_retire).  Time runs forward in this function.\n\n    Parameters\n    ----------\n    IncomeDstn : [income distributions]\n        The discrete approximation to the income distribution in each time period.\n    tax_rate : float\n        A flat income tax rate to be applied to all employed income.\n    T_retire : int\n        The time index after which the agent retires.\n    unemployed_indices : [int]\n        Indices of transitory shocks that represent unemployment states (no tax).\n    transitory_index : int\n        The index of each element of IncomeDstn representing transitory shocks.\n\n    Returns\n    -------\n    IncomeDstn_new : [income distributions]\n        The updated income distributions, after applying the tax.\n    '''\n    IncomeDstn_new = deepcopy(IncomeDstn)\n    i = transitory_index\n    for t in range(len(IncomeDstn)):\n        if t < T_retire:\n            for j in range((IncomeDstn[t][i]).size):\n                if j not in unemployed_indices:\n                    IncomeDstn_new[t][i][j] = IncomeDstn[t][i][j]*(1-tax_rate)\n    return IncomeDstn_new"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef constructAssetsGrid(parameters):\n    '''\n    Constructs the base grid of post-decision states, representing end-of-period\n    assets above the absolute minimum.\n\n    All parameters are passed as attributes of the single input parameters.  The\n    input can be an instance of a ConsumerType, or a custom Parameters class.\n\n    Parameters\n    ----------\n    aXtraMin:                  float\n        Minimum value for the a-grid\n    aXtraMax:                  float\n        Maximum value for the a-grid\n    aXtraCount:                 int\n        Size of the a-grid\n    aXtraExtra:                [float]\n        Extra values for the a-grid.\n    exp_nest:               int\n        Level of nesting for the exponentially spaced grid\n\n    Returns\n    -------\n    aXtraGrid:     np.ndarray\n        Base array of values for the post-decision-state grid.\n    '''\n    # Unpack the parameters\n    aXtraMin     = parameters.aXtraMin\n    aXtraMax     = parameters.aXtraMax\n    aXtraCount   = parameters.aXtraCount\n    aXtraExtra   = parameters.aXtraExtra\n    grid_type    = 'exp_mult'\n    exp_nest     = parameters.aXtraNestFac\n\n    # Set up post decision state grid:\n    aXtraGrid = None\n    if grid_type == \"linear\":\n        aXtraGrid = np.linspace(aXtraMin, aXtraMax, aXtraCount)\n    elif grid_type == \"exp_mult\":\n        aXtraGrid = makeGridExpMult(ming=aXtraMin, maxg=aXtraMax, ng=aXtraCount, timestonest=exp_nest)\n    else:\n        raise Exception(\"grid_type not recognized in __init__.\" + \\\n                         \"Please ensure grid_type is 'linear' or 'exp_mult'\")\n\n    # Add in additional points for the grid:\n    for a in aXtraExtra:\n        if (a is not None):\n            if a not in aXtraGrid:\n                j      = aXtraGrid.searchsorted(a)\n                aXtraGrid = np.insert(aXtraGrid, j, a)\n\n    return aXtraGrid", "response": "Constructs the base grid of assets that are at the end of the absolute minimum."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef appendSolution(self,new_solution):\n        '''\n        Appends one solution to another to create a ConsumerSolution whose\n        attributes are lists.  Used in ConsMarkovModel, where we append solutions\n        *conditional* on a particular value of a Markov state to each other in\n        order to get the entire solution.\n\n        Parameters\n        ----------\n        new_solution : ConsumerSolution\n            The solution to a consumption-saving problem; each attribute is a\n            list representing state-conditional values or functions.\n\n        Returns\n        -------\n        None\n        '''\n        if type(self.cFunc)!=list:\n            # Then we assume that self is an empty initialized solution instance.\n            # Begin by checking this is so.\n            assert NullFunc().distance(self.cFunc) == 0, 'appendSolution called incorrectly!'\n\n            # We will need the attributes of the solution instance to be lists.  Do that here.\n            self.cFunc       = [new_solution.cFunc]\n            self.vFunc       = [new_solution.vFunc]\n            self.vPfunc      = [new_solution.vPfunc]\n            self.vPPfunc     = [new_solution.vPPfunc]\n            self.mNrmMin     = [new_solution.mNrmMin]\n        else:\n            self.cFunc.append(new_solution.cFunc)\n            self.vFunc.append(new_solution.vFunc)\n            self.vPfunc.append(new_solution.vPfunc)\n            self.vPPfunc.append(new_solution.vPPfunc)\n            self.mNrmMin.append(new_solution.mNrmMin)", "response": "Append a new solution to the current instance of the ConsumerSolution."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef assignParameters(self,solution_next,DiscFac,LivPrb,CRRA,Rfree,PermGroFac):\n        '''\n        Saves necessary parameters as attributes of self for use by other methods.\n\n        Parameters\n        ----------\n        solution_next : ConsumerSolution\n            The solution to next period's one period problem.\n        DiscFac : float\n            Intertemporal discount factor for future utility.\n        LivPrb : float\n            Survival probability; likelihood of being alive at the beginning of\n            the succeeding period.\n        CRRA : float\n            Coefficient of relative risk aversion.\n        Rfree : float\n            Risk free interest factor on end-of-period assets.\n        PermGroFac : float\n            Expected permanent income growth factor at the end of this period.\n\n        Returns\n        -------\n        none\n        '''\n        self.solution_next  = solution_next\n        self.DiscFac        = DiscFac\n        self.LivPrb         = LivPrb\n        self.CRRA           = CRRA\n        self.Rfree          = Rfree\n        self.PermGroFac     = PermGroFac", "response": "Assign the following parameters to the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndefining the utility functions for this period and its derivatives.", "response": "def defUtilityFuncs(self):\n        '''\n        Defines CRRA utility function for this period (and its derivatives),\n        saving them as attributes of self for other methods to use.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        self.u   = lambda c : utility(c,gam=self.CRRA)  # utility function\n        self.uP  = lambda c : utilityP(c,gam=self.CRRA) # marginal utility function\n        self.uPP = lambda c : utilityPP(c,gam=self.CRRA)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndefine the value and marginal value function for this period.", "response": "def defValueFuncs(self):\n        '''\n        Defines the value and marginal value function for this period.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        MPCnvrs      = self.MPC**(-self.CRRA/(1.0-self.CRRA))\n        vFuncNvrs    = LinearInterp(np.array([self.mNrmMin, self.mNrmMin+1.0]),np.array([0.0, MPCnvrs]))\n        self.vFunc   = ValueFunc(vFuncNvrs,self.CRRA)\n        self.vPfunc  = MargValueFunc(self.cFunc,self.CRRA)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef makePFcFunc(self):\n        '''\n        Makes the (linear) consumption function for this period.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        # Calculate human wealth this period (and lower bound of m)\n        self.hNrmNow = (self.PermGroFac/self.Rfree)*(self.solution_next.hNrm + 1.0)\n        self.mNrmMin = -self.hNrmNow\n        # Calculate the (constant) marginal propensity to consume\n        PatFac       = ((self.Rfree*self.DiscFacEff)**(1.0/self.CRRA))/self.Rfree\n        self.MPC     = 1.0/(1.0 + PatFac/self.solution_next.MPCmin)\n        # Construct the consumption function\n        self.cFunc   = LinearInterp([self.mNrmMin, self.mNrmMin+1.0],[0.0, self.MPC])\n        # Add two attributes to enable calculation of steady state market resources\n        self.ExIncNext = 1.0 # Perfect foresight income of 1\n        self.mNrmMinNow = self.mNrmMin", "response": "Makes the linear consumption function for this period."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind steady state (normalized) market resources and adds it to the solution. This is the level of market resources such that the expectation of market resources in the next period is unchanged. This value doesn't necessarily exist. Parameters ---------- solution : ConsumerSolution Solution to this period's problem, which must have attribute cFunc. Returns ------- solution : ConsumerSolution Same solution that was passed, but now with the attribute mNrmSS.", "response": "def addSSmNrm(self,solution):\n        '''\n        Finds steady state (normalized) market resources and adds it to the\n        solution.  This is the level of market resources such that the expectation\n        of market resources in the next period is unchanged.  This value doesn't\n        necessarily exist.\n\n        Parameters\n        ----------\n        solution : ConsumerSolution\n            Solution to this period's problem, which must have attribute cFunc.\n        Returns\n        -------\n        solution : ConsumerSolution\n            Same solution that was passed, but now with the attribute mNrmSS.\n        '''\n        # Make a linear function of all combinations of c and m that yield mNext = mNow\n        mZeroChangeFunc = lambda m : (1.0-self.PermGroFac/self.Rfree)*m + (self.PermGroFac/self.Rfree)*self.ExIncNext\n\n        # Find the steady state level of market resources\n        searchSSfunc = lambda m : solution.cFunc(m) - mZeroChangeFunc(m) # A zero of this is SS market resources\n        m_init_guess = self.mNrmMinNow + self.ExIncNext # Minimum market resources plus next income is okay starting guess\n        try:\n            mNrmSS = newton(searchSSfunc,m_init_guess)\n        except:\n            mNrmSS = None\n\n        # Add mNrmSS to the solution and return it\n        solution.mNrmSS = mNrmSS\n        return solution"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef solve(self):\n        '''\n        Solves the one period perfect foresight consumption-saving problem.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        solution : ConsumerSolution\n            The solution to this period's problem.\n        '''\n        self.defUtilityFuncs()\n        self.DiscFacEff = self.DiscFac*self.LivPrb\n        self.makePFcFunc()\n        self.defValueFuncs()\n        solution = ConsumerSolution(cFunc=self.cFunc, vFunc=self.vFunc, vPfunc=self.vPfunc,\n                                    mNrmMin=self.mNrmMin, hNrm=self.hNrmNow,\n                                    MPCmin=self.MPC, MPCmax=self.MPC)\n        #solution = self.addSSmNrm(solution)\n        return solution", "response": "Solves the one period perfect foresight consumption - saving problem. This method creates a new consumer solution for this period."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef assignParameters(self,solution_next,IncomeDstn,LivPrb,DiscFac,CRRA,Rfree,\n                                PermGroFac,BoroCnstArt,aXtraGrid,vFuncBool,CubicBool):\n        '''\n        Assigns period parameters as attributes of self for use by other methods\n\n        Parameters\n        ----------\n        solution_next : ConsumerSolution\n            The solution to next period's one period problem.\n        IncomeDstn : [np.array]\n            A list containing three arrays of floats, representing a discrete\n            approximation to the income process between the period being solved\n            and the one immediately following (in solution_next). Order: event\n            probabilities, permanent shocks, transitory shocks.\n        LivPrb : float\n            Survival probability; likelihood of being alive at the beginning of\n            the succeeding period.\n        DiscFac : float\n            Intertemporal discount factor for future utility.\n        CRRA : float\n            Coefficient of relative risk aversion.\n        Rfree : float\n            Risk free interest factor on end-of-period assets.\n        PermGroFac : float\n            Expected permanent income growth factor at the end of this period.\n        BoroCnstArt: float or None\n            Borrowing constraint for the minimum allowable assets to end the\n            period with.  If it is less than the natural borrowing constraint,\n            then it is irrelevant; BoroCnstArt=None indicates no artificial bor-\n            rowing constraint.\n        aXtraGrid: np.array\n            Array of \"extra\" end-of-period asset values-- assets above the\n            absolute minimum acceptable level.\n        vFuncBool: boolean\n            An indicator for whether the value function should be computed and\n            included in the reported solution.\n        CubicBool: boolean\n            An indicator for whether the solver should use cubic or linear inter-\n            polation.\n\n        Returns\n        -------\n        none\n        '''\n        ConsPerfForesightSolver.assignParameters(self,solution_next,DiscFac,LivPrb,\n                                                CRRA,Rfree,PermGroFac)\n        self.BoroCnstArt    = BoroCnstArt\n        self.IncomeDstn     = IncomeDstn\n        self.aXtraGrid      = aXtraGrid\n        self.vFuncBool      = vFuncBool\n        self.CubicBool      = CubicBool", "response": "Assigns period parameters as attributes of self for use by other methods. This method is called by the constructor of the ConsumerSolution class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndefine the utility functions for this period and saves them as attributes of self for other methods .", "response": "def defUtilityFuncs(self):\n        '''\n        Defines CRRA utility function for this period (and its derivatives,\n        and their inverses), saving them as attributes of self for other methods\n        to use.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        ConsPerfForesightSolver.defUtilityFuncs(self)\n        self.uPinv     = lambda u : utilityP_inv(u,gam=self.CRRA)\n        self.uPinvP    = lambda u : utilityP_invP(u,gam=self.CRRA)\n        self.uinvP     = lambda u : utility_invP(u,gam=self.CRRA)\n        if self.vFuncBool:\n            self.uinv  = lambda u : utility_inv(u,gam=self.CRRA)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setAndUpdateValues(self,solution_next,IncomeDstn,LivPrb,DiscFac):\n        '''\n        Unpacks some of the inputs (and calculates simple objects based on them),\n        storing the results in self for use by other methods.  These include:\n        income shocks and probabilities, next period's marginal value function\n        (etc), the probability of getting the worst income shock next period,\n        the patience factor, human wealth, and the bounding MPCs.\n\n        Parameters\n        ----------\n        solution_next : ConsumerSolution\n            The solution to next period's one period problem.\n        IncomeDstn : [np.array]\n            A list containing three arrays of floats, representing a discrete\n            approximation to the income process between the period being solved\n            and the one immediately following (in solution_next). Order: event\n            probabilities, permanent shocks, transitory shocks.\n        LivPrb : float\n            Survival probability; likelihood of being alive at the beginning of\n            the succeeding period.\n        DiscFac : float\n            Intertemporal discount factor for future utility.\n\n        Returns\n        -------\n        None\n        '''\n        self.DiscFacEff       = DiscFac*LivPrb # \"effective\" discount factor\n        self.ShkPrbsNext      = IncomeDstn[0]\n        self.PermShkValsNext  = IncomeDstn[1]\n        self.TranShkValsNext  = IncomeDstn[2]\n        self.PermShkMinNext   = np.min(self.PermShkValsNext)\n        self.TranShkMinNext   = np.min(self.TranShkValsNext)\n        self.vPfuncNext       = solution_next.vPfunc\n        self.WorstIncPrb      = np.sum(self.ShkPrbsNext[\n                                (self.PermShkValsNext*self.TranShkValsNext)==\n                                (self.PermShkMinNext*self.TranShkMinNext)])\n\n        if self.CubicBool:\n            self.vPPfuncNext  = solution_next.vPPfunc\n\n        if self.vFuncBool:\n            self.vFuncNext    = solution_next.vFunc\n\n        # Update the bounding MPCs and PDV of human wealth:\n        self.PatFac       = ((self.Rfree*self.DiscFacEff)**(1.0/self.CRRA))/self.Rfree\n        self.MPCminNow    = 1.0/(1.0 + self.PatFac/solution_next.MPCmin)\n        self.ExIncNext    = np.dot(self.ShkPrbsNext,self.TranShkValsNext*self.PermShkValsNext)\n        self.hNrmNow      = self.PermGroFac/self.Rfree*(self.ExIncNext + solution_next.hNrm)\n        self.MPCmaxNow    = 1.0/(1.0 + (self.WorstIncPrb**(1.0/self.CRRA))*\n                                        self.PatFac/solution_next.MPCmax)", "response": "Sets the values of the object self based on the values of the income shocks and the income steward and the discharge factor of the next period. This method updates the internal state of self based on the values of the income shocks and the income steward and the discharge factor of the next period."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms preparatory work before calculating the unconstrained consumption function.", "response": "def prepareToSolve(self):\n        '''\n        Perform preparatory work before calculating the unconstrained consumption\n        function.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        self.setAndUpdateValues(self.solution_next,self.IncomeDstn,self.LivPrb,self.DiscFac)\n        self.defBoroCnst(self.BoroCnstArt)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprepare to calculate the end - of - period marginal value for the agent.", "response": "def prepareToCalcEndOfPrdvP(self):\n        '''\n        Prepare to calculate end-of-period marginal value by creating an array\n        of market resources that the agent could have next period, considering\n        the grid of end-of-period assets and the distribution of shocks he might\n        experience next period.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        aNrmNow : np.array\n            A 1D array of end-of-period assets; also stored as attribute of self.\n        '''\n\n        # We define aNrmNow all the way from BoroCnstNat up to max(self.aXtraGrid)\n        # even if BoroCnstNat < BoroCnstArt, so we can construct the consumption\n        # function as the lower envelope of the (by the artificial borrowing con-\n        # straint) uconstrained consumption function, and the artificially con-\n        # strained consumption function.\n        aNrmNow     = np.asarray(self.aXtraGrid) + self.BoroCnstNat\n        ShkCount    = self.TranShkValsNext.size\n        aNrm_temp   = np.tile(aNrmNow,(ShkCount,1))\n\n        # Tile arrays of the income shocks and put them into useful shapes\n        aNrmCount         = aNrmNow.shape[0]\n        PermShkVals_temp  = (np.tile(self.PermShkValsNext,(aNrmCount,1))).transpose()\n        TranShkVals_temp  = (np.tile(self.TranShkValsNext,(aNrmCount,1))).transpose()\n        ShkPrbs_temp      = (np.tile(self.ShkPrbsNext,(aNrmCount,1))).transpose()\n\n        # Get cash on hand next period\n        mNrmNext          = self.Rfree/(self.PermGroFac*PermShkVals_temp)*aNrm_temp + TranShkVals_temp\n\n        # Store and report the results\n        self.PermShkVals_temp  = PermShkVals_temp\n        self.ShkPrbs_temp      = ShkPrbs_temp\n        self.mNrmNext          = mNrmNext\n        self.aNrmNow           = aNrmNow\n        return aNrmNow"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the end - of - period marginal value of the assets at each point in the time series aNrmNow. This is done by taking a weighted sum of next period marginal values across the income shocks and calculating the marginal value of the assets at each point in the time series.", "response": "def calcEndOfPrdvP(self):\n        '''\n        Calculate end-of-period marginal value of assets at each point in aNrmNow.\n        Does so by taking a weighted sum of next period marginal values across\n        income shocks (in a preconstructed grid self.mNrmNext).\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        EndOfPrdvP : np.array\n            A 1D array of end-of-period marginal value of assets\n        '''\n\n        EndOfPrdvP  = self.DiscFacEff*self.Rfree*self.PermGroFac**(-self.CRRA)*np.sum(\n                      self.PermShkVals_temp**(-self.CRRA)*\n                      self.vPfuncNext(self.mNrmNext)*self.ShkPrbs_temp,axis=0)\n        return EndOfPrdvP"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind interpolation points (c,m) for the consumption function. Parameters ---------- EndOfPrdvP : np.array Array of end-of-period marginal values. aNrmNow : np.array Array of end-of-period asset values that yield the marginal values in EndOfPrdvP. Returns ------- c_for_interpolation : np.array Consumption points for interpolation. m_for_interpolation : np.array Corresponding market resource points for interpolation.", "response": "def getPointsForInterpolation(self,EndOfPrdvP,aNrmNow):\n        '''\n        Finds interpolation points (c,m) for the consumption function.\n\n        Parameters\n        ----------\n        EndOfPrdvP : np.array\n            Array of end-of-period marginal values.\n        aNrmNow : np.array\n            Array of end-of-period asset values that yield the marginal values\n            in EndOfPrdvP.\n\n        Returns\n        -------\n        c_for_interpolation : np.array\n            Consumption points for interpolation.\n        m_for_interpolation : np.array\n            Corresponding market resource points for interpolation.\n        '''\n        cNrmNow = self.uPinv(EndOfPrdvP)\n        mNrmNow = cNrmNow + aNrmNow\n\n        # Limiting consumption is zero as m approaches mNrmMin\n        c_for_interpolation = np.insert(cNrmNow,0,0.,axis=-1)\n        m_for_interpolation = np.insert(mNrmNow,0,self.BoroCnstNat,axis=-1)\n\n        # Store these for calcvFunc\n        self.cNrmNow = cNrmNow\n        self.mNrmNow = mNrmNow\n\n        return c_for_interpolation,m_for_interpolation"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct a basic solution for this period including the consumption points for interpolation.", "response": "def usePointsForInterpolation(self,cNrm,mNrm,interpolator):\n        '''\n        Constructs a basic solution for this period, including the consumption\n        function and marginal value function.\n\n        Parameters\n        ----------\n        cNrm : np.array\n            (Normalized) consumption points for interpolation.\n        mNrm : np.array\n            (Normalized) corresponding market resource points for interpolation.\n        interpolator : function\n            A function that constructs and returns a consumption function.\n\n        Returns\n        -------\n        solution_now : ConsumerSolution\n            The solution to this period's consumption-saving problem, with a\n            consumption function, marginal value function, and minimum m.\n        '''\n        # Construct the unconstrained consumption function\n        cFuncNowUnc = interpolator(mNrm,cNrm)\n\n        # Combine the constrained and unconstrained functions into the true consumption function\n        cFuncNow = LowerEnvelope(cFuncNowUnc,self.cFuncNowCnst)\n\n        # Make the marginal value function and the marginal marginal value function\n        vPfuncNow = MargValueFunc(cFuncNow,self.CRRA)\n\n        # Pack up the solution and return it\n        solution_now = ConsumerSolution(cFunc=cFuncNow, vPfunc=vPfuncNow, mNrmMin=self.mNrmMinNow)\n        return solution_now"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive end of period assets and end of period marginal value, construct the basic solution for this period. Parameters ---------- EndOfPrdvP : np.array Array of end-of-period marginal values. aNrm : np.array Array of end-of-period asset values that yield the marginal values in EndOfPrdvP. interpolator : function A function that constructs and returns a consumption function. Returns ------- solution_now : ConsumerSolution The solution to this period's consumption-saving problem, with a consumption function, marginal value function, and minimum m.", "response": "def makeBasicSolution(self,EndOfPrdvP,aNrm,interpolator):\n        '''\n        Given end of period assets and end of period marginal value, construct\n        the basic solution for this period.\n\n        Parameters\n        ----------\n        EndOfPrdvP : np.array\n            Array of end-of-period marginal values.\n        aNrm : np.array\n            Array of end-of-period asset values that yield the marginal values\n            in EndOfPrdvP.\n\n        interpolator : function\n            A function that constructs and returns a consumption function.\n\n        Returns\n        -------\n        solution_now : ConsumerSolution\n            The solution to this period's consumption-saving problem, with a\n            consumption function, marginal value function, and minimum m.\n        '''\n        cNrm,mNrm    = self.getPointsForInterpolation(EndOfPrdvP,aNrm)\n        solution_now = self.usePointsForInterpolation(cNrm,mNrm,interpolator)\n        return solution_now"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef addMPCandHumanWealth(self,solution):\n        '''\n        Take a solution and add human wealth and the bounding MPCs to it.\n\n        Parameters\n        ----------\n        solution : ConsumerSolution\n            The solution to this period's consumption-saving problem.\n\n        Returns:\n        ----------\n        solution : ConsumerSolution\n            The solution to this period's consumption-saving problem, but now\n            with human wealth and the bounding MPCs.\n        '''\n        solution.hNrm   = self.hNrmNow\n        solution.MPCmin = self.MPCminNow\n        solution.MPCmax = self.MPCmaxEff\n        return solution", "response": "Take a solution and add human wealth and the bounding MPCs to it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef makeLinearcFunc(self,mNrm,cNrm):\n        '''\n        Makes a linear interpolation to represent the (unconstrained) consumption function.\n\n        Parameters\n        ----------\n        mNrm : np.array\n            Corresponding market resource points for interpolation.\n        cNrm : np.array\n            Consumption points for interpolation.\n\n        Returns\n        -------\n        cFuncUnc : LinearInterp\n            The unconstrained consumption function for this period.\n        '''\n        cFuncUnc = LinearInterp(mNrm,cNrm,self.MPCminNow*self.hNrmNow,self.MPCminNow)\n        return cFuncUnc", "response": "Makes a linear interpolation to represent the unconstrained consumption function for this period."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsolve a one period consumption saving problem with risky income.", "response": "def solve(self):\n        '''\n        Solves a one period consumption saving problem with risky income.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        solution : ConsumerSolution\n            The solution to the one period problem.\n        '''\n        aNrm       = self.prepareToCalcEndOfPrdvP()\n        EndOfPrdvP = self.calcEndOfPrdvP()\n        solution   = self.makeBasicSolution(EndOfPrdvP,aNrm,self.makeLinearcFunc)\n        solution   = self.addMPCandHumanWealth(solution)\n        return solution"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes a cubic spline interpolation of the unconstrained consumption function for this period.", "response": "def makeCubiccFunc(self,mNrm,cNrm):\n        '''\n        Makes a cubic spline interpolation of the unconstrained consumption\n        function for this period.\n\n        Parameters\n        ----------\n        mNrm : np.array\n            Corresponding market resource points for interpolation.\n        cNrm : np.array\n            Consumption points for interpolation.\n\n        Returns\n        -------\n        cFuncUnc : CubicInterp\n            The unconstrained consumption function for this period.\n        '''\n        EndOfPrdvPP = self.DiscFacEff*self.Rfree*self.Rfree*self.PermGroFac**(-self.CRRA-1.0)* \\\n                      np.sum(self.PermShkVals_temp**(-self.CRRA-1.0)*\n                             self.vPPfuncNext(self.mNrmNext)*self.ShkPrbs_temp,axis=0)\n        dcda        = EndOfPrdvPP/self.uPP(np.array(cNrm[1:]))\n        MPC         = dcda/(dcda+1.)\n        MPC         = np.insert(MPC,0,self.MPCmaxNow)\n\n        cFuncNowUnc = CubicInterp(mNrm,cNrm,MPC,self.MPCminNow*self.hNrmNow,self.MPCminNow)\n        return cFuncNowUnc"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef makeEndOfPrdvFunc(self,EndOfPrdvP):\n        '''\n        Construct the end-of-period value function for this period, storing it\n        as an attribute of self for use by other methods.\n\n        Parameters\n        ----------\n        EndOfPrdvP : np.array\n            Array of end-of-period marginal value of assets corresponding to the\n            asset values in self.aNrmNow.\n\n        Returns\n        -------\n        none\n        '''\n        VLvlNext            = (self.PermShkVals_temp**(1.0-self.CRRA)*\\\n                               self.PermGroFac**(1.0-self.CRRA))*self.vFuncNext(self.mNrmNext)\n        EndOfPrdv           = self.DiscFacEff*np.sum(VLvlNext*self.ShkPrbs_temp,axis=0)\n        EndOfPrdvNvrs       = self.uinv(EndOfPrdv) # value transformed through inverse utility\n        EndOfPrdvNvrsP      = EndOfPrdvP*self.uinvP(EndOfPrdv)\n        EndOfPrdvNvrs       = np.insert(EndOfPrdvNvrs,0,0.0)\n        EndOfPrdvNvrsP      = np.insert(EndOfPrdvNvrsP,0,EndOfPrdvNvrsP[0]) # This is a very good approximation, vNvrsPP = 0 at the asset minimum\n        aNrm_temp           = np.insert(self.aNrmNow,0,self.BoroCnstNat)\n        EndOfPrdvNvrsFunc   = CubicInterp(aNrm_temp,EndOfPrdvNvrs,EndOfPrdvNvrsP)\n        self.EndOfPrdvFunc  = ValueFunc(EndOfPrdvNvrsFunc,self.CRRA)", "response": "Constructs the end - of - period value function for this period storing it\n        EndOfPrdvP as an attribute of self."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates the value function for this period adds it to the solution.", "response": "def addvFunc(self,solution,EndOfPrdvP):\n        '''\n        Creates the value function for this period and adds it to the solution.\n\n        Parameters\n        ----------\n        solution : ConsumerSolution\n            The solution to this single period problem, likely including the\n            consumption function, marginal value function, etc.\n        EndOfPrdvP : np.array\n            Array of end-of-period marginal value of assets corresponding to the\n            asset values in self.aNrmNow.\n\n        Returns\n        -------\n        solution : ConsumerSolution\n            The single period solution passed as an input, but now with the\n            value function (defined over market resources m) as an attribute.\n        '''\n        self.makeEndOfPrdvFunc(EndOfPrdvP)\n        solution.vFunc = self.makevFunc(solution)\n        return solution"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef makevFunc(self,solution):\n        '''\n        Creates the value function for this period, defined over market resources m.\n        self must have the attribute EndOfPrdvFunc in order to execute.\n\n        Parameters\n        ----------\n        solution : ConsumerSolution\n            The solution to this single period problem, which must include the\n            consumption function.\n\n        Returns\n        -------\n        vFuncNow : ValueFunc\n            A representation of the value function for this period, defined over\n            normalized market resources m: v = vFuncNow(m).\n        '''\n        # Compute expected value and marginal value on a grid of market resources\n        mNrm_temp   = self.mNrmMinNow + self.aXtraGrid\n        cNrmNow     = solution.cFunc(mNrm_temp)\n        aNrmNow     = mNrm_temp - cNrmNow\n        vNrmNow     = self.u(cNrmNow) + self.EndOfPrdvFunc(aNrmNow)\n        vPnow       = self.uP(cNrmNow)\n\n        # Construct the beginning-of-period value function\n        vNvrs        = self.uinv(vNrmNow) # value transformed through inverse utility\n        vNvrsP       = vPnow*self.uinvP(vNrmNow)\n        mNrm_temp    = np.insert(mNrm_temp,0,self.mNrmMinNow)\n        vNvrs        = np.insert(vNvrs,0,0.0)\n        vNvrsP       = np.insert(vNvrsP,0,self.MPCmaxEff**(-self.CRRA/(1.0-self.CRRA)))\n        MPCminNvrs   = self.MPCminNow**(-self.CRRA/(1.0-self.CRRA))\n        vNvrsFuncNow = CubicInterp(mNrm_temp,vNvrs,vNvrsP,MPCminNvrs*self.hNrmNow,MPCminNvrs)\n        vFuncNow     = ValueFunc(vNvrsFuncNow,self.CRRA)\n        return vFuncNow", "response": "Creates the value function for this period defined over market resources m. This function creates the value function for this period defined over market resources m. This function is called by the value function constructor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding the marginal value function to an existing solution so that the next solver can evaluate vPP.", "response": "def addvPPfunc(self,solution):\n        '''\n        Adds the marginal marginal value function to an existing solution, so\n        that the next solver can evaluate vPP and thus use cubic interpolation.\n\n        Parameters\n        ----------\n        solution : ConsumerSolution\n            The solution to this single period problem, which must include the\n            consumption function.\n\n        Returns\n        -------\n        solution : ConsumerSolution\n            The same solution passed as input, but with the marginal marginal\n            value function for this period added as the attribute vPPfunc.\n        '''\n        vPPfuncNow        = MargMargValueFunc(solution.cFunc,self.CRRA)\n        solution.vPPfunc  = vPPfuncNow\n        return solution"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef solve(self):\n        '''\n        Solves the single period consumption-saving problem using the method of\n        endogenous gridpoints.  Solution includes a consumption function cFunc\n        (using cubic or linear splines), a marginal value function vPfunc, a min-\n        imum acceptable level of normalized market resources mNrmMin, normalized\n        human wealth hNrm, and bounding MPCs MPCmin and MPCmax.  It might also\n        have a value function vFunc and marginal marginal value function vPPfunc.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        solution : ConsumerSolution\n            The solution to the single period consumption-saving problem.\n        '''\n        # Make arrays of end-of-period assets and end-of-period marginal value\n        aNrm         = self.prepareToCalcEndOfPrdvP()\n        EndOfPrdvP   = self.calcEndOfPrdvP()\n\n        # Construct a basic solution for this period\n        if self.CubicBool:\n            solution   = self.makeBasicSolution(EndOfPrdvP,aNrm,interpolator=self.makeCubiccFunc)\n        else:\n            solution   = self.makeBasicSolution(EndOfPrdvP,aNrm,interpolator=self.makeLinearcFunc)\n        solution       = self.addMPCandHumanWealth(solution) # add a few things\n        solution       = self.addSSmNrm(solution) # find steady state m\n\n        # Add the value function if requested, as well as the marginal marginal\n        # value function if cubic splines were used (to prepare for next period)\n        if self.vFuncBool:\n            solution = self.addvFunc(solution,EndOfPrdvP)\n        if self.CubicBool:\n            solution = self.addvPPfunc(solution)\n        return solution", "response": "Solves the single period consumption - saving problem using the method of\n        endogenous gridpoints."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprepares to calculate the end - of - period marginal value for the agent.", "response": "def prepareToCalcEndOfPrdvP(self):\n        '''\n        Prepare to calculate end-of-period marginal value by creating an array\n        of market resources that the agent could have next period, considering\n        the grid of end-of-period assets and the distribution of shocks he might\n        experience next period.  This differs from the baseline case because\n        different savings choices yield different interest rates.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        aNrmNow : np.array\n            A 1D array of end-of-period assets; also stored as attribute of self.\n        '''\n        KinkBool = self.Rboro > self.Rsave # Boolean indicating that there is actually a kink.\n        # When Rboro == Rsave, this method acts just like it did in IndShock.\n        # When Rboro < Rsave, the solver would have terminated when it was called.\n\n        # Make a grid of end-of-period assets, including *two* copies of a=0\n        if KinkBool:\n            aNrmNow       = np.sort(np.hstack((np.asarray(self.aXtraGrid) + self.mNrmMinNow,\n                                                   np.array([0.0,0.0]))))\n        else:\n            aNrmNow       = np.asarray(self.aXtraGrid) + self.mNrmMinNow\n        aXtraCount        = aNrmNow.size\n\n        # Make tiled versions of the assets grid and income shocks\n        ShkCount          = self.TranShkValsNext.size\n        aNrm_temp         = np.tile(aNrmNow,(ShkCount,1))\n        PermShkVals_temp  = (np.tile(self.PermShkValsNext,(aXtraCount,1))).transpose()\n        TranShkVals_temp  = (np.tile(self.TranShkValsNext,(aXtraCount,1))).transpose()\n        ShkPrbs_temp      = (np.tile(self.ShkPrbsNext,(aXtraCount,1))).transpose()\n\n        # Make a 1D array of the interest factor at each asset gridpoint\n        Rfree_vec         = self.Rsave*np.ones(aXtraCount)\n        if KinkBool:\n            Rfree_vec[0:(np.sum(aNrmNow<=0)-1)] = self.Rboro\n        self.Rfree        = Rfree_vec\n        Rfree_temp        = np.tile(Rfree_vec,(ShkCount,1))\n\n        # Make an array of market resources that we could have next period,\n        # considering the grid of assets and the income shocks that could occur\n        mNrmNext          = Rfree_temp/(self.PermGroFac*PermShkVals_temp)*aNrm_temp + TranShkVals_temp\n\n        # Recalculate the minimum MPC and human wealth using the interest factor on saving.\n        # This overwrites values from setAndUpdateValues, which were based on Rboro instead.\n        if KinkBool:\n            PatFacTop         = ((self.Rsave*self.DiscFacEff)**(1.0/self.CRRA))/self.Rsave\n            self.MPCminNow    = 1.0/(1.0 + PatFacTop/self.solution_next.MPCmin)\n            self.hNrmNow      = self.PermGroFac/self.Rsave*(np.dot(self.ShkPrbsNext,\n                                self.TranShkValsNext*self.PermShkValsNext) + self.solution_next.hNrm)\n\n        # Store some of the constructed arrays for later use and return the assets grid\n        self.PermShkVals_temp = PermShkVals_temp\n        self.ShkPrbs_temp     = ShkPrbs_temp\n        self.mNrmNext         = mNrmNext\n        self.aNrmNow          = aNrmNow\n        return aNrmNow"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the terminal period solution. This method should be run when a new AgentType is created and CRRA changes.", "response": "def updateSolutionTerminal(self):\n        '''\n        Update the terminal period solution.  This method should be run when a\n        new AgentType is created or when CRRA changes.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        self.solution_terminal.vFunc   = ValueFunc(self.cFunc_terminal_,self.CRRA)\n        self.solution_terminal.vPfunc  = MargValueFunc(self.cFunc_terminal_,self.CRRA)\n        self.solution_terminal.vPPfunc = MargMargValueFunc(self.cFunc_terminal_,self.CRRA)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unpackcFunc(self):\n        '''\n        \"Unpacks\" the consumption functions into their own field for easier access.\n        After the model has been solved, the consumption functions reside in the\n        attribute cFunc of each element of ConsumerType.solution.  This method\n        creates a (time varying) attribute cFunc that contains a list of consumption\n        functions.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        self.cFunc = []\n        for solution_t in self.solution:\n            self.cFunc.append(solution_t.cFunc)\n        self.addToTimeVary('cFunc')", "response": "Unpacks the consumption functions into their own field for easier access."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake new consumers for the given indices.", "response": "def simBirth(self,which_agents):\n        '''\n        Makes new consumers for the given indices.  Initialized variables include aNrm and pLvl, as\n        well as time variables t_age and t_cycle.  Normalized assets and permanent income levels\n        are drawn from lognormal distributions given by aNrmInitMean and aNrmInitStd (etc).\n\n        Parameters\n        ----------\n        which_agents : np.array(Bool)\n            Boolean array of size self.AgentCount indicating which agents should be \"born\".\n\n        Returns\n        -------\n        None\n        '''\n        # Get and store states for newly born agents\n        N = np.sum(which_agents) # Number of new consumers to make\n        self.aNrmNow[which_agents] = drawLognormal(N,mu=self.aNrmInitMean,sigma=self.aNrmInitStd,seed=self.RNG.randint(0,2**31-1))\n        pLvlInitMeanNow = self.pLvlInitMean + np.log(self.PlvlAggNow) # Account for newer cohorts having higher permanent income\n        self.pLvlNow[which_agents] = drawLognormal(N,mu=pLvlInitMeanNow,sigma=self.pLvlInitStd,seed=self.RNG.randint(0,2**31-1))\n        self.t_age[which_agents]   = 0 # How many periods since each agent was born\n        self.t_cycle[which_agents] = 0 # Which period of the cycle each agent is currently in\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining which agents die this period and must be replaced.", "response": "def simDeath(self):\n        '''\n        Determines which agents die this period and must be replaced.  Uses the sequence in LivPrb\n        to determine survival probabilities for each agent.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        which_agents : np.array(bool)\n            Boolean array of size AgentCount indicating which agents die.\n        '''\n        # Determine who dies\n        DiePrb_by_t_cycle = 1.0 - np.asarray(self.LivPrb)\n        DiePrb = DiePrb_by_t_cycle[self.t_cycle-1] # Time has already advanced, so look back one\n        DeathShks = drawUniform(N=self.AgentCount,seed=self.RNG.randint(0,2**31-1))\n        which_agents = DeathShks < DiePrb\n        if self.T_age is not None: # Kill agents that have lived for too many periods\n            too_old = self.t_age >= self.T_age\n            which_agents = np.logical_or(which_agents,too_old)\n        return which_agents"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getShocks(self):\n        '''\n        Finds permanent and transitory income \"shocks\" for each agent this period.  As this is a\n        perfect foresight model, there are no stochastic shocks: PermShkNow = PermGroFac for each\n        agent (according to their t_cycle) and TranShkNow = 1.0 for all agents.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        PermGroFac = np.array(self.PermGroFac)\n        self.PermShkNow = PermGroFac[self.t_cycle-1] # cycle time has already been advanced\n        self.TranShkNow = np.ones(self.AgentCount)", "response": "Finds permanent and transitory income shocks for each agent in this period."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate updated values of normalized market resources and permanent income level for each agent. Uses pLvlNow aNrmNow PermShkNow TranShkNow. Returns None if there are no new states.", "response": "def getStates(self):\n        '''\n        Calculates updated values of normalized market resources and permanent income level for each\n        agent.  Uses pLvlNow, aNrmNow, PermShkNow, TranShkNow.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        pLvlPrev = self.pLvlNow\n        aNrmPrev = self.aNrmNow\n        RfreeNow = self.getRfree()\n\n        # Calculate new states: normalized market resources and permanent income level\n        self.pLvlNow = pLvlPrev*self.PermShkNow # Updated permanent income level\n        self.PlvlAggNow = self.PlvlAggNow*self.PermShkAggNow # Updated aggregate permanent productivity level\n        ReffNow      = RfreeNow/self.PermShkNow # \"Effective\" interest factor on normalized assets\n        self.bNrmNow = ReffNow*aNrmPrev         # Bank balances before labor income\n        self.mNrmNow = self.bNrmNow + self.TranShkNow # Market resources after income\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the income process of this agent based on his own attributes.", "response": "def updateIncomeProcess(self):\n        '''\n        Updates this agent's income process based on his own attributes.\n\n        Parameters\n        ----------\n        none\n\n        Returns:\n        -----------\n        none\n        '''\n        original_time = self.time_flow\n        self.timeFwd()\n        IncomeDstn, PermShkDstn, TranShkDstn = constructLognormalIncomeProcessUnemployment(self)\n        self.IncomeDstn = IncomeDstn\n        self.PermShkDstn = PermShkDstn\n        self.TranShkDstn = TranShkDstn\n        self.addToTimeVary('IncomeDstn','PermShkDstn','TranShkDstn')\n        if not original_time:\n            self.timeRev()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef updateAssetsGrid(self):\n        '''\n        Updates this agent's end-of-period assets grid by constructing a multi-\n        exponentially spaced grid of aXtra values.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        '''\n        aXtraGrid = constructAssetsGrid(self)\n        self.aXtraGrid = aXtraGrid\n        self.addToTimeInv('aXtraGrid')", "response": "Updates this agent s end - of - period assets grid by constructing a multi - nanos exponentially spaced grid of aXtra values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef makeEulerErrorFunc(self,mMax=100,approx_inc_dstn=True):\n        '''\n        Creates a \"normalized Euler error\" function for this instance, mapping\n        from market resources to \"consumption error per dollar of consumption.\"\n        Stores result in attribute eulerErrorFunc as an interpolated function.\n        Has option to use approximate income distribution stored in self.IncomeDstn\n        or to use a (temporary) very dense approximation.\n\n        Only works on (one period) infinite horizon models at this time, will\n        be generalized later.\n\n        Parameters\n        ----------\n        mMax : float\n            Maximum normalized market resources for the Euler error function.\n        approx_inc_dstn : Boolean\n            Indicator for whether to use the approximate discrete income distri-\n            bution stored in self.IncomeDstn[0], or to use a very accurate\n            discrete approximation instead.  When True, uses approximation in\n            IncomeDstn; when False, makes and uses a very dense approximation.\n\n        Returns\n        -------\n        None\n        '''\n        # Get the income distribution (or make a very dense one)\n        if approx_inc_dstn:\n            IncomeDstn = self.IncomeDstn[0]\n        else:\n            TranShkDstn = approxMeanOneLognormal(N=200,sigma=self.TranShkStd[0],\n                                                 tail_N=50,tail_order=1.3, tail_bound=[0.05,0.95])\n            TranShkDstn = addDiscreteOutcomeConstantMean(TranShkDstn,self.UnempPrb,self.IncUnemp)\n            PermShkDstn = approxMeanOneLognormal(N=200,sigma=self.PermShkStd[0],\n                                                 tail_N=50,tail_order=1.3, tail_bound=[0.05,0.95])\n            IncomeDstn  = combineIndepDstns(PermShkDstn,TranShkDstn)\n\n        # Make a grid of market resources\n        mNowMin  = self.solution[0].mNrmMin + 10**(-15) # add tiny bit to get around 0/0 problem\n        mNowMax  = mMax\n        mNowGrid = np.linspace(mNowMin,mNowMax,1000)\n\n        # Get the consumption function this period and the marginal value function\n        # for next period.  Note that this part assumes a one period cycle.\n        cFuncNow   = self.solution[0].cFunc\n        vPfuncNext = self.solution[0].vPfunc\n\n        # Calculate consumption this period at each gridpoint (and assets)\n        cNowGrid = cFuncNow(mNowGrid)\n        aNowGrid = mNowGrid - cNowGrid\n\n        # Tile the grids for fast computation\n        ShkCount          = IncomeDstn[0].size\n        aCount            = aNowGrid.size\n        aNowGrid_tiled    = np.tile(aNowGrid,(ShkCount,1))\n        PermShkVals_tiled = (np.tile(IncomeDstn[1],(aCount,1))).transpose()\n        TranShkVals_tiled = (np.tile(IncomeDstn[2],(aCount,1))).transpose()\n        ShkPrbs_tiled     = (np.tile(IncomeDstn[0],(aCount,1))).transpose()\n\n        # Calculate marginal value next period for each gridpoint and each shock\n        mNextArray        = self.Rfree/(self.PermGroFac[0]*PermShkVals_tiled)*aNowGrid_tiled + TranShkVals_tiled\n        vPnextArray       = vPfuncNext(mNextArray)\n\n        # Calculate expected marginal value and implied optimal consumption\n        ExvPnextGrid = self.DiscFac*self.Rfree*self.LivPrb[0]*self.PermGroFac[0]**(-self.CRRA)* \\\n                       np.sum(PermShkVals_tiled**(-self.CRRA)*vPnextArray*ShkPrbs_tiled,axis=0)\n        cOptGrid     = ExvPnextGrid**(-1.0/self.CRRA)\n\n        # Calculate Euler error and store an interpolated function\n        EulerErrorNrmGrid = (cNowGrid - cOptGrid)/cOptGrid\n        eulerErrorFunc    = LinearInterp(mNowGrid,EulerErrorNrmGrid)\n        self.eulerErrorFunc = eulerErrorFunc", "response": "Creates an interpolated Euler error function for this instance mapping the market resources from consumption error per dollar of consumption to consumption error per dollar of consumption."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calcBoundingValues(self):\n        '''\n        Calculate human wealth plus minimum and maximum MPC in an infinite\n        horizon model with only one period repeated indefinitely.  Store results\n        as attributes of self.  Human wealth is the present discounted value of\n        expected future income after receiving income this period, ignoring mort-\n        ality.  The maximum MPC is the limit of the MPC as m --> mNrmMin.  The\n        minimum MPC is the limit of the MPC as m --> infty.  This version deals\n        with the different interest rates on borrowing vs saving.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n        # Unpack the income distribution and get average and worst outcomes\n        PermShkValsNext   = self.IncomeDstn[0][1]\n        TranShkValsNext   = self.IncomeDstn[0][2]\n        ShkPrbsNext       = self.IncomeDstn[0][0]\n        ExIncNext         = np.dot(ShkPrbsNext,PermShkValsNext*TranShkValsNext)\n        PermShkMinNext    = np.min(PermShkValsNext)\n        TranShkMinNext    = np.min(TranShkValsNext)\n        WorstIncNext      = PermShkMinNext*TranShkMinNext\n        WorstIncPrb       = np.sum(ShkPrbsNext[(PermShkValsNext*TranShkValsNext)==WorstIncNext])\n\n        # Calculate human wealth and the infinite horizon natural borrowing constraint\n        hNrm              = (ExIncNext*self.PermGroFac[0]/self.Rsave)/(1.0-self.PermGroFac[0]/self.Rsave)\n        temp              = self.PermGroFac[0]*PermShkMinNext/self.Rboro\n        BoroCnstNat       = -TranShkMinNext*temp/(1.0-temp)\n\n        PatFacTop = (self.DiscFac*self.LivPrb[0]*self.Rsave)**(1.0/self.CRRA)/self.Rsave\n        PatFacBot = (self.DiscFac*self.LivPrb[0]*self.Rboro)**(1.0/self.CRRA)/self.Rboro\n        if BoroCnstNat < self.BoroCnstArt:\n            MPCmax    = 1.0 # if natural borrowing constraint is overridden by artificial one, MPCmax is 1\n        else:\n            MPCmax    = 1.0 - WorstIncPrb**(1.0/self.CRRA)*PatFacBot\n        MPCmin = 1.0 - PatFacTop\n\n        # Store the results as attributes of self\n        self.hNrm   = hNrm\n        self.MPCmin = MPCmin\n        self.MPCmax = MPCmax", "response": "Calculates the approximate and maximum of a human wealth plus minimum and maximum MPC in an infinite - level horizon model with only one repeated indefinitely."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getRfree(self):\n        '''\n        Returns an array of size self.AgentCount with self.Rboro or self.Rsave in each entry, based\n        on whether self.aNrmNow >< 0.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        RfreeNow : np.array\n             Array of size self.AgentCount with risk free interest rate for each agent.\n        '''\n        RfreeNow = self.Rboro*np.ones(self.AgentCount)\n        RfreeNow[self.aNrmNow > 0] = self.Rsave\n        return RfreeNow", "response": "Returns an array of size self. AgentCount with risk free interest rate for each agent."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecode the data return some kind of PDU or None.", "response": "def decode_packet(data):\n    \"\"\"decode the data, return some kind of PDU.\"\"\"\n    if _debug: decode_packet._debug(\"decode_packet %r\", data)\n\n    # empty strings are some other kind of pcap content\n    if not data:\n        return None\n\n    # assume it is ethernet for now\n    d = decode_ethernet(data)\n    pduSource = Address(d['source_address'])\n    pduDestination = Address(d['destination_address'])\n    data = d['data']\n\n    # there could be a VLAN header\n    if (d['type'] == 0x8100):\n        if _debug: decode_packet._debug(\"    - vlan found\")\n\n        d = decode_vlan(data)\n        data = d['data']\n\n    # look for IP packets\n    if (d['type'] == 0x0800):\n        if _debug: decode_packet._debug(\"    - IP found\")\n\n        d = decode_ip(data)\n        pduSource, pduDestination = d['source_address'], d['destination_address']\n        data = d['data']\n\n        if (d['protocol'] == 'udp'):\n            if _debug: decode_packet._debug(\"    - UDP found\")\n\n            d = decode_udp(data)\n            data = d['data']\n\n            pduSource = Address((pduSource, d['source_port']))\n            pduDestination = Address((pduDestination, d['destination_port']))\n            if _debug:\n                decode_packet._debug(\"    - pduSource: %r\", pduSource)\n                decode_packet._debug(\"    - pduDestination: %r\", pduDestination)\n        else:\n            if _debug: decode_packet._debug(\"    - not a UDP packet\")\n    else:\n        if _debug: decode_packet._debug(\"    - not an IP packet\")\n\n    # check for empty\n    if not data:\n        if _debug: decode_packet._debug(\"    - empty packet\")\n        return None\n\n    # build a PDU\n    pdu = PDU(data, source=pduSource, destination=pduDestination)\n\n    # check for a BVLL header\n    if (pdu.pduData[0] == 0x81):\n        if _debug: decode_packet._debug(\"    - BVLL header found\")\n\n        try:\n            xpdu = BVLPDU()\n            xpdu.decode(pdu)\n            pdu = xpdu\n        except Exception as err:\n            if _debug: decode_packet._debug(\"    - BVLPDU decoding error: %r\", err)\n            return pdu\n\n        # make a more focused interpretation\n        atype = bvl_pdu_types.get(pdu.bvlciFunction)\n        if not atype:\n            if _debug: decode_packet._debug(\"    - unknown BVLL type: %r\", pdu.bvlciFunction)\n            return pdu\n\n        # decode it as one of the basic types\n        try:\n            xpdu = pdu\n            bpdu = atype()\n            bpdu.decode(pdu)\n            if _debug: decode_packet._debug(\"    - bpdu: %r\", bpdu)\n\n            pdu = bpdu\n\n            # lift the address for forwarded NPDU's\n            if atype is ForwardedNPDU:\n                pdu.pduSource = bpdu.bvlciAddress\n            # no deeper decoding for some\n            elif atype not in (DistributeBroadcastToNetwork, OriginalUnicastNPDU, OriginalBroadcastNPDU):\n                return pdu\n\n        except Exception as err:\n            if _debug: decode_packet._debug(\"    - decoding Error: %r\", err)\n            return xpdu\n\n    # check for version number\n    if (pdu.pduData[0] != 0x01):\n        if _debug: decode_packet._debug(\"    - not a version 1 packet: %s...\", btox(pdu.pduData[:30], '.'))\n        return None\n\n    # it's an NPDU\n    try:\n        npdu = NPDU()\n        npdu.decode(pdu)\n    except Exception as err:\n        if _debug: decode_packet._debug(\"    - decoding Error: %r\", err)\n        return None\n\n    # application or network layer message\n    if npdu.npduNetMessage is None:\n        if _debug: decode_packet._debug(\"    - not a network layer message, try as an APDU\")\n\n        # decode as a generic APDU\n        try:\n            xpdu = APDU()\n            xpdu.decode(npdu)\n            apdu = xpdu\n        except Exception as err:\n            if _debug: decode_packet._debug(\"    - decoding Error: %r\", err)\n            return npdu\n\n        # \"lift\" the source and destination address\n        if npdu.npduSADR:\n            apdu.pduSource = npdu.npduSADR\n        else:\n            apdu.pduSource = npdu.pduSource\n        if npdu.npduDADR:\n            apdu.pduDestination = npdu.npduDADR\n        else:\n            apdu.pduDestination = npdu.pduDestination\n\n        # make a more focused interpretation\n        atype = apdu_types.get(apdu.apduType)\n        if not atype:\n            if _debug: decode_packet._debug(\"    - unknown APDU type: %r\", apdu.apduType)\n            return apdu\n\n        # decode it as one of the basic types\n        try:\n            xpdu = apdu\n            apdu = atype()\n            apdu.decode(xpdu)\n        except Exception as err:\n            if _debug: decode_packet._debug(\"    - decoding Error: %r\", err)\n            return xpdu\n\n        # decode it at the next level\n        if isinstance(apdu, ConfirmedRequestPDU):\n            atype = confirmed_request_types.get(apdu.apduService)\n            if not atype:\n                if _debug: decode_packet._debug(\"    - no confirmed request decoder: %r\", apdu.apduService)\n                return apdu\n\n        elif isinstance(apdu, UnconfirmedRequestPDU):\n            atype = unconfirmed_request_types.get(apdu.apduService)\n            if not atype:\n                if _debug: decode_packet._debug(\"    - no unconfirmed request decoder: %r\", apdu.apduService)\n                return apdu\n\n        elif isinstance(apdu, SimpleAckPDU):\n            atype = None\n\n        elif isinstance(apdu, ComplexAckPDU):\n            atype = complex_ack_types.get(apdu.apduService)\n            if not atype:\n                if _debug: decode_packet._debug(\"    - no complex ack decoder: %r\", apdu.apduService)\n                return apdu\n\n        elif isinstance(apdu, SegmentAckPDU):\n            atype = None\n\n        elif isinstance(apdu, ErrorPDU):\n            atype = error_types.get(apdu.apduService)\n            if not atype:\n                if _debug: decode_packet._debug(\"    - no error decoder: %r\", apdu.apduService)\n                return apdu\n\n        elif isinstance(apdu, RejectPDU):\n            atype = None\n\n        elif isinstance(apdu, AbortPDU):\n            atype = None\n        if _debug: decode_packet._debug(\"    - atype: %r\", atype)\n\n        # deeper decoding\n        try:\n            if atype:\n                xpdu = apdu\n                apdu = atype()\n                apdu.decode(xpdu)\n        except Exception as err:\n            if _debug: decode_packet._debug(\"    - decoding error: %r\", err)\n            return xpdu\n\n        # success\n        return apdu\n\n    else:\n        # make a more focused interpretation\n        ntype = npdu_types.get(npdu.npduNetMessage)\n        if not ntype:\n            if _debug: decode_packet._debug(\"    - no network layer decoder: %r\", npdu.npduNetMessage)\n            return npdu\n        if _debug: decode_packet._debug(\"    - ntype: %r\", ntype)\n\n        # deeper decoding\n        try:\n            xpdu = npdu\n            npdu = ntype()\n            npdu.decode(xpdu)\n        except Exception as err:\n            if _debug: decode_packet._debug(\"    - decoding error: %r\", err)\n            return xpdu\n\n        # success\n        return npdu"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive the name of a pcap file open it decode the contents and yield each packet.", "response": "def decode_file(fname):\n    \"\"\"Given the name of a pcap file, open it, decode the contents and yield each packet.\"\"\"\n    if _debug: decode_file._debug(\"decode_file %r\", fname)\n\n    if not pcap:\n        raise RuntimeError(\"failed to import pcap\")\n\n    # create a pcap object, reading from the file\n    p = pcap.pcap(fname)\n\n    # loop through the packets\n    for i, (timestamp, data) in enumerate(p):\n        try:\n            pkt = decode_packet(data)\n            if not pkt:\n                continue\n        except Exception as err:\n            if _debug: decode_file._debug(\"    - exception decoding packet %d: %r\", i+1, err)\n            continue\n\n        # save the packet number (as viewed in Wireshark) and timestamp\n        pkt._number = i + 1\n        pkt._timestamp = timestamp\n\n        yield pkt"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstop running the current node.", "response": "def stop(*args):\n    \"\"\"Call to stop running, may be called with a signum and frame\n    parameter if called as a signal handler.\"\"\"\n    if _debug: stop._debug(\"stop\")\n    global running, taskManager\n\n    if args:\n        sys.stderr.write(\"===== TERM Signal, %s\\n\" % time.strftime(\"%d-%b-%Y %H:%M:%S\"))\n        sys.stderr.flush()\n\n    running = False\n\n    # trigger the task manager event\n    if taskManager and taskManager.trigger:\n        if _debug: stop._debug(\"    - trigger\")\n        taskManager.trigger.set()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef print_stack(sig, frame):\n    if _debug: print_stack._debug(\"print_stack %r %r\", sig, frame)\n    global running, deferredFns, sleeptime\n\n    sys.stderr.write(\"==== USR1 Signal, %s\\n\" % time.strftime(\"%d-%b-%Y %H:%M:%S\"))\n\n    sys.stderr.write(\"---------- globals\\n\")\n    sys.stderr.write(\"    running: %r\\n\" % (running,))\n    sys.stderr.write(\"    deferredFns: %r\\n\" % (deferredFns,))\n    sys.stderr.write(\"    sleeptime: %r\\n\" % (sleeptime,))\n\n    sys.stderr.write(\"---------- stack\\n\")\n    traceback.print_stack(frame)\n\n    # make a list of interesting frames\n    flist = []\n    f = frame\n    while f.f_back:\n        flist.append(f)\n        f = f.f_back\n\n    # reverse the list so it is in the same order as print_stack\n    flist.reverse()\n    for f in flist:\n        sys.stderr.write(\"---------- frame: %s\\n\" % (f,))\n        for k, v in f.f_locals.items():\n            sys.stderr.write(\"    %s: %r\\n\" % (k, v))\n\n    sys.stderr.flush()", "response": "Signal handler to print a stack trace and some interesting values."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new class starting with the base and adding capabilities.", "response": "def compose_capability(base, *classes):\n    \"\"\"Create a new class starting with the base and adding capabilities.\"\"\"\n    if _debug: compose_capability._debug(\"compose_capability %r %r\", base, classes)\n\n    # make sure the base is a Collector\n    if not issubclass(base, Collector):\n        raise TypeError(\"base must be a subclass of Collector\")\n\n    # make sure you only add capabilities\n    for cls in classes:\n        if not issubclass(cls, Capability):\n            raise TypeError(\"%s is not a Capability subclass\" % (cls,))\n\n    # start with everything the base has and add the new ones\n    bases = (base,) + classes\n\n    # build a new name\n    name = base.__name__\n    for cls in classes:\n        name += '+' + cls.__name__\n\n    # return a new type\n    return type(name, bases, {})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd capabilites to an existing base.", "response": "def add_capability(base, *classes):\n    \"\"\"Add capabilites to an existing base, all objects get the additional\n    functionality, but don't get inited.  Use with great care!\"\"\"\n    if _debug: add_capability._debug(\"add_capability %r %r\", base, classes)\n\n    # start out with a collector\n    if not issubclass(base, Collector):\n        raise TypeError(\"base must be a subclass of Collector\")\n\n    # make sure you only add capabilities\n    for cls in classes:\n        if not issubclass(cls, Capability):\n            raise TypeError(\"%s is not a Capability subclass\" % (cls,))\n\n    base.__bases__ += classes\n    for cls in classes:\n        base.__name__ += '+' + cls.__name__"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _search_capability(self, base):\n        if _debug: Collector._debug(\"_search_capability %r\", base)\n\n        rslt = []\n        for cls in base.__bases__:\n            if issubclass(cls, Collector):\n                map( rslt.append, self._search_capability(cls))\n            elif issubclass(cls, Capability):\n                rslt.append(cls)\n        if _debug: Collector._debug(\"    - rslt: %r\", rslt)\n\n        return rslt", "response": "Given a class return a list of all of the derived classes that are themselves derived from Capability."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef capability_functions(self, fn):\n        if _debug: Collector._debug(\"capability_functions %r\", fn)\n\n        # build a list of functions to call\n        fns = []\n        for cls in self.capabilities:\n            xfn = getattr(cls, fn, None)\n            if _debug: Collector._debug(\"    - cls, xfn: %r, %r\", cls, xfn)\n            if xfn:\n                fns.append( (getattr(cls, '_zindex', None), xfn) )\n\n        # sort them by z-index\n        fns.sort(key=lambda v: v[0])\n        if _debug: Collector._debug(\"    - fns: %r\", fns)\n\n        # now yield them in order\n        for xindx, xfn in fns:\n            if _debug: Collector._debug(\"    - yield xfn: %r\", xfn)\n            yield xfn", "response": "This generator yields functions that match the requested capability sorted by z - index."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a capability to this object.", "response": "def add_capability(self, cls):\n        \"\"\"Add a capability to this object.\"\"\"\n        if _debug: Collector._debug(\"add_capability %r\", cls)\n\n        # the new type has everything the current one has plus this new one\n        bases = (self.__class__, cls)\n        if _debug: Collector._debug(\"    - bases: %r\", bases)\n\n        # save this additional class\n        self.capabilities.append(cls)\n\n        # morph into a new type\n        newtype = type(self.__class__.__name__ + '+' + cls.__name__, bases, {})\n        self.__class__ = newtype\n\n        # allow the new type to init\n        if hasattr(cls, '__init__'):\n            if _debug: Collector._debug(\"    - calling %r.__init__\", cls)\n            cls.__init__(self)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a composite pattern and compile it.", "response": "def _merge(*args):\n    \"\"\"Create a composite pattern and compile it.\"\"\"\n    return re.compile(r'^' + r'[/-]'.join(args) + r'(?:\\s+' + _dow + ')?$')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set(self, tclass, tnum, tlvt=0, tdata=b''):\n        if isinstance(tdata, bytearray):\n            tdata = bytes(tdata)\n        elif not isinstance(tdata, bytes):\n            raise TypeError(\"tag data must be bytes or bytearray\")\n\n        self.tagClass = tclass\n        self.tagNumber = tnum\n        self.tagLVT = tlvt\n        self.tagData = tdata", "response": "set the values of the tag."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nencode a tag on the end of the PDU.", "response": "def encode(self, pdu):\n        \"\"\"Encode a tag on the end of the PDU.\"\"\"\n        # check for special encoding\n        if (self.tagClass == Tag.contextTagClass):\n            data = 0x08\n        elif (self.tagClass == Tag.openingTagClass):\n            data = 0x0E\n        elif (self.tagClass == Tag.closingTagClass):\n            data = 0x0F\n        else:\n            data = 0x00\n\n        # encode the tag number part\n        if (self.tagNumber < 15):\n            data += (self.tagNumber << 4)\n        else:\n            data += 0xF0\n\n        # encode the length/value/type part\n        if (self.tagLVT < 5):\n            data += self.tagLVT\n        else:\n            data += 0x05\n\n        # save this and the extended tag value\n        pdu.put( data )\n        if (self.tagNumber >= 15):\n            pdu.put(self.tagNumber)\n\n        # really short lengths are already done\n        if (self.tagLVT >= 5):\n            if (self.tagLVT <= 253):\n                pdu.put( self.tagLVT )\n            elif (self.tagLVT <= 65535):\n                pdu.put( 254 )\n                pdu.put_short( self.tagLVT )\n            else:\n                pdu.put( 255 )\n                pdu.put_long( self.tagLVT )\n\n        # now put the data\n        pdu.put_data(self.tagData)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the application object encoded by the tag.", "response": "def app_to_object(self):\n        \"\"\"Return the application object encoded by the tag.\"\"\"\n        if self.tagClass != Tag.applicationTagClass:\n            raise ValueError(\"application tag required\")\n\n        # get the class to build\n        klass = self._app_tag_class[self.tagNumber]\n        if not klass:\n            return None\n\n        # build an object, tell it to decode this tag, and return it\n        return klass(self)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove the first tag from the front of the list and return it.", "response": "def Pop(self):\n        \"\"\"Remove the tag from the front of the list and return it.\"\"\"\n        if self.tagList:\n            tag = self.tagList[0]\n            del self.tagList[0]\n        else:\n            tag = None\n\n        return tag"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_context(self, context):\n        # forward pass\n        i = 0\n        while i < len(self.tagList):\n            tag = self.tagList[i]\n\n            # skip application stuff\n            if tag.tagClass == Tag.applicationTagClass:\n                pass\n\n            # check for context encoded atomic value\n            elif tag.tagClass == Tag.contextTagClass:\n                if tag.tagNumber == context:\n                    return tag\n\n            # check for context encoded group\n            elif tag.tagClass == Tag.openingTagClass:\n                keeper = tag.tagNumber == context\n                rslt = []\n                i += 1\n                lvl = 0\n                while i < len(self.tagList):\n                    tag = self.tagList[i]\n                    if tag.tagClass == Tag.openingTagClass:\n                        lvl += 1\n                    elif tag.tagClass == Tag.closingTagClass:\n                        lvl -= 1\n                        if lvl < 0: break\n\n                    rslt.append(tag)\n                    i += 1\n\n                # make sure everything balances\n                if lvl >= 0:\n                    raise InvalidTag(\"mismatched open/close tags\")\n\n                # get everything we need?\n                if keeper:\n                    return TagList(rslt)\n            else:\n                raise InvalidTag(\"unexpected tag\")\n\n            # try the next tag\n            i += 1\n\n        # nothing found\n        return None", "response": "Return a tag or a list of tags context encoded."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decode(self, pdu):\n        while pdu.pduData:\n            self.tagList.append( Tag(pdu) )", "response": "decode the tags from a PDU."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving an arg return the appropriate value given the class.", "response": "def coerce(cls, arg):\n        \"\"\"Given an arg, return the appropriate value given the class.\"\"\"\n        try:\n            return cls(arg).value\n        except (ValueError, TypeError):\n            raise InvalidParameterDatatype(\"%s coerce error\" % (cls.__name__,))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if arg is valid value for the class.", "response": "def is_valid(cls, arg):\n        \"\"\"Return True if arg is valid value for the class.\"\"\"\n        if isinstance(arg, list):\n            allInts = allStrings = True\n            for elem in arg:\n                allInts = allInts and ((elem == 0) or (elem == 1))\n                allStrings = allStrings and elem in cls.bitNames\n\n            if allInts or allStrings:\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of names in order by value.", "response": "def keylist(self):\n        \"\"\"Return a list of names in order by value.\"\"\"\n        items = self.enumerations.items()\n        items.sort(lambda a, b: self.cmp(a[1], b[1]))\n\n        # last item has highest value\n        rslt = [None] * (items[-1][1] + 1)\n\n        # map the values\n        for key, value in items:\n            rslt[value] = key\n\n        # return the result\n        return rslt"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the correct day of the week.", "response": "def CalcDayOfWeek(self):\n        \"\"\"Calculate the correct day of the week.\"\"\"\n        # rip apart the value\n        year, month, day, day_of_week = self.value\n\n        # assume the worst\n        day_of_week = 255\n\n        # check for special values\n        if year == 255:\n            pass\n        elif month in _special_mon_inv:\n            pass\n        elif day in _special_day_inv:\n            pass\n        else:\n            try:\n                today = time.mktime( (year + 1900, month, day, 0, 0, 0, 0, 0, -1) )\n                day_of_week = time.gmtime(today)[6] + 1\n            except OverflowError:\n                pass\n\n        # put it back together\n        self.value = (year, month, day, day_of_week)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the current value based on the seconds since the epoch.", "response": "def now(self, when=None):\n        \"\"\"Set the current value to the correct tuple based on the seconds\n        since the epoch.  If 'when' is not provided, get the current time\n        from the task manager.\n        \"\"\"\n        if when is None:\n            when = _TaskManager().get_time()\n        tup = time.localtime(when)\n\n        self.value = (tup[0]-1900, tup[1], tup[2], tup[6] + 1)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the unsigned integer tuple of the identifier.", "response": "def get_tuple(self):\n        \"\"\"Return the unsigned integer tuple of the identifier.\"\"\"\n        objType, objInstance = self.value\n\n        if isinstance(objType, int):\n            pass\n        elif isinstance(objType, str):\n            # turn it back into an integer\n            objType = self.objectTypeClass()[objType]\n        else:\n            raise TypeError(\"invalid datatype for objType\")\n\n        # pack the components together\n        return (objType, objInstance)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef has_device_info(self, key):\n        if _debug: DeviceInfoCache._debug(\"has_device_info %r\", key)\n\n        return key in self.cache", "response": "Return true iff the cache has information about the device."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef iam_device_info(self, apdu):\n        if _debug: DeviceInfoCache._debug(\"iam_device_info %r\", apdu)\n\n        # make sure the apdu is an I-Am\n        if not isinstance(apdu, IAmRequest):\n            raise ValueError(\"not an IAmRequest: %r\" % (apdu,))\n\n        # get the device instance\n        device_instance = apdu.iAmDeviceIdentifier[1]\n\n        # get the existing cache record if it exists\n        device_info = self.cache.get(device_instance, None)\n\n        # maybe there is a record for this address\n        if not device_info:\n            device_info = self.cache.get(apdu.pduSource, None)\n\n        # make a new one using the class provided\n        if not device_info:\n            device_info = self.device_info_class(device_instance, apdu.pduSource)\n\n        # jam in the correct values\n        device_info.deviceIdentifier = device_instance\n        device_info.address = apdu.pduSource\n        device_info.maxApduLengthAccepted = apdu.maxAPDULengthAccepted\n        device_info.segmentationSupported = apdu.segmentationSupported\n        device_info.vendorID = apdu.vendorID\n\n        # tell the cache this is an updated record\n        self.update_device_info(device_info)", "response": "Create a new record based on the contents of an I - AmRequest and put it in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_device_info(self, device_info):\n        if _debug: DeviceInfoCache._debug(\"update_device_info %r\", device_info)\n\n        # give this a reference count if it doesn't have one\n        if not hasattr(device_info, '_ref_count'):\n            device_info._ref_count = 0\n\n        # get the current keys\n        cache_id, cache_address = getattr(device_info, '_cache_keys', (None, None))\n\n        if (cache_id is not None) and (device_info.deviceIdentifier != cache_id):\n            if _debug: DeviceInfoCache._debug(\"    - device identifier updated\")\n\n            # remove the old reference, add the new one\n            del self.cache[cache_id]\n            self.cache[device_info.deviceIdentifier] = device_info\n\n        if (cache_address is not None) and (device_info.address != cache_address):\n            if _debug: DeviceInfoCache._debug(\"    - device address updated\")\n\n            # remove the old reference, add the new one\n            del self.cache[cache_address]\n            self.cache[device_info.address] = device_info\n\n        # update the keys\n        device_info._cache_keys = (device_info.deviceIdentifier, device_info.address)", "response": "Update the device info record with the new information."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the known information about the device and mark the record as being used by a segmenation state machine.", "response": "def acquire(self, key):\n        \"\"\"Return the known information about the device and mark the record\n        as being used by a segmenation state machine.\"\"\"\n        if _debug: DeviceInfoCache._debug(\"acquire %r\", key)\n\n        if isinstance(key, int):\n            device_info = self.cache.get(key, None)\n\n        elif not isinstance(key, Address):\n            raise TypeError(\"key must be integer or an address\")\n\n        elif key.addrType not in (Address.localStationAddr, Address.remoteStationAddr):\n            raise TypeError(\"address must be a local or remote station\")\n\n        else:\n            device_info = self.cache.get(key, None)\n\n        if device_info:\n            if _debug: DeviceInfoCache._debug(\"    - reference bump\")\n            device_info._ref_count += 1\n\n        if _debug: DeviceInfoCache._debug(\"    - device_info: %r\", device_info)\n\n        return device_info"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_object(self, obj):\n        if _debug: Application._debug(\"add_object %r\", obj)\n\n        # extract the object name and identifier\n        object_name = obj.objectName\n        if not object_name:\n            raise RuntimeError(\"object name required\")\n        object_identifier = obj.objectIdentifier\n        if not object_identifier:\n            raise RuntimeError(\"object identifier required\")\n\n        # assuming the object identifier is well formed, check the instance number\n        if (object_identifier[1] >= ObjectIdentifier.maximum_instance_number):\n            raise RuntimeError(\"invalid object identifier\")\n\n        # make sure it hasn't already been defined\n        if object_name in self.objectName:\n            raise RuntimeError(\"already an object with name %r\" % (object_name,))\n        if object_identifier in self.objectIdentifier:\n            raise RuntimeError(\"already an object with identifier %r\" % (object_identifier,))\n\n        # now put it in local dictionaries\n        self.objectName[object_name] = obj\n        self.objectIdentifier[object_identifier] = obj\n\n        # append the new object's identifier to the local device's object list\n        # if there is one and it has an object list property\n        if self.localDevice and self.localDevice.objectList:\n            self.localDevice.objectList.append(object_identifier)\n\n        # let the object know which application stack it belongs to\n        obj._app = self", "response": "Add an object to the local collection."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd an object to the local collection.", "response": "def delete_object(self, obj):\n        \"\"\"Add an object to the local collection.\"\"\"\n        if _debug: Application._debug(\"delete_object %r\", obj)\n\n        # extract the object name and identifier\n        object_name = obj.objectName\n        object_identifier = obj.objectIdentifier\n\n        # delete it from the application\n        del self.objectName[object_name]\n        del self.objectIdentifier[object_identifier]\n\n        # remove the object's identifier from the device's object list\n        # if there is one and it has an object list property\n        if self.localDevice and self.localDevice.objectList:\n            indx = self.localDevice.objectList.index(object_identifier)\n            del self.localDevice.objectList[indx]\n\n        # make sure the object knows it's detached from an application\n        obj._app = None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_services_supported(self):\n        if _debug: Application._debug(\"get_services_supported\")\n\n        services_supported = ServicesSupported()\n\n        # look through the confirmed services\n        for service_choice, service_request_class in confirmed_request_types.items():\n            service_helper = \"do_\" + service_request_class.__name__\n            if hasattr(self, service_helper):\n                service_supported = ConfirmedServiceChoice._xlate_table[service_choice]\n                services_supported[service_supported] = 1\n\n        # look through the unconfirmed services\n        for service_choice, service_request_class in unconfirmed_request_types.items():\n            service_helper = \"do_\" + service_request_class.__name__\n            if hasattr(self, service_helper):\n                service_supported = UnconfirmedServiceChoice._xlate_table[service_choice]\n                services_supported[service_supported] = 1\n\n        # return the bit list\n        return services_supported", "response": "Return a ServicesSupported bit string based in introspection look\n        for helper methods that match confirmed and unconfirmed services."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the contents of an object as a dict.", "response": "def key_value_contents(use_dict=None, as_class=dict, key_values=()):\n    \"\"\"Return the contents of an object as a dict.\"\"\"\n    if _debug: key_value_contents._debug(\"key_value_contents use_dict=%r as_class=%r key_values=%r\", use_dict, as_class, key_values)\n\n    # make/extend the dictionary of content\n    if use_dict is None:\n        use_dict = as_class()\n\n    # loop through the values and save them\n    for k, v in key_values:\n        if v is not None:\n            if hasattr(v, 'dict_contents'):\n                v = v.dict_contents(as_class=as_class)\n            use_dict.__setitem__(k, v)\n\n    # return what we built/updated\n    return use_dict"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bvlpdu_contents(self, use_dict=None, as_class=dict):\n        broadcast_distribution_table = []\n        for bdte in self.bvlciBDT:\n            broadcast_distribution_table.append(str(bdte))\n\n        return key_value_contents(use_dict=use_dict, as_class=as_class,\n            key_values=(\n                ('function', 'ReadBroadcastDistributionTableAck'),\n                ('bdt', broadcast_distribution_table),\n            ))", "response": "Return the contents of an object as a dict."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bvlpdu_contents(self, use_dict=None, as_class=dict):\n        # make/extend the dictionary of content\n        if use_dict is None:\n            use_dict = as_class()\n\n        # save the content\n        use_dict.__setitem__('address', str(self.fdAddress))\n        use_dict.__setitem__('ttl', self.fdTTL)\n        use_dict.__setitem__('remaining', self.fdRemain)\n\n        # return what we built/updated\n        return use_dict", "response": "Return the contents of an object as a dict."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bvlpdu_contents(self, use_dict=None, as_class=dict):\n        return key_value_contents(use_dict=use_dict, as_class=as_class,\n            key_values=(\n                ('function', 'RegisterForeignDevice'),\n                ('ttl', self.bvlciTimeToLive),\n            ))", "response": "Return the contents of an object as a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bvlpdu_contents(self, use_dict=None, as_class=dict):\n        foreign_device_table = []\n        for fdte in self.bvlciFDT:\n            foreign_device_table.append(fdte.dict_contents(as_class=as_class))\n\n        return key_value_contents(use_dict=use_dict, as_class=as_class,\n            key_values=(\n                ('function', 'ReadForeignDeviceTableAck'),\n                ('foreign_device_table', foreign_device_table),\n            ))", "response": "Return the contents of an object as a dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the contents of an object as a dict.", "response": "def bvlpdu_contents(self, use_dict=None, as_class=dict):\n        \"\"\"Return the contents of an object as a dict.\"\"\"\n        return key_value_contents(use_dict=use_dict, as_class=as_class,\n            key_values=(\n                ('function', 'DeleteForeignDeviceTableEntry'),\n                ('address', str(self.bvlciAddress)),\n            ))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the contents of an object as a dict.", "response": "def bvlpdu_contents(self, use_dict=None, as_class=dict):\n        \"\"\"Return the contents of an object as a dict.\"\"\"\n\n        # make/extend the dictionary of content\n        if use_dict is None:\n            use_dict = as_class()\n\n        # call the normal procedure\n        key_value_contents(use_dict=use_dict, as_class=as_class,\n            key_values=(\n                ('function', 'DistributeBroadcastToNetwork'),\n            ))\n\n        # this message has data\n        PDUData.dict_contents(self, use_dict=use_dict, as_class=as_class)\n\n        # return what we built/updated\n        return use_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main():\n    global server_address\n\n    # check the version\n    if (sys.version_info[:2] != (2, 5)):\n        sys.stderr.write(\"Python 2.5 only\\n\")\n        sys.exit(1)\n\n    # parse the command line arguments\n    parser = ArgumentParser(description=__doc__)\n    parser.add_argument(\n        \"host\", nargs='?',\n        help=\"address of host (default %r)\" % (SERVER_HOST,),\n        default=SERVER_HOST,\n        )\n    parser.add_argument(\n        \"port\", nargs='?', type=int,\n        help=\"server port (default %r)\" % (SERVER_PORT,),\n        default=SERVER_PORT,\n        )\n    args = parser.parse_args()\n\n    if _debug: _log.debug(\"initialization\")\n    if _debug: _log.debug(\"    - args: %r\", args)\n\n    # extract the server address and port\n    host = args.host\n    port = args.port\n    server_address = (host, port)\n    if _debug: _log.debug(\"    - server_address: %r\", server_address)\n\n    # build the stack\n    this_console = ConsoleClient()\n    if _debug: _log.debug(\"    - this_console: %r\", this_console)\n\n    this_middle_man = MiddleMan()\n    if _debug: _log.debug(\"    - this_middle_man: %r\", this_middle_man)\n\n    this_director = TCPClientDirector()\n    if _debug: _log.debug(\"    - this_director: %r\", this_director)\n\n    bind(this_console, this_middle_man, this_director)\n    bind(MiddleManASE(), this_director)\n\n    # create a task manager for scheduled functions\n    task_manager = TaskManager()\n    if _debug: _log.debug(\"    - task_manager: %r\", task_manager)\n\n    # don't wait to connect\n    this_director.connect(server_address)\n\n    if _debug: _log.debug(\"running\")\n\n    run()", "response": "This function is called by the application s main function. It is called by the application s main function."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_next_task(self):\n        if _debug: TaskManager._debug(\"get_next_task\")\n\n        # get the time\n        now = _time()\n\n        task = None\n        delta = None\n\n        if self.tasks:\n            # look at the first task\n            when, n, nxttask = self.tasks[0]\n            if when <= now:\n                # pull it off the list and mark that it's no longer scheduled\n                heappop(self.tasks)\n                task = nxttask\n                task.isScheduled = False\n\n                if self.tasks:\n                    when, n, nxttask = self.tasks[0]\n                    # peek at the next task, return how long to wait\n                    delta = max(when - now, 0.0)\n            else:\n                delta = when - now\n\n        # return the task to run and how long to wait for the next one\n        return (task, delta)", "response": "get the next task if there s one that should be processed and return how long it will be until the next one should be processed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef match_date(date, date_pattern):\n    # unpack the date and pattern\n    year, month, day, day_of_week = date\n    year_p, month_p, day_p, day_of_week_p = date_pattern\n\n    # check the year\n    if year_p == 255:\n        # any year\n        pass\n    elif year != year_p:\n        # specific year\n        return False\n\n    # check the month\n    if month_p == 255:\n        # any month\n        pass\n    elif month_p == 13:\n        # odd months\n        if (month % 2) == 0:\n            return False\n    elif month_p == 14:\n        # even months\n        if (month % 2) == 1:\n            return False\n    elif month != month_p:\n        # specific month\n        return False\n\n    # check the day\n    if day_p == 255:\n        # any day\n        pass\n    elif day_p == 32:\n        # last day of the month\n        last_day = calendar.monthrange(year + 1900, month)[1]\n        if day != last_day:\n            return False\n    elif day_p == 33:\n        # odd days of the month\n        if (day % 2) == 0:\n            return False\n    elif day_p == 34:\n        # even days of the month\n        if (day % 2) == 1:\n            return False\n    elif day != day_p:\n        # specific day\n        return False\n\n    # check the day of week\n    if day_of_week_p == 255:\n        # any day of the week\n        pass\n    elif day_of_week != day_of_week_p:\n        # specific day of the week\n        return False\n\n    # all tests pass\n    return True", "response": "Match a specific date a four - tuple with no special values with a date_pattern four - tuple possibly having special values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef datetime_to_time(date, time):\n    if (255 in date) or (255 in time):\n        raise RuntimeError(\"specific date and time required\")\n\n    time_tuple = (\n        date[0]+1900, date[1], date[2],\n        time[0], time[1], time[2],\n        0, 0, -1,\n        )\n    return _mktime(time_tuple)", "response": "Take the date and time 4 - tuples and return the time in seconds since\n    the epoch as a floating point number."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_reliability(self, old_value=None, new_value=None):\n        if _debug: LocalScheduleObject._debug(\"_check_reliability %r %r\", old_value, new_value)\n\n        try:\n            schedule_default = self.scheduleDefault\n\n            if schedule_default is None:\n                raise ValueError(\"scheduleDefault expected\")\n            if not isinstance(schedule_default, Atomic):\n                raise TypeError(\"scheduleDefault must be an instance of an atomic type\")\n\n            schedule_datatype = schedule_default.__class__\n            if _debug: LocalScheduleObject._debug(\"    - schedule_datatype: %r\", schedule_datatype)\n\n            if (self.weeklySchedule is None) and (self.exceptionSchedule is None):\n                raise ValueError(\"schedule required\")\n\n            # check the weekly schedule values\n            if self.weeklySchedule:\n                for daily_schedule in self.weeklySchedule:\n                    for time_value in daily_schedule.daySchedule:\n                        if _debug: LocalScheduleObject._debug(\"    - daily time_value: %r\", time_value)\n                        if time_value is None:\n                            pass\n                        elif not isinstance(time_value.value, (Null, schedule_datatype)):\n                            if _debug: LocalScheduleObject._debug(\"    - wrong type: expected %r, got %r\",\n                                schedule_datatype,\n                                time_value.__class__,\n                                )\n                            raise TypeError(\"wrong type\")\n                        elif 255 in time_value.time:\n                            if _debug: LocalScheduleObject._debug(\"    - wildcard in time\")\n                            raise ValueError(\"must be a specific time\")\n\n            # check the exception schedule values\n            if self.exceptionSchedule:\n                for special_event in self.exceptionSchedule:\n                    for time_value in special_event.listOfTimeValues:\n                        if _debug: LocalScheduleObject._debug(\"    - special event time_value: %r\", time_value)\n                        if time_value is None:\n                            pass\n                        elif not isinstance(time_value.value, (Null, schedule_datatype)):\n                            if _debug: LocalScheduleObject._debug(\"    - wrong type: expected %r, got %r\",\n                                schedule_datatype,\n                                time_value.__class__,\n                                )\n                            raise TypeError(\"wrong type\")\n\n            # check list of object property references\n            obj_prop_refs = self.listOfObjectPropertyReferences\n            if obj_prop_refs:\n                for obj_prop_ref in obj_prop_refs:\n                    if obj_prop_ref.deviceIdentifier:\n                        raise RuntimeError(\"no external references\")\n\n                    # get the datatype of the property to be written\n                    obj_type = obj_prop_ref.objectIdentifier[0]\n                    datatype = get_datatype(obj_type, obj_prop_ref.propertyIdentifier)\n                    if _debug: LocalScheduleObject._debug(\"    - datatype: %r\", datatype)\n\n                    if issubclass(datatype, Array) and (obj_prop_ref.propertyArrayIndex is not None):\n                        if obj_prop_ref.propertyArrayIndex == 0:\n                            datatype = Unsigned\n                        else:\n                            datatype = datatype.subtype\n                        if _debug: LocalScheduleObject._debug(\"    - datatype: %r\", datatype)\n\n                    if datatype is not schedule_datatype:\n                        if _debug: LocalScheduleObject._debug(\"    - wrong type: expected %r, got %r\",\n                            datatype,\n                            schedule_datatype,\n                            )\n                        raise TypeError(\"wrong type\")\n\n            # all good\n            self.reliability = 'noFaultDetected'\n            if _debug: LocalScheduleObject._debug(\"    - no fault detected\")\n\n        except Exception as err:\n            if _debug: LocalScheduleObject._debug(\"    - exception: %r\", err)\n            self.reliability = 'configurationError'", "response": "This function is called when the object is created and after the property has been changed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef present_value_changed(self, old_value, new_value):\n        if _debug: LocalScheduleInterpreter._debug(\"present_value_changed %s %s\", old_value, new_value)\n\n        # if this hasn't been added to an application, there's nothing to do\n        if not self.sched_obj._app:\n            if _debug: LocalScheduleInterpreter._debug(\"    - no application\")\n            return\n\n        # process the list of [device] object property [array index] references\n        obj_prop_refs = self.sched_obj.listOfObjectPropertyReferences\n        if not obj_prop_refs:\n            if _debug: LocalScheduleInterpreter._debug(\"    - no writes defined\")\n            return\n\n        # primitive values just set the value part\n        new_value = new_value.value\n\n        # loop through the writes\n        for obj_prop_ref in obj_prop_refs:\n            if obj_prop_ref.deviceIdentifier:\n                if _debug: LocalScheduleInterpreter._debug(\"    - no externals\")\n                continue\n\n            # get the object from the application\n            obj = self.sched_obj._app.get_object_id(obj_prop_ref.objectIdentifier)\n            if not obj:\n                if _debug: LocalScheduleInterpreter._debug(\"    - no object\")\n                continue\n\n            # try to change the value\n            try:\n                obj.WriteProperty(\n                    obj_prop_ref.propertyIdentifier,\n                    new_value,\n                    arrayIndex=obj_prop_ref.propertyArrayIndex,\n                    priority=self.sched_obj.priorityForWriting,\n                    )\n                if _debug: LocalScheduleInterpreter._debug(\"    - success\")\n            except Exception as err:\n                if _debug: LocalScheduleInterpreter._debug(\"    - error: %r\", err)", "response": "This function is called when the presentValue of the local schedule\n            object has changed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef eval(self, edate, etime):\n        if _debug: LocalScheduleInterpreter._debug(\"eval %r %r\", edate, etime)\n\n        # reference the schedule object\n        sched_obj = self.sched_obj\n        if _debug: LocalScheduleInterpreter._debug(\"    sched_obj: %r\", sched_obj)\n\n        # verify the date falls in the effective period\n        if not match_date_range(edate, sched_obj.effectivePeriod):\n            return None\n\n        # the event priority is a list of values that are in effect for\n        # exception schedules with the special event priority, see 135.1-2013\n        # clause 7.3.2.23.10.3.8, Revision 4 Event Priority Test\n        event_priority = [None] * 16\n\n        next_day = (24, 0, 0, 0)\n        next_transition_time = [None] * 16\n\n        # check the exception schedule values\n        if sched_obj.exceptionSchedule:\n            for special_event in sched_obj.exceptionSchedule:\n                if _debug: LocalScheduleInterpreter._debug(\"    - special_event: %r\", special_event)\n\n                # check the special event period\n                special_event_period = special_event.period\n                if special_event_period is None:\n                    raise RuntimeError(\"special event period required\")\n\n                match = False\n                calendar_entry = special_event_period.calendarEntry\n                if calendar_entry:\n                    if _debug: LocalScheduleInterpreter._debug(\"    - calendar_entry: %r\", calendar_entry)\n                    match = date_in_calendar_entry(edate, calendar_entry)\n                else:\n                    # get the calendar object from the application\n                    calendar_object = sched_obj._app.get_object_id(special_event_period.calendarReference)\n                    if not calendar_object:\n                        raise RuntimeError(\"invalid calendar object reference\")\n                    if _debug: LocalScheduleInterpreter._debug(\"    - calendar_object: %r\", calendar_object)\n\n                    for calendar_entry in calendar_object.dateList:\n                        if _debug: LocalScheduleInterpreter._debug(\"    - calendar_entry: %r\", calendar_entry)\n                        match = date_in_calendar_entry(edate, calendar_entry)\n                        if match:\n                            break\n\n                # didn't match the period, try the next special event\n                if not match:\n                    if _debug: LocalScheduleInterpreter._debug(\"    - no matching calendar entry\")\n                    continue\n\n                # event priority array index\n                priority = special_event.eventPriority - 1\n                if _debug: LocalScheduleInterpreter._debug(\"    - priority: %r\", priority)\n\n                # look for all of the possible times\n                for time_value in special_event.listOfTimeValues:\n                    tval = time_value.time\n                    if tval <= etime:\n                        if isinstance(time_value.value, Null):\n                            if _debug: LocalScheduleInterpreter._debug(\"    - relinquish exception @ %r\", tval)\n                            event_priority[priority] = None\n                            next_transition_time[priority] = None\n                        else:\n                            if _debug: LocalScheduleInterpreter._debug(\"    - consider exception @ %r\", tval)\n                            event_priority[priority] = time_value.value\n                            next_transition_time[priority] = next_day\n                    else:\n                        next_transition_time[priority] = tval\n                        break\n\n        # assume the next transition will be at the start of the next day\n        earliest_transition = next_day\n\n        # check if any of the special events came up with something\n        for priority_value, next_transition in zip(event_priority, next_transition_time):\n            if next_transition is not None:\n                earliest_transition = min(earliest_transition, next_transition)\n            if priority_value is not None:\n                if _debug: LocalScheduleInterpreter._debug(\"    - priority_value: %r\", priority_value)\n                return priority_value, earliest_transition\n\n        # start out with the default\n        daily_value = sched_obj.scheduleDefault\n\n        # check the daily schedule\n        if sched_obj.weeklySchedule:\n            daily_schedule = sched_obj.weeklySchedule[edate[3]]\n            if _debug: LocalScheduleInterpreter._debug(\"    - daily_schedule: %r\", daily_schedule)\n\n            # look for all of the possible times\n            for time_value in daily_schedule.daySchedule:\n                if _debug: LocalScheduleInterpreter._debug(\"    - time_value: %r\", time_value)\n\n                tval = time_value.time\n                if tval <= etime:\n                    if isinstance(time_value.value, Null):\n                        if _debug: LocalScheduleInterpreter._debug(\"    - back to normal @ %r\", tval)\n                        daily_value = sched_obj.scheduleDefault\n                    else:\n                        if _debug: LocalScheduleInterpreter._debug(\"    - new value @ %r\", tval)\n                        daily_value = time_value.value\n                else:\n                    earliest_transition = min(earliest_transition, tval)\n                    break\n\n        # return what was matched, if anything\n        return daily_value, earliest_transition", "response": "Evaluate the schedule according to the provided date and time and return the appropriate present value or None if not in the effective period and the date is not in the effective period."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef do_gc(self, args):\n\n        ### humm...\n        instance_type = getattr(types, 'InstanceType', object)\n\n        # snapshot of counts\n        type2count = {}\n        type2all = {}\n        for o in gc.get_objects():\n            if type(o) == instance_type:\n                type2count[o.__class__] = type2count.get(o.__class__,0) + 1\n                type2all[o.__class__] = type2all.get(o.__class__,0) + sys.getrefcount(o)\n\n        # count the things that have changed\n        ct = [ ( t.__module__\n            , t.__name__\n            , type2count[t]\n            , type2count[t] - self.type2count.get(t,0)\n            , type2all[t] - self.type2all.get(t,0)\n            ) for t in type2count.iterkeys()\n            ]\n\n        # ready for the next time\n        self.type2count = type2count\n        self.type2all = type2all\n\n        fmt = \"%-30s %-30s %6s %6s %6s\\n\"\n        self.stdout.write(fmt % (\"Module\", \"Type\", \"Count\", \"dCount\", \"dRef\"))\n\n        # sorted by count\n        ct.sort(lambda x, y: cmp(y[2], x[2]))\n        for i in range(min(10,len(ct))):\n            m, n, c, delta1, delta2 = ct[i]\n            self.stdout.write(fmt % (m, n, c, delta1, delta2))\n        self.stdout.write(\"\\n\")\n\n        self.stdout.write(fmt % (\"Module\", \"Type\", \"Count\", \"dCount\", \"dRef\"))\n\n        # sorted by module and class\n        ct.sort()\n        for m, n, c, delta1, delta2 in ct:\n            if delta1 or delta2:\n                self.stdout.write(fmt % (m, n, c, delta1, delta2))\n        self.stdout.write(\"\\n\")", "response": "print out the garbage collection information for the current object"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef do_bugin(self, args):\n        args = args.split()\n        if _debug: ConsoleCmd._debug(\"do_bugin %r\", args)\n\n        # get the logger name and logger\n        if args:\n            loggerName = args[0]\n            if loggerName in logging.Logger.manager.loggerDict:\n                logger = logging.getLogger(loggerName)\n            else:\n                logger = None\n        else:\n            loggerName = '__root__'\n            logger = logging.getLogger()\n\n        # add a logging handler\n        if not logger:\n            self.stdout.write(\"not a valid logger name\\n\")\n        elif loggerName in self.handlers:\n            self.stdout.write(\"%s already has a handler\\n\" % loggerName)\n        else:\n            handler = ConsoleLogHandler(logger)\n            self.handlers[loggerName] = handler\n            self.stdout.write(\"handler to %s added\\n\" % loggerName)\n        self.stdout.write(\"\\n\")", "response": "bugin [ logger name ] - add a console logging handler to a logger"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef do_bugout(self, args):\n        args = args.split()\n        if _debug: ConsoleCmd._debug(\"do_bugout %r\", args)\n\n        # get the logger name and logger\n        if args:\n            loggerName = args[0]\n            if loggerName in logging.Logger.manager.loggerDict:\n                logger = logging.getLogger(loggerName)\n            else:\n                logger = None\n        else:\n            loggerName = '__root__'\n            logger = logging.getLogger()\n\n        # remove the logging handler\n        if not logger:\n            self.stdout.write(\"not a valid logger name\\n\")\n        elif not loggerName in self.handlers:\n            self.stdout.write(\"no handler for %s\\n\" % loggerName)\n        else:\n            handler = self.handlers[loggerName]\n            del self.handlers[loggerName]\n\n            # see if this (or its parent) is a module level logger\n            if hasattr(logger, 'globs'):\n                logger.globs['_debug'] -= 1\n            elif hasattr(logger.parent, 'globs'):\n                logger.parent.globs['_debug'] -= 1\n\n            # remove it from the logger\n            logger.removeHandler(handler)\n            self.stdout.write(\"handler to %s removed\\n\" % loggerName)\n        self.stdout.write(\"\\n\")", "response": "bugout - remove a console logging handler from a logger"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuggers - list the console logging handlers", "response": "def do_buggers(self, args):\n        \"\"\"buggers  - list the console logging handlers\"\"\"\n        args = args.split()\n        if _debug: ConsoleCmd._debug(\"do_buggers %r\", args)\n\n        if not self.handlers:\n            self.stdout.write(\"no handlers\\n\")\n        else:\n            self.stdout.write(\"handlers: \")\n            self.stdout.write(', '.join(loggerName or '__root__' for loggerName in self.handlers))\n            self.stdout.write(\"\\n\")\n\n        loggers = logging.Logger.manager.loggerDict.keys()\n        for loggerName in sorted(loggers):\n            if args and (not args[0] in loggerName):\n                continue\n\n            if loggerName in self.handlers:\n                self.stdout.write(\"* %s\\n\" % loggerName)\n            else:\n                self.stdout.write(\"  %s\\n\" % loggerName)\n        self.stdout.write(\"\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexits on system end of file character", "response": "def do_EOF(self, args):\n        \"\"\"Exit on system end of file character\"\"\"\n        if _debug: ConsoleCmd._debug(\"do_EOF %r\", args)\n        return self.do_exit(args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef do_shell(self, args):\n        if _debug: ConsoleCmd._debug(\"do_shell %r\", args)\n\n        os.system(args)", "response": "Pass command to a system shell when line begins with!"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget help on commands", "response": "def do_help(self, args):\n        \"\"\"Get help on commands\n        'help' or '?' with no arguments prints a list of commands for which help is available\n        'help <command>' or '? <command>' gives help on <command>\n        \"\"\"\n        if _debug: ConsoleCmd._debug(\"do_help %r\", args)\n\n        # the only reason to define this method is for the help text in the doc string\n        cmd.Cmd.do_help(self, args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef postloop(self):\n        try:\n            if readline:\n                readline.write_history_file(sys.argv[0] + \".history\")\n        except Exception, err:\n            if not isinstance(err, IOError):\n                self.stdout.write(\"history error: %s\\n\" % err)\n\n        # clean up command completion\n        cmd.Cmd.postloop(self)\n\n        if self.interactive:\n            self.stdout.write(\"Exiting...\\n\")\n\n        # tell the core we have stopped\n        core.deferred(core.stop)", "response": "Take care of any unfinished business."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndecode upstream PDUs and pass them up to the service access point.", "response": "def confirmation(self, pdu):\n        \"\"\"Decode upstream PDUs and pass them up to the service access point.\"\"\"\n        if _debug: NetworkAdapter._debug(\"confirmation %r (net=%r)\", pdu, self.adapterNet)\n\n        npdu = NPDU(user_data=pdu.pduUserData)\n        npdu.decode(pdu)\n        self.adapterSAP.process_npdu(self, npdu)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nencode NPDUs from the service access point and send them downstream.", "response": "def process_npdu(self, npdu):\n        \"\"\"Encode NPDUs from the service access point and send them downstream.\"\"\"\n        if _debug: NetworkAdapter._debug(\"process_npdu %r (net=%r)\", npdu, self.adapterNet)\n\n        pdu = PDU(user_data=npdu.pduUserData)\n        npdu.encode(pdu)\n        self.request(pdu)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bind(self, server, net=None, address=None):\n        if _debug: NetworkServiceAccessPoint._debug(\"bind %r net=%r address=%r\", server, net, address)\n\n        # make sure this hasn't already been called with this network\n        if net in self.adapters:\n            raise RuntimeError(\"already bound\")\n\n        # create an adapter object, add it to our map\n        adapter = NetworkAdapter(self, net)\n        self.adapters[net] = adapter\n        if _debug: NetworkServiceAccessPoint._debug(\"    - adapters[%r]: %r\", net, adapter)\n\n        # if the address was given, make it the \"local\" one\n        if address and not self.local_address:\n            self.local_adapter = adapter\n            self.local_address = address\n\n        # bind to the server\n        bind(adapter, server)", "response": "Create a network adapter object and bind it to the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_router_references(self, snet, address, dnets):\n        if _debug: NetworkServiceAccessPoint._debug(\"add_router_references %r %r %r\", snet, address, dnets)\n\n        # see if we have an adapter for the snet\n        if snet not in self.adapters:\n            raise RuntimeError(\"no adapter for network: %d\" % (snet,))\n\n        # pass this along to the cache\n        self.router_info_cache.update_router_info(snet, address, dnets)", "response": "Add or update routing entries to routers."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes the routing table entries associated with a specific network.", "response": "def delete_router_references(self, snet, address=None, dnets=None):\n        \"\"\"Delete references to routers/networks.\"\"\"\n        if _debug: NetworkServiceAccessPoint._debug(\"delete_router_references %r %r %r\", snet, address, dnets)\n\n        # see if we have an adapter for the snet\n        if snet not in self.adapters:\n            raise RuntimeError(\"no adapter for network: %d\" % (snet,))\n\n        # pass this along to the cache\n        self.router_info_cache.delete_router_info(snet, address, dnets)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pack_ip_addr(addr):\n    addr, port = addr\n    return socket.inet_aton(addr) + struct.pack('!H', port & _short_mask)", "response": "Given an IP address tuple like 1. 2. 3. 4 47808 return the six - octet string\n    useful for a BACnet address."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a six - octet BACnet address return an IP address tuple.", "response": "def unpack_ip_addr(addr):\n    \"\"\"Given a six-octet BACnet address, return an IP address tuple.\"\"\"\n    if isinstance(addr, bytearray):\n        addr = bytes(addr)\n    return (socket.inet_ntoa(addr[0:4]), struct.unpack('!H', addr[4:6])[0])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef btox(data, sep=''):\n    # translate the blob into hex\n    hex_str = binascii.hexlify(data)\n\n    # inject the separator if it was given\n    if sep:\n        hex_str = sep.join(hex_str[i:i+2] for i in range(0, len(hex_str), 2))\n\n    # return the result\n    return hex_str", "response": "Return the hex encoding of a blob ( string )."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninterprets the hex encoding of a blob ( string ).", "response": "def xtob(data, sep=''):\n    \"\"\"Interpret the hex encoding of a blob (string).\"\"\"\n    # remove the non-hex characters\n    data = re.sub(\"[^0-9a-fA-F]\", '', data)\n\n    # interpret the hex\n    return binascii.unhexlify(data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a module level logger.", "response": "def ModuleLogger(globs):\n    \"\"\"Create a module level logger.\n\n    To debug a module, create a _debug variable in the module, then use the\n    ModuleLogger function to create a \"module level\" logger.  When a handler\n    is added to this logger or a child of this logger, the _debug variable will\n    be incremented.\n\n    All of the calls within functions or class methods within the module should\n    first check to see if _debug is set to prevent calls to formatter objects\n    that aren't necessary.\n    \"\"\"\n    # make sure that _debug is defined\n    if not globs.has_key('_debug'):\n        raise RuntimeError(\"define _debug before creating a module logger\")\n\n    # logger name is the module name\n    logger_name = globs['__name__']\n\n    # create a logger to be assigned to _log\n    logger = logging.getLogger(logger_name)\n\n    # put in a reference to the module globals\n    logger.globs = globs\n\n    # if this is a \"root\" logger add a default handler for warnings and up\n    if '.' not in logger_name:\n        hdlr = logging.StreamHandler()\n        hdlr.setLevel(logging.WARNING)\n        hdlr.setFormatter(logging.Formatter(logging.BASIC_FORMAT, None))\n        logger.addHandler(hdlr)\n\n    return logger"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfunctioning for attaching a debugging logger to a class or function.", "response": "def bacpypes_debugging(obj):\n    \"\"\"Function for attaching a debugging logger to a class or function.\"\"\"\n    # create a logger for this object\n    logger = logging.getLogger(obj.__module__ + '.' + obj.__name__)\n\n    # make it available to instances\n    obj._logger = logger\n    obj._debug = logger.debug\n    obj._info = logger.info\n    obj._warning = logger.warning\n    obj._error = logger.error\n    obj._exception = logger.exception\n    obj._fatal = logger.fatal\n\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef debug_contents(self, indent=1, file=sys.stdout, _ids=None):\n        if _debug: _log.debug(\"debug_contents indent=%r file=%r _ids=%r\", indent, file, _ids)\n\n        klasses = list(self.__class__.__mro__)\n        klasses.reverse()\n        if _debug: _log.debug(\"    - klasses: %r\", klasses)\n\n        # loop through the classes and look for _debug_contents\n        attrs = []\n        cids = []\n        ownFn = []\n        for klass in klasses:\n            if klass is DebugContents:\n                continue\n\n            if not issubclass(klass, DebugContents) and hasattr(klass, 'debug_contents'):\n                for i, seenAlready in enumerate(ownFn):\n                    if issubclass(klass, seenAlready):\n                        del ownFn[i]\n                        break\n                ownFn.append(klass)\n                continue\n\n            # look for a tuple of attribute names\n            if not hasattr(klass, '_debug_contents'):\n                continue\n\n            debugContents = klass._debug_contents\n            if not isinstance(debugContents, tuple):\n                raise RuntimeError(\"%s._debug_contents must be a tuple\" % (klass.__name__,))\n\n            # already seen it?\n            if id(debugContents) in cids:\n                continue\n            cids.append(id(debugContents))\n\n            for attr in debugContents:\n                if attr not in attrs:\n                    attrs.append(attr)\n\n        # a bit of debugging\n        if _debug:\n            _log.debug(\"    - attrs: %r\", attrs)\n            _log.debug(\"    - ownFn: %r\", ownFn)\n\n        # make/extend the list of objects already seen\n        if _ids is None:\n            _ids = []\n\n        # loop through the attributes\n        for attr in attrs:\n            # assume you're going deep, but not into lists and dicts\n            goDeep = True\n            goListDict = False\n            goHexed = False\n\n            # attribute list might want to go deep\n            if attr.endswith(\"-\"):\n                goDeep = False\n                attr = attr[:-1]\n            elif attr.endswith(\"*\"):\n                goHexed = True\n                attr = attr[:-1]\n            elif attr.endswith(\"+\"):\n                goDeep = False\n                goListDict = True\n                attr = attr[:-1]\n                if attr.endswith(\"+\"):\n                    goDeep = True\n                    attr = attr[:-1]\n\n            value = getattr(self, attr, None)\n\n            # skip None\n            if value is None:\n                continue\n\n            # standard output\n            if goListDict and isinstance(value, list) and value:\n                file.write(\"%s%s = [\\n\" % ('    ' * indent, attr))\n                indent += 1\n                for i, elem in enumerate(value):\n                    file.write(\"%s[%d] %r\\n\" % ('    ' * indent, i, elem))\n                    if goDeep and hasattr(elem, 'debug_contents'):\n                        if id(elem) not in _ids:\n                            _ids.append(id(elem))\n                            elem.debug_contents(indent + 1, file, _ids)\n                indent -= 1\n                file.write(\"%s    ]\\n\" % ('    ' * indent,))\n            elif goListDict and isinstance(value, dict) and value:\n                file.write(\"%s%s = {\\n\" % ('    ' * indent, attr))\n                indent += 1\n                for key, elem in value.items():\n                    file.write(\"%s%r : %r\\n\" % ('    ' * indent, key, elem))\n                    if goDeep and hasattr(elem, 'debug_contents'):\n                        if id(elem) not in _ids:\n                            _ids.append(id(elem))\n                            elem.debug_contents(indent + 1, file, _ids)\n                indent -= 1\n                file.write(\"%s    }\\n\" % ('    ' * indent,))\n            elif goHexed and isinstance(value, str):\n                if len(value) > 20:\n                    hexed = btox(value[:20], '.') + \"...\"\n                else:\n                    hexed = btox(value, '.')\n                file.write(\"%s%s = x'%s'\\n\" % ('    ' * indent, attr, hexed))\n#           elif goHexed and isinstance(value, int):\n#               file.write(\"%s%s = 0x%X\\n\" % ('    ' * indent, attr, value))\n            else:\n                file.write(\"%s%s = %r\\n\" % ('    ' * indent, attr, value))\n\n                # go nested if it is debugable\n                if goDeep and hasattr(value, 'debug_contents'):\n                    if id(value) not in _ids:\n                        _ids.append(id(value))\n                        value.debug_contents(indent + 1, file, _ids)\n\n        # go through the functions\n        ownFn.reverse()\n        for klass in ownFn:\n            klass.debug_contents(self, indent, file, _ids)", "response": "Debug the contents of an object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do_read(self, args):\n        args = args.split()\n        if _debug: ReadPropertyConsoleCmd._debug(\"do_read %r\", args)\n\n        try:\n            addr, obj_id, prop_id = args[:3]\n            obj_id = ObjectIdentifier(obj_id).value\n\n            datatype = get_datatype(obj_id[0], prop_id)\n            if not datatype:\n                raise ValueError(\"invalid property for object type\")\n\n            # build a request\n            request = ReadPropertyRequest(\n                objectIdentifier=obj_id,\n                propertyIdentifier=prop_id,\n                )\n            request.pduDestination = Address(addr)\n\n            if len(args) == 4:\n                request.propertyArrayIndex = int(args[3])\n            if _debug: ReadPropertyConsoleCmd._debug(\"    - request: %r\", request)\n\n            # make an IOCB\n            iocb = IOCB(request)\n            if _debug: ReadPropertyConsoleCmd._debug(\"    - iocb: %r\", iocb)\n\n            # give it to the application\n            deferred(this_application.request_io, iocb)\n\n            # wait for it to complete\n            iocb.wait()\n\n            # do something for success\n            if iocb.ioResponse:\n                apdu = iocb.ioResponse\n\n                # should be an ack\n                if not isinstance(apdu, ReadPropertyACK):\n                    if _debug: ReadPropertyConsoleCmd._debug(\"    - not an ack\")\n                    return\n\n                # find the datatype\n                datatype = get_datatype(apdu.objectIdentifier[0], apdu.propertyIdentifier)\n                if _debug: ReadPropertyConsoleCmd._debug(\"    - datatype: %r\", datatype)\n                if not datatype:\n                    raise TypeError(\"unknown datatype\")\n\n                # special case for array parts, others are managed by cast_out\n                if issubclass(datatype, Array) and (apdu.propertyArrayIndex is not None):\n                    if apdu.propertyArrayIndex == 0:\n                        value = apdu.propertyValue.cast_out(Unsigned)\n                    else:\n                        value = apdu.propertyValue.cast_out(datatype.subtype)\n                else:\n                    value = apdu.propertyValue.cast_out(datatype)\n                if _debug: ReadPropertyConsoleCmd._debug(\"    - value: %r\", value)\n\n                sys.stdout.write(str(value) + '\\n')\n                if hasattr(value, 'debug_contents'):\n                    value.debug_contents(file=sys.stdout)\n                sys.stdout.flush()\n\n            # do something for error/reject/abort\n            if iocb.ioError:\n                sys.stdout.write(str(iocb.ioError) + '\\n')\n\n        except Exception, error:\n            ReadPropertyConsoleCmd._exception(\"exception: %r\", error)", "response": "This function is called by the application when a property read operation is performed on the virtual machine."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_node(self, node):\n        if _debug: Network._debug(\"add_node %r\", node)\n\n        self.nodes.append(node)\n        node.lan = self\n\n        # update the node name\n        if not node.name:\n            node.name = '%s:%s' % (self.name, node.address)", "response": "Add a node to this network."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving a node from this network.", "response": "def remove_node(self, node):\n        \"\"\" Remove a node from this network. \"\"\"\n        if _debug: Network._debug(\"remove_node %r\", node)\n\n        self.nodes.remove(node)\n        node.lan = None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_pdu(self, pdu):\n        if _debug: Network._debug(\"process_pdu(%s) %r\", self.name, pdu)\n\n        # if there is a traffic log, call it with the network name and pdu\n        if self.traffic_log:\n            self.traffic_log(self.name, pdu)\n\n        # randomly drop a packet\n        if self.drop_percent != 0.0:\n            if (random.random() * 100.0) < self.drop_percent:\n                if _debug: Network._debug(\"    - packet dropped\")\n                return\n\n        if pdu.pduDestination == self.broadcast_address:\n            if _debug: Network._debug(\"    - broadcast\")\n            for node in self.nodes:\n                if (pdu.pduSource != node.address):\n                    if _debug: Network._debug(\"    - match: %r\", node)\n                    node.response(deepcopy(pdu))\n        else:\n            if _debug: Network._debug(\"    - unicast\")\n            for node in self.nodes:\n                if node.promiscuous or (pdu.pduDestination == node.address):\n                    if _debug: Network._debug(\"    - match: %r\", node)\n                    node.response(deepcopy(pdu))", "response": "Process a PDU by sending a copy to each node and then sending a copy to each node."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbind to a LAN", "response": "def bind(self, lan):\n        \"\"\"bind to a LAN.\"\"\"\n        if _debug: Node._debug(\"bind %r\", lan)\n\n        lan.add_node(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef indication(self, pdu):\n        if _debug: Node._debug(\"indication(%s) %r\", self.name, pdu)\n\n        # make sure we're connected\n        if not self.lan:\n            raise ConfigurationError(\"unbound node\")\n\n        # if the pduSource is unset, fill in our address, otherwise\n        # leave it alone to allow for simulated spoofing\n        if pdu.pduSource is None:\n            pdu.pduSource = self.address\n        elif (not self.spoofing) and (pdu.pduSource != self.address):\n            raise RuntimeError(\"spoofing address conflict\")\n\n        # actual network delivery is a zero-delay task\n        OneShotFunction(self.lan.process_pdu, pdu)", "response": "This function is called when a message is received from the node. It is called by the node to process the message."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn true iff addr1 matches addr2.", "response": "def Match(addr1, addr2):\n    \"\"\"Return true iff addr1 matches addr2.\"\"\"\n    if _debug: Match._debug(\"Match %r %r\", addr1, addr2)\n\n    if (addr2.addrType == Address.localBroadcastAddr):\n        # match any local station\n        return (addr1.addrType == Address.localStationAddr) or (addr1.addrType == Address.localBroadcastAddr)\n    elif (addr2.addrType == Address.localStationAddr):\n        # match a specific local station\n        return (addr1.addrType == Address.localStationAddr) and (addr1.addrAddr == addr2.addrAddr)\n    elif (addr2.addrType == Address.remoteBroadcastAddr):\n        # match any remote station or remote broadcast on a matching network\n        return ((addr1.addrType == Address.remoteStationAddr) or (addr1.addrType == Address.remoteBroadcastAddr)) \\\n            and (addr1.addrNet == addr2.addrNet)\n    elif (addr2.addrType == Address.remoteStationAddr):\n        # match a specific remote station\n        return (addr1.addrType == Address.remoteStationAddr) and \\\n            (addr1.addrNet == addr2.addrNet) and (addr1.addrAddr == addr2.addrAddr)\n    elif (addr2.addrType == Address.globalBroadcastAddr):\n        # match a global broadcast address\n        return (addr1.addrType == Address.globalBroadcastAddr)\n    else:\n        raise RuntimeError, \"invalid match combination\""}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the contents of an object as a dict.", "response": "def dict_contents(self, use_dict=None, as_class=dict):\n        \"\"\"Return the contents of an object as a dict.\"\"\"\n        if _debug: Sequence._debug(\"dict_contents use_dict=%r as_class=%r\", use_dict, as_class)\n        global _sequence_of_classes, _list_of_classes\n\n        # make/extend the dictionary of content\n        if use_dict is None:\n            use_dict = as_class()\n\n        # loop through the elements\n        for element in self.sequenceElements:\n            value = getattr(self, element.name, None)\n            if value is None:\n                continue\n\n            if (element.klass in _sequence_of_classes) or (element.klass in _list_of_classes):\n                helper = element.klass(value)\n                mapped_value = helper.dict_contents(as_class=as_class)\n\n            elif issubclass(element.klass, Atomic):\n                mapped_value = value                        ### ambiguous\n\n            elif issubclass(element.klass, AnyAtomic):\n                mapped_value = value.value                  ### ambiguous\n\n            elif isinstance(value, element.klass):\n                mapped_value = value.dict_contents(as_class=as_class)\n                use_dict.__setitem__(element.name, mapped_value)\n\n            else:\n                continue\n\n            # update the dictionary being built\n            use_dict.__setitem__(element.name, mapped_value)\n\n        # return what we built/updated\n        return use_dict"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dict_contents(self, use_dict=None, as_class=dict):\n        if _debug: Choice._debug(\"dict_contents use_dict=%r as_class=%r\", use_dict, as_class)\n\n        # make/extend the dictionary of content\n        if use_dict is None:\n            use_dict = as_class()\n\n        # look for the chosen element\n        for element in self.choiceElements:\n            value = getattr(self, element.name, None)\n            if value is None:\n                continue\n\n            if issubclass(element.klass, Atomic):\n                mapped_value = value                    ### ambiguous\n            elif issubclass(element.klass, AnyAtomic):\n                mapped_value = value.value              ### ambiguous\n            elif isinstance(value, element.klass):\n                mapped_value = value.dict_contents(as_class=as_class)\n\n            use_dict.__setitem__(element.name, mapped_value)\n            break\n\n        # return what we built/updated\n        return use_dict", "response": "Return the contents of an object as a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cast_in(self, element):\n        if _debug: Any._debug(\"cast_in %r\", element)\n\n        t = TagList()\n        if isinstance(element, Atomic):\n            tag = Tag()\n            element.encode(tag)\n            t.append(tag)\n        elif isinstance(element, AnyAtomic):\n            tag = Tag()\n            element.value.encode(tag)\n            t.append(tag)\n        else:\n            element.encode(t)\n\n        self.tagList.extend(t.tagList)", "response": "encode the element into the internal tag list."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninterprets the content as a particular class.", "response": "def cast_out(self, klass):\n        \"\"\"Interpret the content as a particular class.\"\"\"\n        if _debug: Any._debug(\"cast_out %r\", klass)\n        global _sequence_of_classes, _list_of_classes\n\n        # check for a sequence element\n        if (klass in _sequence_of_classes) or (klass in _list_of_classes):\n            # build a sequence helper\n            helper = klass()\n\n            # make a copy of the tag list\n            t = TagList(self.tagList[:])\n\n            # let it decode itself\n            helper.decode(t)\n\n            # make sure everything was consumed\n            if len(t) != 0:\n                raise DecodingError(\"incomplete cast\")\n\n            # return what was built\n            return helper.value\n\n        # check for an array element\n        elif klass in _array_of_classes:\n            # build a sequence helper\n            helper = klass()\n\n            # make a copy of the tag list\n            t = TagList(self.tagList[:])\n\n            # let it decode itself\n            helper.decode(t)\n\n            # make sure everything was consumed\n            if len(t) != 0:\n                raise DecodingError(\"incomplete cast\")\n\n            # return what was built with Python list semantics\n            return helper.value[1:]\n\n        elif issubclass(klass, (Atomic, AnyAtomic)):\n            # make sure there's only one piece\n            if len(self.tagList) == 0:\n                raise DecodingError(\"missing cast component\")\n            if len(self.tagList) > 1:\n                raise DecodingError(\"too many cast components\")\n\n            if _debug: Any._debug(\"    - building helper: %r\", klass)\n\n            # a helper cooperates between the atomic value and the tag\n            helper = klass(self.tagList[0])\n\n            # return the value\n            return helper.value\n\n        else:\n            if _debug: Any._debug(\"    - building value: %r\", klass)\n\n            # build an element\n            value = klass()\n\n            # make a copy of the tag list\n            t = TagList(self.tagList[:])\n\n            # let it decode itself\n            value.decode(t)\n\n            # make sure everything was consumed\n            if len(t) != 0:\n                raise DecodingError(\"incomplete cast\")\n\n            # return what was built\n            return value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dict_contents(self, use_dict=None, as_class=dict):\n        if _debug: Any._debug(\"dict_contents use_dict=%r as_class=%r\", use_dict, as_class)\n\n        # result will be a list\n        rslt_list = []\n\n        # loop through the tags\n        for tag in self.tagList:\n            # build a tag thing\n            use_dict = as_class()\n\n            # save the pieces\n            use_dict.__setitem__('class', tag.tagClass)\n            use_dict.__setitem__('number', tag.tagNumber)\n            use_dict.__setitem__('lvt', tag.tagLVT)\n\n            ### use_dict.__setitem__('data', '.'.join('%02X' % ord(c) for c in tag.tagData))\n\n            # add it to the list\n            rslt_list = use_dict\n\n        # return what we built\n        return rslt_list", "response": "Return the contents of an object as a dict."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cast_in(self, element):\n        if _debug: SequenceOfAny._debug(\"cast_in %r\", element)\n\n        # make sure it is a list\n        if not isinstance(element, List):\n            raise EncodingError(\"%r is not a list\" % (element,))\n\n        t = TagList()\n        element.encode(t)\n\n        self.tagList.extend(t.tagList)", "response": "encode the element into the internal tag list."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninterprets the content as a particular class.", "response": "def cast_out(self, klass):\n        \"\"\"Interpret the content as a particular class.\"\"\"\n        if _debug: SequenceOfAny._debug(\"cast_out %r\", klass)\n\n        # make sure it is a list\n        if not issubclass(klass, List):\n            raise DecodingError(\"%r is not a list\" % (klass,))\n\n        # build a helper\n        helper = klass()\n\n        # make a copy of the tag list\n        t = TagList(self.tagList[:])\n\n        # let it decode itself\n        helper.decode(t)\n\n        # make sure everything was consumed\n        if len(t) != 0:\n            raise DecodingError(\"incomplete cast\")\n\n        return helper.value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrespond to a Who - Is request.", "response": "def do_WhoIsRequest(self, apdu):\n        \"\"\"Respond to a Who-Is request.\"\"\"\n        if _debug: WhoIsIAmServices._debug(\"do_WhoIsRequest %r\", apdu)\n\n        # ignore this if there's no local device\n        if not self.localDevice:\n            if _debug: WhoIsIAmServices._debug(\"    - no local device\")\n            return\n\n        # extract the parameters\n        low_limit = apdu.deviceInstanceRangeLowLimit\n        high_limit = apdu.deviceInstanceRangeHighLimit\n\n        # check for consistent parameters\n        if (low_limit is not None):\n            if (high_limit is None):\n                raise MissingRequiredParameter(\"deviceInstanceRangeHighLimit required\")\n            if (low_limit < 0) or (low_limit > 4194303):\n                raise ParameterOutOfRange(\"deviceInstanceRangeLowLimit out of range\")\n        if (high_limit is not None):\n            if (low_limit is None):\n                raise MissingRequiredParameter(\"deviceInstanceRangeLowLimit required\")\n            if (high_limit < 0) or (high_limit > 4194303):\n                raise ParameterOutOfRange(\"deviceInstanceRangeHighLimit out of range\")\n\n        # see we should respond\n        if (low_limit is not None):\n            if (self.localDevice.objectIdentifier[1] < low_limit):\n                return\n        if (high_limit is not None):\n            if (self.localDevice.objectIdentifier[1] > high_limit):\n                return\n\n        # generate an I-Am\n        self.i_am(address=apdu.pduSource)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nresponds to an I - Am request.", "response": "def do_IAmRequest(self, apdu):\n        \"\"\"Respond to an I-Am request.\"\"\"\n        if _debug: WhoIsIAmServices._debug(\"do_IAmRequest %r\", apdu)\n\n        # check for required parameters\n        if apdu.iAmDeviceIdentifier is None:\n            raise MissingRequiredParameter(\"iAmDeviceIdentifier required\")\n        if apdu.maxAPDULengthAccepted is None:\n            raise MissingRequiredParameter(\"maxAPDULengthAccepted required\")\n        if apdu.segmentationSupported is None:\n            raise MissingRequiredParameter(\"segmentationSupported required\")\n        if apdu.vendorID is None:\n            raise MissingRequiredParameter(\"vendorID required\")\n\n        # extract the device instance number\n        device_instance = apdu.iAmDeviceIdentifier[1]\n        if _debug: WhoIsIAmServices._debug(\"    - device_instance: %r\", device_instance)\n\n        # extract the source address\n        device_address = apdu.pduSource\n        if _debug: WhoIsIAmServices._debug(\"    - device_address: %r\", device_address)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_WhoHasRequest(self, apdu):\n        if _debug: WhoHasIHaveServices._debug(\"do_WhoHasRequest, %r\", apdu)\n\n        # ignore this if there's no local device\n        if not self.localDevice:\n            if _debug: WhoIsIAmServices._debug(\"    - no local device\")\n            return\n\n        # if this has limits, check them like Who-Is\n        if apdu.limits is not None:\n            # extract the parameters\n            low_limit = apdu.limits.deviceInstanceRangeLowLimit\n            high_limit = apdu.limits.deviceInstanceRangeHighLimit\n\n            # check for consistent parameters\n            if (low_limit is None):\n                raise MissingRequiredParameter(\"deviceInstanceRangeLowLimit required\")\n            if (low_limit < 0) or (low_limit > 4194303):\n                raise ParameterOutOfRange(\"deviceInstanceRangeLowLimit out of range\")\n            if (high_limit is None):\n                raise MissingRequiredParameter(\"deviceInstanceRangeHighLimit required\")\n            if (high_limit < 0) or (high_limit > 4194303):\n                raise ParameterOutOfRange(\"deviceInstanceRangeHighLimit out of range\")\n\n            # see we should respond\n            if (self.localDevice.objectIdentifier[1] < low_limit):\n                return\n            if (self.localDevice.objectIdentifier[1] > high_limit):\n                return\n\n        # find the object\n        if apdu.object.objectIdentifier is not None:\n            obj = self.objectIdentifier.get(apdu.object.objectIdentifier, None)\n        elif apdu.object.objectName is not None:\n            obj = self.objectName.get(apdu.object.objectName, None)\n        else:\n            raise InconsistentParameters(\"object identifier or object name required\")\n\n        # maybe we don't have it\n        if not obj:\n            return\n\n        # send out the response\n        self.i_have(obj, address=apdu.pduSource)", "response": "Respond to a Who - Has request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrespond to a I - Have request.", "response": "def do_IHaveRequest(self, apdu):\n        \"\"\"Respond to a I-Have request.\"\"\"\n        if _debug: WhoHasIHaveServices._debug(\"do_IHaveRequest %r\", apdu)\n\n        # check for required parameters\n        if apdu.deviceIdentifier is None:\n            raise MissingRequiredParameter(\"deviceIdentifier required\")\n        if apdu.objectIdentifier is None:\n            raise MissingRequiredParameter(\"objectIdentifier required\")\n        if apdu.objectName is None:\n            raise MissingRequiredParameter(\"objectName required\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_segmentation_context(self, apdu):\n        if _debug: SSM._debug(\"set_segmentation_context %s\", repr(apdu))\n\n        # set the context\n        self.segmentAPDU = apdu", "response": "This function is called to set the segmentation context."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_task(self):\n        if _debug: ClientSSM._debug(\"process_task\")\n\n        if self.state == SEGMENTED_REQUEST:\n            self.segmented_request_timeout()\n        elif self.state == AWAIT_CONFIRMATION:\n            self.await_confirmation_timeout()\n        elif self.state == SEGMENTED_CONFIRMATION:\n            self.segmented_confirmation_timeout()\n        elif self.state == COMPLETED:\n            pass\n        elif self.state == ABORTED:\n            pass\n        else:\n            e = RuntimeError(\"invalid state\")\n            ClientSSM._exception(\"exception: %r\", e)\n            raise e", "response": "This function is called when something has taken too long."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef abort(self, reason):\n        if _debug: ClientSSM._debug(\"abort %r\", reason)\n\n        # change the state to aborted\n        self.set_state(ABORTED)\n\n        # build an abort PDU to return\n        abort_pdu = AbortPDU(False, self.invokeID, reason)\n\n        # return it\n        return abort_pdu", "response": "This function is called when the transaction should be aborted."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef segmented_request(self, apdu):\n        if _debug: ClientSSM._debug(\"segmented_request %r\", apdu)\n\n        # server is ready for the next segment\n        if apdu.apduType == SegmentAckPDU.pduType:\n            if _debug: ClientSSM._debug(\"    - segment ack\")\n\n            # actual window size is provided by server\n            self.actualWindowSize = apdu.apduWin\n\n            # duplicate ack received?\n            if not self.in_window(apdu.apduSeq, self.initialSequenceNumber):\n                if _debug: ClientSSM._debug(\"    - not in window\")\n                self.restart_timer(self.segmentTimeout)\n\n            # final ack received?\n            elif self.sentAllSegments:\n                if _debug: ClientSSM._debug(\"    - all done sending request\")\n                self.set_state(AWAIT_CONFIRMATION, self.apduTimeout)\n\n            # more segments to send\n            else:\n                if _debug: ClientSSM._debug(\"    - more segments to send\")\n\n                self.initialSequenceNumber = (apdu.apduSeq + 1) % 256\n                self.segmentRetryCount = 0\n                self.fill_window(self.initialSequenceNumber)\n                self.restart_timer(self.segmentTimeout)\n\n        # simple ack\n        elif (apdu.apduType == SimpleAckPDU.pduType):\n            if _debug: ClientSSM._debug(\"    - simple ack\")\n\n            if not self.sentAllSegments:\n                abort = self.abort(AbortReason.invalidApduInThisState)\n                self.request(abort)     # send it to the device\n                self.response(abort)    # send it to the application\n            else:\n                self.set_state(COMPLETED)\n                self.response(apdu)\n\n        elif (apdu.apduType == ComplexAckPDU.pduType):\n            if _debug: ClientSSM._debug(\"    - complex ack\")\n\n            if not self.sentAllSegments:\n                abort = self.abort(AbortReason.invalidApduInThisState)\n                self.request(abort)     # send it to the device\n                self.response(abort)    # send it to the application\n\n            elif not apdu.apduSeg:\n                # ack is not segmented\n                self.set_state(COMPLETED)\n                self.response(apdu)\n\n            else:\n                # set the segmented response context\n                self.set_segmentation_context(apdu)\n\n                # minimum of what the server is proposing and this client proposes\n                self.actualWindowSize = min(apdu.apduWin, self.ssmSAP.proposedWindowSize)\n                self.lastSequenceNumber = 0\n                self.initialSequenceNumber = 0\n                self.set_state(SEGMENTED_CONFIRMATION, self.segmentTimeout)\n\n        # some kind of problem\n        elif (apdu.apduType == ErrorPDU.pduType) or (apdu.apduType == RejectPDU.pduType) or (apdu.apduType == AbortPDU.pduType):\n            if _debug: ClientSSM._debug(\"    - error/reject/abort\")\n\n            self.set_state(COMPLETED)\n            self.response = apdu\n            self.response(apdu)\n\n        else:\n            raise RuntimeError(\"invalid APDU (2)\")", "response": "This function is called when the client is sending a segmented request and receives an apdu."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef request(self, apdu):\n        if _debug: ServerSSM._debug(\"request %r\", apdu)\n\n        # make sure it has a good source and destination\n        apdu.pduSource = self.pdu_address\n        apdu.pduDestination = None\n\n        # send it via the device\n        self.ssmSAP.sap_request(apdu)", "response": "This function is called by the application to send a new version of the application."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef indication(self, apdu):\n        if _debug: ServerSSM._debug(\"indication %r\", apdu)\n\n        if self.state == IDLE:\n            self.idle(apdu)\n        elif self.state == SEGMENTED_REQUEST:\n            self.segmented_request(apdu)\n        elif self.state == AWAIT_RESPONSE:\n            self.await_response(apdu)\n        elif self.state == SEGMENTED_RESPONSE:\n            self.segmented_response(apdu)\n        else:\n            if _debug: ServerSSM._debug(\"    - invalid state\")", "response": "This function is called for each downstream packet related to\n        the transaction."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_task(self):\n        if _debug: ServerSSM._debug(\"process_task\")\n\n        if self.state == SEGMENTED_REQUEST:\n            self.segmented_request_timeout()\n        elif self.state == AWAIT_RESPONSE:\n            self.await_response_timeout()\n        elif self.state == SEGMENTED_RESPONSE:\n            self.segmented_response_timeout()\n        elif self.state == COMPLETED:\n            pass\n        elif self.state == ABORTED:\n            pass\n        else:\n            if _debug: ServerSSM._debug(\"invalid state\")\n            raise RuntimeError(\"invalid state\")", "response": "This function is called when the client has taken too long to send all of the segments of a segmented request and the application has taken too long to ack the segments of a segmented response."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_next_invoke_id(self, addr):\n        if _debug: StateMachineAccessPoint._debug(\"get_next_invoke_id\")\n\n        initialID = self.nextInvokeID\n        while 1:\n            invokeID = self.nextInvokeID\n            self.nextInvokeID = (self.nextInvokeID + 1) % 256\n\n            # see if we've checked for them all\n            if initialID == self.nextInvokeID:\n                raise RuntimeError(\"no available invoke ID\")\n\n            for tr in self.clientTransactions:\n                if (invokeID == tr.invokeID) and (addr == tr.pdu_address):\n                    break\n            else:\n                break\n\n        return invokeID", "response": "Returns the next available invoke ID for the given address."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef confirmation(self, pdu):\n        if _debug: StateMachineAccessPoint._debug(\"confirmation %r\", pdu)\n\n        # check device communication control\n        if self.dccEnableDisable == 'enable':\n            if _debug: StateMachineAccessPoint._debug(\"    - communications enabled\")\n        elif self.dccEnableDisable == 'disable':\n            if (pdu.apduType == 0) and (pdu.apduService == 17):\n                if _debug: StateMachineAccessPoint._debug(\"    - continue with DCC request\")\n            elif (pdu.apduType == 0) and (pdu.apduService == 20):\n                if _debug: StateMachineAccessPoint._debug(\"    - continue with reinitialize device\")\n            elif (pdu.apduType == 1) and (pdu.apduService == 8):\n                if _debug: StateMachineAccessPoint._debug(\"    - continue with Who-Is\")\n            else:\n                if _debug: StateMachineAccessPoint._debug(\"    - not a Who-Is, dropped\")\n                return\n        elif self.dccEnableDisable == 'disableInitiation':\n            if _debug: StateMachineAccessPoint._debug(\"    - initiation disabled\")\n\n        # make a more focused interpretation\n        atype = apdu_types.get(pdu.apduType)\n        if not atype:\n            StateMachineAccessPoint._warning(\"    - unknown apduType: %r\", pdu.apduType)\n            return\n\n        # decode it\n        apdu = atype()\n        apdu.decode(pdu)\n        if _debug: StateMachineAccessPoint._debug(\"    - apdu: %r\", apdu)\n\n        if isinstance(apdu, ConfirmedRequestPDU):\n            # find duplicates of this request\n            for tr in self.serverTransactions:\n                if (apdu.apduInvokeID == tr.invokeID) and (apdu.pduSource == tr.pdu_address):\n                    break\n            else:\n                # build a server transaction\n                tr = ServerSSM(self, apdu.pduSource)\n\n                # add it to our transactions to track it\n                self.serverTransactions.append(tr)\n\n            # let it run with the apdu\n            tr.indication(apdu)\n\n        elif isinstance(apdu, UnconfirmedRequestPDU):\n            # deliver directly to the application\n            self.sap_request(apdu)\n\n        elif isinstance(apdu, SimpleAckPDU) \\\n            or isinstance(apdu, ComplexAckPDU) \\\n            or isinstance(apdu, ErrorPDU) \\\n            or isinstance(apdu, RejectPDU):\n\n            # find the client transaction this is acking\n            for tr in self.clientTransactions:\n                if (apdu.apduInvokeID == tr.invokeID) and (apdu.pduSource == tr.pdu_address):\n                    break\n            else:\n                return\n\n            # send the packet on to the transaction\n            tr.confirmation(apdu)\n\n        elif isinstance(apdu, AbortPDU):\n            # find the transaction being aborted\n            if apdu.apduSrv:\n                for tr in self.clientTransactions:\n                    if (apdu.apduInvokeID == tr.invokeID) and (apdu.pduSource == tr.pdu_address):\n                        break\n                else:\n                    return\n\n                # send the packet on to the transaction\n                tr.confirmation(apdu)\n            else:\n                for tr in self.serverTransactions:\n                    if (apdu.apduInvokeID == tr.invokeID) and (apdu.pduSource == tr.pdu_address):\n                        break\n                else:\n                    return\n\n                # send the packet on to the transaction\n                tr.indication(apdu)\n\n        elif isinstance(apdu, SegmentAckPDU):\n            # find the transaction being aborted\n            if apdu.apduSrv:\n                for tr in self.clientTransactions:\n                    if (apdu.apduInvokeID == tr.invokeID) and (apdu.pduSource == tr.pdu_address):\n                        break\n                else:\n                    return\n\n                # send the packet on to the transaction\n                tr.confirmation(apdu)\n            else:\n                for tr in self.serverTransactions:\n                    if (apdu.apduInvokeID == tr.invokeID) and (apdu.pduSource == tr.pdu_address):\n                        break\n                else:\n                    return\n\n                # send the packet on to the transaction\n                tr.indication(apdu)\n\n        else:\n            raise RuntimeError(\"invalid APDU (8)\")", "response": "This function is called by the application to process a confirmation request."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sap_indication(self, apdu):\n        if _debug: StateMachineAccessPoint._debug(\"sap_indication %r\", apdu)\n\n        # check device communication control\n        if self.dccEnableDisable == 'enable':\n            if _debug: StateMachineAccessPoint._debug(\"    - communications enabled\")\n\n        elif self.dccEnableDisable == 'disable':\n            if _debug: StateMachineAccessPoint._debug(\"    - communications disabled\")\n            return\n\n        elif self.dccEnableDisable == 'disableInitiation':\n            if _debug: StateMachineAccessPoint._debug(\"    - initiation disabled\")\n\n            if (apdu.apduType == 1) and (apdu.apduService == 0):\n                if _debug: StateMachineAccessPoint._debug(\"    - continue with I-Am\")\n            else:\n                if _debug: StateMachineAccessPoint._debug(\"    - not an I-Am\")\n                return\n\n        if isinstance(apdu, UnconfirmedRequestPDU):\n            # deliver to the device\n            self.request(apdu)\n\n        elif isinstance(apdu, ConfirmedRequestPDU):\n            # make sure it has an invoke ID\n            if apdu.apduInvokeID is None:\n                apdu.apduInvokeID = self.get_next_invoke_id(apdu.pduDestination)\n            else:\n                # verify the invoke ID isn't already being used\n                for tr in self.clientTransactions:\n                    if (apdu.apduInvokeID == tr.invokeID) and (apdu.pduDestination == tr.pdu_address):\n                        raise RuntimeError(\"invoke ID in use\")\n\n            # warning for bogus requests\n            if (apdu.pduDestination.addrType != Address.localStationAddr) and (apdu.pduDestination.addrType != Address.remoteStationAddr):\n                StateMachineAccessPoint._warning(\"%s is not a local or remote station\", apdu.pduDestination)\n\n            # create a client transaction state machine\n            tr = ClientSSM(self, apdu.pduDestination)\n            if _debug: StateMachineAccessPoint._debug(\"    - client segmentation state machine: %r\", tr)\n\n            # add it to our transactions to track it\n            self.clientTransactions.append(tr)\n\n            # let it run\n            tr.indication(apdu)\n\n        else:\n            raise RuntimeError(\"invalid APDU (9)\")", "response": "This function is called when the application is requesting a new transaction as a client. It is called by the application when the application is requesting a new transaction as a client."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sap_confirmation(self, apdu):\n        if _debug: StateMachineAccessPoint._debug(\"sap_confirmation %r\", apdu)\n\n        if isinstance(apdu, SimpleAckPDU) \\\n                or isinstance(apdu, ComplexAckPDU) \\\n                or isinstance(apdu, ErrorPDU) \\\n                or isinstance(apdu, RejectPDU) \\\n                or isinstance(apdu, AbortPDU):\n            # find the appropriate server transaction\n            for tr in self.serverTransactions:\n                if (apdu.apduInvokeID == tr.invokeID) and (apdu.pduDestination == tr.pdu_address):\n                    break\n            else:\n                return\n\n            # pass control to the transaction\n            tr.confirmation(apdu)\n\n        else:\n            raise RuntimeError(\"invalid APDU (10)\")", "response": "This function is called when the application is responding\n            to a request. It is called when the application is responding\n            to a request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_peer(self, peerAddr, networks=None):\n        if _debug: BTR._debug(\"add_peer %r networks=%r\", peerAddr, networks)\n\n        # see if this is already a peer\n        if peerAddr in self.peers:\n            # add the (new?) reachable networks\n            if not networks:\n                networks = []\n            else:\n                self.peers[peerAddr].extend(networks)\n        else:\n            if not networks:\n                networks = []\n\n            # save the networks\n            self.peers[peerAddr] = networks", "response": "Add a peer and optionally provide a list of the reachable networks."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes a peer from the cache.", "response": "def delete_peer(self, peerAddr):\n        \"\"\"Delete a peer.\"\"\"\n        if _debug: BTR._debug(\"delete_peer %r\", peerAddr)\n\n        # get the peer networks\n        # networks = self.peers[peerAddr]\n\n        ### send a control message upstream that these are no longer reachable\n\n        # now delete the peer\n        del self.peers[peerAddr]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef register(self, addr, ttl):\n        # a little error checking\n        if ttl <= 0:\n            raise ValueError(\"time-to-live must be greater than zero\")\n\n        # save the BBMD address and time-to-live\n        if isinstance(addr, Address):\n            self.bbmdAddress = addr\n        else:\n            self.bbmdAddress = Address(addr)\n        self.bbmdTimeToLive = ttl\n\n        # install this task to run when it gets a chance\n        self.install_task(when=0)", "response": "Initiate the process of registering with a BBMD."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unregister(self):\n        pdu = RegisterForeignDevice(0)\n        pdu.pduDestination = self.bbmdAddress\n\n        # send it downstream\n        self.request(pdu)\n\n        # change the status to unregistered\n        self.registrationStatus = -2\n\n        # clear the BBMD address and time-to-live\n        self.bbmdAddress = None\n        self.bbmdTimeToLive = None", "response": "Drop the registration with a BBMD."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_task(self):\n        pdu = RegisterForeignDevice(self.bbmdTimeToLive)\n        pdu.pduDestination = self.bbmdAddress\n\n        # send it downstream\n        self.request(pdu)", "response": "Called when the registration request should be sent to the BBMD."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a foreign device to the FDT.", "response": "def register_foreign_device(self, addr, ttl):\n        \"\"\"Add a foreign device to the FDT.\"\"\"\n        if _debug: BIPBBMD._debug(\"register_foreign_device %r %r\", addr, ttl)\n\n        # see if it is an address or make it one\n        if isinstance(addr, Address):\n            pass\n        elif isinstance(addr, str):\n            addr = Address(addr)\n        else:\n            raise TypeError(\"addr must be a string or an Address\")\n\n        for fdte in self.bbmdFDT:\n            if addr == fdte.fdAddress:\n                break\n        else:\n            fdte = FDTEntry()\n            fdte.fdAddress = addr\n            self.bbmdFDT.append( fdte )\n\n        fdte.fdTTL = ttl\n        fdte.fdRemain = ttl + 5\n\n        # return success\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dict_contents(self, use_dict=None, as_class=dict):\n        # make/extend the dictionary of content\n        if use_dict is None:\n            use_dict = as_class()\n\n        # save the content\n        use_dict.__setitem__('dnet', self.rtDNET)\n        use_dict.__setitem__('port_id', self.rtPortID)\n        use_dict.__setitem__('port_info', self.rtPortInfo)\n\n        # return what we built/updated\n        return use_dict", "response": "Return the contents of an object as a dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_valid(cls, arg):\n        if not isinstance(arg, (int, long)) or isinstance(arg, bool):\n            return False\n        if (arg < cls._low_limit):\n            return False\n        if (cls._high_limit is not None) and (arg > cls._high_limit):\n            return False\n        return True", "response": "Return True if arg is valid for the class."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if arg is valid value for the class.", "response": "def is_valid(cls, arg):\n        \"\"\"Return True if arg is valid value for the class.\"\"\"\n        return isinstance(arg, (int, long)) and (not isinstance(arg, bool))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_valid(cls, arg):\n        return (isinstance(arg, (int, long)) and (arg >= 0)) or \\\n            isinstance(arg, str)", "response": "Return True if the argument is a valid value for the class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_tuple(self):\n        objType, objInstance = self.value\n\n        if isinstance(objType, int):\n            pass\n        elif isinstance(objType, long):\n            objType = int(objType)\n        elif isinstance(objType, basestring):\n            # turn it back into an integer\n            objType = self.objectTypeClass()[objType]\n        else:\n            raise TypeError(\"invalid datatype for objType\")\n\n        # pack the components together\n        return (objType, objInstance)", "response": "Return the unsigned integer tuple of the identifier."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_AtomicReadFileRequest(self, apdu):\n        if _debug: FileServices._debug(\"do_AtomicReadFileRequest %r\", apdu)\n\n        if (apdu.fileIdentifier[0] != 'file'):\n            raise ExecutionError('services', 'inconsistentObjectType')\n\n        # get the object\n        obj = self.get_object_id(apdu.fileIdentifier)\n        if _debug: FileServices._debug(\"    - object: %r\", obj)\n\n        if not obj:\n            raise ExecutionError('object', 'unknownObject')\n\n        if apdu.accessMethod.recordAccess:\n            # check against the object\n            if obj.fileAccessMethod != 'recordAccess':\n                raise ExecutionError('services', 'invalidFileAccessMethod')\n\n            # simplify\n            record_access = apdu.accessMethod.recordAccess\n\n            # check for required parameters\n            if record_access.fileStartRecord is None:\n                raise MissingRequiredParameter(\"fileStartRecord required\")\n            if record_access.requestedRecordCount is None:\n                raise MissingRequiredParameter(\"requestedRecordCount required\")\n\n            ### verify start is valid - double check this (empty files?)\n            if (record_access.fileStartRecord < 0) or \\\n                    (record_access.fileStartRecord >= len(obj)):\n                raise ExecutionError('services', 'invalidFileStartPosition')\n\n            # pass along to the object\n            end_of_file, record_data = obj.read_record(\n                record_access.fileStartRecord,\n                record_access.requestedRecordCount,\n                )\n            if _debug: FileServices._debug(\"    - record_data: %r\", record_data)\n\n            # this is an ack\n            resp = AtomicReadFileACK(context=apdu,\n                endOfFile=end_of_file,\n                accessMethod=AtomicReadFileACKAccessMethodChoice(\n                    recordAccess=AtomicReadFileACKAccessMethodRecordAccess(\n                        fileStartRecord=record_access.fileStartRecord,\n                        returnedRecordCount=len(record_data),\n                        fileRecordData=record_data,\n                        ),\n                    ),\n                )\n\n        elif apdu.accessMethod.streamAccess:\n            # check against the object\n            if obj.fileAccessMethod != 'streamAccess':\n                raise ExecutionError('services', 'invalidFileAccessMethod')\n\n            # simplify\n            stream_access = apdu.accessMethod.streamAccess\n\n            # check for required parameters\n            if stream_access.fileStartPosition is None:\n                raise MissingRequiredParameter(\"fileStartPosition required\")\n            if stream_access.requestedOctetCount is None:\n                raise MissingRequiredParameter(\"requestedOctetCount required\")\n\n            ### verify start is valid - double check this (empty files?)\n            if (stream_access.fileStartPosition < 0) or \\\n                    (stream_access.fileStartPosition >= len(obj)):\n                raise ExecutionError('services', 'invalidFileStartPosition')\n\n            # pass along to the object\n            end_of_file, record_data = obj.read_stream(\n                stream_access.fileStartPosition,\n                stream_access.requestedOctetCount,\n                )\n            if _debug: FileServices._debug(\"    - record_data: %r\", record_data)\n\n            # this is an ack\n            resp = AtomicReadFileACK(context=apdu,\n                endOfFile=end_of_file,\n                accessMethod=AtomicReadFileACKAccessMethodChoice(\n                    streamAccess=AtomicReadFileACKAccessMethodStreamAccess(\n                        fileStartPosition=stream_access.fileStartPosition,\n                        fileData=record_data,\n                        ),\n                    ),\n                )\n\n        if _debug: FileServices._debug(\"    - resp: %r\", resp)\n\n        # return the result\n        self.response(resp)", "response": "This function is called by the application to process a request to get one of our records."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef do_AtomicWriteFileRequest(self, apdu):\n        if _debug: FileServices._debug(\"do_AtomicWriteFileRequest %r\", apdu)\n\n        if (apdu.fileIdentifier[0] != 'file'):\n            raise ExecutionError('services', 'inconsistentObjectType')\n\n        # get the object\n        obj = self.get_object_id(apdu.fileIdentifier)\n        if _debug: FileServices._debug(\"    - object: %r\", obj)\n\n        if not obj:\n            raise ExecutionError('object', 'unknownObject')\n\n        if apdu.accessMethod.recordAccess:\n            # check against the object\n            if obj.fileAccessMethod != 'recordAccess':\n                raise ExecutionError('services', 'invalidFileAccessMethod')\n\n            # simplify\n            record_access = apdu.accessMethod.recordAccess\n\n            # check for required parameters\n            if record_access.fileStartRecord is None:\n                raise MissingRequiredParameter(\"fileStartRecord required\")\n            if record_access.recordCount is None:\n                raise MissingRequiredParameter(\"recordCount required\")\n            if record_access.fileRecordData is None:\n                raise MissingRequiredParameter(\"fileRecordData required\")\n\n            # check for read-only\n            if obj.readOnly:\n                raise ExecutionError('services', 'fileAccessDenied')\n\n            # pass along to the object\n            start_record = obj.write_record(\n                record_access.fileStartRecord,\n                record_access.recordCount,\n                record_access.fileRecordData,\n                )\n            if _debug: FileServices._debug(\"    - start_record: %r\", start_record)\n\n            # this is an ack\n            resp = AtomicWriteFileACK(context=apdu,\n                fileStartRecord=start_record,\n                )\n\n        elif apdu.accessMethod.streamAccess:\n            # check against the object\n            if obj.fileAccessMethod != 'streamAccess':\n                raise ExecutionError('services', 'invalidFileAccessMethod')\n\n            # simplify\n            stream_access = apdu.accessMethod.streamAccess\n\n            # check for required parameters\n            if stream_access.fileStartPosition is None:\n                raise MissingRequiredParameter(\"fileStartPosition required\")\n            if stream_access.fileData is None:\n                raise MissingRequiredParameter(\"fileData required\")\n\n            # check for read-only\n            if obj.readOnly:\n                raise ExecutionError('services', 'fileAccessDenied')\n\n            # pass along to the object\n            start_position = obj.write_stream(\n                stream_access.fileStartPosition,\n                stream_access.fileData,\n                )\n            if _debug: FileServices._debug(\"    - start_position: %r\", start_position)\n\n            # this is an ack\n            resp = AtomicWriteFileACK(context=apdu,\n                fileStartPosition=start_position,\n                )\n\n        if _debug: FileServices._debug(\"    - resp: %r\", resp)\n\n        # return the result\n        self.response(resp)", "response": "This function is called by the application to process a write request."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nencodes the maximum number of segments the device will accept Section 20. 1. 2. 4 and Section 20. 1. 2. 4 and Section 20. 1. 2. 4.", "response": "def encode_max_segments_accepted(arg):\n    \"\"\"Encode the maximum number of segments the device will accept, Section\n    20.1.2.4, and if the device says it can only accept one segment it shouldn't\n    say that it supports segmentation!\"\"\"\n    # unspecified\n    if not arg:\n        return 0\n\n    if arg > 64:\n        return 7\n\n    # the largest number not greater than the arg\n    for i in range(6, 0, -1):\n        if _max_segments_accepted_encoding[i] <= arg:\n            return i\n\n    raise ValueError(\"invalid max max segments accepted: %r\" % (arg,))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the encoding of the highest encodable value less than the max APDU length accepted.", "response": "def encode_max_apdu_length_accepted(arg):\n    \"\"\"Return the encoding of the highest encodable value less than the\n    value of the arg.\"\"\"\n    for i in range(5, -1, -1):\n        if (arg >= _max_apdu_length_encoding[i]):\n            return i\n\n    raise ValueError(\"invalid max APDU length accepted: %r\" % (arg,))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encode(self, pdu):\n        if _debug: APCI._debug(\"encode %r\", pdu)\n\n        PCI.update(pdu, self)\n\n        if (self.apduType == ConfirmedRequestPDU.pduType):\n            # PDU type\n            buff = self.apduType << 4\n            if self.apduSeg:\n                buff += 0x08\n            if self.apduMor:\n                buff += 0x04\n            if self.apduSA:\n                buff += 0x02\n            pdu.put(buff)\n            pdu.put((self.apduMaxSegs << 4) + self.apduMaxResp)\n            pdu.put(self.apduInvokeID)\n            if self.apduSeg:\n                pdu.put(self.apduSeq)\n                pdu.put(self.apduWin)\n            pdu.put(self.apduService)\n\n        elif (self.apduType == UnconfirmedRequestPDU.pduType):\n            pdu.put(self.apduType << 4)\n            pdu.put(self.apduService)\n\n        elif (self.apduType == SimpleAckPDU.pduType):\n            pdu.put(self.apduType << 4)\n            pdu.put(self.apduInvokeID)\n            pdu.put(self.apduService)\n\n        elif (self.apduType == ComplexAckPDU.pduType):\n            # PDU type\n            buff = self.apduType << 4\n            if self.apduSeg:\n                buff += 0x08\n            if self.apduMor:\n                buff += 0x04\n            pdu.put(buff)\n            pdu.put(self.apduInvokeID)\n            if self.apduSeg:\n                pdu.put(self.apduSeq)\n                pdu.put(self.apduWin)\n            pdu.put(self.apduService)\n\n        elif (self.apduType == SegmentAckPDU.pduType):\n            # PDU type\n            buff = self.apduType << 4\n            if self.apduNak:\n                buff += 0x02\n            if self.apduSrv:\n                buff += 0x01\n            pdu.put(buff)\n            pdu.put(self.apduInvokeID)\n            pdu.put(self.apduSeq)\n            pdu.put(self.apduWin)\n\n        elif (self.apduType == ErrorPDU.pduType):\n            pdu.put(self.apduType << 4)\n            pdu.put(self.apduInvokeID)\n            pdu.put(self.apduService)\n\n        elif (self.apduType == RejectPDU.pduType):\n            pdu.put(self.apduType << 4)\n            pdu.put(self.apduInvokeID)\n            pdu.put(self.apduAbortRejectReason)\n\n        elif (self.apduType == AbortPDU.pduType):\n            # PDU type\n            buff = self.apduType << 4\n            if self.apduSrv:\n                buff += 0x01\n            pdu.put(buff)\n            pdu.put(self.apduInvokeID)\n            pdu.put(self.apduAbortRejectReason)\n\n        else:\n            raise ValueError(\"invalid APCI.apduType\")", "response": "encode the contents of the APCI into the PDU."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndecoding the contents of the PDU into the APCI.", "response": "def decode(self, pdu):\n        \"\"\"decode the contents of the PDU into the APCI.\"\"\"\n        if _debug: APCI._debug(\"decode %r\", pdu)\n\n        PCI.update(self, pdu)\n\n        # decode the first octet\n        buff = pdu.get()\n\n        # decode the APCI type\n        self.apduType = (buff >> 4) & 0x0F\n\n        if (self.apduType == ConfirmedRequestPDU.pduType):\n            self.apduSeg = ((buff & 0x08) != 0)\n            self.apduMor = ((buff & 0x04) != 0)\n            self.apduSA  = ((buff & 0x02) != 0)\n            buff = pdu.get()\n            self.apduMaxSegs = (buff >> 4) & 0x07\n            self.apduMaxResp = buff & 0x0F\n            self.apduInvokeID = pdu.get()\n            if self.apduSeg:\n                self.apduSeq = pdu.get()\n                self.apduWin = pdu.get()\n            self.apduService = pdu.get()\n            self.pduData = pdu.pduData\n\n        elif (self.apduType == UnconfirmedRequestPDU.pduType):\n            self.apduService = pdu.get()\n            self.pduData = pdu.pduData\n\n        elif (self.apduType == SimpleAckPDU.pduType):\n            self.apduInvokeID = pdu.get()\n            self.apduService = pdu.get()\n\n        elif (self.apduType == ComplexAckPDU.pduType):\n            self.apduSeg = ((buff & 0x08) != 0)\n            self.apduMor = ((buff & 0x04) != 0)\n            self.apduInvokeID = pdu.get()\n            if self.apduSeg:\n                self.apduSeq = pdu.get()\n                self.apduWin = pdu.get()\n            self.apduService = pdu.get()\n            self.pduData = pdu.pduData\n\n        elif (self.apduType == SegmentAckPDU.pduType):\n            self.apduNak = ((buff & 0x02) != 0)\n            self.apduSrv = ((buff & 0x01) != 0)\n            self.apduInvokeID = pdu.get()\n            self.apduSeq = pdu.get()\n            self.apduWin = pdu.get()\n\n        elif (self.apduType == ErrorPDU.pduType):\n            self.apduInvokeID = pdu.get()\n            self.apduService = pdu.get()\n            self.pduData = pdu.pduData\n\n        elif (self.apduType == RejectPDU.pduType):\n            self.apduInvokeID = pdu.get()\n            self.apduAbortRejectReason = pdu.get()\n\n        elif (self.apduType == AbortPDU.pduType):\n            self.apduSrv = ((buff & 0x01) != 0)\n            self.apduInvokeID = pdu.get()\n            self.apduAbortRejectReason = pdu.get()\n            self.pduData = pdu.pduData\n\n        else:\n            raise DecodingError(\"invalid APDU type\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef apci_contents(self, use_dict=None, as_class=dict):\n        if _debug: APCI._debug(\"apci_contents use_dict=%r as_class=%r\", use_dict, as_class)\n\n        # make/extend the dictionary of content\n        if use_dict is None:\n            use_dict = as_class()\n\n        # copy the source and destination to make it easier to search\n        if self.pduSource:\n            use_dict.__setitem__('source', str(self.pduSource))\n        if self.pduDestination:\n            use_dict.__setitem__('destination', str(self.pduDestination))\n\n        # loop through the elements\n        for attr in APCI._debug_contents:\n            value = getattr(self, attr, None)\n            if value is None:\n                continue\n\n            if attr == 'apduType':\n                mapped_value = apdu_types[self.apduType].__name__\n            elif attr == 'apduService':\n                if self.apduType in (ConfirmedRequestPDU.pduType, SimpleAckPDU.pduType, ComplexAckPDU.pduType):\n                    mapped_value = confirmed_request_types[self.apduService].__name__\n                elif (self.apduType == UnconfirmedRequestPDU.pduType):\n                    mapped_value = unconfirmed_request_types[self.apduService].__name__\n                elif (self.apduType == ErrorPDU.pduType):\n                    mapped_value = error_types[self.apduService].__name__\n            else:\n                mapped_value = value\n\n            # save the mapped value\n            use_dict.__setitem__(attr, mapped_value)\n\n        # return what we built/updated\n        return use_dict", "response": "Return the contents of an object as a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the contents of an object as a dict.", "response": "def dict_contents(self, use_dict=None, as_class=dict):\n        \"\"\"Return the contents of an object as a dict.\"\"\"\n        if _debug: APDU._debug(\"dict_contents use_dict=%r as_class=%r\", use_dict, as_class)\n\n        # make/extend the dictionary of content\n        if use_dict is None:\n            use_dict = as_class()\n\n        # call the parent classes\n        self.apci_contents(use_dict=use_dict, as_class=as_class)\n        self.apdu_contents(use_dict=use_dict, as_class=as_class)\n\n        # return what we built/updated\n        return use_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the contents of an object as a dict.", "response": "def apdu_contents(self, use_dict=None, as_class=dict):\n        \"\"\"Return the contents of an object as a dict.\"\"\"\n        if _debug: APCISequence._debug(\"apdu_contents use_dict=%r as_class=%r\", use_dict, as_class)\n\n        # make/extend the dictionary of content\n        if use_dict is None:\n            use_dict = as_class()\n\n        # set the function based on the class name\n        use_dict.__setitem__('function', self.__class__.__name__)\n\n        # fill in from the sequence contents\n        Sequence.dict_contents(self, use_dict=use_dict, as_class=as_class)\n\n        # return what we built/updated\n        return use_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set(self, tclass, tnum, tlvt=0, tdata=''):\n        if not isinstance(tdata, str):\n            raise TypeError(\"tag data must be str\")\n\n        self.tagClass = tclass\n        self.tagNumber = tnum\n        self.tagLVT = tlvt\n        self.tagData = tdata", "response": "set the values of the tag."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the values of the tag", "response": "def set_app_data(self, tnum, tdata):\n        \"\"\"set the values of the tag.\"\"\"\n        if not isinstance(tdata, str):\n            raise TypeError(\"tag data must be str\")\n\n        self.tagClass = Tag.applicationTagClass\n        self.tagNumber = tnum\n        self.tagLVT = len(tdata)\n        self.tagData = tdata"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndecodes a tag from the PDU.", "response": "def decode(self, pdu):\n        \"\"\"Decode a tag from the PDU.\"\"\"\n        try:\n            tag = pdu.get()\n\n            # extract the type\n            self.tagClass = (tag >> 3) & 0x01\n\n            # extract the tag number\n            self.tagNumber = (tag >> 4)\n            if (self.tagNumber == 0x0F):\n                self.tagNumber = pdu.get()\n\n            # extract the length\n            self.tagLVT = tag & 0x07\n            if (self.tagLVT == 5):\n                self.tagLVT = pdu.get()\n                if (self.tagLVT == 254):\n                    self.tagLVT = pdu.get_short()\n                elif (self.tagLVT == 255):\n                    self.tagLVT = pdu.get_long()\n            elif (self.tagLVT == 6):\n                self.tagClass = Tag.openingTagClass\n                self.tagLVT = 0\n            elif (self.tagLVT == 7):\n                self.tagClass = Tag.closingTagClass\n                self.tagLVT = 0\n\n            # application tagged boolean has no more data\n            if (self.tagClass == Tag.applicationTagClass) and (self.tagNumber == Tag.booleanAppTag):\n                # tagLVT contains value\n                self.tagData = ''\n            else:\n                # tagLVT contains length\n                self.tagData = pdu.get_data(self.tagLVT)\n        except DecodingError:\n            raise InvalidTag(\"invalid tag encoding\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef app_to_context(self, context):\n        if self.tagClass != Tag.applicationTagClass:\n            raise ValueError(\"application tag required\")\n\n        # application tagged boolean now has data\n        if (self.tagNumber == Tag.booleanAppTag):\n            return ContextTag(context, chr(self.tagLVT))\n        else:\n            return ContextTag(context, self.tagData)", "response": "Return a context encoded tag."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef context_to_app(self, dataType):\n        if self.tagClass != Tag.contextTagClass:\n            raise ValueError(\"context tag required\")\n\n        # context booleans have value in data\n        if (dataType == Tag.booleanAppTag):\n            return Tag(Tag.applicationTagClass, Tag.booleanAppTag, struct.unpack('B', self.tagData)[0], '')\n        else:\n            return ApplicationTag(dataType, self.tagData)", "response": "Return an application encoded tag."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if the argument is valid for the class.", "response": "def is_valid(cls, arg):\n        \"\"\"Return True if arg is valid value for the class.  If the string\n        value is wrong for the enumeration, the encoding will fail.\n        \"\"\"\n        return (isinstance(arg, (int, long)) and (arg >= 0)) or \\\n            isinstance(arg, basestring)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encode(self, pdu):\n        if _debug: BSLCI._debug(\"encode %r\", pdu)\n\n        # copy the basics\n        PCI.update(pdu, self)\n\n        pdu.put( self.bslciType )               # 0x83\n        pdu.put( self.bslciFunction )\n\n        if (self.bslciLength != len(self.pduData) + 4):\n            raise EncodingError(\"invalid BSLCI length\")\n\n        pdu.put_short( self.bslciLength )", "response": "encode the contents of the BSLCI into the PDU."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndecode the contents of the PDU into the BSLCI.", "response": "def decode(self, pdu):\n        \"\"\"decode the contents of the PDU into the BSLCI.\"\"\"\n        if _debug: BSLCI._debug(\"decode %r\", pdu)\n\n        # copy the basics\n        PCI.update(self, pdu)\n\n        self.bslciType = pdu.get()\n        if self.bslciType != 0x83:\n            raise DecodingError(\"invalid BSLCI type\")\n\n        self.bslciFunction = pdu.get()\n        self.bslciLength = pdu.get_short()\n\n        if (self.bslciLength != len(pdu.pduData) + 4):\n            raise DecodingError(\"invalid BSLCI length\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ConsoleLogHandler(loggerRef='', handler=None, level=logging.DEBUG, color=None):\n    if isinstance(loggerRef, logging.Logger):\n        pass\n\n    elif isinstance(loggerRef, str):\n        # check for root\n        if not loggerRef:\n            loggerRef = _log\n\n        # check for a valid logger name\n        elif loggerRef not in logging.Logger.manager.loggerDict:\n            raise RuntimeError(\"not a valid logger name: %r\" % (loggerRef,))\n\n        # get the logger\n        loggerRef = logging.getLogger(loggerRef)\n\n    else:\n        raise RuntimeError(\"not a valid logger reference: %r\" % (loggerRef,))\n\n    # see if this (or its parent) is a module level logger\n    if hasattr(loggerRef, 'globs'):\n        loggerRef.globs['_debug'] += 1\n    elif hasattr(loggerRef.parent, 'globs'):\n        loggerRef.parent.globs['_debug'] += 1\n\n    # make a handler if one wasn't provided\n    if not handler:\n        handler = logging.StreamHandler()\n        handler.setLevel(level)\n\n    # use our formatter\n    handler.setFormatter(LoggingFormatter(color))\n\n    # add it to the logger\n    loggerRef.addHandler(handler)\n\n    # make sure the logger has at least this level\n    loggerRef.setLevel(level)", "response": "Add a handler to stderr with our custom formatter to a logger."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_args(self, *args, **kwargs):\n        if _debug: ArgumentParser._debug(\"parse_args\")\n\n        # pass along to the parent class\n        result_args = _ArgumentParser.parse_args(self, *args, **kwargs)\n\n        # check to dump labels\n        if result_args.buggers:\n            loggers = sorted(logging.Logger.manager.loggerDict.keys())\n            for loggerName in loggers:\n                sys.stdout.write(loggerName + '\\n')\n            sys.exit(0)\n\n        # check for debug\n        if result_args.debug is None:\n            # --debug not specified\n            result_args.debug = []\n        elif not result_args.debug:\n            # --debug, but no arguments\n            result_args.debug = [\"__main__\"]\n\n        # check for debugging from the environment\n        if BACPYPES_DEBUG:\n            result_args.debug.extend(BACPYPES_DEBUG.split())\n        if BACPYPES_COLOR:\n            result_args.color = True\n\n        # keep track of which files are going to be used\n        file_handlers = {}\n\n        # loop through the bug list\n        for i, debug_name in enumerate(result_args.debug):\n            color = (i % 6) + 2 if result_args.color else None\n\n            debug_specs = debug_name.split(':')\n            if len(debug_specs) == 1:\n                ConsoleLogHandler(debug_name, color=color)\n            else:\n                # the debugger name is just the first component\n                debug_name = debug_specs[0]\n\n                # if the file is already being used, use the already created handler\n                file_name = debug_specs[1]\n                if file_name in file_handlers:\n                    handler = file_handlers[file_name]\n                else:\n                    if len(debug_specs) >= 3:\n                        maxBytes = int(debug_specs[2])\n                    else:\n                        maxBytes = BACPYPES_MAXBYTES\n                    if len(debug_specs) >= 4:\n                        backupCount = int(debug_specs[3])\n                    else:\n                        backupCount = BACPYPES_BACKUPCOUNT\n\n                    # create a handler\n                    handler = logging.handlers.RotatingFileHandler(\n                        file_name, maxBytes=maxBytes, backupCount=backupCount,\n                        )\n                    handler.setLevel(logging.DEBUG)\n\n                    # save it for more than one instance\n                    file_handlers[file_name] = handler\n\n                # use this handler, no color\n                ConsoleLogHandler(debug_name, handler=handler)\n\n        # return what was parsed\n        return result_args", "response": "Parse the arguments as usual then add default processing."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the arguments as usual then add default processing.", "response": "def parse_args(self, *args, **kwargs):\n        \"\"\"Parse the arguments as usual, then add default processing.\"\"\"\n        if _debug: ConfigArgumentParser._debug(\"parse_args\")\n\n        # pass along to the parent class\n        result_args = ArgumentParser.parse_args(self, *args, **kwargs)\n\n        # read in the configuration file\n        config = _ConfigParser()\n        config.read(result_args.ini)\n        if _debug: _log.debug(\"    - config: %r\", config)\n\n        # check for BACpypes section\n        if not config.has_section('BACpypes'):\n            raise RuntimeError(\"INI file with BACpypes section required\")\n\n        # convert the contents to an object\n        ini_obj = type('ini', (object,), dict(config.items('BACpypes')))\n        if _debug: _log.debug(\"    - ini_obj: %r\", ini_obj)\n\n        # add the object to the parsed arguments\n        setattr(result_args, 'ini', ini_obj)\n\n        # return what was parsed\n        return result_args"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the specified property of the object with the optional array index and cast the result into an Any object.", "response": "def read_property_to_any(obj, propertyIdentifier, propertyArrayIndex=None):\n    \"\"\"Read the specified property of the object, with the optional array index,\n    and cast the result into an Any object.\"\"\"\n    if _debug: read_property_to_any._debug(\"read_property_to_any %s %r %r\", obj, propertyIdentifier, propertyArrayIndex)\n\n    # get the datatype\n    datatype = obj.get_datatype(propertyIdentifier)\n    if _debug: read_property_to_any._debug(\"    - datatype: %r\", datatype)\n    if datatype is None:\n        raise ExecutionError(errorClass='property', errorCode='datatypeNotSupported')\n\n    # get the value\n    value = obj.ReadProperty(propertyIdentifier, propertyArrayIndex)\n    if _debug: read_property_to_any._debug(\"    - value: %r\", value)\n    if value is None:\n        raise ExecutionError(errorClass='property', errorCode='unknownProperty')\n\n    # change atomic values into something encodeable\n    if issubclass(datatype, Atomic):\n        value = datatype(value)\n    elif issubclass(datatype, Array) and (propertyArrayIndex is not None):\n        if propertyArrayIndex == 0:\n            value = Unsigned(value)\n        elif issubclass(datatype.subtype, Atomic):\n            value = datatype.subtype(value)\n        elif not isinstance(value, datatype.subtype):\n            raise TypeError(\"invalid result datatype, expecting %s and got %s\" \\\n                % (datatype.subtype.__name__, type(value).__name__))\n    elif not isinstance(value, datatype):\n        raise TypeError(\"invalid result datatype, expecting %s and got %s\" \\\n            % (datatype.__name__, type(value).__name__))\n    if _debug: read_property_to_any._debug(\"    - encodeable value: %r\", value)\n\n    # encode the value\n    result = Any()\n    result.cast_in(value)\n    if _debug: read_property_to_any._debug(\"    - result: %r\", result)\n\n    # return the object\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the specified property of the object with the optional array index and return the result as an Any object.", "response": "def read_property_to_result_element(obj, propertyIdentifier, propertyArrayIndex=None):\n    \"\"\"Read the specified property of the object, with the optional array index,\n    and cast the result into an Any object.\"\"\"\n    if _debug: read_property_to_result_element._debug(\"read_property_to_result_element %s %r %r\", obj, propertyIdentifier, propertyArrayIndex)\n\n    # save the result in the property value\n    read_result = ReadAccessResultElementChoice()\n\n    try:\n        if not obj:\n            raise ExecutionError(errorClass='object', errorCode='unknownObject')\n\n        read_result.propertyValue = read_property_to_any(obj, propertyIdentifier, propertyArrayIndex)\n        if _debug: read_property_to_result_element._debug(\"    - success\")\n    except PropertyError as error:\n        if _debug: read_property_to_result_element._debug(\"    - error: %r\", error)\n        read_result.propertyAccessError = ErrorType(errorClass='property', errorCode='unknownProperty')\n    except ExecutionError as error:\n        if _debug: read_property_to_result_element._debug(\"    - error: %r\", error)\n        read_result.propertyAccessError = ErrorType(errorClass=error.errorClass, errorCode=error.errorCode)\n\n    # make an element for this value\n    read_access_result_element = ReadAccessResultElement(\n        propertyIdentifier=propertyIdentifier,\n        propertyArrayIndex=propertyArrayIndex,\n        readResult=read_result,\n        )\n    if _debug: read_property_to_result_element._debug(\"    - read_access_result_element: %r\", read_access_result_element)\n\n    # fini\n    return read_access_result_element"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run_once():\n    if _debug: run_once._debug(\"run_once\")\n    global taskManager, deferredFns\n\n    # reference the task manager (a singleton)\n    taskManager = TaskManager()\n\n    try:\n        delta = 0.0\n        while delta == 0.0:\n            # get the next task\n            task, delta = taskManager.get_next_task()\n            if _debug: run_once._debug(\"    - task, delta: %r, %r\", task, delta)\n\n            # if there is a task to process, do it\n            if task:\n                taskManager.process_task(task)\n\n            # check for deferred functions\n            while deferredFns:\n                # get a reference to the list\n                fnlist = deferredFns\n                deferredFns = []\n\n                # call the functions\n                for fn, args, kwargs in fnlist:\n                    if _debug: run_once._debug(\"    - call: %r %r %r\", fn, args, kwargs)\n                    fn(*args, **kwargs)\n\n                # done with this list\n                del fnlist\n\n    except KeyboardInterrupt:\n        if _debug: run_once._info(\"keyboard interrupt\")\n    except Exception as err:\n        if _debug: run_once._exception(\"an error has occurred: %s\", err)", "response": "This function runs the tasks and deferred functions in a single thread."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_object_class(object_type, vendor_id=0):\n    if _debug: get_object_class._debug(\"get_object_class %r vendor_id=%r\", object_type, vendor_id)\n\n    # find the klass as given\n    cls = registered_object_types.get((object_type, vendor_id))\n    if _debug: get_object_class._debug(\"    - direct lookup: %s\", repr(cls))\n\n    # if the class isn't found and the vendor id is non-zero, try the standard class for the type\n    if (not cls) and vendor_id:\n        cls = registered_object_types.get((object_type, 0))\n        if _debug: get_object_class._debug(\"    - default lookup: %s\", repr(cls))\n\n    return cls", "response": "Return the class associated with an object type."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the datatype for the property of an object.", "response": "def get_datatype(object_type, propid, vendor_id=0):\n    \"\"\"Return the datatype for the property of an object.\"\"\"\n    if _debug: get_datatype._debug(\"get_datatype %r %r vendor_id=%r\", object_type, propid, vendor_id)\n\n    # get the related class\n    cls = get_object_class(object_type, vendor_id)\n    if not cls:\n        return None\n\n    # get the property\n    prop = cls._properties.get(propid)\n    if not prop:\n        return None\n\n    # return the datatype\n    return prop.datatype"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _attr_to_property(self, attr):\n\n        # get the property\n        prop = self._properties.get(attr)\n        if not prop:\n            raise PropertyError(attr)\n\n        # found it\n        return prop", "response": "Common routine to translate a python attribute name to a property name and\n        return the appropriate property."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a property to an object.", "response": "def add_property(self, prop):\n        \"\"\"Add a property to an object.  The property is an instance of\n        a Property or one of its derived classes.  Adding a property\n        disconnects it from the collection of properties common to all of the\n        objects of its class.\"\"\"\n        if _debug: Object._debug(\"add_property %r\", prop)\n\n        # make a copy of the properties dictionary\n        self._properties = _copy(self._properties)\n\n        # save the property reference and default value (usually None)\n        self._properties[prop.identifier] = prop\n        self._values[prop.identifier] = prop.default"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes a property from an object.", "response": "def delete_property(self, prop):\n        \"\"\"Delete a property from an object.  The property is an instance of\n        a Property or one of its derived classes, but only the property\n        is relavent.  Deleting a property disconnects it from the collection of\n        properties common to all of the objects of its class.\"\"\"\n        if _debug: Object._debug(\"delete_property %r\", prop)\n\n        # make a copy of the properties dictionary\n        self._properties = _copy(self._properties)\n\n        # delete the property from the dictionary and values\n        del self._properties[prop.identifier]\n        if prop.identifier in self._values:\n            del self._values[prop.identifier]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_datatype(self, propid):\n        if _debug: Object._debug(\"get_datatype %r\", propid)\n\n        # get the property\n        prop = self._properties.get(propid)\n        if not prop:\n            raise PropertyError(propid)\n\n        # return the datatype\n        return prop.datatype", "response": "Return the datatype for the property of an object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _dict_contents(self, use_dict=None, as_class=dict):\n        if _debug: Object._debug(\"dict_contents use_dict=%r as_class=%r\", use_dict, as_class)\n\n        # make/extend the dictionary of content\n        if use_dict is None:\n            use_dict = as_class()\n\n        klasses = list(self.__class__.__mro__)\n        klasses.reverse()\n\n        # build a list of property identifiers \"bottom up\"\n        property_names = []\n        properties_seen = set()\n        for c in klasses:\n            for prop in getattr(c, 'properties', []):\n                if prop.identifier not in properties_seen:\n                    property_names.append(prop.identifier)\n                    properties_seen.add(prop.identifier)\n\n        # extract the values\n        for property_name in property_names:\n            # get the value\n            property_value = self._properties.get(property_name).ReadProperty(self)\n            if property_value is None:\n                continue\n\n            # if the value has a way to convert it to a dict, use it\n            if hasattr(property_value, \"dict_contents\"):\n                property_value = property_value.dict_contents(as_class=as_class)\n\n            # save the value\n            use_dict.__setitem__(property_name, property_value)\n\n        # return what we built/updated\n        return use_dict", "response": "Return the contents of an object as a dict."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef debug_contents(self, indent=1, file=sys.stdout, _ids=None):\n        klasses = list(self.__class__.__mro__)\n        klasses.reverse()\n\n        # print special attributes \"bottom up\"\n        previous_attrs = ()\n        for c in klasses:\n            attrs = getattr(c, '_debug_contents', ())\n\n            # if we have seen this list already, move to the next class\n            if attrs is previous_attrs:\n                continue\n\n            for attr in attrs:\n                file.write(\"%s%s = %s\\n\" % (\"    \" * indent, attr, getattr(self, attr)))\n            previous_attrs = attrs\n\n        # build a list of property identifiers \"bottom up\"\n        property_names = []\n        properties_seen = set()\n        for c in klasses:\n            for prop in getattr(c, 'properties', []):\n                if prop.identifier not in properties_seen:\n                    property_names.append(prop.identifier)\n                    properties_seen.add(prop.identifier)\n\n        # print out the values\n        for property_name in property_names:\n            property_value = self._values.get(property_name, None)\n\n            # printing out property values that are None is tedious\n            if property_value is None:\n                continue\n\n            if hasattr(property_value, \"debug_contents\"):\n                file.write(\"%s%s\\n\" % (\"    \" * indent, property_name))\n                property_value.debug_contents(indent+1, file, _ids)\n            else:\n                file.write(\"%s%s = %r\\n\" % (\"    \" * indent, property_name, property_value))", "response": "Print out interesting things about the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads a property from a set of properties", "response": "def do_read(self, args):\n        \"\"\"read <addr> ( <objid> ( <prop> [ <indx> ] )... )...\"\"\"\n        args = args.split()\n        if _debug: ReadPropertyMultipleConsoleCmd._debug(\"do_read %r\", args)\n\n        try:\n            i = 0\n            addr = args[i]\n            i += 1\n\n            read_access_spec_list = []\n            while i < len(args):\n                obj_id = ObjectIdentifier(args[i]).value\n                i += 1\n\n                prop_reference_list = []\n                while i < len(args):\n                    prop_id = args[i]\n                    if prop_id not in PropertyIdentifier.enumerations:\n                        break\n\n                    i += 1\n                    if prop_id in ('all', 'required', 'optional'):\n                        pass\n                    else:\n                        datatype = get_datatype(obj_id[0], prop_id)\n                        if not datatype:\n                            raise ValueError(\"invalid property for object type\")\n\n                    # build a property reference\n                    prop_reference = PropertyReference(\n                        propertyIdentifier=prop_id,\n                        )\n\n                    # check for an array index\n                    if (i < len(args)) and args[i].isdigit():\n                        prop_reference.propertyArrayIndex = int(args[i])\n                        i += 1\n\n                    # add it to the list\n                    prop_reference_list.append(prop_reference)\n\n                # check for at least one property\n                if not prop_reference_list:\n                    raise ValueError(\"provide at least one property\")\n\n                # build a read access specification\n                read_access_spec = ReadAccessSpecification(\n                    objectIdentifier=obj_id,\n                    listOfPropertyReferences=prop_reference_list,\n                    )\n\n                # add it to the list\n                read_access_spec_list.append(read_access_spec)\n\n            # check for at least one\n            if not read_access_spec_list:\n                raise RuntimeError(\"at least one read access specification required\")\n\n            # build the request\n            request = ReadPropertyMultipleRequest(\n                listOfReadAccessSpecs=read_access_spec_list,\n                )\n            request.pduDestination = Address(addr)\n            if _debug: ReadPropertyMultipleConsoleCmd._debug(\"    - request: %r\", request)\n\n            # make an IOCB\n            iocb = IOCB(request)\n            if _debug: ReadPropertyMultipleConsoleCmd._debug(\"    - iocb: %r\", iocb)\n\n            # give it to the application\n            deferred(this_application.request_io, iocb)\n\n            # wait for it to complete\n            iocb.wait()\n\n            # do something for success\n            if iocb.ioResponse:\n                apdu = iocb.ioResponse\n\n                # should be an ack\n                if not isinstance(apdu, ReadPropertyMultipleACK):\n                    if _debug: ReadPropertyMultipleConsoleCmd._debug(\"    - not an ack\")\n                    return\n\n                # loop through the results\n                for result in apdu.listOfReadAccessResults:\n                    # here is the object identifier\n                    objectIdentifier = result.objectIdentifier\n                    if _debug: ReadPropertyMultipleConsoleCmd._debug(\"    - objectIdentifier: %r\", objectIdentifier)\n\n                    # now come the property values per object\n                    for element in result.listOfResults:\n                        # get the property and array index\n                        propertyIdentifier = element.propertyIdentifier\n                        if _debug: ReadPropertyMultipleConsoleCmd._debug(\"    - propertyIdentifier: %r\", propertyIdentifier)\n                        propertyArrayIndex = element.propertyArrayIndex\n                        if _debug: ReadPropertyMultipleConsoleCmd._debug(\"    - propertyArrayIndex: %r\", propertyArrayIndex)\n\n                        # here is the read result\n                        readResult = element.readResult\n\n                        sys.stdout.write(propertyIdentifier)\n                        if propertyArrayIndex is not None:\n                            sys.stdout.write(\"[\" + str(propertyArrayIndex) + \"]\")\n\n                        # check for an error\n                        if readResult.propertyAccessError is not None:\n                            sys.stdout.write(\" ! \" + str(readResult.propertyAccessError) + '\\n')\n\n                        else:\n                            # here is the value\n                            propertyValue = readResult.propertyValue\n\n                            # find the datatype\n                            datatype = get_datatype(objectIdentifier[0], propertyIdentifier)\n                            if _debug: ReadPropertyMultipleConsoleCmd._debug(\"    - datatype: %r\", datatype)\n                            if not datatype:\n                                raise TypeError(\"unknown datatype\")\n\n                            # special case for array parts, others are managed by cast_out\n                            if issubclass(datatype, Array) and (propertyArrayIndex is not None):\n                                if propertyArrayIndex == 0:\n                                    value = propertyValue.cast_out(Unsigned)\n                                else:\n                                    value = propertyValue.cast_out(datatype.subtype)\n                            else:\n                                value = propertyValue.cast_out(datatype)\n                            if _debug: ReadPropertyMultipleConsoleCmd._debug(\"    - value: %r\", value)\n\n                            sys.stdout.write(\" = \" + str(value) + '\\n')\n                        sys.stdout.flush()\n\n            # do something for error/reject/abort\n            if iocb.ioError:\n                sys.stdout.write(str(iocb.ioError) + '\\n')\n\n        except Exception, error:\n            ReadPropertyMultipleConsoleCmd._exception(\"exception: %r\", error)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef encode(self, pdu):\n        if _debug: BVLCI._debug(\"encode %s\", str(pdu))\n\n        # copy the basics\n        PCI.update(pdu, self)\n\n        pdu.put( self.bvlciType )               # 0x81\n        pdu.put( self.bvlciFunction )\n\n        if (self.bvlciLength != len(self.pduData) + 4):\n            raise EncodingError(\"invalid BVLCI length\")\n\n        pdu.put_short( self.bvlciLength )", "response": "encode the contents of the BVLCI into the PDU."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndecode the contents of the PDU into the BVLCI.", "response": "def decode(self, pdu):\n        \"\"\"decode the contents of the PDU into the BVLCI.\"\"\"\n        if _debug: BVLCI._debug(\"decode %s\", str(pdu))\n\n        # copy the basics\n        PCI.update(self, pdu)\n\n        self.bvlciType = pdu.get()\n        if self.bvlciType != 0x81:\n            raise DecodingError(\"invalid BVLCI type\")\n\n        self.bvlciFunction = pdu.get()\n        self.bvlciLength = pdu.get_short()\n\n        if (self.bvlciLength != len(pdu.pduData) + 4):\n            raise DecodingError(\"invalid BVLCI length\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the contents of an object as a dict.", "response": "def bvlci_contents(self, use_dict=None, as_class=dict):\n        \"\"\"Return the contents of an object as a dict.\"\"\"\n        if _debug: BVLCI._debug(\"bvlci_contents use_dict=%r as_class=%r\", use_dict, as_class)\n\n        # make/extend the dictionary of content\n        if use_dict is None:\n            use_dict = as_class()\n\n        # save the mapped value\n        use_dict.__setitem__('type', self.bvlciType)\n        use_dict.__setitem__('function', self.bvlciFunction)\n        use_dict.__setitem__('length', self.bvlciLength)\n\n        # return what we built/updated\n        return use_dict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the contents of an object as a dict.", "response": "def dict_contents(self, use_dict=None, as_class=dict, key_values=()):\n        \"\"\"Return the contents of an object as a dict.\"\"\"\n        if _debug: BVLPDU._debug(\"dict_contents use_dict=%r as_class=%r key_values=%r\", use_dict, as_class, key_values)\n\n        # make/extend the dictionary of content\n        if use_dict is None:\n            use_dict = as_class()\n\n        # call the superclasses\n        self.bvlci_contents(use_dict=use_dict, as_class=as_class)\n        self.bvlpdu_contents(use_dict=use_dict, as_class=as_class)\n\n        # return what we built/updated\n        return use_dict"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(self, pci):\n        self.pduUserData = pci.pduUserData\n        self.pduSource = pci.pduSource\n        self.pduDestination = pci.pduDestination", "response": "Update the fields of the PCI object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pci_contents(self, use_dict=None, as_class=dict):\n        if _debug: PCI._debug(\"pci_contents use_dict=%r as_class=%r\", use_dict, as_class)\n\n        # make/extend the dictionary of content\n        if use_dict is None:\n            use_dict = as_class()\n\n        # save the values\n        for k, v in (('user_data', self.pduUserData), ('source', self.pduSource), ('destination', self.pduDestination)):\n            if _debug: PCI._debug(\"    - %r: %r\", k, v)\n            if v is None:\n                continue\n\n            if hasattr(v, 'dict_contents'):\n                v = v.dict_contents(as_class=as_class)\n            use_dict.__setitem__(k, v)\n\n        # return what we built/updated\n        return use_dict", "response": "Return the contents of an object as a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dict_contents(self, use_dict=None, as_class=dict):\n        if _debug: PCI._debug(\"dict_contents use_dict=%r as_class=%r\", use_dict, as_class)\n\n        return self.pci_contents(use_dict=use_dict, as_class=as_class)", "response": "Return the contents of an object as a dict."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the contents of an object as a dict.", "response": "def pdudata_contents(self, use_dict=None, as_class=dict):\n        \"\"\"Return the contents of an object as a dict.\"\"\"\n        if _debug: PDUData._debug(\"pdudata_contents use_dict=%r as_class=%r\", use_dict, as_class)\n\n        # make/extend the dictionary of content\n        if use_dict is None:\n            use_dict = as_class()\n\n        # add the data if it is not None\n        v = self.pduData\n        if v is not None:\n            if isinstance(v, str):\n                v = btox(v)\n            elif hasattr(v, 'dict_contents'):\n                v = v.dict_contents(as_class=as_class)\n            use_dict.__setitem__('data', v)\n\n        # return what we built/updated\n        return use_dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the contents of an object as a dict.", "response": "def dict_contents(self, use_dict=None, as_class=dict):\n        \"\"\"Return the contents of an object as a dict.\"\"\"\n        if _debug: PDUData._debug(\"dict_contents use_dict=%r as_class=%r\", use_dict, as_class)\n\n        return self.pdudata_contents(use_dict=use_dict, as_class=as_class)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the contents of an object as a dict.", "response": "def dict_contents(self, use_dict=None, as_class=dict):\n        \"\"\"Return the contents of an object as a dict.\"\"\"\n        if _debug: PDU._debug(\"dict_contents use_dict=%r as_class=%r\", use_dict, as_class)\n\n        # make/extend the dictionary of content\n        if use_dict is None:\n            use_dict = as_class()\n\n        # call into the two base classes\n        self.pci_contents(use_dict=use_dict, as_class=as_class)\n        self.pdudata_contents(use_dict=use_dict, as_class=as_class)\n\n        # return what we built/updated\n        return use_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef confirmation(self, *args, **kwargs):\n        if not self.current_terminal:\n            raise RuntimeError(\"no active terminal\")\n        if not isinstance(self.current_terminal, Client):\n            raise RuntimeError(\"current terminal not a client\")\n\n        self.current_terminal.confirmation(*args, **kwargs)", "response": "Upstream packet send to current terminal."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_ReadPropertyRequest(self, apdu):\n        if _debug: ReadWritePropertyServices._debug(\"do_ReadPropertyRequest %r\", apdu)\n\n        # extract the object identifier\n        objId = apdu.objectIdentifier\n\n        # check for wildcard\n        if (objId == ('device', 4194303)) and self.localDevice is not None:\n            if _debug: ReadWritePropertyServices._debug(\"    - wildcard device identifier\")\n            objId = self.localDevice.objectIdentifier\n\n        # get the object\n        obj = self.get_object_id(objId)\n        if _debug: ReadWritePropertyServices._debug(\"    - object: %r\", obj)\n\n        if not obj:\n            raise ExecutionError(errorClass='object', errorCode='unknownObject')\n\n        try:\n            # get the datatype\n            datatype = obj.get_datatype(apdu.propertyIdentifier)\n            if _debug: ReadWritePropertyServices._debug(\"    - datatype: %r\", datatype)\n\n            # get the value\n            value = obj.ReadProperty(apdu.propertyIdentifier, apdu.propertyArrayIndex)\n            if _debug: ReadWritePropertyServices._debug(\"    - value: %r\", value)\n            if value is None:\n                raise PropertyError(apdu.propertyIdentifier)\n\n            # change atomic values into something encodeable\n            if issubclass(datatype, Atomic):\n                value = datatype(value)\n            elif issubclass(datatype, Array) and (apdu.propertyArrayIndex is not None):\n                if apdu.propertyArrayIndex == 0:\n                    value = Unsigned(value)\n                elif issubclass(datatype.subtype, Atomic):\n                    value = datatype.subtype(value)\n                elif not isinstance(value, datatype.subtype):\n                    raise TypeError(\"invalid result datatype, expecting %r and got %r\" \\\n                        % (datatype.subtype.__name__, type(value).__name__))\n            elif not isinstance(value, datatype):\n                raise TypeError(\"invalid result datatype, expecting %r and got %r\" \\\n                    % (datatype.__name__, type(value).__name__))\n            if _debug: ReadWritePropertyServices._debug(\"    - encodeable value: %r\", value)\n\n            # this is a ReadProperty ack\n            resp = ReadPropertyACK(context=apdu)\n            resp.objectIdentifier = objId\n            resp.propertyIdentifier = apdu.propertyIdentifier\n            resp.propertyArrayIndex = apdu.propertyArrayIndex\n\n            # save the result in the property value\n            resp.propertyValue = Any()\n            resp.propertyValue.cast_in(value)\n            if _debug: ReadWritePropertyServices._debug(\"    - resp: %r\", resp)\n\n        except PropertyError:\n            raise ExecutionError(errorClass='property', errorCode='unknownProperty')\n\n        # return the result\n        self.response(resp)", "response": "Return the value of some property of one of our objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nchange the value of some property of one of our objects.", "response": "def do_WritePropertyRequest(self, apdu):\n        \"\"\"Change the value of some property of one of our objects.\"\"\"\n        if _debug: ReadWritePropertyServices._debug(\"do_WritePropertyRequest %r\", apdu)\n\n        # get the object\n        obj = self.get_object_id(apdu.objectIdentifier)\n        if _debug: ReadWritePropertyServices._debug(\"    - object: %r\", obj)\n        if not obj:\n            raise ExecutionError(errorClass='object', errorCode='unknownObject')\n\n        try:\n            # check if the property exists\n            if obj.ReadProperty(apdu.propertyIdentifier, apdu.propertyArrayIndex) is None:\n                raise PropertyError(apdu.propertyIdentifier)\n\n            # get the datatype, special case for null\n            if apdu.propertyValue.is_application_class_null():\n                datatype = Null\n            else:\n                datatype = obj.get_datatype(apdu.propertyIdentifier)\n            if _debug: ReadWritePropertyServices._debug(\"    - datatype: %r\", datatype)\n\n            # special case for array parts, others are managed by cast_out\n            if issubclass(datatype, Array) and (apdu.propertyArrayIndex is not None):\n                if apdu.propertyArrayIndex == 0:\n                    value = apdu.propertyValue.cast_out(Unsigned)\n                else:\n                    value = apdu.propertyValue.cast_out(datatype.subtype)\n            else:\n                value = apdu.propertyValue.cast_out(datatype)\n            if _debug: ReadWritePropertyServices._debug(\"    - value: %r\", value)\n\n            # change the value\n            value = obj.WriteProperty(apdu.propertyIdentifier, value, apdu.propertyArrayIndex, apdu.priority)\n\n            # success\n            resp = SimpleAckPDU(context=apdu)\n            if _debug: ReadWritePropertyServices._debug(\"    - resp: %r\", resp)\n\n        except PropertyError:\n            raise ExecutionError(errorClass='property', errorCode='unknownProperty')\n\n        # return the result\n        self.response(resp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresponds to a ReadPropertyMultiple Request.", "response": "def do_ReadPropertyMultipleRequest(self, apdu):\n        \"\"\"Respond to a ReadPropertyMultiple Request.\"\"\"\n        if _debug: ReadWritePropertyMultipleServices._debug(\"do_ReadPropertyMultipleRequest %r\", apdu)\n\n        # response is a list of read access results (or an error)\n        resp = None\n        read_access_result_list = []\n\n        # loop through the request\n        for read_access_spec in apdu.listOfReadAccessSpecs:\n            # get the object identifier\n            objectIdentifier = read_access_spec.objectIdentifier\n            if _debug: ReadWritePropertyMultipleServices._debug(\"    - objectIdentifier: %r\", objectIdentifier)\n\n            # check for wildcard\n            if (objectIdentifier == ('device', 4194303)) and self.localDevice is not None:\n                if _debug: ReadWritePropertyMultipleServices._debug(\"    - wildcard device identifier\")\n                objectIdentifier = self.localDevice.objectIdentifier\n\n            # get the object\n            obj = self.get_object_id(objectIdentifier)\n            if _debug: ReadWritePropertyMultipleServices._debug(\"    - object: %r\", obj)\n\n            # build a list of result elements\n            read_access_result_element_list = []\n\n            # loop through the property references\n            for prop_reference in read_access_spec.listOfPropertyReferences:\n                # get the property identifier\n                propertyIdentifier = prop_reference.propertyIdentifier\n                if _debug: ReadWritePropertyMultipleServices._debug(\"    - propertyIdentifier: %r\", propertyIdentifier)\n\n                # get the array index (optional)\n                propertyArrayIndex = prop_reference.propertyArrayIndex\n                if _debug: ReadWritePropertyMultipleServices._debug(\"    - propertyArrayIndex: %r\", propertyArrayIndex)\n\n                # check for special property identifiers\n                if propertyIdentifier in ('all', 'required', 'optional'):\n                    if not obj:\n                        # build a property access error\n                        read_result = ReadAccessResultElementChoice()\n                        read_result.propertyAccessError = ErrorType(errorClass='object', errorCode='unknownObject')\n\n                        # make an element for this error\n                        read_access_result_element = ReadAccessResultElement(\n                            propertyIdentifier=propertyIdentifier,\n                            propertyArrayIndex=propertyArrayIndex,\n                            readResult=read_result,\n                            )\n\n                        # add it to the list\n                        read_access_result_element_list.append(read_access_result_element)\n                    else:\n                        for propId, prop in obj._properties.items():\n                            if _debug: ReadWritePropertyMultipleServices._debug(\"    - checking: %r %r\", propId, prop.optional)\n\n                            if (propertyIdentifier == 'all'):\n                                pass\n                            elif (propertyIdentifier == 'required') and (prop.optional):\n                                if _debug: ReadWritePropertyMultipleServices._debug(\"    - not a required property\")\n                                continue\n                            elif (propertyIdentifier == 'optional') and (not prop.optional):\n                                if _debug: ReadWritePropertyMultipleServices._debug(\"    - not an optional property\")\n                                continue\n\n                            # read the specific property\n                            read_access_result_element = read_property_to_result_element(obj, propId, propertyArrayIndex)\n\n                            # check for undefined property\n                            if read_access_result_element.readResult.propertyAccessError \\\n                                and read_access_result_element.readResult.propertyAccessError.errorCode == 'unknownProperty':\n                                continue\n\n                            # add it to the list\n                            read_access_result_element_list.append(read_access_result_element)\n\n                else:\n                    # read the specific property\n                    read_access_result_element = read_property_to_result_element(obj, propertyIdentifier, propertyArrayIndex)\n\n                    # add it to the list\n                    read_access_result_element_list.append(read_access_result_element)\n\n            # build a read access result\n            read_access_result = ReadAccessResult(\n                objectIdentifier=objectIdentifier,\n                listOfResults=read_access_result_element_list\n                )\n            if _debug: ReadWritePropertyMultipleServices._debug(\"    - read_access_result: %r\", read_access_result)\n\n            # add it to the list\n            read_access_result_list.append(read_access_result)\n\n        # this is a ReadPropertyMultiple ack\n        if not resp:\n            resp = ReadPropertyMultipleACK(context=apdu)\n            resp.listOfReadAccessResults = read_access_result_list\n            if _debug: ReadWritePropertyMultipleServices._debug(\"    - resp: %r\", resp)\n\n        # return the result\n        self.response(resp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmatches a specific date a four - tuple with no special values with a a BACnetWeekNDay an octet string with three octets.", "response": "def match_weeknday(date, weeknday):\n    \"\"\"\n    Match a specific date, a four-tuple with no special values, with a\n    BACnetWeekNDay, an octet string with three (unsigned) octets.\n    \"\"\"\n    # unpack the date\n    year, month, day, day_of_week = date\n    last_day = calendar.monthrange(year + 1900, month)[1]\n\n    # unpack the date pattern octet string\n    weeknday_unpacked = [c for c in weeknday]\n    month_p, week_of_month_p, day_of_week_p = weeknday_unpacked\n\n    # check the month\n    if month_p == 255:\n        # any month\n        pass\n    elif month_p == 13:\n        # odd months\n        if (month % 2) == 0:\n            return False\n    elif month_p == 14:\n        # even months\n        if (month % 2) == 1:\n            return False\n    elif month != month_p:\n        # specific month\n        return False\n\n    # check the week of the month\n    if week_of_month_p == 255:\n        # any week\n        pass\n    elif week_of_month_p == 1:\n        # days numbered 1-7\n        if (day > 7):\n            return False\n    elif week_of_month_p == 2:\n        # days numbered 8-14\n        if (day < 8) or (day > 14):\n            return False\n    elif week_of_month_p == 3:\n        # days numbered 15-21\n        if (day < 15) or (day > 21):\n            return False\n    elif week_of_month_p == 4:\n        # days numbered 22-28\n        if (day < 22) or (day > 28):\n            return False\n    elif week_of_month_p == 5:\n        # days numbered 29-31\n        if (day < 29) or (day > 31):\n            return False\n    elif week_of_month_p == 6:\n        # last 7 days of this month\n        if (day < last_day - 6):\n            return False\n    elif week_of_month_p == 7:\n        # any of the 7 days prior to the last 7 days of this month\n        if (day < last_day - 13) or (day > last_day - 7):\n            return False\n    elif week_of_month_p == 8:\n        # any of the 7 days prior to the last 14 days of this month\n        if (day < last_day - 20) or (day > last_day - 14):\n            return False\n    elif week_of_month_p == 9:\n        # any of the 7 days prior to the last 21 days of this month\n        if (day < last_day - 27) or (day > last_day - 21):\n            return False\n\n    # check the day\n    if day_of_week_p == 255:\n        # any day\n        pass\n    elif day_of_week != day_of_week_p:\n        # specific day\n        return False\n\n    # all tests pass\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecode an address from a string.", "response": "def decode_address(self, addr):\n        \"\"\"Initialize the address from a string.  Lots of different forms are supported.\"\"\"\n        if _debug: Address._debug(\"decode_address %r (%s)\", addr, type(addr))\n\n        # start out assuming this is a local station\n        self.addrType = Address.localStationAddr\n        self.addrNet = None\n\n        if addr == \"*\":\n            if _debug: Address._debug(\"    - localBroadcast\")\n\n            self.addrType = Address.localBroadcastAddr\n            self.addrNet = None\n            self.addrAddr = None\n            self.addrLen = None\n\n        elif addr == \"*:*\":\n            if _debug: Address._debug(\"   - globalBroadcast\")\n\n            self.addrType = Address.globalBroadcastAddr\n            self.addrNet = None\n            self.addrAddr = None\n            self.addrLen = None\n\n        elif isinstance(addr, int):\n            if _debug: Address._debug(\"    - int\")\n            if (addr < 0) or (addr >= 256):\n                raise ValueError(\"address out of range\")\n\n            self.addrAddr = struct.pack('B', addr)\n            self.addrLen = 1\n\n        elif isinstance(addr, basestring):\n            if _debug: Address._debug(\"    - str\")\n\n            m = ip_address_mask_port_re.match(addr)\n            if m:\n                if _debug: Address._debug(\"    - IP address\")\n\n                net, addr, mask, port = m.groups()\n                if not mask: mask = '32'\n                if not port: port = '47808'\n                if _debug: Address._debug(\"    - net, addr, mask, port: %r, %r, %r, %r\", net, addr, mask, port)\n\n                if net:\n                    net = int(net)\n                    if (net >= 65535):\n                        raise ValueError(\"network out of range\")\n                    self.addrType = Address.remoteStationAddr\n                    self.addrNet = net\n\n                self.addrPort = int(port)\n                self.addrTuple = (addr, self.addrPort)\n\n                addrstr = socket.inet_aton(addr)\n                self.addrIP = struct.unpack('!L', addrstr)[0]\n                self.addrMask = (_long_mask << (32 - int(mask))) & _long_mask\n                self.addrHost = (self.addrIP & ~self.addrMask)\n                self.addrSubnet = (self.addrIP & self.addrMask)\n\n                bcast = (self.addrSubnet | ~self.addrMask)\n                self.addrBroadcastTuple = (socket.inet_ntoa(struct.pack('!L', bcast & _long_mask)), self.addrPort)\n\n                self.addrAddr = addrstr + struct.pack('!H', self.addrPort & _short_mask)\n                self.addrLen = 6\n\n            elif ethernet_re.match(addr):\n                if _debug: Address._debug(\"    - ethernet\")\n\n                self.addrAddr = xtob(addr, ':')\n                self.addrLen = len(self.addrAddr)\n\n            elif re.match(r\"^\\d+$\", addr):\n                if _debug: Address._debug(\"    - int\")\n\n                addr = int(addr)\n                if (addr > 255):\n                    raise ValueError(\"address out of range\")\n\n                self.addrAddr = struct.pack('B', addr)\n                self.addrLen = 1\n\n            elif re.match(r\"^\\d+:[*]$\", addr):\n                if _debug: Address._debug(\"    - remote broadcast\")\n\n                addr = int(addr[:-2])\n                if (addr >= 65535):\n                    raise ValueError(\"network out of range\")\n\n                self.addrType = Address.remoteBroadcastAddr\n                self.addrNet = addr\n                self.addrAddr = None\n                self.addrLen = None\n\n            elif re.match(r\"^\\d+:\\d+$\",addr):\n                if _debug: Address._debug(\"    - remote station\")\n\n                net, addr = addr.split(':')\n                net = int(net)\n                addr = int(addr)\n                if (net >= 65535):\n                    raise ValueError(\"network out of range\")\n                if (addr > 255):\n                    raise ValueError(\"address out of range\")\n\n                self.addrType = Address.remoteStationAddr\n                self.addrNet = net\n                self.addrAddr = struct.pack('B', addr)\n                self.addrLen = 1\n\n            elif re.match(r\"^0x([0-9A-Fa-f][0-9A-Fa-f])+$\",addr):\n                if _debug: Address._debug(\"    - modern hex string\")\n\n                self.addrAddr = xtob(addr[2:])\n                self.addrLen = len(self.addrAddr)\n\n            elif re.match(r\"^X'([0-9A-Fa-f][0-9A-Fa-f])+'$\",addr):\n                if _debug: Address._debug(\"    - old school hex string\")\n\n                self.addrAddr = xtob(addr[2:-1])\n                self.addrLen = len(self.addrAddr)\n\n            elif re.match(r\"^\\d+:0x([0-9A-Fa-f][0-9A-Fa-f])+$\",addr):\n                if _debug: Address._debug(\"    - remote station with modern hex string\")\n\n                net, addr = addr.split(':')\n                net = int(net)\n                if (net >= 65535):\n                    raise ValueError(\"network out of range\")\n\n                self.addrType = Address.remoteStationAddr\n                self.addrNet = net\n                self.addrAddr = xtob(addr[2:])\n                self.addrLen = len(self.addrAddr)\n\n            elif re.match(r\"^\\d+:X'([0-9A-Fa-f][0-9A-Fa-f])+'$\",addr):\n                if _debug: Address._debug(\"    - remote station with old school hex string\")\n\n                net, addr = addr.split(':')\n                net = int(net)\n                if (net >= 65535):\n                    raise ValueError(\"network out of range\")\n\n                self.addrType = Address.remoteStationAddr\n                self.addrNet = net\n                self.addrAddr = xtob(addr[2:-1])\n                self.addrLen = len(self.addrAddr)\n\n            elif netifaces and interface_re.match(addr):\n                if _debug: Address._debug(\"    - interface name with optional port\")\n\n                interface, port = interface_re.match(addr).groups()\n                if port is not None:\n                    self.addrPort = int(port)\n                else:\n                    self.addrPort = 47808\n\n                interfaces = netifaces.interfaces()\n                if interface not in interfaces:\n                    raise ValueError(\"not an interface: %s\" % (interface,))\n                if _debug: Address._debug(\"    - interfaces: %r\", interfaces)\n\n                ifaddresses = netifaces.ifaddresses(interface)\n                if netifaces.AF_INET not in ifaddresses:\n                    raise ValueError(\"interface does not support IPv4: %s\" % (interface,))\n\n                ipv4addresses = ifaddresses[netifaces.AF_INET]\n                if len(ipv4addresses) > 1:\n                    raise ValueError(\"interface supports multiple IPv4 addresses: %s\" % (interface,))\n                ifaddress = ipv4addresses[0]\n                if _debug: Address._debug(\"    - ifaddress: %r\", ifaddress)\n\n                addr = ifaddress['addr']\n                self.addrTuple = (addr, self.addrPort)\n                if _debug: Address._debug(\"    - addrTuple: %r\", self.addrTuple)\n\n                addrstr = socket.inet_aton(addr)\n                self.addrIP = struct.unpack('!L', addrstr)[0]\n\n                if 'netmask' in ifaddress:\n                    maskstr = socket.inet_aton(ifaddress['netmask'])\n                    self.addrMask = struct.unpack('!L', maskstr)[0]\n                else:\n                    self.addrMask = _long_mask\n\n                self.addrHost = (self.addrIP & ~self.addrMask)\n                self.addrSubnet = (self.addrIP & self.addrMask)\n\n                if 'broadcast' in ifaddress:\n                    self.addrBroadcastTuple = (ifaddress['broadcast'], self.addrPort)\n                else:\n                    self.addrBroadcastTuple = None\n                if _debug: Address._debug(\"    - addrBroadcastTuple: %r\", self.addrBroadcastTuple)\n\n                self.addrAddr = addrstr + struct.pack('!H', self.addrPort & _short_mask)\n                self.addrLen = 6\n\n            else:\n                raise ValueError(\"unrecognized format\")\n\n        elif isinstance(addr, tuple):\n            addr, port = addr\n            self.addrPort = int(port)\n\n            if isinstance(addr, basestring):\n                if not addr:\n                    # when ('', n) is passed it is the local host address, but that\n                    # could be more than one on a multihomed machine, the empty string\n                    # means \"any\".\n                    addrstr = '\\0\\0\\0\\0'\n                else:\n                    addrstr = socket.inet_aton(addr)\n                self.addrTuple = (addr, self.addrPort)\n\n            elif isinstance(addr, (int, long)):\n                addrstr = struct.pack('!L', addr & _long_mask)\n                self.addrTuple = (socket.inet_ntoa(addrstr), self.addrPort)\n\n            else:\n                raise TypeError(\"tuple must be (string, port) or (long, port)\")\n            if _debug: Address._debug(\"    - addrstr: %r\", addrstr)\n\n            self.addrIP = struct.unpack('!L', addrstr)[0]\n            self.addrMask = _long_mask\n            self.addrHost = None\n            self.addrSubnet = None\n            self.addrBroadcastTuple = self.addrTuple\n\n            self.addrAddr = addrstr + struct.pack('!H', self.addrPort & _short_mask)\n            self.addrLen = 6\n\n        else:\n            raise TypeError(\"integer, string or tuple required\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dict_contents(self, use_dict=None, as_class=None):\n        if _debug: _log.debug(\"dict_contents use_dict=%r as_class=%r\", use_dict, as_class)\n\n        # exception to the rule of returning a dict\n        return str(self)", "response": "Return the contents of an object as a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self, pci):\n        _PCI.update(self, pci)\n\n        # now do the BACnet PCI fields\n        self.pduExpectingReply = pci.pduExpectingReply\n        self.pduNetworkPriority = pci.pduNetworkPriority", "response": "Copy the fields from the given PCI object to the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pci_contents(self, use_dict=None, as_class=dict):\n        if _debug: PCI._debug(\"pci_contents use_dict=%r as_class=%r\", use_dict, as_class)\n\n        # make/extend the dictionary of content\n        if use_dict is None:\n            use_dict = as_class()\n\n        # call the parent class\n        _PCI.pci_contents(self, use_dict=use_dict, as_class=as_class)\n\n        # save the values\n        use_dict.__setitem__('expectingReply', self.pduExpectingReply)\n        use_dict.__setitem__('networkPriority', self.pduNetworkPriority)\n\n        # return what we built/updated\n        return use_dict", "response": "Return the contents of an object as a dict."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef SequenceOf(klass):\n    if _debug: SequenceOf._debug(\"SequenceOf %r\", klass)\n\n    global _sequence_of_map\n    global _sequence_of_classes, _array_of_classes\n\n    # if this has already been built, return the cached one\n    if klass in _sequence_of_map:\n        if _debug: SequenceOf._debug(\"    - found in cache\")\n        return _sequence_of_map[klass]\n\n    # no SequenceOf(SequenceOf(...)) allowed\n    if klass in _sequence_of_classes:\n        raise TypeError(\"nested sequences disallowed\")\n    # no SequenceOf(ArrayOf(...)) allowed\n    if klass in _array_of_classes:\n        raise TypeError(\"sequences of arrays disallowed\")\n\n    # define a generic class for lists\n    @bacpypes_debugging\n    class _SequenceOf:\n\n        subtype = None\n\n        def __init__(self, value=None):\n            if _debug: _SequenceOf._debug(\"(%r)__init__ %r (subtype=%r)\", self.__class__.__name__, value, self.subtype)\n\n            if value is None:\n                self.value = []\n            elif isinstance(value, list):\n                self.value = value\n            else:\n                raise TypeError(\"invalid constructor datatype\")\n\n        def append(self, value):\n            if issubclass(self.subtype, Atomic):\n                pass\n            elif issubclass(self.subtype, AnyAtomic) and not isinstance(value, Atomic):\n                raise TypeError(\"instance of an atomic type required\")\n            elif not isinstance(value, self.subtype):\n                raise TypeError(\"%s value required\" % (self.subtype.__name__,))\n            self.value.append(value)\n\n        def __len__(self):\n            return len(self.value)\n\n        def __getitem__(self, item):\n            return self.value[item]\n\n        def __iter__(self):\n            return iter(self.value)\n\n        def encode(self, taglist):\n            if _debug: _SequenceOf._debug(\"(%r)encode %r\", self.__class__.__name__, taglist)\n            for value in self.value:\n                if issubclass(self.subtype, (Atomic, AnyAtomic)):\n                    # a helper cooperates between the atomic value and the tag\n                    helper = self.subtype(value)\n\n                    # build a tag and encode the data into it\n                    tag = Tag()\n                    helper.encode(tag)\n\n                    # now encode the tag\n                    taglist.append(tag)\n                elif isinstance(value, self.subtype):\n                    # it must have its own encoder\n                    value.encode(taglist)\n                else:\n                    raise TypeError(\"%s must be a %s\" % (value, self.subtype.__name__))\n\n        def decode(self, taglist):\n            if _debug: _SequenceOf._debug(\"(%r)decode %r\", self.__class__.__name__, taglist)\n\n            while len(taglist) != 0:\n                tag = taglist.Peek()\n                if tag.tagClass == Tag.closingTagClass:\n                    return\n\n                if issubclass(self.subtype, (Atomic, AnyAtomic)):\n                    if _debug: _SequenceOf._debug(\"    - building helper: %r %r\", self.subtype, tag)\n                    taglist.Pop()\n\n                    # a helper cooperates between the atomic value and the tag\n                    helper = self.subtype(tag)\n\n                    # save the value\n                    self.value.append(helper.value)\n                else:\n                    if _debug: _SequenceOf._debug(\"    - building value: %r\", self.subtype)\n                    # build an element\n                    value = self.subtype()\n\n                    # let it decode itself\n                    value.decode(taglist)\n\n                    # save what was built\n                    self.value.append(value)\n\n        def debug_contents(self, indent=1, file=sys.stdout, _ids=None):\n            i = 0\n            for value in self.value:\n                if issubclass(self.subtype, (Atomic, AnyAtomic)):\n                    file.write(\"%s[%d] = %r\\n\" % (\"    \" * indent, i, value))\n                elif isinstance(value, self.subtype):\n                    file.write(\"%s[%d]\" % (\"    \" * indent, i))\n                    value.debug_contents(indent+1, file, _ids)\n                else:\n                    file.write(\"%s[%d] %s must be a %s\" % (\"    \" * indent, i, value, self.subtype.__name__))\n                i += 1\n\n        def dict_contents(self, use_dict=None, as_class=dict):\n            # return sequences as arrays\n            mapped_value = []\n\n            for value in self.value:\n                if issubclass(self.subtype, Atomic):\n                    mapped_value.append(value)              ### ambiguous\n                elif issubclass(self.subtype, AnyAtomic):\n                    mapped_value.append(value.value)        ### ambiguous\n                elif isinstance(value, self.subtype):\n                    mapped_value.append(value.dict_contents(as_class=as_class))\n\n            # return what we built\n            return mapped_value\n\n    # constrain it to a list of a specific type of item\n    setattr(_SequenceOf, 'subtype', klass)\n    _SequenceOf.__name__ = 'SequenceOf' + klass.__name__\n    if _debug: SequenceOf._debug(\"    - build this class: %r\", _SequenceOf)\n\n    # cache this type\n    _sequence_of_map[klass] = _SequenceOf\n    _sequence_of_classes[_SequenceOf] = 1\n\n    # return this new type\n    return _SequenceOf", "response": "Function to return a class that can encode and decode a list of\n    some other type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfunctioning to return a class that can encode and decode a list of some other type.", "response": "def ArrayOf(klass, fixed_length=None, prototype=None):\n    \"\"\"Function to return a class that can encode and decode a list of\n    some other type.\"\"\"\n    global _array_of_map\n    global _array_of_classes, _sequence_of_classes\n\n    # check the parameters for consistency\n    if issubclass(klass, Atomic):\n        if prototype is None:\n            pass\n        elif not klass.is_valid(prototype):\n            raise ValueError(\"prototype %r not valid for %s\" % (prototype, klass.__name__))\n    else:\n        if prototype is None:\n            ### TODO This should be an error, a prototype should always be\n            ### required for non-atomic types, even if it's only klass()\n            ### for a default object which will be deep copied\n            pass\n        elif not isinstance(prototype, klass):\n            raise ValueError(\"prototype %r not valid for %s\" % (prototype, klass.__name__))\n\n    # build a signature of the parameters\n    array_signature = (klass, fixed_length, prototype)\n\n    # if this has already been built, return the cached one\n    if array_signature in _array_of_map:\n        return _array_of_map[array_signature]\n\n    # no ArrayOf(ArrayOf(...)) allowed\n    if klass in _array_of_classes:\n        raise TypeError(\"nested arrays disallowed\")\n    # no ArrayOf(SequenceOf(...)) allowed\n    if klass in _sequence_of_classes:\n        raise TypeError(\"arrays of SequenceOf disallowed\")\n\n    # define a generic class for arrays\n    @bacpypes_debugging\n    class ArrayOf(Array):\n\n        subtype = None\n        fixed_length = None\n        prototype = None\n\n        def __init__(self, value=None):\n            if value is None:\n                self.value = [0]\n                if self.fixed_length is not None:\n                    self.fix_length(self.fixed_length)\n\n            elif isinstance(value, list):\n                if (self.fixed_length is not None) and (len(value) != self.fixed_length):\n                    raise ValueError(\"invalid array length\")\n\n                self.value = [len(value)]\n                self.value.extend(value)\n            else:\n                raise TypeError(\"invalid constructor datatype\")\n\n        def fix_length(self, new_length):\n            if len(self.value) > new_length + 1:\n                # trim off the excess\n                del self.value[new_length + 1:]\n            elif len(self.value) < new_length + 1:\n                # how many do we need\n                element_count = new_length - len(self.value) + 1\n\n                # extend or append\n                if issubclass(self.subtype, Atomic):\n                    if self.prototype is None:\n                        extend_value = self.subtype().value\n                    else:\n                        extend_value = self.prototype\n                    self.value.extend( [extend_value] * element_count )\n                else:\n                    for i in range(element_count):\n                        if self.prototype is None:\n                            append_value = self.subtype()\n                        else:\n                            append_value = _deepcopy(self.prototype)\n                        self.value.append(append_value)\n\n            self.value[0] = new_length\n\n        def append(self, value):\n            if self.fixed_length is not None:\n                raise TypeError(\"fixed length array\")\n\n            if issubclass(self.subtype, Atomic):\n                pass\n            elif issubclass(self.subtype, AnyAtomic) and not isinstance(value, Atomic):\n                raise TypeError(\"instance of an atomic type required\")\n            elif not isinstance(value, self.subtype):\n                raise TypeError(\"%s value required\" % (self.subtype.__name__,))\n\n            self.value.append(value)\n            self.value[0] = len(self.value) - 1\n\n        def __len__(self):\n            return self.value[0]\n\n        def __getitem__(self, item):\n            # no wrapping index\n            if (item < 0) or (item > self.value[0]):\n                raise IndexError(\"index out of range\")\n\n            return self.value[item]\n\n        def __setitem__(self, item, value):\n            # no wrapping index\n            if (item < 0) or (item > self.value[0]):\n                raise IndexError(\"index out of range\")\n\n            # special length handling for index 0\n            if item == 0:\n                if (self.fixed_length is not None):\n                    if (value != self.value[0]):\n                        raise TypeError(\"fixed length array\")\n                    return\n\n                self.fix_length(value)\n            else:\n                self.value[item] = value\n\n        def __delitem__(self, item):\n            if self.fixed_length is not None:\n                raise TypeError(\"fixed length array\")\n\n            # no wrapping index\n            if (item < 1) or (item > self.value[0]):\n                raise IndexError(\"index out of range\")\n\n            # delete the item and update the length\n            del self.value[item]\n            self.value[0] -= 1\n\n        def __iter__(self):\n            return iter(self.value[1:])\n\n        def index(self, value):\n            # only search through values\n            for i in range(1, self.value[0] + 1):\n                if value == self.value[i]:\n                    return i\n\n            # not found\n            raise ValueError(\"%r not in array\" % (value,))\n\n        def remove(self, item):\n            if self.fixed_length is not None:\n                raise TypeError(\"fixed length array\")\n\n            # find the index of the item and delete it\n            indx = self.index(item)\n            self.__delitem__(indx)\n\n        def encode(self, taglist):\n            if _debug: ArrayOf._debug(\"(%r)encode %r\", self.__class__.__name__, taglist)\n\n            for value in self.value[1:]:\n                if issubclass(self.subtype, (Atomic, AnyAtomic)):\n                    # a helper cooperates between the atomic value and the tag\n                    helper = self.subtype(value)\n\n                    # build a tag and encode the data into it\n                    tag = Tag()\n                    helper.encode(tag)\n\n                    # now encode the tag\n                    taglist.append(tag)\n                elif isinstance(value, self.subtype):\n                    # it must have its own encoder\n                    value.encode(taglist)\n                else:\n                    raise TypeError(\"%s must be a %s\" % (value, self.subtype.__name__))\n\n        def decode(self, taglist):\n            if _debug: ArrayOf._debug(\"(%r)decode %r\", self.__class__.__name__, taglist)\n\n            # start with an empty array\n            new_value = []\n\n            while len(taglist) != 0:\n                tag = taglist.Peek()\n                if tag.tagClass == Tag.closingTagClass:\n                    break\n\n                if issubclass(self.subtype, (Atomic, AnyAtomic)):\n                    if _debug: ArrayOf._debug(\"    - building helper: %r %r\", self.subtype, tag)\n                    taglist.Pop()\n\n                    # a helper cooperates between the atomic value and the tag\n                    helper = self.subtype(tag)\n\n                    # save the value\n                    new_value.append(helper.value)\n                else:\n                    if _debug: ArrayOf._debug(\"    - building value: %r\", self.subtype)\n                    # build an element\n                    value = self.subtype()\n\n                    # let it decode itself\n                    value.decode(taglist)\n\n                    # save what was built\n                    new_value.append(value)\n\n            # check the length\n            if self.fixed_length is not None:\n                if self.fixed_length != len(new_value):\n                    raise ValueError(\"invalid array length\")\n\n            # update the length\n            self.value = [len(new_value)] + new_value\n\n        def encode_item(self, item, taglist):\n            if _debug: ArrayOf._debug(\"(%r)encode_item %r %r\", self.__class__.__name__, item, taglist)\n\n            if item == 0:\n                # a helper cooperates between the atomic value and the tag\n                helper = Unsigned(self.value[0])\n\n                # build a tag and encode the data into it\n                tag = Tag()\n                helper.encode(tag)\n\n                # now encode the tag\n                taglist.append(tag)\n            else:\n                value = self.value[item]\n\n                if issubclass(self.subtype, (Atomic, AnyAtomic)):\n                    # a helper cooperates between the atomic value and the tag\n                    helper = self.subtype(self.value[item])\n\n                    # build a tag and encode the data into it\n                    tag = Tag()\n                    helper.encode(tag)\n\n                    # now encode the tag\n                    taglist.append(tag)\n                elif isinstance(value, self.subtype):\n                    # it must have its own encoder\n                    value.encode(taglist)\n                else:\n                    raise TypeError(\"%s must be a %s\" % (value, self.subtype.__name__))\n\n        def decode_item(self, item, taglist):\n            if _debug: ArrayOf._debug(\"(%r)decode_item %r %r\", self.__class__.__name__, item, taglist)\n\n            if item == 0:\n                # a helper cooperates between the atomic value and the tag\n                helper = Unsigned(taglist.Pop())\n\n                # save the value\n                self.value = helper.value\n            elif issubclass(self.subtype, (Atomic, AnyAtomic)):\n                if _debug: ArrayOf._debug(\"    - building helper: %r\", self.subtype)\n\n                # a helper cooperates between the atomic value and the tag\n                helper = self.subtype(taglist.Pop())\n\n                # save the value\n                self.value = helper.value\n            else:\n                if _debug: ArrayOf._debug(\"    - building value: %r\", self.subtype)\n                # build an element\n                value = self.subtype()\n\n                # let it decode itself\n                value.decode(taglist)\n\n                # save what was built\n                self.value = value\n\n        def debug_contents(self, indent=1, file=sys.stdout, _ids=None):\n            try:\n                value_list = enumerate(self.value)\n            except TypeError:\n                file.write(\"%s(non-sequence) %r\\n\" % (\"    \" * indent, self.value))\n                return\n\n            for i, value in value_list:\n                if i == 0:\n                    file.write(\"%slength = %d\\n\" % (\"    \" * indent, value))\n                elif issubclass(self.subtype, (Atomic, AnyAtomic)):\n                    file.write(\"%s[%d] = %r\\n\" % (\"    \" * indent, i, value))\n                elif isinstance(value, self.subtype):\n                    file.write(\"%s[%d]\\n\" % (\"    \" * indent, i))\n                    value.debug_contents(indent+1, file, _ids)\n                else:\n                    file.write(\"%s%s must be a %s\" % (\"    \" * indent, value, self.subtype.__name__))\n\n        def dict_contents(self, use_dict=None, as_class=dict):\n            # return arrays as arrays\n            mapped_value = []\n\n            for value in self.value:\n                if issubclass(self.subtype, Atomic):\n                    mapped_value.append(value)              ### ambiguous\n                elif issubclass(self.subtype, AnyAtomic):\n                    mapped_value.append(value.value)        ### ambiguous\n                elif isinstance(value, self.subtype):\n                    mapped_value.append(value.dict_contents(as_class=as_class))\n\n            # return what we built\n            return mapped_value\n\n    # constrain it to a list of a specific type of item\n    setattr(ArrayOf, 'subtype', klass)\n    setattr(ArrayOf, 'fixed_length', fixed_length)\n    setattr(ArrayOf, 'prototype', prototype)\n\n    # update the name\n    ArrayOf.__name__ = 'ArrayOf' + klass.__name__\n\n    # cache this type\n    _array_of_map[array_signature] = ArrayOf\n    _array_of_classes[ArrayOf] = 1\n\n    # return this new type\n    return ArrayOf"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_actor(self, actor):\n        if _debug: UDPDirector._debug(\"add_actor %r\", actor)\n\n        self.peers[actor.peer] = actor\n\n        # tell the ASE there is a new client\n        if self.serviceElement:\n            self.sap_request(add_actor=actor)", "response": "Add an actor when a new one is connected."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a PDU from the queue and send it to the peer.", "response": "def handle_write(self):\n        \"\"\"get a PDU from the queue and send it.\"\"\"\n        if _debug: UDPDirector._debug(\"handle_write\")\n\n        try:\n            pdu = self.request.get()\n\n            sent = self.socket.sendto(pdu.pduData, pdu.pduDestination)\n            if _debug: UDPDirector._debug(\"    - sent %d octets to %s\", sent, pdu.pduDestination)\n\n        except socket.error, err:\n            if _debug: UDPDirector._debug(\"    - socket error: %s\", err)\n\n            # get the peer\n            peer = self.peers.get(pdu.pduDestination, None)\n            if peer:\n                # let the actor handle the error\n                peer.handle_error(err)\n            else:\n                # let the director handle the error\n                self.handle_error(err)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef indication(self, pdu):\n        if _debug: UDPDirector._debug(\"indication %r\", pdu)\n\n        # get the destination\n        addr = pdu.pduDestination\n\n        # get the peer\n        peer = self.peers.get(addr, None)\n        if not peer:\n            peer = self.actorClass(self, addr)\n\n        # send the message\n        peer.indication(pdu)", "response": "This function is called when a message is received from the client."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef subscriptions(self):\n        if _debug: ChangeOfValueServices._debug(\"subscriptions\")\n\n        subscription_list = []\n\n        # loop through the object and detection list\n        for obj, cov_detection in self.cov_detections.items():\n            for cov in cov_detection.cov_subscriptions:\n                subscription_list.append(cov)\n\n        return subscription_list", "response": "Generator for the active subscriptions."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbind a list of clients and servers together top down.", "response": "def bind(*args):\n    \"\"\"bind a list of clients and servers together, top down.\"\"\"\n    if _debug: bind._debug(\"bind %r\", args)\n\n    # generic bind is pairs of names\n    if not args:\n        # find unbound clients and bind them\n        for cid, client in client_map.items():\n            # skip those that are already bound\n            if client.clientPeer:\n                continue\n\n            if not cid in server_map:\n                raise RuntimeError(\"unmatched server {!r}\".format(cid))\n            server = server_map[cid]\n\n            if server.serverPeer:\n                raise RuntimeError(\"server already bound %r\".format(cid))\n\n            bind(client, server)\n\n        # see if there are any unbound servers\n        for sid, server in server_map.items():\n            if server.serverPeer:\n                continue\n\n            if not sid in client_map:\n                raise RuntimeError(\"unmatched client {!r}\".format(sid))\n            else:\n                raise RuntimeError(\"mistery unbound server {!r}\".format(sid))\n\n        # find unbound application service elements and bind them\n        for eid, element in element_map.items():\n            # skip those that are already bound\n            if element.elementService:\n                continue\n\n            if not eid in service_map:\n                raise RuntimeError(\"unmatched element {!r}\".format(cid))\n            service = service_map[eid]\n\n            if server.serverPeer:\n                raise RuntimeError(\"service already bound {!r}\".format(cid))\n\n            bind(element, service)\n\n        # see if there are any unbound services\n        for sid, service in service_map.items():\n            if service.serviceElement:\n                continue\n\n            if not sid in element_map:\n                raise RuntimeError(\"unmatched service {!r}\".format(sid))\n            else:\n                raise RuntimeError(\"mistery unbound service {!r}\".format(sid))\n\n    # go through the argument pairs\n    for i in xrange(len(args)-1):\n        client = args[i]\n        if _debug: bind._debug(\"    - client: %r\", client)\n        server = args[i+1]\n        if _debug: bind._debug(\"    - server: %r\", server)\n\n        # make sure we're binding clients and servers\n        if isinstance(client, Client) and isinstance(server, Server):\n            client.clientPeer = server\n            server.serverPeer = client\n\n        # we could be binding application clients and servers\n        elif isinstance(client, ApplicationServiceElement) and isinstance(server, ServiceAccessPoint):\n            client.elementService = server\n            server.serviceElement = client\n\n        # error\n        else:\n            raise TypeError(\"bind() requires a client and server\")\n\n        if _debug: bind._debug(\"    - bound\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nencodes the contents of the NPCI into the PDU.", "response": "def encode(self, pdu):\n        \"\"\"encode the contents of the NPCI into the PDU.\"\"\"\n        if _debug: NPCI._debug(\"encode %s\", repr(pdu))\n\n        PCI.update(pdu, self)\n\n        # only version 1 messages supported\n        pdu.put(self.npduVersion)\n\n        # build the flags\n        if self.npduNetMessage is not None:\n            netLayerMessage = 0x80\n        else:\n            netLayerMessage = 0x00\n\n        # map the destination address\n        dnetPresent = 0x00\n        if self.npduDADR is not None:\n            dnetPresent = 0x20\n\n        # map the source address\n        snetPresent = 0x00\n        if self.npduSADR is not None:\n            snetPresent = 0x08\n\n        # encode the control octet\n        control = netLayerMessage | dnetPresent | snetPresent\n        if self.pduExpectingReply:\n            control |= 0x04\n        control |= (self.pduNetworkPriority & 0x03)\n        self.npduControl = control\n        pdu.put(control)\n\n        # make sure expecting reply and priority get passed down\n        pdu.pduExpectingReply = self.pduExpectingReply\n        pdu.pduNetworkPriority = self.pduNetworkPriority\n\n        # encode the destination address\n        if dnetPresent:\n            if self.npduDADR.addrType == Address.remoteStationAddr:\n                pdu.put_short(self.npduDADR.addrNet)\n                pdu.put(self.npduDADR.addrLen)\n                pdu.put_data(self.npduDADR.addrAddr)\n            elif self.npduDADR.addrType == Address.remoteBroadcastAddr:\n                pdu.put_short(self.npduDADR.addrNet)\n                pdu.put(0)\n            elif self.npduDADR.addrType == Address.globalBroadcastAddr:\n                pdu.put_short(0xFFFF)\n                pdu.put(0)\n\n        # encode the source address\n        if snetPresent:\n            pdu.put_short(self.npduSADR.addrNet)\n            pdu.put(self.npduSADR.addrLen)\n            pdu.put_data(self.npduSADR.addrAddr)\n\n        # put the hop count\n        if dnetPresent:\n            pdu.put(self.npduHopCount)\n\n        # put the network layer message type (if present)\n        if netLayerMessage:\n            pdu.put(self.npduNetMessage)\n            # put the vendor ID\n            if (self.npduNetMessage >= 0x80) and (self.npduNetMessage <= 0xFF):\n                pdu.put_short(self.npduVendorID)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndecodes the contents of the PDU and put them into the NPDU.", "response": "def decode(self, pdu):\n        \"\"\"decode the contents of the PDU and put them into the NPDU.\"\"\"\n        if _debug: NPCI._debug(\"decode %s\", str(pdu))\n\n        PCI.update(self, pdu)\n\n        # check the length\n        if len(pdu.pduData) < 2:\n            raise DecodingError(\"invalid length\")\n\n        # only version 1 messages supported\n        self.npduVersion = pdu.get()\n        if (self.npduVersion != 0x01):\n            raise DecodingError(\"only version 1 messages supported\")\n\n        # decode the control octet\n        self.npduControl = control = pdu.get()\n        netLayerMessage = control & 0x80\n        dnetPresent = control & 0x20\n        snetPresent = control & 0x08\n        self.pduExpectingReply = (control & 0x04) != 0\n        self.pduNetworkPriority = control & 0x03\n\n        # extract the destination address\n        if dnetPresent:\n            dnet = pdu.get_short()\n            dlen = pdu.get()\n            dadr = pdu.get_data(dlen)\n\n            if dnet == 0xFFFF:\n                self.npduDADR = GlobalBroadcast()\n            elif dlen == 0:\n                self.npduDADR = RemoteBroadcast(dnet)\n            else:\n                self.npduDADR = RemoteStation(dnet, dadr)\n\n        # extract the source address\n        if snetPresent:\n            snet = pdu.get_short()\n            slen = pdu.get()\n            sadr = pdu.get_data(slen)\n\n            if snet == 0xFFFF:\n                raise DecodingError(\"SADR can't be a global broadcast\")\n            elif slen == 0:\n                raise DecodingError(\"SADR can't be a remote broadcast\")\n\n            self.npduSADR = RemoteStation(snet, sadr)\n\n        # extract the hop count\n        if dnetPresent:\n            self.npduHopCount = pdu.get()\n\n        # extract the network layer message type (if present)\n        if netLayerMessage:\n            self.npduNetMessage = pdu.get()\n            if (self.npduNetMessage >= 0x80) and (self.npduNetMessage <= 0xFF):\n                # extract the vendor ID\n                self.npduVendorID = pdu.get_short()\n        else:\n            # application layer message\n            self.npduNetMessage = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the contents of an object as a dict.", "response": "def npci_contents(self, use_dict=None, as_class=dict):\n        \"\"\"Return the contents of an object as a dict.\"\"\"\n        if _debug: NPCI._debug(\"npci_contents use_dict=%r as_class=%r\", use_dict, as_class)\n\n        # make/extend the dictionary of content\n        if use_dict is None:\n            if _debug: NPCI._debug(\"    - new use_dict\")\n            use_dict = as_class()\n\n        # version and control are simple\n        use_dict.__setitem__('version', self.npduVersion)\n        use_dict.__setitem__('control', self.npduControl)\n\n        # dnet/dlen/dadr\n        if self.npduDADR is not None:\n            if self.npduDADR.addrType == Address.remoteStationAddr:\n                use_dict.__setitem__('dnet', self.npduDADR.addrNet)\n                use_dict.__setitem__('dlen', self.npduDADR.addrLen)\n                use_dict.__setitem__('dadr', btox(self.npduDADR.addrAddr or ''))\n            elif self.npduDADR.addrType == Address.remoteBroadcastAddr:\n                use_dict.__setitem__('dnet', self.npduDADR.addrNet)\n                use_dict.__setitem__('dlen', 0)\n                use_dict.__setitem__('dadr', '')\n            elif self.npduDADR.addrType == Address.globalBroadcastAddr:\n                use_dict.__setitem__('dnet', 0xFFFF)\n                use_dict.__setitem__('dlen', 0)\n                use_dict.__setitem__('dadr', '')\n\n        # snet/slen/sadr\n        if self.npduSADR is not None:\n            use_dict.__setitem__('snet', self.npduSADR.addrNet)\n            use_dict.__setitem__('slen', self.npduSADR.addrLen)\n            use_dict.__setitem__('sadr', btox(self.npduSADR.addrAddr or ''))\n\n        # hop count\n        if self.npduHopCount is not None:\n            use_dict.__setitem__('hop_count', self.npduHopCount)\n\n        # network layer message name decoded\n        if self.npduNetMessage is not None:\n            use_dict.__setitem__('net_message', self.npduNetMessage)\n        if self.npduVendorID is not None:\n            use_dict.__setitem__('vendor_id', self.npduVendorID)\n\n        # return what we built/updated\n        return use_dict"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dict_contents(self, use_dict=None, as_class=dict):\n        if _debug: NPDU._debug(\"dict_contents use_dict=%r as_class=%r\", use_dict, as_class)\n\n        # make/extend the dictionary of content\n        if use_dict is None:\n            use_dict = as_class()\n\n        # call the parent classes\n        self.npci_contents(use_dict=use_dict, as_class=as_class)\n        self.npdu_contents(use_dict=use_dict, as_class=as_class)\n\n        # return what we built/updated\n        return use_dict", "response": "Return the contents of an object as a dict."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntrap for TCPClient errors otherwise continue.", "response": "def handle_error(self, error=None):\n        \"\"\"Trap for TCPClient errors, otherwise continue.\"\"\"\n        if _debug: TCPClient._debug(\"handle_error %r\", error)\n\n        # core does not take parameters\n        asyncore.dispatcher.handle_error(self)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef indication(self, pdu):\n        if _debug: TCPClient._debug(\"indication %r\", pdu)\n\n        self.request += pdu.pduData", "response": "This function is called when a request is received."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntrap for TCPClient errors otherwise continue.", "response": "def handle_error(self, error=None):\n        \"\"\"Trap for TCPClient errors, otherwise continue.\"\"\"\n        if _debug: TCPClientActor._debug(\"handle_error %r\", error)\n\n        # pass along to the director\n        if error is not None:\n            self.director.actor_error(self, error)\n        else:\n            TCPClient.handle_error(self)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd an actor when a new one is connected.", "response": "def add_actor(self, actor):\n        \"\"\"Add an actor when a new one is connected.\"\"\"\n        if _debug: TCPClientDirector._debug(\"add_actor %r\", actor)\n\n        self.clients[actor.peer] = actor\n\n        # tell the ASE there is a new client\n        if self.serviceElement:\n            self.sap_request(add_actor=actor)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving an actor from the list of active ones.", "response": "def del_actor(self, actor):\n        \"\"\"Remove an actor when the socket is closed.\"\"\"\n        if _debug: TCPClientDirector._debug(\"del_actor %r\", actor)\n\n        del self.clients[actor.peer]\n\n        # tell the ASE the client has gone away\n        if self.serviceElement:\n            self.sap_request(del_actor=actor)\n\n        # see if it should be reconnected\n        if actor.peer in self.reconnect:\n            connect_task = FunctionTask(self.connect, actor.peer)\n            connect_task.install_task(_time() + self.reconnect[actor.peer])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndirect this PDU to the appropriate server create a connection if one hasn t already been created.", "response": "def indication(self, pdu):\n        \"\"\"Direct this PDU to the appropriate server, create a\n        connection if one hasn't already been created.\"\"\"\n        if _debug: TCPClientDirector._debug(\"indication %r\", pdu)\n\n        # get the destination\n        addr = pdu.pduDestination\n\n        # get the client\n        client = self.clients.get(addr, None)\n        if not client:\n            client = self.actorClass(self, addr)\n\n        # send the message\n        client.indication(pdu)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle_error(self, error=None):\n        if _debug: TCPServer._debug(\"handle_error %r\", error)\n\n        # core does not take parameters\n        asyncore.dispatcher.handle_error(self)", "response": "Trap for TCPServer errors otherwise continue."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrequesting are queued for delivery.", "response": "def indication(self, pdu):\n        \"\"\"Requests are queued for delivery.\"\"\"\n        if _debug: TCPServer._debug(\"indication %r\", pdu)\n\n        self.request += pdu.pduData"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle_error(self, error=None):\n        if _debug: TCPServerActor._debug(\"handle_error %r\", error)\n\n        # pass along to the director\n        if error is not None:\n            self.director.actor_error(self, error)\n        else:\n            TCPServer.handle_error(self)", "response": "Trap for TCPServer errors otherwise continue."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndirecting this PDU to the appropriate server.", "response": "def indication(self, pdu):\n        \"\"\"Direct this PDU to the appropriate server.\"\"\"\n        if _debug: TCPServerDirector._debug(\"indication %r\", pdu)\n\n        # get the destination\n        addr = pdu.pduDestination\n\n        # get the server\n        server = self.servers.get(addr, None)\n        if not server:\n            raise RuntimeError(\"not a connected server\")\n\n        # pass the indication to the actor\n        server.indication(pdu)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef indication(self, pdu):\n        if _debug: StreamToPacket._debug(\"indication %r\", pdu)\n\n        # hack it up into chunks\n        for packet in self.packetize(pdu, self.downstreamBuffer):\n            self.request(packet)", "response": "This function is called when a message is going downstream. It is called by the client to process the message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npass a function to be called when IO is complete.", "response": "def add_callback(self, fn, *args, **kwargs):\n        \"\"\"Pass a function to be called when IO is complete.\"\"\"\n        if _debug: IOCB._debug(\"add_callback(%d) %r %r %r\", self.ioID, fn, args, kwargs)\n\n        # store it\n        self.ioCallback.append((fn, args, kwargs))\n\n        # already complete?\n        if self.ioComplete.isSet():\n            self.trigger()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwaits for the completion event to be set.", "response": "def wait(self, *args, **kwargs):\n        \"\"\"Wait for the completion event to be set.\"\"\"\n        if _debug: IOCB._debug(\"wait(%d) %r %r\", self.ioID, args, kwargs)\n\n        # waiting from a non-daemon thread could be trouble\n        return self.ioComplete.wait(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef trigger(self):\n        if _debug: IOCB._debug(\"trigger(%d)\", self.ioID)\n\n        # if it's queued, remove it from its queue\n        if self.ioQueue:\n            if _debug: IOCB._debug(\"    - dequeue\")\n            self.ioQueue.remove(self)\n\n        # if there's a timer, cancel it\n        if self.ioTimeout:\n            if _debug: IOCB._debug(\"    - cancel timeout\")\n            self.ioTimeout.suspend_task()\n\n        # set the completion event\n        self.ioComplete.set()\n        if _debug: IOCB._debug(\"    - complete event set\")\n\n        # make the callback(s)\n        for fn, args, kwargs in self.ioCallback:\n            if _debug: IOCB._debug(\"    - callback fn: %r %r %r\", fn, args, kwargs)\n            fn(self, *args, **kwargs)", "response": "Set the completion event and make the callback ( s )."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall when a transaction has been completed.", "response": "def complete(self, msg):\n        \"\"\"Called to complete a transaction, usually when ProcessIO has\n        shipped the IOCB off to some other thread or function.\"\"\"\n        if _debug: IOCB._debug(\"complete(%d) %r\", self.ioID, msg)\n\n        if self.ioController:\n            # pass to controller\n            self.ioController.complete_io(self, msg)\n        else:\n            # just fill in the data\n            self.ioState = COMPLETED\n            self.ioResponse = msg\n            self.trigger()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalls by a client to abort a transaction.", "response": "def abort(self, err):\n        \"\"\"Called by a client to abort a transaction.\"\"\"\n        if _debug: IOCB._debug(\"abort(%d) %r\", self.ioID, err)\n\n        if self.ioController:\n            # pass to controller\n            self.ioController.abort_io(self, err)\n        elif self.ioState < COMPLETED:\n            # just fill in the data\n            self.ioState = ABORTED\n            self.ioError = err\n            self.trigger()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_timeout(self, delay, err=TimeoutError):\n        if _debug: IOCB._debug(\"set_timeout(%d) %r err=%r\", self.ioID, delay, err)\n\n        # if one has already been created, cancel it\n        if self.ioTimeout:\n            self.ioTimeout.suspend_task()\n        else:\n            self.ioTimeout = FunctionTask(self.abort, err)\n\n        # (re)schedule it\n        self.ioTimeout.install_task(delay=delay)", "response": "Set a timeout for the current IOCB."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef abort_io(self, iocb, err):\n        if _debug: IOChainMixIn._debug(\"abort_io %r %r\", iocb, err)\n\n        # make sure we're being notified of an abort request from\n        # the iocb we are chained from\n        if iocb is not self.ioChain:\n            raise RuntimeError(\"broken chain\")\n\n        # call my own Abort(), which may forward it to a controller or\n        # be overridden by IOGroup\n        self.abort(err)", "response": "Forward the abort downstream."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decode(self):\n        if _debug: IOChainMixIn._debug(\"decode\")\n\n        # refer to the chained iocb\n        iocb = self.ioChain\n\n        # if this has completed successfully, pass it up\n        if self.ioState == COMPLETED:\n            if _debug: IOChainMixIn._debug(\"    - completed: %r\", self.ioResponse)\n\n            # change the state and transform the content\n            iocb.ioState = COMPLETED\n            iocb.ioResponse = self.ioResponse\n\n        # if this aborted, pass that up too\n        elif self.ioState == ABORTED:\n            if _debug: IOChainMixIn._debug(\"    - aborted: %r\", self.ioError)\n\n            # change the state\n            iocb.ioState = ABORTED\n            iocb.ioError = self.ioError\n\n        else:\n            raise RuntimeError(\"invalid state: %d\" % (self.ioState,))", "response": "Hook to transform the response called when this IOCB is\n        completed."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(self, iocb):\n        if _debug: IOGroup._debug(\"add %r\", iocb)\n\n        # add this to our members\n        self.ioMembers.append(iocb)\n\n        # assume all of our members have not completed yet\n        self.ioState = PENDING\n        self.ioComplete.clear()\n\n        # when this completes, call back to the group.  If this\n        # has already completed, it will trigger\n        iocb.add_callback(self.group_callback)", "response": "Add an IOCB to the group."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef group_callback(self, iocb):\n        if _debug: IOGroup._debug(\"group_callback %r\", iocb)\n\n        # check all the members\n        for iocb in self.ioMembers:\n            if not iocb.ioComplete.isSet():\n                if _debug: IOGroup._debug(\"    - waiting for child: %r\", iocb)\n                break\n        else:\n            if _debug: IOGroup._debug(\"    - all children complete\")\n            # everything complete\n            self.ioState = COMPLETED\n            self.trigger()", "response": "Callback when a child iocb completes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef abort(self, err):\n        if _debug: IOGroup._debug(\"abort %r\", err)\n\n        # change the state to reflect that it was killed\n        self.ioState = ABORTED\n        self.ioError = err\n\n        # abort all the members\n        for iocb in self.ioMembers:\n            iocb.abort(err)\n\n        # notify the client\n        self.trigger()", "response": "Called by a client to abort all of the pending member transactions."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding an IOCB to the queue.", "response": "def put(self, iocb):\n        \"\"\"Add an IOCB to a queue.  This is usually called by the function\n        that filters requests and passes them out to the correct processing\n        thread.\"\"\"\n        if _debug: IOQueue._debug(\"put %r\", iocb)\n\n        # requests should be pending before being queued\n        if iocb.ioState != PENDING:\n            raise RuntimeError(\"invalid state transition\")\n\n        # save that it might have been empty\n        wasempty = not self.notempty.isSet()\n\n        # add the request to the end of the list of iocb's at same priority\n        priority = iocb.ioPriority\n        item = (priority, iocb)\n        self.queue.insert(bisect_left(self.queue, (priority+1,)), item)\n\n        # point the iocb back to this queue\n        iocb.ioQueue = self\n\n        # set the event, queue is no longer empty\n        self.notempty.set()\n\n        return wasempty"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, block=1, delay=None):\n        if _debug: IOQueue._debug(\"get block=%r delay=%r\", block, delay)\n\n        # if the queue is empty and we do not block return None\n        if not block and not self.notempty.isSet():\n            if _debug: IOQueue._debug(\"    - not blocking and empty\")\n            return None\n\n        # wait for something to be in the queue\n        if delay:\n            self.notempty.wait(delay)\n            if not self.notempty.isSet():\n                return None\n        else:\n            self.notempty.wait()\n\n        # extract the first element\n        priority, iocb = self.queue[0]\n        del self.queue[0]\n        iocb.ioQueue = None\n\n        # if the queue is empty, clear the event\n        qlen = len(self.queue)\n        if not qlen:\n            self.notempty.clear()\n\n        # return the request\n        return iocb", "response": "Get a request from a queue optionally block until a request is available."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves a control block from the queue.", "response": "def remove(self, iocb):\n        \"\"\"Remove a control block from the queue, called if the request\n        is canceled/aborted.\"\"\"\n        if _debug: IOQueue._debug(\"remove %r\", iocb)\n\n        # remove the request from the queue\n        for i, item in enumerate(self.queue):\n            if iocb is item[1]:\n                if _debug: IOQueue._debug(\"    - found at %d\", i)\n                del self.queue[i]\n\n                # if the queue is empty, clear the event\n                qlen = len(self.queue)\n                if not qlen:\n                    self.notempty.clear()\n\n                # record the new length\n                # self.queuesize.Record( qlen, _time() )\n                break\n        else:\n            if _debug: IOQueue._debug(\"    - not found\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef abort(self, err):\n        if _debug: IOQueue._debug(\"abort %r\", err)\n\n        # send aborts to all of the members\n        try:\n            for iocb in self.queue:\n                iocb.ioQueue = None\n                iocb.abort(err)\n\n            # flush the queue\n            self.queue = []\n\n            # the queue is now empty, clear the event\n            self.notempty.clear()\n        except ValueError:\n            pass", "response": "Abort all of the control blocks in the queue."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalling by a client to start processing a request.", "response": "def request_io(self, iocb):\n        \"\"\"Called by a client to start processing a request.\"\"\"\n        if _debug: IOController._debug(\"request_io %r\", iocb)\n\n        # check that the parameter is an IOCB\n        if not isinstance(iocb, IOCB):\n            raise TypeError(\"IOCB expected\")\n\n        # bind the iocb to this controller\n        iocb.ioController = self\n\n        try:\n            # hopefully there won't be an error\n            err = None\n\n            # change the state\n            iocb.ioState = PENDING\n\n            # let derived class figure out how to process this\n            self.process_io(iocb)\n        except:\n            # extract the error\n            err = sys.exc_info()[1]\n\n        # if there was an error, abort the request\n        if err:\n            self.abort_io(iocb, err)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef active_io(self, iocb):\n        if _debug: IOController._debug(\"active_io %r\", iocb)\n\n        # requests should be idle or pending before coming active\n        if (iocb.ioState != IDLE) and (iocb.ioState != PENDING):\n            raise RuntimeError(\"invalid state transition (currently %d)\" % (iocb.ioState,))\n\n        # change the state\n        iocb.ioState = ACTIVE", "response": "Called by a handler to notify the controller that a request is being processed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef complete_io(self, iocb, msg):\n        if _debug: IOController._debug(\"complete_io %r %r\", iocb, msg)\n\n        # if it completed, leave it alone\n        if iocb.ioState == COMPLETED:\n            pass\n\n        # if it already aborted, leave it alone\n        elif iocb.ioState == ABORTED:\n            pass\n\n        else:\n            # change the state\n            iocb.ioState = COMPLETED\n            iocb.ioResponse = msg\n\n            # notify the client\n            iocb.trigger()", "response": "Called by the handler to return data to the client."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef abort_io(self, iocb, err):\n        if _debug: IOController._debug(\"abort_io %r %r\", iocb, err)\n\n        # if it completed, leave it alone\n        if iocb.ioState == COMPLETED:\n            pass\n\n        # if it already aborted, leave it alone\n        elif iocb.ioState == ABORTED:\n            pass\n\n        else:\n            # change the state\n            iocb.ioState = ABORTED\n            iocb.ioError = err\n\n            # notify the client\n            iocb.trigger()", "response": "Called by a handler or a client to abort a transaction."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nabort all pending requests.", "response": "def abort(self, err):\n        \"\"\"Abort all pending requests.\"\"\"\n        if _debug: IOQController._debug(\"abort %r\", err)\n\n        if (self.state == CTRL_IDLE):\n            if _debug: IOQController._debug(\"    - idle\")\n            return\n\n        while True:\n            iocb = self.ioQueue.get(block=0)\n            if not iocb:\n                break\n            if _debug: IOQController._debug(\"    - iocb: %r\", iocb)\n\n            # change the state\n            iocb.ioState = ABORTED\n            iocb.ioError = err\n\n            # notify the client\n            iocb.trigger()\n\n        if (self.state != CTRL_IDLE):\n            if _debug: IOQController._debug(\"    - busy after aborts\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalls by a client to start processing a request.", "response": "def request_io(self, iocb):\n        \"\"\"Called by a client to start processing a request.\"\"\"\n        if _debug: IOQController._debug(\"request_io %r\", iocb)\n\n        # bind the iocb to this controller\n        iocb.ioController = self\n\n        # if we're busy, queue it\n        if (self.state != CTRL_IDLE):\n            if _debug: IOQController._debug(\"    - busy, request queued, active_iocb: %r\", self.active_iocb)\n\n            iocb.ioState = PENDING\n            self.ioQueue.put(iocb)\n            return\n\n        try:\n            # hopefully there won't be an error\n            err = None\n\n            # let derived class figure out how to process this\n            self.process_io(iocb)\n        except:\n            # extract the error\n            err = sys.exc_info()[1]\n            if _debug: IOQController._debug(\"    - process_io() exception: %r\", err)\n\n        # if there was an error, abort the request\n        if err:\n            if _debug: IOQController._debug(\"    - aborting\")\n            self.abort_io(iocb, err)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling by a handler to notify the controller that a request is being processed.", "response": "def active_io(self, iocb):\n        \"\"\"Called by a handler to notify the controller that a request is\n        being processed.\"\"\"\n        if _debug: IOQController._debug(\"active_io %r\", iocb)\n\n        # base class work first, setting iocb state and timer data\n        IOController.active_io(self, iocb)\n\n        # change our state\n        self.state = CTRL_ACTIVE\n        _statelog.debug(\"%s %s %s\" % (_strftime(), self.name, \"active\"))\n\n        # keep track of the iocb\n        self.active_iocb = iocb"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalls by the handler to return data to the client.", "response": "def complete_io(self, iocb, msg):\n        \"\"\"Called by a handler to return data to the client.\"\"\"\n        if _debug: IOQController._debug(\"complete_io %r %r\", iocb, msg)\n\n        # check to see if it is completing the active one\n        if iocb is not self.active_iocb:\n            raise RuntimeError(\"not the current iocb\")\n\n        # normal completion\n        IOController.complete_io(self, iocb, msg)\n\n        # no longer an active iocb\n        self.active_iocb = None\n\n        # check to see if we should wait a bit\n        if self.wait_time:\n            # change our state\n            self.state = CTRL_WAITING\n            _statelog.debug(\"%s %s %s\" % (_strftime(), self.name, \"waiting\"))\n\n            # schedule a call in the future\n            task = FunctionTask(IOQController._wait_trigger, self)\n            task.install_task(delta=self.wait_time)\n\n        else:\n            # change our state\n            self.state = CTRL_IDLE\n            _statelog.debug(\"%s %s %s\" % (_strftime(), self.name, \"idle\"))\n\n            # look for more to do\n            deferred(IOQController._trigger, self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef abort_io(self, iocb, err):\n        if _debug: IOQController._debug(\"abort_io %r %r\", iocb, err)\n\n        # normal abort\n        IOController.abort_io(self, iocb, err)\n\n        # check to see if it is completing the active one\n        if iocb is not self.active_iocb:\n            if _debug: IOQController._debug(\"    - not current iocb\")\n            return\n\n        # no longer an active iocb\n        self.active_iocb = None\n\n        # change our state\n        self.state = CTRL_IDLE\n        _statelog.debug(\"%s %s %s\" % (_strftime(), self.name, \"idle\"))\n\n        # look for more to do\n        deferred(IOQController._trigger, self)", "response": "Called by a handler or a client to abort a transaction."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _trigger(self):\n        if _debug: IOQController._debug(\"_trigger\")\n\n        # if we are busy, do nothing\n        if self.state != CTRL_IDLE:\n            if _debug: IOQController._debug(\"    - not idle\")\n            return\n\n        # if there is nothing to do, return\n        if not self.ioQueue.queue:\n            if _debug: IOQController._debug(\"    - empty queue\")\n            return\n\n        # get the next iocb\n        iocb = self.ioQueue.get()\n\n        try:\n            # hopefully there won't be an error\n            err = None\n\n            # let derived class figure out how to process this\n            self.process_io(iocb)\n        except:\n            # extract the error\n            err = sys.exc_info()[1]\n\n        # if there was an error, abort the request\n        if err:\n            self.abort_io(iocb, err)\n\n        # if we're idle, call again\n        if self.state == CTRL_IDLE:\n            deferred(IOQController._trigger, self)", "response": "Called to launch the next request in the queue."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _wait_trigger(self):\n        if _debug: IOQController._debug(\"_wait_trigger\")\n\n        # make sure we are waiting\n        if (self.state != CTRL_WAITING):\n            raise RuntimeError(\"not waiting\")\n\n        # change our state\n        self.state = CTRL_IDLE\n        _statelog.debug(\"%s %s %s\" % (_strftime(), self.name, \"idle\"))\n\n        # look for more to do\n        IOQController._trigger(self)", "response": "Called to wait for the next request in the queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_npdu(self, npdu):\n        if _debug: DeviceToDeviceServerService._debug(\"process_npdu %r\", npdu)\n\n        # broadcast messages go to peers\n        if npdu.pduDestination.addrType == Address.localBroadcastAddr:\n            destList = self.connections.keys()\n        else:\n            if npdu.pduDestination not in self.connections:\n                if _debug: DeviceToDeviceServerService._debug(\"    - not a connected client\")\n                return\n            destList = [npdu.pduDestination]\n        if _debug: DeviceToDeviceServerService._debug(\"    - destList: %r\", destList)\n\n        for dest in destList:\n            # make device-to-device APDU\n            xpdu = DeviceToDeviceAPDU(npdu)\n            xpdu.pduDestination = dest\n\n            # send it down to the multiplexer\n            self.service_request(xpdu)", "response": "encode NPDUs from the service access point and send them downstream."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nencode NPDUs from the service access point and send them downstream.", "response": "def process_npdu(self, npdu):\n        \"\"\"encode NPDUs from the service access point and send them downstream.\"\"\"\n        if _debug: DeviceToDeviceClientService._debug(\"process_npdu %r\", npdu)\n\n        # broadcast messages go to everyone\n        if npdu.pduDestination.addrType == Address.localBroadcastAddr:\n            destList = self.connections.keys()\n        else:\n            conn = self.connections.get(npdu.pduDestination, None)\n            if not conn:\n                if _debug: DeviceToDeviceClientService._debug(\"    - not a connected client\")\n\n                # start a connection attempt\n                conn = self.connect(npdu.pduDestination)\n            if not conn.connected:\n                # keep a reference to this npdu to send after the ack comes back\n                conn.pendingNPDU.append(npdu)\n                return\n\n            destList = [npdu.pduDestination]\n        if _debug: DeviceToDeviceClientService._debug(\"    - destList: %r\", destList)\n\n        for dest in destList:\n            # make device-to-device APDU\n            xpdu = DeviceToDeviceAPDU(npdu)\n            xpdu.pduDestination = dest\n\n            # send it down to the multiplexer\n            self.service_request(xpdu)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitiating a connection request to the device.", "response": "def connect(self, addr):\n        \"\"\"Initiate a connection request to the device.\"\"\"\n        if _debug: DeviceToDeviceClientService._debug(\"connect %r\", addr)\n\n        # make a connection\n        conn = ConnectionState(addr)\n        self.multiplexer.connections[addr] = conn\n\n        # associate with this service, but it is not connected until the ack comes back\n        conn.service = self\n\n        # keep a list of pending NPDU objects until the ack comes back\n        conn.pendingNPDU = []\n\n        # build a service request\n        request = ServiceRequest(DEVICE_TO_DEVICE_SERVICE_ID)\n        request.pduDestination = addr\n\n        # send it\n        self.service_request(request)\n\n        # return the connection object\n        return conn"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nencodes NPDUs from the service access point and send them downstream.", "response": "def process_npdu(self, npdu):\n        \"\"\"encode NPDUs from the service access point and send them downstream.\"\"\"\n        if _debug: RouterToRouterService._debug(\"process_npdu %r\", npdu)\n\n        # encode the npdu as if it was about to be delivered to the network\n        pdu = PDU()\n        npdu.encode(pdu)\n        if _debug: RouterToRouterService._debug(\"    - pdu: %r\", pdu)\n\n        # broadcast messages go to everyone\n        if pdu.pduDestination.addrType == Address.localBroadcastAddr:\n            destList = self.connections.keys()\n        else:\n            conn = self.connections.get(pdu.pduDestination, None)\n            if not conn:\n                if _debug: RouterToRouterService._debug(\"    - not a connected client\")\n\n                # start a connection attempt\n                conn = self.connect(pdu.pduDestination)\n            if not conn.connected:\n                # keep a reference to this pdu to send after the ack comes back\n                conn.pendingNPDU.append(pdu)\n                return\n\n            destList = [pdu.pduDestination]\n        if _debug: RouterToRouterService._debug(\"    - destList: %r\", destList)\n\n        for dest in destList:\n            # make a router-to-router NPDU\n            xpdu = RouterToRouterNPDU(pdu)\n            xpdu.pduDestination = dest\n\n            # send it to the multiplexer\n            self.service_request(xpdu)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef connect(self, addr):\n        if _debug: RouterToRouterService._debug(\"connect %r\", addr)\n\n        # make a connection\n        conn = ConnectionState(addr)\n        self.multiplexer.connections[addr] = conn\n\n        # associate with this service, but it is not connected until the ack comes back\n        conn.service = self\n\n        # keep a list of pending NPDU objects until the ack comes back\n        conn.pendingNPDU = []\n\n        # build a service request\n        request = ServiceRequest(ROUTER_TO_ROUTER_SERVICE_ID)\n        request.pduDestination = addr\n\n        # send it\n        self.service_request(request)\n\n        # return the connection object\n        return conn", "response": "Initiate a connection request to the peer router."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_npdu(self, npdu):\n        if _debug: ProxyServiceNetworkAdapter._debug(\"process_npdu %r\", npdu)\n\n        # encode the npdu as if it was about to be delivered to the network\n        pdu = PDU()\n        npdu.encode(pdu)\n        if _debug: ProxyServiceNetworkAdapter._debug(\"    - pdu: %r\", pdu)\n\n        # broadcast messages go to peers\n        if pdu.pduDestination.addrType == Address.localBroadcastAddr:\n            xpdu = ServerToProxyBroadcastNPDU(pdu)\n        else:\n            xpdu = ServerToProxyUnicastNPDU(pdu.pduDestination, pdu)\n\n        # the connection has the correct address\n        xpdu.pduDestination = self.conn.address\n\n        # send it down to the multiplexer\n        self.conn.service.service_request(xpdu)", "response": "encode NPDUs from the network service access point and send them to the proxy."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreceiving packets forwarded by the proxy and send them upstream to the network service access point.", "response": "def service_confirmation(self, bslpdu):\n        \"\"\"Receive packets forwarded by the proxy and send them upstream to the network service access point.\"\"\"\n        if _debug: ProxyServiceNetworkAdapter._debug(\"service_confirmation %r\", bslpdu)\n\n        # build a PDU\n        pdu = NPDU(bslpdu.pduData)\n\n        # the source is from the original source, not the proxy itself\n        pdu.pduSource = bslpdu.bslciAddress\n\n        # if the proxy received a broadcast, send it upstream as a broadcast\n        if isinstance(bslpdu, ProxyToServerBroadcastNPDU):\n            pdu.pduDestination = LocalBroadcast()\n        if _debug: ProxyServiceNetworkAdapter._debug(\"    - pdu: %r\", pdu)\n\n        # decode it, the nework layer needs NPDUs\n        npdu = NPDU()\n        npdu.decode(pdu)\n        if _debug: ProxyServiceNetworkAdapter._debug(\"    - npdu: %r\", npdu)\n\n        # send it to the service access point for processing\n        self.adapterSAP.process_npdu(self, npdu)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreceiving packets forwarded by the proxy and redirect them to the proxy network adapter.", "response": "def service_confirmation(self, conn, bslpdu):\n        \"\"\"Receive packets forwarded by the proxy and redirect them to the proxy network adapter.\"\"\"\n        if _debug: ProxyServerService._debug(\"service_confirmation %r %r\", conn, bslpdu)\n\n        # make sure there is an adapter for it - or something went wrong\n        if not getattr(conn, 'proxyAdapter', None):\n            raise RuntimeError(\"service confirmation received but no adapter for it\")\n\n        # forward along\n        conn.proxyAdapter.service_confirmation(bslpdu)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connect(self, addr=None, userinfo=None):\n        if _debug: ProxyClientService._debug(\"connect addr=%r\", addr)\n\n        # if the address was provided, use it\n        if addr:\n            self.address = addr\n        else:\n            addr = self.address\n\n        # if the user was provided, save it\n        if userinfo:\n            self.userinfo = userinfo\n\n        # make a connection\n        conn = ConnectionState(addr)\n        self.multiplexer.connections[addr] = conn\n        if _debug: ProxyClientService._debug(\"    - conn: %r\", conn)\n\n        # associate with this service, but it is not connected until the ack comes back\n        conn.service = self\n\n        # keep a list of pending BSLPDU objects until the ack comes back\n        conn.pendingBSLPDU = []\n\n        # build a service request\n        request = ServiceRequest(PROXY_SERVICE_ID)\n        request.pduDestination = addr\n\n        # send it\n        self.service_request(request)\n\n        # return the connection object\n        return conn", "response": "Initiate a connection request to the device."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the amount of damage \\ a amount will do taking the spell power and spell power double into account.", "response": "def get_spell_damage(self, amount: int) -> int:\n\t\t\"\"\"\n\t\tReturns the amount of damage \\a amount will do, taking\n\t\tSPELLPOWER and SPELLPOWER_DOUBLE into account.\n\t\t\"\"\"\n\t\tamount += self.spellpower\n\t\tamount <<= self.controller.spellpower_double\n\t\treturn amount"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef can_pay_cost(self, card):\n\t\tif self.spells_cost_health and card.type == CardType.SPELL:\n\t\t\treturn self.hero.health > card.cost\n\t\treturn self.mana >= card.cost", "response": "Returns whether the player can pay the resource cost of a card."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes player pay \\ a amount mana.", "response": "def pay_cost(self, source, amount: int) -> int:\n\t\t\"\"\"\n\t\tMake player pay \\a amount mana.\n\t\tReturns how much mana is spent, after temporary mana adjustments.\n\t\t\"\"\"\n\t\tif self.spells_cost_health and source.type == CardType.SPELL:\n\t\t\tself.log(\"%s spells cost %i health\", self, amount)\n\t\t\tself.game.queue_actions(self, [Hit(self.hero, amount)])\n\t\t\treturn amount\n\t\tif self.temp_mana:\n\t\t\t# Coin, Innervate etc\n\t\t\tused_temp = min(self.temp_mana, amount)\n\t\t\tamount -= used_temp\n\t\t\tself.temp_mana -= used_temp\n\t\tself.log(\"%s pays %i mana\", self, amount)\n\t\tself.used_mana += amount\n\t\treturn amount"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef summon(self, card):\n\t\tif isinstance(card, str):\n\t\t\tcard = self.card(card, zone=Zone.PLAY)\n\t\tself.game.cheat_action(self, [Summon(self, card)])\n\t\treturn card", "response": "Summons a card in the PLAY zone."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntriggering an event on the Entity", "response": "def trigger_event(self, source, event, args):\n\t\t\"\"\"\n\t\tTrigger an event on the Entity\n\t\t* \\a source: The source of the event\n\t\t* \\a event: The event being triggered\n\t\t* \\a args: A list of arguments to pass to the callback\n\t\t\"\"\"\n\t\tactions = []\n\t\tfor action in event.actions:\n\t\t\tif callable(action):\n\t\t\t\tac = action(self, *args)\n\t\t\t\tif not ac:\n\t\t\t\t\t# Handle falsy returns\n\t\t\t\t\tcontinue\n\t\t\t\tif not hasattr(ac, \"__iter__\"):\n\t\t\t\t\tactions.append(ac)\n\t\t\t\telse:\n\t\t\t\t\tactions += action(self, *args)\n\t\t\telse:\n\t\t\t\tactions.append(action)\n\t\tret = source.game.trigger(self, actions, args)\n\t\tif event.once:\n\t\t\tself._events.remove(event)\n\n\t\treturn ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_damage(self, amount: int, target) -> int:\n\t\tif target.immune:\n\t\t\tself.log(\"%r is immune to %s for %i damage\", target, self, amount)\n\t\t\treturn 0\n\t\treturn amount", "response": "Get the damage of a target from the given amount."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _eval_card(source, card):\n\tif isinstance(card, LazyValue):\n\t\tcard = card.evaluate(source)\n\n\tif isinstance(card, Action):\n\t\tcard = card.trigger(source)[0]\n\n\tif not isinstance(card, list):\n\t\tcards = [card]\n\telse:\n\t\tcards = card\n\n\tret = []\n\tfor card in cards:\n\t\tif isinstance(card, str):\n\t\t\tret.append(source.controller.card(card, source))\n\t\telse:\n\t\t\tret.append(card)\n\n\treturn ret", "response": "Evaluate a card from a source object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef then(self, *args):\n\t\tret = self.__class__(*self._args, **self._kwargs)\n\t\tret.callback = args\n\t\tret.times = self.times\n\t\treturn ret", "response": "Create a callback containing an action queue called upon the action s trigger with the arguments available."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a deck of 30 random cards for the \\ a card_class \\ a card_class \\ a exclude list", "response": "def random_draft(card_class: CardClass, exclude=[]):\n\t\"\"\"\n\tReturn a deck of 30 random cards for the \\a card_class\n\t\"\"\"\n\tfrom . import cards\n\tfrom .deck import Deck\n\n\tdeck = []\n\tcollection = []\n\t# hero = card_class.default_hero\n\n\tfor card in cards.db.keys():\n\t\tif card in exclude:\n\t\t\tcontinue\n\t\tcls = cards.db[card]\n\t\tif not cls.collectible:\n\t\t\tcontinue\n\t\tif cls.type == CardType.HERO:\n\t\t\t# Heroes are collectible...\n\t\t\tcontinue\n\t\tif cls.card_class and cls.card_class not in [card_class, CardClass.NEUTRAL]:\n\t\t\t# Play with more possibilities\n\t\t\tcontinue\n\t\tcollection.append(cls)\n\n\twhile len(deck) < Deck.MAX_CARDS:\n\t\tcard = random.choice(collection)\n\t\tif deck.count(card.id) < card.max_count_in_deck:\n\t\t\tdeck.append(card.id)\n\n\treturn deck"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_script_definition(id):\n\tfor cardset in CARD_SETS:\n\t\tmodule = import_module(\"fireplace.cards.%s\" % (cardset))\n\t\tif hasattr(module, id):\n\t\t\treturn getattr(module, id)", "response": "get the script definition for card \\ a id"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef weighted_card_choice(source, weights: List[int], card_sets: List[str], count: int):\n\n\tchosen_cards = []\n\n\t# sum all the weights\n\tcum_weights = []\n\ttotalweight = 0\n\tfor i, w in enumerate(weights):\n\t\ttotalweight += w * len(card_sets[i])\n\t\tcum_weights.append(totalweight)\n\n\t# for each card\n\tfor i in range(count):\n\t\t# choose a set according to weighting\n\t\tchosen_set = bisect(cum_weights, random.random() * totalweight)\n\n\t\t# choose a random card from that set\n\t\tchosen_card_index = random.randint(0, len(card_sets[chosen_set]) - 1)\n\n\t\tchosen_cards.append(card_sets[chosen_set].pop(chosen_card_index))\n\t\ttotalweight -= weights[chosen_set]\n\t\tcum_weights[chosen_set:] = [x - weights[chosen_set] for x in cum_weights[chosen_set:]]\n\n\treturn [source.controller.card(card, source=source) for card in chosen_cards]", "response": "Take a list of weights and a list of card pools and produce a random weighted sample without replacement."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming actions as a result of an event listener.", "response": "def trigger(self, source, actions, event_args):\n\t\t\"\"\"\n\t\tPerform actions as a result of an event listener (TRIGGER)\n\t\t\"\"\"\n\t\ttype = BlockType.TRIGGER\n\t\treturn self.action_block(source, actions, type, event_args=event_args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if one or more players are currently losing.", "response": "def check_for_end_game(self):\n\t\t\"\"\"\n\t\tCheck if one or more player is currently losing.\n\t\tEnd the game if they are.\n\t\t\"\"\"\n\t\tgameover = False\n\t\tfor player in self.players:\n\t\t\tif player.playstate in (PlayState.CONCEDED, PlayState.DISCONNECTED):\n\t\t\t\tplayer.playstate = PlayState.LOSING\n\t\t\tif player.playstate == PlayState.LOSING:\n\t\t\t\tgameover = True\n\n\t\tif gameover:\n\t\t\tif self.players[0].playstate == self.players[1].playstate:\n\t\t\t\tfor player in self.players:\n\t\t\t\t\tplayer.playstate = PlayState.TIED\n\t\t\telse:\n\t\t\t\tfor player in self.players:\n\t\t\t\t\tif player.playstate == PlayState.LOSING:\n\t\t\t\t\t\tplayer.playstate = PlayState.LOST\n\t\t\t\t\telse:\n\t\t\t\t\t\tplayer.playstate = PlayState.WON\n\t\t\tself.state = State.COMPLETE\n\t\t\tself.manager.step(self.next_step, Step.FINAL_WRAPUP)\n\t\t\tself.manager.step(self.next_step, Step.FINAL_GAMEOVER)\n\t\t\tself.manager.step(self.next_step)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nqueue actions for processing from source.", "response": "def queue_actions(self, source, actions, event_args=None):\n\t\t\"\"\"\n\t\tQueue a list of \\a actions for processing from \\a source.\n\t\tTriggers an aura refresh afterwards.\n\t\t\"\"\"\n\t\tsource.event_args = event_args\n\t\tret = self.trigger_actions(source, actions)\n\t\tsource.event_args = None\n\t\treturn ret"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming a list of actions from source.", "response": "def trigger_actions(self, source, actions):\n\t\t\"\"\"\n\t\tPerforms a list of `actions` from `source`.\n\t\tThis should seldom be called directly - use `queue_actions` instead.\n\t\t\"\"\"\n\t\tret = []\n\t\tfor action in actions:\n\t\t\tif isinstance(action, EventListener):\n\t\t\t\t# Queuing an EventListener registers it as a one-time event\n\t\t\t\t# This allows registering events from eg. play actions\n\t\t\t\tself.log(\"Registering event listener %r on %r\", action, self)\n\t\t\t\taction.once = True\n\t\t\t\t# FIXME: Figure out a cleaner way to get the event listener target\n\t\t\t\tif source.type == CardType.SPELL:\n\t\t\t\t\tlistener = source.controller\n\t\t\t\telse:\n\t\t\t\t\tlistener = source\n\t\t\t\tlistener._events.append(action)\n\t\t\telse:\n\t\t\t\tret.append(action.trigger(source))\n\t\treturn ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef evaluate(self, source):\n\t\tret = self.check(source)\n\t\tif self._neg:\n\t\t\tret = not ret\n\t\tif ret:\n\t\t\tif self._if:\n\t\t\t\treturn self._if\n\t\telif self._else:\n\t\t\treturn self._else", "response": "Evaluates the board state from source and returns an iterable of iterable of\n\tActions as a result."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef trigger(self, source):\n\t\tactions = self.evaluate(source)\n\t\tif actions:\n\t\t\tif not hasattr(actions, \"__iter__\"):\n\t\t\t\tactions = (actions, )\n\t\t\tsource.game.trigger_actions(source, actions)", "response": "Triggers all actions meant to trigger on the board state from source."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef copy(self, source, entity):\n\t\tlog.info(\"Creating a copy of %r\", entity)\n\t\treturn source.controller.card(entity.id, source)", "response": "Create a copy of the given entity."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a card pool with all cards matching specified filters", "response": "def find_cards(self, source=None, **filters):\n\t\t\"\"\"\n\t\tGenerate a card pool with all cards matching specified filters\n\t\t\"\"\"\n\t\tif not filters:\n\t\t\tnew_filters = self.filters.copy()\n\t\telse:\n\t\t\tnew_filters = filters.copy()\n\n\t\tfor k, v in new_filters.items():\n\t\t\tif isinstance(v, LazyValue):\n\t\t\t\tnew_filters[k] = v.evaluate(source)\n\n\t\tfrom .. import cards\n\t\treturn cards.filter(**new_filters)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef evaluate(self, source, cards=None) -> str:\n\t\tfrom ..utils import weighted_card_choice\n\n\t\tif cards:\n\t\t\t# Use specific card list if given\n\t\t\tself.weights = [1]\n\t\t\tcard_sets = [list(cards)]\n\t\telif not self.weightedfilters:\n\t\t\t# Use global filters if no weighted filter sets given\n\t\t\tself.weights = [1]\n\t\t\tcard_sets = [self.find_cards(source)]\n\t\telse:\n\t\t\t# Otherwise find cards for each set of filters\n\t\t\t# add the global filters to each set of filters\n\t\t\twf = [{**x, **self.filters} for x in self.weightedfilters]\n\t\t\tcard_sets = [self.find_cards(source, **x) for x in wf]\n\n\t\t# get weighted sample of card pools\n\t\treturn weighted_card_choice(source, self.weights, card_sets, self.count)", "response": "This function evaluates the source with the given cards."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef buff(self, target, buff, **kwargs):\n\t\tret = self.controller.card(buff, self)\n\t\tret.source = self\n\t\tret.apply(target)\n\t\tfor k, v in kwargs.items():\n\t\t\tsetattr(ret, k, v)\n\t\treturn ret", "response": "This method is used to apply it to the target card."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if the card is powered up.", "response": "def powered_up(self):\n\t\t\"\"\"\n\t\tReturns True whether the card is \"powered up\".\n\t\t\"\"\"\n\t\tif not self.data.scripts.powered_up:\n\t\t\treturn False\n\t\tfor script in self.data.scripts.powered_up:\n\t\t\tif not script.check(self):\n\t\t\t\treturn False\n\t\treturn True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the card s position in the zone 1 - indexed in the hand zone 0 - indexed in the hand zone.", "response": "def zone_position(self):\n\t\t\"\"\"\n\t\tReturns the card's position (1-indexed) in its zone, or 0 if not available.\n\t\t\"\"\"\n\t\tif self.zone == Zone.HAND:\n\t\t\treturn self.controller.hand.index(self) + 1\n\t\treturn 0"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nqueue a Play action on the card.", "response": "def play(self, target=None, index=None, choose=None):\n\t\t\"\"\"\n\t\tQueue a Play action on the card.\n\t\t\"\"\"\n\t\tif choose:\n\t\t\tif self.must_choose_one:\n\t\t\t\tchoose = card = self.choose_cards.filter(id=choose)[0]\n\t\t\t\tself.log(\"%r: choosing %r\", self, choose)\n\t\t\telse:\n\t\t\t\traise InvalidAction(\"%r cannot be played with choice %r\" % (self, choose))\n\t\telse:\n\t\t\tif self.must_choose_one:\n\t\t\t\traise InvalidAction(\"%r requires a choice (one of %r)\" % (self, self.choose_cards))\n\t\t\tcard = self\n\t\tif not self.is_playable():\n\t\t\traise InvalidAction(\"%r isn't playable.\" % (self))\n\t\tif card.requires_target():\n\t\t\tif not target:\n\t\t\t\traise InvalidAction(\"%r requires a target to play.\" % (self))\n\t\t\telif target not in self.play_targets:\n\t\t\t\traise InvalidAction(\"%r is not a valid target for %r.\" % (target, self))\n\t\telif target:\n\t\t\tself.logger.warning(\"%r does not require a target, ignoring target %r\", self, target)\n\t\tself.game.play_card(self, target, index, choose)\n\t\treturn self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef morph(self, into):\n\t\treturn self.game.cheat_action(self, [actions.Morph(self, into)])", "response": "Morph the card into another card."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nshuffling the card into the controller s deck.", "response": "def shuffle_into_deck(self):\n\t\t\"\"\"\n\t\tShuffle the card into the controller's deck\n\t\t\"\"\"\n\t\treturn self.game.cheat_action(self, [actions.Shuffle(self.controller, self)])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntrue if the card requires a target", "response": "def battlecry_requires_target(self):\n\t\t\"\"\"\n\t\tTrue if the play action of the card requires a target\n\t\t\"\"\"\n\t\tif self.has_combo and self.controller.combo:\n\t\t\tif PlayReq.REQ_TARGET_FOR_COMBO in self.requirements:\n\t\t\t\treturn True\n\n\t\tfor req in TARGETING_PREREQUISITES:\n\t\t\tif req in self.requirements:\n\t\t\t\treturn True\n\t\treturn False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef requires_target(self):\n\t\tif self.has_combo and PlayReq.REQ_TARGET_FOR_COMBO in self.requirements:\n\t\t\tif self.controller.combo:\n\t\t\t\treturn True\n\t\tif PlayReq.REQ_TARGET_IF_AVAILABLE in self.requirements:\n\t\t\treturn bool(self.play_targets)\n\t\tif PlayReq.REQ_TARGET_IF_AVAILABLE_AND_DRAGON_IN_HAND in self.requirements:\n\t\t\tif self.controller.hand.filter(race=Race.DRAGON):\n\t\t\t\treturn bool(self.play_targets)\n\t\treq = self.requirements.get(PlayReq.REQ_TARGET_IF_AVAILABLE_AND_MINIMUM_FRIENDLY_MINIONS)\n\t\tif req is not None:\n\t\t\tif len(self.controller.field) >= req:\n\t\t\t\treturn bool(self.play_targets)\n\t\treq = self.requirements.get(PlayReq.REQ_TARGET_IF_AVAILABLE_AND_MINIMUM_FRIENDLY_SECRETS)\n\t\tif req is not None:\n\t\t\tif len(self.controller.secrets) >= req:\n\t\t\t\treturn bool(self.play_targets)\n\t\treturn PlayReq.REQ_TARGET_TO_PLAY in self.requirements", "response": "Returns True if the card currently requires a target"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the xmlcard and the card definition of \\a id Then return a merged class of the two", "response": "def merge(id, card, cardscript=None):\n\t\t\"\"\"\n\t\tFind the xmlcard and the card definition of \\a id\n\t\tThen return a merged class of the two\n\t\t\"\"\"\n\t\tif card is None:\n\t\t\tcard = cardxml.CardXML(id)\n\n\t\tif cardscript is None:\n\t\t\tcardscript = get_script_definition(id)\n\n\t\tif cardscript:\n\t\t\tcard.scripts = type(id, (cardscript, ), {})\n\t\telse:\n\t\t\tcard.scripts = type(id, (), {})\n\n\t\tscriptnames = (\n\t\t\t\"activate\", \"combo\", \"deathrattle\", \"draw\", \"inspire\", \"play\",\n\t\t\t\"enrage\", \"update\", \"powered_up\"\n\t\t)\n\n\t\tfor script in scriptnames:\n\t\t\tactions = getattr(card.scripts, script, None)\n\t\t\tif actions is None:\n\t\t\t\t# Set the action by default to avoid runtime hasattr() calls\n\t\t\t\tsetattr(card.scripts, script, [])\n\t\t\telif not callable(actions):\n\t\t\t\tif not hasattr(actions, \"__iter__\"):\n\t\t\t\t\t# Ensure the actions are always iterable\n\t\t\t\t\tsetattr(card.scripts, script, (actions, ))\n\n\t\tfor script in (\"events\", \"secret\"):\n\t\t\tevents = getattr(card.scripts, script, None)\n\t\t\tif events is None:\n\t\t\t\tsetattr(card.scripts, script, [])\n\t\t\telif not hasattr(events, \"__iter__\"):\n\t\t\t\tsetattr(card.scripts, script, [events])\n\n\t\tif not hasattr(card.scripts, \"cost_mod\"):\n\t\t\tcard.scripts.cost_mod = None\n\n\t\tif not hasattr(card.scripts, \"Hand\"):\n\t\t\tcard.scripts.Hand = type(\"Hand\", (), {})\n\n\t\tif not hasattr(card.scripts.Hand, \"events\"):\n\t\t\tcard.scripts.Hand.events = []\n\n\t\tif not hasattr(card.scripts.Hand.events, \"__iter__\"):\n\t\t\tcard.scripts.Hand.events = [card.scripts.Hand.events]\n\n\t\tif not hasattr(card.scripts.Hand, \"update\"):\n\t\t\tcard.scripts.Hand.update = ()\n\n\t\tif not hasattr(card.scripts.Hand.update, \"__iter__\"):\n\t\t\tcard.scripts.Hand.update = (card.scripts.Hand.update, )\n\n\t\t# Set choose one cards\n\t\tif hasattr(cardscript, \"choose\"):\n\t\t\tcard.choose_cards = cardscript.choose[:]\n\t\telse:\n\t\t\tcard.choose_cards = []\n\n\t\tif hasattr(cardscript, \"tags\"):\n\t\t\tfor tag, value in cardscript.tags.items():\n\t\t\t\tcard.tags[tag] = value\n\n\t\t# Set some additional events based on the base tags...\n\t\tif card.poisonous:\n\t\t\tcard.scripts.events.append(POISONOUS)\n\n\t\treturn card"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef filter(self, **kwargs):\n\t\tif not self.initialized:\n\t\t\tself.initialize()\n\n\t\tcards = self.values()\n\n\t\tif \"type\" not in kwargs:\n\t\t\tkwargs[\"type\"] = [CardType.SPELL, CardType.WEAPON, CardType.MINION]\n\n\t\tfor attr, value in kwargs.items():\n\t\t\tif value is not None:\n\t\t\t\t# What? this doesn't work?\n\t\t\t\t# cards = __builtins__[\"filter\"](lambda c: getattr(c, attr) == value, cards)\n\t\t\t\tcards = [\n\t\t\t\t\tcard for card in cards if (isinstance(value, list) and getattr(card, attr) in value) or\n\t\t\t\t\tgetattr(card, attr) == value\n\t\t\t\t]\n\n\t\treturn [card.id for card in cards]", "response": "Returns a list of card IDs matching the given filters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading Dropzone resources with given version and init dropzone configuration.", "response": "def load(js_url='', css_url='', version='5.2.0'):\n        \"\"\"Load Dropzone resources with given version and init dropzone configuration.\n\n        .. versionchanged:: 1.4.3\n            Added ``js_url`` and ``css_url`` parameters to pass custom resource URL.\n\n        .. versionchanged:: 1.4.4\n            This method was deprecated due to inflexible. Now it's divided into three methods:\n            1. Use ``load_css()`` to load css resources.\n            2. Use ``load_js()`` to load js resources.\n            3. Use ``config()`` to configure Dropzone.\n\n        :param js_url: The JavaScript url for Dropzone.js.\n        :param css_url: The CSS url for Dropzone.js.\n        :param version: The version of Dropzone.js.\n        \"\"\"\n        warnings.warn('The method will be removed in 2.0, see docs for more details.')\n        js_filename = 'dropzone.min.js'\n        css_filename = 'dropzone.min.css'\n\n        upload_multiple = current_app.config['DROPZONE_UPLOAD_MULTIPLE']\n        parallel_uploads = current_app.config['DROPZONE_PARALLEL_UPLOADS']\n\n        if upload_multiple in [True, 'true', 'True', 1]:\n            upload_multiple = 'true'\n        else:\n            upload_multiple = 'false'\n\n        serve_local = current_app.config['DROPZONE_SERVE_LOCAL']\n        size = current_app.config['DROPZONE_MAX_FILE_SIZE']\n        param = current_app.config['DROPZONE_INPUT_NAME']\n        redirect_view = current_app.config['DROPZONE_REDIRECT_VIEW']\n\n        if redirect_view is not None:\n            redirect_js = '''\n    this.on(\"queuecomplete\", function(file) {\n    // Called when all files in the queue finish uploading.\n    window.location = \"%s\";\n    });''' % url_for(redirect_view)\n        else:\n            redirect_js = ''\n\n        if not current_app.config['DROPZONE_ALLOWED_FILE_CUSTOM']:\n            allowed_type = allowed_file_extensions[\n                current_app.config['DROPZONE_ALLOWED_FILE_TYPE']]\n        else:\n            allowed_type = current_app.config['DROPZONE_ALLOWED_FILE_TYPE']\n\n        max_files = current_app.config['DROPZONE_MAX_FILES']\n        default_message = current_app.config['DROPZONE_DEFAULT_MESSAGE']\n        invalid_file_type = current_app.config['DROPZONE_INVALID_FILE_TYPE']\n        file_too_big = current_app.config['DROPZONE_FILE_TOO_BIG']\n        server_error = current_app.config['DROPZONE_SERVER_ERROR']\n        browser_unsupported = current_app.config['DROPZONE_BROWSER_UNSUPPORTED']\n        max_files_exceeded = current_app.config['DROPZONE_MAX_FILE_EXCEED']\n        cancelUpload = current_app.config['DROPZONE_CANCEL_UPLOAD']\n        removeFile = current_app.config['DROPZONE_REMOVE_FILE']\n        cancelConfirmation = current_app.config['DROPZONE_CANCEL_CONFIRMATION']\n        uploadCanceled = current_app.config['DROPZONE_UPLOAD_CANCELED']\n\n        timeout = current_app.config['DROPZONE_TIMEOUT']\n        if timeout:\n            timeout_js = 'timeout: %d,' % timeout\n        else:\n            timeout_js = ''\n\n        if serve_local:\n            js = '<script src=\"%s\"></script>\\n' % url_for('dropzone.static', filename=js_filename)\n            css = '<link rel=\"stylesheet\" href=\"%s\" type=\"text/css\">\\n' % \\\n                  url_for('dropzone.static', filename=css_filename)\n        else:\n            js = '<script src=\"https://cdn.jsdelivr.net/npm/dropzone@%s/dist/%s\"></script>\\n' % (version, js_filename)\n            css = '<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/dropzone@%s/dist/min/%s\"' \\\n                  ' type=\"text/css\">\\n' % (version, css_filename)\n\n        if js_url:\n            js = '<script src=\"%s\"></script>\\n' % js_url\n        if css_url:\n            css = '<link rel=\"stylesheet\" href=\"%s\" type=\"text/css\">\\n' % css_url\n\n        return Markup('''\n  %s%s<script>\nDropzone.options.myDropzone = {\n  init: function() {%s},\n  uploadMultiple: %s,\n  parallelUploads: %d,\n  paramName: \"%s\", // The name that will be used to transfer the file\n  maxFilesize: %d, // MB\n  acceptedFiles: \"%s\",\n  maxFiles: %s,\n  dictDefaultMessage: \"%s\", // message display on drop area\n  dictFallbackMessage: \"%s\",\n  dictInvalidFileType: \"%s\",\n  dictFileTooBig: \"%s\",\n  dictResponseError: \"%s\",\n  dictMaxFilesExceeded: \"%s\",\n  dictCancelUpload: \"%s\",\n  dictRemoveFile: \"%s\",\n  dictCancelUploadConfirmation: \"%s\",\n  dictUploadCanceled: \"%s\",\n  %s // timeout\n};\n        </script>\n        ''' % (css, js, redirect_js, upload_multiple, parallel_uploads, param, size, allowed_type, max_files,\n               default_message, browser_unsupported, invalid_file_type, file_too_big, server_error, \n               max_files_exceeded, cancelUpload, removeFile, cancelConfirmation, uploadCanceled, timeout_js))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_css(css_url=None, version='5.2.0'):\n        css_filename = 'dropzone.min.css'\n        serve_local = current_app.config['DROPZONE_SERVE_LOCAL']\n\n        if serve_local:\n            css = '<link rel=\"stylesheet\" href=\"%s\" type=\"text/css\">\\n' % \\\n                  url_for('dropzone.static', filename=css_filename)\n        else:\n            css = '<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/dropzone@%s/dist/min/%s\"' \\\n                  ' type=\"text/css\">\\n' % (version, css_filename)\n\n        if css_url:\n            css = '<link rel=\"stylesheet\" href=\"%s\" type=\"text/css\">\\n' % css_url\n        return Markup(css)", "response": "Load Dropzone s css resources with given version."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_js(js_url=None, version='5.2.0'):\n        js_filename = 'dropzone.min.js'\n        serve_local = current_app.config['DROPZONE_SERVE_LOCAL']\n\n        if serve_local:\n            js = '<script src=\"%s\"></script>\\n' % url_for('dropzone.static', filename=js_filename)\n        else:\n            js = '<script src=\"https://cdn.jsdelivr.net/npm/dropzone@%s/dist/%s\"></script>\\n' % (version, js_filename)\n\n        if js_url:\n            js = '<script src=\"%s\"></script>\\n' % js_url\n        return Markup(js)", "response": "Load Dropzone s js resources with given version."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef config(redirect_url=None, custom_init='', custom_options='', **kwargs):\n        if custom_init and not custom_init.strip().endswith(';'):\n            custom_init += ';'\n\n        if custom_options and not custom_options.strip().endswith(','):\n            custom_options += ','\n\n        upload_multiple = kwargs.get('upload_multiple', current_app.config['DROPZONE_UPLOAD_MULTIPLE'])\n        parallel_uploads = kwargs.get('parallel_uploads', current_app.config['DROPZONE_PARALLEL_UPLOADS'])\n\n        if upload_multiple in [True, 'true', 'True', 1]:\n            upload_multiple = 'true'\n        else:\n            upload_multiple = 'false'\n\n        size = kwargs.get('max_file_size', current_app.config['DROPZONE_MAX_FILE_SIZE'])\n        param = kwargs.get('input_name', current_app.config['DROPZONE_INPUT_NAME'])\n        redirect_view = kwargs.get('redirect_view', current_app.config['DROPZONE_REDIRECT_VIEW'])\n\n        if redirect_view is not None or redirect_url is not None:\n            redirect_url = redirect_url or url_for(redirect_view)\n            redirect_js = '''\n            this.on(\"queuecomplete\", function(file) {\n            // Called when all files in the queue finish uploading.\n            window.location = \"%s\";\n            });''' % redirect_url\n        else:\n            redirect_js = ''\n\n        max_files = kwargs.get('max_files', current_app.config['DROPZONE_MAX_FILES'])\n\n        click_upload = kwargs.get('upload_on_click', current_app.config['DROPZONE_UPLOAD_ON_CLICK'])\n        button_id = kwargs.get('upload_btn_id', current_app.config['DROPZONE_UPLOAD_BTN_ID'])\n        in_form = kwargs.get('in_form', current_app.config['DROPZONE_IN_FORM'])\n        cancelUpload = kwargs.get('cancel_upload', current_app.config['DROPZONE_CANCEL_UPLOAD'])\n        removeFile = kwargs.get('remove_file', current_app.config['DROPZONE_REMOVE_FILE'])\n        cancelConfirmation = kwargs.get('cancel_confirmation', current_app.config['DROPZONE_CANCEL_CONFIRMATION'])\n        uploadCanceled = kwargs.get('upload_canceled', current_app.config['DROPZONE_UPLOAD_CANCELED'])\n\n        if click_upload:\n            if in_form:\n                action = get_url(kwargs.get('upload_action', current_app.config['DROPZONE_UPLOAD_ACTION']))\n\n                click_listener = '''\n                dz = this; // Makes sure that 'this' is understood inside the functions below.\n\n                document.getElementById(\"%s\").addEventListener(\"click\", function handler(e) {\n                    e.currentTarget.removeEventListener(e.type, handler);\n                    e.preventDefault();\n                    e.stopPropagation();\n                    dz.processQueue();\n                });\n                this.on(\"queuecomplete\", function(file) {\n                    // Called when all files in the queue finish uploading.\n                    document.getElementById(\"%s\").click();\n                });\n                ''' % (button_id, button_id)\n                click_option = '''\n                url: \"%s\",\n                autoProcessQueue: false,\n                // addRemoveLinks: true,\n                ''' % action\n            else:\n                click_listener = '''\n                dz = this;\n                document.getElementById(\"%s\").addEventListener(\"click\", function handler(e) {dz.processQueue();});\n                ''' % button_id\n\n                click_option = '''\n                autoProcessQueue: false,\n                // addRemoveLinks: true,\n                '''\n            upload_multiple = 'true'\n            parallel_uploads = max_files if isinstance(max_files, int) else parallel_uploads\n        else:\n            click_listener = ''\n            click_option = ''\n\n        allowed_file_type = kwargs.get('allowed_file_type', current_app.config['DROPZONE_ALLOWED_FILE_TYPE'])\n        allowed_file_custom = kwargs.get('allowed_file_custom', current_app.config['DROPZONE_ALLOWED_FILE_CUSTOM'])\n\n        if allowed_file_custom:\n            allowed_type = allowed_file_type\n        else:\n            allowed_type = allowed_file_extensions[allowed_file_type]\n\n        default_message = kwargs.get('default_message', current_app.config['DROPZONE_DEFAULT_MESSAGE'])\n        invalid_file_type = kwargs.get('invalid_file_type', current_app.config['DROPZONE_INVALID_FILE_TYPE'])\n        file_too_big = kwargs.get('file_too_big', current_app.config['DROPZONE_FILE_TOO_BIG'])\n        server_error = kwargs.get('server_error', current_app.config['DROPZONE_SERVER_ERROR'])\n        browser_unsupported = kwargs.get('browser_unsupported', current_app.config['DROPZONE_BROWSER_UNSUPPORTED'])\n        max_files_exceeded = kwargs.get('max_files_exceeded', current_app.config['DROPZONE_MAX_FILE_EXCEED'])\n\n        timeout = kwargs.get('timeout', current_app.config['DROPZONE_TIMEOUT'])\n        if timeout:\n            timeout_js = 'timeout: %d,' % timeout\n        else:\n            timeout_js = ''\n\n        return Markup('''<script>\n        Dropzone.options.myDropzone = {\n          init: function() {\n              %s  // redirect after queue complete\n              %s  // upload queue when button click\n              %s  // custom init code\n          },\n          %s  // click upload options\n          uploadMultiple: %s,\n          parallelUploads: %d,\n          paramName: \"%s\", // The name that will be used to transfer the file\n          maxFilesize: %d, // MB\n          acceptedFiles: \"%s\",\n          maxFiles: %s,\n          dictDefaultMessage: `%s`, // message display on drop area\n          dictFallbackMessage: \"%s\",\n          dictInvalidFileType: \"%s\",\n          dictFileTooBig: \"%s\",\n          dictResponseError: \"%s\",\n          dictMaxFilesExceeded: \"%s\",\n          dictCancelUpload: \"%s\",\n          dictRemoveFile: \"%s\",\n          dictCancelUploadConfirmation: \"%s\",\n          dictUploadCanceled: \"%s\",\n          %s  // timeout\n          %s  // custom options code\n        };\n                </script>\n                ''' % (redirect_js, click_listener, custom_init, click_option,\n                       upload_multiple, parallel_uploads, param, size, allowed_type, max_files,\n                       default_message, browser_unsupported, invalid_file_type, file_too_big,\n                       server_error, max_files_exceeded, cancelUpload, removeFile, cancelConfirmation, \n                       uploadCanceled, timeout_js, custom_options))", "response": "Initialize dropzone configuration.\n\n        .. versionadded:: 1.4.4\n\n        :param redirect_url: The URL to redirect when upload complete.\n        :param custom_init: Custom javascript code in ``init: function() {}``.\n        :param custom_options: Custom javascript code in ``Dropzone.options.myDropzone = {}``.\n        :param **kwargs: Mirror configuration variable, lowercase and without prefix.\n                         For example, ``DROPZONE_UPLOAD_MULTIPLE`` becomes ``upload_multiple`` here."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a Dropzone form with given action.", "response": "def create(action='', csrf=False, action_view='', **kwargs):\n        \"\"\"Create a Dropzone form with given action.\n\n        .. versionchanged:: 1.4.2\n            Added ``csrf`` parameter to enable CSRF protect.\n\n        .. versionchanged:: 1.4.3\n            Added ``action`` parameter to replace ``action_view``, ``action_view`` was deprecated now.\n\n        .. versionchanged:: 1.5.0\n            If ``DROPZONE_IN_FORM`` set to ``True``, create ``<div>`` instead of ``<form>``.\n\n        :param action: The action attribute in ``<form>``, pass the url which handle uploads.\n        :param csrf: Enable CSRF protect or not, same with ``DROPZONE_ENABLE_CSRF``.\n        :param action_view: The view which handle the post data, deprecated since 1.4.2.\n        \"\"\"\n        if current_app.config['DROPZONE_IN_FORM']:\n            return Markup('<div class=\"dropzone\" id=\"myDropzone\"></div>')\n\n        if action:\n            action_url = get_url(action)\n        else:\n            action_url = url_for(action_view, **kwargs)\n\n        if csrf or current_app.config['DROPZONE_ENABLE_CSRF']:\n            if 'csrf' not in current_app.extensions:\n                raise RuntimeError(\"CSRFProtect is not initialized. It's required to enable CSRF protect, \\\n                    see docs for more details.\")\n            csrf_field = render_template_string('<input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\"/>')\n        else:\n            csrf_field = ''\n        return Markup('''<form action=\"%s\" method=\"post\" class=\"dropzone\" id=\"myDropzone\"\n        enctype=\"multipart/form-data\">%s</form>''' % (action_url, csrf_field))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main():\n    t, kf, t0, major, minor, prod, beta = sympy.symbols(\n        't k_f t0 Y Z X beta', negative=False)\n    for f in funcs:\n        args = [t, kf, prod, major, minor]\n        if f in (pseudo_rev, binary_rev):\n            args.insert(2, kf/beta)\n        expr = f(*args, backend='sympy')\n        with open(f.__name__ + '.png', 'wb') as ofh:\n            sympy.printing.preview(expr, output='png', filename='out.png',\n                                   viewer='BytesIO', outputbuffer=ofh)\n        with open(f.__name__ + '_diff.png', 'wb') as ofh:\n            sympy.printing.preview(expr.diff(t).subs({t0: 0}).simplify(),\n                                   output='png', filename='out.png',\n                                   viewer='BytesIO', outputbuffer=ofh)", "response": "This example demonstrates how to generate pretty equations from an analytic sequence."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a label for the given reaction.", "response": "def _cell_label_html(self, printer, ori_idx, rxn):\n        \"\"\" Reaction formatting callback. (reaction index -> string) \"\"\"\n        pretty = rxn.unicode(self.substances, with_param=True, with_name=False)\n        return '<a title=\"%d: %s\">%s</a>' % (ori_idx, pretty, printer._print(rxn.name or rxn.param))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef roots(self, init_concs, varied_data, varied, x0=None,\n              NumSys=NumSysLog, plot_kwargs=None,\n              neqsys_type='chained_conditional', **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        init_concs : array or dict\n        varied_data : array\n        varied_idx : int or str\n        x0 : array\n        NumSys : _NumSys subclass\n            See :class:`NumSysLin`, :class:`NumSysLog`, etc.\n        plot_kwargs : dict\n            See py:meth:`pyneqsys.NeqSys.solve`. Two additional keys\n            are intercepted here:\n                latex_names: bool (default: False)\n                conc_unit_str: str (default: 'M')\n        neqsys_type : str\n            what method to use for NeqSys construction (get_neqsys_*)\n        \\\\*\\\\*kwargs :\n            Keyword argumetns passed on to py:meth:`pyneqsys.NeqSys.solve_series`.\n\n        \"\"\"\n        _plot = plot_kwargs is not None\n        if _plot:\n            latex_names = plot_kwargs.pop('latex_names', False)\n            conc_unit_str = plot_kwargs.pop('conc_unit_str', 'M')\n            if 'ax' not in plot_kwargs:\n                plot_kwargs['ax'] = self._get_default_plot_ax()\n\n        init_concs = self.as_per_substance_array(init_concs)\n        neqsys = self.get_neqsys(\n            neqsys_type, NumSys=NumSys,\n            rref_equil=kwargs.pop('rref_equil', False),\n            rref_preserv=kwargs.pop('rref_preserv', False),\n            precipitates=kwargs.pop('precipitates', None))\n        if x0 is None:\n            x0 = init_concs\n\n        if _plot:\n            cb = neqsys.solve_and_plot_series\n            if 'plot_kwargs' not in kwargs:\n                kwargs['plot_kwargs'] = plot_kwargs\n            if 'labels' not in kwargs['plot_kwargs']:\n                kwargs['plot_kwargs']['labels'] = (\n                    self.substance_labels(latex_names))\n            if 'substances' in plot_kwargs:\n                if 'indices' in plot_kwargs:\n                    raise ValueError(\"Now I am confused..\")\n                kwargs['plot_kwargs']['indices'] = map(\n                    self.as_substance_index, plot_kwargs.pop('substances'))\n                print(kwargs['plot_kwargs']['indices'])\n        else:\n            cb = neqsys.solve_series\n\n        params = np.concatenate((init_concs, self.eq_constants()))\n        xvecs, info_dicts = cb(\n            x0, params, varied_data, self.as_substance_index(varied),\n            propagate=False, **kwargs)\n        sanity = [self._result_is_sane(init_concs, x) for x in xvecs]\n\n        if _plot:\n            import matplotlib.pyplot as plt\n            from pyneqsys.plotting import mpl_outside_legend\n            mpl_outside_legend(plt.gca())\n            varied_subst = self.substances[varied]\n            xlbl = ('$[' + varied_subst.latex_name + ']_0$' if latex_names\n                    else str(varied_subst))\n            plt.gca().set_xlabel(xlbl + ' / ' + conc_unit_str)\n            plt.gca().set_ylabel('Concentration / ' + conc_unit_str)\n\n        return xvecs, info_dicts, sanity", "response": "This method returns a list of roots for a set of substance objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lg_solubility_ratio(electrolytes, gas, units=None, warn=True):\n    if units is None:\n        M = 1\n    else:\n        M = units.molar\n    if warn and 'F-' in electrolytes:\n        warnings.warn(\"In Schumpe 1993: data for fluoride uncertain.\")\n    return sum([(p_gas_rM[gas]/M+p_ion_rM[k]/M)*v for k, v in electrolytes.items()])", "response": "Returns the log10 value of the solubilty ratio of the given electrolytes and gas."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef water_density(T=None, T0=None, units=None, a=None,\n                  just_return_a=False, warn=True):\n    \"\"\"\n    Density of water (kg/m3) as function of temperature (K)\n    according to VSMOW model between 0 and 40 degree Celsius.\n    Fitted using Thiesen's equation.\n\n    Parameters\n    ----------\n    T : float\n        Temperature (in Kelvin) (default: 298.15).\n    T0 : float\n        Value of T for 0 degree Celsius (default: 273.15).\n    units : object (optional)\n        Object with attributes: Kelvin, meter, kilogram.\n    a : array_like (optional)\n        5 parameters to the equation.\n    just_return_a : bool (optional, default: False)\n        Do not compute rho, just return the parameters ``a``.\n    warn : bool (default: True)\n        Emit UserWarning when outside temperature range.\n\n    Returns\n    -------\n    Density of water (float of kg/m3 if T is float and units is None)\n\n    Examples\n    --------\n    >>> print('%.2f' % water_density(277.13))\n    999.97\n\n    References\n    ----------\n    TANAKA M., GIRARD G., DAVIS R., PEUTO A. and BIGNELL N.,\n        \"Recommanded table for the density of water between 0 \u00b0C and 40 \u00b0C\n        based on recent experimental reports\",\n        Metrologia, 2001, 38, 301-309.\n        http://iopscience.iop.org/article/10.1088/0026-1394/38/4/3\n        doi:10.1088/0026-1394/38/4/3\n    \"\"\"\n    if units is None:\n        K = 1\n        m = 1\n        kg = 1\n    else:\n        K = units.Kelvin\n        m = units.meter\n        kg = units.kilogram\n    if T is None:\n        T = 298.15*K\n    m3 = m**3\n    if a is None:\n        a = (-3.983035*K,  # C\n             301.797*K,  # C\n             522528.9*K*K,  # C**2\n             69.34881*K,  # C\n             999.974950*kg/m3)\n    if just_return_a:\n        return a\n    if T0 is None:\n        T0 = 273.15*K\n    t = T - T0\n    if warn and (_any(t < 0*K) or _any(t > 40*K)):\n        warnings.warn(\"Temperature is outside range (0-40 degC)\")\n    return a[4]*(1-((t + a[0])**2*(t + a[1]))/(a[2]*(t + a[3])))", "response": "Returns the density of the water in the specified temperature range."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Henry_H_at_T(T, H, Tderiv, T0=None, units=None, backend=None):\n    be = get_backend(backend)\n    if units is None:\n        K = 1\n    else:\n        K = units.Kelvin\n    if T0 is None:\n        T0 = 298.15*K\n    return H * be.exp(Tderiv*(1/T - 1/T0))", "response": "Evaluate Henry s constant H at temperature T"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mass_from_composition(composition):\n    mass = 0.0\n    for k, v in composition.items():\n        if k == 0:  # electron\n            mass -= v*5.489e-4\n        else:\n            mass += v*relative_atomic_masses[k-1]\n    return mass", "response": "Calculates molecular mass from atomic weights."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef water_viscosity(T=None, eta20=None, units=None, warn=True):\n    if units is None:\n        cP = 1\n        K = 1\n    else:\n        cP = units.centipoise\n        K = units.kelvin\n    if T is None:\n        T = 298.15*K\n    if eta20 is None:\n        eta20 = eta20_cP*cP\n    t = T - 273.15*K\n    if warn and (_any(t < 0*K) or _any(t > 100*K)):\n        warnings.warn(\"Temperature is outside range (0-100 degC)\")\n    # equation (5) in the paper says \"log\" but they seem to mean \"log10\"\n    # when comparing with Table II.\n    return eta20 * 10**((A*(20 - t) - B*(t - 20)**2)/(t + C))", "response": "Returns the water viscosity at a given temperature."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new object containing the self - diffusion coefficient of the given temperature.", "response": "def water_self_diffusion_coefficient(T=None, units=None, warn=True,\n                                     err_mult=None):\n    \"\"\"\n    Temperature-dependent self-diffusion coefficient of water.\n\n    Parameters\n    ----------\n    T : float\n        Temperature (default: in Kelvin)\n    units : object (optional)\n        object with attributes: Kelvin, meter, kilogram\n    warn : bool (default: True)\n        Emit UserWarning when outside temperature range.\n    err_mult : length 2 array_like (default: None)\n        Perturb paramaters D0 and TS with err_mult[0]*dD0 and\n        err_mult[1]*dTS respectively, where dD0 and dTS are the\n        reported uncertainties in the fitted paramters. Useful\n        for estimating error in diffusion coefficient.\n\n    References\n    ----------\n    Temperature-dependent self-diffusion coefficients of water and six selected\n        molecular liquids for calibration in accurate 1H NMR PFG measurements\n        Manfred Holz, Stefan R. Heila, Antonio Saccob;\n        Phys. Chem. Chem. Phys., 2000,2, 4740-4742\n        http://pubs.rsc.org/en/Content/ArticleLanding/2000/CP/b005319h\n        DOI: 10.1039/B005319H\n    \"\"\"\n    if units is None:\n        K = 1\n        m = 1\n        s = 1\n    else:\n        K = units.Kelvin\n        m = units.meter\n        s = units.second\n    if T is None:\n        T = 298.15*K\n    _D0 = D0 * m**2 * s**-1\n    _TS = TS * K\n    if err_mult is not None:\n        _dD0 = dD0 * m**2 * s**-1\n        _dTS = dTS * K\n        _D0 += err_mult[0]*_dD0\n        _TS += err_mult[1]*_dTS\n    if warn and (_any(T < low_t_bound*K) or _any(T > high_t_bound*K)):\n        warnings.warn(\"Temperature is outside range (0-100 degC)\")\n    return _D0*((T/_TS) - 1)**gamma"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rsys2dot(rsys, tex=False, rprefix='r', rref0=1, nodeparams='[label=\"{}\",shape=diamond]',\n             colors=('maroon', 'darkgreen'), penwidths=None, include_inactive=True):\n    \"\"\"\n    Returns list of lines of DOT (graph description language)\n    formated graph.\n\n    Parameters\n    ==========\n    rsys: ReactionSystem\n    tex: bool (default False)\n        If set True, output will be LaTeX formated\n    (Substance need to have latex_name attribute set)\n    rprefix: string\n        Reaction enumeration prefix, default: r\n    rref0: integer\n        Reaction enumeration inital counter value, default: 1\n    nodeparams: string\n        DOT formated param list, default: [label={} shape=diamond]\n\n    Returns\n    =======\n    list of lines of DOT representation of the graph representation.\n\n    \"\"\"\n    lines = ['digraph \"' + str(rsys.name) + '\" {\\n']\n    ind = '  '  # indentation\n    if penwidths is None:\n        penwidths = [1.0]*rsys.nr\n\n    categories = rsys.categorize_substances(checks=())\n\n    def add_substance(key):\n        fc = 'black'\n        if key in categories['depleted']:\n            fc = colors[0]\n        if key in categories['accumulated']:\n            fc = colors[1]\n        label = ('$%s$' if tex else '%s') % getattr(rsys.substances[key], 'latex_name' if tex else 'name')\n        lines.append(ind + '\"{key}\" [fontcolor={fc} label=\"{lbl}\"];\\n'.format(key=key, fc=fc, lbl=label))\n\n    for sk in rsys.substances:\n        add_substance(sk)\n\n    def add_vertex(key, num, reac, penwidth):\n        snum = str(num) if num > 1 else ''\n        fmt = ','.join(\n            ['label=\"{}\"'.format(snum)] +\n            (['penwidth={}'.format(penwidth)] if penwidth != 1 else [])\n        )\n        lines.append(ind + '\"{}\" -> \"{}\" [color={},fontcolor={},{}];\\n'.format(\n            *((key, rid, colors[0], colors[0], fmt) if reac else\n              (rid, key, colors[1], colors[1], fmt))\n        ))\n\n    if include_inactive:\n        reac_stoichs = rsys.all_reac_stoichs()\n        prod_stoichs = rsys.all_prod_stoichs()\n    else:\n        reac_stoichs = rsys.active_reac_stoichs()\n        prod_stoichs = rsys.active_prod_stoichs()\n\n    for ri, rxn in enumerate(rsys.rxns):\n        rid = rprefix + str(ri+rref0)\n        lines.append(ind + '{')\n        lines.append(ind*2 + 'node ' + nodeparams.format(rxn.name or rid))\n        lines.append(ind*2 + rid)\n        lines.append(ind + '}\\n')\n        for idx, key in enumerate(rsys.substances):\n            num = reac_stoichs[ri, idx]\n            if num == 0:\n                continue\n            add_vertex(key, num, True, penwidths[ri])\n        for idx, key in enumerate(rsys.substances):\n            num = prod_stoichs[ri, idx]\n            if num == 0:\n                continue\n            add_vertex(key, num, False, penwidths[ri])\n    lines.append('}\\n')\n    return lines", "response": "Returns a DOT representation of a ReactionSystem."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rsys2graph(rsys, fname, output_dir=None, prog=None, save=False, **kwargs):\n\n    lines = rsys2dot(rsys, **kwargs)\n    created_tempdir = False\n    try:\n        if output_dir is None:\n            output_dir = tempfile.mkdtemp()\n            created_tempdir = True\n        basename, ext = os.path.splitext(os.path.basename(fname))\n        outpath = os.path.join(output_dir, fname)\n        dotpath = os.path.join(output_dir, basename + '.dot')\n        with open(dotpath, 'wt') as ofh:\n            ofh.writelines(lines)\n        if ext == '.tex':\n            cmds = [prog or 'dot2tex']\n        else:\n            cmds = [prog or 'dot', '-T'+outpath.split('.')[-1]]\n        p = subprocess.Popen(cmds + [dotpath, '-o', outpath])\n        retcode = p.wait()\n        if retcode:\n            fmtstr = \"{}\\n returned with exit status {}\"\n            raise RuntimeError(fmtstr.format(' '.join(cmds), retcode))\n        return outpath\n    finally:\n        if save is True or save == 'True':\n            pass\n        else:\n            if save is False or save == 'False':\n                if created_tempdir:\n                    shutil.rmtree(output_dir)\n            else:\n                # interpret save as path to copy pdf to.\n                shutil.copy(outpath, save)", "response": "Convenience function to call rsys2dot and write output to file\n    and render the graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_permission_safety(path):\n    f_stats = os.stat(path)\n    return (f_stats.st_mode & (stat.S_IRWXG | stat.S_IRWXO)) == 0 and f_stats.st_uid == os.getuid()", "response": "Check if the given path is safe to use as a state file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_private_key(key_path, password_path=None):\n\n    assert key_path, key_path\n    if not os.path.exists(key_path):\n        log.fatal('%s: no such file', key_path)\n        return None\n\n    if not check_permission_safety(key_path):\n        log.fatal('Private key file %s must be readable only by its owner.', key_path)\n        return None\n\n    if password_path and not check_permission_safety(password_path):\n        log.fatal('Password file %s must be readable only by its owner.', password_path)\n        return None\n\n    with open(key_path) as keyfile:\n        private_key = keyfile.readline().strip()\n\n        if is_hex(private_key) and len(decode_hex(private_key)) == 32:\n            log.warning('Private key in raw format. Consider switching to JSON-encoded')\n        else:\n            keyfile.seek(0)\n            try:\n                json_data = json.load(keyfile)\n                if password_path:\n                    with open(password_path) as password_file:\n                        password = password_file.readline().strip()\n                else:\n                    password = getpass.getpass('Enter the private key password: ')\n                if json_data['crypto']['kdf'] == 'pbkdf2':\n                    password = password.encode()  # type: ignore\n                private_key = encode_hex(decode_keyfile_json(json_data, password))\n            except ValueError:\n                log.fatal('Invalid private key format or password!')\n                return None\n\n    return private_key", "response": "Open a JSON - encoded private key and return it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrap around to_be_called. transact that waits until the transaction succeeds.", "response": "def transact(\n            self,\n            contract_method: ContractFunction,\n    ):\n        \"\"\" A wrapper around to_be_called.transact() that waits until the transaction succeeds. \"\"\"\n        txhash = contract_method.transact(self.transaction)\n        LOG.debug(f'Sending txHash={encode_hex(txhash)}')\n        (receipt, _) = check_successful_tx(\n            web3=self.web3,\n            txid=txhash,\n            timeout=self.wait,\n        )\n        return receipt"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef deploy_token_contract(\n            self,\n            token_supply: int,\n            token_decimals: int,\n            token_name: str,\n            token_symbol: str,\n            token_type: str = 'CustomToken',\n    ):\n        \"\"\"Deploy a token contract.\"\"\"\n        receipt = self.deploy(\n            contract_name=token_type,\n            args=[token_supply, token_decimals, token_name, token_symbol],\n        )\n        token_address = receipt['contractAddress']\n        assert token_address and is_address(token_address)\n        token_address = to_checksum_address(token_address)\n        return {token_type: token_address}", "response": "Deploy a token contract."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeploying all required raiden contracts and return a dict of contract_name : address", "response": "def deploy_raiden_contracts(\n            self,\n            max_num_of_token_networks: Optional[int],\n    ) -> DeployedContracts:\n        \"\"\" Deploy all required raiden contracts and return a dict of contract_name:address\n\n        Args:\n            max_num_of_token_networks (Optional[int]): The max number of tokens that can be\n            registered to the TokenNetworkRegistry. If None, the argument is omitted from\n            the call to the constructor of TokenNetworkRegistry.\n        \"\"\"\n\n        deployed_contracts: DeployedContracts = {\n            'contracts_version': self.contract_version_string(),\n            'chain_id': int(self.web3.version.network),\n            'contracts': {},\n        }\n\n        self._deploy_and_remember(CONTRACT_ENDPOINT_REGISTRY, [], deployed_contracts)\n        secret_registry = self._deploy_and_remember(\n            contract_name=CONTRACT_SECRET_REGISTRY,\n            arguments=[],\n            deployed_contracts=deployed_contracts,\n        )\n        token_network_registry_args = [\n            secret_registry.address,\n            deployed_contracts['chain_id'],\n            DEPLOY_SETTLE_TIMEOUT_MIN,\n            DEPLOY_SETTLE_TIMEOUT_MAX,\n        ]\n        if max_num_of_token_networks:\n            token_network_registry_args.append(max_num_of_token_networks)\n        self._deploy_and_remember(\n            contract_name=CONTRACT_TOKEN_NETWORK_REGISTRY,\n            arguments=token_network_registry_args,\n            deployed_contracts=deployed_contracts,\n        )\n\n        return deployed_contracts"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _deploy_and_remember(\n            self,\n            contract_name: str,\n            arguments: List,\n            deployed_contracts: 'DeployedContracts',\n    ) -> Contract:\n        \"\"\" Deploys contract_name with arguments and store the result in deployed_contracts. \"\"\"\n        receipt = self.deploy(contract_name, arguments)\n        deployed_contracts['contracts'][contract_name] = _deployed_data_from_receipt(\n            receipt=receipt,\n            constructor_arguments=arguments,\n        )\n        return self.web3.eth.contract(\n            abi=self.contract_manager.get_contract_abi(contract_name),\n            address=deployed_contracts['contracts'][contract_name]['address'],\n        )", "response": "Deploys contract_name with arguments and store the result in deployed_contracts."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register_token_network(\n            self,\n            token_registry_abi: Dict,\n            token_registry_address: str,\n            token_address: str,\n            channel_participant_deposit_limit: Optional[int],\n            token_network_deposit_limit: Optional[int],\n    ):\n        \"\"\"Register token with a TokenNetworkRegistry contract.\"\"\"\n        with_limits = contracts_version_expects_deposit_limits(self.contracts_version)\n        if with_limits:\n            return self._register_token_network_with_limits(\n                token_registry_abi,\n                token_registry_address,\n                token_address,\n                channel_participant_deposit_limit,\n                token_network_deposit_limit,\n            )\n        else:\n            return self._register_token_network_without_limits(\n                token_registry_abi,\n                token_registry_address,\n                token_address,\n                channel_participant_deposit_limit,\n                token_network_deposit_limit,\n            )", "response": "Register a token with a TokenNetworkRegistry contract."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _register_token_network_without_limits(\n            self,\n            token_registry_abi: Dict,\n            token_registry_address: str,\n            token_address: str,\n            channel_participant_deposit_limit: Optional[int],\n            token_network_deposit_limit: Optional[int],\n    ):\n        \"\"\"Register token with a TokenNetworkRegistry contract\n\n        with a contracts-version that doesn't require deposit limits in the TokenNetwork\n        constructor.\n        \"\"\"\n        if channel_participant_deposit_limit:\n            raise ValueError(\n                'contracts_version below 0.9.0 does not expect '\n                'channel_participant_deposit_limit',\n            )\n        if token_network_deposit_limit:\n            raise ValueError(\n                'contracts_version below 0.9.0 does not expect token_network_deposit_limit',\n            )\n        token_network_registry = self.web3.eth.contract(\n            abi=token_registry_abi,\n            address=token_registry_address,\n        )\n\n        version_from_onchain = token_network_registry.functions.contract_version().call()\n        if version_from_onchain != self.contract_manager.version_string:\n            raise RuntimeError(\n                f'got {version_from_onchain} from the chain, expected '\n                f'{self.contract_manager.version_string} in the deployment data',\n            )\n\n        command = token_network_registry.functions.createERC20TokenNetwork(\n            token_address,\n        )\n        self.transact(command)\n\n        token_network_address = token_network_registry.functions.token_to_token_networks(\n            token_address,\n        ).call()\n        token_network_address = to_checksum_address(token_network_address)\n        LOG.debug(f'TokenNetwork address: {token_network_address}')\n        return token_network_address", "response": "Register a token with a TokenNetworkRegistry with a contracts - version that doesn t require deposit limits in the TokenNetworkRegistry constructor."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeploys 3rd party service contracts.", "response": "def deploy_service_contracts(\n            self,\n            token_address: str,\n            user_deposit_whole_balance_limit: int,\n    ):\n        \"\"\"Deploy 3rd party service contracts\"\"\"\n        chain_id = int(self.web3.version.network)\n        deployed_contracts: DeployedContracts = {\n            'contracts_version': self.contract_version_string(),\n            'chain_id': chain_id,\n            'contracts': {},\n        }\n\n        self._deploy_and_remember(CONTRACT_SERVICE_REGISTRY, [token_address], deployed_contracts)\n        user_deposit = self._deploy_and_remember(\n            contract_name=CONTRACT_USER_DEPOSIT,\n            arguments=[token_address, user_deposit_whole_balance_limit],\n            deployed_contracts=deployed_contracts,\n        )\n\n        monitoring_service_constructor_args = [\n            token_address,\n            deployed_contracts['contracts'][CONTRACT_SERVICE_REGISTRY]['address'],\n            deployed_contracts['contracts'][CONTRACT_USER_DEPOSIT]['address'],\n        ]\n        msc = self._deploy_and_remember(\n            contract_name=CONTRACT_MONITORING_SERVICE,\n            arguments=monitoring_service_constructor_args,\n            deployed_contracts=deployed_contracts,\n        )\n\n        one_to_n = self._deploy_and_remember(\n            contract_name=CONTRACT_ONE_TO_N,\n            arguments=[user_deposit.address, chain_id],\n            deployed_contracts=deployed_contracts,\n        )\n\n        # Tell the UserDeposit instance about other contracts.\n        LOG.debug(\n            'Calling UserDeposit.init() with '\n            f'msc_address={msc.address} '\n            f'one_to_n_address={one_to_n.address}',\n        )\n        self.transact(user_deposit.functions.init(\n            _msc_address=msc.address,\n            _one_to_n_address=one_to_n.address,\n        ))\n\n        return deployed_contracts"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a private key to an Ethereum address.", "response": "def private_key_to_address(private_key: Union[str, bytes]) -> ChecksumAddress:\n    \"\"\" Converts a private key to an Ethereum address. \"\"\"\n    if isinstance(private_key, str):\n        private_key_bytes = to_bytes(hexstr=private_key)\n    else:\n        private_key_bytes = private_key\n    pk = PrivateKey(private_key_bytes)\n    return public_key_to_address(pk.public_key)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a public key to an Ethereum address.", "response": "def public_key_to_address(public_key: Union[PublicKey, bytes]) -> ChecksumAddress:\n    \"\"\" Converts a public key to an Ethereum address. \"\"\"\n    if isinstance(public_key, PublicKey):\n        public_key = public_key.format(compressed=False)\n    assert isinstance(public_key, bytes)\n    return to_checksum_address(sha3(public_key[1:])[-20:])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _handle_waited_log(self, event: dict):\n        txn_hash = event['transactionHash']\n        event_name = event['event']\n        assert event_name in self.event_waiting\n        assert txn_hash in self.event_waiting[event_name]\n\n        self.event_count[event_name][txn_hash] += 1\n        event_entry = self.event_waiting[event_name][txn_hash]\n\n        if event_entry.count == self.event_count[event_name][txn_hash]:\n            self.event_waiting[event_name].pop(txn_hash)\n\n        # Call callback function with event\n        if event_entry.callback:\n            event_entry.callback(event)", "response": "A subroutine of handle_log_waiting that increments self. event_count and forget about waiting and call the callback function with the event if any."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef assert_event(self, txn_hash, event_name, args, timeout=5):\n        def assert_args(event):\n            assert event['args'] == args, f'{event[\"args\"]} == {args}'\n        self.add(txn_hash=txn_hash, event_name=event_name, callback=assert_args)\n        self.check(timeout=timeout)", "response": "Assert that an event is emitted with the args."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuses join - contracts. py to concatenate all imported Solidity files.", "response": "def join_sources(source_module: DeploymentModule, contract_name: str):\n    \"\"\" Use join-contracts.py to concatenate all imported Solidity files.\n\n    Args:\n        source_module: a module name to look up contracts_source_path()\n        contract_name: 'TokenNetworkRegistry', 'SecretRegistry' etc.\n    \"\"\"\n    joined_file = Path(__file__).parent.joinpath('joined.sol')\n    remapping = {module: str(path) for module, path in contracts_source_path().items()}\n    command = [\n        './utils/join-contracts.py',\n        '--import-map',\n        json.dumps(remapping),\n        str(contracts_source_path_of_deployment_module(\n            source_module,\n        ).joinpath(contract_name + '.sol')),\n        str(joined_file),\n    ]\n    working_dir = Path(__file__).parent.parent\n    try:\n        subprocess.check_call(command, cwd=working_dir)\n    except subprocess.CalledProcessError as ex:\n        print(f'cd {str(working_dir)}; {subprocess.list2cmdline(command)} failed.')\n        raise ex\n\n    return joined_file.read_text()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef etherscan_verify_contract(\n        chain_id: int,\n        apikey: str,\n        source_module: DeploymentModule,\n        contract_name: str,\n):\n    \"\"\" Calls Etherscan API for verifying the Solidity source of a contract.\n\n    Args:\n        chain_id: EIP-155 chain id of the Ethereum chain\n        apikey: key for calling Etherscan API\n        source_module: a module name to look up contracts_source_path()\n        contract_name: 'TokenNetworkRegistry', 'SecretRegistry' etc.\n    \"\"\"\n    etherscan_api = api_of_chain_id[chain_id]\n    deployment_info = get_contracts_deployment_info(\n        chain_id=chain_id,\n        module=source_module,\n    )\n    if deployment_info is None:\n        raise FileNotFoundError(\n            f'Deployment file not found for chain_id={chain_id} and module={source_module}',\n        )\n    contract_manager = ContractManager(contracts_precompiled_path())\n\n    data = post_data_for_etherscan_verification(\n        apikey=apikey,\n        deployment_info=deployment_info['contracts'][contract_name],\n        source=join_sources(source_module=source_module, contract_name=contract_name),\n        contract_name=contract_name,\n        metadata=json.loads(contract_manager.contracts[contract_name]['metadata']),\n        constructor_args=get_constructor_args(\n            deployment_info=deployment_info,\n            contract_name=contract_name,\n            contract_manager=contract_manager,\n        ),\n    )\n    response = requests.post(etherscan_api, data=data)\n    content = json.loads(response.content.decode())\n    print(content)\n    print(f'Status: {content[\"status\"]}; {content[\"message\"]} ; GUID = {content[\"result\"]}')\n\n    etherscan_url = etherscan_api.replace('api-', '').replace('api', '')\n    etherscan_url += '/verifyContract2?a=' + data['contractaddress']\n    manual_submission_guide = f\"\"\"Usually a manual submission to Etherscan works.\n    Visit {etherscan_url}\n    Use raiden_contracts/deploy/joined.sol.\"\"\"\n\n    if content['status'] != '1':\n        if content['result'] == 'Contract source code already verified':\n            return\n        else:\n            raise ValueError(\n                'Etherscan submission failed for an unknown reason\\n' +\n                manual_submission_guide,\n            )\n\n    # submission succeeded, obtained GUID\n    guid = content['result']\n    status = '0'\n    retries = 10\n    while status == '0' and retries > 0:\n        retries -= 1\n        r = guid_status(etherscan_api=etherscan_api, guid=guid)\n        status = r['status']\n        if r['result'] == 'Fail - Unable to verify':\n            raise ValueError(manual_submission_guide)\n        if r['result'] == 'Pass - Verified':\n            return\n        print('Retrying...')\n        sleep(5)\n    raise TimeoutError(manual_submission_guide)", "response": "Calls Etherscan API to verify the Solidity source of a contract."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef error_removed_option(message: str):\n    def f(_, param, value):\n        if value is not None:\n            raise click.NoSuchOption(\n                f'--{param.name.replace(\"_\", \"-\")} is no longer a valid option. ' +\n                message,\n            )\n    return f", "response": "Returns a callback that raises NoSuchOption if the value is None."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef common_options(func):\n    @click.option(\n        '--private-key',\n        required=True,\n        help='Path to a private key store.',\n    )\n    @click.option(\n        '--rpc-provider',\n        default='http://127.0.0.1:8545',\n        help='Address of the Ethereum RPC provider',\n    )\n    @click.option(\n        '--wait',\n        default=300,\n        help='Max tx wait time in s.',\n    )\n    @click.option(\n        '--gas-price',\n        default=5,\n        type=int,\n        help='Gas price to use in gwei',\n    )\n    @click.option(\n        '--gas-limit',\n        default=5_500_000,\n    )\n    @click.option(\n        '--contracts-version',\n        default=None,\n        help='Contracts version to verify. Current version will be used by default.',\n    )\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        return func(*args, **kwargs)\n    return wrapper", "response": "A decorator that combines commonly appearing @click. option decorators."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting up deployment context according to common options (shared among all subcommands).", "response": "def setup_ctx(\n        ctx: click.Context,\n        private_key: str,\n        rpc_provider: str,\n        wait: int,\n        gas_price: int,\n        gas_limit: int,\n        contracts_version: None = None,\n):\n    \"\"\"Set up deployment context according to common options (shared among all\n    subcommands).\n    \"\"\"\n\n    if private_key is None:\n        return\n    logging.basicConfig(level=logging.DEBUG)\n    logging.getLogger('web3').setLevel(logging.INFO)\n    logging.getLogger('urllib3').setLevel(logging.INFO)\n\n    web3 = Web3(HTTPProvider(rpc_provider, request_kwargs={'timeout': 60}))\n    web3.middleware_stack.inject(geth_poa_middleware, layer=0)\n    print('Web3 provider is', web3.providers[0])\n    private_key = get_private_key(private_key)\n    assert private_key is not None\n    owner = private_key_to_address(private_key)\n    # pylint: disable=E1101\n    assert web3.eth.getBalance(owner) > 0, 'Account with insuficient funds.'\n    deployer = ContractDeployer(\n        web3=web3,\n        private_key=private_key,\n        gas_limit=gas_limit,\n        gas_price=gas_price,\n        wait=wait,\n        contracts_version=contracts_version,\n    )\n    ctx.obj = {}\n    ctx.obj['deployer'] = deployer\n    ctx.obj['deployed_contracts'] = {}\n    ctx.obj['token_type'] = 'CustomToken'\n    ctx.obj['wait'] = wait"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncompiles all solidity contracts into ABI and BIN.", "response": "def _compile_all_contracts(self) -> Dict:\n        \"\"\"\n        Compile solidity contracts into ABI and BIN. This requires solc somewhere in the $PATH\n        and also the :ref:`ethereum.tools` python library.  The return value is a dict that\n        should be written into contracts.json.\n        \"\"\"\n        ret = {}\n        old_working_dir = Path.cwd()\n        chdir(_BASE)\n\n        def relativise(path):\n            return path.relative_to(_BASE)\n        import_dir_map = [\n            '%s=%s' % (k, relativise(v))\n            for k, v in self.contracts_source_dirs.items()\n        ]\n        import_dir_map.insert(0, '.=.')  # allow solc to compile contracts in all subdirs\n        try:\n            for contracts_dir in self.contracts_source_dirs.values():\n                res = compile_files(\n                    [str(relativise(file)) for file in contracts_dir.glob('*.sol')],\n                    output_values=PRECOMPILED_DATA_FIELDS + ['ast'],\n                    import_remappings=import_dir_map,\n                    optimize=False,\n                )\n\n                # Strip `ast` part from result\n                # TODO: Remove after https://github.com/ethereum/py-solc/issues/56 is fixed\n                res = {\n                    contract_name: {\n                        content_key: content_value\n                        for content_key, content_value in contract_content.items()\n                        if content_key != 'ast'\n                    } for contract_name, contract_content in res.items()\n                }\n                ret.update(_fix_contract_key_names(res))\n        except FileNotFoundError as ex:\n            raise ContractSourceManagerCompilationError(\n                'Could not compile the contract. Check that solc is available.',\n            ) from ex\n        finally:\n            chdir(old_working_dir)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compile_contracts(self, target_path: Path) -> ContractManager:\n        self.checksum_contracts()\n\n        if self.overall_checksum is None:\n            raise ContractSourceManagerCompilationError('Checksumming failed.')\n\n        contracts_compiled = self._compile_all_contracts()\n\n        target_path.parent.mkdir(parents=True, exist_ok=True)\n        with target_path.open(mode='w') as target_file:\n            target_file.write(\n                json.dumps(\n                    dict(\n                        contracts=contracts_compiled,\n                        contracts_checksums=self.contracts_checksums,\n                        overall_checksum=self.overall_checksum,\n                        contracts_version=None,\n                    ),\n                    sort_keys=True,\n                    indent=4,\n                ),\n            )\n\n        return ContractManager(target_path)", "response": "Compile all contracts and store them in a JSON file at target_path."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nverifies that the checksums of the source code and overall source code are the same as those from a precompiled file.", "response": "def verify_precompiled_checksums(self, precompiled_path: Path) -> None:\n        \"\"\" Compare source code checksums with those from a precompiled file. \"\"\"\n\n        # We get the precompiled file data\n        contracts_precompiled = ContractManager(precompiled_path)\n\n        # Silence mypy\n        assert self.contracts_checksums is not None\n\n        # Compare each contract source code checksum with the one from the precompiled file\n        for contract, checksum in self.contracts_checksums.items():\n            try:\n                # Silence mypy\n                assert contracts_precompiled.contracts_checksums is not None\n                precompiled_checksum = contracts_precompiled.contracts_checksums[contract]\n            except KeyError:\n                raise ContractSourceManagerVerificationError(\n                    f'No checksum for {contract}',\n                )\n            if precompiled_checksum != checksum:\n                raise ContractSourceManagerVerificationError(\n                    f'checksum of {contract} does not match {precompiled_checksum} != {checksum}',\n                )\n\n        # Compare the overall source code checksum with the one from the precompiled file\n        if self.overall_checksum != contracts_precompiled.overall_checksum:\n            raise ContractSourceManagerVerificationError(\n                f'overall checksum does not match '\n                f'{self.overall_checksum} != {contracts_precompiled.overall_checksum}',\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef checksum_contracts(self) -> None:\n        checksums: Dict[str, str] = {}\n        for contracts_dir in self.contracts_source_dirs.values():\n            file: Path\n            for file in contracts_dir.glob('*.sol'):\n                checksums[file.name] = hashlib.sha256(file.read_bytes()).hexdigest()\n\n        self.overall_checksum = hashlib.sha256(\n            ':'.join(checksums[key] for key in sorted(checksums)).encode(),\n        ).hexdigest()\n        self.contracts_checksums = checksums", "response": "Remember the checksum of each source and the overall checksum."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _hash_pair(first: bytes, second: bytes) -> bytes:\n    if first is None:\n        return second\n\n    if second is None:\n        return first\n\n    if first > second:\n        return keccak(second + first)\n    else:\n        return keccak(first + second)", "response": "Computes the hash of the items in lexicographic order."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute_merkle_tree(items: Iterable[bytes]) -> MerkleTree:\n\n    if not all(isinstance(l, bytes) and len(l) == 32 for l in items):\n        raise ValueError('Not all items are hashes')\n\n    leaves = sorted(items)\n    if len(leaves) == 0:\n        return MerkleTree(layers=[[EMPTY_MERKLE_ROOT]])\n\n    if not len(leaves) == len(set(leaves)):\n        raise ValueError('The leaves items must not contain duplicate items')\n\n    tree = [leaves]\n    layer = leaves\n    while len(layer) > 1:\n        # [a, b, c, d, e] -> [(a, b), (c, d), (e, None)]\n        iterator = iter(layer)\n        paired_items = zip_longest(iterator, iterator)\n\n        layer = [_hash_pair(a, b) for a, b in paired_items]\n        tree.append(layer)\n\n    return MerkleTree(layers=tree)", "response": "Computes the merkle root for a given list of items."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the root element of the merkle tree.", "response": "def get_merkle_root(merkle_tree: MerkleTree) -> bytes:\n    \"\"\" Returns the root element of the merkle tree. \"\"\"\n    assert merkle_tree.layers, 'the merkle tree layers are empty'\n    assert merkle_tree.layers[-1], 'the root layer is empty'\n\n    return merkle_tree.layers[-1][0]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef contracts_version_expects_deposit_limits(contracts_version: Optional[str]) -> bool:\n    if contracts_version is None:\n        return True\n    if contracts_version == '0.3._':\n        return False\n    return compare(contracts_version, '0.9.0') > -1", "response": "Answers whether TokenNetworkRegistry of the contracts_vesion needs deposit limits."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_successful_tx(web3: Web3, txid: str, timeout=180) -> Tuple[dict, dict]:\n    receipt = wait_for_transaction_receipt(web3=web3, txid=txid, timeout=timeout)\n    txinfo = web3.eth.getTransaction(txid)\n    if 'status' not in receipt:\n        raise KeyError(\n            'A transaction receipt does not contain the \"status\" field. '\n            'Does your chain have Byzantium rules enabled?',\n        )\n    if receipt['status'] == 0:\n        raise ValueError(f'Status 0 indicates failure')\n    if txinfo['gas'] == receipt['gasUsed']:\n        raise ValueError(f'Gas is completely used ({txinfo[\"gas\"]}). Failure?')\n    return (receipt, txinfo)", "response": "Check if a transaction was successfully sent to the chain."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nverifies that the deployed contract is correctly set up.", "response": "def _verify_deployed_contract(\n            self,\n            deployment_data: DeployedContracts,\n            contract_name: str,\n    ) -> Contract:\n        \"\"\" Verify deployment info against the chain\n\n        Verifies:\n        - the runtime bytecode - precompiled data against the chain\n        - information stored in deployment_*.json against the chain,\n        except for the constructor arguments, which have to be checked\n        separately.\n\n        Returns: (onchain_instance, constructor_arguments)\n        \"\"\"\n        contracts = deployment_data['contracts']\n\n        contract_address = contracts[contract_name]['address']\n        contract_instance = self.web3.eth.contract(\n            abi=self.contract_manager.get_contract_abi(contract_name),\n            address=contract_address,\n        )\n\n        # Check that the deployed bytecode matches the precompiled data\n        blockchain_bytecode = self.web3.eth.getCode(contract_address).hex()\n        compiled_bytecode = self.contract_manager.get_runtime_hexcode(contract_name)\n        assert blockchain_bytecode == compiled_bytecode\n\n        print(\n            f'{contract_name} at {contract_address} '\n            f'matches the compiled data from contracts.json',\n        )\n\n        # Check blockchain transaction hash & block information\n        receipt = self.web3.eth.getTransactionReceipt(\n            contracts[contract_name]['transaction_hash'],\n        )\n        assert receipt['blockNumber'] == contracts[contract_name]['block_number'], (\n            f'We have block_number {contracts[contract_name][\"block_number\"]} in the deployment '\n            f'info, but {receipt[\"blockNumber\"]} in the transaction receipt from web3.'\n        )\n        assert receipt['gasUsed'] == contracts[contract_name]['gas_cost'], (\n            f'We have gasUsed {contracts[contract_name][\"gas_cost\"]} in the deployment info, '\n            f'but {receipt[\"gasUsed\"]} in the transaction receipt from web3.'\n        )\n        assert receipt['contractAddress'] == contracts[contract_name]['address'], (\n            f'We have contractAddress {contracts[contract_name][\"address\"]} in the deployment info'\n            f' but {receipt[\"contractAddress\"]} in the transaction receipt from web3.'\n        )\n\n        # Check the contract version\n        version = contract_instance.functions.contract_version().call()\n        assert version == deployment_data['contracts_version'], \\\n            f'got {version} expected {deployment_data[\"contracts_version\"]}.' \\\n            f'contract_manager has contracts_version {self.contract_manager.contracts_version}'\n\n        return contract_instance, contracts[contract_name]['constructor_arguments']"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef contracts_data_path(version: Optional[str] = None):\n    if version is None:\n        return _BASE.joinpath('data')\n    return _BASE.joinpath(f'data_{version}')", "response": "Returns the deployment data directory for a version."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef contracts_precompiled_path(version: Optional[str] = None) -> Path:\n    data_path = contracts_data_path(version)\n    return data_path.joinpath('contracts.json')", "response": "Returns the path of the JSON file where the bytecode can be found."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the path of the deplolyment data JSON file.", "response": "def contracts_deployed_path(\n        chain_id: int,\n        version: Optional[str] = None,\n        services: bool = False,\n):\n    \"\"\"Returns the path of the deplolyment data JSON file.\"\"\"\n    data_path = contracts_data_path(version)\n    chain_name = ID_TO_NETWORKNAME[chain_id] if chain_id in ID_TO_NETWORKNAME else 'private_net'\n\n    return data_path.joinpath(f'deployment_{\"services_\" if services else \"\"}{chain_name}.json')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmerging two deployment JSON files into one.", "response": "def merge_deployment_data(dict1: DeployedContracts, dict2: DeployedContracts) -> DeployedContracts:\n    \"\"\" Take contents of two deployment JSON files and merge them\n\n    The dictionary under 'contracts' key will be merged. The 'contracts'\n    contents from different JSON files must not overlap. The contents\n    under other keys must be identical.\n    \"\"\"\n    if not dict1:\n        return dict2\n    if not dict2:\n        return dict1\n    common_contracts: Dict[str, DeployedContract] = deepcopy(dict1['contracts'])\n    assert not common_contracts.keys() & dict2['contracts'].keys()\n    common_contracts.update(dict2['contracts'])\n\n    assert dict2['chain_id'] == dict1['chain_id']\n    assert dict2['contracts_version'] == dict1['contracts_version']\n\n    return {\n        'contracts': common_contracts,\n        'chain_id': dict1['chain_id'],\n        'contracts_version': dict1['contracts_version'],\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads the deployment data for the specified version of the specified module. Returns None if the file is not found.", "response": "def get_contracts_deployment_info(\n        chain_id: int,\n        version: Optional[str] = None,\n        module: DeploymentModule = DeploymentModule.ALL,\n) -> Optional[DeployedContracts]:\n    \"\"\"Reads the deployment data. Returns None if the file is not found.\n\n    Parameter:\n        module The name of the module. ALL means deployed contracts from all modules that are\n        available for the version.\n    \"\"\"\n    if module not in DeploymentModule:\n        raise ValueError(f'Unknown module {module} given to get_contracts_deployment_info()')\n\n    def module_chosen(to_be_added: DeploymentModule):\n        return module == to_be_added or module == DeploymentModule.ALL\n\n    files: List[Path] = []\n\n    if module_chosen(DeploymentModule.RAIDEN):\n        files.append(contracts_deployed_path(\n            chain_id=chain_id,\n            version=version,\n            services=False,\n        ))\n\n    if module == DeploymentModule.SERVICES and not version_provides_services(version):\n        raise ValueError(\n            f'SERVICES module queried for version {version}, but {version} '\n            'does not provide service contracts.',\n        )\n\n    if module_chosen(DeploymentModule.SERVICES) and version_provides_services(version):\n        files.append(contracts_deployed_path(\n            chain_id=chain_id,\n            version=version,\n            services=True,\n        ))\n\n    deployment_data: DeployedContracts = {}  # type: ignore\n\n    for f in files:\n        deployment_data = merge_deployment_data(\n            deployment_data,\n            _load_json_from_path(f),\n        )\n\n    if not deployment_data:\n        deployment_data = None\n    return deployment_data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the ABI and BIN of the given contract.", "response": "def get_contract(self, contract_name: str) -> Dict:\n        \"\"\" Return ABI, BIN of the given contract. \"\"\"\n        assert self.contracts, 'ContractManager should have contracts compiled'\n        return self.contracts[contract_name]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the ABI for a given contract.", "response": "def get_contract_abi(self, contract_name: str) -> Dict:\n        \"\"\" Returns the ABI for a given contract. \"\"\"\n        assert self.contracts, 'ContractManager should have contracts compiled'\n        return self.contracts[contract_name]['abi']"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_event_abi(self, contract_name: str, event_name: str) -> Dict:\n        # Import locally to avoid web3 dependency during installation via `compile_contracts`\n        from web3.utils.contracts import find_matching_event_abi\n\n        assert self.contracts, 'ContractManager should have contracts compiled'\n        contract_abi = self.get_contract_abi(contract_name)\n        return find_matching_event_abi(\n            abi=contract_abi,\n            event_name=event_name,\n        )", "response": "Returns the ABI for a given event."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef string(s, salt=None):\n        m = hashlib.md5()\n        s = s.encode('utf-8') + salt.encode('utf-8') if salt is not None else s.encode('utf-8')\n        m.update(s)\n        result = m.hexdigest()\n        return result", "response": "Return a 32 byte string from a string"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef file(filename):\n        m = hashlib.md5()\n        with open(filename, 'rb') as f:\n            m.update(f.read())\n            result = m.hexdigest()\n            return result", "response": "Returns 32\u4f4d\u5c0f\u5199 MD5 \u503c"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning 32\u4f4d\u5c0f\u5199 MD5 \u503c", "response": "def big_file(filename):\n        \"\"\"\n        \u83b7\u53d6\u4e00\u4e2a\u5927\u6587\u4ef6\u7684 MD5 \u503c\n\n        :param:\n            * filename: (string) \u9700\u8981\u8fdb\u884c hash \u7684\u5927\u6587\u4ef6\u8def\u5f84\n        :return:\n            * result: (string) 32\u4f4d\u5c0f\u5199 MD5 \u503c\n        \"\"\"\n        \n        md5 = hashlib.md5()\n        with open(filename, 'rb') as f:\n            for chunk in iter(lambda: f.read(8192), b''):\n                md5.update(chunk)\n        \n        result = md5.hexdigest()\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hmac_md5(s, salt):\n        hmac_md5 = hmac.new(salt.encode('utf-8'), s.encode('utf-8'),\n                            digestmod=hashlib.md5).hexdigest()\n        return hmac_md5", "response": "Returns 32\u4f4d\u5c0f\u5199 MD5 \u503c"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a sha256 hash of the message", "response": "def hmac_sha256(secret, message):\n        \"\"\"\n        \u83b7\u53d6\u4e00\u4e2a\u5b57\u7b26\u4e32\u7684\u5728\u5bc6\u94a5 secret \u52a0\u5bc6\u4e0b\u7684 sha256 \u54c8\u5e0c\u503c\n\n        :param:\n            * secret: (string) \u54c8\u5e0c\u7b97\u6cd5\u7684\u5bc6\u94a5\n            * message: (string) \u9700\u8981\u8fdb\u884c\u54c8\u5e0c\u7684\u5b57\u7b26\u4e32\n        :return:\n            * hashed_str: sha256 \u7b97\u6cd5\u54c8\u5e0c\u503c\n        \"\"\"\n        hashed_str = hmac.new(secret.encode('utf-8'),\n                              message.encode('utf-8'),\n                              digestmod=hashlib.sha256).hexdigest()\n        return hashed_str"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hashlib_sha256(message):\n        hashlib_sha256 = hashlib.sha256()\n        hashlib_sha256.update(message.encode('utf-8'))\n        hashed_str = hashlib_sha256.hexdigest()\n        return hashed_str", "response": "Returns a sha256 hash of the given message."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gen_random_str(min_length, max_length, prefix=None, suffix=None,\n                   has_letter=True, has_digit=False, has_punctuation=False):\n    \"\"\"\n    \u6307\u5b9a\u4e00\u4e2a\u524d\u540e\u7f00\u3001\u5b57\u7b26\u4e32\u957f\u5ea6\u4ee5\u53ca\u5b57\u7b26\u4e32\u5305\u542b\u5b57\u7b26\u7c7b\u578b\uff0c\u8fd4\u56de\u968f\u673a\u751f\u6210\u5e26\u6709\u524d\u540e\u7f00\u53ca\u6307\u5b9a\u957f\u5ea6\u7684\u5b57\u7b26\u4e32\n\n    :param:\n        * min_length: (int) \u5b57\u7b26\u4e32\u6700\u5c0f\u957f\u5ea6\n        * max_length: (int) \u5b57\u7b26\u4e32\u6700\u5c0f\u957f\u5ea6\n        * prefix: (string) \u5b57\u7b26\u4e32\u524d\u7f00\n        * suffix: (string) \u5b57\u7b26\u4e32\u540e\u7f00\n        * has_letter: (bool) \u5b57\u7b26\u4e32\u65f6\u5019\u5305\u542b\u5b57\u6bcd\uff0c\u9ed8\u8ba4\u4e3a True\n        * has_digit: (bool) \u5b57\u7b26\u4e32\u662f\u5426\u5305\u542b\u6570\u5b57\uff0c\u9ed8\u8ba4\u4e3a False\n        * has_punctuation: (bool) \u5b57\u7b26\u4e32\u662f\u5426\u5305\u542b\u6807\u70b9\u7b26\u53f7\uff0c\u9ed8\u8ba4\u4e3a False\n\n    :return:\n        * random_str: (string) \u6307\u5b9a\u89c4\u5219\u7684\u968f\u673a\u5b57\u7b26\u4e32\n\n    \u4e3e\u4f8b\u5982\u4e0b::\n\n        print('--- gen_random_str demo ---')\n        print(gen_random_str(5, 7))\n        print(gen_random_str(5, 7, prefix='FISHBASE_'))\n        print(gen_random_str(5, 7, prefix='FISHBASE_', suffix='.py'))\n        print(gen_random_str(5, 7, has_digit=True, has_punctuation=True))\n        print(gen_random_str(5, 7, prefix='FISHBASE_', has_digit=True, has_punctuation=True))\n        print('---')\n\n    \u6267\u884c\u7ed3\u679c::\n\n        --- gen_string_by_range demo ---\n        q4uo6E8\n\n        FISHBASE_8uCBEUH\n\n        FISHBASE_D4wRX2.py\n\n        FISHBASE_65nqlNs\n\n        FISHBASE_3\"uFm$s\n        ---\n    \"\"\"\n    if not all([isinstance(min_length, int), isinstance(max_length, int)]):\n        raise ValueError('min_length and max_length should be int, but we got {} and {}'.\n                         format(type(min_length), type(max_length)))\n\n    if min_length > max_length:\n        raise ValueError('min_length should less than or equal to max_length')\n\n    # \u907f\u514d\u968f\u673a\u6e90\u4e3a\u7a7a\n    if not any([has_letter, has_digit, has_punctuation]):\n        raise ValueError('At least one value is True in has_letter, has_digit and has_punctuation')\n\n    random_str_len = random.randint(min_length, max_length)\n\n    random_source = ''\n    random_source += string.ascii_letters if has_letter else ''\n    random_source += string.digits if has_digit else ''\n    random_source += string.punctuation if has_punctuation else ''\n\n    # \u907f\u514d\u51fa\u73b0 ValueError: Sample larger than population or is negative\n    if random_str_len > len(random_source):\n        random_source *= (random_str_len // len(random_source) + 1)\n\n    mid_random_str = ''.join(random.sample(random_source, random_str_len))\n\n    prefix = prefix if prefix else ''\n    suffix = suffix if suffix else ''\n\n    random_str = ''.join([prefix, mid_random_str, suffix])\n\n    return random_str", "response": "Generate random string from the alphabet."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gen_random_name(family_name=None, gender=None, length=None):\n    family_word = (\"\u8d75\u94b1\u5b59\u674e\u5468\u5434\u90d1\u738b\u51af\u9648\u891a\u536b\u848b\u6c88\u97e9\u6768\u6731\u79e6\u5c24\u8bb8\u4f55\u5415\u65bd\u5f20\u5b54\u66f9\u4e25\u534e\u91d1\u9b4f\u9676\u59dc\u621a\u8c22\u90b9\u55bb\u67cf\u6c34\u7aa6\u7ae0\u4e91\u82cf\u6f58\u845b\"\n                   \"\u595a\u8303\u5f6d\u90ce\u9c81\u97e6\u660c\u9a6c\u82d7\u51e4\u82b1\u65b9\u4fde\u4efb\u8881\u67f3\u9146\u9c8d\u53f2\u5510\u8d39\u5ec9\u5c91\u859b\u96f7\u8d3a\u502a\u6c64\u6ed5\u6bb7\u7f57\u6bd5\u90dd\u90ac\u5b89\u5e38\u4e50\u4e8e\u65f6\u5085\u76ae\u535e\u9f50\u5eb7\"\n                   \"\u4f0d\u4f59\u5143\u535c\u987e\u5b5f\u5e73\u9ec4\u548c\u7a46\u8427\u5c39\u59da\u90b5\u6e5b\u6c6a\u7941\u6bdb\u79b9\u72c4\u7c73\u8d1d\u660e\u81e7\u8ba1\u4f0f\u6210\u6234\u8c08\u5b8b\u8305\u5e9e\u718a\u7eaa\u8212\u5c48\u9879\u795d\u8463\u6881\u675c\u962e\u84dd\u95f5\"\n                   \"\u5e2d\u5b63\u9ebb\u5f3a\u8d3e\u8def\u5a04\u5371\u6c5f\u7ae5\u989c\u90ed\u6885\u76db\u6797\u5201\u949f\u5f90\u90b1\u9a86\u9ad8\u590f\u8521\u7530\u6a0a\u80e1\u51cc\u970d\u865e\u4e07\u652f\u67ef\u548e\u7ba1\u5362\u83ab\u7ecf\u623f\u88d8\u7f2a\u5e72\u89e3\u5e94\u5b97\"\n                   \"\u5ba3\u4e01\u8d32\u9093\u90c1\u5355\u676d\u6d2a\u5305\u8bf8\u5de6\u77f3\u5d14\u5409\u94ae\u9f9a\u7a0b\u5d47\u90a2\u6ed1\u88f4\u9646\u8363\u7fc1\u8340\u7f8a\u65bc\u60e0\u7504\u9b4f\u52a0\u5c01\u82ae\u7fbf\u50a8\u9773\u6c72\u90b4\u7cdc\u677e\u4e95\u6bb5\u5bcc\u5deb\"\n                   \"\u4e4c\u7126\u5df4\u5f13\u7267\u9697\u5c71\u8c37\u8f66\u4faf\u5b93\u84ec\u5168\u90d7\u73ed\u4ef0\u79cb\u4ef2\u4f0a\u5bab\u5b81\u4ec7\u683e\u66b4\u7518\u94ad\u5389\u620e\u7956\u6b66\u7b26\u5218\u59dc\u8a79\u675f\u9f99\u53f6\u5e78\u53f8\u97f6\u90dc\u9ece\u84df\u8584\"\n                   \"\u5370\u5bbf\u767d\u6000\u84b2\u53f0\u4ece\u9102\u7d22\u54b8\u7c4d\u8d56\u5353\u853a\u5c60\u8499\u6c60\u4e54\u9634\u90c1\u80e5\u80fd\u82cd\u53cc\u95fb\u8398\u515a\u7fdf\u8c2d\u8d21\u52b3\u9004\u59ec\u7533\u6276\u5835\u5189\u5bb0\u90e6\u96cd\u5374\u74a9\u6851\u6842\"\n                   \"\u6fee\u725b\u5bff\u901a\u8fb9\u6248\u71d5\u5180\u90cf\u6d66\u5c1a\u519c\u6e29\u522b\u5e84\u664f\u67f4\u77bf\u960e\u5145\u6155\u8fde\u8339\u4e60\u5ba6\u827e\u9c7c\u5bb9\u5411\u53e4\u6613\u614e\u6208\u5ed6\u5e9a\u7ec8\u66a8\u5c45\u8861\u6b65\u90fd\u803f\u6ee1\u5f18\"\n                   \"\u5321\u56fd\u6587\u5bc7\u5e7f\u7984\u9619\u4e1c\u6bb4\u6bb3\u6c83\u5229\u851a\u8d8a\u5914\u9686\u5e08\u5de9\u538d\u8042\u6641\u52fe\u6556\u878d\u51b7\u8a3e\u8f9b\u961a\u90a3\u7b80\u9976\u7a7a\u66fe\u6bcb\u6c99\u4e5c\u517b\u97a0\u987b\u4e30\u5de2\u5173\u84af\u76f8\"\n                   \"\u67e5\u540e\u6c5f\u7ea2\u6e38\u7afa\u6743\u902f\u76d6\u76ca\u6853\u516c\u4e07\u4fdf\u53f8\u9a6c\u4e0a\u5b98\u6b27\u9633\u590f\u4faf\u8bf8\u845b\u95fb\u4eba\u4e1c\u65b9\u8d6b\u8fde\u7687\u752b\u5c09\u8fdf\u516c\u7f8a\u6fb9\u53f0\u516c\u51b6\u5b97\u653f\u6fee\u9633\"\n                   \"\u6df3\u4e8e\u4ef2\u5b59\u592a\u53d4\u7533\u5c60\u516c\u5b59\u4e50\u6b63\u8f69\u8f95\u4ee4\u72d0\u949f\u79bb\u95fe\u4e18\u957f\u5b59\u6155\u5bb9\u9c9c\u4e8e\u5b87\u6587\u53f8\u5f92\u53f8\u7a7a\u4e93\u5b98\u53f8\u5bc7\u4ec9\u7763\u5b50\u8f66\u989b\u5b59\u7aef\u6728\"\n                   \"\u5deb\u9a6c\u516c\u897f\u6f06\u96d5\u4e50\u6b63\u58e4\u9a77\u516c\u826f\u62d3\u62d4\u5939\u8c37\u5bb0\u7236\u8c37\u7cb1\u664b\u695a\u960e\u6cd5\u6c5d\u9122\u6d82\u94a6\u6bb5\u5e72\u767e\u91cc\u4e1c\u90ed\u5357\u95e8\u547c\u5ef6\u5f52\u6d77\u7f8a\u820c\u5fae\u751f\"\n                   \"\u5cb3\u5e05\u7f11\u4ea2\u51b5\u540e\u6709\u7434\u6881\u4e18\u5de6\u4e18\u4e1c\u95e8\u897f\u95e8\u5546\u725f\u4f58\u4f74\u4f2f\u8d4f\u5357\u5bab\u58a8\u54c8\u8c2f\u7b2a\u5e74\u7231\u9633\u4f5f\u7b2c\u4e94\u8a00\u798f\u767e\u5bb6\u59d3\u7eed\")\n\n    name_dict = {\"00\": (\"\u79c0\u5a1f\u82f1\u534e\u6167\u5de7\u7f8e\u5a1c\u9759\u6dd1\u60e0\u73e0\u7fe0\u96c5\u829d\u7389\u840d\u7ea2\u5a25\u73b2\u82ac\u82b3\u71d5\u5f69\u6625\u83ca\u5170\u51e4\u6d01\u6885\u7433\u7d20\u4e91\u83b2\u771f\u73af\u96ea\u8363\u7231\u59b9\u971e\"\n                        \"\u9999\u6708\u83ba\u5a9b\u8273\u745e\u51e1\u4f73\u5609\u743c\u52e4\u73cd\u8d1e\u8389\u6842\u5a23\u53f6\u74a7\u7490\u5a05\u7426\u6676\u598d\u831c\u79cb\u73ca\u838e\u9526\u9edb\u9752\u5029\u5a77\u59e3\u5a49\u5a34\u747e\u9896\u9732\u7476\u6021\u5a75\"\n                        \"\u96c1\u84d3\u7ea8\u4eea\u8377\u4e39\u84c9\u7709\u541b\u7434\u854a\u8587\u83c1\u68a6\u5c9a\u82d1\u5a55\u99a8\u7457\u7430\u97f5\u878d\u56ed\u827a\u548f\u537f\u806a\u6f9c\u7eaf\u6bd3\u60a6\u662d\u51b0\u723d\u742c\u8317\u7fbd\u5e0c\u5b81\u6b23\u98d8\"\n                        \"\u80b2\u6ee2\u99a5\u7b60\u67d4\u7af9\u972d\u51dd\u6653\u6b22\u9704\u67ab\u82b8\u83f2\u5bd2\u4f0a\u4e9a\u5b9c\u53ef\u59ec\u8212\u5f71\u8354\u679d\u601d\u4e3d\"),\n                 \"01\": (\"\u4f1f\u521a\u52c7\u6bc5\u4fca\u5cf0\u5f3a\u519b\u5e73\u4fdd\u4e1c\u6587\u8f89\u529b\u660e\u6c38\u5065\u4e16\u5e7f\u5fd7\u4e49\u5174\u826f\u6d77\u5c71\u4ec1\u6ce2\u5b81\u8d35\u798f\u751f\u9f99\u5143\u5168\u56fd\u80dc\u5b66\u7965\u624d\u53d1\u6b66\"\n                        \"\u65b0\u5229\u6e05\u98de\u5f6c\u5bcc\u987a\u4fe1\u5b50\u6770\u6d9b\u660c\u6210\u5eb7\u661f\u5149\u5929\u8fbe\u5b89\u5ca9\u4e2d\u8302\u8fdb\u6797\u6709\u575a\u548c\u5f6a\u535a\u8bda\u5148\u656c\u9707\u632f\u58ee\u4f1a\u601d\u7fa4\u8c6a\u5fc3\u90a6\"\n                        \"\u627f\u4e50\u7ecd\u529f\u677e\u5584\u539a\u5e86\u78ca\u6c11\u53cb\u88d5\u6cb3\u54f2\u6c5f\u8d85\u6d69\u4eae\u653f\u8c26\u4ea8\u5947\u56fa\u4e4b\u8f6e\u7ff0\u6717\u4f2f\u5b8f\u8a00\u82e5\u9e23\u670b\u658c\u6881\u680b\u7ef4\u542f\u514b\u4f26\u7fd4\"\n                        \"\u65ed\u9e4f\u6cfd\u6668\u8fb0\u58eb\u4ee5\u5efa\u5bb6\u81f4\u6811\u708e\u5fb7\u884c\u65f6\u6cf0\u76db\u96c4\u741b\u94a7\u51a0\u7b56\u817e\u6960\u6995\u98ce\u822a\u5f18\")}\n\n    if family_name is None:\n        family_name = random.choice(family_word)\n    if gender is None or gender not in ['00', '01']:\n        gender = random.choice(['00', '01'])\n    if length is None or length not in [2, 3, 4, 5, 6, 7, 8, 9, 10]:\n        length = random.choice([2, 3])\n    name = \"\".join([random.choice(name_dict[gender]) for _ in range(length - 1)])\n    full_name = \"{family_name}{name}\".format(family_name=family_name, name=name)\n    return full_name", "response": "Generate random name for a new node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef gen_random_mobile():\n    prefix_list = [\"13\",\n                   \"1400\", \"1410\", \"1440\", \"145\", \"146\", \"147\", \"148\",\n                   \"15\",\n                   \"162\", \"165\", \"166\", \"167\",\n                   \"170\", \"171\", \"172\", \"173\", \"175\", \"176\", \"177\", \"178\", \"1740\",\n                   \"18\",\n                   \"191\", \"198\", \"199\"]\n    prefix_str = random.choice(prefix_list)\n    return prefix_str + \"\".join(random.choice(\"0123456789\") for _ in range(11 - len(prefix_str)))", "response": "Generate a random string from the list of mobile words."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates random float number generator.", "response": "def gen_random_float(minimum, maximum, decimals=2):\n    \"\"\"\n    \u6307\u5b9a\u4e00\u4e2a\u6d6e\u70b9\u6570\u8303\u56f4\uff0c\u968f\u673a\u751f\u6210\u5e76\u8fd4\u56de\u533a\u95f4\u5185\u7684\u4e00\u4e2a\u6d6e\u70b9\u6570\uff0c\u533a\u95f4\u4e3a\u95ed\u533a\u95f4\n    \u53d7\u9650\u4e8e random.random \u7cbe\u5ea6\u9650\u5236\uff0c\u652f\u6301\u6700\u5927 15 \u4f4d\u7cbe\u5ea6\n\n    :param:\n        * minimum: (float) \u6d6e\u70b9\u6570\u6700\u5c0f\u53d6\u503c\n        * maximum: (float) \u6d6e\u70b9\u6570\u6700\u5927\u53d6\u503c\n        * decimals: (int) \u5c0f\u6570\u4f4d\u6570\uff0c\u9ed8\u8ba4\u4e3a 2 \u4f4d\n\n    :return:\n        * random_float: (float) \u968f\u673a\u6d6e\u70b9\u6570\n\n    \u4e3e\u4f8b\u5982\u4e0b::\n\n        print('--- gen_random_float demo ---')\n        print(gen_random_float(1.0, 9.0))\n        print(gen_random_float(1.0, 9.0, decimals=10))\n        print(gen_random_float(1.0, 9.0, decimals=20))\n        print('---')\n\n    \u6267\u884c\u7ed3\u679c::\n\n        --- gen_random_float demo ---\n        6.08\n        6.8187342239\n        2.137902497554043\n        ---\n\n    \"\"\"\n    if not (isinstance(minimum, float) and isinstance(maximum, float)):\n        raise ValueError('param minimum, maximum should be float, but got minimum: {} maximum: {}'.\n                         format(type(minimum), type(maximum)))\n    if not isinstance(decimals, int):\n        raise ValueError('param decimals should be a int, but we got {}'.format(type(decimals)))\n    # \u7cbe\u5ea6\u76ee\u524d\u53ea\u652f\u6301\u6700\u5927 15 \u4f4d\n    decimals = 15 if decimals > 15 else decimals\n    # \u5b58\u5728 round \u4e4b\u540e\u56db\u820d\u4e94\u5165\u4e4b\u540e\uff0c\u7cbe\u5ea6\u4e0d\u5339\u914d\u7684\u60c5\u51b5\uff0c\u65b0\u52a0\u5224\u65ad\n    while True:\n        random_float = random.uniform(minimum, maximum)\n        random_float = round(random_float, decimals)\n        if len(str(random_float).split('.')[-1]) == decimals:\n            return random_float"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gen_random_address(zone):\n    # \u83b7\u53d6\u7701\u4efd\u4e0b\u7684\u5730\u533a\u4fe1\u606f\n    province = str(zone)[:2]\n    areanote = IdCard.get_areanote_info(province)\n    if not areanote:\n        raise ValueError('zone error, please check and try again')\n    # \u7b2c\u4e00\u9879\u662f\u7701\u4efd\u540d\u79f0\n    province_name = areanote[0][-1]\n    areanote_info = get_random_areanote(zone)\n    address_word = (\"\u91cd\u5e86\u5927\u53a6,\u9ed1\u9f99\u6c5f\u8def,\u5341\u6885\u5eb5\u8857,\u9075\u4e49\u8def,\u6e58\u6f6d\u8857,\u745e\u91d1\u5e7f\u573a,\u4ed9\u5c71\u8857,\u4ed9\u5c71\u4e1c\u8def,\u4ed9\u5c71\u897f\u5927\u53a6,\u767d\u6c99\u6cb3\u8def,\"\n                    \"\u8d75\u7ea2\u5e7f\u573a,\u673a\u573a\u8def,\u6c11\u822a\u8857,\u957f\u57ce\u5357\u8def,\u6d41\u4ead\u7acb\u4ea4\u6865,\u8679\u6865\u5e7f\u573a,\u957f\u57ce\u5927\u53a6,\u793c\u9633\u8def,\u98ce\u5c97\u8857,\u4e2d\u5ddd\u8def,\"\n                    \"\u767d\u5854\u5e7f\u573a,\u5174\u9633\u8def,\u6587\u9633\u8857,\u7ee3\u57ce\u8def,\u6cb3\u57ce\u5927\u53a6,\u9526\u57ce\u5e7f\u573a,\u5d07\u9633\u8857,\u534e\u57ce\u8def,\u5eb7\u57ce\u8857,\u6b63\u9633\u8def,\u548c\u9633\u5e7f\u573a,\"\n                    \"\u4e2d\u57ce\u8def,\u6c5f\u57ce\u5927\u53a6,\u987a\u57ce\u8def,\u5b89\u57ce\u8857,\u5c71\u57ce\u5e7f\u573a,\u6625\u57ce\u8857,\u56fd\u57ce\u8def,\u6cf0\u57ce\u8857,\u5fb7\u9633\u8def,\u660e\u9633\u5927\u53a6,\u6625\u9633\u8def,\"\n                    \"\u8273\u9633\u8857,\u79cb\u9633\u8def,\u7855\u9633\u8857,\u9752\u5a01\u9ad8\u901f,\u745e\u9633\u8857,\u4e30\u6d77\u8def,\u53cc\u5143\u5927\u53a6,\u60dc\u798f\u9547\u8857\u9053,\u590f\u5e84\u8857\u9053,\u53e4\u5e99\u5de5\u4e1a\u56ed,\"\n                    \"\u4e2d\u5c71\u8857,\u592a\u5e73\u8def,\u5e7f\u897f\u8857,\u6f4d\u53bf\u5e7f\u573a,\u535a\u5c71\u5927\u53a6,\u6e56\u5357\u8def,\u6d4e\u5b81\u8857,\u829d\u7f58\u8def,\u6613\u5dde\u5e7f\u573a,\u8377\u6cfd\u56db\u8def,\"\n                    \"\u8377\u6cfd\u4e8c\u8857,\u8377\u6cfd\u4e00\u8def,\u8377\u6cfd\u4e09\u5927\u53a6,\u89c2\u6d77\u4e8c\u5e7f\u573a,\u5e7f\u897f\u652f\u8857,\u89c2\u6d77\u4e00\u8def,\u6d4e\u5b81\u652f\u8857,\u8392\u53bf\u8def,\u5e73\u5ea6\u5e7f\u573a,\"\n                    \"\u660e\u6c34\u8def,\u8499\u9634\u5927\u53a6,\u9752\u5c9b\u8def,\u6e56\u5317\u8857,\u6c5f\u5b81\u5e7f\u573a,\u90ef\u57ce\u8857,\u5929\u6d25\u8def,\u4fdd\u5b9a\u8857,\u5b89\u5fbd\u8def,\u6cb3\u5317\u5927\u53a6,\u9ec4\u5c9b\u8def,\"\n                    \"\u5317\u4eac\u8857,\u8398\u53bf\u8def,\u6d4e\u5357\u8857,\u5b81\u9633\u5e7f\u573a,\u65e5\u7167\u8857,\u5fb7\u53bf\u8def,\u65b0\u6cf0\u5927\u53a6,\u8377\u6cfd\u8def,\u5c71\u897f\u5e7f\u573a,\u6c82\u6c34\u8def,\u80a5\u57ce\u8857,\"\n                    \"\u5170\u5c71\u8def,\u56db\u65b9\u8857,\u5e73\u539f\u5e7f\u573a,\u6cd7\u6c34\u5927\u53a6,\u6d59\u6c5f\u8def,\u66f2\u961c\u8857,\u5bff\u5eb7\u8def,\u6cb3\u5357\u5e7f\u573a,\u6cf0\u5b89\u8def,\u5927\u6cbd\u8857,\u7ea2\u5c71\u5ce1\u652f\u8def,\"\n                    \"\u897f\u9675\u5ce1\u4e00\u5927\u53a6,\u53f0\u897f\u7eac\u4e00\u5e7f\u573a,\u53f0\u897f\u7eac\u56db\u8857,\u53f0\u897f\u7eac\u4e8c\u8def,\u897f\u9675\u5ce1\u4e8c\u8857,\u897f\u9675\u5ce1\u4e09\u8def,\u53f0\u897f\u7eac\u4e09\u5e7f\u573a,\"\n                    \"\u53f0\u897f\u7eac\u4e94\u8def,\u660e\u6708\u5ce1\u5927\u53a6,\u9752\u94dc\u5ce1\u8def,\u53f0\u897f\u4e8c\u8857,\u89c2\u97f3\u5ce1\u5e7f\u573a,\u77bf\u5858\u5ce1\u8857,\u56e2\u5c9b\u4e8c\u8def,\u56e2\u5c9b\u4e00\u8857,\u53f0\u897f\u4e09\u8def,\"\n                    \"\u53f0\u897f\u4e00\u5927\u53a6,\u90d3\u57ce\u5357\u8def,\u56e2\u5c9b\u4e09\u8857,\u5218\u5bb6\u5ce1\u8def,\u897f\u85cf\u4e8c\u8857,\u897f\u85cf\u4e00\u5e7f\u573a,\u53f0\u897f\u56db\u8857,\u4e09\u95e8\u5ce1\u8def,\u57ce\u6b66\u652f\u5927\u53a6,\"\n                    \"\u7ea2\u5c71\u5ce1\u8def,\u90d3\u57ce\u5317\u5e7f\u573a,\u9f99\u7f8a\u5ce1\u8def,\u897f\u9675\u5ce1\u8857,\u53f0\u897f\u4e94\u8def,\u56e2\u5c9b\u56db\u8857,\u77f3\u6751\u5e7f\u573a,\u5deb\u5ce1\u5927\u53a6,\u56db\u5ddd\u8def,\u5bff\u5f20\u8857,\"\n                    \"\u5609\u7965\u8def,\u5357\u6751\u5e7f\u573a,\u8303\u53bf\u8def,\u897f\u5eb7\u8857,\u4e91\u5357\u8def,\u5de8\u91ce\u5927\u53a6,\u897f\u6c5f\u5e7f\u573a,\u9c7c\u53f0\u8857,\u5355\u53bf\u8def,\u5b9a\u9676\u8857,\u6ed5\u53bf\u8def,\"\n                    \"\u949c\u91ce\u5e7f\u573a,\u89c2\u57ce\u8def,\u6c76\u4e0a\u5927\u53a6,\u671d\u57ce\u8def,\u6ecb\u9633\u8857,\u90b9\u53bf\u5e7f\u573a,\u6fee\u53bf\u8857,\u78c1\u5c71\u8def,\u6c76\u6c34\u8857,\u897f\u85cf\u8def,\u57ce\u6b66\u5927\u53a6,\"\n                    \"\u56e2\u5c9b\u8def,\u5357\u9633\u8857,\u5e7f\u5dde\u8def,\u4e1c\u5e73\u8857,\u67a3\u5e84\u5e7f\u573a,\u8d35\u5dde\u8857,\u8d39\u53bf\u8def,\u5357\u6d77\u5927\u53a6,\u767b\u5dde\u8def,\u6587\u767b\u5e7f\u573a,\u4fe1\u53f7\u5c71\u652f\u8def,\"\n                    \"\u5ef6\u5b89\u4e00\u8857,\u4fe1\u53f7\u5c71\u8def,\u5174\u5b89\u652f\u8857,\u798f\u5c71\u652f\u5e7f\u573a,\u7ea2\u5c9b\u652f\u5927\u53a6,\u83b1\u829c\u4e8c\u8def,\u5434\u53bf\u4e00\u8857,\u91d1\u53e3\u4e09\u8def,\u91d1\u53e3\u4e00\u5e7f\u573a,\"\n                    \"\u4f0f\u9f99\u5c71\u8def,\u9c7c\u5c71\u652f\u8857,\u89c2\u8c61\u4e8c\u8def,\u5434\u53bf\u4e8c\u5927\u53a6,\u83b1\u829c\u4e00\u5e7f\u573a,\u91d1\u53e3\u4e8c\u8857,\u6d77\u9633\u8def,\u9f99\u53e3\u8857,\u6052\u5c71\u8def,\u9c7c\u5c71\u5e7f\u573a,\"\n                    \"\u6396\u53bf\u8def,\u798f\u5c71\u5927\u53a6,\u7ea2\u5c9b\u8def,\u5e38\u5dde\u8857,\u5927\u5b66\u5e7f\u573a,\u9f99\u534e\u8857,\u9f50\u6cb3\u8def,\u83b1\u9633\u8857,\u9ec4\u53bf\u8def,\u5f20\u5e97\u5927\u53a6,\u795a\u5c71\u8def,\u82cf\u5dde\u8857,\"\n                    \"\u534e\u5c71\u8def,\u4f0f\u9f99\u8857,\u6c5f\u82cf\u5e7f\u573a,\u9f99\u6c5f\u8857,\u738b\u6751\u8def,\u7434\u5c7f\u5927\u53a6,\u9f50\u4e1c\u8def,\u4eac\u5c71\u5e7f\u573a,\u9f99\u5c71\u8def,\u725f\u5e73\u8857,\u5ef6\u5b89\u4e09\u8def,\"\n                    \"\u5ef6\u5409\u8857,\u5357\u4eac\u5e7f\u573a,\u4e1c\u6d77\u4e1c\u5927\u53a6,\u94f6\u5ddd\u897f\u8def,\u6d77\u53e3\u8857,\u5c71\u4e1c\u8def,\u7ecd\u5174\u5e7f\u573a,\u829d\u6cc9\u8def,\u4e1c\u6d77\u4e2d\u8857,\u5b81\u590f\u8def,\u9999\u6e2f\u897f\u5927\u53a6,\"\n                    \"\u9686\u5fb7\u5e7f\u573a,\u626c\u5dde\u8857,\u90e7\u9633\u8def,\u592a\u5e73\u89d2\u4e00\u8857,\u5b81\u56fd\u4e8c\u652f\u8def,\u592a\u5e73\u89d2\u4e8c\u5e7f\u573a,\u5929\u53f0\u4e1c\u4e00\u8def,\u592a\u5e73\u89d2\u4e09\u5927\u53a6,\u6f33\u5dde\u8def\u4e00\u8def,\"\n                    \"\u6f33\u5dde\u8857\u4e8c\u8857,\u5b81\u56fd\u4e00\u652f\u5e7f\u573a,\u592a\u5e73\u89d2\u516d\u8857,\u592a\u5e73\u89d2\u56db\u8def,\u5929\u53f0\u4e1c\u4e8c\u8857,\u592a\u5e73\u89d2\u4e94\u8def,\u5b81\u56fd\u4e09\u5927\u53a6,\u6fb3\u95e8\u4e09\u8def,\"\n                    \"\u6c5f\u897f\u652f\u8857,\u6fb3\u95e8\u4e8c\u8def,\u5b81\u56fd\u56db\u8857,\u5927\u5c27\u4e00\u5e7f\u573a,\u54b8\u9633\u652f\u8857,\u6d2a\u6cfd\u6e56\u8def,\u5434\u5174\u4e8c\u5927\u53a6,\u6f84\u6d77\u4e09\u8def,\u5929\u53f0\u4e00\u5e7f\u573a,\"\n                    \"\u65b0\u6e5b\u4e8c\u8def,\u4e09\u660e\u5317\u8857,\u65b0\u6e5b\u652f\u8def,\u6e5b\u5c71\u4e94\u8857,\u6cf0\u5dde\u4e09\u5e7f\u573a,\u6e5b\u5c71\u56db\u5927\u53a6,\u95fd\u6c5f\u4e09\u8def,\u6fb3\u95e8\u56db\u8857,\u5357\u6d77\u652f\u8def,\"\n                    \"\u5434\u5174\u4e09\u5e7f\u573a,\u4e09\u660e\u5357\u8def,\u6e5b\u5c71\u4e8c\u8857,\u4e8c\u8f7b\u65b0\u6751\u9547,\u6c5f\u5357\u5927\u53a6,\u5434\u5174\u4e00\u5e7f\u573a,\u73e0\u6d77\u4e8c\u8857,\u5609\u5cea\u5173\u8def,\u9ad8\u90ae\u6e56\u8857,\"\n                    \"\u6e5b\u5c71\u4e09\u8def,\u6fb3\u95e8\u516d\u5e7f\u573a,\u6cf0\u5dde\u4e8c\u8def,\u4e1c\u6d77\u4e00\u5927\u53a6,\u5929\u53f0\u4e8c\u8def,\u5fae\u5c71\u6e56\u8857,\u6d1e\u5ead\u6e56\u5e7f\u573a,\u73e0\u6d77\u652f\u8857,\u798f\u5dde\u5357\u8def,\"\n                    \"\u6f84\u6d77\u4e8c\u8857,\u6cf0\u5dde\u56db\u8def,\u9999\u6e2f\u4e2d\u5927\u53a6,\u6fb3\u95e8\u4e94\u8def,\u65b0\u6e5b\u4e09\u8857,\u6fb3\u95e8\u4e00\u8def,\u6b63\u9633\u5173\u8857,\u5b81\u6b66\u5173\u5e7f\u573a,\u95fd\u6c5f\u56db\u8857,\"\n                    \"\u65b0\u6e5b\u4e00\u8def,\u5b81\u56fd\u4e00\u5927\u53a6,\u738b\u5bb6\u9ea6\u5c9b,\u6fb3\u95e8\u4e03\u5e7f\u573a,\u6cf0\u5dde\u4e00\u8def,\u6cf0\u5dde\u516d\u8857,\u5927\u5c27\u4e8c\u8def,\u9752\u5927\u4e00\u8857,\u95fd\u6c5f\u4e8c\u5e7f\u573a,\"\n                    \"\u95fd\u6c5f\u4e00\u5927\u53a6,\u5c4f\u4e1c\u652f\u8def,\u6e5b\u5c71\u4e00\u8857,\u4e1c\u6d77\u897f\u8def,\u5f90\u5bb6\u9ea6\u5c9b\u51fd\u8c37\u5173\u5e7f\u573a,\u5927\u5c27\u4e09\u8def,\u6653\u671b\u652f\u8857,\u79c0\u6e5b\u4e8c\u8def,\"\n                    \"\u900d\u9065\u4e09\u5927\u53a6,\u6fb3\u95e8\u4e5d\u5e7f\u573a,\u6cf0\u5dde\u4e94\u8857,\u6f84\u6d77\u4e00\u8def,\u6fb3\u95e8\u516b\u8857,\u798f\u5dde\u5317\u8def,\u73e0\u6d77\u4e00\u5e7f\u573a,\u5b81\u56fd\u4e8c\u8def,\u4e34\u6dee\u5173\u5927\u53a6,\"\n                    \"\u71d5\u513f\u5c9b\u8def,\u7d2b\u8346\u5173\u8857,\u6b66\u80dc\u5173\u5e7f\u573a,\u900d\u9065\u4e00\u8857,\u79c0\u6e5b\u56db\u8def,\u5c45\u5eb8\u5173\u8857,\u5c71\u6d77\u5173\u8def,\u9131\u9633\u6e56\u5927\u53a6,\u65b0\u6e5b\u8def,\u6f33\u5dde\u8857,\"\n                    \"\u4ed9\u6e38\u8def,\u82b1\u83b2\u8857,\u4e50\u6e05\u5e7f\u573a,\u5de2\u6e56\u8857,\u53f0\u5357\u8def,\u5434\u5174\u5927\u53a6,\u65b0\u7530\u8def,\u798f\u6e05\u5e7f\u573a,\u6f84\u6d77\u8def,\u8386\u7530\u8857,\u6d77\u6e38\u8def,\u9547\u6c5f\u8857,\"\n                    \"\u77f3\u5c9b\u5e7f\u573a,\u5b9c\u5174\u5927\u53a6,\u4e09\u660e\u8def,\u4ef0\u53e3\u8857,\u6c9b\u53bf\u8def,\u6f33\u6d66\u5e7f\u573a,\u5927\u9ea6\u5c9b,\u53f0\u6e7e\u8857,\u5929\u53f0\u8def,\u91d1\u6e56\u5927\u53a6,\u9ad8\u96c4\u5e7f\u573a,\u6d77\u6c5f\u8857,\"\n                    \"\u5cb3\u9633\u8def,\u5584\u5316\u8857,\u8363\u6210\u8def,\u6fb3\u95e8\u5e7f\u573a,\u6b66\u660c\u8def,\u95fd\u6c5f\u5927\u53a6,\u53f0\u5317\u8def,\u9f99\u5ca9\u8857,\u54b8\u9633\u5e7f\u573a,\u5b81\u5fb7\u8857,\u9f99\u6cc9\u8def,\u4e3d\u6c34\u8857,\"\n                    \"\u6d77\u5ddd\u8def,\u5f70\u5316\u5927\u53a6,\u91d1\u7530\u8def,\u6cf0\u5dde\u8857,\u592a\u6e56\u8def,\u6c5f\u897f\u8857,\u6cf0\u5174\u5e7f\u573a,\u9752\u5927\u8857,\u91d1\u95e8\u8def,\u5357\u901a\u5927\u53a6,\u65cc\u5fb7\u8def,\u6c47\u6cc9\u5e7f\u573a,\"\n                    \"\u5b81\u56fd\u8def,\u6cc9\u5dde\u8857,\u5982\u4e1c\u8def,\u5949\u5316\u8857,\u9e4a\u5c71\u5e7f\u573a,\u83b2\u5c9b\u5927\u53a6,\u534e\u4e25\u8def,\u5609\u4e49\u8857,\u53e4\u7530\u8def,\u5357\u5e73\u5e7f\u573a,\u79c0\u6e5b\u8def,\u957f\u6c40\u8857,\"\n                    \"\u6e5b\u5c71\u8def,\u5f90\u5dde\u5927\u53a6,\u4e30\u53bf\u5e7f\u573a,\u6c55\u5934\u8857,\u65b0\u7af9\u8def,\u9ec4\u6d77\u8857,\u5b89\u5e86\u8def,\u57fa\u9686\u5e7f\u573a,\u97f6\u5173\u8def,\u4e91\u9704\u5927\u53a6,\u65b0\u5b89\u8def,\u4ed9\u5c45\u8857,\"\n                    \"\u5c4f\u4e1c\u5e7f\u573a,\u6653\u671b\u8857,\u6d77\u95e8\u8def,\u73e0\u6d77\u8857,\u4e0a\u676d\u8def,\u6c38\u5609\u5927\u53a6,\u6f33\u5e73\u8def,\u76d0\u57ce\u8857,\u65b0\u6d66\u8def,\u65b0\u660c\u8857,\u9ad8\u7530\u5e7f\u573a,\u5e02\u573a\u4e09\u8857,\"\n                    \"\u91d1\u4e61\u4e1c\u8def,\u5e02\u573a\u4e8c\u5927\u53a6,\u4e0a\u6d77\u652f\u8def,\u674e\u6751\u652f\u5e7f\u573a,\u60e0\u6c11\u5357\u8def,\u5e02\u573a\u7eac\u8857,\u957f\u5b89\u5357\u8def,\u9675\u53bf\u652f\u8857,\u51a0\u53bf\u652f\u5e7f\u573a,\"\n                    \"\u5c0f\u6e2f\u4e00\u5927\u53a6,\u5e02\u573a\u4e00\u8def,\u5c0f\u6e2f\u4e8c\u8857,\u6e05\u5e73\u8def,\u5e7f\u4e1c\u5e7f\u573a,\u65b0\u7586\u8def,\u535a\u5e73\u8857,\u6e2f\u901a\u8def,\u5c0f\u6e2f\u6cbf,\u798f\u5efa\u5e7f\u573a,\u9ad8\u5510\u8857,\"\n                    \"\u830c\u5e73\u8def,\u6e2f\u9752\u8857,\u9ad8\u5bc6\u8def,\u9633\u8c37\u5e7f\u573a,\u5e73\u9634\u8def,\u590f\u6d25\u5927\u53a6,\u90b1\u53bf\u8def,\u6e24\u6d77\u8857,\u6069\u53bf\u5e7f\u573a,\u65c5\u987a\u8857,\u5802\u9091\u8def,\u674e\u6751\u8857,\"\n                    \"\u5373\u58a8\u8def,\u6e2f\u534e\u5927\u53a6,\u6e2f\u73af\u8def,\u9986\u9676\u8857,\u666e\u96c6\u8def,\u671d\u9633\u8857,\u7518\u8083\u5e7f\u573a,\u6e2f\u590f\u8857,\u6e2f\u8054\u8def,\u9675\u53bf\u5927\u53a6,\u4e0a\u6d77\u8def,\u5b9d\u5c71\u5e7f\u573a,\"\n                    \"\u6b66\u5b9a\u8def,\u957f\u6e05\u8857,\u957f\u5b89\u8def,\u60e0\u6c11\u8857,\u6b66\u57ce\u5e7f\u573a,\u804a\u57ce\u5927\u53a6,\u6d77\u6cca\u8def,\u6ca7\u53e3\u8857,\u5b81\u6ce2\u8def,\u80f6\u5dde\u5e7f\u573a,\u83b1\u5dde\u8def,\u62db\u8fdc\u8857,\"\n                    \"\u51a0\u53bf\u8def,\u516d\u7801\u5934,\u91d1\u4e61\u5e7f\u573a,\u79b9\u57ce\u8857,\u4e34\u6e05\u8def,\u4e1c\u963f\u8857,\u5434\u6dde\u8def,\u5927\u6e2f\u6cbf,\u8fbd\u5b81\u8def,\u68e3\u7eac\u4e8c\u5927\u53a6,\u5927\u6e2f\u7eac\u4e00\u8def,\u8d2e\u6c34\u5c71\u652f\u8857,\"\n                    \"\u65e0\u68e3\u7eac\u4e00\u5e7f\u573a,\u5927\u6e2f\u7eac\u4e09\u8857,\u5927\u6e2f\u7eac\u4e94\u8def,\u5927\u6e2f\u7eac\u56db\u8857,\u5927\u6e2f\u7eac\u4e8c\u8def,\u65e0\u68e3\u4e8c\u5927\u53a6,\u5409\u6797\u652f\u8def,\u5927\u6e2f\u56db\u8857,\u666e\u96c6\u652f\u8def,\"\n                    \"\u65e0\u68e3\u4e09\u8857,\u9ec4\u53f0\u652f\u5e7f\u573a,\u5927\u6e2f\u4e09\u8857,\u65e0\u68e3\u4e00\u8def,\u8d2e\u6c34\u5c71\u5927\u53a6,\u6cf0\u5c71\u652f\u8def,\u5927\u6e2f\u4e00\u5e7f\u573a,\u65e0\u68e3\u56db\u8def,\u5927\u8fde\u652f\u8857,\u5927\u6e2f\u4e8c\u8def,\"\n                    \"\u9526\u5dde\u652f\u8857,\u5fb7\u5e73\u5e7f\u573a,\u9ad8\u82d1\u5927\u53a6,\u957f\u5c71\u8def,\u4e50\u9675\u8857,\u4e34\u9091\u8def,\u5ae9\u6c5f\u5e7f\u573a,\u5408\u6c5f\u8def,\u5927\u8fde\u8857,\u535a\u5174\u8def,\u84b2\u53f0\u5927\u53a6,\u9ec4\u53f0\u5e7f\u573a,\"\n                    \"\u57ce\u9633\u8857,\u4e34\u6dc4\u8def,\u5b89\u90b1\u8857,\u4e34\u6710\u8def,\u9752\u57ce\u5e7f\u573a,\u5546\u6cb3\u8def,\u70ed\u6cb3\u5927\u53a6,\u6d4e\u9633\u8def,\u627f\u5fb7\u8857,\u6dc4\u5ddd\u5e7f\u573a,\u8fbd\u5317\u8857,\u9633\u4fe1\u8def,\u76ca\u90fd\u8857,\"\n                    \"\u677e\u6c5f\u8def,\u6d41\u4ead\u5927\u53a6,\u5409\u6797\u8def,\u6052\u53f0\u8857,\u5305\u5934\u8def,\u65e0\u68e3\u8857,\u94c1\u5c71\u5e7f\u573a,\u9526\u5dde\u8857,\u6853\u53f0\u8def,\u5174\u5b89\u5927\u53a6,\u90b9\u5e73\u8def,\u80f6\u4e1c\u5e7f\u573a,\u7ae0\u4e18\u8def,\"\n                    \"\u4e39\u4e1c\u8857,\u534e\u9633\u8def,\u9752\u6d77\u8857,\u6cf0\u5c71\u5e7f\u573a,\u5468\u6751\u5927\u53a6,\u56db\u5e73\u8def,\u53f0\u4e1c\u897f\u4e03\u8857,\u53f0\u4e1c\u4e1c\u4e8c\u8def,\u53f0\u4e1c\u4e1c\u4e03\u5e7f\u573a,\u53f0\u4e1c\u897f\u4e8c\u8def,\u4e1c\u4e94\u8857,\"\n                    \"\u4e91\u95e8\u4e8c\u8def,\u8299\u84c9\u5c71\u6751,\u5ef6\u5b89\u4e8c\u5e7f\u573a,\u4e91\u95e8\u4e00\u8857,\u53f0\u4e1c\u56db\u8def,\u53f0\u4e1c\u4e00\u8857,\u53f0\u4e1c\u4e8c\u8def,\u676d\u5dde\u652f\u5e7f\u573a,\u5185\u8499\u53e4\u8def,\u53f0\u4e1c\u4e03\u5927\u53a6,\"\n                    \"\u53f0\u4e1c\u516d\u8def,\u5e7f\u9976\u652f\u8857,\u53f0\u4e1c\u516b\u5e7f\u573a,\u53f0\u4e1c\u4e09\u8857,\u56db\u5e73\u652f\u8def,\u90ed\u53e3\u4e1c\u8857,\u9752\u6d77\u652f\u8def,\u6c88\u9633\u652f\u5927\u53a6,\u83dc\u5e02\u4e8c\u8def,\u83dc\u5e02\u4e00\u8857,\"\n                    \"\u5317\u4ef2\u4e09\u8def,\u745e\u4e91\u8857,\u6ee8\u53bf\u5e7f\u573a,\u5e86\u7965\u8857,\u4e07\u5bff\u8def,\u5927\u6210\u5927\u53a6,\u8299\u84c9\u8def,\u5386\u57ce\u5e7f\u573a,\u5927\u540d\u8def,\u660c\u5e73\u8857,\u5e73\u5b9a\u8def,\u957f\u5174\u8857,\"\n                    \"\u6d66\u53e3\u5e7f\u573a,\u8bf8\u57ce\u5927\u53a6,\u548c\u5174\u8def,\u5fb7\u76db\u8857,\u5b81\u6d77\u8def,\u5a01\u6d77\u5e7f\u573a,\u4e1c\u5c71\u8def,\u6e05\u548c\u8857,\u59dc\u6c9f\u8def,\u96d2\u53e3\u5927\u53a6,\u677e\u5c71\u5e7f\u573a,\u957f\u6625\u8857,\"\n                    \"\u6606\u660e\u8def,\u987a\u5174\u8857,\u5229\u6d25\u8def,\u9633\u660e\u5e7f\u573a,\u4eba\u548c\u8def,\u90ed\u53e3\u5927\u53a6,\u8425\u53e3\u8def,\u660c\u9091\u8857,\u5b5f\u5e84\u5e7f\u573a,\u4e30\u76db\u8857,\u57d5\u53e3\u8def,\u4e39\u9633\u8857,\u6c49\u53e3\u8def,\"\n                    \"\u6d2e\u5357\u5927\u53a6,\u6851\u6893\u8def,\u6cbe\u5316\u8857,\u5c71\u53e3\u8def,\u6c88\u9633\u8857,\u5357\u53e3\u5e7f\u573a,\u632f\u5174\u8857,\u901a\u5316\u8def,\u798f\u5bfa\u5927\u53a6,\u5cc4\u53bf\u8def,\u5bff\u5149\u5e7f\u573a,\u66f9\u53bf\u8def,\u660c\u4e50\u8857,\"\n                    \"\u9053\u53e3\u8def,\u5357\u4e5d\u6c34\u8857,\u53f0\u6e5b\u5e7f\u573a,\u4e1c\u5149\u5927\u53a6,\u9a7c\u5cf0\u8def,\u592a\u5e73\u5c71,\u6807\u5c71\u8def,\u4e91\u6eaa\u5e7f\u573a,\u592a\u6e05\u8def\")\n    random_addr = random.choice(address_word.split(','))\n    if random_addr.endswith('\u8def') or random_addr.endswith('\u8857'):\n        random_addr = ''.join([random_addr, str(random.randint(1, 1000)), '\u53f7'])\n    address_pattern = '{province_name}{areanote_info}{random_addr}'\n    return address_pattern.format(province_name=province_name,\n                                  areanote_info=areanote_info,\n                                  random_addr=random_addr)", "response": "Generate a random address for the given zone."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gen_random_bank_card(bankname, card_type):\n    bank_info = CardBin.get_bank_info(bankname)\n    if not bank_info:\n        raise ValueError('bankname {} error, check and try again'.format(bankname))\n\n    # \u83b7\u53d6\u94f6\u884c\u4ee3\u7801\n    bank = bank_info[0][0]\n\n    # \u83b7\u53d6 cardbin\n    cardbin_info = CardBin.get_cardbin_info(bank, card_type)\n    if not cardbin_info:\n        raise ValueError('card_type {} error, check and try again'.format(card_type))\n\n    random_cardbin = random.choice(cardbin_info)\n\n    cardbin = random_cardbin[0]\n    card_len = random_cardbin[-1]\n\n    # \u94f6\u884c\u5361\u524d\u7f00\n    card_number_str = cardbin\n\n    # \u968f\u673a\u751f\u6210\u524dN-1\u4f4d\n    while len(card_number_str) < card_len - 1:\n        card_number_str += str(random.randint(0, 9))\n\n    # \u83b7\u53d6\u6821\u9a8c\u4f4d\n    check_code = CardBin.get_checkcode(card_number_str)\n    return card_number_str + check_code", "response": "Generate a random non - empty random card from a bank."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gen_random_id_card(zone=None, gender=None, age=None, result_type='SINGLE_STR'):\n    if zone:\n        province = zone[:2]\n    else:\n        province_list = IdCard.get_province_info()\n        province = random.choice(province_list)[0]\n        zone = province + '0000'\n    areanote_list = IdCard.get_areanote_info(province)\n\n    # \u5224\u65ad zone \u662f\u5426\u5408\u6cd5\n    if zone and (zone not in set([item[0] for item in areanote_list])):\n        raise ValueError('zone {} error, check and try again'.format(zone))\n\n    # \u5220\u9664\u7701\u4efd\u7f16\u53f7\uff0c\u8eab\u4efd\u8bc1\u524d\u7f00\u4e0d\u5305\u542b\u7701\u4efd\u7f16\u53f7\n    areanote_list.remove([item for item in areanote_list if item[0] == zone][0])\n\n    zone_list = [item[0] for item in areanote_list]\n\n    total_num = 1 if result_type == 'SINGLE_STR' else 20\n\n    id_num_list = list()\n\n    for _ in range(total_num):\n        # \u987a\u5e8f\u7801\u7684\u5947\u6570\u5206\u914d\u7ed9\u7537\u6027\uff0c\u5076\u6570\u5206\u914d\u7ed9\u5973\u6027\n        gender = gender if gender else random.choice(['00', '01'])\n        gender_dict = {'00': [0, 2, 4, 6, 8],\n                       '01': [1, 3, 5, 7, 9]}\n\n        now_date_str = FishDateTimeFormat.strftime(time_format='%Y-%m-%d')\n        year, month, day = now_date_str.split('-')\n        # \u5e74\u9f84\u6700\u5927\u4e3a 1970 \u5e74\u5f00\u59cb\n        if age:\n            age = min(age, int(year) - 1970)\n        else:\n            age = random.randint(0, int(year) - 1970)\n\n        start_date_str = '{year}-{month}-{day}'.format(year=int(year) - age, month=month, day=day)\n        birth = GetRandomTime.gen_date_by_range(start_date_str, now_date_str, date_format=\"%Y%m%d\")\n        birth = str(int(year) - age) + birth[4:]\n\n        zone = random.choice(zone_list)\n        random_str = str(random.randint(10, 99))\n        gender_str = str(random.choice(gender_dict.get(gender)))\n        _, check_code = IdCard.get_checkcode(zone + birth + random_str + gender_str)\n        random_id = (\"{zone}{birth_date}{random_str}{gender}{check_code}\".\n                     format(zone=zone,\n                            birth_date=birth,\n                            random_str=random_str,\n                            gender=gender_str,\n                            check_code=check_code))\n        id_num_list.append(random_id)\n    return id_num_list", "response": "Generate random ID card."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a random company name", "response": "def gen_random_company_name():\n    \"\"\"\n    \u968f\u673a\u751f\u6210\u4e00\u4e2a\u516c\u53f8\u540d\u79f0\n\n    :returns:\n        * company_name: (string) \u94f6\u884c\u540d\u79f0\n\n    \u4e3e\u4f8b\u5982\u4e0b::\n\n        print('--- gen_random_company_name demo ---')\n        print(gen_random_company_name())\n        print('---')\n\n    \u8f93\u51fa\u7ed3\u679c::\n\n        --- gen_random_company_name demo ---\n        \u4e0a\u6d77\u5927\u5347\u65c5\u6e38\u8d28\u8be2\u6709\u9650\u8d23\u4efb\u516c\u53f8\n        ---\n\n    \"\"\"\n    region_info = (\"\u5317\u4eac,\u4e0a\u6d77,\u5e7f\u5dde,\u6df1\u5733,\u5929\u6d25,\u6210\u90fd,\u676d\u5dde,\u82cf\u5dde,\u91cd\u5e86,\u6b66\u6c49,\u5357\u4eac,\u5927\u8fde,\u6c88\u9633,\u957f\u6c99,\u90d1\u5dde,\u897f\u5b89,\u9752\u5c9b,\"\n                   \"\u65e0\u9521,\u6d4e\u5357,\u5b81\u6ce2,\u4f5b\u5c71,\u5357\u901a,\u54c8\u5c14\u6ee8,\u4e1c\u839e,\u798f\u5dde,\u957f\u6625,\u77f3\u5bb6\u5e84,\u70df\u53f0,\u5408\u80a5,\u5510\u5c71,\u5e38\u5dde,\u592a\u539f,\u6606\u660e,\"\n                   \"\u6f4d\u574a,\u5357\u660c,\u6cc9\u5dde,\u6e29\u5dde,\u7ecd\u5174,\u5609\u5174,\u53a6\u95e8,\u8d35\u9633,\u6dc4\u535a,\u5f90\u5dde,\u5357\u5b81,\u626c\u5dde,\u547c\u548c\u6d69\u7279,\u9102\u5c14\u591a\u65af,\u4e4c\u9c81\u6728\u9f50,\"\n                   \"\u91d1\u534e,\u53f0\u5dde,\u9547\u6c5f,\u5a01\u6d77,\u73e0\u6d77,\u4e1c\u8425,\u5927\u5e86,\u4e2d\u5c71,\u76d0\u57ce,\u5305\u5934,\u4fdd\u5b9a,\u6d4e\u5b81,\u6cf0\u5dde,\u5eca\u574a,\u5170\u5dde,\u6d1b\u9633,\u5b9c\u660c,\"\n                   \"\u6ca7\u5dde,\u4e34\u6c82,\u6cf0\u5b89,\u978d\u5c71,\u90af\u90f8,\u60e0\u5dde,\u6c5f\u95e8,\u8944\u9633,\u6e56\u5dde,\u5409\u6797,\u829c\u6e56,\u5fb7\u5dde,\u804a\u57ce,\u6f33\u5dde,\u682a\u6d32,\u6dee\u5b89,\u6986\u6797,\"\n                   \"\u5e38\u5fb7,\u54b8\u9633,\u8861\u9633,\u6ee8\u5dde,\u67f3\u5dde,\u9075\u4e49,\u83cf\u6cfd,\u5357\u9633,\u65b0\u4e61,\u6e5b\u6c5f,\u5cb3\u9633,\u90f4\u5dde,\u8bb8\u660c,\u8fde\u4e91\u6e2f,\u67a3\u5e84,\u8302\u540d,\u5468\u53e3,\"\n                   \"\u5bbf\u8fc1\")\n    middle_word = (\"\u6cf0\u5b8f\u672c\u6676\u8f89\u660c\u660c\u672c\u540c\u6c38\u5eb7\u6d2a\u7687\u8d35\u4e45\u5723\u6b63\u88d5\u5982\u6052\u957f\u4f73\u534f\u4e49\u6676\u5408\u4f18\u8363\u6c47\u6d2a\u5343\u4e1c\u7965\u590d\u660c\u7687\u4e45\u4e30\u5174\u660c\u56fd\u88d5\u4e9a\u5927\"\n                   \"\u8363\u5eb7\u901a\u4ec1\u5143\u88d5\u539a\u745e\u5982\u5f18\u5347\u4e45\u9686\u65fa\u5409\u5fb7\u8c26\u957f\u8d35\u767e\u4e45\u6c47\u767e\u4f1f\u5347\u9686\u590d\u98de\u4f73\u9686\u6d69\u53d1\u4e30\u4ea8\u516c\u8363\u590d\u5149\u798f\u7f8e\u7984\u6b23\u4e30\u5927\"\n                   \"\u7965\u6676\u5b8f\u4e2d\u4ec1\u5b8f\u534e\u9686\u76c8\u65fa\u4ec1\u987a\u6625\u6ee1\u7f8e\u4e2d\u8c26\u745e\u548c\u5723\u591a\u4fe1\u5408\u76db\u5343\u4e9a\u6676\u7965\u946b\u9686\u98de\u946b\u4f18\u5408\u672c\u65fa\u53d1\u4e45\u56fd\u6c47\u767e\u6052\u4f73\u4e1c\"\n                   \"\u6d2a\u901a\u6052\u5927\u516c\u4e2d\u4f18\u5e7f\u5b9d\u76c8\u6cf0\u5982\u5408\u4e30\u6377\u672c\u4f1f\u534e\u6625\u5143\u4e9a\u5e7f\u4e2d\u6676\u5982\u6d69\u4ec1\u6c47\u4e9a\u6c38\u51ef\u5bcc\u5bcc\u88d5\u8302\u534e\u4e2d\u98de\u6d69\u53f0\u7f8e\u4f73\u5723\u4ec1\"\n                   \"\u6210\u5168\u6da6\u91d1\u5e86\u767e\u8d35\u5eb7\u4ec1\u8302\u7687\u4e1c\u5e7f\u8363\u5b8f\u8363\u65b0\u5143\u5eb7\u516c\u5347\u4ea8\u6d2a\u798f\u4f1f\u6c38\u4e49\u5de8\u56fd\u5347\u8fdb\u5408\u8000\u5de8\u6da6\u5de8\u5143\u53d1\u6d2a\u6e90\u5bff\u4ec1\u53d1\u5149\"\n                   \"\u987a\u5347\u51ef\u5168\u5168\u8f89\u6b23\u6210\u516c\u88d5\u5eb7\u5408\u7984\u5174\u6c47\u987a\u6d69\u8d35\u6676\u6377\u4e1c\u98de\u76ca\u798f\u5b8f\u56fd\u7984\u5143\u660c\u5f18\u548c\u6ee1\u53d1\u5de8\u5b9d\u751f\u8000\u9686\u5927\u6b23\u660c\u4f73\u672c\u5174\"\n                   \"\u5409\u751f\u5b9d\u51ef\u6da6\u65b0\u9ad8\u548c\u5143\u4ea8\u5de8\u4e45\u5149\u76ca\u65fa\u6625\u5de8\u946b\u8fdb\u4e1c\u6676\u4e2d\u98de\u5174\u4e2d\u7f8e\u4e30\u540c\u6676\u590d\u8000\u8fdb\u6d2a\u5168\u5174\u6c47\u5b9d\u6377\u4f1f\u4ec1\u5b89\u5b8f\u591a\u5e86\"\n                   \"\u76ca\u751f\u548c\u5e72\u5e72\u798f\u4e9a\u65b0\u590d\u5409\u4e9a\u6052\u4e9a\u6625\u5fb7\u98de\u4f1f\u5229\u5e86\u534e\u4e30\u5b8f\u5408\u5fb7\u745e\u8fdb\u987a\u7965\u4fe1\u5408\u5eb7\u5bcc\u76ca\u5168\u5de8\u8302\u53f0\u8c26\u539a\u53f0\u6210\u798f\u6377\u6d69\"\n                   \"\u4fe1\u957f\u98de\u957f\u91d1\u5229\u7f8e\u660c\u6ee1\u4e30\u5e72\u4f73\u7f8e\u91d1\u6d2a\u660c\u5bcc\u5343\u548c\u7f8e\u65fa\u65fa\u6676\u6625\u4ec1\u534e\u4e2d\u51ef\u6d69\u9f0e\u6cf0\u8f89\u65b0\u5e72\u9ad8\u8fdb\u8f89\u540c\u6b23\u5e7f\u5e86\u5409\u76ca\u5fb7\"\n                   \"\u6d69\u4e2d\u6da6\u548c\u6625\u5143\u751f\u9ad8\u8fdb\u7687\u8302\u5229\u540c\u76c8\u590d\u590d\u6676\u591a\u5de8\u5723\u5f18\u6377\u516c\u5b9d\u6c47\u946b\u6210\u9ad8\u65b0\u6b63\u548c\u548c\u5de8\u7965\u5149\u5b8f\u5927\u4e30\u6b23\u6052\u660c\u660c\u539a\u5408\"\n                   \"\u5e86\u6cf0\u4e30\u5e72\u76ca\u548c\u91d1\u6d2a\u590d\u5143\u987a\u6377\u91d1\u4e07\u8f89\u5168\u5409\u5e86\u5fb7\u745e\u4f18\u957f\u9f0e\u987a\u6c47\u987a\u6b23\u98de\u6d69\u8363\u7965\u5149\u6cf0\u591a\u6625\u51ef\u4fe1\u8fdb\u516c\u4f18\u98de\u660c\u534f\u7f8e\"\n                   \"\u591a\u53d1\u4e2d\u76c8\u534f\u6210\u7965\u76ca\u660c\u6c47\u6cf0\u6625\u6ee1\u5343\u9f0e\u4e1c\u5149\u4f18\u8c26\u4ec1\u4e2d\u98de\u751f\u6052\u4f1f\u798f\u6676\u5b9d\u4fe1\u8f89\u91d1\u7687\u5347\u98de\u4ea8\u946b\u5b89\u4f1f\u534e\u5143\u65fa\u76ca\u5927\u5bff\"\n                   \"\u7687\u5143\u5eb7\u8000\u4e45\u8363\u6ee1\u534f\u4fe1\u51ef\u8c26\u5b9d\u5de8\u4e30\u6b63\u5149\u53d1\u5eb7\u5eb7\u6377\u4e2d\u6e90\u56fd\u591a\u591a\u5eb7\u516c\u5229\u987a\u5149\u8f89\u5982\u8302\u6676\u6c38\u5927\u9ad8\u6210\u751f\u88d5\u88d5\u548c\u4e07\u5e72\"\n                   \"\u98de\u5168\u6d2a\u4f1f\u540c\u53d1\u7984\u5347\u6b23\u76c8\u9ad8\u6b23\u8c26\u4ea8\u88d5\u5eb7\u5b9d\u590d\u5e86\u5149\u7687\u6e90\u51ef\u51ef\u5723\u53d1\u4e1c\u672c\u8f89\u5bff\u6377\u8302\u548c\u5e86\u4e30\u591a\u5b8f\u4e9a\u4e07\u76ca\u516c\u798f\u6377\u5347\"\n                   \"\u798f\u8302\u5b9d\u6377\u540c\u590d\u5408\u9686\u4e2d\u6c47\u7984\u946b\u4e2d\u65b0\u5fb7\u660c\u65b0\u5927\u7687\u5b89\u4e1c\u4fe1\u745e\u5143\u7687\u7687\u6d2a\u745e\u5f18\u6377\u672c\u946b\u4e2d\u4ea8\u4e9a\u5e7f\u660c\u6c38\u5b8f\u6da6\u540c\u6210\u9ad8\u5229\"\n                   \"\u53f0\u4e2d\u751f\u5982\u767e\u5eb7\u65fa\u5de8\u798f\u5fb7\u6625\u5143\u901a\u56fd\u6210\u6d69\u6c38\u5eb7\u6cf0\u76db\u6cf0\u5229\u751f\u8302\u5de8\u4e45\u660c\u4f73\u590d\u5bcc\u9686\u901a\u76c8\u540c\u5e86\u7687\u987a\u5982\u8f89\u5168\u65fa\u6377\u7687\u957f\"\n                   \"\u5168\u5bcc\u5e7f\u6e90\u6052\u9f0e\u987a\u6c47\u672c\u767e\u6d2a\u9f0e\u8fdb\u6b23\u5409\u51ef\u6c47\u6b23\u4e49\u4e1c\u957f\u7984\u6377\u6d69\u76ca\u65fa\u590d\u5f18\u660c\u751f\u53d1\u4f1f\u8363\u9ad8\u4ea8\u5143\u805a\u5e7f\u65b0\u590d\u591a\u5bcc\u5343\u4e2d\"\n                   \"\u5174\u4f73\u5347\u5eb7\u6210\u540c\u8d35\u5b9d\u751f\u6377\u6676\u5168\u6cf0\u5168\u6c38\u65fa\u53d1\u5bcc\u5eb7\u4ec1\u5174\u8c26\u5229\u8302\u4ea8\u6d2a\u4f73\u6d2a\u5143\u9f0e\u5168\u56fd\u672c\u4e30\u4ea8\u946b\u5f18\u5bcc\u5e72\u5bff\u6625\u8d35\u56fd\u6210\"\n                   \"\u76db\u5927\u53d1\u4e45\u5f18\u56fd\u5927\u91d1\u751f\u4e45\u9ad8\u4e45\u76ca\u6d69\u6676\u76c8\u76ca\u745e\u6b63\u4e30\u767e\u6d69\u6cf0\u53f0\u5408\u5fb7\u660c\u660c\u7f8e\u7687\u5408\u9686\u88d5\u4e1c\u5e7f\u4e9a\u56fd\u5347\u76ca\u798f\u65fa\u9ad8\u8d35\u4fe1\"\n                   \"\u751f\u6c47\u591a\u6cf0\u5143\u539a\u745e\u98de\u5343\u987a\u76db\u5982\u5927\u5fb7\u6da6\u65b0\u65b0\u987a\u6da6\u98de\u745e\u4f18\u6e90\u5b8f\u5343\u76db\u5409\u9ad8\u5927\u8000\u8fdb\u4fe1\u6b23\u4fe1\u5229\u745e\u8363\u5347\u4ea8\u76c8\u76db\u5343\u5408\u590d\"\n                   \"\u9686\u8d35\u4e30\u4e49\u516c\u4f18\u8363\u5b8f\u5e7f\u798f\u534e\u6d2a\u6d2a\u6377\u5409\u8fdb\u76db\u76db\u88d5\u56fd\u6d2a\u6d69\u7965\u6676\u5f18\u5409\u6b23\u9f0e\u5fb7\u4f73\u6210\u548c\u6ee1\u53f0\u5149\u590d\u6c47\u4f73\u901a\u6d69\u660c\u6b23\u5eb7\u745e\"\n                   \"\u4e07\u4e9a\u8c26\u5174\u798f\u5229\u5343\u5143\u7687\u745e\u6da6\u7984\u4fe1\u5408\u957f\u6da6\u6377\u4e2d\u65fa\u6210\u91d1\u76ca\u516c\u9686\u5b8f\u5eb7\u4e9a\u7984\u9686\u901a\u5149\u5e7f\u56fd\u4e49\u4e2d\u4f18\u591a\u5bcc\u590d\u76db\u5e86\u5343\u957f\u6c38\"\n                   \"\u56fd\u6e90\u5b89\u6c38\u5343\u4e2d\u6b63\u5eb7\u53d1\u590d\u534f\u5229\u7687\u4e9a\u534f\u946b\u4e49\u5de8\u6e90\u4e2d\u6da6\u65fa\u9ad8\u8fdb\u5de8\u65b0\u9ad8\u534f\u5174\u751f\u798f\u6052\u5bcc\u56fd\u534f\u6377\u76db\u540c\u590d\u5de8\u5343\u76ca\u957f\u6d2a\"\n                   \"\u4e9a\u6b23\u7f8e\u590d\u5eb7\u6d2a\u5168\u9ad8\u5b89\u8fdb\u5343\u6c47\u901a\u76ca\u7f8e\u8000\u7f8e\u53f0\u8000\u4e07\u5eb7\u5408\u6d2a\u7984\u4e2d\u5b8f\u767e\u51ef\u534e\u4f18\u946b\u534f\u6cf0\u5174\u88d5\u6b23\u8fdb\u5b89\u8302\u4e30\u5149\u98de\u5168\u98de\"\n                   \"\u9ad8\u5eb7\u8fdb\u540c\u5927\u6d2a\u6c38\u7965\u98de\u7f8e\u6ee1\u5174\u4e30\u8c26\u548c\u946b\u8d35\u767e\u6d2a\u901a\u88d5\u5347\u5e72\u6c38\u5347\u4ea8\u5149\u5fb7\u76db\u6c38\u91d1\u4e1c\u9f0e\u6c38\u88d5\u4f73\u548c\u5fb7\u4ec1\u8363\u8f89\u540c\u745e\u6052\"\n                   \"\u805a\u8c26\u957f\u5e7f\u946b\u91d1\u4e45\u5e86\u56fd\u5409\u7984\u5f18\u987a\u6c47\u6052\u6c47\u745e\u9686\u6d2a\u5149\u9f0e\u590d\u516c\u9f0e\u6cf0\u76db\u4f73\u6052\u9f0e\u4e2d\u98de\u805a\u4e9a\u5b8f\u76c8\u5149\u5b89\u8c26\u6210\u5408\u5de8\u6d2a\u98de\u5e86\"\n                   \"\u4e45\u745e\u6b63\u8302\u4fe1\u534f\u767e\u751f\u76db\u5408\u56fd\u5723\u76db\u540c\u540c\u76c8\u4fe1\u5b8f\u7984\u4ec1\u5927\u4e2d\u7687\u5b9d\u5fb7\u91d1\u53f0\u4f18\u957f\u6210\u6210\u4e9a\u76db\u516c\u7f8e\u8363\u6210\u660c\u4e45\u7984\u6cf0\u4e9a\u8fdb\u53f0\"\n                   \"\u8f89\u4f73\u51ef\u5b89\u4e45\u672c\u8363\u98de\u6676\u9686\u6676\u5f18\u540c\u4e30\u8f89\u534e\u9ad8\u5149\u5174\u5e86\u8d35\u5982\u8000\u98de\u4ec1\u5b8f\u6b23\u7687\u6d2a\u5b8f\u91d1\u6ee1\u9f0e\u8000\u5de8\u4e49\u5fb7\u660c\u6e90\u4e2d\u6d2a\u88d5\u7965\u6676\"\n                   \"\u672c\u56fd\u91d1\u6d2a\u660c\u91d1\u6e90\u6052\u798f\u4e07\u4e49\u4e45\u591a\u8c26\u9ad8\u4f73\u6b23\u548c\u51ef\u672c\u6cf0\u6625\u8d35\u5927\u6d69\u6c38\u5bff\u660c\u7984\u91d1\u5f18\u4ec1\u7f8e\u4e45\u5347\u4ea8\u8f89\u4e45\u8302\u7687\u5f18\u6cf0\u5fb7\u6210\"\n                   \"\u5b8f\u7f8e\u8f89\u8f89\u7984\u4ec1\u534e\u6676\u6625\u5e72\u5723\u957f\u540c\u8000\u5149\u5e86\u534e\u6676\u751f\u65b0\u8f89\u946b\u91d1\u6ee1\u4e2d\u5343\u8c26\u745e\u7965\u660c\u8302\u590d\u957f\u65b0\u7965\u7965\u798f\u540c\u4f18\u4f73\u6052\u5343\u5982\u5174\"\n                   \"\u88d5\u534e\u51ef\u5eb7\u5168\u8d35\u5de8\u65fa\u7965\u6377\u539a\u8d35\u5bcc\u5b8f\u4e49\u76db\u8c26\u540c\u76db\u540c\u76ca\u8c26\u6da6\u4e1c\u5e7f\u5343\u8fdb\u8f89\u5347\u590d\u660c\u805a\u5409\u98de\u98de\u5143\u516c\u53f0\u672c\u534e\u5347\u7f8e\u4e45\u957f\"\n                   \"\u5e86\u4e9a\u5347\u4e1c\u6b63\u9ad8\u5f18\u4e9a\u5e86\u548c\u5bff\u5b8f\u6ee1\u4e07\u4f18\u4f1f\u6d69\u65b0\u5408\u805a\u5e86\u4e07\u5e7f\u5bff\u4e1c\u6052\u5149\u5723\u6da6\u540c\u9ad8\u8c26\u660c\u5174\u4e49\u4ec1\u5b89\u672c\u6377\u516c\u8fdb\u5eb7\u76ca\u91d1\"\n                   \"\u5e86\u6b63\u8fdb\u6b63\u5343\u8f89\u548c\u5347\u672c\u76ca\u9ad8\u5e7f\u4e2d\u767e\u65b0\u5e86\u91d1\u540c\u5982\u9f0e\u5bff\u8302\u9f0e\u5e86\u8302\u745e\u5168\u7984\u8f89\u7f8e\u8d35\u4f18\u4e30\u76ca\u540c\u4fe1\u5174\u805a\u6d69\u65b0\u534f\u5b9d\u8000\u5723\"\n                   \"\u6676\u76c8\u98de\u5b89\u8363\u5bcc\u5343\u7965\u6210\u6e90\u88d5\u5408\u5174\u4f73\u88d5\u65fa\u91d1\u957f\u7984\u4ea8\u672c\u5927\u5fb7\u6210\u4ea8\u7687\u901a\u5168\u534e\u8d35\u5f18\u6210\u798f\u805a\u4fe1\u798f\u5149\u76db\u4e30\u6ee1\u5b8f\u798f\u76ca\u56fd\"\n                   \"\u5f18\u751f\u5f18\u6e90\u65b0\u4e07\u6cf0\u6210\u751f\u4f1f\u5174\u5174\u8f89\u548c\u5927\u5143\u548c\u534f\u901a\u5343\u5b9d\u534f\u4f1f\u8363\u957f\u7984\u6676\u76db\u6b23\u9686\u65b0\u672c\u590d\u6b63\u76db\u548c\u7687\u5347\u4e07\u76ca\u9ad8\u76c8\u4e49\u88d5\"\n                   \"\u6210\u4ec1\u5de8\u5f18\u5343\u4e9a\u8000\u5409\u5e86\u539a\u56fd\u65b0\u9ad8\u5229\u548c\u6da6\u4e2d\u6377\u4e9a\u4fe1\u767e\u5408\u4ea8\u4f73\u4f73\u591a\u4fe1\u946b\u6c38\u590d\u516c\u5343\u4f73\u6377\u5143\u4e1c\u5b9d\u534f\u5927\u8d35\u672c\u6ee1\u6cf0\u957f\"\n                   \"\u534f\u8000\u5723\u4ec1\u65fa\u751f\u5e72\u76db\u6052\u4e49\u591a\u5b8f\u76ca\u534f\u6da6\u957f\u7687\u4f1f\u6676\u8302\u5927\u8f89\u8c26\u591a\u53f0\u9ad8\u6052\u5de8\u5174\u8f89\u53f0\u534e\u5347\u6ee1\u516c\u5347\u6210\u5143\u5229\u5229\u539a\u9686\u88d5\u539a\"\n                   \"\u9ad8\u516c\u901a\u6d69\u51ef\u91d1\u7687\u5e86\u65b0\u53d1\u5b8f\u5927\u672c\u8c26\u5347\u6b23\u5347\u534e\u76ca\u5de8\u76ca\u767e\u8f89\u4ea8\u8f89\u6210\u6b23\u5e86\u540c\u6676\u745e\u4e49\u4e45\u6210\u4f73\u5229\u4f18\u8fdb\u6ee1\u5eb7\u4fe1\u76c8\u4e1c\u76db\"\n                   \"\u534e\u4e49\u516c\u8d35\u7f8e\u5b9d\u4fe1\u4e30\u6b63\u8c26\u65fa\u534e\u7687\u5409\u5982\u946b\u6cf0\u534f\u5168\u4f18\u798f\u5bff\u4e2d\u751f\u539a\u6210\u751f\u4e9a\u516c\u5f18\u987a\u5343\u4fe1\u7965\u548c\u5723\u91d1\u534e\u5eb7\u5fb7\u53f0\u987a\u5168\u539a\"\n                   \"\u534f\u4ea8\u7f8e\u4e07\u745e\u7f8e\u4e1c\u98de\u4e07\u98de\u5982\u957f\u4ec1\u9ad8\u5168\u6c47\u5347\u5b8f\u5229\u5409\u6cf0\u76ca\u53d1\u8c26\u4e9a\u6c47\u4e9a\u6052\u8000\u6052\u98de\u6d69\u76ca\u901a\u6377\u4ea8\u65b0\u6052\u767e\u4f73\u4e2d\u6210\u516c\u5723\"\n                   \"\u5b8f\u6ee1\u946b\u6210\u65fa\u7984\u5143\u798f\u51ef\u767e\u6c38\u4e1c\u6e90\u5e86\u8000\u4e07\u9f0e\u516c\u6625\u660c\u5e7f\u6da6\u5168\u805a\u5fb7\u65fa\u6d2a\u9686\u5b9d\u4f1f\u4ea8\u5408\u6ee1\u9686\u8fdb\u5347\u76db\u4e1c\u6b63\u65b0\u591a\u8fdb\u6d69\u5eb7\"\n                   \"\u957f\u5408\u5927\u8000\u548c\u7f8e\u539a\u5982\u5bff\u946b\u7984\u5fb7\u4ec1\u53d1\u5e86\u5149\u901a\u4e49\u8363\u76c8\u660c\u5347\u8363\u4f18\u534e\u56fd\u6210\u6b23\u5927\u5b8f\u4e30\u5149\u4e9a\u590d\u4e07\u5149\u6625\u9f0e\u6c47\u65fa\u548c\u8f89\u8f89\u4f1f\"\n                   \"\u6377\u6c47\u901a\u5bff\u8000\u76ca\u7687\u76db\u6676\u9686\u4e49\u540c\u5408\u76ca\u6625\u901a\u4e07\u98de\u5f18\u5982\u5b89\u4fe1\u672c\u5229\u5b89\u590d\u534f\u5e86\u5409\u65b0\u6c38\u4e45\u516c\u946b\u5e7f\u540c\u5bcc\u6e90\u516c\u5b8f\u53f0\u957f\u8f89\u8000\"\n                   \"\u5149\u5343\u4f73\u5b9d\u5eb7\u7965\u76db\u5bcc\u5347\u987a\u4e9a\u5409\u7687\u7f8e\u6da6\u4ec1\u5e7f\u4ec1\u53f0\u745e\u5e72\u9686\u7f8e\u4fe1\u4f18\u4f1f\u5b89\u751f\u5982\u6210\u8000\u76db\u6da6\u5347\u6b63\u5347\u65b0\u516c\u8363\u5b8f\u6052\u6d2a\u5723\u6cf0\"\n                   \"\u5f18\u5347\u7f8e\u76ca\u987a\u9686\u5927\u751f\u65b0\u8302\u590d\u4e30\u4e9a\u534e\u6052\u4ec1\u5f18\u5bcc\u516c\u7f8e\u660c\u5e72\u6c38\u6ee1\u6c47\u5982\u6d2a\u660c\u8363\u98de\u65b0\u8c26\u4e07\u767e\u4e30\u8fdb\u5b9d\u7984\u8d35\u5343\u751f\u8fdb\u5927\u6da6\"\n                   \"\u7984\u7965\u516c\u91d1\u7965\u805a\u5174\u548c\u65fa\u76c8\u6676\u767e\u4e49\u534f\u5de8\u987a\u88d5\u4e2d\u53d1\u5343\u8f89\u4ea8\u7f8e\u672c\u5143\u4e30\u91d1\u76c8\u76db\u65b0\u5168\u56fd\u6e90\u548c\u534f\u5bcc\u8c26\u53d1\u4e07\u8000\u798f\u5927\u53d1\u6d69\"\n                   \"\u9686\u6b63\u5b8f\u5347\u5f18\u65fa\u957f\u5fb7\u767e\u53d1\u9f0e\u91d1\u6ee1\u6625\u65b0\u6210\u65b0\u53f0\u6b63\u5f18\u6da6\u6676\u5927\u76c8\u8302\u539a\u5bcc\u6cf0\u901a\u539a\u534f\u767e\u6e90\u590d\u5e7f\u6052\u6b23\u5408\u5723\u672c\u5de8\u590d\u591a\u6b63\"\n                   \"\u4f1f\u6da6\u9ad8\u6ee1\u51ef\u4ec1\u51ef\u9ad8\u7984\u4e07\u672c\u590d\u4fe1\u6ee1\u5fb7\u5347\u8302\u91d1\u5982\u5bcc\u8c26\u65fa\u4f73\u7f8e\u76c8\u5343\u53d1\u5b9d\u7984\u8fdb\u5174\u9f0e\u4e30\u5723\u5e7f\u516c\u8fdb\u660c\u4e1c\u6da6\u8fdb\u4f18\u7965\u751f\"\n                   \"\u8f89\u8302\u5b89\u987a\u6b63\u4f1f\u5723\u5b9d\u4f18\u5e86\u539a\u65b0\u76ca\u4e9a\u946b\u7687\u6d69\u5174\u987a\u591a\u751f\u5bff\u91d1\u76ca\u5343\u4e30\u65fa\u4e49\u4e1c\u5149\u5e86\u6cf0\u5168\u534f\u5409\u5174\u5343\u745e\u4e30\u5174\u8302\u6cf0\u5e86\u6377\"\n                   \"\u4e30\u5347\u5f18\u8302\u9f0e\u6da6\u590d\u6c38\u53d1\u591a\u6210\u7f8e\u805a\u798f\u8d35\u5408\u5149\u4e9a\u805a\u5e86\u5927\u5927\u4e07\u987a\u8d35\u8fdb\u5149\u56fd\u987a\u98de\u8000\u4f73\u5408\u5de8\u6d2a\u6e90\u7965\u805a\u767e\u6c47\u5174\u672c\u6d2a\u8363\"\n                   \"\u5229\u6625\u5e86\u534f\u6210\u660c\u745e\u540c\u539a\u6625\u767e\u5149\u56fd\u5982\u5347\u540c\u4ec1\u4f73\u5408\u6210\u590d\u51ef\u4f73\u6c47\u5347\u9f0e\u5b9d\u5b9d\u8fdb\u6d2a\u548c\u4fe1\u660c\u5eb7\u6da6\u6e90\u5723\u5de8\u5eb7\u540c\u6b23\u6d69\u8f89\u6b63\"\n                   \"\u6c38\u6c47\u6cf0\u7984\u5f18\u9f0e\u591a\u539a\u548c\u4f73\u8fdb\u8363\u5982\u8302\u5168\u8d35\u7965\u98de\u7965\u7965\u6c47\u7984\u5408\u6e90\u76c8\u5982\u548c\u5e86\u5229\u5bff\u65fa\u6c47\u6625\u76c8\u8363\u6d2a\u5b8f\u51ef\u5b9d\u6da6\u5982\u6d2a\u91d1\u9f0e\"\n                   \"\u805a\u5b89\u548c\u5409\u5b8f\u6377\u4e9a\u4f1f\u7f8e\u6d2a\u5143\u5409\u539a\u8c26\u5409\u51ef\u6c47\u6676\u4e2d\u4e49\u5347\u534f\u5409\u5927\u76ca\u7965\u4e2d\u946b\u6210\u6b63\u76db\u798f\u6ee1\u8f89\u6210\u4ea8\u798f\u5bcc\u76ca\u6d2a\u539a\u7984\u4f73\u76ca\"\n                   \"\u4ea8\u5de8\u5723\u8f89\u539a\u7687\")\n    service_type = (\"\u54a8\u8be2,\u4e2d\u4ecb,\u79d1\u6280\u670d\u52a1,\u6587\u5316\u4ea4\u6d41\u670d\u52a1,\u6280\u672f\u670d\u52a1,\u4fe1\u606f\u670d\u52a1,\u96f6\u552e\u8d38\u6613,\u5236\u9020,\u6279\u53d1\u8d38\u6613,\u96c6\u56e2,\u9910\u996e\u670d\u52a1,\"\n                    \"\u9910\u996e\u7ba1\u7406,\u65c5\u6e38\u8d28\u8be2,\u4eba\u4e8b\u670d\u52a1\")\n    company_type = \"\u80a1\u4efd\u6709\u9650\u516c\u53f8,\u6709\u9650\u8d23\u4efb\u516c\u53f8\"\n\n    company_pattern = '{region_info}{middle_word}{service_type}{company_type}'\n\n    return company_pattern.format(region_info=random.choice(region_info.split(',')),\n                                  middle_word=''.join([random.choice(middle_word)\n                                                       for _ in range(random.randint(2, 5))]),\n                                  service_type=random.choice(service_type.split(',')),\n                                  company_type=random.choice(company_type.split(',')))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nemitting a record. Always check time", "response": "def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        Always check time\n        \"\"\"\n        try:\n            if self.check_base_filename(record):\n                self.build_base_filename()\n            FileHandler.emit(self, record)\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except:\n            self.handleError(record)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_base_filename(self, record):\n        time_tuple = time.localtime()\n\n        if self.suffix_time != time.strftime(self.suffix, time_tuple) or not os.path.exists(\n                self.baseFilename + '.' + self.suffix_time):\n            return 1\n        else:\n            return 0", "response": "Determine if the base filename of the record is the same as the base filename of the record."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndoing builder; in this case, old time stamp is removed from filename and a new time stamp is append to the filename", "response": "def build_base_filename(self):\n        \"\"\"\n        do builder; in this case,\n        old time stamp is removed from filename and\n        a new time stamp is append to the filename\n        \"\"\"\n        if self.stream:\n            self.stream.close()\n            self.stream = None\n\n        # remove old suffix\n        if self.suffix_time != \"\":\n            index = self.baseFilename.find(\".\" + self.suffix_time)\n            if index == -1:\n                index = self.baseFilename.rfind(\".\")\n            self.baseFilename = self.baseFilename[:index]\n\n        # add new suffix\n        current_time_tuple = time.localtime()\n        self.suffix_time = time.strftime(self.suffix, current_time_tuple)\n        self.baseFilename = self.baseFilename + \".\" + self.suffix_time\n\n        self.mode = 'a'\n        if not self.delay:\n            self.stream = self._open()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_years(months=0, refer=None):\n    if refer is None:\n        refer = datetime.now()\n    # \u8ba1\u7b97\u5f53\u524d\u603b\u6708\u4efd\u6570\n    try:\n        months_count = refer.year * 12 + refer.month\n    except Exception:\n        raise TypeError('refer except {}, got an {}'.format(type(datetime.now()), type(refer)))\n    \n    # \u8ba1\u7b97\u7ed3\u679c\u603b\u6708\u5206\u6570\n    months_count += months\n    \n    y, m = divmod(months_count, 12)\n    \n    # \u5c06m\u7684\u503c\u8f6c\u6362\u4e3a1-12\n    if m == 0:\n        y -= 1\n        m = 12\n    \n    return ''.join(['%04d' % y, '%02d' % m])", "response": "\u83b7\u53d6\u57fa\u51c6\u65f6\u6708\u4efd\u589e\u91cf\u7684\u5e74\u6708\n\n    :param:\n        * months: (int) \u6708\u4efd\u589e\u91cf\uff0c\u6b63\u6570\u4e3a\u5f80\u540e\u5e74\u6708\uff0c\u6574\u6570\u4e3a\u5f80\u524d\u5e74\u6708\n        * refer: (datetime obj) datetime \u5bf9\u8c61\uff0c\u6216\u8005\u6709 month \u548c year \u5c5e\u6027\u7684\u5b9e\u4f8b\uff0c\u9ed8\u8ba4\u4e3a\u5f53\u524d\u65f6\u95f4\n    :return:\n        * result: (string) \u5e74\u6708\u5b57\u7b26\u4e32\n\n    \u4e3e\u4f8b\u5982\u4e0b::\n\n        print('--- get_years demo ---')\n        print(get_years(-5))\n        print(get_years(7, datetime.now()))\n        print('---')\n\n    \u6267\u884c\u7ed3\u679c::\n\n        --- get_years demo ---\n        201801\n        201901\n        ---"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_time_interval(start_time, end_time):\n    if not isinstance(start_time, int) or not isinstance(end_time, int):\n        raise TypeError('start_time and end_time should be int, bu we got {0} and {1}'.\n                        format(type(start_time), type(end_time)))\n        \n    # \u8ba1\u7b97\u5929\u6570\n    time_diff = abs(end_time - start_time)\n    days = (time_diff // (60*60*24))\n\n    # \u8ba1\u7b97\u5c0f\u65f6\u6570\n    remain = time_diff % (60*60*24)\n    hours = (remain // (60*60))\n\n    # \u8ba1\u7b97\u5206\u949f\u6570\n    remain = remain % (60*60)\n    minutes = (remain // 60)\n\n    # \u8ba1\u7b97\u79d2\u6570\n    seconds = remain % 60\n    interval_dict = {\"days\": days, \"hours\": hours, \"minutes\": minutes, \"seconds\": seconds}\n\n    return interval_dict", "response": "get_time_interval - Get the time interval of a node"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef transform_datetime_to_unix(dtime=None):\n    if not dtime:\n        dtime = datetime.now()\n\n    if not isinstance(dtime, datetime):\n        raise TypeError('dtime should be datetime, but we got {}'.format(type(dtime)))\n\n    return time.mktime(dtime.timetuple())", "response": "transform_datetime_to_unix - transform a datetime to a unix time"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef date_time_this_month():\n        now = datetime.now()\n        this_month_start = now.replace(\n            day=1, hour=0, minute=0, second=0, microsecond=0)\n        this_month_days = calendar.monthrange(now.year, now.month)\n        random_seconds = random.randint(0, this_month_days[1]*A_DAY_SECONDS)\n\n        return this_month_start + timedelta(seconds=random_seconds)", "response": "Get a random date time for a specific month."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef date_time_this_year():\n        now = datetime.now()\n        this_year_start = now.replace(\n            month=1, day=1, hour=0, minute=0, second=0, microsecond=0)\n        this_year_days = sum(calendar.mdays)\n        random_seconds = random.randint(0, this_year_days*A_DAY_SECONDS)\n\n        return this_year_start + timedelta(seconds=random_seconds)", "response": "Get a random date time for a specific year."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating date string for a given year.", "response": "def gen_date_by_year(year):\n        \"\"\"\n        \u83b7\u53d6\u5f53\u524d\u5e74\u7684\u968f\u673a\u65f6\u95f4\u5b57\u7b26\u4e32\n\n        :param:\n            * year: (string) \u957f\u5ea6\u4e3a 4 \u4f4d\u7684\u5e74\u4efd\u5b57\u7b26\u4e32\n\n        :return:\n            * date_str: (string) \u4f20\u5165\u5e74\u4efd\u7684\u968f\u673a\u5408\u6cd5\u7684\u65e5\u671f\n        \n        \u4e3e\u4f8b\u5982\u4e0b::\n        \n            print('--- GetRandomTime.gen_date_by_year demo ---')\n            print(GetRandomTime.gen_date_by_year(\"2010\"))\n            print('---')\n        \n        \u6267\u884c\u7ed3\u679c::\n        \n            --- GetRandomTime.gen_date_by_year demo ---\n            20100505\n            ---\n        \"\"\"\n        if isinstance(year, int) and len(str(year)) != 4:\n            raise ValueError(\"year should be int year like 2018, but we got {}, {}\".\n                             format(year, type(year)))\n        if isinstance(year, str) and len(year) != 4:\n            raise ValueError(\"year should be string year like '2018', but we got {}, {}\".\n                             format(year, type(year)))\n        if isinstance(year, int):\n            year = str(year)\n\n        date_str = GetRandomTime.gen_date_by_range(year + \"-01-01\", year + \"-12-31\", \"%Y%m%d\")\n\n        return date_str"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef strftime(date_time=None, time_format=None):\n        if not date_time:\n            datetime_now = datetime.now()\n        else:\n            datetime_now = date_time\n        if not time_format:\n            time_format = '%Y/%m/%d %H:%M:%S'\n        return datetime.strftime(datetime_now, time_format)", "response": "strftime - \u5c06 datetime \u5bf9\u8c61\u8f6c\u6362\u4e3a str"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef strptime(time_str, time_format):\n        try:\n            datetime_obj = datetime.strptime(time_str, time_format)\n            return datetime_obj\n        except ValueError as ex:\n            raise ValueError(ex)", "response": "\u5c06 str \u8f6c\u6362\u4e3a datetime \u5bf9\u8c61\n\n        :param:\n            * time_str: (string) \u65e5\u671f\u5b57\u7b26\u4e32\n            * time_format: (sting) \u65e5\u671f\u683c\u5f0f\u5b57\u7b26\u4e32\n        :return:\n            * datetime_obj: (obj) datetime \u5bf9\u8c61"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef init_project_by_yml(project_config=None, dist=None):\n    if os.path.isfile(project_config):\n        project_config = open(project_config)\n    try:\n        yml_data = yaml.load(project_config)\n        project_name = yml_data['project']\n        project_tree = yml_data['tree']\n    except Exception as e:\n        raise KeyError('project config format Error: {}'.format(e))\n    if dist is None:\n        dist = os.path.abspath('.')\n    project_path = os.path.join(dist, project_name)\n    _makedirs_if_not_extis(project_path)\n    _makedirs_and_files(project_path, project_tree)", "response": "init_project_by_yml - init project by yml"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_abs_filename_with_sub_path(sub_path, filename):\n\n    \"\"\"\n        \u751f\u6210\u5f53\u524d\u8def\u5f84\u4e0b\u4e00\u7ea7\u8def\u5f84\u67d0\u6587\u4ef6\u7684\u5b8c\u6574\u6587\u4ef6\u540d\uff1b\n\n        :param:\n            * sub_path: (string) \u4e0b\u4e00\u7ea7\u7684\u67d0\u8def\u5f84\u540d\u79f0\n            * filename: (string) \u4e0b\u4e00\u7ea7\u8def\u5f84\u7684\u67d0\u4e2a\u6587\u4ef6\u540d\n        :returns:\n            * \u8fd4\u56de\u7c7b\u578b (tuple)\uff0c\u6709\u4e24\u4e2a\u503c\uff0c\u7b2c\u4e00\u4e2a\u4e3a flag\uff0c\u7b2c\u4e8c\u4e2a\u4e3a\u6587\u4ef6\u540d\uff0c\u8bf4\u660e\u89c1\u4e0b\n            * flag: (bool) \u5982\u679c\u6587\u4ef6\u5b58\u5728\uff0c\u8fd4\u56de True\uff0c\u6587\u4ef6\u4e0d\u5b58\u5728\uff0c\u8fd4\u56de False\n            * abs_filename: (string) \u6307\u5b9a filename \u7684\u5305\u542b\u8def\u5f84\u7684\u957f\u6587\u4ef6\u540d\n\n        \u4e3e\u4f8b\u5982\u4e0b::\n            \n            print('--- get_abs_filename_with_sub_path demo ---')\n            # define sub dir\n            path_name = 'sub_dir'\n            # define not exists file\n            filename = 'test_file.txt'\n\n            abs_filename = get_abs_filename_with_sub_path(path_name, filename)\n            # return False and abs filename\n            print(abs_filename)\n\n            # define exists file\n            filename = 'demo.txt'\n            abs_filename = get_abs_filename_with_sub_path(path_name, filename)\n            # return True and abs filename\n            print(abs_filename)\n            print('---')\n\n        \u8f93\u51fa\u7ed3\u679c::\n            \n            --- get_abs_filename_with_sub_path demo ---\n            (False, '/Users/****/Documents/dev_python/fishbase/demo/sub_dir/test_file.txt')\n            (True, '/Users/****/Documents/dev_python/fishbase/demo/sub_dir/demo.txt')\n            ---\n            \n    \"\"\"\n\n    try:\n        cur_path = pathlib.Path.cwd()\n        abs_filename = cur_path / pathlib.Path(sub_path) / filename\n        flag = pathlib.Path.is_file(abs_filename)\n\n        # \u5c06 path \u5bf9\u8c61\u8f6c\u6362\u6210\u5b57\u7b26\u4e32\n        return flag, str(abs_filename)\n\n    except:\n\n        flag = False\n        return flag, None", "response": "get_abs_filename_with_sub_path - Get absolute filename of a file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_sub_path_create(sub_path):\n\n    \"\"\"\n    \u68c0\u67e5\u5f53\u524d\u8def\u5f84\u4e0b\u7684\u67d0\u4e2a\u5b50\u8def\u5f84\u662f\u5426\u5b58\u5728, \u4e0d\u5b58\u5728\u5219\u521b\u5efa\uff1b\n\n    :param:\n        * sub_path: (string) \u4e0b\u4e00\u7ea7\u7684\u67d0\u8def\u5f84\u540d\u79f0\n    :return:\n        * \u8fd4\u56de\u7c7b\u578b (tuple)\uff0c\u6709\u4e24\u4e2a\u503c\n        * True: \u8def\u5f84\u5b58\u5728\uff0cFalse: \u4e0d\u9700\u8981\u521b\u5efa\n        * False: \u8def\u5f84\u4e0d\u5b58\u5728\uff0cTrue: \u521b\u5efa\u6210\u529f\n\n    \u4e3e\u4f8b\u5982\u4e0b::\n        \n        print('--- check_sub_path_create demo ---')\n        # \u5b9a\u4e49\u5b50\u8def\u5f84\u540d\u79f0\n        sub_path = 'demo_sub_dir'\n        # \u68c0\u67e5\u5f53\u524d\u8def\u5f84\u4e0b\u7684\u4e00\u4e2a\u5b50\u8def\u5f84\u662f\u5426\u5b58\u5728\uff0c\u4e0d\u5b58\u5728\u5219\u521b\u5efa\n        print('check sub path:', sub_path)\n        result = check_sub_path_create(sub_path)\n        print(result)\n        print('---')\n\n    \u8f93\u51fa\u7ed3\u679c::\n        \n        --- check_sub_path_create demo ---\n        check sub path: demo_sub_dir\n        (True, False)\n        ---\n        \n    \"\"\"\n\n    # \u83b7\u5f97\u5f53\u524d\u8def\u5f84\n    temp_path = pathlib.Path()\n    cur_path = temp_path.resolve()\n\n    # \u751f\u6210 \u5e26\u6709 sub_path_name \u7684\u8def\u5f84\n    path = cur_path / pathlib.Path(sub_path)\n\n    # \u5224\u65ad\u662f\u5426\u5b58\u5728\u5e26\u6709 sub_path \u8def\u5f84\n    if path.exists():\n        # \u8fd4\u56de True: \u8def\u5f84\u5b58\u5728, False: \u4e0d\u9700\u8981\u521b\u5efa\n        return True, False\n    else:\n        path.mkdir(parents=True)\n        # \u8fd4\u56de False: \u8def\u5f84\u4e0d\u5b58\u5728  True: \u8def\u5f84\u5df2\u7ecf\u521b\u5efa\n        return False, True", "response": "check if a sub_path exists in the nunjucks tree"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget checkcode from id_number_str", "response": "def get_checkcode(cls, id_number_str):\n        \"\"\"\n        \u8ba1\u7b97\u8eab\u4efd\u8bc1\u53f7\u7801\u7684\u6821\u9a8c\u4f4d\uff1b\n\n        :param:\n            * id_number_str: (string) \u8eab\u4efd\u8bc1\u53f7\u7684\u524d17\u4f4d\uff0c\u6bd4\u5982 3201241987010100\n        :returns:\n            * \u8fd4\u56de\u7c7b\u578b (tuple)\n            * flag: (bool) \u5982\u679c\u8eab\u4efd\u8bc1\u53f7\u683c\u5f0f\u6b63\u786e\uff0c\u8fd4\u56de True\uff1b\u683c\u5f0f\u9519\u8bef\uff0c\u8fd4\u56de False\n            * checkcode: \u8ba1\u7b97\u8eab\u4efd\u8bc1\u524d17\u4f4d\u7684\u6821\u9a8c\u7801\n\n        \u4e3e\u4f8b\u5982\u4e0b::\n\n            from fishbase.fish_data import *\n\n            print('--- fish_data get_checkcode demo ---')\n\n            # id number\n            id1 = '32012419870101001'\n            print(id1, IdCard.get_checkcode(id1)[1])\n\n            # id number\n            id2 = '13052219840731647'\n            print(id2, IdCard.get_checkcode(id2)[1])\n\n            print('---')\n\n        \u8f93\u51fa\u7ed3\u679c::\n\n            --- fish_data get_checkcode demo ---\n            32012419870101001 5\n            13052219840731647 1\n            ---\n\n        \"\"\"\n\n        # \u5224\u65ad\u957f\u5ea6\uff0c\u5982\u679c\u4e0d\u662f 17 \u4f4d\uff0c\u76f4\u63a5\u8fd4\u56de\u5931\u8d25\n        if len(id_number_str) != 17:\n            return False, -1\n\n        id_regex = '[1-9][0-9]{14}([0-9]{2}[0-9X])?'\n\n        if not re.match(id_regex, id_number_str):\n            return False, -1\n\n        items = [int(item) for item in id_number_str]\n\n        # \u52a0\u6743\u56e0\u5b50\u8868\n        factors = (7, 9, 10, 5, 8, 4, 2, 1, 6, 3, 7, 9, 10, 5, 8, 4, 2)\n\n        # \u8ba1\u7b9717\u4f4d\u6570\u5b57\u5404\u4f4d\u6570\u5b57\u4e0e\u5bf9\u5e94\u7684\u52a0\u6743\u56e0\u5b50\u7684\u4e58\u79ef\n        copulas = sum([a * b for a, b in zip(factors, items)])\n\n        # \u6821\u9a8c\u7801\u8868\n        check_codes = ('1', '0', 'X', '9', '8', '7', '6', '5', '4', '3', '2')\n\n        checkcode = check_codes[copulas % 11].upper()\n\n        return True, checkcode"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_number(cls, id_number):\n        if isinstance(id_number, int):\n            id_number = str(id_number)\n\n        # \u8c03\u7528\u51fd\u6570\u8ba1\u7b97\u8eab\u4efd\u8bc1\u524d\u976217\u4f4d\u7684 checkcode\n        result = IdCard.get_checkcode(id_number[0:17])\n\n        # \u8fd4\u56de\u7b2c\u4e00\u4e2a flag \u662f\u9519\u8bef\u7684\u8bdd\uff0c\u8868\u793a\u8eab\u4efd\u8bc1\u683c\u5f0f\u9519\u8bef\uff0c\u76f4\u63a5\u900f\u4f20\u8fd4\u56de\uff0c\u7b2c\u4e8c\u4e2a\u4e3a\u83b7\u5f97\u7684\u6821\u9a8c\u7801\n        flag = result[0]\n        checkcode = result[1]\n\n        if not flag:\n            return flag,\n\n        # \u5224\u65ad\u6821\u9a8c\u7801\u662f\u5426\u6b63\u786e\n        return checkcode == id_number[-1].upper(),", "response": "check_number - Check if a number is in the cache"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_zone_info(cls, area_str, match_type='EXACT', result_type='LIST'):\n        values = []\n\n        if match_type == 'EXACT':\n            values = sqlite_query('fish_data.sqlite',\n                                  'select zone, areanote from cn_idcard where areanote = :area', {\"area\": area_str})\n        if match_type == 'FUZZY':\n            values = sqlite_query('fish_data.sqlite',\n                                  'select zone, areanote from cn_idcard where areanote like :area',\n                                  {\"area\": '%' + area_str + '%'})\n\n        # result_type \u7ed3\u679c\u6570\u91cf\u5224\u65ad\u5904\u7406\n\n        if result_type == 'LIST':\n            # \u5982\u679c\u8fd4\u56de\u8bb0\u5f55\u591a\uff0c\u5927\u4e8e 20 \u9879\uff0c\u53ea\u8fd4\u56de\u524d\u9762 20 \u4e2a\u7ed3\u679c\n            if len(values) > 20:\n                values = values[0:20]\n\n            return values\n\n        if result_type == 'SINGLE_STR':\n            if len(values) == 0:\n                return ''\n            if len(values) > 0:\n                value_str = values[0][0]\n                return value_str", "response": "Get zone info from a string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget checkcode from card number", "response": "def get_checkcode(cls, card_number_str):\n        \"\"\"\n        \u8ba1\u7b97\u94f6\u884c\u5361\u6821\u9a8c\u4f4d\uff1b\n\n        :param:\n            * card_number_str: (string) \u8981\u67e5\u8be2\u7684\u94f6\u884c\u5361\u53f7\n        :returns:\n            checkcode: (string) \u94f6\u884c\u5361\u7684\u6821\u9a8c\u4f4d\n\n        \u4e3e\u4f8b\u5982\u4e0b::\n\n            from fishbase.fish_data import *\n\n            print('--- fish_data get_checkcode demo ---')\n\n            # \u4e0d\u80fd\u653e\u771f\u7684\u5361\u4fe1\u606f\uff0c\u6709\u98ce\u9669\n            print(CardBin.get_checkcode('439188000699010'))\n\n            print('---')\n\n        \u8f93\u51fa\u7ed3\u679c::\n\n            --- fish_data get_checkcode demo ---\n            9\n            ---\n\n        \"\"\"\n        total = 0\n        even = True\n\n        for item in card_number_str[-1::-1]:\n            item = int(item)\n            if even:\n                item <<= 1\n            if item > 9:\n                item -= 9\n            total += item\n            even = not even\n\n        checkcode = (10 - (total % 10)) % 10\n\n        return str(checkcode)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_bankcard(cls, card_number_str):\n\n        if isinstance(card_number_str, int):\n            card_number_str = str(card_number_str)\n\n        checkcode = card_number_str[-1]\n\n        result = CardBin.get_checkcode(card_number_str[0:-1])\n\n        return checkcode == result", "response": "check_bankcard \u53d6\u5f97 \u5361 \u6821\u9a8c\u4f4d"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses ini file into dict", "response": "def conf_as_dict(conf_filename, encoding=None, case_sensitive=False):\n    \"\"\"\n    \u8bfb\u5165 ini \u914d\u7f6e\u6587\u4ef6\uff0c\u8fd4\u56de\u6839\u636e\u914d\u7f6e\u6587\u4ef6\u5185\u5bb9\u751f\u6210\u7684\u5b57\u5178\u7c7b\u578b\u53d8\u91cf\uff1b\n\n    :param:\n        * conf_filename: (string) \u9700\u8981\u8bfb\u5165\u7684 ini \u914d\u7f6e\u6587\u4ef6\u957f\u6587\u4ef6\u540d\n        * encoding: (string) \u6587\u4ef6\u7f16\u7801\n        * case_sensitive: (bool) \u662f\u5426\u5927\u5c0f\u5199\u654f\u611f\uff0c\u9ed8\u8ba4\u4e3a False\n    :return:\n        * flag: (bool) \u8bfb\u53d6\u914d\u7f6e\u6587\u4ef6\u662f\u5426\u6b63\u786e\uff0c\u6b63\u786e\u8fd4\u56de True\uff0c\u9519\u8bef\u8fd4\u56de False\n        * d: (dict) \u5982\u679c\u8bfb\u53d6\u914d\u7f6e\u6587\u4ef6\u6b63\u786e\u8fd4\u56de\u7684\u5305\u542b\u914d\u7f6e\u6587\u4ef6\u5185\u5bb9\u7684\u5b57\u5178\uff0c\u5b57\u5178\u5185\u5bb9\u987a\u5e8f\u4e0e\u914d\u7f6e\u6587\u4ef6\u987a\u5e8f\u4fdd\u6301\u4e00\u81f4\n        * count: (int) \u8bfb\u53d6\u5230\u7684\u914d\u7f6e\u6587\u4ef6\u6709\u591a\u5c11\u4e2a key \u7684\u6570\u91cf\n\n    \u4e3e\u4f8b\u5982\u4e0b::\n\n        print('--- conf_as_dict demo---')\n        # \u5b9a\u4e49\u914d\u7f6e\u6587\u4ef6\u540d\n        conf_filename = 'test_conf.ini'\n        # \u8bfb\u53d6\u914d\u7f6e\u6587\u4ef6\n        ds = conf_as_dict(conf_filename)\n        ds1 = conf_as_dict(conf_filename, case_sensitive=True)\n        # \u663e\u793a\u662f\u5426\u6210\u529f\uff0c\u6240\u6709 dict \u7684\u5185\u5bb9\uff0cdict \u7684 key \u6570\u91cf\n        print('flag:', ds[0])\n        print('dict:', ds[1])\n        print('length:', ds[2])\n\n        d = ds[1]\n        d1 = ds1[1]\n\n        # \u663e\u793a\u4e00\u4e2a section \u4e0b\u7684\u6240\u6709\u5185\u5bb9\n        print('section show_opt:', d['show_opt'])\n        # \u663e\u793a\u4e00\u4e2a section \u4e0b\u7684\u6240\u6709\u5185\u5bb9\uff0c\u5927\u5c0f\u5199\u654f\u611f\n        print('section show_opt:', d1['show_opt'])\n        # \u663e\u793a\u4e00\u4e2a section \u4e0b\u9762\u7684 key \u7684 value \u5185\u5bb9\n        print('section show_opt, key short_opt:', d['show_opt']['short_opt'])\n\n        # \u8bfb\u53d6\u4e00\u4e2a\u590d\u6742\u7684section\uff0c\u5148\u8bfb\u51fa key \u4e2d\u7684 count \u5185\u5bb9\uff0c\u518d\u904d\u5386\u6bcf\u4e2a key \u7684 value\n        i = int(d['get_extra_rules']['erule_count'])\n        print('section get_extra_rules, key erule_count:', i)\n        for j in range(i):\n            print('section get_extra_rules, key erule_type:', d['get_extra_rules']['erule_'+str(j)])\n        print('---')\n\n    \u6267\u884c\u7ed3\u679c::\n\n        --- conf_as_dict demo---\n        flag: True\n        dict: (omit)\n        length: 7\n        section show_opt: {'short_opt': 'b:d:v:p:f:', 'long_opt': 'region=,prov=,mer_id=,mer_short_name=,web_status='}\n        section show_opt: {'Short_Opt': 'b:d:v:p:f:', 'Long_Opt': 'region=,prov=,mer_id=,mer_short_name=,web_status='}\n        section show_opt, key short_opt: b:d:v:p:f:\n        section get_extra_rules, key erule_count: 2\n        section get_extra_rules, key erule_type: extra_rule_1\n        section get_extra_rules, key erule_type: extra_rule_2\n        ---\n\n    \"\"\"\n    flag = False\n    \n    # \u68c0\u67e5\u6587\u4ef6\u662f\u5426\u5b58\u5728\n    if not pathlib.Path(conf_filename).is_file():\n        return flag,\n    \n    # \u5224\u65ad\u662f\u5426\u5bf9\u5927\u5c0f\u5199\u654f\u611f\n    cf = configparser.ConfigParser() if not case_sensitive else MyConfigParser()\n    \n    # \u8bfb\u5165 config \u6587\u4ef6\n    try:\n        if sys.version > '3':\n            cf.read(conf_filename, encoding=encoding)\n        else:\n            cf.read(conf_filename)\n    except:\n        flag = False\n        return flag,\n    \n    d = OrderedDict(cf._sections)\n    for k in d:\n        d[k] = OrderedDict(cf._defaults, **d[k])\n        d[k].pop('__name__', None)\n    \n    flag = True\n    \n    # \u8ba1\u7b97\u6709\u591a\u5c11 key\n    count = len(d.keys())\n    \n    return flag, d, count"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef serialize_instance(obj):\n    obj_dict = {'__classname__': type(obj).__name__}\n    obj_dict.update(obj.__dict__)\n    for key, value in obj_dict.items():\n        if not isinstance(value, commonDataType):\n            sub_dict = serialize_instance(value)\n            obj_dict.update({key: sub_dict})\n        else:\n            continue\n    return obj_dict", "response": "Serializes an object to a dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef has_space_element(source):\n    if isinstance(source, dict):\n        check_list = list(source.values())\n    elif isinstance(source, list) or isinstance(source, tuple):\n        check_list = list(source)\n    else:\n        raise TypeError('source except list, tuple or dict, but got {}'.format(type(source)))\n    for i in check_list:\n        if i is 0:\n            continue\n        if not (i and str(i).strip()):\n            return True\n    return False", "response": "test if there is a space element in the list"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef if_json_contain(left_json, right_json, op='strict'):\n    \n    key_list = left_json.keys()\n    \n    if op == 'strict':\n        for key in key_list:\n            if not right_json.get(key) == left_json.get(key):\n                return False\n        return True", "response": "check if json contains"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef join_url_params(dic):\n    \n    od = OrderedDict(sorted(dic.items()))\n    \n    url = '?'\n    temp_str = urlencode(od)\n    \n    url = url + temp_str\n    \n    return url", "response": "join_url_params - join url params"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef has_special_char(p_str, check_style=charChinese):\n    if check_style == charChinese:\n        check_pattern = re.compile(u'[\\u4e00-\\u9fa5]+')\n    elif check_style == charNum:\n        check_pattern = re.compile(u'[0-9]+')\n    else:\n        return False\n    \n    try:\n        if check_pattern.search(p_str):\n            return True\n        else:\n            return False\n    except TypeError as ex:\n        raise TypeError(str(ex))", "response": "Check if a string contains a special character."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_files(path, exts=None):\n    files_list = []\n    for root, dirs, files in os.walk(path):\n        for name in files:\n            files_list.append(os.path.join(root, name))\n    \n    if exts is not None:\n        return [file for file in files_list if pathlib.Path(file).suffix in exts]\n    \n    return files_list", "response": "find_files - Finds all files in path and returns a list of all files in the order they appear."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_random_str(length, letters=True, digits=False, punctuation=False):\n    show_deprecation_warn('get_random_str', 'fish_random.gen_random_str')\n    from fishbase.fish_random import gen_random_str\n    return gen_random_str(length, length, has_letter=letters, has_digit=digits,\n                          has_punctuation=punctuation)", "response": "\u83b7\u5f97\u6307\u5b9a\u957f\u5ea6\uff0c\u4e0d\u540c\u89c4\u5219\u7684\u968f\u673a\u5b57\u7b26\u4e32\uff0c\u53ef\u4ee5\u5305\u542b\u6570\u5b57\uff0c\u5b57\u6bcd\u548c\u6807\u70b9\u7b26\u53f7\n\n    :param:\n        * length: (int) \u968f\u673a\u5b57\u7b26\u4e32\u7684\u957f\u5ea6\n        * letters: (bool) \u968f\u673a\u5b57\u7b26\u4e32\u662f\u5426\u5305\u542b\u5b57\u6bcd\uff0c\u9ed8\u8ba4\u5305\u542b\n        * digits: (bool) \u968f\u673a\u5b57\u7b26\u4e32\u662f\u5426\u5305\u542b\u6570\u5b57\uff0c\u9ed8\u8ba4\u4e0d\u5305\u542b\n        * punctuation: (bool) \u968f\u673a\u5b57\u7b26\u4e32\u662f\u5426\u5305\u542b\u7279\u6b8a\u6807\u70b9\u7b26\u53f7\uff0c\u9ed8\u8ba4\u4e0d\u5305\u542b\n\n    :return:\n        * random_str: (string) \u6307\u5b9a\u89c4\u5219\u7684\u968f\u673a\u5b57\u7b26\u4e32\n\n    \u4e3e\u4f8b\u5982\u4e0b::\n\n        print('--- get_random_str demo---')\n        print(get_random_str(6))\n        print(get_random_str(6, digits=True))\n        print(get_random_str(12, punctuation=True))\n        print(get_random_str(6, letters=False, digits=True))\n        print(get_random_str(12, letters=False, digits=True, punctuation=True))\n        print('---')\n\n    \u6267\u884c\u7ed3\u679c::\n\n        --- get_random_str demo---\n        nRBDHf\n        jXG5wR\n        )I;rz{ob&Clg\n        427681\n        *\"4$0^`2}%9{\n        ---"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsorts objects by attribute.", "response": "def sort_objs_by_attr(objs, key, reverse=False):\n    \"\"\"\n    \u5bf9\u539f\u751f\u4e0d\u652f\u6301\u6bd4\u8f83\u64cd\u4f5c\u7684\u5bf9\u8c61\u6839\u636e\u5c5e\u6027\u6392\u5e8f\n\n    :param:\n        * objs: (list) \u9700\u8981\u6392\u5e8f\u7684\u5bf9\u8c61\u5217\u8868\n        * key: (string) \u9700\u8981\u8fdb\u884c\u6392\u5e8f\u7684\u5bf9\u8c61\u5c5e\u6027\n        * reverse: (bool) \u6392\u5e8f\u7ed3\u679c\u662f\u5426\u8fdb\u884c\u53cd\u8f6c\uff0c\u9ed8\u8ba4\u4e3a False\uff0c\u4e0d\u8fdb\u884c\u53cd\u8f6c\n\n    :return:\n        * result: (list) \u6392\u5e8f\u540e\u7684\u5bf9\u8c61\u5217\u8868\n\n    \u4e3e\u4f8b\u5982\u4e0b::\n\n        print('--- sorted_objs_by_attr demo---')\n\n\n        class User(object):\n            def __init__(self, user_id):\n                self.user_id = user_id\n\n\n        users = [User(23), User(3), User(99)]\n        result = sorted_objs_by_attr(users, key='user_id')\n        reverse_result = sorted_objs_by_attr(users, key='user_id', reverse=True)\n        print([item.user_id for item in result])\n        print([item.user_id for item in reverse_result])\n        print('---')\n\n    \u6267\u884c\u7ed3\u679c::\n\n        --- sorted_objs_by_attr demo---\n        [3, 23, 99]\n        [99, 23, 3]\n        ---\n\n    \"\"\"\n    if len(objs) == 0:\n        return []\n    if not hasattr(objs[0], key):\n        raise AttributeError('{0} object has no attribute {1}'.format(type(objs[0]), key))\n    result = sorted(objs, key=attrgetter(key), reverse=reverse)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_query_param_from_url(url):\n    url_obj = urlsplit(url)\n    query_dict = parse_qs(url_obj.query)\n    \n    return OrderedDict(query_dict)", "response": "get query param from url"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the n - tuple of data_list", "response": "def paging(data_list, group_number=1, group_size=10):\n    \"\"\"\n    \u83b7\u53d6\u5206\u7ec4\u5217\u8868\u6570\u636e\n\n    :param:\n        * data_list: (list) \u9700\u8981\u83b7\u53d6\u5206\u7ec4\u7684\u6570\u636e\u5217\u8868\n        * group_number: (int) \u5206\u7ec4\u4fe1\u606f\uff0c\u9ed8\u8ba4\u4e3a 1\n        * group_size: (int) \u5206\u7ec4\u5927\u5c0f\uff0c\u9ed8\u8ba4\u4e3a 10\n\n    :return:\n        * group_data: (list) \u5206\u7ec4\u6570\u636e\n\n    \u4e3e\u4f8b\u5982\u4e0b::\n\n        print('--- paging demo---')\n        all_records = [1, 2, 3, 4, 5]\n        print(get_group_list_data(all_records))\n\n        all_records1 = list(range(100))\n        print(get_group_list_data(all_records1, group_number=5, group_size=15))\n        print(get_group_list_data(all_records1, group_number=7, group_size=15))\n        print('---')\n\n    \u6267\u884c\u7ed3\u679c::\n\n        --- paging demo---\n        [1, 2, 3, 4, 5]\n        [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]\n        [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n        ---\n\n    \"\"\"\n    if not isinstance(data_list, list):\n        raise TypeError('data_list should be a list, but we got {}'.format(type(data_list)))\n    \n    if not isinstance(group_number, int) or not isinstance(group_size, int):\n        raise TypeError('group_number and group_size should be int, but we got group_number: {0}, '\n                        'group_size: {1}'.format(type(group_number), type(group_size)))\n    if group_number < 0 or group_size < 0:\n        raise ValueError('group_number and group_size should be positive int, but we got '\n                         'group_number: {0}, group_size: {1}'.format(group_number, group_size))\n    \n    start = (group_number - 1) * group_size\n    end = group_number * group_size\n    \n    return data_list[start:end]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_sub_dict(data_dict, key_list, default_value='default_value'):\n    if not isinstance(data_dict, dict):\n        raise TypeError('data_dict should be dict, but we got {}'.format(type(data_dict)))\n    \n    if not isinstance(key_list, list):\n        raise TypeError('key_list should be list, but we got {}'.format(type(key_list)))\n    \n    sub_dict = dict()\n    for item in key_list:\n        sub_dict.update({item: data_dict.get(item, default_value)})\n    return sub_dict", "response": "get_sub_dict - get sub dict of the base class"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting camel case to underline", "response": "def camelcase_to_underline(param_dict):\n    \"\"\"\n    \u5c06\u9a7c\u5cf0\u547d\u540d\u7684\u53c2\u6570\u5b57\u5178\u952e\u8f6c\u6362\u4e3a\u4e0b\u5212\u7ebf\u53c2\u6570\n\n    :param:\n        * param_dict: (dict) \u8bf7\u6c42\u53c2\u6570\u5b57\u5178\n\n    :return:\n        * temp_dict: (dict) \u8f6c\u6362\u540e\u7684\u53c2\u6570\u5b57\u5178\n\n    \u4e3e\u4f8b\u5982\u4e0b::\n\n        print('--- transform_hump_to_underline demo---')\n        hump_param_dict = {'firstName': 'Python', 'Second_Name': 'san', 'right_name': 'name'}\n        underline_param_dict = transform_hump_to_underline(hump_param_dict )\n        print(underline_param_dict )\n        print('---')\n\n    \u6267\u884c\u7ed3\u679c::\n\n        --- transform_hump_to_underline demo---\n        {'first_name': 'Python', 'second_name': 'san', 'right_name': 'name'}\n        ---\n\n    \"\"\"\n    temp_dict = copy.deepcopy(param_dict)\n    \n    # \u6b63\u5219\n    hump_to_underline = re.compile(r'([a-z]|\\d)([A-Z])')\n    for key in list(param_dict.keys()):\n        # \u5c06\u9a7c\u5cf0\u503c\u66ff\u6362\u4e3a\u4e0b\u5212\u7ebf\n        underline_sub = re.sub(hump_to_underline, r'\\1_\\2', key).lower()\n        temp_dict[underline_sub] = temp_dict.pop(key)\n    return temp_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef yaml_conf_as_dict(file_path, encoding=None):\n    if not pathlib.Path(file_path).is_file():\n        return False, {}, 'File not exist'\n    \n    try:\n        if sys.version > '3':\n            with open(file_path, 'r', encoding=encoding) as f:\n                d = OrderedDict(yaml.load(f.read()))\n                return True, d, 'Success'\n        else:\n            with open(file_path, 'r') as f:\n                d = OrderedDict(yaml.load(f.read()))\n                return True, d, 'Success'\n    except:\n        return False, {}, 'Unknow error'", "response": "\u8bfb\u5165 yaml \u914d\u7f6e\u6587\u4ef6\uff0c\u8fd4\u56de\u6839\u636e\u914d\u7f6e\u6587\u4ef6\u5185\u5bb9\u751f\u6210\u7684\u5b57\u5178\u7c7b\u578b\u53d8\u91cf\n\n    :param:\n        * file_path: (string) \u9700\u8981\u8bfb\u5165\u7684 yaml \u914d\u7f6e\u6587\u4ef6\u957f\u6587\u4ef6\u540d\n        * encoding: (string) \u6587\u4ef6\u7f16\u7801\n        * msg: (string) \u8bfb\u53d6\u914d\u7f6e\u4fe1\u606f\n\n    :return:\n        * flag: (bool) \u8bfb\u53d6\u914d\u7f6e\u6587\u4ef6\u662f\u5426\u6b63\u786e\uff0c\u6b63\u786e\u8fd4\u56de True\uff0c\u9519\u8bef\u8fd4\u56de False\n        * d: (dict) \u5982\u679c\u8bfb\u53d6\u914d\u7f6e\u6587\u4ef6\u6b63\u786e\u8fd4\u56de\u7684\u5305\u542b\u914d\u7f6e\u6587\u4ef6\u5185\u5bb9\u7684\u5b57\u5178\uff0c\u5b57\u5178\u5185\u5bb9\u987a\u5e8f\u4e0e\u914d\u7f6e\u6587\u4ef6\u987a\u5e8f\u4fdd\u6301\u4e00\u81f4\n\n    \u4e3e\u4f8b\u5982\u4e0b::\n\n        print('--- yaml_conf_as_dict demo---')\n        # \u5b9a\u4e49\u914d\u7f6e\u6587\u4ef6\u540d\n        conf_filename = 'test_conf.yaml'\n        # \u8bfb\u53d6\u914d\u7f6e\u6587\u4ef6\n        ds = yaml_conf_as_dict(conf_filename, encoding='utf-8')\n        # \u663e\u793a\u662f\u5426\u6210\u529f\uff0c\u6240\u6709 dict \u7684\u5185\u5bb9\uff0cdict \u7684 key \u6570\u91cf\n        print('flag:', ds[0])\n        print('dict length:', len(ds[1]))\n        print('msg:', len(ds[1]))\n        print('conf info: ', ds[1].get('tree'))\n        print('---')\n\n    \u6267\u884c\u7ed3\u679c::\n\n        --- yaml_conf_as_dict demo---\n        flag: True\n        dict length: 2\n        msg: Success\n        conf info:  ['README.md', 'requirements.txt', {'hellopackage': ['__init__.py']},\n        {'test': ['__init__.py']}, {'doc': ['doc.rst']}]\n        ---"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list2csv(data_list, csv_filename='./list2csv.csv'):\n\n    \"\"\"\n    \u5c06\u5b57\u5178\u5199\u5165\u5230\u6307\u5b9a\u7684 csv \u6587\u4ef6\uff0c\u5e76\u8fd4\u56de\u6587\u4ef6\u7684\u957f\u6587\u4ef6\u540d\uff1b\n\n    :param:\n        * data_list: (list) \u9700\u8981\u5199\u5165 csv \u7684\u6570\u636e\u5b57\u5178\n        * csv_filename: (string) csv \u6587\u4ef6\u7684\u957f\u6587\u4ef6\u540d\n    :return:\n        * csv_filename: (string) csv \u6587\u4ef6\u7684\u957f\u6587\u4ef6\u540d\n\n    \u4e3e\u4f8b\u5982\u4e0b::\n\n        from fishbase.fish_csv import *\n\n        def test_list2csv():\n            data_list = ['a', 'b', 'c']\n            csv_file = list2csv(data_list)\n            print(csv_file)\n\n\n        if __name__ == '__main__':\n            test_list2csv()\n\n    \"\"\"\n    with open(csv_filename, \"w\") as csv_file:\n        csv_writer = csv.writer(csv_file)\n        for data in data_list:\n            csv_writer.writerow(data)\n    return csv_filename", "response": "\u5c06\u5b57\u5178\u5199\u5165\u5230\u6307\u5b9a\u7684 csv \u6587\u4ef6\uff0c\u5e76\u8fd4\u56de\u6587\u4ef6\u7684\u957f\u6587\u4ef6\u540d\uff1b\n\n    :param:\n        * data_list: (list) \u9700\u8981\u5199\u5165 csv \u7684\u6570\u636e\u5b57\u5178\n        * csv_filename: (string) csv \u6587\u4ef6\u7684\u957f\u6587\u4ef6\u540d\n    :return:\n        * csv_filename: (string) csv \u6587\u4ef6\u7684\u957f\u6587\u4ef6\u540d\n\n    \u4e3e\u4f8b\u5982\u4e0b::\n\n        from fishbase.fish_csv import *\n\n        def test_list2csv():\n            data_list = ['a', 'b', 'c']\n            csv_file = list2csv(data_list)\n            print(csv_file)\n\n\n        if __name__ == '__main__':\n            test_list2csv()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef csv2dict(csv_filename, deli=',', encoding=None, key_is_header=False):\n\n    \"\"\"\n    \u5c06\u6307\u5b9a\u7684 csv \u6587\u4ef6\u8f6c\u6362\u4e3a list \u8fd4\u56de\uff1b\n\n    :param:\n        * csv_filename: (string) csv \u6587\u4ef6\u7684\u957f\u6587\u4ef6\u540d\n        * deli: (string) csv \u6587\u4ef6\u5206\u9694\u7b26\uff0c\u9ed8\u8ba4\u4e3a\u9017\u53f7\n        * del_blank_row: (string) \u662f\u5426\u8981\u5220\u9664\u7a7a\u884c\uff0c\u9ed8\u8ba4\u4e3a\u5220\u9664\n        * encode: (string) \u6587\u4ef6\u7f16\u7801\n    :return:\n        * csv_data: (dict) \u8bfb\u53d6\u540e\u7684\u6570\u636e\n\n    \u4e3e\u4f8b\u5982\u4e0b::\n\n        from fishbase.fish_file import *\n        from fishbase.fish_csv import *\n\n        def test_csv2dict():\n            csv_filename = get_abs_filename_with_sub_path('csv', 'test_csv.csv')[1]\n            print(csv_filename)\n            csv_dict = csv2dict(csv_filename)\n            print(csv_dict)\n\n\n        if __name__ == '__main__':\n            test_csv2dict()\n\n    \"\"\"\n    with open(csv_filename, encoding=encoding) as csv_file:\n        if key_is_header:\n            reader = csv.reader(csv_file, delimiter=deli)\n            # \u8bfb\u53d6\u5b57\u5178 key\n            fieldnames = next(reader)\n            reader = csv.DictReader(csv_file, fieldnames=fieldnames, delimiter=deli)\n            return [dict(row) for row in reader]\n        reader = csv.reader(csv_file, delimiter=deli)\n        return {row[0]: row[1] for row in reader if row}", "response": "\u5c06\u6307\u5b9a\u7684 csv \u6587\u4ef6\u8f6c\u6362\u4e3a list \u8fd4\u56de\uff1b\n\n    :param:\n        * csv_filename: (string) csv \u6587\u4ef6\u7684\u957f\u6587\u4ef6\u540d\n        * deli: (string) csv \u6587\u4ef6\u5206\u9694\u7b26\uff0c\u9ed8\u8ba4\u4e3a\u9017\u53f7\n        * del_blank_row: (string) \u662f\u5426\u8981\u5220\u9664\u7a7a\u884c\uff0c\u9ed8\u8ba4\u4e3a\u5220\u9664\n        * encode: (string) \u6587\u4ef6\u7f16\u7801\n    :return:\n        * csv_data: (dict) \u8bfb\u53d6\u540e\u7684\u6570\u636e\n\n    \u4e3e\u4f8b\u5982\u4e0b::\n\n        from fishbase.fish_file import *\n        from fishbase.fish_csv import *\n\n        def test_csv2dict():\n            csv_filename = get_abs_filename_with_sub_path('csv', 'test_csv.csv')[1]\n            print(csv_filename)\n            csv_dict = csv2dict(csv_filename)\n            print(csv_dict)\n\n\n        if __name__ == '__main__':\n            test_csv2dict()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dict2csv(data_dict, csv_filename='./dict2csv.csv', key_is_header=False):\n\n    \"\"\"\n    \u5c06\u5b57\u5178\u5199\u5165\u5230\u6307\u5b9a\u7684 csv \u6587\u4ef6\uff0c\u5e76\u8fd4\u56de\u6587\u4ef6\u7684\u957f\u6587\u4ef6\u540d\uff1b\n\n    :param:\n        * data_dict: (dict) \u9700\u8981\u5199\u5165 csv \u7684\u6570\u636e\u5b57\u5178\n        * csv_filename: (string) csv \u6587\u4ef6\u7684\u957f\u6587\u4ef6\u540d\n        * key_is_header: (bool) csv \u6587\u4ef6\u7b2c\u4e00\u884c\u662f\u5426\u5168\u4e3a\u5b57\u5178 key\n    :return:\n        * csv_filename: (string) csv \u6587\u4ef6\u7684\u957f\u6587\u4ef6\u540d\n\n    \u4e3e\u4f8b\u5982\u4e0b::\n\n        from fishbase.fish_csv import *\n\n        def test_dict2csv():\n            data_dict = {'a': 1, 'b': 2}\n            csv_file = dict2csv(data_dict)\n            print(csv_file)\n\n\n        if __name__ == '__main__':\n            test_dict2csv()\n\n    \"\"\"\n    with open(csv_filename, \"w\") as csv_file:\n        csv_writer = csv.writer(csv_file)\n        if key_is_header:\n            if isinstance(data_dict, dict):\n                csv_writer.writerow(list(data_dict.keys()))\n                csv_writer.writerow(list(data_dict.values()))\n            elif isinstance(data_dict, list) and all([isinstance(item, dict) for\n                                                      item in data_dict]):\n                for item in data_dict:\n                    csv_writer.writerow(list(item.keys()))\n                    csv_writer.writerow(list(item.values()))\n            else:\n                raise ValueError('data_dict should be a dict or list which member is dict, '\n                                 'but we got {}'.format(data_dict))\n        else:\n            csv_writer.writerows(data_dict.items())\n    return csv_filename", "response": "CSV \u6587\u4ef6\u540d \u5199\u5165 dict2csv."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds next proceedings and clones them for cycling if it exists.", "response": "def _cycle_proceedings(self):\n        \"\"\"\n         Finds next proceedings and clone them for cycling if it exists.\n        \"\"\"\n        next_approvals = self._get_next_approvals().exclude(\n            status=PENDING).exclude(cloned=True)\n        for ta in next_approvals:\n            clone_transition_approval, c = TransitionApproval.objects.get_or_create(\n                source_state=ta.source_state,\n                destination_state=ta.destination_state,\n                content_type=ta.content_type,\n                object_id=ta.object_id,\n                field_name=ta.field_name,\n                skip=ta.skip,\n                priority=ta.priority,\n                enabled=ta.enabled,\n                status=PENDING,\n                meta=ta.meta\n            )\n\n            if c:\n                clone_transition_approval.permissions.add(*ta.permissions.all())\n                clone_transition_approval.groups.add(*ta.groups.all())\n            next_approvals.update(cloned=True)\n\n        return True if next_approvals.count() else False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_upload_params(request):\n    file_name = request.POST['name']\n    file_type = request.POST['type']\n    file_size = int(request.POST['size'])\n\n    dest = get_s3direct_destinations().get(\n        request.POST.get('dest', None), None)\n    if not dest:\n        resp = json.dumps({'error': 'File destination does not exist.'})\n        return HttpResponseNotFound(resp, content_type='application/json')\n\n    auth = dest.get('auth')\n    if auth and not auth(request.user):\n        resp = json.dumps({'error': 'Permission denied.'})\n        return HttpResponseForbidden(resp, content_type='application/json')\n\n    allowed = dest.get('allowed')\n    if (allowed and file_type not in allowed) and allowed != '*':\n        resp = json.dumps({'error': 'Invalid file type (%s).' % file_type})\n        return HttpResponseBadRequest(resp, content_type='application/json')\n\n    cl_range = dest.get('content_length_range')\n    if (cl_range and not cl_range[0] <= file_size <= cl_range[1]):\n        msg = 'Invalid file size (must be between %s and %s bytes).'\n        resp = json.dumps({'error': (msg % cl_range)})\n        return HttpResponseBadRequest(resp, content_type='application/json')\n\n    key = dest.get('key')\n    if not key:\n        resp = json.dumps({'error': 'Missing destination path.'})\n        return HttpResponseServerError(resp, content_type='application/json')\n\n    bucket = dest.get('bucket',\n                      getattr(settings, 'AWS_STORAGE_BUCKET_NAME', None))\n    if not bucket:\n        resp = json.dumps({'error': 'S3 bucket config missing.'})\n        return HttpResponseServerError(resp, content_type='application/json')\n\n    region = dest.get('region', getattr(settings, 'AWS_S3_REGION_NAME', None))\n    if not region:\n        resp = json.dumps({'error': 'S3 region config missing.'})\n        return HttpResponseServerError(resp, content_type='application/json')\n\n    endpoint = dest.get('endpoint',\n                        getattr(settings, 'AWS_S3_ENDPOINT_URL', None))\n    if not endpoint:\n        resp = json.dumps({'error': 'S3 endpoint config missing.'})\n        return HttpResponseServerError(resp, content_type='application/json')\n\n    aws_credentials = get_aws_credentials()\n    if not aws_credentials.secret_key or not aws_credentials.access_key:\n        resp = json.dumps({'error': 'AWS credentials config missing.'})\n        return HttpResponseServerError(resp, content_type='application/json')\n\n    upload_data = {\n        'object_key': get_key(key, file_name, dest),\n        'access_key_id': aws_credentials.access_key,\n        'session_token': aws_credentials.token,\n        'region': region,\n        'bucket': bucket,\n        'endpoint': endpoint,\n        'acl': dest.get('acl') or 'public-read',\n        'allow_existence_optimization': dest.get('allow_existence_optimization', False)\n    }\n\n    optional_params = [\n        'content_disposition', 'cache_control', 'server_side_encryption'\n    ]\n\n    for optional_param in optional_params:\n        if optional_param in dest:\n            option = dest.get(optional_param)\n            if hasattr(option, '__call__'):\n                upload_data[optional_param] = option(file_name)\n            else:\n                upload_data[optional_param] = option\n\n    resp = json.dumps(upload_data)\n    return HttpResponse(resp, content_type='application/json')", "response": "Authorises user and validates given file properties."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprocesses the subscription data according to out model", "response": "def process_subscription_data(post_data):\n    \"\"\"Process the subscription data according to out model\"\"\"\n    subscription_data = post_data.pop(\"subscription\", {})\n    # As our database saves the auth and p256dh key in separate field,\n    # we need to refactor it and insert the auth and p256dh keys in the same dictionary\n    keys = subscription_data.pop(\"keys\", {})\n    subscription_data.update(keys)\n    # Insert the browser name\n    subscription_data[\"browser\"] = post_data.pop(\"browser\")\n    return subscription_data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the serializer instance that should be used for validating and deserializing input and serializing output.", "response": "def get_serializer_in(self, *args, **kwargs):\n        \"\"\"\n        Return the serializer instance that should be used for validating and\n        deserializing input, and for serializing output.\n        \"\"\"\n        serializer_class = self.get_serializer_class_in()\n        kwargs['context'] = self.get_serializer_context()\n        return serializer_class(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_avatar(strategy, details, user=None, *args, **kwargs):\n    if user:\n        backend_name = kwargs['backend'].__class__.__name__.lower()\n        response = kwargs.get('response', {})\n        social_thumb = None\n        if 'facebook' in backend_name:\n            if 'id' in response:\n                social_thumb = (\n                    'http://graph.facebook.com/{0}/picture?type=normal'\n                ).format(response['id'])\n        elif 'twitter' in backend_name and response.get('profile_image_url'):\n            social_thumb = response['profile_image_url']\n        elif 'googleoauth2' in backend_name and response.get('image', {}).get('url'):\n            social_thumb = response['image']['url'].split('?')[0]\n        else:\n            social_thumb = 'http://www.gravatar.com/avatar/'\n            social_thumb += hashlib.md5(user.email.lower().encode('utf8')).hexdigest()\n            social_thumb += '?size=100'\n        if social_thumb and user.social_thumb != social_thumb:\n            user.social_thumb = social_thumb\n            strategy.storage.user.changed(user)", "response": "Get user avatar from social provider."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef default_image_loader(filename, flags, **kwargs):\n\n    def load(rect=None, flags=None):\n        return filename, rect, flags\n\n    return load", "response": "This is a default image loader that returns filename rect and any flags"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a few common variations of true and false to boolean", "response": "def convert_to_bool(text):\n    \"\"\" Convert a few common variations of \"true\" and \"false\" to boolean\n\n    :param text: string to test\n    :return: boolean\n    :raises: ValueError\n    \"\"\"\n    # handle \"1\" and \"0\"\n    try:\n        return bool(int(text))\n    except:\n        pass\n\n    text = str(text).lower()\n    if text == \"true\":\n        return True\n    if text == \"yes\":\n        return True\n    if text == \"false\":\n        return False\n    if text == \"no\":\n        return False\n\n    raise ValueError"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_properties(node):\n    d = dict()\n    for child in node.findall('properties'):\n        for subnode in child.findall('property'):\n            cls = None\n            try:\n                if \"type\" in subnode.keys():\n                    module = importlib.import_module('builtins')\n                    cls = getattr(module, subnode.get(\"type\"))\n            except AttributeError:\n                logger.info(\"Type [} Not a built-in type. Defaulting to string-cast.\")\n            d[subnode.get('name')] = cls(subnode.get('value')) if cls is not None else subnode.get('value')\n    return d", "response": "Parse a Tiled xml node and return a dict that represents a tiled property"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _set_properties(self, node):\n        self._cast_and_set_attributes_from_node_items(node.items())\n        properties = parse_properties(node)\n        if (not self.allow_duplicate_names and\n                self._contains_invalid_property_name(properties.items())):\n            self._log_property_error_message()\n            raise ValueError(\"Reserved names and duplicate names are not allowed. Please rename your property inside the .tmx-file\")\n\n        self.properties = properties", "response": "Create dictionary containing Tiled object attributes from xml node and fill properties with values from the properties attribute."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a map from ElementTree xml node node.", "response": "def parse_xml(self, node):\n        \"\"\" Parse a map from ElementTree xml node\n\n        :param node: ElementTree xml node\n        :return: self\n        \"\"\"\n        self._set_properties(node)\n        self.background_color = node.get('backgroundcolor',\n                                         self.background_color)\n\n        # ***         do not change this load order!         *** #\n        # ***    gid mapping errors will occur if changed    *** #\n        for subnode in node.findall('layer'):\n            self.add_layer(TiledTileLayer(self, subnode))\n\n        for subnode in node.findall('imagelayer'):\n            self.add_layer(TiledImageLayer(self, subnode))\n\n        for subnode in node.findall('objectgroup'):\n            self.add_layer(TiledObjectGroup(self, subnode))\n\n        for subnode in node.findall('tileset'):\n            self.add_tileset(TiledTileset(self, subnode))\n\n        # \"tile objects\", objects with a GID, have need to have their attributes\n        # set after the tileset is loaded, so this step must be performed last\n        # also, this step is performed for objects to load their tiles.\n\n        # tiled stores the origin of GID objects by the lower right corner\n        # this is different for all other types, so i just adjust it here\n        # so all types loaded with pytmx are uniform.\n\n        # iterate through tile objects and handle the image\n        for o in [o for o in self.objects if o.gid]:\n\n            # gids might also have properties assigned to them\n            # in that case, assign the gid properties to the object as well\n            p = self.get_tile_properties_by_gid(o.gid)\n            if p:\n                for key in p:\n                    o.properties.setdefault(key, p[key])\n\n            if self.invert_y:\n                o.y -= o.height\n\n        self.reload_images()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reload_images(self):\n        self.images = [None] * self.maxgid\n\n        # iterate through tilesets to get source images\n        for ts in self.tilesets:\n\n            # skip tilesets without a source\n            if ts.source is None:\n                continue\n\n            path = os.path.join(os.path.dirname(self.filename), ts.source)\n            colorkey = getattr(ts, 'trans', None)\n            loader = self.image_loader(path, colorkey, tileset=ts)\n\n            p = product(range(ts.margin,\n                              ts.height + ts.margin - ts.tileheight + 1,\n                              ts.tileheight + ts.spacing),\n                        range(ts.margin,\n                              ts.width + ts.margin - ts.tilewidth + 1,\n                              ts.tilewidth + ts.spacing))\n\n            # iterate through the tiles\n            for real_gid, (y, x) in enumerate(p, ts.firstgid):\n                rect = (x, y, ts.tilewidth, ts.tileheight)\n                gids = self.map_gid(real_gid)\n\n                # gids is None if the tile is never used\n                # but give another chance to load the gid anyway\n                if gids is None:\n                    if self.load_all_tiles or real_gid in self.optional_gids:\n                        # TODO: handle flags? - might never be an issue, though\n                        gids = [self.register_gid(real_gid, flags=0)]\n\n                if gids:\n                    # flags might rotate/flip the image, so let the loader\n                    # handle that here\n                    for gid, flags in gids:\n                        self.images[gid] = loader(rect, flags)\n\n        # load image layer images\n        for layer in (i for i in self.layers if isinstance(i, TiledImageLayer)):\n            source = getattr(layer, 'source', None)\n            if source:\n                colorkey = getattr(layer, 'trans', None)\n                real_gid = len(self.images)\n                gid = self.register_gid(real_gid)\n                layer.gid = gid\n                path = os.path.join(os.path.dirname(self.filename), source)\n                loader = self.image_loader(path, colorkey)\n                image = loader()\n                self.images.append(image)\n\n        # load images in tiles.\n        # instead of making a new gid, replace the reference to the tile that\n        # was loaded from the tileset\n        for real_gid, props in self.tile_properties.items():\n            source = props.get('source', None)\n            if source:\n                colorkey = props.get('trans', None)\n                path = os.path.join(os.path.dirname(self.filename), source)\n                loader = self.image_loader(path, colorkey)\n                image = loader()\n                self.images[real_gid] = image", "response": "Load the map images from disk"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_tile_image(self, x, y, layer):\n        try:\n            assert (x >= 0 and y >= 0)\n        except AssertionError:\n            raise ValueError\n\n        try:\n            layer = self.layers[layer]\n        except IndexError:\n            raise ValueError\n\n        assert (isinstance(layer, TiledTileLayer))\n\n        try:\n            gid = layer.data[y][x]\n        except (IndexError, ValueError):\n            raise ValueError\n        except TypeError:\n            msg = \"Tiles must be specified in integers.\"\n            logger.debug(msg)\n            raise TypeError\n\n        else:\n            return self.get_tile_image_by_gid(gid)", "response": "Return the tile image for this location."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_tile_image_by_gid(self, gid):\n        try:\n            assert (int(gid) >= 0)\n            return self.images[gid]\n        except TypeError:\n            msg = \"GIDs must be expressed as a number.  Got: {0}\"\n            logger.debug(msg.format(gid))\n            raise TypeError\n        except (AssertionError, IndexError):\n            msg = \"Coords: ({0},{1}) in layer {2} has invalid GID: {3}\"\n            logger.debug(msg.format(gid))\n            raise ValueError", "response": "Returns the tile image for this location by its GID."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the tile image GID for this location", "response": "def get_tile_gid(self, x, y, layer):\n        \"\"\" Return the tile image GID for this location\n\n        :param x: x coordinate\n        :param y: y coordinate\n        :param layer: layer number\n        :rtype: surface if found, otherwise ValueError\n        \"\"\"\n        try:\n            assert (x >= 0 and y >= 0 and layer >= 0)\n        except AssertionError:\n            raise ValueError\n\n        try:\n            return self.layers[int(layer)].data[int(y)][int(x)]\n        except (IndexError, ValueError):\n            msg = \"Coords: ({0},{1}) in layer {2} is invalid\"\n            logger.debug(msg, (x, y, layer))\n            raise ValueError"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_tile_properties(self, x, y, layer):\n        try:\n            assert (x >= 0 and y >= 0 and layer >= 0)\n        except AssertionError:\n            raise ValueError\n\n        try:\n            gid = self.layers[int(layer)].data[int(y)][int(x)]\n        except (IndexError, ValueError):\n            msg = \"Coords: ({0},{1}) in layer {2} is invalid.\"\n            logger.debug(msg.format(x, y, layer))\n            raise Exception\n\n        else:\n            try:\n                return self.tile_properties[gid]\n            except (IndexError, ValueError):\n                msg = \"Coords: ({0},{1}) in layer {2} has invalid GID: {3}\"\n                logger.debug(msg.format(x, y, layer, gid))\n                raise Exception\n            except KeyError:\n                return None", "response": "Return the tile properties for this location"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsearches map for tile locations by the GID.", "response": "def get_tile_locations_by_gid(self, gid):\n        \"\"\" Search map for tile locations by the GID\n\n        Return (int, int, int) tuples, where the layer is index of\n        the visible tile layers.\n\n        Note: Not a fast operation.  Cache results if used often.\n\n        :param gid: GID to be searched for\n        :rtype: generator of tile locations\n        \"\"\"\n        for l in self.visible_tile_layers:\n            for x, y, _gid in [i for i in self.layers[l].iter_data() if i[2] == gid]:\n                yield x, y, l"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the tile properties of each GID in layer.", "response": "def get_tile_properties_by_layer(self, layer):\n        \"\"\" Get the tile properties of each GID in layer\n\n        :param layer: layer number\n        :rtype: iterator of (gid, properties) tuples\n        \"\"\"\n        try:\n            assert (int(layer) >= 0)\n            layer = int(layer)\n        except (TypeError, AssertionError):\n            msg = \"Layer must be a positive integer.  Got {0} instead.\"\n            logger.debug(msg.format(type(layer)))\n            raise ValueError\n\n        p = product(range(self.width), range(self.height))\n        layergids = set(self.layers[layer].data[y][x] for x, y in p)\n\n        for gid in layergids:\n            try:\n                yield gid, self.tile_properties[gid]\n            except KeyError:\n                continue"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_layer(self, layer):\n        assert (\n            isinstance(layer,\n                       (TiledTileLayer, TiledImageLayer, TiledObjectGroup)))\n\n        self.layers.append(layer)\n        self.layernames[layer.name] = layer", "response": "Add a layer to the set of layers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_tileset(self, tileset):\n        assert (isinstance(tileset, TiledTileset))\n        self.tilesets.append(tileset)", "response": "Adds a tileset to the map\n."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_layer_by_name(self, name):\n        try:\n            return self.layernames[name]\n        except KeyError:\n            msg = 'Layer \"{0}\" not found.'\n            logger.debug(msg.format(name))\n            raise ValueError", "response": "Return a layer object by name."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds an object by name.", "response": "def get_object_by_name(self, name):\n        \"\"\"Find an object\n\n        :param name: Name of object.  Case-sensitive.\n        :rtype: Object if found, otherwise ValueError\n        \"\"\"\n        for obj in self.objects:\n            if obj.name == name:\n                return obj\n        raise ValueError"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_tileset_from_gid(self, gid):\n        try:\n            tiled_gid = self.tiledgidmap[gid]\n        except KeyError:\n            raise ValueError\n\n        for tileset in sorted(self.tilesets, key=attrgetter('firstgid'),\n                              reverse=True):\n            if tiled_gid >= tileset.firstgid:\n                return tileset\n\n        raise ValueError", "response": "Return the TiledTileset that owns the gid"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn iterator of layer indexes that are set visible", "response": "def visible_tile_layers(self):\n        \"\"\"Return iterator of layer indexes that are set 'visible'\n\n        :rtype: Iterator\n        \"\"\"\n        return (i for (i, l) in enumerate(self.layers)\n                if l.visible and isinstance(l, TiledTileLayer))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef visible_object_groups(self):\n        return (i for (i, l) in enumerate(self.layers)\n                if l.visible and isinstance(l, TiledObjectGroup))", "response": "Return iterator of object group indexes that are set visible"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nuse to manage the mapping of GIDs between the tmx and pytmx", "response": "def register_gid(self, tiled_gid, flags=None):\n        \"\"\" Used to manage the mapping of GIDs between the tmx and pytmx\n\n        :param tiled_gid: GID that is found in TMX data\n        :rtype: GID that pytmx uses for the the GID passed\n        \"\"\"\n        if flags is None:\n            flags = TileFlags(0, 0, 0)\n\n        if tiled_gid:\n            try:\n                return self.imagemap[(tiled_gid, flags)][0]\n            except KeyError:\n                gid = self.maxgid\n                self.maxgid += 1\n                self.imagemap[(tiled_gid, flags)] = (gid, flags)\n                self.gidmap[tiled_gid].append((gid, flags))\n                self.tiledgidmap[gid] = tiled_gid\n                return gid\n\n        else:\n            return 0"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing to map a GID to a TMX file s data .", "response": "def map_gid(self, tiled_gid):\n        \"\"\" Used to lookup a GID read from a TMX file's data\n\n        :param tiled_gid: GID that is found in TMX data\n        :rtype: (GID, flags) for the the GID passed, None if not found\n        \"\"\"\n        try:\n            return self.gidmap[int(tiled_gid)]\n        except KeyError:\n            return None\n        except TypeError:\n            msg = \"GIDs must be an integer\"\n            logger.debug(msg)\n            raise TypeError"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef map_gid2(self, tiled_gid):\n        tiled_gid = int(tiled_gid)\n\n        # gidmap is a default dict, so cannot trust to raise KeyError\n        if tiled_gid in self.gidmap:\n            return self.gidmap[tiled_gid]\n        else:\n            gid = self.register_gid(tiled_gid)\n            return [(gid, None)]", "response": "WIP. need to refactor the gid code\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_xml(self, node):\n        import os\n\n        # if true, then node references an external tileset\n        source = node.get('source', None)\n        if source:\n            if source[-4:].lower() == \".tsx\":\n\n                # external tilesets don't save this, store it for later\n                self.firstgid = int(node.get('firstgid'))\n\n                # we need to mangle the path - tiled stores relative paths\n                dirname = os.path.dirname(self.parent.filename)\n                path = os.path.abspath(os.path.join(dirname, source))\n                try:\n                    node = ElementTree.parse(path).getroot()\n                except IOError:\n                    msg = \"Cannot load external tileset: {0}\"\n                    logger.error(msg.format(path))\n                    raise Exception\n\n            else:\n                msg = \"Found external tileset, but cannot handle type: {0}\"\n                logger.error(msg.format(self.source))\n                raise Exception\n\n        self._set_properties(node)\n\n        # since tile objects [probably] don't have a lot of metadata,\n        # we store it separately in the parent (a TiledMap instance)\n        register_gid = self.parent.register_gid\n        for child in node.getiterator('tile'):\n            tiled_gid = int(child.get(\"id\"))\n\n            p = {k: types[k](v) for k, v in child.items()}\n            p.update(parse_properties(child))\n            \n            # images are listed as relative to the .tsx file, not the .tmx file:\n            if source and \"path\" in p:\n                p[\"path\"] = os.path.join(os.path.dirname(source), p[\"path\"])\n\n            # handle tiles that have their own image\n            image = child.find('image')\n            if image is None:\n                p['width'] = self.tilewidth\n                p['height'] = self.tileheight\n            else:\n                tile_source = image.get('source')\n                # images are listed as relative to the .tsx file, not the .tmx file:\n                if tile_source: \n                    tile_source = os.path.join(os.path.dirname(source), tile_source)\n                p['source'] = tile_source\n                p['trans'] = image.get('trans', None)\n                p['width'] = image.get('width')\n                p['height'] = image.get('height')\n\n            # handle tiles with animations\n            anim = child.find('animation')\n            frames = list()\n            p['frames'] = frames\n            if anim is not None:\n                for frame in anim.findall(\"frame\"):\n                    duration = int(frame.get('duration'))\n                    gid = register_gid(int(frame.get('tileid')) + self.firstgid)\n                    frames.append(AnimationFrame(gid, duration))\n\n            for gid, flags in self.parent.map_gid2(tiled_gid + self.firstgid):\n                self.parent.set_tile_properties(gid, p)\n\n        # handle the optional 'tileoffset' node\n        self.offset = node.find('tileoffset')\n        if self.offset is None:\n            self.offset = (0, 0)\n        else:\n            self.offset = (self.offset.get('x', 0), self.offset.get('y', 0))\n\n        image_node = node.find('image')\n        if image_node is not None:\n            self.source = image_node.get('source')\n            \n            # When loading from tsx, tileset image path is relative to the tsx file, not the tmx:\n            if source:\n                self.source = os.path.join(os.path.dirname(source), self.source)\n            \n            self.trans = image_node.get('trans', None)\n            self.width = int(image_node.get('width'))\n            self.height = int(image_node.get('height'))\n\n        return self", "response": "Parse a Tileset from ElementTree xml element node."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef iter_data(self):\n        for y, row in enumerate(self.data):\n            for x, gid in enumerate(row):\n                yield x, y, gid", "response": "Iterate over the data in the layer and yield X Y GID tuples for each tile in the layer"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tiles(self):\n        images = self.parent.images\n        for x, y, gid in [i for i in self.iter_data() if i[2]]:\n            yield x, y, images[gid]", "response": "Iterate over tile images of this layer and return them as tuples."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_xml(self, node):\n        import struct\n        import array\n\n        self._set_properties(node)\n        data = None\n        next_gid = None\n        data_node = node.find('data')\n\n        encoding = data_node.get('encoding', None)\n        if encoding == 'base64':\n            from base64 import b64decode\n\n            data = b64decode(data_node.text.strip())\n\n        elif encoding == 'csv':\n            next_gid = map(int, \"\".join(\n                line.strip() for line in data_node.text.strip()).split(\",\"))\n\n        elif encoding:\n            msg = 'TMX encoding type: {0} is not supported.'\n            logger.error(msg.format(encoding))\n            raise Exception\n\n        compression = data_node.get('compression', None)\n        if compression == 'gzip':\n            import gzip\n\n            with gzip.GzipFile(fileobj=six.BytesIO(data)) as fh:\n                data = fh.read()\n\n        elif compression == 'zlib':\n            import zlib\n\n            data = zlib.decompress(data)\n\n        elif compression:\n            msg = 'TMX compression type: {0} is not supported.'\n            logger.error(msg.format(compression))\n            raise Exception\n\n        # if data is None, then it was not decoded or decompressed, so\n        # we assume here that it is going to be a bunch of tile elements\n        # TODO: this will/should raise an exception if there are no tiles\n        if encoding == next_gid is None:\n            def get_children(parent):\n                for child in parent.findall('tile'):\n                    yield int(child.get('gid'))\n\n            next_gid = get_children(data_node)\n\n        elif data:\n            if type(data) == bytes:\n                fmt = struct.Struct('<L')\n                iterator = (data[i:i + 4] for i in range(0, len(data), 4))\n                next_gid = (fmt.unpack(i)[0] for i in iterator)\n            else:\n                msg = 'layer data not in expected format ({})'\n                logger.error(msg.format(type(data)))\n                raise Exception\n\n        init = lambda: [0] * self.width\n        reg = self.parent.register_gid\n\n        # H (16-bit) may be a limitation for very detailed maps\n        self.data = tuple(array.array('H', init()) for i in range(self.height))\n        for (y, x) in product(range(self.height), range(self.width)):\n            self.data[y][x] = reg(*decode_gid(next(next_gid)))\n\n        return self", "response": "Parse a Tile Layer from ElementTree xml node node."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_xml(self, node):\n        self._set_properties(node)\n        self.extend(TiledObject(self.parent, child)\n                    for child in node.findall('object'))\n\n        return self", "response": "Parse an Object Group from ElementTree xml node node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse an object from ElementTree xml node node.", "response": "def parse_xml(self, node):\n        \"\"\" Parse an Object from ElementTree xml node\n\n        :param node: ElementTree xml node\n        :return: self\n        \"\"\"\n\n        def read_points(text):\n            \"\"\"parse a text string of float tuples and return [(x,...),...]\n            \"\"\"\n            return tuple(tuple(map(float, i.split(','))) for i in text.split())\n\n        self._set_properties(node)\n\n        # correctly handle \"tile objects\" (object with gid set)\n        if self.gid:\n            self.gid = self.parent.register_gid(self.gid)\n\n        points = None\n        polygon = node.find('polygon')\n        if polygon is not None:\n            points = read_points(polygon.get('points'))\n            self.closed = True\n\n        polyline = node.find('polyline')\n        if polyline is not None:\n            points = read_points(polyline.get('points'))\n            self.closed = False\n\n        if points:\n            x1 = x2 = y1 = y2 = 0\n            for x, y in points:\n                if x < x1: x1 = x\n                if x > x2: x2 = x\n                if y < y1: y1 = y\n                if y > y2: y2 = y\n            self.width = abs(x1) + abs(x2)\n            self.height = abs(y1) + abs(y2)\n            self.points = tuple(\n                [(i[0] + self.x, i[1] + self.y) for i in points])\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse an Image Layer from ElementTree xml node node.", "response": "def parse_xml(self, node):\n        \"\"\" Parse an Image Layer from ElementTree xml node\n\n        :param node: ElementTree xml node\n        :return: self\n        \"\"\"\n        self._set_properties(node)\n        self.name = node.get('name', None)\n        self.opacity = node.get('opacity', self.opacity)\n        self.visible = node.get('visible', self.visible)\n        image_node = node.find('image')\n        self.source = image_node.get('source', None)\n        self.trans = image_node.get('trans', None)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef smart_convert(original, colorkey, pixelalpha):\n    tile_size = original.get_size()\n    threshold = 127  # the default\n\n    try:\n        # count the number of pixels in the tile that are not transparent\n        px = pygame.mask.from_surface(original, threshold).count()\n    except:\n        # pygame_sdl2 will fail because the mask module is not included\n        # in this case, just convert_alpha and return it\n        return original.convert_alpha()\n\n    # there are no transparent pixels in the image\n    if px == tile_size[0] * tile_size[1]:\n        tile = original.convert()\n\n    # there are transparent pixels, and tiled set a colorkey\n    elif colorkey:\n        tile = original.convert()\n        tile.set_colorkey(colorkey, pygame.RLEACCEL)\n\n    # there are transparent pixels, and set for perpixel alpha\n    elif pixelalpha:\n        tile = original.convert_alpha()\n\n    # there are transparent pixels, and we won't handle them\n    else:\n        tile = original.convert()\n\n    return tile", "response": "This method does a smart conversion of the image to the correct format."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload a TMX file images and return a TiledMap object.", "response": "def load_pygame(filename, *args, **kwargs):\n    \"\"\" Load a TMX file, images, and return a TiledMap class\n\n    PYGAME USERS: Use me.\n\n    this utility has 'smart' tile loading.  by default any tile without\n    transparent pixels will be loaded for quick blitting.  if the tile has\n    transparent pixels, then it will be loaded with per-pixel alpha.  this is\n    a per-tile, per-image check.\n\n    if a color key is specified as an argument, or in the tmx data, the\n    per-pixel alpha will not be used at all. if the tileset's image has colorkey\n    transparency set in Tiled, the util_pygam will return images that have their\n    transparency already set.\n\n    TL;DR:\n    Don't attempt to convert() or convert_alpha() the individual tiles.  It is\n    already done for you.\n    \"\"\"\n    kwargs['image_loader'] = pygame_image_loader\n    return pytmx.TiledMap(filename, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a set of non - overlapping rects that represents the distribution of the specified gid.", "response": "def build_rects(tmxmap, layer, tileset=None, real_gid=None):\n    \"\"\"generate a set of non-overlapping rects that represents the distribution\n       of the specified gid.\n\n    useful for generating rects for use in collision detection\n\n    Use at your own risk: this is experimental...will change in future\n\n    GID Note: You will need to add 1 to the GID reported by Tiled.\n\n    :param tmxmap: TiledMap object\n    :param layer: int or string name of layer\n    :param tileset: int or string name of tileset\n    :param real_gid: Tiled GID of the tile + 1 (see note)\n    :return: List of pygame Rect objects\n    \"\"\"\n    if isinstance(tileset, int):\n        try:\n            tileset = tmxmap.tilesets[tileset]\n        except IndexError:\n            msg = \"Tileset #{0} not found in map {1}.\"\n            logger.debug(msg.format(tileset, tmxmap))\n            raise IndexError\n\n    elif isinstance(tileset, str):\n        try:\n            tileset = [t for t in tmxmap.tilesets if t.name == tileset].pop()\n        except IndexError:\n            msg = \"Tileset \\\"{0}\\\" not found in map {1}.\"\n            logger.debug(msg.format(tileset, tmxmap))\n            raise ValueError\n\n    elif tileset:\n        msg = \"Tileset must be either a int or string. got: {0}\"\n        logger.debug(msg.format(type(tileset)))\n        raise TypeError\n\n    gid = None\n    if real_gid:\n        try:\n            gid, flags = tmxmap.map_gid(real_gid)[0]\n        except IndexError:\n            msg = \"GID #{0} not found\"\n            logger.debug(msg.format(real_gid))\n            raise ValueError\n\n    if isinstance(layer, int):\n        layer_data = tmxmap.get_layer_data(layer)\n    elif isinstance(layer, str):\n        try:\n            layer = [l for l in tmxmap.layers if l.name == layer].pop()\n            layer_data = layer.data\n        except IndexError:\n            msg = \"Layer \\\"{0}\\\" not found in map {1}.\"\n            logger.debug(msg.format(layer, tmxmap))\n            raise ValueError\n\n    p = itertools.product(range(tmxmap.width), range(tmxmap.height))\n    if gid:\n        points = [(x, y) for (x, y) in p if layer_data[y][x] == gid]\n    else:\n        points = [(x, y) for (x, y) in p if layer_data[y][x]]\n\n    rects = simplify(points, tmxmap.tilewidth, tmxmap.tileheight)\n    return rects"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a list of points return a list of rects that represent them.", "response": "def simplify(all_points, tilewidth, tileheight):\n    \"\"\"Given a list of points, return list of rects that represent them\n    kludge:\n\n    \"A kludge (or kluge) is a workaround, a quick-and-dirty solution,\n    a clumsy or inelegant, yet effective, solution to a problem, typically\n    using parts that are cobbled together.\"\n\n    -- wikipedia\n\n    turn a list of points into a rects\n    adjacent rects will be combined.\n\n    plain english:\n        the input list must be a list of tuples that represent\n        the areas to be combined into rects\n        the rects will be blended together over solid groups\n\n        so if data is something like:\n\n        0 1 1 1 0 0 0\n        0 1 1 0 0 0 0\n        0 0 0 0 0 4 0\n        0 0 0 0 0 4 0\n        0 0 0 0 0 0 0\n        0 0 1 1 1 1 1\n\n        you'll have the 4 rects that mask the area like this:\n\n        ..######......\n        ..####........\n        ..........##..\n        ..........##..\n        ..............\n        ....##########\n\n        pretty cool, right?\n\n    there may be cases where the number of rectangles is not as low as possible,\n    but I haven't found that it is excessively bad.  certainly much better than\n    making a list of rects, one for each tile on the map!\n    \"\"\"\n\n    def pick_rect(points, rects):\n        ox, oy = sorted([(sum(p), p) for p in points])[0][1]\n        x = ox\n        y = oy\n        ex = None\n\n        while 1:\n            x += 1\n            if not (x, y) in points:\n                if ex is None:\n                    ex = x - 1\n\n                if (ox, y + 1) in points:\n                    if x == ex + 1:\n                        y += 1\n                        x = ox\n\n                    else:\n                        y -= 1\n                        break\n                else:\n                    if x <= ex: y -= 1\n                    break\n\n        c_rect = pygame.Rect(ox * tilewidth, oy * tileheight,\n                             (ex - ox + 1) * tilewidth,\n                             (y - oy + 1) * tileheight)\n\n        rects.append(c_rect)\n\n        rect = pygame.Rect(ox, oy, ex - ox + 1, y - oy + 1)\n        kill = [p for p in points if rect.collidepoint(p)]\n        [points.remove(i) for i in kill]\n\n        if points:\n            pick_rect(points, rects)\n\n    rect_list = []\n    while all_points:\n        pick_rect(all_points, rect_list)\n\n    return rect_list"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pyglet_image_loader(filename, colorkey, **kwargs):\n    if colorkey:\n        logger.debug('colorkey not implemented')\n\n    image = pyglet.image.load(filename)\n\n    def load_image(rect=None, flags=None):\n        if rect:\n            try:\n                x, y, w, h = rect\n                y = image.height - y - h\n                tile = image.get_region(x, y, w, h)\n            except:\n                logger.error('cannot get region %s of image', rect)\n                raise\n        else:\n            tile = image\n\n        if flags:\n            logger.error('tile flags are not implemented')\n\n        return tile\n\n    return load_image", "response": "basic image loading with pyglet"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reduce_wind_speed(wind_speed, wind_efficiency_curve_name='dena_mean'):\n    # Get wind efficiency curve\n    wind_efficiency_curve = get_wind_efficiency_curve(\n        curve_name=wind_efficiency_curve_name)\n    # Reduce wind speed by wind efficiency\n    reduced_wind_speed = wind_speed * np.interp(\n        wind_speed, wind_efficiency_curve['wind_speed'],\n        wind_efficiency_curve['efficiency'])\n    return reduced_wind_speed", "response": "r Reduce the wind speed by a wind efficiency curve."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_weather_data(filename='weather.csv', **kwargs):\n\n    if 'datapath' not in kwargs:\n        kwargs['datapath'] = os.path.join(os.path.split(\n            os.path.dirname(__file__))[0], 'example')\n    file = os.path.join(kwargs['datapath'], filename)\n    # read csv file\n    weather_df = pd.read_csv(\n        file, index_col=0, header=[0, 1],\n        date_parser=lambda idx: pd.to_datetime(idx, utc=True))\n    # change type of index to datetime and set time zone\n    weather_df.index = pd.to_datetime(weather_df.index).tz_convert(\n        'Europe/Berlin')\n    # change type of height from str to int by resetting columns\n    weather_df.columns = [weather_df.axes[1].levels[0][\n                              weather_df.axes[1].codes[0]],\n                          weather_df.axes[1].levels[1][\n                              weather_df.axes[1].codes[1]].astype(int)]\n    return weather_df", "response": "r Reads the weather data from a file and returns a DataFrame with the data as a MultiIndex."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef initialize_wind_turbines():\n\n    # specification of own wind turbine (Note: power values and nominal power\n    # have to be in Watt)\n    my_turbine = {\n        'name': 'myTurbine',\n        'nominal_power': 3e6,  # in W\n        'hub_height': 105,  # in m\n        'rotor_diameter': 90,  # in m\n        'power_curve': pd.DataFrame(\n            data={'value': [p * 1000 for p in [\n                      0.0, 26.0, 180.0, 1500.0, 3000.0, 3000.0]],  # in W\n                  'wind_speed': [0.0, 3.0, 5.0, 10.0, 15.0, 25.0]})  # in m/s\n    }\n    # initialize WindTurbine object\n    my_turbine = WindTurbine(**my_turbine)\n\n    # specification of wind turbine where power curve is provided in the oedb\n    # if you want to use the power coefficient curve change the value of\n    # 'fetch_curve' to 'power_coefficient_curve'\n    enercon_e126 = {\n        'name': 'E-126/4200',  # turbine type as in register #\n        'hub_height': 135,  # in m\n        'rotor_diameter': 127,  # in m\n        'fetch_curve': 'power_curve',  # fetch power curve #\n        'data_source': 'oedb'  # data source oedb or name of csv file\n    }\n    # initialize WindTurbine object\n    e126 = WindTurbine(**enercon_e126)\n\n    # specification of wind turbine where power coefficient curve is provided\n    # by a csv file\n    csv_file = os.path.join(os.path.dirname(__file__), 'data',\n                            'example_power_coefficient_curves.csv')\n    dummy_turbine = {\n        'name': 'DUMMY 1',  # turbine type as in file #\n        'hub_height': 100,  # in m\n        'rotor_diameter': 70,  # in m\n        'fetch_curve': 'power_coefficient_curve',  # fetch cp curve #\n        'data_source': csv_file  # data source\n    }\n    # initialize WindTurbine object\n    dummy_turbine = WindTurbine(**dummy_turbine)\n\n    return my_turbine, e126, dummy_turbine", "response": "r Initializes two wind turbine objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calculate_power_output(weather, my_turbine, e126, dummy_turbine):\n\n    # power output calculation for my_turbine\n    # initialize ModelChain with default parameters and use run_model method\n    # to calculate power output\n    mc_my_turbine = ModelChain(my_turbine).run_model(weather)\n    # write power output time series to WindTurbine object\n    my_turbine.power_output = mc_my_turbine.power_output\n\n    # power output calculation for e126\n    # own specifications for ModelChain setup\n    modelchain_data = {\n        'wind_speed_model': 'logarithmic',  # 'logarithmic' (default),\n                                            # 'hellman' or\n                                            # 'interpolation_extrapolation'\n        'density_model': 'ideal_gas',  # 'barometric' (default), 'ideal_gas' or\n                                       # 'interpolation_extrapolation'\n        'temperature_model': 'linear_gradient',  # 'linear_gradient' (def.) or\n                                                 # 'interpolation_extrapolation'\n        'power_output_model': 'power_curve',  # 'power_curve' (default) or\n                                              # 'power_coefficient_curve'\n        'density_correction': True,  # False (default) or True\n        'obstacle_height': 0,  # default: 0\n        'hellman_exp': None}  # None (default) or None\n    # initialize ModelChain with own specifications and use run_model method\n    # to calculate power output\n    mc_e126 = ModelChain(e126, **modelchain_data).run_model(weather)\n    # write power output time series to WindTurbine object\n    e126.power_output = mc_e126.power_output\n\n    # power output calculation for example_turbine\n    # own specification for 'power_output_model'\n    mc_example_turbine = ModelChain(\n        dummy_turbine,\n        power_output_model='power_coefficient_curve').run_model(weather)\n    dummy_turbine.power_output = mc_example_turbine.power_output\n\n    return", "response": "r Calculates the power output of a single object using the ModelChain."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef logarithmic_profile(wind_speed, wind_speed_height, hub_height,\n                        roughness_length, obstacle_height=0.0):\n    r\"\"\"\n    Calculates the wind speed at hub height using a logarithmic wind profile.\n\n    The logarithmic height equation is used. There is the possibility of\n    including the height of the surrounding obstacles in the calculation. This\n    function is carried out when the parameter `wind_speed_model` of an\n    instance of the :class:`~.modelchain.ModelChain` class is 'logarithmic'.\n\n    Parameters\n    ----------\n    wind_speed : pandas.Series or numpy.array\n        Wind speed time series.\n    wind_speed_height : float\n        Height for which the parameter `wind_speed` applies.\n    hub_height : float\n        Hub height of wind turbine.\n    roughness_length : pandas.Series or numpy.array or float\n        Roughness length.\n    obstacle_height : float\n        Height of obstacles in the surrounding area of the wind turbine. Set\n        `obstacle_height` to zero for wide spread obstacles. Default: 0.\n\n    Returns\n    -------\n    pandas.Series or numpy.array\n        Wind speed at hub height. Data type depends on type of `wind_speed`.\n\n    Notes\n    -----\n    The following equation is used [1]_, [2]_, [3]_:\n\n    .. math:: v_{wind,hub}=v_{wind,data}\\cdot\n        \\frac{\\ln\\left(\\frac{h_{hub}-d}{z_{0}}\\right)}{\\ln\\left(\n        \\frac{h_{data}-d}{z_{0}}\\right)}\n\n    with:\n        v: wind speed, h: height, :math:`z_{0}`: roughness length,\n        d: boundary layer offset (estimated by d = 0.7 * `obstacle_height`)\n\n    For  d = 0 it results in the following equation [2]_, [3]_:\n\n    .. math:: v_{wind,hub}=v_{wind,data}\\cdot\\frac{\\ln\\left(\\frac{h_{hub}}\n        {z_{0}}\\right)}{\\ln\\left(\\frac{h_{data}}{z_{0}}\\right)}\n\n    :math:`h_{data}` is the height at which the wind speed\n    :math:`v_{wind,data}` is measured and :math:`v_{wind,hub}` is the wind\n    speed at hub height :math:`h_{hub}` of the wind turbine.\n\n    Parameters `wind_speed_height`, `roughness_length`, `hub_height` and\n    `obstacle_height` have to be of the same unit.\n\n    References\n    ----------\n    .. [1] Quaschning V.: \"Regenerative Energiesysteme\". M\u00fcnchen, Hanser\n            Verlag, 2011, p. 278\n    .. [2] Gasch, R., Twele, J.: \"Windkraftanlagen\". 6. Auflage, Wiesbaden,\n            Vieweg + Teubner, 2010, p. 129\n    .. [3] Hau, E.: \"Windkraftanlagen - Grundlagen, Technik, Einsatz,\n            Wirtschaftlichkeit\". 4. Auflage, Springer-Verlag, 2008, p. 515\n\n    \"\"\"\n    if 0.7 * obstacle_height > wind_speed_height:\n        raise ValueError(\"To take an obstacle height of {0} m \".format(\n                         obstacle_height) + \"into consideration, wind \" +\n                         \"speed data of a greater height is needed.\")\n    # Return np.array if wind_speed is np.array\n    if (isinstance(wind_speed, np.ndarray) and\n            isinstance(roughness_length, pd.Series)):\n        roughness_length = np.array(roughness_length)\n\n    return (wind_speed * np.log((hub_height - 0.7 * obstacle_height) /\n                                roughness_length) /\n            np.log((wind_speed_height - 0.7 * obstacle_height) /\n                   roughness_length))", "response": "r Calculates the wind speed at hub height using a logarithmic wind profile."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hellman(wind_speed, wind_speed_height, hub_height,\n            roughness_length=None, hellman_exponent=None):\n    r\"\"\"\n    Calculates the wind speed at hub height using the hellman equation.\n\n    It is assumed that the wind profile follows a power law. This function is\n    carried out when the parameter `wind_speed_model` of an instance of\n    the :class:`~.modelchain.ModelChain` class is 'hellman'.\n\n    Parameters\n    ----------\n    wind_speed : pandas.Series or numpy.array\n        Wind speed time series.\n    wind_speed_height : float\n        Height for which the parameter `wind_speed` applies.\n    hub_height : float\n        Hub height of wind turbine.\n    roughness_length : pandas.Series or numpy.array or float\n        Roughness length. If given and `hellman_exponent` is None:\n        `hellman_exponent` = 1 / ln(hub_height/roughness_length),\n        otherwise `hellman_exponent` = 1/7. Default: None.\n    hellman_exponent : None or float\n        The Hellman exponent, which combines the increase in wind speed due to\n        stability of atmospheric conditions and surface roughness into one\n        constant. If None and roughness length is given\n        `hellman_exponent` = 1 / ln(hub_height/roughness_length),\n        otherwise `hellman_exponent` = 1/7. Default: None.\n\n    Returns\n    -------\n    pandas.Series or numpy.array\n        Wind speed at hub height. Data type depends on type of `wind_speed`.\n\n    Notes\n    -----\n    The following equation is used [1]_, [2]_, [3]_:\n\n    .. math:: v_{wind,hub}=v_{wind,data}\\cdot \\left(\\frac{h_{hub}}{h_{data}}\n        \\right)^\\alpha\n\n    with:\n        v: wind speed, h: height, :math:`\\alpha`: Hellman exponent\n\n    :math:`h_{data}` is the height in which the wind speed\n    :math:`v_{wind,data}` is measured and :math:`v_{wind,hub}` is the wind\n    speed at hub height :math:`h_{hub}` of the wind turbine.\n\n    For the Hellman exponent :math:`\\alpha` many studies use a value of 1/7 for\n    onshore and a value of 1/9 for offshore. The Hellman exponent can also\n    be calulated by the following equation [2]_, [3]_:\n\n    .. math:: \\alpha = \\frac{1}{\\ln\\left(\\frac{h_{hub}}{z_0} \\right)}\n\n    with:\n        :math:`z_{0}`: roughness length\n\n    Parameters `wind_speed_height`, `roughness_length`, `hub_height` and\n    `obstacle_height` have to be of the same unit.\n\n    References\n    ----------\n    .. [1] Sharp, E.: \"Spatiotemporal disaggregation of GB scenarios depicting\n            increased wind capacity and electrified heat demand in dwellings\".\n            UCL, Energy Institute, 2015, p. 83\n    .. [2] Hau, E.: \"Windkraftanlagen - Grundlagen, Technik, Einsatz,\n            Wirtschaftlichkeit\". 4. Auflage, Springer-Verlag, 2008, p. 517\n    .. [3] Quaschning V.: \"Regenerative Energiesysteme\". M\u00fcnchen, Hanser\n            Verlag, 2011, p. 279\n\n    \"\"\"\n    if hellman_exponent is None:\n        if roughness_length is not None:\n            # Return np.array if wind_speed is np.array\n            if (isinstance(wind_speed, np.ndarray) and\n                    isinstance(roughness_length, pd.Series)):\n                roughness_length = np.array(roughness_length)\n            hellman_exponent = 1 / np.log(hub_height / roughness_length)\n        else:\n            hellman_exponent = 1/7\n    return wind_speed * (hub_height / wind_speed_height) ** hellman_exponent", "response": "r Calculates the wind speed at hub height using the hellman equation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef smooth_power_curve(power_curve_wind_speeds, power_curve_values,\n                       block_width=0.5, wind_speed_range=15.0,\n                       standard_deviation_method='turbulence_intensity',\n                       mean_gauss=0, **kwargs):\n    r\"\"\"\n    Smooths the input power curve values by using a Gauss distribution.\n\n    The smoothing serves for taking the distribution of wind speeds over space\n    into account.\n\n    Parameters\n    ----------\n    power_curve_wind_speeds : pandas.Series or numpy.array\n        Wind speeds in m/s for which the power curve values are provided in\n        `power_curve_values`.\n    power_curve_values : pandas.Series or numpy.array\n        Power curve values corresponding to wind speeds in\n        `power_curve_wind_speeds`.\n    block_width : float\n        Width between the wind speeds in the sum of equation :eq:`power`.\n        Default: 0.5.\n    wind_speed_range : float\n        The sum in the equation below is taken for this wind speed range below\n        and above the power curve wind speed. Default: 15.0.\n    standard_deviation_method : string\n        Method for calculating the standard deviation for the Gauss\n        distribution. Options: 'turbulence_intensity', 'Staffell_Pfenninger'.\n        Default: 'turbulence_intensity'.\n    mean_gauss : float\n        Mean of the Gauss distribution in\n        :py:func:`~.tools.gauss_distribution`. Default: 0.\n\n    Other Parameters\n    ----------------\n    turbulence intensity : float, optional\n        Turbulence intensity at hub height of the wind turbine, wind farm or\n        wind turbine cluster the power curve is smoothed for.\n\n    Returns\n    -------\n    smoothed_power_curve_df : pd.DataFrame\n        Smoothed power curve. DataFrame has 'wind_speed' and 'value' columns\n        with wind speeds in m/s and the corresponding power curve value in W.\n\n    Notes\n    -----\n    The following equation is used to calculated the power curves values of the\n    smoothed power curve [1]_:\n\n    .. math:: P_{smoothed}(v_{std}) = \\sum\\limits_{v_i} \\Delta v_i \\cdot P(v_i)\n        \\cdot \\frac{1}{\\sigma \\sqrt{2 \\pi}}\n        \\exp \\left[-\\frac{(v_{std} - v_i -\\mu)^2}{2 \\sigma^2} \\right]\n       :label: power\n\n    with:\n        P: power [W], v: wind speed [m/s],\n        :math:`\\sigma`: standard deviation (Gauss), :math:`\\mu`: mean (Gauss)\n\n        :math:`P_{smoothed}` is the smoothed power curve value,\n        :math:`v_{std}` is the standard wind speed in the power curve,\n        :math:`\\Delta v_i` is the interval length between\n        :math:`v_\\text{i}` and :math:`v_\\text{i+1}`\n\n    Power curve smoothing is applied to take account for the spatial\n    distribution of wind speed. This way of smoothing power curves is also used\n    in [2]_ and [3]_.\n\n    The standard deviation :math:`\\sigma` of the above equation can be\n    calculated by the following methods.\n\n    'turbulence_intensity' [2]_:\n\n    .. math:: \\sigma = v_\\text{std} \\cdot \\sigma_\\text{n} = v_\\text{std}\n        \\cdot TI\n\n    with:\n        TI: turbulence intensity\n\n    'Staffell_Pfenninger' [4]_:\n\n    .. math:: \\sigma = 0.6 \\cdot 0.2 \\cdot v_\\text{std}\n\n    References\n    ----------\n    .. [1]  Knorr, K.: \"Modellierung von raum-zeitlichen Eigenschaften der\n             Windenergieeinspeisung f\u00fcr wetterdatenbasierte\n             Windleistungssimulationen\". Universit\u00e4t Kassel, Diss., 2016,\n             p. 106\n    .. [2]  N\u00f8rgaard, P. and Holttinen, H.: \"A Multi-Turbine and Power Curve\n             Approach\". Nordic Wind Power Conference, 1.\u20132.3.2004, 2000, p. 5\n    .. [3]  Kohler, S. and Agricola, A.-Cl. and Seidl, H.:\n             \"dena-Netzstudie II. Integration erneuerbarer Energien in die\n             deutsche Stromversorgung im Zeitraum 2015 \u2013 2020 mit Ausblick\n             2025\". Technical report, 2010.\n    .. [4]  Staffell, I. and Pfenninger, S.: \"Using Bias-Corrected Reanalysis\n              to Simulate Current and Future Wind Power Output\". 2005, p. 11\n\n    \"\"\"\n    # Specify normalized standard deviation\n    if standard_deviation_method == 'turbulence_intensity':\n        if ('turbulence_intensity' in kwargs and\n                kwargs['turbulence_intensity'] is not np.nan):\n            normalized_standard_deviation = kwargs['turbulence_intensity']\n        else:\n            raise ValueError(\"Turbulence intensity must be defined for \" +\n                             \"using 'turbulence_intensity' as \" +\n                             \"`standard_deviation_method`\")\n    elif standard_deviation_method == 'Staffell_Pfenninger':\n        normalized_standard_deviation = 0.2\n    else:\n        raise ValueError(\"{} is no valid `standard_deviation_method`. Valid \"\n                         + \"options are 'turbulence_intensity', or \"\n                         + \"'Staffell_Pfenninger'\".format(\n                                 standard_deviation_method))\n    # Initialize list for power curve values\n    smoothed_power_curve_values = []\n    # Append wind speeds to `power_curve_wind_speeds`\n    maximum_value = power_curve_wind_speeds.values[-1] + wind_speed_range\n    while power_curve_wind_speeds.values[-1] < maximum_value:\n        power_curve_wind_speeds = power_curve_wind_speeds.append(\n            pd.Series(power_curve_wind_speeds.iloc[-1] + 0.5,\n                      index=[power_curve_wind_speeds.index[-1] + 1]))\n        power_curve_values = power_curve_values.append(\n            pd.Series(0.0, index=[power_curve_values.index[-1] + 1]))\n    for power_curve_wind_speed in power_curve_wind_speeds:\n        # Create array of wind speeds for the sum\n        wind_speeds_block = (np.arange(\n            -wind_speed_range, wind_speed_range + block_width, block_width) +\n            power_curve_wind_speed)\n        # Get standard deviation for Gauss function\n        standard_deviation = (\n            (power_curve_wind_speed * normalized_standard_deviation + 0.6)\n            if standard_deviation_method is 'Staffell_Pfenninger'\n            else power_curve_wind_speed * normalized_standard_deviation)\n        # Get the smoothed value of the power output\n        if standard_deviation == 0.0:\n            # The gaussian distribution is not defined for a standard deviation\n            # of zero. Smoothed power curve value is set to zero.\n            smoothed_value = 0.0\n        else:\n            smoothed_value = sum(\n                block_width * np.interp(wind_speed, power_curve_wind_speeds,\n                                        power_curve_values, left=0, right=0) *\n                tools.gauss_distribution(\n                    power_curve_wind_speed - wind_speed,\n                    standard_deviation, mean_gauss)\n                for wind_speed in wind_speeds_block)\n        # Add value to list - add zero if `smoothed_value` is nan as Gauss\n        # distribution for a standard deviation of zero.\n        smoothed_power_curve_values.append(smoothed_value)\n    # Create smoothed power curve data frame\n    smoothed_power_curve_df = pd.DataFrame(\n        data=[list(power_curve_wind_speeds.values),\n              smoothed_power_curve_values]).transpose()\n    # Rename columns of the data frame\n    smoothed_power_curve_df.columns = ['wind_speed', 'value']\n    return smoothed_power_curve_df", "response": "r Returns a new tree that is a new tree that is created by moving the input power curves over a given wind speed range."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wind_speed_hub(self, weather_df):\n        if self.power_plant.hub_height in weather_df['wind_speed']:\n            wind_speed_hub = weather_df['wind_speed'][\n                self.power_plant.hub_height]\n        elif self.wind_speed_model == 'logarithmic':\n            logging.debug('Calculating wind speed using logarithmic wind '\n                          'profile.')\n            closest_height = weather_df['wind_speed'].columns[\n                min(range(len(weather_df['wind_speed'].columns)),\n                    key=lambda i: abs(weather_df['wind_speed'].columns[i] -\n                                      self.power_plant.hub_height))]\n            wind_speed_hub = wind_speed.logarithmic_profile(\n                weather_df['wind_speed'][closest_height], closest_height,\n                self.power_plant.hub_height,\n                weather_df['roughness_length'].iloc[:, 0],\n                self.obstacle_height)\n        elif self.wind_speed_model == 'hellman':\n            logging.debug('Calculating wind speed using hellman equation.')\n            closest_height = weather_df['wind_speed'].columns[\n                min(range(len(weather_df['wind_speed'].columns)),\n                    key=lambda i: abs(weather_df['wind_speed'].columns[i] -\n                                      self.power_plant.hub_height))]\n            wind_speed_hub = wind_speed.hellman(\n                weather_df['wind_speed'][closest_height], closest_height,\n                self.power_plant.hub_height,\n                weather_df['roughness_length'].iloc[:, 0],\n                self.hellman_exp)\n        elif self.wind_speed_model == 'interpolation_extrapolation':\n            logging.debug('Calculating wind speed using linear inter- or '\n                          'extrapolation.')\n            wind_speed_hub = tools.linear_interpolation_extrapolation(\n                weather_df['wind_speed'], self.power_plant.hub_height)\n        elif self.wind_speed_model == 'log_interpolation_extrapolation':\n            logging.debug('Calculating wind speed using logarithmic inter- or '\n                          'extrapolation.')\n            wind_speed_hub = tools.logarithmic_interpolation_extrapolation(\n                weather_df['wind_speed'], self.power_plant.hub_height)\n        else:\n            raise ValueError(\"'{0}' is an invalid value. \".format(\n                self.wind_speed_model) + \"`wind_speed_model` must be \"\n                \"'logarithmic', 'hellman', 'interpolation_extrapolation' \" +\n                \"or 'log_interpolation_extrapolation'.\")\n        return wind_speed_hub", "response": "r Calculates the wind speed at hub height."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef logarithmic_interpolation_extrapolation(df, target_height):\n    # find closest heights\n    heights_sorted = df.columns[\n        sorted(range(len(df.columns)),\n               key=lambda i: abs(df.columns[i] - target_height))]\n    return ((np.log(target_height) *\n             (df[heights_sorted[1]] - df[heights_sorted[0]]) -\n             df[heights_sorted[1]] * np.log(heights_sorted[0]) +\n             df[heights_sorted[0]] * np.log(heights_sorted[1])) /\n            (np.log(heights_sorted[1]) - np.log(heights_sorted[0])))", "response": "r Logarithmic interpolation of the wind speed at hub height."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_model(self, weather_df):\n\n        self.assign_power_curve(weather_df)\n        self.power_plant.mean_hub_height()\n        wind_speed_hub = self.wind_speed_hub(weather_df)\n        density_hub = (None if (self.power_output_model == 'power_curve' and\n                                self.density_correction is False)\n                       else self.density_hub(weather_df))\n        if (self.wake_losses_model != 'power_efficiency_curve' and\n                self.wake_losses_model != 'constant_efficiency' and\n                self.wake_losses_model is not None):\n            # Reduce wind speed with wind efficiency curve\n            wind_speed_hub = wake_losses.reduce_wind_speed(\n                wind_speed_hub,\n                wind_efficiency_curve_name=self.wake_losses_model)\n        self.power_output = self.calculate_power_output(wind_speed_hub,\n                                                        density_hub)\n        return self", "response": "r This function runs the model of the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef power_curve(wind_speed, power_curve_wind_speeds, power_curve_values,\n                density=None, density_correction=False):\n    r\"\"\"\n    Calculates the turbine power output using a power curve.\n\n    This function is carried out when the parameter `power_output_model` of an\n    instance of the :class:`~.modelchain.ModelChain` class is 'power_curve'. If\n    the parameter `density_correction` is True the density corrected power\n    curve (See :py:func:`~.power_curve_density_correction`) is used.\n\n    Parameters\n    ----------\n    wind_speed : pandas.Series or numpy.array\n        Wind speed at hub height in m/s.\n    power_curve_wind_speeds : pandas.Series or numpy.array\n        Wind speeds in m/s for which the power curve values are provided in\n        `power_curve_values`.\n    power_curve_values : pandas.Series or numpy.array\n        Power curve values corresponding to wind speeds in\n        `power_curve_wind_speeds`.\n    density : pandas.Series or numpy.array\n        Density of air at hub height in kg/m\u00b3. This parameter is needed\n        if `density_correction` is True. Default: None.\n    density_correction : boolean\n        If the parameter is True the density corrected power curve is used for\n        the calculation of the turbine power output. In this case `density`\n        cannot be None. Default: False.\n\n    Returns\n    -------\n    pandas.Series or numpy.array\n        Electrical power output of the wind turbine in W.\n        Data type depends on type of `wind_speed`.\n\n    Notes\n    -------\n    It is assumed that the power output for wind speeds above the maximum\n    and below the minimum wind speed given in the power curve is zero.\n\n    \"\"\"\n    if density_correction is False:\n        power_output = np.interp(wind_speed, power_curve_wind_speeds,\n                                 power_curve_values, left=0, right=0)\n        # Power_output as pd.Series if wind_speed is pd.Series (else: np.array)\n        if isinstance(wind_speed, pd.Series):\n            power_output = pd.Series(data=power_output, index=wind_speed.index,\n                                     name='feedin_power_plant')\n        else:\n            power_output = np.array(power_output)\n    elif density_correction is True:\n        power_output = power_curve_density_correction(\n            wind_speed, power_curve_wind_speeds, power_curve_values, density)\n    else:\n        raise TypeError(\"'{0}' is an invalid type. \".format(type(\n                        density_correction)) + \"`density_correction` must \" +\n                        \"be Boolean (True or False).\")\n    return power_output", "response": "r Calculates the turbine power output for a given wind speed."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef initialize_wind_farms(my_turbine, e126):\n\n    # specification of wind farm data\n    example_farm_data = {\n        'name': 'example_farm',\n        'wind_turbine_fleet': [{'wind_turbine': my_turbine,\n                                'number_of_turbines': 6},\n                               {'wind_turbine': e126,\n                                'number_of_turbines': 3}\n                               ]}\n\n    # initialize WindFarm object\n    example_farm = WindFarm(**example_farm_data)\n\n    # specification of wind farm data (2) containing a wind farm efficiency\n    # and coordinates\n    example_farm_2_data = {\n        'name': 'example_farm_2',\n        'wind_turbine_fleet': [{'wind_turbine': my_turbine,\n                                'number_of_turbines': 6},\n                               {'wind_turbine': e126,\n                                'number_of_turbines': 3}],\n        'efficiency': 0.9,\n        'coordinates': [52.2, 13.1]}\n\n    # initialize WindFarm object\n    example_farm_2 = WindFarm(**example_farm_2_data)\n\n    return example_farm, example_farm_2", "response": "r This function initializes two WindFarm objects with the same name and a different power curve."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef initialize_wind_turbine_cluster(example_farm, example_farm_2):\n\n    # specification of cluster data\n    example_cluster_data = {\n        'name': 'example_cluster',\n        'wind_farms': [example_farm, example_farm_2]}\n\n    # initialize WindTurbineCluster object\n    example_cluster = WindTurbineCluster(**example_cluster_data)\n\n    return example_cluster", "response": "r Initializes a WindTurbineCluster object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calculate_power_output(weather, example_farm, example_cluster):\n\n    # set efficiency of example_farm to apply wake losses\n    example_farm.efficiency = 0.9\n    # power output calculation for example_farm\n    # initialize TurbineClusterModelChain with default parameters and use\n    # run_model method to calculate power output\n    mc_example_farm = TurbineClusterModelChain(example_farm).run_model(weather)\n    # write power output time series to WindFarm object\n    example_farm.power_output = mc_example_farm.power_output\n\n    # power output calculation for turbine_cluster\n    # own specifications for TurbineClusterModelChain setup\n    modelchain_data = {\n        'wake_losses_model': 'constant_efficiency',  #\n                                           # 'dena_mean' (default), None,\n                                           # 'power_efficiency_curve',\n                                           # 'constant_efficiency' or name of\n                                           #  a wind efficiency curve\n                #  see :py:func:`~.wake_losses.get_wind_efficiency_curve`\n        'smoothing': True,  # False (default) or True\n        'block_width': 0.5,  # default: 0.5\n        'standard_deviation_method': 'Staffell_Pfenninger',  #\n                                            # 'turbulence_intensity' (default)\n                                            # or 'Staffell_Pfenninger'\n        'smoothing_order': 'wind_farm_power_curves',  #\n                                        # 'wind_farm_power_curves' (default) or\n                                        # 'turbine_power_curves'\n        'wind_speed_model': 'logarithmic',  # 'logarithmic' (default),\n                                            # 'hellman' or\n                                            # 'interpolation_extrapolation'\n        'density_model': 'ideal_gas',  # 'barometric' (default), 'ideal_gas' or\n                                       # 'interpolation_extrapolation'\n        'temperature_model': 'linear_gradient',  # 'linear_gradient' (def.) or\n                                                 # 'interpolation_extrapolation'\n        'power_output_model': 'power_curve',  # 'power_curve' (default) or\n                                              # 'power_coefficient_curve'\n        'density_correction': True,  # False (default) or True\n        'obstacle_height': 0,  # default: 0\n        'hellman_exp': None}  # None (default) or None\n    # initialize TurbineClusterModelChain with own specifications and use\n    # run_model method to calculate power output\n    mc_example_cluster = TurbineClusterModelChain(\n            example_cluster, **modelchain_data).run_model(weather)\n    # write power output time series to WindTurbineCluster object\n    example_cluster.power_output = mc_example_cluster.power_output\n\n    return", "response": "r Calculates the power output of the wind farms and clusters using the TurbineClusterModelChain."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot_or_print(example_farm, example_cluster):\n\n    # plot or print power output\n    if plt:\n        example_cluster.power_output.plot(legend=True, label='example cluster')\n        example_farm.power_output.plot(legend=True, label='example farm')\n        plt.show()\n    else:\n        print(example_cluster.power_output)\n        print(example_farm.power_output)", "response": "r Plot or print the power output of the given cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_turbine_data_from_file(turbine_type, file_):\n    def isfloat(x):\n        try:\n            float(x)\n            return x\n        except ValueError:\n            return False\n\n    try:\n        df = pd.read_csv(file_, index_col=0)\n    except FileNotFoundError:\n        raise FileNotFoundError(\"The file '{}' was not found.\".format(file_))\n    wpp_df = df[df.turbine_id == turbine_type]\n    # if turbine not in data file\n    if wpp_df.shape[0] == 0:\n        pd.set_option('display.max_rows', len(df))\n        logging.info('Possible types: \\n{0}'.format(df.turbine_id))\n        pd.reset_option('display.max_rows')\n        sys.exit('Cannot find the wind converter type: {0}'.format(\n            turbine_type))\n    # if turbine in data file select power (coefficient) curve columns and\n    # drop nans\n    cols = [_ for _ in wpp_df.columns if isfloat(_)]\n    curve_data = wpp_df[cols].dropna(axis=1)\n    df = curve_data.transpose().reset_index()\n    df.columns = ['wind_speed', 'value']\n    df['wind_speed'] = df['wind_speed'].apply(lambda x: float(x))\n    nominal_power = wpp_df['p_nom'].iloc[0]\n    return df, nominal_power", "response": "r Returns the data for a given turbine type and file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef distance(latitude_1, longitude_1, latitude_2, longitude_2):\n\n    coef = mod_math.cos(latitude_1 / 180. * mod_math.pi)\n    x = latitude_1 - latitude_2\n    y = (longitude_1 - longitude_2) * coef\n\n    return mod_math.sqrt(x * x + y * y) * ONE_DEGREE", "response": "Distance between two points."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_color_between(color1, color2, i):\n    if i <= 0:\n        return color1\n    if i >= 1:\n        return color2\n    return (int(color1[0] + (color2[0] - color1[0]) * i),\n            int(color1[1] + (color2[1] - color1[1]) * i),\n            int(color1[2] + (color2[2] - color1[2]) * i))", "response": "Get the color between two colors."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _IDW(self, latitude, longitude, radius=1):\n        tile = self.get_file(latitude, longitude)\n        if tile is None:\n            return None\n        return tile._InverseDistanceWeighted(latitude, longitude, radius)", "response": "Returns the interpolated elevation at a point."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_file(self, latitude, longitude):\n        file_name = self.get_file_name(latitude, longitude)\n\n        if not file_name:\n            return None\n\n        if (file_name in self.files):\n            return self.files[file_name]\n        else:\n            data = self.retrieve_or_load_file_data(file_name)\n            if not data:\n                return None\n\n            result = GeoElevationFile(file_name, data, self)\n\n            # Store file (if in batch mode, just keep most recent)\n            if self.batch_mode:\n                self.files = {file_name: result}\n            else:\n                self.files[file_name] = result\n\n            return result", "response": "Retrieves a file from the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an image of size latitude_interval longitude_interval max_elevation and min_elevation.", "response": "def get_image(self, size, latitude_interval, longitude_interval, max_elevation, min_elevation=0,\n                  unknown_color = (255, 255, 255, 255), zero_color = (0, 0, 255, 255),\n                  min_color = (0, 0, 0, 255), max_color = (0, 255, 0, 255),\n                  mode='image'):\n        \"\"\"\n        Returns a numpy array or PIL image.\n        \"\"\"\n\n        if not size or len(size) != 2:\n            raise Exception('Invalid size %s' % size)\n        if not latitude_interval or len(latitude_interval) != 2:\n            raise Exception('Invalid latitude interval %s' % latitude_interval)\n        if not longitude_interval or len(longitude_interval) != 2:\n            raise Exception('Invalid longitude interval %s' % longitude_interval)\n\n        width, height = size\n        width, height = int(width), int(height)\n\n        latitude_from,  latitude_to  = latitude_interval\n        longitude_from, longitude_to = longitude_interval\n\n\n        if mode == 'array':\n            import numpy as np\n            array = np.empty((height,width))\n            for row in range(height):\n                for column in range(width):\n                    latitude  = latitude_from  + float(row) / height * (latitude_to  - latitude_from)\n                    longitude = longitude_from + float(column) / width * (longitude_to - longitude_from)\n                    elevation = self.get_elevation(latitude, longitude)\n                    array[row,column] = elevation\n\n            return array\n\n        elif mode == 'image':\n            try:    import Image as mod_image\n            except: from PIL import Image as mod_image\n            try:    import ImageDraw as mod_imagedraw\n            except: from PIL import ImageDraw as mod_imagedraw\n\n            image = mod_image.new('RGBA', (width, height),\n                              (255, 255, 255, 255))\n            draw = mod_imagedraw.Draw(image)\n\n            max_elevation -= min_elevation\n\n            for row in range(height):\n                for column in range(width):\n                    latitude  = latitude_from  + float(row) / height * (latitude_to  - latitude_from)\n                    longitude = longitude_from + float(column) / width * (longitude_to - longitude_from)\n                    elevation = self.get_elevation(latitude, longitude)\n                    if elevation == None:\n                        color = unknown_color\n                    else:\n                        elevation_coef = (elevation - min_elevation) / float(max_elevation)\n                        if elevation_coef < 0: elevation_coef = 0\n                        if elevation_coef > 1: elevation_coef = 1\n                        color = mod_utils.get_color_between(min_color, max_color, elevation_coef)\n                        if elevation <= 0:\n                            color = zero_color\n                    draw.point((column, height - row), color)\n\n            return image\n        else:\n            raise Exception('Invalid mode ' + mode)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding elevation values to the current object.", "response": "def add_elevations(self, gpx, only_missing=False, smooth=False, gpx_smooth_no=0):\n        \"\"\"\n        only_missing -- if True only points without elevation will get a SRTM value\n\n        smooth -- if True interpolate between points\n\n        if gpx_smooth_no > 0 -- execute gpx.smooth(vertical=True)\n        \"\"\"\n        if only_missing:\n            original_elevations = list(map(lambda point: point.elevation, gpx.walk(only_points=True)))\n\n        if smooth:\n            self._add_sampled_elevations(gpx)\n        else:\n            for point in gpx.walk(only_points=True):\n                ele = self.get_elevation(point.latitude, point.longitude)\n                if ele is not None:\n                    point.elevation = ele\n\n        for i in range(gpx_smooth_no):\n            gpx.smooth(vertical=True, horizontal=False)\n\n        if only_missing:\n            for original_elevation, point in zip(original_elevations, list(gpx.walk(only_points=True))):\n                if original_elevation != None:\n                    point.elevation = original_elevation"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _add_interval_elevations(self, gpx, min_interval_length=100):\n        for track in gpx.tracks:\n            for segment in track.segments:\n                last_interval_changed = 0\n                previous_point = None\n                length = 0\n                for no, point in enumerate(segment.points):\n                    if previous_point:\n                        length += point.distance_2d(previous_point)\n\n                    if no == 0 or no == len(segment.points) - 1 or length > last_interval_changed:\n                        last_interval_changed += min_interval_length\n                        point.elevation = self.get_elevation(point.latitude, point.longitude)\n                    else:\n                        point.elevation = None\n                    previous_point = point\n        gpx.add_missing_elevations()", "response": "Adds elevation on points every min_interval_length and add missing_elevations on points every min_interval_length and add elevation between them"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the elevation of the specified location.", "response": "def get_elevation(self, latitude, longitude, approximate=None):\n        \"\"\"\n        If approximate is True then only the points from SRTM grid will be\n        used, otherwise a basic aproximation of nearby points will be calculated.\n        \"\"\"\n        if not (self.latitude - self.resolution <= latitude < self.latitude + 1):\n            raise Exception('Invalid latitude %s for file %s' % (latitude, self.file_name))\n        if not (self.longitude <= longitude < self.longitude + 1 + self.resolution):\n            raise Exception('Invalid longitude %s for file %s' % (longitude, self.file_name))\n\n        row, column = self.get_row_and_column(latitude, longitude)\n\n        if approximate:\n            return self.approximation(latitude, longitude)\n        else:\n            return self.get_elevation_from_row_and_column(int(row), int(column))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the approximate elevation of the entry in the cache.", "response": "def approximation(self, latitude, longitude):\n        \"\"\"\n        Dummy approximation with nearest points. The nearest the neighbour the\n        more important will be its elevation.\n        \"\"\"\n        d = 1. / self.square_side\n        d_meters = d * mod_utils.ONE_DEGREE\n\n        # Since the less the distance => the more important should be the\n        # distance of the point, we'll use d-distance as importance coef\n        # here:\n        importance_1 = d_meters - mod_utils.distance(latitude + d, longitude, latitude, longitude)\n        elevation_1  = self.geo_elevation_data.get_elevation(latitude + d, longitude, approximate=False)\n\n        importance_2 = d_meters - mod_utils.distance(latitude - d, longitude, latitude, longitude)\n        elevation_2  = self.geo_elevation_data.get_elevation(latitude - d, longitude, approximate=False)\n\n        importance_3 = d_meters - mod_utils.distance(latitude, longitude + d, latitude, longitude)\n        elevation_3  = self.geo_elevation_data.get_elevation(latitude, longitude + d, approximate=False)\n\n        importance_4 = d_meters - mod_utils.distance(latitude, longitude - d, latitude, longitude)\n        elevation_4  = self.geo_elevation_data.get_elevation(latitude, longitude - d, approximate=False)\n        # TODO(TK) Check if coordinates inside the same file, and only then decide if to call\n        # self.geo_elevation_data.get_elevation or just self.get_elevation\n\n        if elevation_1 == None or elevation_2 == None or elevation_3 == None or elevation_4 == None:\n            elevation = self.get_elevation(latitude, longitude, approximate=False)\n            if not elevation:\n                return None\n            elevation_1 = elevation_1 or elevation\n            elevation_2 = elevation_2 or elevation\n            elevation_3 = elevation_3 or elevation\n            elevation_4 = elevation_4 or elevation\n\n        # Normalize importance:\n        sum_importances = float(importance_1 + importance_2 + importance_3 + importance_4)\n\n        # Check normalization:\n        assert abs(importance_1 / sum_importances + \\\n                   importance_2 / sum_importances + \\\n                   importance_3 / sum_importances + \\\n                   importance_4 / sum_importances - 1 ) < 0.000001\n\n        result = importance_1 / sum_importances * elevation_1 + \\\n               importance_2 / sum_importances * elevation_2 + \\\n               importance_3 / sum_importances * elevation_3 + \\\n               importance_4 / sum_importances * elevation_4\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the Inverse Distance Weighted Elevation. Interpolate the elevation of the given point using the inverse distance weigthing algorithm (exp of 1) in the form: sum((1/distance) * elevation)/sum(1/distance) for each point in the matrix. The matrix size is determined by the radius. A radius of 1 uses 5 points and a radius of 2 uses 13 points. The matrices are set up to use cells adjacent to and including the one that contains the given point. Any cells referenced by the matrix that are on neighboring tiles are ignored. Args: latitude: float of the latitude in decimal degrees longitude: float of the longitude in decimal degrees radius: int of 1 or 2 indicating the size of the matrix Returns: a float of the interpolated elevation in the same units as the underlying .hgt file (meters) Exceptions: raises a ValueError if an invalid radius is supplied", "response": "def _InverseDistanceWeighted(self, latitude, longitude, radius=1):\n        \"\"\"\n        Return the Inverse Distance Weighted Elevation.\n\n        Interpolate the elevation of the given point using the inverse\n        distance weigthing algorithm (exp of 1) in the form:\n            sum((1/distance) * elevation)/sum(1/distance)\n            for each point in the matrix.\n        The matrix size is determined by the radius. A radius of 1 uses\n        5 points and a radius of 2 uses 13 points. The matrices are set\n        up to use cells adjacent to and including the one that contains\n        the given point. Any cells referenced by the matrix that are on\n        neighboring tiles are ignored.\n\n        Args:\n            latitude: float of the latitude in decimal degrees\n            longitude: float of the longitude in decimal degrees\n            radius: int of 1 or 2 indicating the size of the matrix\n\n        Returns:\n            a float of the interpolated elevation in the same units as\n            the underlying .hgt file (meters)\n\n        Exceptions:\n            raises a ValueError if an invalid radius is supplied\n\n        \"\"\"\n        if radius == 1:\n            offsetmatrix = (None, (0, 1), None,\n                           (-1, 0), (0, 0), (1, 0),\n                           None, (0, -1), None)\n\n        elif radius == 2:\n            offsetmatrix = (None, None, (0, 2), None, None,\n                            None, (-1, 1), (0, 1), (1, 1), None,\n                            (-2, 0), (-1, 0), (0, 0), (1, 0), (2, 0),\n                            None, (-1, -1), (0, -1), (1, -1), None,\n                            None, None, (0, -2), None, None)\n        else:\n            raise ValueError(\"Radius {} invalid, \"\n                             \"expected 1 or 2\".format(radius))\n\n        row, column = self.get_row_and_column(latitude, longitude)\n        center_lat, center_long = self.get_lat_and_long(row, column)\n        if latitude == center_lat and longitude == center_long:\n            # return direct elev at point (infinite weight)\n            return self.get_elevation_from_row_and_column(int(row), int(column))\n        weights = 0\n        elevation = 0\n\n        for offset in offsetmatrix:\n            if (offset is not None and\n                0 <= row + offset[0] < self.square_side and\n                0 <= column + offset[1] < self.square_side):\n                cell = self.get_elevation_from_row_and_column(int(row + offset[0]),\n                                                              int(column + offset[1]))\n                if cell is not None:\n                    # does not need to be meters, anything proportional\n                    distance = mod_utils.distance(latitude, longitude,\n                                                  center_lat + float(offset[0])/(self.square_side-1),\n                                                  center_long + float(offset[1])/(self.square_side-1))\n                    weights += 1/distance\n                    elevation += cell/distance\n        return elevation/weights"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_file_name_starting_position(self):\n        groups = mod_re.findall('([NS])(\\d+)([EW])(\\d+)\\.hgt', self.file_name)\n\n        assert groups and len(groups) == 1 and len(groups[0]) == 4, 'Invalid file name {0}'.format(self.file_name)\n\n        groups = groups[0]\n\n        if groups[0] == 'N':\n            latitude = float(groups[1])\n        else:\n            latitude = - float(groups[1])\n\n        if groups[2] == 'E':\n            longitude = float(groups[3])\n        else:\n            longitude = - float(groups[3])\n\n        self.latitude = latitude\n        self.longitude = longitude", "response": "Parses the starting position of the file name"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the data for the elevation dataset.", "response": "def get_data(srtm1=True, srtm3=True, leave_zipped=False, file_handler=None,\n             use_included_urls=True, batch_mode=False):\n    \"\"\"\n    Get the utility object for querying elevation data.\n\n    All data files will be stored in localy (note that it may be\n    gigabytes of data so clean it from time to time).\n\n    On first run -- all files needed url will be stored and for every next\n    elevation query if the SRTM file is not found it will be retrieved and\n    saved.\n\n    If you need to change the way the files are saved locally (for example if\n    you need to save them locally) -- change the file_handler. See\n    srtm.main.FileHandler.\n\n    If leave_zipped is True then files will be stored locally as compressed\n    zip files. That means less disk space but more computing space for every\n    file loaded.\n\n    If use_included_urls is True urls to SRTM files included in the library\n    will be used. Set to false if you need to reload them on first run.\n\n    If batch_mode is True, only the most recent file will be stored. This is\n    ideal for situations where you want to use this function to enrich a very\n    large dataset. If your data are spread over a wide geographic area, this\n    setting will make this function slower but will greatly reduce the risk\n    of out-of-memory errors. Default is False.\n\n    With srtm1 or srtm3 params you can decide which SRTM format to use. Srtm3\n    has a resolution of three arc-seconds (cca 90 meters between points).\n    Srtm1 has a resolution of one arc-second (cca 30 meters). Srtm1 is\n    available only for the United states. If both srtm1 ans srtm3 are True and\n    both files are present for a location -- the srtm1 will be used.\n    \"\"\"\n    if not file_handler:\n        file_handler = FileHandler()\n\n    if not srtm1 and not srtm3:\n        raise Exception('At least one of srtm1 and srtm3 must be True')\n\n    srtm1_files, srtm3_files = _get_urls(use_included_urls, file_handler)\n\n    assert srtm1_files\n    assert srtm3_files\n\n    if not srtm1:\n        srtm1_files = {}\n    if not srtm3:\n        srtm3_files = {}\n\n    assert srtm1_files or srtm3_files\n\n    return mod_data.GeoElevationData(srtm1_files, srtm3_files, file_handler=file_handler,\n                                     leave_zipped=leave_zipped, batch_mode=batch_mode)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def subscribe(self, topic: str):\n\n        req_msg = {\n            'type': 'subscribe',\n            'topic': topic,\n            'response': True\n        }\n\n        await self._conn.send_message(req_msg)", "response": "Subscribe to a channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _generate_signature(self, nonce, method, path, data):\n\n        data_json = \"\"\n        endpoint = path\n        if method == \"get\":\n            if data:\n                query_string = self._get_params_for_sig(data)\n                endpoint = \"{}?{}\".format(path, query_string)\n        elif data:\n            data_json = compact_json_dict(data)\n        sig_str = (\"{}{}{}{}\".format(nonce, method.upper(), endpoint, data_json)).encode('utf-8')\n        m = hmac.new(self.API_SECRET.encode('utf-8'), sig_str, hashlib.sha256)\n        return base64.b64encode(m.digest())", "response": "Generate the signature string for the call"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate an account in Kucoin", "response": "def create_account(self, account_type, currency):\n        \"\"\"Create an account\n\n        https://docs.kucoin.com/#create-an-account\n\n        :param account_type: Account type - main or trade\n        :type account_type: string\n        :param currency: Currency code\n        :type currency: string\n\n        .. code:: python\n\n            account = client.create_account('trade', 'BTC')\n\n        :returns: API Response\n\n        .. code-block:: python\n\n            {\n                \"id\": \"5bd6e9286d99522a52e458de\"\n            }\n\n        :raises:  KucoinResponseException, KucoinAPIException\n\n        \"\"\"\n\n        data = {\n            'type': account_type,\n            'currency': currency\n        }\n\n        return self._post('accounts', True, data=data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_account_activity(self, account_id, start=None, end=None, page=None, limit=None):\n\n        data = {}\n        if start:\n            data['startAt'] = start\n        if end:\n            data['endAt'] = end\n        if page:\n            data['currentPage'] = page\n        if limit:\n            data['pageSize'] = limit\n\n        return self._get('accounts/{}/ledgers'.format(account_id), True, data=data)", "response": "Get list of account activity for the given account."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_account_holds(self, account_id, page=None, page_size=None):\n\n        data = {}\n        if page:\n            data['currentPage'] = page\n        if page_size:\n            data['pageSize'] = page_size\n\n        return self._get('accounts/{}/holds'.format(account_id), True, data=data)", "response": "Get account holds for any active orders or pending withdraw requests."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating an inner transfer for any active orders or pending withdraw", "response": "def create_inner_transfer(self, from_account_id, to_account_id, amount, order_id=None):\n        \"\"\"Get account holds placed for any active orders or pending withdraw requests\n\n        https://docs.kucoin.com/#get-holds\n\n        :param from_account_id: ID of account to transfer funds from - from list_accounts()\n        :type from_account_id: str\n        :param to_account_id: ID of account to transfer funds to - from list_accounts()\n        :type to_account_id: str\n        :param amount: Amount to transfer\n        :type amount: int\n        :param order_id: (optional) Request ID (default flat_uuid())\n        :type order_id: string\n\n        .. code:: python\n\n            transfer = client.create_inner_transfer('5bd6e9216d99522a52e458d6', 5bc7f080b39c5c03286eef8e', 20)\n\n        :returns: API Response\n\n        .. code-block:: python\n\n            {\n                \"orderId\": \"5bd6e9286d99522a52e458de\"\n            }\n\n        :raises:  KucoinResponseException, KucoinAPIException\n\n        \"\"\"\n\n        data = {\n            'payAccountId': from_account_id,\n            'recAccountId': to_account_id,\n            'amount': amount\n        }\n\n        if order_id:\n            data['clientOid'] = order_id\n        else:\n            data['clientOid'] = flat_uuid()\n\n        return self._post('accounts/inner-transfer', True, data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate deposit address of currency. You can just create one deposit address. You can just create one deposit address. You can just create one deposit address.", "response": "def create_deposit_address(self, currency):\n        \"\"\"Create deposit address of currency for deposit. You can just create one deposit address.\n\n        https://docs.kucoin.com/#create-deposit-address\n\n        :param currency: Name of currency\n        :type currency: string\n\n        .. code:: python\n\n            address = client.create_deposit_address('NEO')\n\n        :returns: ApiResponse\n\n        .. code:: python\n\n            {\n                \"address\": \"0x78d3ad1c0aa1bf068e19c94a2d7b16c9c0fcd8b1\",\n                \"memo\": \"5c247c8a03aa677cea2a251d\"\n            }\n\n        :raises: KucoinResponseException, KucoinAPIException\n\n        \"\"\"\n\n        data = {\n            'currency': currency\n        }\n\n        return self._post('deposit-addresses', True, data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the deposit address for a currency", "response": "def get_deposit_address(self, currency):\n        \"\"\"Get deposit address for a currency\n\n        https://docs.kucoin.com/#get-deposit-address\n\n        :param currency: Name of currency\n        :type currency: string\n\n        .. code:: python\n\n            address = client.get_deposit_address('NEO')\n\n        :returns: ApiResponse\n\n        .. code:: python\n\n            {\n                \"address\": \"0x78d3ad1c0aa1bf068e19c94a2d7b16c9c0fcd8b1\",\n                \"memo\": \"5c247c8a03aa677cea2a251d\"\n            }\n\n        :raises: KucoinResponseException, KucoinAPIException\n\n        \"\"\"\n\n        data = {\n            'currency': currency\n        }\n\n        return self._get('deposit-addresses', True, data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_withdrawals(self, currency=None, status=None, start=None, end=None, page=None, limit=None):\n\n        data = {}\n        if currency:\n            data['currency'] = currency\n        if status:\n            data['status'] = status\n        if start:\n            data['startAt'] = start\n        if end:\n            data['endAt'] = end\n        if limit:\n            data['pageSize'] = limit\n        if page:\n            data['page'] = page\n\n        return self._get('withdrawals', True, data=data)", "response": "Get withdrawals records for a currency."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting withdrawal quotas for a currency", "response": "def get_withdrawal_quotas(self, currency):\n        \"\"\"Get withdrawal quotas for a currency\n\n        https://docs.kucoin.com/#get-withdrawal-quotas\n\n        :param currency: Name of currency\n        :type currency: string\n\n        .. code:: python\n\n            quotas = client.get_withdrawal_quotas('ETH')\n\n        :returns: ApiResponse\n\n        .. code:: python\n\n            {\n                \"currency\": \"ETH\",\n                \"availableAmount\": 2.9719999,\n                \"remainAmount\": 2.9719999,\n                \"withdrawMinSize\": 0.1000000,\n                \"limitBTCAmount\": 2.0,\n                \"innerWithdrawMinFee\": 0.00001,\n                \"isWithdrawEnabled\": true,\n                \"withdrawMinFee\": 0.0100000,\n                \"precision\": 7\n            }\n\n        :raises: KucoinResponseException, KucoinAPIException\n\n        \"\"\"\n\n        data = {\n            'currency': currency\n        }\n\n        return self._get('withdrawals/quotas', True, data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_withdrawal(self, currency, amount, address, memo=None, is_inner=False, remark=None):\n\n        data = {\n            'currency': currency,\n            'amount': amount,\n            'address': address\n        }\n\n        if memo:\n            data['memo'] = memo\n        if is_inner:\n            data['isInner'] = is_inner\n        if remark:\n            data['remark'] = remark\n\n        return self._post('withdrawals', True, data=data)", "response": "Create a withdrawal for a specific key."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_market_order(self, symbol, side, size=None, funds=None, client_oid=None, remark=None, stp=None):\n\n        if not size and not funds:\n            raise MarketOrderException('Need size or fund parameter')\n\n        if size and funds:\n            raise MarketOrderException('Need size or fund parameter not both')\n\n        data = {\n            'side': side,\n            'symbol': symbol,\n            'type': self.ORDER_MARKET\n        }\n\n        if size:\n            data['size'] = size\n        if funds:\n            data['funds'] = funds\n        if client_oid:\n            data['clientOid'] = client_oid\n        else:\n            data['clientOid'] = flat_uuid()\n        if remark:\n            data['remark'] = remark\n        if stp:\n            data['stp'] = stp\n\n        return self._post('orders', True, data=data)", "response": "Create a new market order"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_limit_order(self, symbol, side, price, size, client_oid=None, remark=None,\n                           time_in_force=None, stop=None, stop_price=None, stp=None, cancel_after=None, post_only=None,\n                           hidden=None, iceberg=None, visible_size=None):\n        \"\"\"Create an order\n\n        https://docs.kucoin.com/#place-a-new-order\n\n        :param symbol: Name of symbol e.g. KCS-BTC\n        :type symbol: string\n        :param side: buy or sell\n        :type side: string\n        :param price: Name of coin\n        :type price: string\n        :param size: Amount of base currency to buy or sell\n        :type size: string\n        :param client_oid: (optional) Unique order_id  default flat_uuid()\n        :type client_oid: string\n        :param remark: (optional) remark for the order, max 100 utf8 characters\n        :type remark: string\n        :param stp: (optional) self trade protection CN, CO, CB or DC (default is None)\n        :type stp: string\n        :param time_in_force: (optional) GTC, GTT, IOC, or FOK (default is GTC)\n        :type time_in_force: string\n        :param stop: (optional) stop type loss or entry - requires stop_price\n        :type stop: string\n        :param stop_price: (optional) trigger price for stop order\n        :type stop_price: string\n        :param cancel_after: (optional) number of seconds to cancel the order if not filled\n            required time_in_force to be GTT\n        :type cancel_after: string\n        :param post_only: (optional) indicates that the order should only make liquidity. If any part of\n            the order results in taking liquidity, the order will be rejected and no part of it will execute.\n        :type post_only: bool\n        :param hidden: (optional) Orders not displayed in order book\n        :type hidden: bool\n        :param iceberg:  (optional) Only visible portion of the order is displayed in the order book\n        :type iceberg: bool\n        :param visible_size: (optional) The maximum visible size of an iceberg order\n        :type visible_size: bool\n\n        .. code:: python\n\n            order = client.create_limit_order('KCS-BTC', Client.SIDE_BUY, '0.01', '1000')\n\n        :returns: ApiResponse\n\n        .. code:: python\n\n            {\n                \"orderOid\": \"596186ad07015679730ffa02\"\n            }\n\n        :raises: KucoinResponseException, KucoinAPIException, LimitOrderException\n\n        \"\"\"\n\n        if stop and not stop_price:\n            raise LimitOrderException('Stop order needs stop_price')\n\n        if stop_price and not stop:\n            raise LimitOrderException('Stop order type required with stop_price')\n\n        if cancel_after and time_in_force != self.TIMEINFORCE_GOOD_TILL_TIME:\n            raise LimitOrderException('Cancel after only works with time_in_force = \"GTT\"')\n\n        if hidden and iceberg:\n            raise LimitOrderException('Order can be either \"hidden\" or \"iceberg\"')\n\n        if iceberg and not visible_size:\n            raise LimitOrderException('Iceberg order requires visible_size')\n\n        data = {\n            'symbol': symbol,\n            'side': side,\n            'type': self.ORDER_LIMIT,\n            'price': price,\n            'size': size\n        }\n\n        if client_oid:\n            data['clientOid'] = client_oid\n        else:\n            data['clientOid'] = flat_uuid()\n        if remark:\n            data['remark'] = remark\n        if stp:\n            data['stp'] = stp\n        if time_in_force:\n            data['timeInForce'] = time_in_force\n        if cancel_after:\n            data['cancelAfter'] = cancel_after\n        if post_only:\n            data['postOnly'] = post_only\n        if stop:\n            data['stop'] = stop\n            data['stopPrice'] = stop_price\n        if hidden:\n            data['hidden'] = hidden\n        if iceberg:\n            data['iceberg'] = iceberg\n            data['visible_size'] = visible_size\n\n        return self._post('orders', True, data=data)", "response": "Create a limit order for a given symbol side price size and client_oid"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncanceling all orders for the current user.", "response": "def cancel_all_orders(self, symbol=None):\n        \"\"\"Cancel all orders\n\n        https://docs.kucoin.com/#cancel-all-orders\n\n        .. code:: python\n\n            res = client.cancel_all_orders()\n\n        :returns: ApiResponse\n\n        .. code:: python\n\n            {\n                \"cancelledOrderIds\": [\n                    \"5bd6e9286d99522a52e458de\"\n                ]\n            }\n\n        :raises: KucoinResponseException, KucoinAPIException\n\n        \"\"\"\n        data = {}\n        if symbol is not None:\n            data['symbol'] = symbol\n        return self._delete('orders', True, data=data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_orders(self, symbol=None, status=None, side=None, order_type=None,\n                   start=None, end=None, page=None, limit=None):\n        \"\"\"Get list of orders\n\n        https://docs.kucoin.com/#list-orders\n\n        :param symbol: (optional) Name of symbol e.g. KCS-BTC\n        :type symbol: string\n        :param status: (optional) Specify status active or done (default done)\n        :type status: string\n        :param side: (optional) buy or sell\n        :type side: string\n        :param order_type: (optional) limit, market, limit_stop or market_stop\n        :type order_type: string\n        :param start: (optional) Start time as unix timestamp\n        :type start: string\n        :param end: (optional) End time as unix timestamp\n        :type end: string\n        :param page: (optional) Page to fetch\n        :type page: int\n        :param limit: (optional) Number of orders\n        :type limit: int\n\n        .. code:: python\n\n            orders = client.get_orders(symbol='KCS-BTC', status='active')\n\n        :returns: ApiResponse\n\n        .. code:: python\n\n            {\n                \"currentPage\": 1,\n                \"pageSize\": 1,\n                \"totalNum\": 153408,\n                \"totalPage\": 153408,\n                \"items\": [\n                    {\n                        \"id\": \"5c35c02703aa673ceec2a168\",\n                        \"symbol\": \"BTC-USDT\",\n                        \"opType\": \"DEAL\",\n                        \"type\": \"limit\",\n                        \"side\": \"buy\",\n                        \"price\": \"10\",\n                        \"size\": \"2\",\n                        \"funds\": \"0\",\n                        \"dealFunds\": \"0.166\",\n                        \"dealSize\": \"2\",\n                        \"fee\": \"0\",\n                        \"feeCurrency\": \"USDT\",\n                        \"stp\": \"\",\n                        \"stop\": \"\",\n                        \"stopTriggered\": false,\n                        \"stopPrice\": \"0\",\n                        \"timeInForce\": \"GTC\",\n                        \"postOnly\": false,\n                        \"hidden\": false,\n                        \"iceberge\": false,\n                        \"visibleSize\": \"0\",\n                        \"cancelAfter\": 0,\n                        \"channel\": \"IOS\",\n                        \"clientOid\": null,\n                        \"remark\": null,\n                        \"tags\": null,\n                        \"isActive\": false,\n                        \"cancelExist\": false,\n                        \"createdAt\": 1547026471000\n                    }\n                ]\n            }\n\n        :raises: KucoinResponseException, KucoinAPIException\n\n        \"\"\"\n\n        data = {}\n\n        if symbol:\n            data['symbol'] = symbol\n        if status:\n            data['status'] = status\n        if side:\n            data['side'] = side\n        if order_type:\n            data['type'] = order_type\n        if start:\n            data['startAt'] = start\n        if end:\n            data['endAt'] = end\n        if page:\n            data['page'] = page\n        if limit:\n            data['pageSize'] = limit\n\n        return self._get('orders', True, data=data)", "response": "Get list of orders in KCS - BTC"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting list of historical orders.", "response": "def get_historical_orders(self, symbol=None, side=None,\n                              start=None, end=None, page=None, limit=None):\n        \"\"\"List of KuCoin V1 historical orders.\n\n        https://docs.kucoin.com/#get-v1-historical-orders-list\n\n        :param symbol: (optional) Name of symbol e.g. KCS-BTC\n        :type symbol: string\n        :param side: (optional) buy or sell\n        :type side: string\n        :param start: (optional) Start time as unix timestamp\n        :type start: string\n        :param end: (optional) End time as unix timestamp\n        :type end: string\n        :param page: (optional) Page to fetch\n        :type page: int\n        :param limit: (optional) Number of orders\n        :type limit: int\n\n        .. code:: python\n\n            orders = client.get_historical_orders(symbol='KCS-BTC')\n\n        :returns: ApiResponse\n\n        .. code:: python\n\n            {\n                \"currentPage\": 1,\n                \"pageSize\": 50,\n                \"totalNum\": 1,\n                \"totalPage\": 1,\n                \"items\": [\n                    {\n                        \"symbol\": \"SNOV-ETH\",\n                        \"dealPrice\": \"0.0000246\",\n                        \"dealValue\": \"0.018942\",\n                        \"amount\": \"770\",\n                        \"fee\": \"0.00001137\",\n                        \"side\": \"sell\",\n                        \"createdAt\": 1540080199\n                    }\n                ]\n            }\n\n        :raises: KucoinResponseException, KucoinAPIException\n\n        \"\"\"\n\n        data = {}\n\n        if symbol:\n            data['symbol'] = symbol\n        if side:\n            data['side'] = side\n        if start:\n            data['startAt'] = start\n        if end:\n            data['endAt'] = end\n        if page:\n            data['page'] = page\n        if limit:\n            data['pageSize'] = limit\n\n        return self._get('hist-orders', True, data=data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a list of recent fills.", "response": "def get_fills(self, order_id=None, symbol=None, side=None, order_type=None,\n                  start=None, end=None, page=None, limit=None):\n        \"\"\"Get a list of recent fills.\n\n        https://docs.kucoin.com/#list-fills\n\n        :param order_id: (optional) generated order id\n        :type order_id: string\n        :param symbol: (optional) Name of symbol e.g. KCS-BTC\n        :type symbol: string\n        :param side: (optional) buy or sell\n        :type side: string\n        :param order_type: (optional) limit, market, limit_stop or market_stop\n        :type order_type: string\n        :param start: Start time as unix timestamp (optional)\n        :type start: string\n        :param end: End time as unix timestamp (optional)\n        :type end: string\n        :param page: optional - Page to fetch\n        :type page: int\n        :param limit: optional - Number of orders\n        :type limit: int\n        .. code:: python\n\n            fills = client.get_fills()\n\n        :returns: ApiResponse\n\n        .. code:: python\n\n            {\n                \"currentPage\":1,\n                \"pageSize\":1,\n                \"totalNum\":251915,\n                \"totalPage\":251915,\n                \"items\":[\n                    {\n                        \"symbol\":\"BTC-USDT\",\n                        \"tradeId\":\"5c35c02709e4f67d5266954e\",\n                        \"orderId\":\"5c35c02703aa673ceec2a168\",\n                        \"counterOrderId\":\"5c1ab46003aa676e487fa8e3\",\n                        \"side\":\"buy\",\n                        \"liquidity\":\"taker\",\n                        \"forceTaker\":true,\n                        \"price\":\"0.083\",\n                        \"size\":\"0.8424304\",\n                        \"funds\":\"0.0699217232\",\n                        \"fee\":\"0\",\n                        \"feeRate\":\"0\",\n                        \"feeCurrency\":\"USDT\",\n                        \"stop\":\"\",\n                        \"type\":\"limit\",\n                        \"createdAt\":1547026472000\n                    }\n                ]\n            }\n\n        :raises: KucoinResponseException, KucoinAPIException\n\n        \"\"\"\n\n        data = {}\n\n        if order_id:\n            data['orderId'] = order_id\n        if symbol:\n            data['symbol'] = symbol\n        if side:\n            data['side'] = side\n        if order_type:\n            data['type'] = order_type\n        if start:\n            data['startAt'] = start\n        if end:\n            data['endAt'] = end\n        if page:\n            data['page'] = page\n        if limit:\n            data['pageSize'] = limit\n\n        return self._get('fills', True, data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the ticker for a specific symbol", "response": "def get_ticker(self, symbol=None):\n        \"\"\"Get symbol tick\n\n        https://docs.kucoin.com/#get-ticker\n\n        :param symbol: (optional) Name of symbol e.g. KCS-BTC\n        :type symbol: string\n\n        .. code:: python\n\n            all_ticks = client.get_ticker()\n\n            ticker = client.get_ticker('ETH-BTC')\n\n        :returns: ApiResponse\n\n        .. code:: python\n\n            {\n                \"sequence\": \"1545825031840\",      # now sequence\n                \"price\": \"3494.367783\",           # last trade price\n                \"size\": \"0.05027185\",             # last trade size\n                \"bestBid\": \"3494.367783\",         # best bid price\n                \"bestBidSize\": \"2.60323254\",      # size at best bid price\n                \"bestAsk\": \"3499.12\",             # best ask price\n                \"bestAskSize\": \"0.01474011\"       # size at best ask price\n            }\n\n        :raises: KucoinResponseException, KucoinAPIException\n\n        \"\"\"\n        data = {}\n        tick_path = 'market/allTickers'\n        if symbol is not None:\n            tick_path = 'market/orderbook/level1'\n            data = {\n                'symbol': symbol\n            }\n        return self._get(tick_path, False, data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_fiat_prices(self, base=None, symbol=None):\n\n        data = {}\n\n        if base is not None:\n            data['base'] = base\n        if symbol is not None:\n            data['currencies'] = symbol\n\n        return self._get('prices', False, data=data)", "response": "Get fiat price for currency."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_24hr_stats(self, symbol):\n\n        data = {\n            'symbol': symbol\n        }\n\n        return self._get('market/stats', False, data=data)", "response": "Get 24hr stats for a symbol."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a list of bids and asks aggregated by price for a symbol. Returns up to 100 depth each side.", "response": "def get_order_book(self, symbol):\n        \"\"\"Get a list of bids and asks aggregated by price for a symbol.\n\n        Returns up to 100 depth each side. Fastest Order book API\n\n        https://docs.kucoin.com/#get-part-order-book-aggregated\n\n        :param symbol: Name of symbol e.g. KCS-BTC\n        :type symbol: string\n\n        .. code:: python\n\n            orders = client.get_order_book('KCS-BTC')\n\n        :returns: ApiResponse\n\n        .. code:: python\n\n            {\n                \"sequence\": \"3262786978\",\n                \"bids\": [\n                    [\"6500.12\", \"0.45054140\"],  # [price, size]\n                    [\"6500.11\", \"0.45054140\"]\n                ],\n                \"asks\": [\n                    [\"6500.16\", \"0.57753524\"],\n                    [\"6500.15\", \"0.57753524\"]\n                ]\n            }\n\n        :raises: KucoinResponseException, KucoinAPIException\n\n        \"\"\"\n\n        data = {\n            'symbol': symbol\n        }\n\n        return self._get('market/orderbook/level2_100', False, data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a list of all bids and asks aggregated by price for a symbol.", "response": "def get_full_order_book(self, symbol):\n        \"\"\"Get a list of all bids and asks aggregated by price for a symbol.\n\n        This call is generally used by professional traders because it uses more server resources and traffic,\n        and Kucoin has strict access frequency control.\n\n        https://docs.kucoin.com/#get-full-order-book-aggregated\n\n        :param symbol: Name of symbol e.g. KCS-BTC\n        :type symbol: string\n\n        .. code:: python\n\n            orders = client.get_order_book('KCS-BTC')\n\n        :returns: ApiResponse\n\n        .. code:: python\n\n            {\n                \"sequence\": \"3262786978\",\n                \"bids\": [\n                    [\"6500.12\", \"0.45054140\"],  # [price size]\n                    [\"6500.11\", \"0.45054140\"]\n                ],\n                \"asks\": [\n                    [\"6500.16\", \"0.57753524\"],\n                    [\"6500.15\", \"0.57753524\"]\n                ]\n            }\n\n        :raises: KucoinResponseException, KucoinAPIException\n\n        \"\"\"\n\n        data = {\n            'symbol': symbol\n        }\n\n        return self._get('market/orderbook/level2', False, data=data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a list of all bids and asks non - aggregated for a symbol.", "response": "def get_full_order_book_level3(self, symbol):\n        \"\"\"Get a list of all bids and asks non-aggregated for a symbol.\n\n        This call is generally used by professional traders because it uses more server resources and traffic,\n        and Kucoin has strict access frequency control.\n\n        https://docs.kucoin.com/#get-full-order-book-atomic\n\n        :param symbol: Name of symbol e.g. KCS-BTC\n        :type symbol: string\n\n        .. code:: python\n\n            orders = client.get_order_book('KCS-BTC')\n\n        :returns: ApiResponse\n\n        .. code:: python\n\n            {\n                \"sequence\": \"1545896707028\",\n                \"bids\": [\n                    [\n                        \"5c2477e503aa671a745c4057\",   # orderId\n                        \"6\",                          # price\n                        \"0.999\"                       # size\n                    ],\n                    [\n                        \"5c2477e103aa671a745c4054\",\n                        \"5\",\n                        \"0.999\"\n                    ]\n                ],\n                \"asks\": [\n                    [\n                        \"5c24736703aa671a745c401e\",\n                        \"200\",\n                        \"1\"\n                    ],\n                    [\n                        \"5c2475c903aa671a745c4033\",\n                        \"201\",\n                        \"1\"\n                    ]\n                ]\n            }\n\n        :raises: KucoinResponseException, KucoinAPIException\n\n        \"\"\"\n\n        data = {\n            'symbol': symbol\n        }\n\n        return self._get('market/orderbook/level3', False, data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_trade_histories(self, symbol):\n\n        data = {\n            'symbol': symbol\n        }\n\n        return self._get('market/histories', False, data=data)", "response": "Get latest trades for a symbol."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_kline_data(self, symbol, kline_type='5min', start=None, end=None):\n\n        data = {\n            'symbol': symbol\n        }\n\n        if kline_type is not None:\n            data['type'] = kline_type\n        if start is not None:\n            data['startAt'] = start\n        else:\n            data['startAt'] = calendar.timegm(datetime.utcnow().date().timetuple())\n        if end is not None:\n            data['endAt'] = end\n        else:\n            data['endAt'] = int(time.time())\n\n        return self._get('market/candles', False, data=data)", "response": "Get kline data for a given symbol."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_ws_endpoint(self, private=False):\n\n        path = 'bullet-public'\n        signed = private\n        if private:\n            path = 'bullet-private'\n\n        return self._post(path, signed)", "response": "Get websocket channel details."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_real_stored_key(self, session_key):\n        prefix = settings.SESSION_REDIS_PREFIX\n        if not prefix:\n            return session_key\n        return ':'.join([prefix, session_key])", "response": "Return the real key name in redis storage\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding all edges in a stream of tiles.", "response": "def edges(inputtiles, parsenames):\n    \"\"\"\n    For a stream of [<x>, <y>, <z>] tiles, return only those tiles that are on the edge.\n    \"\"\"\n    try:\n        inputtiles = click.open_file(inputtiles).readlines()\n    except IOError:\n        inputtiles = [inputtiles]\n\n    # parse the input stream into an array\n    tiles = edge_finder.findedges(inputtiles, parsenames)\n\n    for t in tiles:\n        click.echo(t.tolist())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef union(inputtiles, parsenames):\n    try:\n        inputtiles = click.open_file(inputtiles).readlines()\n    except IOError:\n        inputtiles = [inputtiles]\n    unioned = uniontiles.union(inputtiles, parsenames)\n    for u in unioned:\n        click.echo(json.dumps(u))", "response": "Returns the unioned shape of a steeam of [ x y z ) tiles in GeoJSON."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nburns a stream of GeoJSONs into a output stream of the tiles they intersect for a given zoom.", "response": "def burn(features, sequence, zoom):\n    \"\"\"\n    Burn a stream of GeoJSONs into a output stream of the tiles they intersect for a given zoom.\n    \"\"\"\n    features = [f for f in super_utils.filter_polygons(features)]\n\n    tiles = burntiles.burn(features, zoom)\n    for t in tiles:\n        click.echo(t.tolist())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save(self, *args, **kwargs):\n\n        max_retries = getattr(\n            settings,\n            'LOCALIZED_FIELDS_MAX_RETRIES',\n            100\n        )\n\n        if not hasattr(self, 'retries'):\n            self.retries = 0\n\n        with transaction.atomic():\n            try:\n                return super().save(*args, **kwargs)\n            except IntegrityError as ex:\n                # this is as retarded as it looks, there's no\n                # way we can put the retry logic inside the slug\n                # field class... we can also not only catch exceptions\n                # that apply to slug fields... so yea.. this is as\n                # retarded as it gets... i am sorry :(\n                if 'slug' not in str(ex):\n                    raise ex\n\n                if self.retries >= max_retries:\n                    raise ex\n\n        self.retries += 1\n        return self.save()", "response": "Saves this model instance to the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_python(self, value: Union[Dict[str, int], int, None]) -> LocalizedIntegerValue:\n\n        db_value = super().to_python(value)\n        return self._convert_localized_value(db_value)", "response": "Converts the value from a database value into a Python value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_prep_value(self, value: LocalizedIntegerValue) -> dict:\n\n        # apply default values\n        default_values = LocalizedIntegerValue(self.default)\n        if isinstance(value, LocalizedIntegerValue):\n            for lang_code, _ in settings.LANGUAGES:\n                local_value = value.get(lang_code)\n                if local_value is None:\n                    value.set(lang_code, default_values.get(lang_code, None))\n\n        prepped_value = super().get_prep_value(value)\n        if prepped_value is None:\n            return None\n\n        # make sure all values are proper integers\n        for lang_code, _ in settings.LANGUAGES:\n            local_value = prepped_value[lang_code]\n            try:\n                if local_value is not None:\n                    int(local_value)\n            except (TypeError, ValueError):\n                raise IntegrityError('non-integer value in column \"%s.%s\" violates '\n                                     'integer constraint' % (self.name, lang_code))\n\n            # convert to a string before saving because the underlying\n            # type is hstore, which only accept strings\n            prepped_value[lang_code] = str(local_value) if local_value is not None else None\n\n        return prepped_value", "response": "Gets the value in a format to store into the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the form field associated with this field.", "response": "def formfield(self, **kwargs):\n        \"\"\"Gets the form field associated with this field.\"\"\"\n        defaults = {\n            'form_class': LocalizedIntegerFieldForm\n        }\n\n        defaults.update(kwargs)\n        return super().formfield(**defaults)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert from the given value to the equivalent value in the given value.", "response": "def _convert_localized_value(value: LocalizedValue) -> LocalizedIntegerValue:\n        \"\"\"Converts from :see:LocalizedValue to :see:LocalizedIntegerValue.\"\"\"\n\n        integer_values = {}\n        for lang_code, _ in settings.LANGUAGES:\n            local_value = value.get(lang_code, None)\n            if local_value is None or local_value.strip() == '':\n                local_value = None\n\n            try:\n                integer_values[lang_code] = int(local_value)\n            except (ValueError, TypeError):\n                integer_values[lang_code] = None\n\n        return LocalizedIntegerValue(integer_values)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncompresses the values from individual fields into a single localized value instance.", "response": "def compress(self, value: List[str]) -> value_class:\n        \"\"\"Compresses the values from individual fields\n        into a single :see:LocalizedValue instance.\n\n        Arguments:\n            value:\n                The values from all the widgets.\n\n        Returns:\n            A :see:LocalizedValue containing all\n            the value in several languages.\n        \"\"\"\n\n        localized_value = self.value_class()\n\n        for (lang_code, _), value in zip(settings.LANGUAGES, value):\n            localized_value.set(lang_code, value)\n\n        return localized_value"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncleans the value of a single value.", "response": "def clean(self, value, initial=None):\n        \"\"\"\n        Most part of this method is a copy of\n        django.forms.MultiValueField.clean, with the exception of initial\n        value handling (this need for correct processing FileField's).\n        All original comments saved.\n        \"\"\"\n        if initial is None:\n            initial = [None for x in range(0, len(value))]\n        else:\n            if not isinstance(initial, list):\n                initial = self.widget.decompress(initial)\n\n        clean_data = []\n        errors = []\n        if not value or isinstance(value, (list, tuple)):\n            if (not value or not [v for v in value if\n                                  v not in self.empty_values]) \\\n                    and (not initial or not [v for v in initial if\n                                             v not in self.empty_values]):\n                if self.required:\n                    raise ValidationError(self.error_messages['required'],\n                                          code='required')\n        else:\n            raise ValidationError(self.error_messages['invalid'],\n                                  code='invalid')\n        for i, field in enumerate(self.fields):\n            try:\n                field_value = value[i]\n            except IndexError:\n                field_value = None\n            try:\n                field_initial = initial[i]\n            except IndexError:\n                field_initial = None\n\n            if field_value in self.empty_values and \\\n                    field_initial in self.empty_values:\n                if self.require_all_fields:\n                    # Raise a 'required' error if the MultiValueField is\n                    # required and any field is empty.\n                    if self.required:\n                        raise ValidationError(self.error_messages['required'],\n                                              code='required')\n                elif field.required:\n                    # Otherwise, add an 'incomplete' error to the list of\n                    # collected errors and skip field cleaning, if a required\n                    # field is empty.\n                    if field.error_messages['incomplete'] not in errors:\n                        errors.append(field.error_messages['incomplete'])\n                    continue\n            try:\n                clean_data.append(field.clean(field_value, field_initial))\n            except ValidationError as e:\n                # Collect all validation errors in a single list, which we'll\n                # raise at the end of clean(), rather than raising a single\n                # exception for the first error we encounter. Skip duplicates.\n                errors.extend(m for m in e.error_list if m not in errors)\n        if errors:\n            raise ValidationError(errors)\n\n        out = self.compress(clean_data)\n        self.validate(out)\n        self.run_validators(out)\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef formfield(self, **kwargs):\n\n        defaults = {\n            'form_class': LocalizedTextFieldForm\n        }\n\n        defaults.update(kwargs)\n        return super().formfield(**defaults)", "response": "Gets the form field associated with this field."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef formfield(self, **kwargs):\n\n        defaults = {\n            'form_class': forms.CharField,\n            'required': False\n        }\n\n        defaults.update(kwargs)\n\n        form_field = super().formfield(**defaults)\n        form_field.widget = forms.HiddenInput()\n\n        return form_field", "response": "Gets the form field associated with this field."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun just before the model is saved allows us to built the slug.", "response": "def pre_save(self, instance, add: bool):\n        \"\"\"Ran just before the model is saved, allows us to built\n        the slug.\n\n        Arguments:\n            instance:\n                The model that is being saved.\n\n            add:\n                Indicates whether this is a new entry\n                to the database or an update.\n        \"\"\"\n\n        slugs = LocalizedValue()\n\n        for lang_code, value in self._get_populate_values(instance):\n            if not value:\n                continue\n\n            if self.include_time:\n                value += '-%s' % datetime.now().microsecond\n\n            def is_unique(slug: str, language: str) -> bool:\n                \"\"\"Gets whether the specified slug is unique.\"\"\"\n\n                unique_filter = {\n                    '%s__%s' % (self.name, language): slug\n                }\n\n                return not type(instance).objects.filter(**unique_filter).exists()\n\n            slug = self._make_unique_slug(\n                slugify(value, allow_unicode=True),\n                lang_code,\n                is_unique\n            )\n\n            slugs.set(lang_code, slug)\n\n        setattr(instance, self.name, slugs)\n        return slugs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _make_unique_slug(slug: str, language: str, is_unique: Callable[[str], bool]) -> str:\n\n        index = 1\n        unique_slug = slug\n\n        while not is_unique(unique_slug, language):\n            unique_slug = '%s-%d' % (slug, index)\n            index += 1\n\n        return unique_slug", "response": "Makes a unique slug for the specified language."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_populate_values(self, instance) -> Tuple[str, str]:\n\n        return [\n            (\n                lang_code,\n                self._get_populate_from_value(\n                    instance,\n                    self.populate_from,\n                    lang_code\n                ),\n            )\n            for lang_code, _ in settings.LANGUAGES\n        ]", "response": "Gets all values for each language from the object s populate_from field."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_populate_from_value(instance, field_name: Union[str, Tuple[str]], language: str):\n\n        if callable(field_name):\n            return field_name(instance)\n\n        def get_field_value(name):\n            value = resolve_object_property(instance, name)\n            with translation.override(language):\n                return str(value)\n\n        if isinstance(field_name, tuple) or isinstance(field_name, list):\n            value = '-'.join([\n                value\n                for value in [get_field_value(name) for name in field_name]\n                if value\n            ])\n            return value\n\n        return get_field_value(field_name)", "response": "Gets the value to populate a slug from in the specified language."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef deconstruct(self):\n\n        name, path, args, kwargs = super(\n            LocalizedUniqueSlugField, self).deconstruct()\n\n        kwargs['populate_from'] = self.populate_from\n        kwargs['include_time'] = self.include_time\n        return name, path, args, kwargs", "response": "Deconstructs the field into something the database\n        can store."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns just before the model is saved allows us to built the slug.", "response": "def pre_save(self, instance, add: bool):\n        \"\"\"Ran just before the model is saved, allows us to built\n        the slug.\n\n        Arguments:\n            instance:\n                The model that is being saved.\n\n            add:\n                Indicates whether this is a new entry\n                to the database or an update.\n\n        Returns:\n            The localized slug that was generated.\n        \"\"\"\n\n        if not isinstance(instance, AtomicSlugRetryMixin):\n            raise ImproperlyConfigured((\n                'Model \\'%s\\' does not inherit from AtomicSlugRetryMixin. '\n                'Without this, the LocalizedUniqueSlugField will not work.'\n            ) % type(instance).__name__)\n\n        slugs = LocalizedValue()\n\n        for lang_code, value in self._get_populate_values(instance):\n            if not value:\n                continue\n\n            slug = slugify(value, allow_unicode=True)\n\n            # verify whether it's needed to re-generate a slug,\n            # if not, re-use the same slug\n            if instance.pk is not None:\n                current_slug = getattr(instance, self.name).get(lang_code)\n                if current_slug is not None:\n                    stripped_slug = current_slug[0:current_slug.rfind('-')]\n                    if slug == stripped_slug:\n                        slugs.set(lang_code, current_slug)\n                        continue\n\n            if self.include_time:\n                slug += '-%d' % datetime.now().microsecond\n\n            retries = getattr(instance, 'retries', 0)\n            if retries > 0:\n                # do not add another - if we already added time\n                if not self.include_time:\n                    slug += '-'\n                slug += '%d' % retries\n\n            slugs.set(lang_code, slug)\n\n        setattr(instance, self.name, slugs)\n        return slugs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding this field to the specifed model.", "response": "def contribute_to_class(self, model, name, **kwargs):\n        \"\"\"Adds this field to the specifed model.\n\n        Arguments:\n            cls:\n                The model to add the field to.\n\n            name:\n                The name of the field to add.\n        \"\"\"\n        super(LocalizedField, self).contribute_to_class(model, name, **kwargs)\n        setattr(model, self.name, self.descriptor_class(self))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nturn the specified database value into its Python equivalent.", "response": "def from_db_value(cls, value, *_) -> Optional[LocalizedValue]:\n        \"\"\"Turns the specified database value into its Python\n        equivalent.\n\n        Arguments:\n            value:\n                The value that is stored in the database and\n                needs to be converted to its Python equivalent.\n\n        Returns:\n            A :see:LocalizedValue instance containing the\n            data extracted from the database.\n        \"\"\"\n\n        if not value:\n            if getattr(settings, 'LOCALIZED_FIELDS_EXPERIMENTAL', False):\n                return None\n            else:\n                return cls.attr_class()\n\n        # we can get a list if an aggregation expression was used..\n        # if we the expression was flattened when only one key was selected\n        # then we don't wrap each value in a localized value, otherwise we do\n        if isinstance(value, list):\n            result = []\n            for inner_val in value:\n                if isinstance(inner_val, dict):\n                    if inner_val is None:\n                        result.append(None)\n                    else:\n                        result.append(cls.attr_class(inner_val))\n                else:\n                    result.append(inner_val)\n\n            return result\n\n        # this is for when you select an individual key, it will be string,\n        # not a dictionary, we'll give it to you as a flat value, not as a\n        # localized value instance\n        if not isinstance(value, dict):\n            return value\n\n        return cls.attr_class(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nturns the specified database value into its Python equivalent.", "response": "def to_python(self, value: Union[dict, str, None]) -> LocalizedValue:\n        \"\"\"Turns the specified database value into its Python\n        equivalent.\n\n        Arguments:\n            value:\n                The value that is stored in the database and\n                needs to be converted to its Python equivalent.\n\n        Returns:\n            A :see:LocalizedValue instance containing the\n            data extracted from the database.\n        \"\"\"\n\n        # first let the base class  handle the deserialization, this is in case we\n        # get specified a json string representing a dict\n        try:\n            deserialized_value = super(LocalizedField, self).to_python(value)\n        except json.JSONDecodeError:\n            deserialized_value = value\n\n        if not deserialized_value:\n            return self.attr_class()\n\n        return self.attr_class(deserialized_value)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nturning the specified value into something the database can store.", "response": "def get_prep_value(self, value: LocalizedValue) -> dict:\n        \"\"\"Turns the specified value into something the database\n        can store.\n\n        If an illegal value (non-LocalizedValue instance) is\n        specified, we'll treat it as an empty :see:LocalizedValue\n        instance, on which the validation will fail.\n\n        Dictonaries are converted into :see:LocalizedValue instances.\n\n        Arguments:\n            value:\n                The :see:LocalizedValue instance to serialize\n                into a data type that the database can understand.\n\n        Returns:\n            A dictionary containing a key for every language,\n            extracted from the specified value.\n        \"\"\"\n\n        if isinstance(value, dict):\n            value = LocalizedValue(value)\n\n        # default to None if this is an unknown type\n        if not isinstance(value, LocalizedValue) and value:\n            value = None\n\n        if value:\n            cleaned_value = self.clean(value)\n            self.validate(cleaned_value)\n        else:\n            cleaned_value = value\n\n        return super(LocalizedField, self).get_prep_value(\n            cleaned_value.__dict__ if cleaned_value else None\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clean(self, value, *_):\n\n        if not value or not isinstance(value, LocalizedValue):\n            return None\n\n        # are any of the language fiels None/empty?\n        is_all_null = True\n        for lang_code, _ in settings.LANGUAGES:\n            if value.get(lang_code) is not None:\n                is_all_null = False\n                break\n\n        # all fields have been left empty and we support\n        # null values, let's return null to represent that\n        if is_all_null and self.null:\n            return None\n\n        return value", "response": "Cleans the specified value into something we can store in the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates that the values have been filled in for all required languages and returns the value.", "response": "def validate(self, value: LocalizedValue, *_):\n        \"\"\"Validates that the values has been filled in for all required\n        languages\n\n        Exceptions are raises in order to notify the user\n        of invalid values.\n\n        Arguments:\n            value:\n                The value to validate.\n        \"\"\"\n\n        if self.null:\n            return\n\n        for lang in self.required:\n            lang_val = getattr(value, settings.LANGUAGE_CODE)\n\n            if lang_val is None:\n                raise IntegrityError('null value in column \"%s.%s\" violates '\n                                     'not-null constraint' % (self.name, lang))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef formfield(self, **kwargs):\n\n        defaults = dict(\n            form_class=LocalizedFieldForm,\n            required=False if self.blank else self.required\n        )\n        defaults.update(kwargs)\n        return super().formfield(**defaults)", "response": "Gets the form field associated with this field."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(self, language: str=None, default: str=None) -> str:\n\n        language = language or settings.LANGUAGE_CODE\n        value = super().get(language, default)\n        return value if value is not None else default", "response": "Gets the underlying value in the specified language."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the value in the specified language.", "response": "def set(self, language: str, value: str):\n        \"\"\"Sets the value in the specified language.\n\n        Arguments:\n            language:\n                The language to set the value in.\n\n            value:\n                The value to set.\n        \"\"\"\n\n        self[language] = value\n        self.__dict__.update(self)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef deconstruct(self) -> dict:\n\n        path = 'localized_fields.value.%s' % self.__class__.__name__\n        return path, [self.__dict__], {}", "response": "Deconstructs this value into a primitive type."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _interpret_value(self, value):\n\n        for lang_code, _ in settings.LANGUAGES:\n            self.set(lang_code, self.default_value)\n\n        if isinstance(value, str):\n            self.set(settings.LANGUAGE_CODE, value)\n\n        elif isinstance(value, dict):\n            for lang_code, _ in settings.LANGUAGES:\n                lang_value = value.get(lang_code, self.default_value)\n                self.set(lang_code, lang_value)\n\n        elif isinstance(value, collections.Iterable):\n            for val in value:\n                self._interpret_value(val)", "response": "Interprets a value passed in the constructor as\n            a : see : LocalizedValue."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef translate(self) -> Optional[str]:\n\n        fallbacks = getattr(settings, 'LOCALIZED_FIELDS_FALLBACKS', {})\n\n        language = translation.get_language() or settings.LANGUAGE_CODE\n        languages = fallbacks.get(language, [settings.LANGUAGE_CODE])[:]\n        languages.insert(0, language)\n\n        for lang_code in languages:\n            value = self.get(lang_code)\n            if value:\n                return value or None\n\n        return None", "response": "Gets the value in the current language or falls\n        back to the next language if there s no value in the current language or falls\n        back to the next language if there s no value in the current language. Returns None if there s no value in the current language or falls\n        if there s no value in the current language."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the value in the current language or in the configured fallbck language.", "response": "def translate(self):\n        \"\"\"Gets the value in the current language, or\n        in the configured fallbck language.\"\"\"\n\n        value = super().translate()\n        if value is None or (isinstance(value, str) and value.strip() == ''):\n            return None\n\n        return int(value)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_prep_value(self, value):\n\n        if isinstance(value, LocalizedValue):\n            prep_value = LocalizedValue()\n            for k, v in value.__dict__.items():\n                if v is None:\n                    prep_value.set(k, '')\n                else:\n                    # Need to convert File objects provided via a form to\n                    # unicode for database insertion\n                    prep_value.set(k, six.text_type(v))\n            return super().get_prep_value(prep_value)\n        return super().get_prep_value(value)", "response": "Returns field s value prepared for saving into a database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pre_save(self, model_instance, add):\n        value = super().pre_save(model_instance, add)\n        if isinstance(value, LocalizedValue):\n            for file in value.__dict__.values():\n                if file and not file._committed:\n                    file.save(file.name, file, save=False)\n        return value", "response": "Returns field s value just before saving."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resolve_object_property(obj, path: str):\n\n    value = obj\n    for path_part in path.split('.'):\n        value = getattr(value, path_part)\n\n    return value", "response": "Resolves the value of a property on an object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decompress(self, value: LocalizedValue) -> List[str]:\n\n        result = []\n        for lang_code, _ in settings.LANGUAGES:\n            if value:\n                result.append(value.get(lang_code))\n            else:\n                result.append(None)\n\n        return result", "response": "Decompresses the specified value so\n            it can be spread over the internal widgets."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef formfield(self, **kwargs):\n        defaults = {\n            'form_class': LocalizedCharFieldForm\n        }\n\n        defaults.update(kwargs)\n        return super().formfield(**defaults)", "response": "Gets the form field associated with this field."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun just before the model is saved allows us to built the entry the slug.", "response": "def pre_save(self, instance, add: bool):\n        \"\"\"Ran just before the model is saved, allows us to built\n        the slug.\n\n        Arguments:\n            instance:\n                The model that is being saved.\n\n            add:\n                Indicates whether this is a new entry\n                to the database or an update.\n        \"\"\"\n\n        localized_value = getattr(instance, self.attname)\n        if not localized_value:\n            return None\n\n        for lang_code, _ in settings.LANGUAGES:\n            value = localized_value.get(lang_code)\n            if not value:\n                continue\n\n            localized_value.set(\n                lang_code,\n                bleach.clean(value, **get_bleach_default_options())\n            )\n\n        return localized_value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef confirm(what, where):\n    '''\n    Method to show a CLI based confirmation message, waiting for a yes/no answer.\n    \"what\" and \"where\" are used to better define the message.\n    '''\n    ans = input('Are you sure you want to delete the '\n                '{} {} from the service?\\n[yN]> '.format(what, where))\n    if 'y' in ans:\n        ans = loop_input('Are you really sure? there\\'s no coming back!\\n'\n                    '[type \\'burn!\\' to proceed]> ')\n        if 'burn!' != ans:\n            return False\n    else:\n        return False\n    return True", "response": "Method to show a CLI based confirmation message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef register_target(repo_cmd, repo_service):\n    def decorate(klass):\n        log.debug('Loading service module class: {}'.format(klass.__name__) )\n        klass.command = repo_cmd\n        klass.name = repo_service\n        RepositoryService.service_map[repo_service] = klass\n        RepositoryService.command_map[repo_cmd] = repo_service\n        return klass\n    return decorate", "response": "Decorator to register a class with an repo_cmd"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nformatting the repository s URL", "response": "def format_path(self, repository, namespace=None, rw=False):\n        '''format the repository's URL\n\n        :param repository: name of the repository\n        :param namespace: namespace of the repository\n        :param rw: return a git+ssh URL if true, an https URL otherwise\n        :return: the full URI of the repository ready to use as remote\n\n        if namespace is not given, repository is expected to be of format\n        `<namespace>/<repository>`.\n        '''\n        repo = repository\n        if namespace:\n            repo = '{}/{}'.format(namespace, repository)\n\n        if not rw and repo.count('/') >= self._min_nested_namespaces:\n            return '{}/{}'.format(self.url_ro, repo)\n        elif rw and repo.count('/') >= self._min_nested_namespaces:\n            if self.url_rw.startswith('ssh://'):\n                return '{}/{}'.format(self.url_rw, repo)\n            else:\n                return '{}:{}'.format(self.url_rw, repo)\n        else:\n            raise ArgumentError(\"Cannot tell how to handle this url: `{}/{}`!\".format(namespace, repo))"}
