{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef zinnia_loop_template(context, default_template):\n    matching, context_object = get_context_first_matching_object(\n        context,\n        ['category', 'tag', 'author', 'pattern',\n         'year', 'month', 'week', 'day'])\n    context_positions = get_context_loop_positions(context)\n\n    templates = loop_template_list(\n        context_positions, context_object, matching,\n        default_template, ENTRY_LOOP_TEMPLATES)\n\n    return select_template(templates)", "response": "Return a selected template from his position within a loop\n    and the filtering context."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_gravatar(email, size=80, rating='g', default=None,\n                 protocol=PROTOCOL):\n    \"\"\"\n    Return url for a Gravatar.\n    \"\"\"\n    gravatar_protocols = {'http': 'http://www',\n                          'https': 'https://secure'}\n    url = '%s.gravatar.com/avatar/%s' % (\n        gravatar_protocols[protocol],\n        md5(email.strip().lower().encode('utf-8')).hexdigest())\n    options = {'s': size, 'r': rating}\n    if default:\n        options['d'] = default\n\n    url = '%s?%s' % (url, urlencode(options))\n    return url.replace('&', '&amp;')", "response": "Return url for a Gravatar."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a cloud of published tags.", "response": "def get_tag_cloud(context, steps=6, min_count=None,\n                  template='zinnia/tags/tag_cloud.html'):\n    \"\"\"\n    Return a cloud of published tags.\n    \"\"\"\n    tags = Tag.objects.usage_for_queryset(\n        Entry.published.all(), counts=True,\n        min_count=min_count)\n    return {'template': template,\n            'tags': calculate_cloud(tags, steps),\n            'context_tag': context.get('tag')}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef widont(value, autoescape=None):\n    esc = autoescape and conditional_escape or (lambda x: x)\n\n    def replace(matchobj):\n        return '&nbsp;%s' % matchobj.group(1)\n\n    value = END_PUNCTUATION_WIDONT_REGEXP.sub(replace, esc(smart_text(value)))\n    value = WIDONT_REGEXP.sub(replace, value)\n    value = DOUBLE_SPACE_PUNCTUATION_WIDONT_REGEXP.sub(replace, value)\n\n    return mark_safe(value)", "response": "Returns a string with HTML non - breaking space between the final\n    two words of the final\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the Python week number of a date.", "response": "def week_number(date):\n    \"\"\"\n    Return the Python week number of a date.\n    The django \\|date:\"W\" returns incompatible value\n    with the view implementation.\n    \"\"\"\n    week_number = date.strftime('%W')\n    if int(week_number) < 10:\n        week_number = week_number[-1]\n    return week_number"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the admin URL name for the comment app used.", "response": "def comment_admin_urlname(action):\n    \"\"\"\n    Return the admin URLs for the comment app used.\n    \"\"\"\n    comment = get_comment_model()\n    return 'admin:%s_%s_%s' % (\n        comment._meta.app_label, comment._meta.model_name,\n        action)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef user_admin_urlname(action):\n    user = get_user_model()\n    return 'admin:%s_%s_%s' % (\n        user._meta.app_label, user._meta.model_name,\n        action)", "response": "Return the admin URL name for the user app used."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns statistics on the content of Zinnia.", "response": "def zinnia_statistics(template='zinnia/tags/statistics.html'):\n    \"\"\"\n    Return statistics on the content of Zinnia.\n    \"\"\"\n    content_type = ContentType.objects.get_for_model(Entry)\n    discussions = get_comment_model().objects.filter(\n        content_type=content_type)\n\n    entries = Entry.published\n    categories = Category.objects\n    tags = tags_published()\n    authors = Author.published\n    replies = discussions.filter(\n        flags=None, is_public=True)\n    pingbacks = discussions.filter(\n        flags__flag=PINGBACK, is_public=True)\n    trackbacks = discussions.filter(\n        flags__flag=TRACKBACK, is_public=True)\n    rejects = discussions.filter(is_public=False)\n\n    entries_count = entries.count()\n    replies_count = replies.count()\n    pingbacks_count = pingbacks.count()\n    trackbacks_count = trackbacks.count()\n\n    if entries_count:\n        first_entry = entries.order_by('publication_date')[0]\n        last_entry = entries.latest()\n        months_count = (last_entry.publication_date -\n                        first_entry.publication_date).days / 31.0\n        entries_per_month = entries_count / (months_count or 1.0)\n\n        comments_per_entry = float(replies_count) / entries_count\n        linkbacks_per_entry = float(pingbacks_count + trackbacks_count) / \\\n            entries_count\n\n        total_words_entry = 0\n        for e in entries.all():\n            total_words_entry += e.word_count\n        words_per_entry = float(total_words_entry) / entries_count\n\n        words_per_comment = 0.0\n        if replies_count:\n            total_words_comment = 0\n            for c in replies.all():\n                total_words_comment += len(c.comment.split())\n            words_per_comment = float(total_words_comment) / replies_count\n    else:\n        words_per_entry = words_per_comment = entries_per_month = \\\n            comments_per_entry = linkbacks_per_entry = 0.0\n\n    return {'template': template,\n            'entries': entries_count,\n            'categories': categories.count(),\n            'tags': tags.count(),\n            'authors': authors.count(),\n            'comments': replies_count,\n            'pingbacks': pingbacks_count,\n            'trackbacks': trackbacks_count,\n            'rejects': rejects.count(),\n            'words_per_entry': words_per_entry,\n            'words_per_comment': words_per_comment,\n            'entries_per_month': entries_per_month,\n            'comments_per_entry': comments_per_entry,\n            'linkbacks_per_entry': linkbacks_per_entry}"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nimplements cache on get_object method to avoid repetitive calls in POST.", "response": "def get_object(self, queryset=None):\n        \"\"\"\n        Implement cache on ``get_object`` method to\n        avoid repetitive calls, in POST.\n        \"\"\"\n        if self._cached_object is None:\n            self._cached_object = super(EntryCacheMixin, self).get_object(\n                queryset)\n        return self._cached_object"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the password view.", "response": "def password(self):\n        \"\"\"\n        Return the password view.\n        \"\"\"\n        return self.response_class(request=self.request,\n                                   template='zinnia/password.html',\n                                   context={'error': self.error})"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndoes the login and password protection.", "response": "def get(self, request, *args, **kwargs):\n        \"\"\"\n        Do the login and password protection.\n        \"\"\"\n        response = super(EntryProtectionMixin, self).get(\n            request, *args, **kwargs)\n        if self.object.login_required and not request.user.is_authenticated:\n            return self.login()\n        if (self.object.password and self.object.password !=\n                self.request.session.get(self.session_key % self.object.pk)):\n            return self.password()\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndoing the login and password protection.", "response": "def post(self, request, *args, **kwargs):\n        \"\"\"\n        Do the login and password protection.\n        \"\"\"\n        self.object = self.get_object()\n        self.login()\n        if self.object.password:\n            entry_password = self.request.POST.get('entry_password')\n            if entry_password:\n                if entry_password == self.object.password:\n                    self.request.session[self.session_key %\n                                         self.object.pk] = self.object.password\n                    return self.get(request, *args, **kwargs)\n                else:\n                    self.error = True\n            return self.password()\n        return self.get(request, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npinging directories when an entry is saved.", "response": "def ping_directories_handler(sender, **kwargs):\n    \"\"\"\n    Ping directories when an entry is saved.\n    \"\"\"\n    entry = kwargs['instance']\n\n    if entry.is_visible and settings.SAVE_PING_DIRECTORIES:\n        for directory in settings.PING_DIRECTORIES:\n            DirectoryPinger(directory, [entry])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ping_external_urls_handler(sender, **kwargs):\n    entry = kwargs['instance']\n\n    if entry.is_visible and settings.SAVE_PING_EXTERNAL_URLS:\n        ExternalUrlsPinger(entry)", "response": "Ping external URLs when an entry is saved."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the count of each type of discussion on an entry.", "response": "def count_discussions_handler(sender, **kwargs):\n    \"\"\"\n    Update the count of each type of discussion on an entry.\n    \"\"\"\n    if kwargs.get('instance') and kwargs.get('created'):\n        # The signal is emitted by the comment creation,\n        # so we do nothing, comment_was_posted is used instead.\n        return\n\n    comment = 'comment' in kwargs and kwargs['comment'] or kwargs['instance']\n    entry = comment.content_object\n\n    if isinstance(entry, Entry):\n        entry.comment_count = entry.comments.count()\n        entry.pingback_count = entry.pingbacks.count()\n        entry.trackback_count = entry.trackbacks.count()\n        entry.save(update_fields=[\n            'comment_count', 'pingback_count', 'trackback_count'])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate Entry. comment_count when a public comment was posted.", "response": "def count_comments_handler(sender, **kwargs):\n    \"\"\"\n    Update Entry.comment_count when a public comment was posted.\n    \"\"\"\n    comment = kwargs['comment']\n    if comment.is_public:\n        entry = comment.content_object\n        if isinstance(entry, Entry):\n            entry.comment_count = F('comment_count') + 1\n            entry.save(update_fields=['comment_count'])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef count_pingbacks_handler(sender, **kwargs):\n    entry = kwargs['entry']\n    entry.pingback_count = F('pingback_count') + 1\n    entry.save(update_fields=['pingback_count'])", "response": "Update Entry. pingback_count when a pingback was posted."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate Entry. trackback_count when a trackback was posted.", "response": "def count_trackbacks_handler(sender, **kwargs):\n    \"\"\"\n    Update Entry.trackback_count when a trackback was posted.\n    \"\"\"\n    entry = kwargs['entry']\n    entry.trackback_count = F('trackback_count') + 1\n    entry.save(update_fields=['trackback_count'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconnect all the signals on Entry model.", "response": "def connect_entry_signals():\n    \"\"\"\n    Connect all the signals on Entry model.\n    \"\"\"\n    post_save.connect(\n        ping_directories_handler, sender=Entry,\n        dispatch_uid=ENTRY_PS_PING_DIRECTORIES)\n    post_save.connect(\n        ping_external_urls_handler, sender=Entry,\n        dispatch_uid=ENTRY_PS_PING_EXTERNAL_URLS)\n    post_save.connect(\n        flush_similar_cache_handler, sender=Entry,\n        dispatch_uid=ENTRY_PS_FLUSH_SIMILAR_CACHE)\n    post_delete.connect(\n        flush_similar_cache_handler, sender=Entry,\n        dispatch_uid=ENTRY_PD_FLUSH_SIMILAR_CACHE)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef disconnect_entry_signals():\n    post_save.disconnect(\n        sender=Entry,\n        dispatch_uid=ENTRY_PS_PING_DIRECTORIES)\n    post_save.disconnect(\n        sender=Entry,\n        dispatch_uid=ENTRY_PS_PING_EXTERNAL_URLS)\n    post_save.disconnect(\n        sender=Entry,\n        dispatch_uid=ENTRY_PS_FLUSH_SIMILAR_CACHE)\n    post_delete.disconnect(\n        sender=Entry,\n        dispatch_uid=ENTRY_PD_FLUSH_SIMILAR_CACHE)", "response": "Disconnect all the signals on Entry model."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconnects all the signals on the Comment model to maintain a valid discussion count on each entry.", "response": "def connect_discussion_signals():\n    \"\"\"\n    Connect all the signals on the Comment model to\n    maintains a valid discussion count on each entries\n    when an action is done with the comments.\n    \"\"\"\n    post_save.connect(\n        count_discussions_handler, sender=comment_model,\n        dispatch_uid=COMMENT_PS_COUNT_DISCUSSIONS)\n    post_delete.connect(\n        count_discussions_handler, sender=comment_model,\n        dispatch_uid=COMMENT_PD_COUNT_DISCUSSIONS)\n    comment_was_flagged.connect(\n        count_discussions_handler, sender=comment_model,\n        dispatch_uid=COMMENT_WF_COUNT_DISCUSSIONS)\n    comment_was_posted.connect(\n        count_comments_handler, sender=comment_model,\n        dispatch_uid=COMMENT_WP_COUNT_COMMENTS)\n    pingback_was_posted.connect(\n        count_pingbacks_handler, sender=comment_model,\n        dispatch_uid=PINGBACK_WF_COUNT_PINGBACKS)\n    trackback_was_posted.connect(\n        count_trackbacks_handler, sender=comment_model,\n        dispatch_uid=TRACKBACK_WF_COUNT_TRACKBACKS)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef disconnect_discussion_signals():\n    post_save.disconnect(\n        sender=comment_model,\n        dispatch_uid=COMMENT_PS_COUNT_DISCUSSIONS)\n    post_delete.disconnect(\n        sender=comment_model,\n        dispatch_uid=COMMENT_PD_COUNT_DISCUSSIONS)\n    comment_was_flagged.disconnect(\n        sender=comment_model,\n        dispatch_uid=COMMENT_WF_COUNT_DISCUSSIONS)\n    comment_was_posted.disconnect(\n        sender=comment_model,\n        dispatch_uid=COMMENT_WP_COUNT_COMMENTS)\n    pingback_was_posted.disconnect(\n        sender=comment_model,\n        dispatch_uid=PINGBACK_WF_COUNT_PINGBACKS)\n    trackback_was_posted.disconnect(\n        sender=comment_model,\n        dispatch_uid=TRACKBACK_WF_COUNT_TRACKBACKS)", "response": "Disconnect all the signals on Comment model\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dispatch(self, *args, **kwargs):\n        return super(QuickEntry, self).dispatch(*args, **kwargs)", "response": "Decorate the view dispatcher with permission_required."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles the datas for posting a quick entry to the admin.", "response": "def post(self, request, *args, **kwargs):\n        \"\"\"\n        Handle the datas for posting a quick entry,\n        and redirect to the admin in case of error or\n        to the entry's page in case of success.\n        \"\"\"\n        now = timezone.now()\n        data = {\n            'title': request.POST.get('title'),\n            'slug': slugify(request.POST.get('title')),\n            'status': DRAFT if 'save_draft' in request.POST else PUBLISHED,\n            'sites': [Site.objects.get_current().pk],\n            'authors': [request.user.pk],\n            'content_template': 'zinnia/_entry_detail.html',\n            'detail_template': 'entry_detail.html',\n            'publication_date': now,\n            'creation_date': now,\n            'last_update': now,\n            'content': request.POST.get('content'),\n            'tags': request.POST.get('tags')}\n        form = QuickEntryForm(data)\n        if form.is_valid():\n            form.instance.content = self.htmlize(form.cleaned_data['content'])\n            entry = form.save()\n            return redirect(entry)\n\n        data = {'title': smart_str(request.POST.get('title', '')),\n                'content': smart_str(self.htmlize(\n                    request.POST.get('content', ''))),\n                'tags': smart_str(request.POST.get('tags', '')),\n                'slug': slugify(request.POST.get('title', '')),\n                'authors': request.user.pk,\n                'sites': Site.objects.get_current().pk}\n        return redirect('%s?%s' % (reverse('admin:zinnia_entry_add'),\n                                   urlencode(data)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef moderate(self, comment, entry, request):\n        if self.auto_moderate_comments:\n            return True\n\n        if check_is_spam(comment, entry, request,\n                         self.spam_checker_backends):\n            return True\n\n        return False", "response": "Determine if a new comment should be moderated."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend email notifications needed.", "response": "def email(self, comment, entry, request):\n        \"\"\"\n        Send email notifications needed.\n        \"\"\"\n        current_language = get_language()\n        try:\n            activate(settings.LANGUAGE_CODE)\n            site = Site.objects.get_current()\n            if self.auto_moderate_comments or comment.is_public:\n                self.do_email_notification(comment, entry, site)\n            if comment.is_public:\n                self.do_email_authors(comment, entry, site)\n                self.do_email_reply(comment, entry, site)\n        finally:\n            activate(current_language)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_email_notification(self, comment, entry, site):\n        if not self.mail_comment_notification_recipients:\n            return\n\n        template = loader.get_template(\n            'comments/zinnia/entry/email/notification.txt')\n        context = {\n            'comment': comment,\n            'entry': entry,\n            'site': site,\n            'protocol': PROTOCOL\n        }\n        subject = _('[%(site)s] New comment posted on \"%(title)s\"') % \\\n            {'site': site.name, 'title': entry.title}\n        message = template.render(context)\n\n        send_mail(\n            subject, message,\n            settings.DEFAULT_FROM_EMAIL,\n            self.mail_comment_notification_recipients,\n            fail_silently=not settings.DEBUG\n        )", "response": "Send an email notification of a new comment to the site staff."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending email notification of a new comment to the authors of the entry.", "response": "def do_email_authors(self, comment, entry, site):\n        \"\"\"\n        Send email notification of a new comment to\n        the authors of the entry.\n        \"\"\"\n        if not self.email_authors:\n            return\n\n        exclude_list = self.mail_comment_notification_recipients + ['']\n        recipient_list = (\n            set([author.email for author in entry.authors.all()])\n            - set(exclude_list)\n        )\n        if not recipient_list:\n            return\n\n        template = loader.get_template(\n            'comments/zinnia/entry/email/authors.txt')\n        context = {\n            'comment': comment,\n            'entry': entry,\n            'site': site,\n            'protocol': PROTOCOL\n        }\n        subject = _('[%(site)s] New comment posted on \"%(title)s\"') % \\\n            {'site': site.name, 'title': entry.title}\n        message = template.render(context)\n\n        send_mail(\n            subject, message,\n            settings.DEFAULT_FROM_EMAIL,\n            recipient_list,\n            fail_silently=not settings.DEBUG\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend an email notification of a new comment to the author of the previous comments.", "response": "def do_email_reply(self, comment, entry, site):\n        \"\"\"\n        Send email notification of a new comment to\n        the authors of the previous comments.\n        \"\"\"\n        if not self.email_reply:\n            return\n\n        exclude_list = (\n            self.mail_comment_notification_recipients\n            + [author.email for author in entry.authors.all()]\n            + [comment.email]\n        )\n        recipient_list = (\n            set([other_comment.email\n                 for other_comment in entry.comments\n                 if other_comment.email])\n            - set(exclude_list)\n        )\n        if not recipient_list:\n            return\n\n        template = loader.get_template(\n            'comments/zinnia/entry/email/reply.txt')\n        context = {\n            'comment': comment,\n            'entry': entry,\n            'site': site,\n            'protocol': PROTOCOL\n        }\n        subject = _('[%(site)s] New comment posted on \"%(title)s\"') % \\\n            {'site': site.name, 'title': entry.title}\n        message = template.render(context)\n\n        mail = EmailMessage(\n            subject, message,\n            settings.DEFAULT_FROM_EMAIL,\n            bcc=recipient_list)\n        mail.send(fail_silently=not settings.DEBUG)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\noverrides get_dated_items to add a useful week_end_day variable in the extra context of the view.", "response": "def get_dated_items(self):\n        \"\"\"\n        Override get_dated_items to add a useful 'week_end_day'\n        variable in the extra context of the view.\n        \"\"\"\n        self.date_list, self.object_list, extra_context = super(\n            EntryWeek, self).get_dated_items()\n        self.date_list = self.get_date_list(self.object_list, 'day')\n        extra_context['week_end_day'] = extra_context[\n            'week'] + datetime.timedelta(days=6)\n        return self.date_list, self.object_list, extra_context"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_dated_items(self):\n        now = timezone.now()\n        if timezone.is_aware(now):\n            now = timezone.localtime(now)\n        today = now.date()\n        self.year, self.month, self.day = today.isoformat().split('-')\n        return self._get_dated_items(today)", "response": "Return a list of items that are older than the current date."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef formatday(self, day, weekday):\n        if day and day in self.day_entries:\n            day_date = date(self.current_year, self.current_month, day)\n            archive_day_url = reverse('zinnia:entry_archive_day',\n                                      args=[day_date.strftime('%Y'),\n                                            day_date.strftime('%m'),\n                                            day_date.strftime('%d')])\n            return '<td class=\"%s entry\"><a href=\"%s\" '\\\n                   'class=\"archives\">%d</a></td>' % (\n                       self.cssclasses[weekday], archive_day_url, day)\n\n        return super(Calendar, self).formatday(day, weekday)", "response": "Return a day as a table cell with a link to the archive day."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a footer for a previous and next month.", "response": "def formatfooter(self, previous_month, next_month):\n        \"\"\"\n        Return a footer for a previous and next month.\n        \"\"\"\n        footer = '<tfoot><tr>' \\\n                 '<td colspan=\"3\" class=\"prev\">%s</td>' \\\n                 '<td class=\"pad\">&nbsp;</td>' \\\n                 '<td colspan=\"3\" class=\"next\">%s</td>' \\\n                 '</tr></tfoot>'\n        if previous_month:\n            previous_content = '<a href=\"%s\" class=\"previous-month\">%s</a>' % (\n                reverse('zinnia:entry_archive_month', args=[\n                    previous_month.strftime('%Y'),\n                    previous_month.strftime('%m')]),\n                date_format(previous_month, 'YEAR_MONTH_FORMAT'))\n        else:\n            previous_content = '&nbsp;'\n\n        if next_month:\n            next_content = '<a href=\"%s\" class=\"next-month\">%s</a>' % (\n                reverse('zinnia:entry_archive_month', args=[\n                    next_month.strftime('%Y'),\n                    next_month.strftime('%m')]),\n                date_format(next_month, 'YEAR_MONTH_FORMAT'))\n        else:\n            next_content = '&nbsp;'\n\n        return footer % (previous_content, next_content)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a month name translated as a table row.", "response": "def formatmonthname(self, theyear, themonth, withyear=True):\n        \"\"\"Return a month name translated as a table row.\"\"\"\n        monthname = '%s %s' % (MONTHS[themonth].title(), theyear)\n        return '<caption>%s</caption>' % monthname"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef formatmonth(self, theyear, themonth, withyear=True,\n                    previous_month=None, next_month=None):\n        \"\"\"\n        Return a formatted month as a table\n        with new attributes computed for formatting a day,\n        and thead/tfooter.\n        \"\"\"\n        self.current_year = theyear\n        self.current_month = themonth\n        self.day_entries = [date.day\n                            for date in Entry.published.filter(\n                                publication_date__year=theyear,\n                                publication_date__month=themonth\n                                ).datetimes('publication_date', 'day')]\n        v = []\n        a = v.append\n        a('<table class=\"%s\">' % (\n            self.day_entries and 'entries-calendar' or 'no-entries-calendar'))\n        a('\\n')\n        a(self.formatmonthname(theyear, themonth, withyear=withyear))\n        a('\\n')\n        a(self.formatweekheader())\n        a('\\n')\n        a(self.formatfooter(previous_month, next_month))\n        a('\\n<tbody>\\n')\n        for week in self.monthdays2calendar(theyear, themonth):\n            a(self.formatweek(week))\n            a('\\n')\n        a('</tbody>\\n</table>')\n        a('\\n')\n        return ''.join(v)", "response": "Return a formatted month as a table containing the day header and footer."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconcatenate a path and a position.", "response": "def append_position(path, position, separator=''):\n    \"\"\"\n    Concatenate a path and a position,\n    between the filename and the extension.\n    \"\"\"\n    filename, extension = os.path.splitext(path)\n    return ''.join([filename, separator, str(position), extension])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a list of templates from a position within a loop.", "response": "def loop_template_list(loop_positions, instance, instance_type,\n                       default_template, registry):\n    \"\"\"\n    Build a list of templates from a position within a loop\n    and a registry of templates.\n    \"\"\"\n    templates = []\n    local_loop_position = loop_positions[1]\n    global_loop_position = loop_positions[0]\n    instance_string = slugify(str(instance))\n\n    for key in ['%s-%s' % (instance_type, instance_string),\n                instance_string,\n                instance_type,\n                'default']:\n        try:\n            templates.append(registry[key][global_loop_position])\n        except KeyError:\n            pass\n\n    templates.append(\n        append_position(default_template, global_loop_position, '-'))\n    templates.append(\n        append_position(default_template, local_loop_position, '_'))\n    templates.append(default_template)\n\n    return templates"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_url_shortener():\n    try:\n        backend_module = import_module(URL_SHORTENER_BACKEND)\n        backend = getattr(backend_module, 'backend')\n    except (ImportError, AttributeError):\n        warnings.warn('%s backend cannot be imported' % URL_SHORTENER_BACKEND,\n                      RuntimeWarning)\n        backend = default_backend\n    except ImproperlyConfigured as e:\n        warnings.warn(str(e), RuntimeWarning)\n        backend = default_backend\n\n    return backend", "response": "Return the selected URL shortener backend."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a description text for the pingback.", "response": "def generate_pingback_content(soup, target, max_length, trunc_char='...'):\n    \"\"\"\n    Generate a description text for the pingback.\n    \"\"\"\n    link = soup.find('a', href=target)\n\n    content = strip_tags(six.text_type(link.findParent()))\n    index = content.index(link.string)\n\n    if len(content) > max_length:\n        middle = max_length // 2\n        start = index - middle\n        end = index + middle\n\n        if start <= 0:\n            end -= start\n            extract = content[0:end]\n        else:\n            extract = '%s%s' % (trunc_char, content[start:end])\n\n        if end < len(content):\n            extract += trunc_char\n        return extract\n\n    return content"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pingback_ping(source, target):\n    try:\n        if source == target:\n            return UNDEFINED_ERROR\n\n        site = Site.objects.get_current()\n        try:\n            document = ''.join(map(\n                lambda byte_line: byte_line.decode('utf-8'),\n                urlopen(source).readlines()))\n        except (HTTPError, URLError):\n            return SOURCE_DOES_NOT_EXIST\n\n        if target not in document:\n            return SOURCE_DOES_NOT_LINK\n\n        target_splitted = urlsplit(target)\n        if target_splitted.netloc != site.domain:\n            return TARGET_DOES_NOT_EXIST\n\n        try:\n            view, args, kwargs = resolve(target_splitted.path)\n        except Resolver404:\n            return TARGET_DOES_NOT_EXIST\n\n        try:\n            entry = Entry.published.get(\n                slug=kwargs['slug'],\n                publication_date__year=kwargs['year'],\n                publication_date__month=kwargs['month'],\n                publication_date__day=kwargs['day'])\n            if not entry.pingbacks_are_open:\n                return TARGET_IS_NOT_PINGABLE\n        except (KeyError, Entry.DoesNotExist):\n            return TARGET_IS_NOT_PINGABLE\n\n        soup = BeautifulSoup(document, 'html.parser')\n        title = six.text_type(soup.find('title'))\n        title = title and strip_tags(title) or _('No title')\n        description = generate_pingback_content(soup, target,\n                                                PINGBACK_CONTENT_LENGTH)\n\n        pingback_klass = comments.get_model()\n        pingback_datas = {\n            'content_type': ContentType.objects.get_for_model(Entry),\n            'object_pk': entry.pk,\n            'site': site,\n            'user_url': source,\n            'user_name': title,\n            'comment': description\n        }\n        pingback = pingback_klass(**pingback_datas)\n        if check_is_spam(pingback, entry, FakeRequest()):\n            return PINGBACK_IS_SPAM\n\n        pingback_defaults = {'comment': pingback_datas.pop('comment'),\n                             'user_name': pingback_datas.pop('user_name')}\n        pingback, created = pingback_klass.objects.get_or_create(\n            defaults=pingback_defaults,\n            **pingback_datas)\n        if created:\n            pingback.flags.create(user=get_user_flagger(), flag=PINGBACK)\n            pingback_was_posted.send(pingback.__class__,\n                                     pingback=pingback,\n                                     entry=entry)\n            return 'Pingback from %s to %s registered.' % (source, target)\n        return PINGBACK_ALREADY_REGISTERED\n    except Exception:\n        return UNDEFINED_ERROR", "response": "pingback.ping(sourceURI, targetURI) => 'Pingback message'\n\n    Notifies the server that a link has been added to sourceURI,\n    pointing to targetURI.\n\n    See: http://hixie.ch/specs/pingback/pingback-1.0"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pingback_extensions_get_pingbacks(target):\n    site = Site.objects.get_current()\n\n    target_splitted = urlsplit(target)\n    if target_splitted.netloc != site.domain:\n        return TARGET_DOES_NOT_EXIST\n\n    try:\n        view, args, kwargs = resolve(target_splitted.path)\n    except Resolver404:\n        return TARGET_DOES_NOT_EXIST\n\n    try:\n        entry = Entry.published.get(\n            slug=kwargs['slug'],\n            publication_date__year=kwargs['year'],\n            publication_date__month=kwargs['month'],\n            publication_date__day=kwargs['day'])\n    except (KeyError, Entry.DoesNotExist):\n        return TARGET_IS_NOT_PINGABLE\n\n    return [pingback.user_url for pingback in entry.pingbacks]", "response": "Returns an array of URLs that link to the specified target."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndecorates the view dispatcher with csrf_exempt.", "response": "def dispatch(self, *args, **kwargs):\n        \"\"\"\n        Decorate the view dispatcher with csrf_exempt.\n        \"\"\"\n        return super(EntryTrackback, self).dispatch(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self, request, *args, **kwargs):\n        entry = self.get_object()\n        return HttpResponsePermanentRedirect(entry.get_absolute_url())", "response": "This is a GET only do a permanent redirection to the Entry. get_absolute_url"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post(self, request, *args, **kwargs):\n        url = request.POST.get('url')\n\n        if not url:\n            return self.get(request, *args, **kwargs)\n\n        entry = self.get_object()\n        site = Site.objects.get_current()\n\n        if not entry.trackbacks_are_open:\n            return self.render_to_response(\n                {'error': 'Trackback is not enabled for %s' % entry.title})\n\n        title = request.POST.get('title') or url\n        excerpt = request.POST.get('excerpt') or title\n        blog_name = request.POST.get('blog_name') or title\n        ip_address = request.META.get('REMOTE_ADDR', None)\n\n        trackback_klass = comments.get_model()\n        trackback_datas = {\n            'content_type': ContentType.objects.get_for_model(Entry),\n            'object_pk': entry.pk,\n            'site': site,\n            'user_url': url,\n            'user_name': blog_name,\n            'ip_address': ip_address,\n            'comment': excerpt\n        }\n\n        trackback = trackback_klass(**trackback_datas)\n        if check_is_spam(trackback, entry, request):\n            return self.render_to_response(\n                {'error': 'Trackback considered like spam'})\n\n        trackback_defaults = {'comment': trackback_datas.pop('comment')}\n        trackback, created = trackback_klass.objects.get_or_create(\n            defaults=trackback_defaults,\n            **trackback_datas)\n        if created:\n            trackback.flags.create(user=get_user_flagger(), flag=TRACKBACK)\n            trackback_was_posted.send(trackback.__class__,\n                                      trackback=trackback,\n                                      entry=entry)\n        else:\n            return self.render_to_response(\n                {'error': 'Trackback is already registered'})\n        return self.render_to_response({})", "response": "Check if trackbacks are enabled on the Entry and if so create a trackback."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npopulating the context of the template with all published entries and categories and authors.", "response": "def get_context_data(self, **kwargs):\n        \"\"\"\n        Populate the context of the template\n        with all published entries and all the categories.\n        \"\"\"\n        context = super(Sitemap, self).get_context_data(**kwargs)\n        context.update(\n            {'entries': Entry.published.all(),\n             'categories': Category.published.all(),\n             'authors': Author.published.all()}\n        )\n        return context"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the selected spam checker backend.", "response": "def get_spam_checker(backend_path):\n    \"\"\"\n    Return the selected spam checker backend.\n    \"\"\"\n    try:\n        backend_module = import_module(backend_path)\n        backend = getattr(backend_module, 'backend')\n    except (ImportError, AttributeError):\n        warnings.warn('%s backend cannot be imported' % backend_path,\n                      RuntimeWarning)\n        backend = None\n    except ImproperlyConfigured as e:\n        warnings.warn(str(e), RuntimeWarning)\n        backend = None\n\n    return backend"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_is_spam(content, content_object, request,\n                  backends=None):\n    \"\"\"\n    Return True if the content is a spam, else False.\n    \"\"\"\n    if backends is None:\n        backends = SPAM_CHECKER_BACKENDS\n\n    for backend_path in backends:\n        spam_checker = get_spam_checker(backend_path)\n        if spam_checker is not None:\n            is_spam = spam_checker(content, content_object, request)\n            if is_spam:\n                return True\n\n    return False", "response": "Check if the content is a spam."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dict of the next and previous date periods with published entries.", "response": "def get_previous_next_published(self, date):\n        \"\"\"\n        Returns a dict of the next and previous date periods\n        with published entries.\n        \"\"\"\n        previous_next = getattr(self, 'previous_next', None)\n\n        if previous_next is None:\n            date_year = datetime(date.year, 1, 1)\n            date_month = datetime(date.year, date.month, 1)\n            date_day = datetime(date.year, date.month, date.day)\n            date_next_week = date_day + timedelta(weeks=1)\n            previous_next = {'year': [None, None],\n                             'week': [None, None],\n                             'month': [None, None],\n                             'day':  [None, None]}\n            dates = self.get_queryset().datetimes(\n                'publication_date', 'day', order='ASC')\n            for d in dates:\n                d_year = datetime(d.year, 1, 1)\n                d_month = datetime(d.year, d.month, 1)\n                d_day = datetime(d.year, d.month, d.day)\n                if d_year < date_year:\n                    previous_next['year'][0] = d_year.date()\n                elif d_year > date_year and not previous_next['year'][1]:\n                    previous_next['year'][1] = d_year.date()\n                if d_month < date_month:\n                    previous_next['month'][0] = d_month.date()\n                elif d_month > date_month and not previous_next['month'][1]:\n                    previous_next['month'][1] = d_month.date()\n                if d_day < date_day:\n                    previous_next['day'][0] = d_day.date()\n                    previous_next['week'][0] = d_day.date() - timedelta(\n                        days=d_day.weekday())\n                elif d_day > date_day and not previous_next['day'][1]:\n                    previous_next['day'][1] = d_day.date()\n                if d_day > date_next_week and not previous_next['week'][1]:\n                    previous_next['week'][1] = d_day.date() - timedelta(\n                        days=d_day.weekday())\n\n            setattr(self, 'previous_next', previous_next)\n        return previous_next"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the list of entries that match the pattern.", "response": "def get_queryset(self):\n        \"\"\"\n        Overridde the get_queryset method to\n        do some validations and build the search queryset.\n        \"\"\"\n        entries = Entry.published.none()\n\n        if self.request.GET:\n            self.pattern = self.request.GET.get('pattern', '')\n            if len(self.pattern) < 3:\n                self.error = _('The pattern is too short')\n            else:\n                entries = Entry.published.search(self.pattern)\n        else:\n            self.error = _('No pattern to search found')\n        return entries"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd error and pattern in context.", "response": "def get_context_data(self, **kwargs):\n        \"\"\"\n        Add error and pattern in context.\n        \"\"\"\n        context = super(BaseEntrySearch, self).get_context_data(**kwargs)\n        context.update({'error': self.error, 'pattern': self.pattern})\n        return context"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving the author by his username and build a queryset of his published entries.", "response": "def get_queryset(self):\n        \"\"\"\n        Retrieve the author by his username and\n        build a queryset of his published entries.\n        \"\"\"\n        self.author = get_object_or_404(\n            Author, **{Author.USERNAME_FIELD: self.kwargs['username']})\n        return self.author.entries_published()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_context_data(self, **kwargs):\n        context = super(BaseAuthorDetail, self).get_context_data(**kwargs)\n        context['author'] = self.author\n        return context", "response": "Add the current author in context."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve the tag by his name and AttributeNames build a queryset of his published entries.", "response": "def get_queryset(self):\n        \"\"\"\n        Retrieve the tag by his name and\n        build a queryset of his published entries.\n        \"\"\"\n        self.tag = get_tag(self.kwargs['tag'])\n        if self.tag is None:\n            raise Http404(_('No Tag found matching \"%s\".') %\n                          self.kwargs['tag'])\n        return TaggedItem.objects.get_by_model(\n            Entry.published.all(), self.tag)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds the current tag in context.", "response": "def get_context_data(self, **kwargs):\n        \"\"\"\n        Add the current tag in context.\n        \"\"\"\n        context = super(BaseTagDetail, self).get_context_data(**kwargs)\n        context['tag'] = self.tag\n        return context"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_analogy_file(filename):\n    \n    section = None\n\n    with open(filename, 'r') as questions_file:\n        for line in questions_file:\n            if line.startswith(':'):\n                section = line[2:].replace('\\n', '')\n                continue\n            else:\n                words = line.replace('\\n', '').split(' ')\n\n            yield section, words", "response": "Read the analogy task test set from a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the analogy rank score for the given set of analogies and word vectors.", "response": "def analogy_rank_score(analogies, word_vectors, no_threads=1):\n    \"\"\"\n    Calculate the analogy rank score for the given set of analogies.\n\n    A rank of zero denotes a perfect score; with random word vectors\n    we would expect a rank of 0.5.\n\n    Arguments:\n    - analogies: a numpy array holding the ids of the words in the analogy tasks,\n                 as constructed by `construct_analogy_test_set`.\n    - word_vectors: numpy array holding the word vectors to use.\n    - num_threads: number of parallel threads to use in the calculation.\n\n    Returns:\n    - ranks: a numpy array holding the normalized rank of the target word\n             in each analogy task. Rank 0 means that the target words was\n             returned first; rank 1 means it was returned last.\n    \"\"\"\n\n    # The mean of the vectors for the\n    # second, third, and the negative of\n    # the first word.\n    input_vectors = (word_vectors[analogies[:, 1]]\n                     + word_vectors[analogies[:, 2]]\n                     - word_vectors[analogies[:, 0]])\n\n    word_vector_norms = np.linalg.norm(word_vectors,\n                                       axis=1)\n\n    # Pre-allocate the array storing the rank violations\n    rank_violations = np.zeros(input_vectors.shape[0], dtype=np.int32)\n\n    compute_rank_violations(word_vectors,\n                            word_vector_norms,\n                            input_vectors,\n                            analogies[:, 3],\n                            analogies,\n                            rank_violations,\n                            no_threads)\n\n    return rank_violations / float(word_vectors.shape[0])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nperform a pass through the corpus to construct a cooccurrence matrix.", "response": "def fit(self, corpus, window=10, ignore_missing=False):\n        \"\"\"\n        Perform a pass through the corpus to construct\n        the cooccurrence matrix.\n\n        Parameters:\n        - iterable of lists of strings corpus\n        - int window: the length of the (symmetric)\n          context window used for cooccurrence.\n        - bool ignore_missing: whether to ignore words missing from\n                               the dictionary (if it was supplied).\n                               Context window distances will be preserved\n                               even if out-of-vocabulary words are\n                               ignored.\n                               If False, a KeyError is raised.\n        \"\"\"\n\n        self.matrix = construct_cooccurrence_matrix(corpus,\n                                                    self.dictionary,\n                                                    int(self.dictionary_supplied),\n                                                    int(window),\n                                                    int(ignore_missing))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nestimate the word embeddings. Parameters: - scipy.sparse.coo_matrix matrix: coocurrence matrix - int epochs: number of training epochs - int no_threads: number of training threads - bool verbose: print progress messages if True", "response": "def fit(self, matrix, epochs=5, no_threads=2, verbose=False):\n        \"\"\"\n        Estimate the word embeddings.\n\n        Parameters:\n        - scipy.sparse.coo_matrix matrix: coocurrence matrix\n        - int epochs: number of training epochs\n        - int no_threads: number of training threads\n        - bool verbose: print progress messages if True\n        \"\"\"\n\n        shape = matrix.shape\n\n        if (len(shape) != 2 or\n            shape[0] != shape[1]):\n            raise Exception('Coocurrence matrix must be square')\n\n        if not sp.isspmatrix_coo(matrix):\n            raise Exception('Coocurrence matrix must be in the COO format')\n\n        random_state = check_random_state(self.random_state)\n        self.word_vectors = ((random_state.rand(shape[0],\n                                                self.no_components) - 0.5)\n                             / self.no_components)\n        self.word_biases = np.zeros(shape[0],\n                                    dtype=np.float64)\n\n        self.vectors_sum_gradients = np.ones_like(self.word_vectors)\n        self.biases_sum_gradients = np.ones_like(self.word_biases)\n\n        shuffle_indices = np.arange(matrix.nnz, dtype=np.int32)\n\n        if verbose:\n            print('Performing %s training epochs '\n                  'with %s threads' % (epochs, no_threads))\n\n        for epoch in range(epochs):\n\n            if verbose:\n                print('Epoch %s' % epoch)\n\n            # Shuffle the coocurrence matrix\n            random_state.shuffle(shuffle_indices)\n\n            fit_vectors(self.word_vectors,\n                        self.vectors_sum_gradients,\n                        self.word_biases,\n                        self.biases_sum_gradients,\n                        matrix.row,\n                        matrix.col,\n                        matrix.data,\n                        shuffle_indices,\n                        self.learning_rate,\n                        self.max_count,\n                        self.alpha,\n                        self.max_loss,\n                        int(no_threads))\n\n            if not np.isfinite(self.word_vectors).all():\n                raise Exception('Non-finite values in word vectors. '\n                                'Try reducing the learning rate or the '\n                                'max_loss parameter.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transform_paragraph(self, paragraph, epochs=50, ignore_missing=False):\n\n        if self.word_vectors is None:\n            raise Exception('Model must be fit to transform paragraphs')\n\n        if self.dictionary is None:\n            raise Exception('Dictionary must be provided to '\n                            'transform paragraphs')\n\n        cooccurrence = collections.defaultdict(lambda: 0.0)\n\n        for token in paragraph:\n            try:\n                cooccurrence[self.dictionary[token]] += self.max_count / 10.0\n            except KeyError:\n                if not ignore_missing:\n                    raise\n\n        random_state = check_random_state(self.random_state)\n\n        word_ids = np.array(cooccurrence.keys(), dtype=np.int32)\n        values = np.array(cooccurrence.values(), dtype=np.float64)\n        shuffle_indices = np.arange(len(word_ids), dtype=np.int32)\n\n        # Initialize the vector to mean of constituent word vectors\n        paragraph_vector = np.mean(self.word_vectors[word_ids], axis=0)\n        sum_gradients = np.ones_like(paragraph_vector)\n\n        # Shuffle the coocurrence matrix\n        random_state.shuffle(shuffle_indices)\n        transform_paragraph(self.word_vectors,\n                            self.word_biases,\n                            paragraph_vector,\n                            sum_gradients,\n                            word_ids,\n                            values,\n                            shuffle_indices,\n                            self.learning_rate,\n                            self.max_count,\n                            self.alpha,\n                            epochs)\n\n        return paragraph_vector", "response": "Transform an iterable of tokens into its vector representation."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a word - id dictionary to the model.", "response": "def add_dictionary(self, dictionary):\n        \"\"\"\n        Supply a word-id dictionary to allow similarity queries.\n        \"\"\"\n\n        if self.word_vectors is None:\n            raise Exception('Model must be fit before adding a dictionary')\n\n        if len(dictionary) > self.word_vectors.shape[0]:\n            raise Exception('Dictionary length must be smaller '\n                            'or equal to the number of word vectors')\n\n        self.dictionary = dictionary\n        if hasattr(self.dictionary, 'iteritems'):\n            # Python 2 compat\n            items_iterator = self.dictionary.iteritems()\n        else:\n            items_iterator = self.dictionary.items()\n\n        self.inverse_dictionary = {v: k for k, v in items_iterator}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save(self, filename):\n\n        with open(filename, 'wb') as savefile:\n            pickle.dump(self.__dict__,\n                        savefile,\n                        protocol=pickle.HIGHEST_PROTOCOL)", "response": "Serialize the current state of the object to filename."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload a new object from a file.", "response": "def load(cls, filename):\n        \"\"\"\n        Load model from filename.\n        \"\"\"\n\n        instance = Glove()\n\n        with open(filename, 'rb') as savefile:\n            instance.__dict__ = pickle.load(savefile)\n\n        return instance"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload the model from the output files generated by the Glove model from the input file.", "response": "def load_stanford(cls, filename):\n        \"\"\"\n        Load model from the output files generated by\n        the C code from http://nlp.stanford.edu/projects/glove/.\n\n        The entries of the word dictionary will be of type\n        unicode in Python 2 and str in Python 3.\n        \"\"\"\n\n        dct = {}\n        vectors = array.array('d')\n\n        # Read in the data.\n        with io.open(filename, 'r', encoding='utf-8') as savefile:\n            for i, line in enumerate(savefile):\n                tokens = line.split(' ')\n\n                word = tokens[0]\n                entries = tokens[1:]\n\n                dct[word] = i\n                vectors.extend(float(x) for x in entries)\n\n        # Infer word vectors dimensions.\n        no_components = len(entries)\n        no_vectors = len(dct)\n\n        # Set up the model instance.\n        instance = Glove()\n        instance.no_components = no_components\n        instance.word_vectors = (np.array(vectors)\n                                 .reshape(no_vectors,\n                                          no_components))\n        instance.word_biases = np.zeros(no_vectors)\n        instance.add_dictionary(dct)\n\n        return instance"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun a similarity query and retrieve the most similar words.", "response": "def most_similar(self, word, number=5):\n        \"\"\"\n        Run a similarity query, retrieving number\n        most similar words.\n        \"\"\"\n\n        if self.word_vectors is None:\n            raise Exception('Model must be fit before querying')\n\n        if self.dictionary is None:\n            raise Exception('No word dictionary supplied')\n\n        try:\n            word_idx = self.dictionary[word]\n        except KeyError:\n            raise Exception('Word not in dictionary')\n\n        return self._similarity_query(self.word_vectors[word_idx], number)[1:]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the most similar words to a given paragraph.", "response": "def most_similar_paragraph(self, paragraph, number=5, **kwargs):\n        \"\"\"\n        Return words most similar to a given paragraph (iterable of tokens).\n        \"\"\"\n\n        paragraph_vector = self.transform_paragraph(paragraph, **kwargs)\n\n        return self._similarity_query(paragraph_vector, number)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntries to find and use GCC on OSX for OpenMP support.", "response": "def set_gcc():\n    \"\"\"\n    Try to find and use GCC on OSX for OpenMP support.\n    \"\"\"\n\n    # For macports and homebrew\n    patterns = ['/opt/local/bin/gcc-mp-[0-9].[0-9]',\n                '/opt/local/bin/gcc-mp-[0-9]',\n                '/usr/local/bin/gcc-[0-9].[0-9]',\n                '/usr/local/bin/gcc-[0-9]']\n\n    if 'darwin' in platform.platform().lower():\n\n        gcc_binaries = []\n        for pattern in patterns:\n            gcc_binaries += glob.glob(pattern)\n        gcc_binaries.sort()\n\n        if gcc_binaries:\n            _, gcc = os.path.split(gcc_binaries[-1])\n            os.environ[\"CC\"] = gcc\n\n        else:\n            raise Exception('No GCC available. Install gcc from Homebrew '\n                            'using brew install gcc.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gapply(grouped_data, func, schema, *cols):\n    import pandas as pd\n    minPandasVersion = '0.7.1'\n    if LooseVersion(pd.__version__) < LooseVersion(minPandasVersion):\n        raise ImportError('Pandas installed but version is {}, {} required'\n                          .format(pd.__version__, minPandasVersion))\n\n    # Do a null aggregation to retrieve the keys first (should be no computation)\n    # Also consistent with spark.sql.retainGroupColumns\n    keySchema = grouped_data.agg({}).schema\n    keyCols = grouped_data.agg({}).columns\n\n    if not cols:\n        # Extract the full column list with the parent df\n        javaDFName = \"org$apache$spark$sql$RelationalGroupedDataset$$df\"\n        parentDF = java_gateway.get_field(grouped_data._jgd, javaDFName)\n        allCols = DataFrame(parentDF, None).columns\n        keyColsSet = set(keyCols)\n        cols = [col for col in allCols if col not in keyColsSet]\n\n    if \"*\" in cols:\n        raise ValueError(\"cols expected to contain only singular columns\")\n\n    if len(set(cols)) < len(cols):\n        raise ValueError(\"cols expected not to contain duplicate columns\")\n\n    if not isinstance(schema, StructType):\n        raise ValueError(\"output schema should be a StructType\")\n\n    inputAggDF = grouped_data.agg({col: 'collect_list' for col in cols})\n    # Recover canonical order (aggregation may change column order)\n    canonicalOrder = chain(keyCols, [inputAggDF['collect_list(' + col + ')'] for col in cols])\n    inputAggDF = inputAggDF.select(*canonicalOrder)\n\n    # Wraps the user-provided function with another python function, which prepares the\n    # input in the form specified by the documentation. Then, once the function completes,\n    # this wrapper prepends the keys to the output values and converts from pandas.\n    def pandasWrappedFunc(*args):\n        nvals = len(cols)\n        keys, collectedCols = args[:-nvals], args[-nvals:]\n        paramKeys = tuple(keys)\n        if len(paramKeys) == 1:\n            paramKeys = paramKeys[0]\n        valuesDF = pd.DataFrame.from_dict(dict(zip(cols, collectedCols)))\n        valuesDF = valuesDF[list(cols)]  # reorder to canonical\n        outputDF = func(paramKeys, valuesDF)\n        valCols = outputDF.columns.tolist()\n        for key, keyName in zip(keys, keyCols):\n            outputDF[keyName] = key\n        outputDF = outputDF[keyCols + valCols]  # reorder to canonical\n        # To recover native python types for serialization, we need\n        # to convert the pandas dataframe to a numpy array, then to a\n        # native list (can't go straight to native, since pandas will\n        # attempt to perserve the numpy type).\n        return outputDF.values.tolist()\n\n    keyPrependedSchema = StructType(list(chain(keySchema, schema)))\n    outputAggSchema = ArrayType(keyPrependedSchema, containsNull=False)\n    pandasUDF = udf(pandasWrappedFunc, outputAggSchema)\n    outputAggDF = inputAggDF.select(pandasUDF(*inputAggDF))\n\n    explodedDF = outputAggDF.select(explode(*outputAggDF).alias(\"gapply\"))\n    # automatically retrieves nested schema column names\n    return explodedDF.select(\"gapply.*\")", "response": "Applies a function func to data grouped by key."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fit(self, X, y=None, groups=None):\n        return self._fit(X, y, groups, ParameterGrid(self.param_grid))", "response": "Fit the model with all sets of parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a scikit - learn model to a Spark ML model.", "response": "def toSpark(self, model):\n        \"\"\"\n        Convert a scikit-learn model to a Spark ML model from the Pipelines API (spark.ml).\n        Currently supported models:\n        - sklearn.linear_model.LogisticRegression (binary classification only, not multiclass)\n        - sklearn.linear_model.LinearRegression\n\n        :param model: scikit-learn model\n        :return: Spark ML model with equivalent predictive behavior.\n                 Currently, parameters or arguments for training are not copied.\n        \"\"\"\n        if isinstance(model, SKL_LogisticRegression) or isinstance(model, SKL_LinearRegression):\n            return self._toSparkGLM(model)\n        else:\n            supported_types = map(lambda t: type(t), self._supported_skl_types)\n            raise ValueError(\"Converter.toSpark cannot convert type: %s.  Supported types: %s\" %\n                             (type(model), \", \".join(supported_types)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef toSKLearn(self, model):\n        if isinstance(model, LogisticRegressionModel):\n            return self._toSKLGLM(model, True)\n        if isinstance(model, LinearRegressionModel):\n            return self._toSKLGLM(model, False)\n        else:\n            supported_types = map(lambda t: type(t), self._supported_spark_types)\n            raise ValueError(\"Converter.toSKLearn cannot convert type: %s.  Supported types: %s\" %\n                             (type(model), \", \".join(supported_types)))", "response": "Convert a Spark MLlib model from the Pipelines API ( spark. ml ) to a scikit - learn model."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _toSKLGLM(self, model, is_classifier):\n        py_cls = type(model)\n        skl_cls = self._spark2skl_classes[py_cls]\n        intercept = model.intercept\n        weights = model.coefficients\n        skl = skl_cls()\n        skl.intercept_ = np.float64(intercept)\n        skl.coef_ = weights.toArray().reshape(1, -1)\n        if is_classifier:\n            skl.classes_ = np.array([0, 1])\n        return skl", "response": "Private method for converting a GLM to a scikit - learn model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning fit on the estimator with randomly drawn parameters.", "response": "def fit(self, X, y=None, groups=None):\n        \"\"\"Run fit on the estimator with randomly drawn parameters.\n\n        Parameters\n        ----------\n\n        X : array-like, shape = [n_samples, n_features]\n            Training vector, where n_samples in the number of samples and\n            n_features is the number of features.\n\n        y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n            Target relative to X for classification or regression;\n            None for unsupervised learning.\n\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n        \"\"\"\n        sampled_params = ParameterSampler(self.param_distributions,\n                                          self.n_iter,\n                                          random_state=self.random_state)\n\n        return self._fit(X, y, groups, sampled_params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _new_java_obj(sc, java_class, *args):\n    java_obj = _jvm()\n    for name in java_class.split(\".\"):\n        java_obj = getattr(java_obj, name)\n    java_args = [_py2java(sc, arg) for arg in args]\n    return java_obj(*java_args)", "response": "Construct a new Java object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall the Java method specified by name with the given arguments.", "response": "def _call_java(sc, java_obj, name, *args):\n    \"\"\"\n    Method copied from pyspark.ml.wrapper.  Uses private Spark APIs.\n    \"\"\"\n    m = getattr(java_obj, name)\n    java_args = [_py2java(sc, arg) for arg in args]\n    return _java2py(sc, m(*java_args))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload a dataset from the file.", "response": "def get_dataset(self, key, info):\n        \"\"\"Load a dataset.\"\"\"\n        if self._channel != key.name:\n            return\n\n        logger.debug('Reading %s.', key.name)\n        # FIXME: get this from MTD_MSIL1C.xml\n        quantification_value = 10000.\n        jp2 = glymur.Jp2k(self.filename)\n        bitdepth = 0\n        for seg in jp2.codestream.segment:\n            try:\n                bitdepth = max(bitdepth, seg.bitdepth[0])\n            except AttributeError:\n                pass\n\n        jp2.dtype = (np.uint8 if bitdepth <= 8 else np.uint16)\n\n        # Initialize the jp2 reader / doesn't work in a multi-threaded context.\n        # jp2[0, 0]\n        # data = da.from_array(jp2, chunks=CHUNK_SIZE) / quantification_value * 100\n\n        data = da.from_delayed(delayed(jp2.read)(), jp2.shape, jp2.dtype)\n        data = data.rechunk(CHUNK_SIZE) / quantification_value * 100\n\n        proj = DataArray(data, dims=['y', 'x'])\n        proj.attrs = info.copy()\n        proj.attrs['units'] = '%'\n        proj.attrs['platform_name'] = self.platform_name\n        return proj"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the area definition of the dataset.", "response": "def get_area_def(self, dsid):\n        \"\"\"Get the area definition of the dataset.\"\"\"\n        geocoding = self.root.find('.//Tile_Geocoding')\n        epsg = geocoding.find('HORIZONTAL_CS_CODE').text\n        rows = int(geocoding.find('Size[@resolution=\"' + str(dsid.resolution) + '\"]/NROWS').text)\n        cols = int(geocoding.find('Size[@resolution=\"' + str(dsid.resolution) + '\"]/NCOLS').text)\n        geoposition = geocoding.find('Geoposition[@resolution=\"' + str(dsid.resolution) + '\"]')\n        ulx = float(geoposition.find('ULX').text)\n        uly = float(geoposition.find('ULY').text)\n        xdim = float(geoposition.find('XDIM').text)\n        ydim = float(geoposition.find('YDIM').text)\n        area_extent = (ulx, uly + rows * ydim, ulx + cols * xdim, uly)\n        area = geometry.AreaDefinition(\n                    self.tile,\n                    \"On-the-fly area\",\n                    self.tile,\n                    {'init': epsg},\n                    cols,\n                    rows,\n                    area_extent)\n        return area"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_coarse_dataset(self, key, info):\n        angles = self.root.find('.//Tile_Angles')\n        if key in ['solar_zenith_angle', 'solar_azimuth_angle']:\n            elts = angles.findall(info['xml_tag'] + '/Values_List/VALUES')\n            return np.array([[val for val in elt.text.split()] for elt in elts],\n                            dtype=np.float)\n\n        elif key in ['satellite_zenith_angle', 'satellite_azimuth_angle']:\n            arrays = []\n            elts = angles.findall(info['xml_tag'] + '[@bandId=\"1\"]')\n            for elt in elts:\n                items = elt.findall(info['xml_item'] + '/Values_List/VALUES')\n                arrays.append(np.array([[val for val in item.text.split()] for item in items],\n                                       dtype=np.float))\n            return np.nanmean(np.dstack(arrays), -1)\n        else:\n            return", "response": "Get the coarse dataset refered to by key from the XML data."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the dataset refered to by key.", "response": "def get_dataset(self, key, info):\n        \"\"\"Get the dataset refered to by `key`.\"\"\"\n\n        angles = self._get_coarse_dataset(key, info)\n        if angles is None:\n            return\n\n        # Fill gaps at edges of swath\n        darr = DataArray(angles, dims=['y', 'x'])\n        darr = darr.bfill('x')\n        darr = darr.ffill('x')\n        angles = darr.data\n\n        res = self.interpolate_angles(angles, key.resolution)\n\n        proj = DataArray(res, dims=['y', 'x'])\n        proj.attrs = info.copy()\n        proj.attrs['units'] = 'degrees'\n        proj.attrs['platform_name'] = self.platform_name\n        return proj"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget output shape of specified dataset.", "response": "def get_shape(self, ds_id, ds_info):\n        \"\"\"Get output shape of specified dataset.\"\"\"\n        var_path = ds_info['file_key']\n        shape = self[var_path + '/shape']\n        if ((ds_info.get('standard_name') == \"longitude\" or ds_info.get('standard_name') == \"latitude\") and\n                ds_id.resolution == 10000):\n            return shape[0], int(shape[1] / 2)\n        return shape"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_dataset(self, ds_id, ds_info):\n        var_path = ds_info['file_key']\n        fill_value = ds_info.get('fill_value', 65535)\n        metadata = self.get_metadata(ds_id, ds_info)\n\n        data = self[var_path]\n        if ((ds_info.get('standard_name') == \"longitude\" or\n             ds_info.get('standard_name') == \"latitude\") and\n                ds_id.resolution == 10000):\n            # FIXME: Lower frequency channels need CoRegistration parameters applied\n            data = data[:, ::2] * self[var_path + \"/attr/SCALE FACTOR\"]\n        else:\n            data = data * self[var_path + \"/attr/SCALE FACTOR\"]\n        data = data.where(data != fill_value)\n        data.attrs.update(metadata)\n        return data", "response": "Get output data and metadata of specified dataset."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning index in to coefficient arrays for this band's wavelength. This function search through the `COEFF_INDEX_MAP` dictionary and finds the first key where the nominal wavelength of `wavelength_range` falls between the minimum wavelength and maximum wavelength of the key. `wavelength_range` can also be the standard name of the band. For example, \"M05\" for VIIRS or \"1\" for MODIS. :param sensor: sensor of band to be corrected :param wavelength_range: 3-element tuple of (min wavelength, nominal wavelength, max wavelength) :param resolution: resolution of the band to be corrected :return: index in to coefficient arrays like `aH2O`, `aO3`, etc. None is returned if no matching wavelength is found", "response": "def find_coefficient_index(sensor, wavelength_range, resolution=0):\n    \"\"\"Return index in to coefficient arrays for this band's wavelength.\n\n    This function search through the `COEFF_INDEX_MAP` dictionary and\n    finds the first key where the nominal wavelength of `wavelength_range`\n    falls between the minimum wavelength and maximum wavelength of the key.\n    `wavelength_range` can also be the standard name of the band. For\n    example, \"M05\" for VIIRS or \"1\" for MODIS.\n\n    :param sensor: sensor of band to be corrected\n    :param wavelength_range: 3-element tuple of (min wavelength, nominal wavelength, max wavelength)\n    :param resolution: resolution of the band to be corrected\n    :return: index in to coefficient arrays like `aH2O`, `aO3`, etc.\n             None is returned if no matching wavelength is found\n    \"\"\"\n\n    index_map = COEFF_INDEX_MAP[sensor.lower()]\n    # Find the best resolution of coefficients\n    for res in sorted(index_map.keys()):\n        if resolution <= res:\n            index_map = index_map[res]\n            break\n    else:\n        raise ValueError(\"Unrecognized data resolution: {}\", resolution)\n    # Find the best wavelength of coefficients\n    if isinstance(wavelength_range, str):\n        # wavelength range is actually a band name\n        return index_map[wavelength_range]\n    else:\n        for k, v in index_map.items():\n            if isinstance(k, str):\n                # we are analyzing wavelengths and ignoring dataset names\n                continue\n            if k[0] <= wavelength_range[1] <= k[2]:\n                return v"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_crefl(refl, coeffs,\n              lon,\n              lat,\n              sensor_azimuth,\n              sensor_zenith,\n              solar_azimuth,\n              solar_zenith,\n              avg_elevation=None,\n              percent=False,\n              use_abi=False):\n    \"\"\"Run main crefl algorithm.\n\n    All input parameters are per-pixel values meaning they are the same size\n    and shape as the input reflectance data, unless otherwise stated.\n\n    :param reflectance_bands: tuple of reflectance band arrays\n    :param coefficients: tuple of coefficients for each band (see `get_coefficients`)\n    :param lon: input swath longitude array\n    :param lat: input swath latitude array\n    :param sensor_azimuth: input swath sensor azimuth angle array\n    :param sensor_zenith: input swath sensor zenith angle array\n    :param solar_azimuth: input swath solar azimuth angle array\n    :param solar_zenith: input swath solar zenith angle array\n    :param avg_elevation: average elevation (usually pre-calculated and stored in CMGDEM.hdf)\n    :param percent: True if input reflectances are on a 0-100 scale instead of 0-1 scale (default: False)\n\n    \"\"\"\n    # FUTURE: Find a way to compute the average elevation before hand\n    # Get digital elevation map data for our granule, set ocean fill value to 0\n    if avg_elevation is None:\n        LOG.debug(\"No average elevation information provided in CREFL\")\n        # height = np.zeros(lon.shape, dtype=np.float)\n        height = 0.\n    else:\n        LOG.debug(\"Using average elevation information provided to CREFL\")\n        lat[(lat <= -90) | (lat >= 90)] = np.nan\n        lon[(lon <= -180) | (lon >= 180)] = np.nan\n        row = ((90.0 - lat) * avg_elevation.shape[0] / 180.0).astype(np.int32)\n        col = ((lon + 180.0) * avg_elevation.shape[1] / 360.0).astype(np.int32)\n        space_mask = da.isnull(lon) | da.isnull(lat)\n        row[space_mask] = 0\n        col[space_mask] = 0\n\n        height = da.map_blocks(_avg_elevation_index, avg_elevation, row, col, dtype=avg_elevation.dtype)\n        height = xr.DataArray(height, dims=['y', 'x'])\n        # negative heights aren't allowed, clip to 0\n        height = height.where((height >= 0.) & ~space_mask, 0.0)\n        del lat, lon, row, col\n    mus = da.cos(da.deg2rad(solar_zenith))\n    mus = mus.where(mus >= 0)\n    muv = da.cos(da.deg2rad(sensor_zenith))\n    phi = solar_azimuth - sensor_azimuth\n\n    if use_abi:\n        LOG.debug(\"Using ABI CREFL algorithm\")\n        a_O3 = [268.45, 0.5, 115.42, -3.2922]\n        a_H2O = [0.0311, 0.1, 92.471, -1.3814]\n        a_O2 = [0.4567, 0.007, 96.4884, -1.6970]\n        G_O3 = G_calc(solar_zenith, a_O3) + G_calc(sensor_zenith, a_O3)\n        G_H2O = G_calc(solar_zenith, a_H2O) + G_calc(sensor_zenith, a_H2O)\n        G_O2 = G_calc(solar_zenith, a_O2) + G_calc(sensor_zenith, a_O2)\n        # Note: bh2o values are actually ao2 values for abi\n        sphalb, rhoray, TtotraytH2O, tOG = get_atm_variables_abi(mus, muv, phi, height, G_O3, G_H2O, G_O2, *coeffs)\n    else:\n        LOG.debug(\"Using original VIIRS CREFL algorithm\")\n        sphalb, rhoray, TtotraytH2O, tOG = get_atm_variables(mus, muv, phi, height, *coeffs)\n\n    del solar_azimuth, solar_zenith, sensor_zenith, sensor_azimuth\n    # Note: Assume that fill/invalid values are either NaN or we are dealing\n    # with masked arrays\n    if percent:\n        corr_refl = ((refl / 100.) / tOG - rhoray) / TtotraytH2O\n    else:\n        corr_refl = (refl / tOG - rhoray) / TtotraytH2O\n    corr_refl /= (1.0 + corr_refl * sphalb)\n    return corr_refl.clip(REFLMIN, REFLMAX)", "response": "Run main crefl algorithm.\n\n    All input parameters are per-pixel values meaning they are the same size\n    and shape as the input reflectance data, unless otherwise stated.\n\n    :param reflectance_bands: tuple of reflectance band arrays\n    :param coefficients: tuple of coefficients for each band (see `get_coefficients`)\n    :param lon: input swath longitude array\n    :param lat: input swath latitude array\n    :param sensor_azimuth: input swath sensor azimuth angle array\n    :param sensor_zenith: input swath sensor zenith angle array\n    :param solar_azimuth: input swath solar azimuth angle array\n    :param solar_zenith: input swath solar zenith angle array\n    :param avg_elevation: average elevation (usually pre-calculated and stored in CMGDEM.hdf)\n    :param percent: True if input reflectances are on a 0-100 scale instead of 0-1 scale (default: False)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_day_night_masks(solarZenithAngle,\n                         good_mask,\n                         highAngleCutoff,\n                         lowAngleCutoff,\n                         stepsDegrees=None):\n    \"\"\"\n    given information on the solarZenithAngle for each point,\n    generate masks defining where the day, night, and mixed regions are\n\n    optionally provide the highAngleCutoff and lowAngleCutoff that define\n    the limits of the terminator region (if no cutoffs are given the\n    DEFAULT_HIGH_ANGLE and DEFAULT_LOW_ANGLE will be used)\n\n    optionally provide the stepsDegrees that define how many degrees each\n    \"mixed\" mask in the terminator region should be (if no stepsDegrees is\n    given, the whole terminator region will be one mask)\n    \"\"\"\n    # if the caller passes None, we're only doing one step\n    stepsDegrees = highAngleCutoff - lowAngleCutoff if stepsDegrees is None else stepsDegrees\n\n    night_mask = (solarZenithAngle > highAngleCutoff) & good_mask\n    day_mask = (solarZenithAngle <= lowAngleCutoff) & good_mask\n    mixed_mask = []\n    steps = list(range(lowAngleCutoff, highAngleCutoff + 1, stepsDegrees))\n    if steps[-1] >= highAngleCutoff:\n        steps[-1] = highAngleCutoff\n    steps = zip(steps, steps[1:])\n    for i, j in steps:\n        LOG.debug(\"Processing step %d to %d\" % (i, j))\n        tmp = (solarZenithAngle > i) & (solarZenithAngle <= j) & good_mask\n        if tmp.any():\n            LOG.debug(\"Adding step %d to %d\" % (i, j))\n            # log.debug(\"Points to process in this range: \" + str(np.sum(tmp)))\n            mixed_mask.append(tmp)\n        del tmp\n\n    return day_mask, mixed_mask, night_mask", "response": "Generate day night and mixed masks for a solar Zenith angle."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef histogram_equalization(\n        data,\n        mask_to_equalize,\n        number_of_bins=1000,\n        std_mult_cutoff=4.0,\n        do_zerotoone_normalization=True,\n        valid_data_mask=None,\n\n        # these are theoretically hooked up, but not useful with only one\n        # equalization\n        clip_limit=None,\n        slope_limit=None,\n\n        # these parameters don't do anything, they're just here to mirror those\n        # in the other call\n        do_log_scale=False,\n        log_offset=None,\n        local_radius_px=None,\n        out=None):\n    \"\"\"\n    Perform a histogram equalization on the data selected by mask_to_equalize.\n    The data will be separated into number_of_bins levels for equalization and\n    outliers beyond +/- std_mult_cutoff*std will be ignored.\n\n    If do_zerotoone_normalization is True the data selected by mask_to_equalize\n    will be returned in the 0 to 1 range. Otherwise the data selected by\n    mask_to_equalize will be returned in the 0 to number_of_bins range.\n\n    Note: the data will be changed in place.\n    \"\"\"\n\n    out = out if out is not None else data.copy()\n    mask_to_use = mask_to_equalize if valid_data_mask is None else valid_data_mask\n\n    LOG.debug(\"determining DNB data range for histogram equalization\")\n    avg = np.mean(data[mask_to_use])\n    std = np.std(data[mask_to_use])\n    # limit our range to +/- std_mult_cutoff*std; e.g. the default\n    # std_mult_cutoff is 4.0 so about 99.8% of the data\n    concervative_mask = (data < (avg + std * std_mult_cutoff)) & (\n        data > (avg - std * std_mult_cutoff)) & mask_to_use\n\n    LOG.debug(\"running histogram equalization\")\n    cumulative_dist_function, temp_bins = _histogram_equalization_helper(\n        data[concervative_mask],\n        number_of_bins,\n        clip_limit=clip_limit,\n        slope_limit=slope_limit)\n\n    # linearly interpolate using the distribution function to get the new\n    # values\n    out[mask_to_equalize] = np.interp(data[mask_to_equalize], temp_bins[:-1],\n                                      cumulative_dist_function)\n\n    # if we were asked to, normalize our data to be between zero and one,\n    # rather than zero and number_of_bins\n    if do_zerotoone_normalization:\n        _linear_normalization_from_0to1(out, mask_to_equalize, number_of_bins)\n\n    return out", "response": "This function takes a 2D array of data and returns a 2D array of the same size as the input data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nequalize the provided data (in the mask_to_equalize) using adaptive histogram equalization. tiles of width/height (2 * local_radius_px + 1) will be calculated and results for each pixel will be bilinerarly interpolated from the nearest 4 tiles when pixels fall near the edge of the image (there is no adjacent tile) the resultant interpolated sum from the available tiles will be multipled to account for the weight of any missing tiles:: pixel total interpolated value = pixel available interpolated value / (1 - missing interpolation weight) if ``do_zerotoone_normalization`` is True the data will be scaled so that all data in the mask_to_equalize falls between 0 and 1; otherwise the data in mask_to_equalize will all fall between 0 and number_of_bins Returns: The equalized data", "response": "def local_histogram_equalization(data, mask_to_equalize, valid_data_mask=None, number_of_bins=1000,\n                                 std_mult_cutoff=3.0,\n                                 do_zerotoone_normalization=True,\n                                 local_radius_px=300,\n                                 clip_limit=60.0,  # 20.0,\n                                 slope_limit=3.0,  # 0.5,\n                                 do_log_scale=True,\n                                 # can't take the log of zero, so the offset\n                                 # may be needed; pass 0.0 if your data doesn't\n                                 # need it\n                                 log_offset=0.00001,\n                                 out=None\n                                 ):\n    \"\"\"Equalize the provided data (in the mask_to_equalize) using adaptive histogram equalization.\n\n    tiles of width/height (2 * local_radius_px + 1) will be calculated and results for each pixel will be bilinerarly\n    interpolated from the nearest 4 tiles when pixels fall near the edge of the image (there is no adjacent tile) the\n    resultant interpolated sum from the available tiles will be multipled to account for the weight of any missing\n    tiles::\n\n        pixel total interpolated value = pixel available interpolated value / (1 - missing interpolation weight)\n\n    if ``do_zerotoone_normalization`` is True the data will be scaled so that all data in the mask_to_equalize falls\n    between 0 and 1; otherwise the data in mask_to_equalize will all fall between 0 and number_of_bins\n\n    Returns:\n\n        The equalized data\n\n    \"\"\"\n\n    out = out if out is not None else np.zeros_like(data)\n    # if we don't have a valid mask, use the mask of what we should be\n    # equalizing\n    if valid_data_mask is None:\n        valid_data_mask = mask_to_equalize\n\n    # calculate some useful numbers for our tile math\n    total_rows = data.shape[0]\n    total_cols = data.shape[1]\n    tile_size = int((local_radius_px * 2.0) + 1.0)\n    row_tiles = int(total_rows / tile_size) if (\n        (total_rows % tile_size) == 0) else int(total_rows / tile_size) + 1\n    col_tiles = int(total_cols / tile_size) if (\n        (total_cols % tile_size) == 0) else int(total_cols / tile_size) + 1\n\n    # an array of our distribution functions for equalization\n    all_cumulative_dist_functions = [[]]\n    # an array of our bin information for equalization\n    all_bin_information = [[]]\n\n    # loop through our tiles and create the histogram equalizations for each\n    # one\n    for num_row_tile in range(row_tiles):\n\n        # make sure we have enough rows available to store info on this next\n        # row of tiles\n        if len(all_cumulative_dist_functions) <= num_row_tile:\n            all_cumulative_dist_functions.append([])\n        if len(all_bin_information) <= num_row_tile:\n            all_bin_information.append([])\n\n        # go through each tile in this row and calculate the equalization\n        for num_col_tile in range(col_tiles):\n\n            # calculate the range for this tile (min is inclusive, max is\n            # exclusive)\n            min_row = num_row_tile * tile_size\n            max_row = min_row + tile_size\n            min_col = num_col_tile * tile_size\n            max_col = min_col + tile_size\n\n            # for speed of calculation, pull out the mask of pixels that should\n            # be used to calculate the histogram\n            mask_valid_data_in_tile = valid_data_mask[min_row:max_row, min_col:\n                                                      max_col]\n\n            # if we have any valid data in this tile, calculate a histogram equalization for this tile\n            # (note: even if this tile does no fall in the mask_to_equalize, it's histogram may be used by other tiles)\n            cumulative_dist_function, temp_bins = None, None\n            if mask_valid_data_in_tile.any():\n\n                # use all valid data in the tile, so separate sections will\n                # blend cleanly\n                temp_valid_data = data[min_row:max_row, min_col:max_col][\n                    mask_valid_data_in_tile]\n                temp_valid_data = temp_valid_data[\n                    temp_valid_data >= 0\n                ]  # TEMP, testing to see if negative data is messing everything up\n                # limit the contrast by only considering data within a certain\n                # range of the average\n                if std_mult_cutoff is not None:\n                    avg = np.mean(temp_valid_data)\n                    std = np.std(temp_valid_data)\n                    # limit our range to avg +/- std_mult_cutoff*std; e.g. the\n                    # default std_mult_cutoff is 4.0 so about 99.8% of the data\n                    concervative_mask = (\n                        temp_valid_data < (avg + std * std_mult_cutoff)) & (\n                            temp_valid_data > (avg - std * std_mult_cutoff))\n                    temp_valid_data = temp_valid_data[concervative_mask]\n\n                # if we are taking the log of our data, do so now\n                if do_log_scale:\n                    temp_valid_data = np.log(temp_valid_data + log_offset)\n\n                # do the histogram equalization and get the resulting\n                # distribution function and bin information\n                if temp_valid_data.size > 0:\n                    cumulative_dist_function, temp_bins = _histogram_equalization_helper(\n                        temp_valid_data,\n                        number_of_bins,\n                        clip_limit=clip_limit,\n                        slope_limit=slope_limit)\n\n            # hang on to our equalization related information for use later\n            all_cumulative_dist_functions[num_row_tile].append(\n                cumulative_dist_function)\n            all_bin_information[num_row_tile].append(temp_bins)\n\n    # get the tile weight array so we can use it to interpolate our data\n    tile_weights = _calculate_weights(tile_size)\n\n    # now loop through our tiles and linearly interpolate the equalized\n    # versions of the data\n    for num_row_tile in range(row_tiles):\n        for num_col_tile in range(col_tiles):\n\n            # calculate the range for this tile (min is inclusive, max is\n            # exclusive)\n            min_row = num_row_tile * tile_size\n            max_row = min_row + tile_size\n            min_col = num_col_tile * tile_size\n            max_col = min_col + tile_size\n\n            # for convenience, pull some of these tile sized chunks out\n            temp_all_data = data[min_row:max_row, min_col:max_col].copy()\n            temp_mask_to_equalize = mask_to_equalize[min_row:max_row, min_col:\n                                                     max_col]\n            temp_all_valid_data_mask = valid_data_mask[min_row:max_row,\n                                                       min_col:max_col]\n\n            # if we have any data in this tile, calculate our weighted sum\n            if temp_mask_to_equalize.any():\n                if do_log_scale:\n                    temp_all_data[temp_all_valid_data_mask] = np.log(\n                        temp_all_data[temp_all_valid_data_mask] + log_offset)\n                temp_data_to_equalize = temp_all_data[temp_mask_to_equalize]\n                temp_all_valid_data = temp_all_data[temp_all_valid_data_mask]\n\n                # a place to hold our weighted sum that represents the interpolated contributions\n                # of the histogram equalizations from the surrounding tiles\n                temp_sum = np.zeros_like(temp_data_to_equalize)\n\n                # how much weight were we unable to use because those tiles\n                # fell off the edge of the image?\n                unused_weight = np.zeros(temp_data_to_equalize.shape,\n                                         dtype=tile_weights.dtype)\n\n                # loop through all the surrounding tiles and process their\n                # contributions to this tile\n                for weight_row in range(3):\n                    for weight_col in range(3):\n                        # figure out which adjacent tile we're processing (in\n                        # overall tile coordinates instead of relative to our\n                        # current tile)\n                        calculated_row = num_row_tile - 1 + weight_row\n                        calculated_col = num_col_tile - 1 + weight_col\n                        tmp_tile_weights = tile_weights[\n                            weight_row, weight_col][np.where(\n                                temp_mask_to_equalize)]\n\n                        # if we're inside the tile array and the tile we're\n                        # processing has a histogram equalization for us to\n                        # use, process it\n                        if ((calculated_row >= 0) and\n                            (calculated_row < row_tiles) and\n                            (calculated_col >= 0) and\n                            (calculated_col < col_tiles) and (\n                                all_bin_information[calculated_row][\n                                    calculated_col] is not None) and\n                            (all_cumulative_dist_functions[calculated_row][\n                                calculated_col] is not None)):\n\n                            # equalize our current tile using the histogram\n                            # equalization from the tile we're processing\n                            temp_equalized_data = np.interp(\n                                temp_all_valid_data, all_bin_information[\n                                    calculated_row][calculated_col][:-1],\n                                all_cumulative_dist_functions[calculated_row][\n                                    calculated_col])\n                            temp_equalized_data = temp_equalized_data[np.where(\n                                temp_mask_to_equalize[\n                                    temp_all_valid_data_mask])]\n\n                            # add the contribution for the tile we're\n                            # processing to our weighted sum\n                            temp_sum += (temp_equalized_data *\n                                         tmp_tile_weights)\n\n                        # if the tile we're processing doesn't exist, hang onto the weight we\n                        # would have used for it so we can correct that later\n                        else:\n                            unused_weight -= tmp_tile_weights\n\n                # if we have unused weights, scale our values to correct for\n                # that\n                if unused_weight.any():\n                    # TODO, if the mask masks everything out this will be a\n                    # zero!\n                    temp_sum /= unused_weight + 1\n\n                # now that we've calculated the weighted sum for this tile, set\n                # it in our data array\n                out[min_row:max_row, min_col:max_col][\n                    temp_mask_to_equalize] = temp_sum\n                # TEMP, test without using weights\n                # data[min_row:max_row, min_col:max_col][temp_mask_to_equalize] = \\\n                #     np.interp(temp_data_to_equalize, all_bin_information[num_row_tile][num_col_tile][:-1],\n                #               all_cumulative_dist_functions[num_row_tile][num_col_tile])\n\n    # if we were asked to, normalize our data to be between zero and one,\n    # rather than zero and number_of_bins\n    if do_zerotoone_normalization:\n        _linear_normalization_from_0to1(out, mask_to_equalize, number_of_bins)\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _histogram_equalization_helper(valid_data, number_of_bins, clip_limit=None, slope_limit=None):\n\n    # bucket all the selected data using np's histogram function\n    temp_histogram, temp_bins = np.histogram(valid_data, number_of_bins)\n\n    # if we have a clip limit and we should do our clipping before building\n    # the cumulative distribution function, clip off our histogram\n    if clip_limit is not None:\n        # clip our histogram and remember how much we removed\n        pixels_to_clip_at = int(clip_limit *\n                                (valid_data.size / float(number_of_bins)))\n        mask_to_clip = temp_histogram > clip_limit\n        # num_bins_clipped = sum(mask_to_clip)\n        # num_pixels_clipped = sum(temp_histogram[mask_to_clip]) - (num_bins_clipped * pixels_to_clip_at)\n        temp_histogram[mask_to_clip] = pixels_to_clip_at\n\n    # calculate the cumulative distribution function\n    cumulative_dist_function = temp_histogram.cumsum()\n\n    # if we have a clip limit and we should do our clipping after building the\n    # cumulative distribution function, clip off our cdf\n    if slope_limit is not None:\n        # clip our cdf and remember how much we removed\n        pixel_height_limit = int(slope_limit *\n                                 (valid_data.size / float(number_of_bins)))\n        cumulative_excess_height = 0\n        num_clipped_pixels = 0\n        weight_metric = np.zeros(cumulative_dist_function.shape, dtype=float)\n\n        for pixel_index in range(1, cumulative_dist_function.size):\n\n            current_pixel_count = cumulative_dist_function[pixel_index]\n\n            diff_from_acceptable = (\n                current_pixel_count - cumulative_dist_function[pixel_index - 1]\n                - pixel_height_limit - cumulative_excess_height)\n            if diff_from_acceptable < 0:\n                weight_metric[pixel_index] = abs(diff_from_acceptable)\n            cumulative_excess_height += max(diff_from_acceptable, 0)\n            cumulative_dist_function[\n                pixel_index] = current_pixel_count - cumulative_excess_height\n            num_clipped_pixels = num_clipped_pixels + cumulative_excess_height\n\n    # now normalize the overall distribution function\n    cumulative_dist_function = (number_of_bins - 1) * cumulative_dist_function / cumulative_dist_function[-1]\n\n    # return what someone else will need in order to apply the equalization later\n    return cumulative_dist_function, temp_bins", "response": "Calculate the simplest possible histogram equalization using only valid data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _calculate_weights(tile_size):\n\n    # we are essentially making a set of weight masks for an ideal center tile\n    # that has all 8 surrounding tiles available\n\n    # create our empty template tiles\n    template_tile = np.zeros((3, 3, tile_size, tile_size), dtype=np.float32)\n    \"\"\"\n    # TEMP FOR TESTING, create a weight tile that does no interpolation\n    template_tile[1,1] = template_tile[1,1] + 1.0\n    \"\"\"\n\n    # for ease of calculation, figure out the index of the center pixel in a tile\n    # and how far that pixel is from the edge of the tile (in pixel units)\n    center_index = int(tile_size / 2)\n    center_dist = tile_size / 2.0\n\n    # loop through each pixel in the tile and calculate the 9 weights for that pixel\n    # were weights for a pixel are 0.0 they are not set (since the template_tile\n    # starts out as all zeros)\n    for row in range(tile_size):\n        for col in range(tile_size):\n\n            vertical_dist = abs(\n                center_dist - row\n            )  # the distance from our pixel to the center of our tile, vertically\n            horizontal_dist = abs(\n                center_dist - col\n            )  # the distance from our pixel to the center of our tile, horizontally\n\n            # pre-calculate which 3 adjacent tiles will affect our tile\n            # (note: these calculations aren't quite right if center_index equals the row or col)\n            horizontal_index = 0 if col < center_index else 2\n            vertical_index = 0 if row < center_index else 2\n\n            # if this is the center pixel, we only need to use it's own tile\n            # for it\n            if (row is center_index) and (col is center_index):\n\n                # all of the weight for this pixel comes from it's own tile\n                template_tile[1, 1][row, col] = 1.0\n\n            # if this pixel is in the center row, but is not the center pixel\n            # we're going to need to linearly interpolate it's tile and the\n            # tile that is horizontally nearest to it\n            elif (row is center_index) and (col is not center_index):\n\n                # linear interp horizontally\n\n                beside_weight = horizontal_dist / tile_size  # the weight from the adjacent tile\n                local_weight = (\n                    tile_size -\n                    horizontal_dist) / tile_size  # the weight from this tile\n\n                # set the weights for the two relevant tiles\n                template_tile[1, 1][row, col] = local_weight\n                template_tile[1, horizontal_index][row, col] = beside_weight\n\n            # if this pixel is in the center column, but is not the center pixel\n            # we're going to need to linearly interpolate it's tile and the\n            # tile that is vertically nearest to it\n            elif (row is not center_index) and (col is center_index):\n\n                # linear interp vertical\n\n                beside_weight = vertical_dist / tile_size  # the weight from the adjacent tile\n                local_weight = (\n                    tile_size -\n                    vertical_dist) / tile_size  # the weight from this tile\n\n                # set the weights for the two relevant tiles\n                template_tile[1, 1][row, col] = local_weight\n                template_tile[vertical_index, 1][row, col] = beside_weight\n\n            # if the pixel is in one of the four quadrants that are above or below the center\n            # row and column, we need to bilinearly interpolate it between the\n            # nearest four tiles\n            else:\n\n                # bilinear interpolation\n\n                local_weight = ((tile_size - vertical_dist) / tile_size) * (\n                    (tile_size - horizontal_dist) /\n                    tile_size)  # the weight from this tile\n                vertical_weight = ((vertical_dist) / tile_size) * (\n                    (tile_size - horizontal_dist) / tile_size\n                )  # the weight from the vertically   adjacent tile\n                horizontal_weight = (\n                    (tile_size - vertical_dist) / tile_size) * (\n                        (horizontal_dist) / tile_size\n                )  # the weight from the horizontally adjacent tile\n                diagonal_weight = ((vertical_dist) / tile_size) * (\n                    (horizontal_dist) / tile_size\n                )  # the weight from the diagonally   adjacent tile\n\n                # set the weights for the four relevant tiles\n                template_tile[1, 1, row, col] = local_weight\n                template_tile[vertical_index, 1, row, col] = vertical_weight\n                template_tile[1, horizontal_index, row,\n                              col] = horizontal_weight\n                template_tile[vertical_index, horizontal_index, row,\n                              col] = diagonal_weight\n\n    # return the weights for an ideal center tile\n    return template_tile", "response": "calculate the weights for the equalizations of a single image in a single image in a single image"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndoing a linear normalization so all data is in the 0 to 1 range. This is a sloppy but fast calculation that relies on parameters giving it the correct theoretical current max and min so it can scale the data accordingly.", "response": "def _linear_normalization_from_0to1(\n        data,\n        mask,\n        theoretical_max,\n        theoretical_min=0,\n        message=\"normalizing equalized data to fit in 0 to 1 range\"):\n    \"\"\"Do a linear normalization so all data is in the 0 to 1 range.\n\n    This is a sloppy but fast calculation that relies on parameters giving it\n    the correct theoretical current max and min so it can scale the data\n    accordingly.\n    \"\"\"\n\n    LOG.debug(message)\n    if theoretical_min != 0:\n        data[mask] = data[mask] - theoretical_min\n        theoretical_max = theoretical_max - theoretical_min\n    data[mask] = data[mask] / theoretical_max"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _run_dnb_normalization(self, dnb_data, sza_data):\n        # convert dask arrays to DataArray objects\n        dnb_data = xr.DataArray(dnb_data, dims=('y', 'x'))\n        sza_data = xr.DataArray(sza_data, dims=('y', 'x'))\n\n        good_mask = ~(dnb_data.isnull() | sza_data.isnull())\n        output_dataset = dnb_data.where(good_mask)\n        # we only need the numpy array\n        output_dataset = output_dataset.values.copy()\n        dnb_data = dnb_data.values\n        sza_data = sza_data.values\n\n        day_mask, mixed_mask, night_mask = make_day_night_masks(\n            sza_data,\n            good_mask.values,\n            self.high_angle_cutoff,\n            self.low_angle_cutoff,\n            stepsDegrees=self.mixed_degree_step)\n\n        did_equalize = False\n        if day_mask.any():\n            LOG.debug(\"Histogram equalizing DNB day data...\")\n            histogram_equalization(dnb_data, day_mask, out=output_dataset)\n            did_equalize = True\n        if mixed_mask:\n            for mask in mixed_mask:\n                if mask.any():\n                    LOG.debug(\"Histogram equalizing DNB mixed data...\")\n                    histogram_equalization(dnb_data, mask, out=output_dataset)\n                    did_equalize = True\n        if night_mask.any():\n            LOG.debug(\"Histogram equalizing DNB night data...\")\n            histogram_equalization(dnb_data, night_mask, out=output_dataset)\n            did_equalize = True\n\n        if not did_equalize:\n            raise RuntimeError(\"No valid data found to histogram equalize\")\n\n        return output_dataset", "response": "This method scales the DNB data using a histogram equalization method."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nscale the DNB data using a adaptive histogram equalization method. Args: dnb_data (ndarray): Day/Night Band data array sza_data (ndarray): Solar Zenith Angle data array", "response": "def _run_dnb_normalization(self, dnb_data, sza_data):\n        \"\"\"Scale the DNB data using a adaptive histogram equalization method.\n\n        Args:\n            dnb_data (ndarray): Day/Night Band data array\n            sza_data (ndarray): Solar Zenith Angle data array\n\n        \"\"\"\n        # convert dask arrays to DataArray objects\n        dnb_data = xr.DataArray(dnb_data, dims=('y', 'x'))\n        sza_data = xr.DataArray(sza_data, dims=('y', 'x'))\n\n        good_mask = ~(dnb_data.isnull() | sza_data.isnull())\n        # good_mask = ~(dnb_data.mask | sza_data.mask)\n        output_dataset = dnb_data.where(good_mask)\n        # we only need the numpy array\n        output_dataset = output_dataset.values.copy()\n        dnb_data = dnb_data.values\n        sza_data = sza_data.values\n\n        day_mask, mixed_mask, night_mask = make_day_night_masks(\n            sza_data,\n            good_mask.values,\n            self.high_angle_cutoff,\n            self.low_angle_cutoff,\n            stepsDegrees=self.mixed_degree_step)\n\n        did_equalize = False\n        has_multi_times = len(mixed_mask) > 0\n        if day_mask.any():\n            did_equalize = True\n            if self.adaptive_day == \"always\" or (\n                    has_multi_times and self.adaptive_day == \"multiple\"):\n                LOG.debug(\"Adaptive histogram equalizing DNB day data...\")\n                local_histogram_equalization(\n                    dnb_data,\n                    day_mask,\n                    valid_data_mask=good_mask.values,\n                    local_radius_px=self.day_radius_pixels,\n                    out=output_dataset)\n            else:\n                LOG.debug(\"Histogram equalizing DNB day data...\")\n                histogram_equalization(dnb_data,\n                                       day_mask,\n                                       out=output_dataset)\n        if mixed_mask:\n            for mask in mixed_mask:\n                if mask.any():\n                    did_equalize = True\n                    if self.adaptive_mixed == \"always\" or (\n                            has_multi_times and\n                            self.adaptive_mixed == \"multiple\"):\n                        LOG.debug(\n                            \"Adaptive histogram equalizing DNB mixed data...\")\n                        local_histogram_equalization(\n                            dnb_data,\n                            mask,\n                            valid_data_mask=good_mask.values,\n                            local_radius_px=self.mixed_radius_pixels,\n                            out=output_dataset)\n                    else:\n                        LOG.debug(\"Histogram equalizing DNB mixed data...\")\n                        histogram_equalization(dnb_data,\n                                               day_mask,\n                                               out=output_dataset)\n        if night_mask.any():\n            did_equalize = True\n            if self.adaptive_night == \"always\" or (\n                    has_multi_times and self.adaptive_night == \"multiple\"):\n                LOG.debug(\"Adaptive histogram equalizing DNB night data...\")\n                local_histogram_equalization(\n                    dnb_data,\n                    night_mask,\n                    valid_data_mask=good_mask.values,\n                    local_radius_px=self.night_radius_pixels,\n                    out=output_dataset)\n            else:\n                LOG.debug(\"Histogram equalizing DNB night data...\")\n                histogram_equalization(dnb_data,\n                                       night_mask,\n                                       out=output_dataset)\n\n        if not did_equalize:\n            raise RuntimeError(\"No valid data found to histogram equalize\")\n\n        return output_dataset"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef available_datasets(self):\n        sensor = self.get_sensor(self['/attr/sensor'])\n        nadir_resolution = self.get_nadir_resolution(sensor)\n        for var_name, val in self.file_content.items():\n            if isinstance(val, SDS):\n                ds_info = {\n                    'file_type': self.filetype_info['file_type'],\n                    'resolution': nadir_resolution,\n                }\n                if self._is_polar():\n                    ds_info['coordinates'] = ['longitude', 'latitude']\n                yield DatasetID(name=var_name, resolution=nadir_resolution), ds_info", "response": "Automatically determine datasets provided by this file"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads from recent PUG format where axes are in meters", "response": "def _read_pug_fixed_grid(projection, distance_multiplier=1.0):\n        \"\"\"Read from recent PUG format, where axes are in meters\n        \"\"\"\n        a = projection.semi_major_axis\n        h = projection.perspective_point_height\n        b = projection.semi_minor_axis\n\n        lon_0 = projection.longitude_of_projection_origin\n        sweep_axis = projection.sweep_angle_axis[0]\n\n        proj_dict = {'a': float(a) * distance_multiplier,\n                     'b': float(b) * distance_multiplier,\n                     'lon_0': float(lon_0),\n                     'h': float(h) * distance_multiplier,\n                     'proj': 'geos',\n                     'units': 'm',\n                     'sweep': sweep_axis}\n        return proj_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_axi_fixed_grid(self, l1b_attr):\n        LOG.debug(\"looking for corresponding input file for {0}\"\n                  \" to act as fixed grid navigation donor\".format(l1b_attr))\n        l1b_path = self._find_input_nc(l1b_attr)\n        LOG.info(\"Since CLAVR-x does not include fixed-grid parameters,\"\n                 \" using input file {0} as donor\".format(l1b_path))\n        l1b = netCDF4.Dataset(l1b_path)\n        proj = None\n        proj_var = l1b.variables.get(\"Projection\", None)\n        if proj_var is not None:\n            # hsd2nc input typically used by CLAVR-x uses old-form km for axes/height\n            LOG.debug(\"found hsd2nc-style draft PUG fixed grid specification\")\n            proj = self._read_pug_fixed_grid(proj_var, 1000.0)\n        if proj is None:  # most likely to come into play for ABI cases\n            proj_var = l1b.variables.get(\"goes_imager_projection\", None)\n            if proj_var is not None:\n                LOG.debug(\"found cmip-style final PUG fixed grid specification\")\n                proj = self._read_pug_fixed_grid(proj_var)\n        if not proj:\n            raise ValueError(\"Unable to recover projection information\"\n                             \" for {0}\".format(self.filename))\n\n        h = float(proj['h'])\n        x, y = l1b['x'], l1b['y']\n        area_extent, ncols, nlines = self._area_extent(x, y, h)\n\n        # LOG.debug(repr(proj))\n        # LOG.debug(repr(area_extent))\n\n        area = geometry.AreaDefinition(\n            'ahi_geos',\n            \"AHI L2 file area\",\n            'ahi_geos',\n            proj,\n            ncols,\n            nlines,\n            np.asarray(area_extent))\n\n        return area", "response": "Read the CLAVR - x fixed - grid parameters from the original input file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_area_def(self, key):\n        if self._is_polar():  # then it doesn't have a fixed grid\n            return super(CLAVRXFileHandler, self).get_area_def(key)\n\n        l1b_att = str(self.file_content.get('/attr/L1B', None))\n        return self._read_axi_fixed_grid(l1b_att)", "response": "Get the area definition of the data at hand."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_yaml_config(self, conf):\n        with open(conf) as fd:\n            self.config = recursive_dict_update(self.config, yaml.load(fd, Loader=UnsafeLoader))", "response": "Load a YAML configuration file and recursively update the overall configuration."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexpanding data according to alignment and expansion.", "response": "def expand_array(data,\n                 scans,\n                 c_align,\n                 c_exp,\n                 scan_size=16,\n                 tpz_size=16,\n                 nties=200,\n                 track_offset=0.5,\n                 scan_offset=0.5):\n    \"\"\"Expand *data* according to alignment and expansion.\"\"\"\n    nties = np.asscalar(nties)\n    tpz_size = np.asscalar(tpz_size)\n    s_scan, s_track = da.meshgrid(np.arange(nties * tpz_size),\n                                  np.arange(scans * scan_size))\n    s_track = (s_track.reshape(scans, scan_size, nties, tpz_size) % scan_size +\n               track_offset) / scan_size\n    s_scan = (s_scan.reshape(scans, scan_size, nties, tpz_size) % tpz_size +\n              scan_offset) / tpz_size\n\n    a_scan = s_scan + s_scan * (1 - s_scan) * c_exp + s_track * (\n        1 - s_track) * c_align\n    a_track = s_track\n\n    data_a = data[:scans * 2:2, np.newaxis, :-1, np.newaxis]\n    data_b = data[:scans * 2:2, np.newaxis, 1:, np.newaxis]\n    data_c = data[1:scans * 2:2, np.newaxis, 1:, np.newaxis]\n    data_d = data[1:scans * 2:2, np.newaxis, :-1, np.newaxis]\n\n    fdata = ((1 - a_track) *\n             ((1 - a_scan) * data_a + a_scan * data_b) + a_track * (\n                 (1 - a_scan) * data_d + a_scan * data_c))\n    return fdata.reshape(scans * scan_size, nties * tpz_size)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a dataset from the database.", "response": "def get_dataset(self, key, info):\n        \"\"\"Load a dataset\n        \"\"\"\n\n        logger.debug('Reading %s.', key.name)\n        if key.name in chans_dict:\n            m_data = self.read_dataset(key, info)\n        else:\n            m_data = self.read_geo(key, info)\n        m_data.attrs.update(info)\n        return m_data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the datetime object of the time since epoch given in days and msecs of day", "response": "def get_cds_time(days, msecs):\n    \"\"\"Get the datetime object of the time since epoch given in days and\n    milliseconds of day\n    \"\"\"\n    return datetime(1958, 1, 1) + timedelta(days=float(days),\n                                            milliseconds=float(msecs))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndecoding 10 bits data into 16 bits words.", "response": "def dec10216(inbuf):\n    \"\"\"Decode 10 bits data into 16 bits words.\n\n    ::\n\n        /*\n         * pack 4 10-bit words in 5 bytes into 4 16-bit words\n         *\n         * 0       1       2       3       4       5\n         * 01234567890123456789012345678901234567890\n         * 0         1         2         3         4\n         */\n        ip = &in_buffer[i];\n        op = &out_buffer[j];\n        op[0] = ip[0]*4 + ip[1]/64;\n        op[1] = (ip[1] & 0x3F)*16 + ip[2]/16;\n        op[2] = (ip[2] & 0x0F)*64 + ip[3]/4;\n        op[3] = (ip[3] & 0x03)*256 +ip[4];\n\n    \"\"\"\n    arr10 = inbuf.astype(np.uint16)\n    arr16_len = int(len(arr10) * 4 / 5)\n    arr10_len = int((arr16_len * 5) / 4)\n    arr10 = arr10[:arr10_len]  # adjust size\n\n    # dask is slow with indexing\n    arr10_0 = arr10[::5]\n    arr10_1 = arr10[1::5]\n    arr10_2 = arr10[2::5]\n    arr10_3 = arr10[3::5]\n    arr10_4 = arr10[4::5]\n\n    arr16_0 = (arr10_0 << 2) + (arr10_1 >> 6)\n    arr16_1 = ((arr10_1 & 63) << 4) + (arr10_2 >> 4)\n    arr16_2 = ((arr10_2 & 15) << 6) + (arr10_3 >> 2)\n    arr16_3 = ((arr10_3 & 3) << 8) + arr10_4\n    arr16 = da.stack([arr16_0, arr16_1, arr16_2, arr16_3], axis=-1).ravel()\n    arr16 = da.rechunk(arr16, arr16.shape[0])\n\n    return arr16"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef chebyshev(coefs, time, domain):\n    return Chebyshev(coefs, domain=domain)(time) - 0.5 * coefs[0]", "response": "Evaluate a Chebyshev polynomial in a log - A domain"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _erads2bt(self, data, channel_name):\n        cal_info = CALIB[self.platform_id][channel_name]\n        alpha = cal_info[\"ALPHA\"]\n        beta = cal_info[\"BETA\"]\n        wavenumber = CALIB[self.platform_id][channel_name][\"VC\"]\n\n        return (self._tl15(data, wavenumber) - beta) / alpha", "response": "Compute the effective radiance."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ir_calibrate(self, data, channel_name, cal_type):\n        if cal_type == 1:\n            # spectral radiances\n            return self._srads2bt(data, channel_name)\n        elif cal_type == 2:\n            # effective radiances\n            return self._erads2bt(data, channel_name)\n        else:\n            raise NotImplementedError('Unknown calibration type')", "response": "Calibrate to brightness temperature."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _srads2bt(self, data, channel_name):\n        a__, b__, c__ = BTFIT[channel_name]\n        wavenumber = CALIB[self.platform_id][channel_name][\"VC\"]\n        temp = self._tl15(data, wavenumber)\n\n        return a__ * temp * temp + b__ * temp + c__", "response": "Computation based on spectral radiance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the L15 temperature.", "response": "def _tl15(self, data, wavenumber):\n        \"\"\"Compute the L15 temperature.\"\"\"\n\n        return ((C2 * wavenumber) /\n                xu.log((1.0 / data) * C1 * wavenumber ** 3 + 1.0))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_dataset(self, key, info=None):\n        if key in self.cache:\n            return self.cache[key]\n\n        logger.debug('Reading {}'.format(key.name))\n        # Get the dataset\n        # Get metadata for given dataset\n        variable = self.nc['/data/{}/measured/effective_radiance'\n                           .format(key.name)]\n        # Convert to xarray\n        radiances = xr.DataArray(np.asarray(variable, np.float32), dims=['y', 'x'])\n        radiances.attrs['scale_factor'] = variable.attrs['scale_factor']\n        radiances.attrs['offset'] = variable.attrs.get('add_offset', 0)\n        radiances.attrs['FillValue'] = variable.attrs['_FillValue']\n        # Set invalid values to NaN\n        radiances.values[radiances == radiances.attrs['FillValue']] = np.nan\n        # Apply scale factor and offset\n        radiances = radiances * (radiances.attrs['scale_factor'] * 1.0) + radiances.attrs['offset']\n\n        # TODO: Calibration is disabled, waiting for calibration parameters from EUMETSAT\n        res = self.calibrate(radiances, key)\n\n        self.cache[key] = res\n        self.nlines, self.ncols = res.shape\n\n        return res", "response": "Load a dataset from the EUMETSAT key."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the area extent for a dataset.", "response": "def calc_area_extent(self, key):\n        \"\"\"Calculate area extent for a dataset.\"\"\"\n        # Calculate the area extent of the swath based on start line and column\n        # information, total number of segments and channel resolution\n        xyres = {500: 22272, 1000: 11136, 2000: 5568}\n        chkres = xyres[key.resolution]\n\n        # Get metadata for given dataset\n        measured = self.nc['/data/{}/measured'.format(key.name)]\n        variable = self.nc['/data/{}/measured/effective_radiance'\n                           .format(key.name)]\n        # Get start/end line and column of loaded swath.\n        self.startline = int(measured['start_position_row'][...])\n        self.endline = int(measured['end_position_row'][...])\n        self.startcol = int(measured['start_position_column'][...])\n        self.endcol = int(measured['end_position_column'][...])\n        self.nlines, self.ncols = variable[:].shape\n\n        logger.debug('Channel {} resolution: {}'.format(key.name, chkres))\n        logger.debug('Row/Cols: {} / {}'.format(self.nlines, self.ncols))\n        logger.debug('Start/End row: {} / {}'.format(self.startline, self.endline))\n        logger.debug('Start/End col: {} / {}'.format(self.startcol, self.endcol))\n        # total_segments = 70\n\n        # Calculate full globe line extent\n        max_y = 5432229.9317116784\n        min_y = -5429229.5285458621\n        full_y = max_y + abs(min_y)\n        # Single swath line extent\n        res_y = full_y / chkres  # Extent per pixel resolution\n        startl = min_y + res_y * self.startline - 0.5 * (res_y)\n        endl = min_y + res_y * self.endline + 0.5 * (res_y)\n        logger.debug('Start / end extent: {} / {}'.format(startl, endl))\n\n        chk_extent = (-5432229.9317116784, endl,\n                      5429229.5285458621, startl)\n        return(chk_extent)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind a valid binary for the xRITDecompress command.", "response": "def get_xritdecompress_cmd():\n    \"\"\"Find a valid binary for the xRITDecompress command.\"\"\"\n    cmd = os.environ.get('XRIT_DECOMPRESS_PATH', None)\n    if not cmd:\n        raise IOError(\"XRIT_DECOMPRESS_PATH is not defined (complete path to xRITDecompress)\")\n\n    question = (\"Did you set the environment variable XRIT_DECOMPRESS_PATH correctly?\")\n    if not os.path.exists(cmd):\n        raise IOError(str(cmd) + \" does not exist!\\n\" + question)\n    elif os.path.isdir(cmd):\n        raise IOError(str(cmd) + \" is a directory!\\n\" + question)\n\n    return cmd"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nanalyse the output of the xRITDecompress command call and return the file.", "response": "def get_xritdecompress_outfile(stdout):\n    \"\"\"Analyse the output of the xRITDecompress command call and return the file.\"\"\"\n    outfile = b''\n    for line in stdout:\n        try:\n            k, v = [x.strip() for x in line.split(b':', 1)]\n        except ValueError:\n            break\n        if k == b'Decompressed file':\n            outfile = v\n            break\n\n    return outfile"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndecompress an XRIT data file and return the path to the decompressed file.", "response": "def decompress(infile, outdir='.'):\n    \"\"\"Decompress an XRIT data file and return the path to the decompressed file.\n\n    It expect to find Eumetsat's xRITDecompress through the environment variable\n    XRIT_DECOMPRESS_PATH.\n    \"\"\"\n    cmd = get_xritdecompress_cmd()\n    infile = os.path.abspath(infile)\n    cwd = os.getcwd()\n    os.chdir(outdir)\n\n    p = Popen([cmd, infile], stdout=PIPE)\n    stdout = BytesIO(p.communicate()[0])\n    status = p.returncode\n    os.chdir(cwd)\n\n    if status != 0:\n        raise IOError(\"xrit_decompress '%s', failed, status=%d\" % (infile, status))\n\n    outfile = get_xritdecompress_outfile(stdout)\n\n    if not outfile:\n        raise IOError(\"xrit_decompress '%s', failed, no output file is generated\" % infile)\n\n    return os.path.join(outdir, outfile.decode('utf-8'))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nopening the file read and get the basic file header info and set the mda dictionary self. mda self. mda_master and self. mda_master attributes.", "response": "def _get_hd(self, hdr_info):\n        \"\"\"Open the file, read and get the basic file header info and set the mda\n           dictionary\n        \"\"\"\n\n        hdr_map, variable_length_headers, text_headers = hdr_info\n\n        with open(self.filename) as fp:\n            total_header_length = 16\n            while fp.tell() < total_header_length:\n                hdr_id = np.fromfile(fp, dtype=common_hdr, count=1)[0]\n                the_type = hdr_map[hdr_id['hdr_id']]\n                if the_type in variable_length_headers:\n                    field_length = int((hdr_id['record_length'] - 3) /\n                                       the_type.itemsize)\n                    current_hdr = np.fromfile(fp,\n                                              dtype=the_type,\n                                              count=field_length)\n                    key = variable_length_headers[the_type]\n                    if key in self.mda:\n                        if not isinstance(self.mda[key], list):\n                            self.mda[key] = [self.mda[key]]\n                        self.mda[key].append(current_hdr)\n                    else:\n                        self.mda[key] = current_hdr\n                elif the_type in text_headers:\n                    field_length = int((hdr_id['record_length'] - 3) /\n                                       the_type.itemsize)\n                    char = list(the_type.fields.values())[0][0].char\n                    new_type = np.dtype(char + str(field_length))\n                    current_hdr = np.fromfile(fp,\n                                              dtype=new_type,\n                                              count=1)[0]\n                    self.mda[text_headers[the_type]] = current_hdr\n                else:\n                    current_hdr = np.fromfile(fp,\n                                              dtype=the_type,\n                                              count=1)[0]\n                    self.mda.update(\n                        dict(zip(current_hdr.dtype.names, current_hdr)))\n\n                total_header_length = self.mda['total_header_length']\n\n        self.mda.setdefault('number_of_bits_per_pixel', 10)\n\n        self.mda['projection_parameters'] = {'a': 6378169.00,\n                                             'b': 6356583.80,\n                                             'h': 35785831.00,\n                                             # FIXME: find a reasonable SSP\n                                             'SSP_longitude': 0.0}\n        self.mda['navigation_parameters'] = {}"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload a dataset from the cache.", "response": "def get_dataset(self, key, info):\n        \"\"\"Load a dataset.\"\"\"\n        # Read bands\n        data = self.read_band(key, info)\n\n        # Convert to xarray\n        xdata = xr.DataArray(data, dims=['y', 'x'])\n\n        return xdata"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_xy_from_linecol(self, line, col, offsets, factors):\n        loff, coff = offsets\n        lfac, cfac = factors\n        x__ = (col - coff) / cfac * 2**16\n        y__ = (line - loff) / lfac * 2**16\n\n        return x__, y__", "response": "Get the intermediate coordinates from line & col."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the area extent of the file.", "response": "def get_area_extent(self, size, offsets, factors, platform_height):\n        \"\"\"Get the area extent of the file.\"\"\"\n        nlines, ncols = size\n        h = platform_height\n\n        # count starts at 1\n        cols = 1 - 0.5\n        lines = 1 - 0.5\n        ll_x, ll_y = self.get_xy_from_linecol(lines, cols, offsets, factors)\n\n        cols += ncols\n        lines += nlines\n        ur_x, ur_y = self.get_xy_from_linecol(lines, cols, offsets, factors)\n\n        return (np.deg2rad(ll_x) * h, np.deg2rad(ll_y) * h,\n                np.deg2rad(ur_x) * h, np.deg2rad(ur_y) * h)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_area_def(self, dsid):\n        cfac = np.int32(self.mda['cfac'])\n        lfac = np.int32(self.mda['lfac'])\n        coff = np.float32(self.mda['coff'])\n        loff = np.float32(self.mda['loff'])\n\n        a = self.mda['projection_parameters']['a']\n        b = self.mda['projection_parameters']['b']\n        h = self.mda['projection_parameters']['h']\n        lon_0 = self.mda['projection_parameters']['SSP_longitude']\n        nlines = int(self.mda['number_of_lines'])\n        ncols = int(self.mda['number_of_columns'])\n\n        area_extent = self.get_area_extent((nlines, ncols),\n                                           (loff, coff),\n                                           (lfac, cfac),\n                                           h)\n\n        proj_dict = {'a': float(a),\n                     'b': float(b),\n                     'lon_0': float(lon_0),\n                     'h': float(h),\n                     'proj': 'geos',\n                     'units': 'm'}\n\n        area = geometry.AreaDefinition(\n            'some_area_name',\n            \"On-the-fly area\",\n            'geosmsg',\n            proj_dict,\n            ncols,\n            nlines,\n            area_extent)\n\n        self.area = area\n        return area", "response": "Get the area definition of the band."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_band(self, key, info):\n        shape = int(np.ceil(self.mda['data_field_length'] / 8.))\n        if self.mda['number_of_bits_per_pixel'] == 16:\n            dtype = '>u2'\n            shape //= 2\n        elif self.mda['number_of_bits_per_pixel'] in [8, 10]:\n            dtype = np.uint8\n        shape = (shape, )\n        data = np.memmap(self.filename, mode='r',\n                         offset=self.mda['total_header_length'],\n                         dtype=dtype,\n                         shape=shape)\n        data = da.from_array(data, chunks=shape[0])\n        if self.mda['number_of_bits_per_pixel'] == 10:\n            data = dec10216(data)\n        data = data.reshape((self.mda['number_of_lines'],\n                             self.mda['number_of_columns']))\n        return data", "response": "Read the data from the MAD file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _wl_dist(wl_a, wl_b):\n    if isinstance(wl_a, tuple):\n        # central wavelength\n        wl_a = wl_a[1]\n    if isinstance(wl_b, tuple):\n        wl_b = wl_b[1]\n    if wl_a is None or wl_b is None:\n        return 1000.\n    return abs(wl_a - wl_b)", "response": "Return the distance between two requested wavelengths."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_best_dataset_key(key, choices):\n    # Choose the wavelength closest to the choice\n    if key.wavelength is not None and choices:\n        # find the dataset with a central wavelength nearest to the\n        # requested wavelength\n        nearest_wl = min([_wl_dist(key.wavelength, x.wavelength)\n                          for x in choices if x.wavelength is not None])\n        choices = [c for c in choices\n                   if _wl_dist(key.wavelength, c.wavelength) == nearest_wl]\n    if key.modifiers is None and choices:\n        num_modifiers = min(len(x.modifiers or tuple()) for x in choices)\n        choices = [c for c in choices if len(\n            c.modifiers or tuple()) == num_modifiers]\n    if key.calibration is None and choices:\n        best_cal = [x.calibration for x in choices if x.calibration]\n        if best_cal:\n            best_cal = min(best_cal, key=lambda x: CALIBRATION_ORDER[x])\n            choices = [c for c in choices if c.calibration == best_cal]\n    if key.resolution is None and choices:\n        low_res = [x.resolution for x in choices if x.resolution]\n        if low_res:\n            low_res = min(low_res)\n            choices = [c for c in choices if c.resolution == low_res]\n    if key.level is None and choices:\n        low_level = [x.level for x in choices if x.level]\n        if low_level:\n            low_level = max(low_level)\n            choices = [c for c in choices if c.level == low_level]\n\n    return choices", "response": "Returns the key of the best dataset from choices based on key."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef filter_keys_by_dataset_id(did, key_container):\n    keys = iter(key_container)\n\n    for key in DATASET_KEYS:\n        if getattr(did, key) is not None:\n            if key == \"wavelength\":\n                keys = [k for k in keys\n                        if (getattr(k, key) is not None and\n                            DatasetID.wavelength_match(getattr(k, key),\n                                                       getattr(did, key)))]\n            else:\n                keys = [k for k in keys\n                        if getattr(k, key) is not None and getattr(k, key)\n                        == getattr(did, key)]\n\n    return keys", "response": "Filter the keys in the key_container by the provided dataset ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_key(key, key_container, num_results=1, best=True,\n            resolution=None, calibration=None, polarization=None,\n            level=None, modifiers=None):\n    \"\"\"Get the fully-specified key best matching the provided key.\n\n    Only the best match is returned if `best` is `True` (default). See\n    `get_best_dataset_key` for more information on how this is determined.\n\n    The `resolution` and other identifier keywords are provided as a\n    convenience to filter by multiple parameters at once without having\n    to filter by multiple `key` inputs.\n\n    Args:\n        key (DatasetID): DatasetID of query parameters to use for\n                         searching. Any parameter that is `None`\n                         is considered a wild card and any match is\n                         accepted.\n        key_container (dict or set): Container of DatasetID objects that\n                                     uses hashing to quickly access items.\n        num_results (int): Number of results to return. Use `0` for all\n                           matching results. If `1` then the single matching\n                           key is returned instead of a list of length 1.\n                           (default: 1)\n        best (bool): Sort results to get \"best\" result first\n                     (default: True). See `get_best_dataset_key` for details.\n        resolution (float, int, or list): Resolution of the dataset in\n                                          dataset units (typically\n                                          meters). This can also be a\n                                          list of these numbers.\n        calibration (str or list): Dataset calibration\n                                   (ex.'reflectance'). This can also be a\n                                   list of these strings.\n        polarization (str or list): Dataset polarization\n                                    (ex.'V'). This can also be a\n                                    list of these strings.\n        level (number or list): Dataset level (ex. 100). This can also be a\n                                list of these numbers.\n        modifiers (list): Modifiers applied to the dataset. Unlike\n                          resolution and calibration this is the exact\n                          desired list of modifiers for one dataset, not\n                          a list of possible modifiers.\n\n\n    Returns (list or DatasetID): Matching key(s)\n\n    Raises: KeyError if no matching results or if more than one result is\n            found when `num_results` is `1`.\n\n    \"\"\"\n    if isinstance(key, numbers.Number):\n        # we want this ID to act as a query so we set modifiers to None\n        # meaning \"we don't care how many modifiers it has\".\n        key = DatasetID(wavelength=key, modifiers=None)\n    elif isinstance(key, (str, six.text_type)):\n        # ID should act as a query (see wl comment above)\n        key = DatasetID(name=key, modifiers=None)\n    elif not isinstance(key, DatasetID):\n        raise ValueError(\"Expected 'DatasetID', str, or number dict key, \"\n                         \"not {}\".format(str(type(key))))\n\n    res = filter_keys_by_dataset_id(key, key_container)\n\n    # further filter by other parameters\n    if resolution is not None:\n        if not isinstance(resolution, (list, tuple)):\n            resolution = (resolution, )\n        res = [k for k in res\n               if k.resolution is not None and k.resolution in resolution]\n    if polarization is not None:\n        if not isinstance(polarization, (list, tuple)):\n            polarization = (polarization, )\n        res = [k for k in res\n               if k.polarization is not None and k.polarization in\n               polarization]\n    if calibration is not None:\n        if not isinstance(calibration, (list, tuple)):\n            calibration = (calibration, )\n        res = [k for k in res\n               if k.calibration is not None and k.calibration in calibration]\n    if level is not None:\n        if not isinstance(level, (list, tuple)):\n            level = (level, )\n        res = [k for k in res\n               if k.level is not None and k.level in level]\n    if modifiers is not None:\n        res = [k for k in res\n               if k.modifiers is not None and k.modifiers == modifiers]\n\n    if best:\n        res = get_best_dataset_key(key, res)\n\n    if num_results == 1 and not res:\n        raise KeyError(\"No dataset matching '{}' found\".format(str(key)))\n    elif num_results == 1 and len(res) != 1:\n        raise TooManyResults(\"No unique dataset matching {}\".format(str(key)))\n    elif num_results == 1:\n        return res[0]\n    elif num_results == 0:\n        return res\n    else:\n        return res[:num_results]", "response": "Get the fully - specified key best matching the provided key."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngroup files by file pattern information.", "response": "def group_files(files_to_sort, reader=None, time_threshold=10,\n                group_keys=None, ppp_config_dir=None, reader_kwargs=None):\n    \"\"\"Group series of files by file pattern information.\n\n    By default this will group files by their filename ``start_time``\n    assuming it exists in the pattern. By passing the individual\n    dictionaries returned by this function to the Scene classes'\n    ``filenames``, a series `Scene` objects can be easily created.\n\n    .. versionadded:: 0.12\n\n    Args:\n        files_to_sort (iterable): File paths to sort in to group\n        reader (str): Reader whose file patterns should be used to sort files.\n            This\n        time_threshold (int): Number of seconds used to consider time elements\n            in a group as being equal. For example, if the 'start_time' item\n            is used to group files then any time within `time_threshold`\n            seconds of the first file's 'start_time' will be seen as occurring\n            at the same time.\n        group_keys (list or tuple): File pattern information to use to group\n            files. Keys are sorted in order and only the first key is used when\n            comparing datetime elements with `time_threshold` (see above). This\n            means it is recommended that datetime values should only come from\n            the first key in ``group_keys``. Otherwise, there is a good chance\n            that files will not be grouped properly (datetimes being barely\n            unequal). Defaults to a reader's ``group_keys`` configuration (set\n            in YAML), otherwise ``('start_time',)``.\n        ppp_config_dir (str): Root usser configuration directory for Satpy.\n            This will be deprecated in the future, but is here for consistency\n            with other Satpy features.\n        reader_kwargs (dict): Additional keyword arguments to pass to reader\n            creation.\n\n    Returns:\n        List of dictionaries mapping 'reader' to a list of filenames.\n        Each of these dictionaries can be passed as ``filenames`` to\n        a `Scene` object.\n\n    \"\"\"\n    # FUTURE: Find the best reader for each filename using `find_files_and_readers`\n    if reader is None:\n        raise ValueError(\"'reader' keyword argument is required.\")\n    elif not isinstance(reader, (list, tuple)):\n        reader = [reader]\n\n    # FUTURE: Handle multiple readers\n    reader = reader[0]\n    reader_configs = list(configs_for_reader(reader, ppp_config_dir))[0]\n    reader_kwargs = reader_kwargs or {}\n    try:\n        reader_instance = load_reader(reader_configs, **reader_kwargs)\n    except (KeyError, IOError, yaml.YAMLError) as err:\n        LOG.info('Cannot use %s', str(reader_configs))\n        LOG.debug(str(err))\n        # if reader and (isinstance(reader, str) or len(reader) == 1):\n        #     # if it is a single reader then give a more usable error\n        #     raise\n        raise\n\n    if group_keys is None:\n        group_keys = reader_instance.info.get('group_keys', ('start_time',))\n    file_keys = []\n    for filetype, filetype_info in reader_instance.sorted_filetype_items():\n        for f, file_info in reader_instance.filename_items_for_filetype(files_to_sort, filetype_info):\n            group_key = tuple(file_info.get(k) for k in group_keys)\n            file_keys.append((group_key, f))\n\n    prev_key = None\n    threshold = timedelta(seconds=time_threshold)\n    file_groups = {}\n    for gk, f in sorted(file_keys):\n        # use first element of key as time identifier (if datetime type)\n        if prev_key is None:\n            is_new_group = True\n            prev_key = gk\n        elif isinstance(gk[0], datetime):\n            # datetimes within threshold difference are \"the same time\"\n            is_new_group = (gk[0] - prev_key[0]) > threshold\n        else:\n            is_new_group = gk[0] != prev_key[0]\n\n        # compare keys for those that are found for both the key and\n        # this is a generator and is not computed until the if statement below\n        # when we know that `prev_key` is not None\n        vals_not_equal = (this_val != prev_val for this_val, prev_val in zip(gk[1:], prev_key[1:])\n                          if this_val is not None and prev_val is not None)\n        # if this is a new group based on the first element\n        if is_new_group or any(vals_not_equal):\n            file_groups[gk] = [f]\n            prev_key = gk\n        else:\n            file_groups[prev_key].append(f)\n    sorted_group_keys = sorted(file_groups)\n    # passable to Scene as 'filenames'\n    return [{reader: file_groups[group_key]} for group_key in sorted_group_keys]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_reader(reader_configs, **reader_kwargs):\n    reader_info = read_reader_config(reader_configs)\n    reader_instance = reader_info['reader'](config_files=reader_configs, **reader_kwargs)\n    return reader_instance", "response": "Import and setup the reader from *reader_configs*."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of available reader names.", "response": "def available_readers(as_dict=False):\n    \"\"\"Available readers based on current configuration.\n\n    Args:\n        as_dict (bool): Optionally return reader information as a dictionary.\n                        Default: False\n\n    Returns: List of available reader names. If `as_dict` is `True` then\n             a list of dictionaries including additionally reader information\n             is returned.\n\n    \"\"\"\n    readers = []\n    for reader_configs in configs_for_reader():\n        try:\n            reader_info = read_reader_config(reader_configs)\n        except (KeyError, IOError, yaml.YAMLError):\n            LOG.warning(\"Could not import reader config from: %s\", reader_configs)\n            LOG.debug(\"Error loading YAML\", exc_info=True)\n            continue\n        readers.append(reader_info if as_dict else reader_info['name'])\n    return readers"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_files_and_readers(start_time=None, end_time=None, base_dir=None,\n                           reader=None, sensor=None, ppp_config_dir=None,\n                           filter_parameters=None, reader_kwargs=None):\n    \"\"\"Find on-disk files matching the provided parameters.\n\n    Use `start_time` and/or `end_time` to limit found filenames by the times\n    in the filenames (not the internal file metadata). Files are matched if\n    they fall anywhere within the range specified by these parameters.\n\n    Searching is **NOT** recursive.\n\n    The returned dictionary can be passed directly to the `Scene` object\n    through the `filenames` keyword argument.\n\n    The behaviour of time-based filtering depends on whether or not the filename\n    contains information about the end time of the data or not:\n\n      - if the end time is not present in the filename, the start time of the filename\n        is used and has to fall between (inclusive) the requested start and end times\n      - otherwise, the timespan of the filename has to overlap the requested timespan\n\n    Args:\n        start_time (datetime): Limit used files by starting time.\n        end_time (datetime): Limit used files by ending time.\n        base_dir (str): The directory to search for files containing the\n                        data to load. Defaults to the current directory.\n        reader (str or list): The name of the reader to use for loading the data or a list of names.\n        sensor (str or list): Limit used files by provided sensors.\n        ppp_config_dir (str): The directory containing the configuration\n                              files for Satpy.\n        filter_parameters (dict): Filename pattern metadata to filter on. `start_time` and `end_time` are\n                                  automatically added to this dictionary. Shortcut for\n                                  `reader_kwargs['filter_parameters']`.\n        reader_kwargs (dict): Keyword arguments to pass to specific reader\n                              instances to further configure file searching.\n\n    Returns: Dictionary mapping reader name string to list of filenames\n\n    \"\"\"\n    if ppp_config_dir is None:\n        ppp_config_dir = get_environ_config_dir()\n    reader_files = {}\n    reader_kwargs = reader_kwargs or {}\n    filter_parameters = filter_parameters or reader_kwargs.get('filter_parameters', {})\n    sensor_supported = False\n\n    if start_time or end_time:\n        filter_parameters['start_time'] = start_time\n        filter_parameters['end_time'] = end_time\n    reader_kwargs['filter_parameters'] = filter_parameters\n\n    for reader_configs in configs_for_reader(reader, ppp_config_dir):\n        try:\n            reader_instance = load_reader(reader_configs, **reader_kwargs)\n        except (KeyError, IOError, yaml.YAMLError) as err:\n            LOG.info('Cannot use %s', str(reader_configs))\n            LOG.debug(str(err))\n            if reader and (isinstance(reader, str) or len(reader) == 1):\n                # if it is a single reader then give a more usable error\n                raise\n            continue\n\n        if not reader_instance.supports_sensor(sensor):\n            continue\n        elif sensor is not None:\n            # sensor was specified and a reader supports it\n            sensor_supported = True\n        loadables = reader_instance.select_files_from_directory(base_dir)\n        if loadables:\n            loadables = list(\n                reader_instance.filter_selected_filenames(loadables))\n        if loadables:\n            reader_files[reader_instance.name] = list(loadables)\n\n    if sensor and not sensor_supported:\n        raise ValueError(\"Sensor '{}' not supported by any readers\".format(sensor))\n\n    if not reader_files:\n        raise ValueError(\"No supported files found\")\n    return reader_files", "response": "Find files and readers for a given set of parameters."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_key(self, match_key, num_results=1, best=True, **dfilter):\n        return get_key(match_key, self.keys(), num_results=num_results,\n                       best=best, **dfilter)", "response": "Get multiple fully - specified keys that match the provided query."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget value with optional default.", "response": "def get(self, key, default=None):\n        \"\"\"Get value with optional default.\"\"\"\n        try:\n            key = self.get_key(key)\n        except KeyError:\n            return default\n        return super(DatasetDict, self).get(key, default)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\napplying enhancements to dataset dset and return the resulting data array of the image.", "response": "def enhance2dataset(dset):\n    \"\"\"Apply enhancements to dataset *dset* and return the resulting data\n    array of the image.\"\"\"\n    attrs = dset.attrs\n    img = get_enhanced_image(dset)\n    # Clip image data to interval [0.0, 1.0]\n    data = img.data.clip(0.0, 1.0)\n    data.attrs = attrs\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds bands so that they match *bands*", "response": "def add_bands(data, bands):\n    \"\"\"Add bands so that they match *bands*\"\"\"\n    # Add R, G and B bands, remove L band\n    if 'L' in data['bands'].data and 'R' in bands.data:\n        lum = data.sel(bands='L')\n        new_data = xr.concat((lum, lum, lum), dim='bands')\n        new_data['bands'] = ['R', 'G', 'B']\n        data = new_data\n    # Add alpha band\n    if 'A' not in data['bands'].data and 'A' in bands.data:\n        new_data = [data.sel(bands=band) for band in data['bands'].data]\n        # Create alpha band based on a copy of the first \"real\" band\n        alpha = new_data[0].copy()\n        alpha.data = da.ones((data.sizes['y'],\n                              data.sizes['x']),\n                             chunks=new_data[0].chunks)\n        # Rename band to indicate it's alpha\n        alpha['bands'] = 'A'\n        new_data.append(alpha)\n        new_data = xr.concat(new_data, dim='bands')\n        data = new_data\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef zero_missing_data(data1, data2):\n    nans = xu.logical_and(xu.isnan(data1), xu.logical_not(xu.isnan(data2)))\n    return data1.where(~nans, 0)", "response": "Replace NaN values with zeros in data1 if the data is valid in data2."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload all compositor configs for the provided sensor.", "response": "def load_sensor_composites(self, sensor_name):\n        \"\"\"Load all compositor configs for the provided sensor.\"\"\"\n        config_filename = sensor_name + \".yaml\"\n        LOG.debug(\"Looking for composites config file %s\", config_filename)\n        composite_configs = config_search_paths(\n            os.path.join(\"composites\", config_filename),\n            self.ppp_config_dir, check_exists=True)\n        if not composite_configs:\n            LOG.debug(\"No composite config found called {}\".format(\n                config_filename))\n            return\n        self._load_config(composite_configs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_compositors(self, sensor_names):\n        comps = {}\n        mods = {}\n        for sensor_name in sensor_names:\n            if sensor_name not in self.compositors:\n                self.load_sensor_composites(sensor_name)\n            if sensor_name in self.compositors:\n                comps[sensor_name] = DatasetDict(\n                    self.compositors[sensor_name].copy())\n                mods[sensor_name] = self.modifiers[sensor_name].copy()\n        return comps, mods", "response": "Loads all compositor configs for the provided sensors."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitiates the 3. x reflectance derivations.", "response": "def _init_refl3x(self, projectables):\n        \"\"\"Initiate the 3.x reflectance derivations.\"\"\"\n        if not Calculator:\n            LOG.info(\"Couldn't load pyspectral\")\n            raise ImportError(\"No module named pyspectral.near_infrared_reflectance\")\n        _nir, _tb11 = projectables\n        self._refl3x = Calculator(_nir.attrs['platform_name'], _nir.attrs['sensor'], _nir.attrs['name'])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate 3. x reflectance with pyspectral.", "response": "def _get_reflectance(self, projectables, optional_datasets):\n        \"\"\"Calculate 3.x reflectance with pyspectral.\"\"\"\n        _nir, _tb11 = projectables\n        LOG.info('Getting reflective part of %s', _nir.attrs['name'])\n\n        sun_zenith = None\n        tb13_4 = None\n\n        for dataset in optional_datasets:\n            wavelengths = dataset.attrs.get('wavelength', [100., 0, 0])\n            if (dataset.attrs.get('units') == 'K' and\n                    wavelengths[0] <= 13.4 <= wavelengths[2]):\n                tb13_4 = dataset\n            elif (\"standard_name\" in dataset.attrs and\n                  dataset.attrs[\"standard_name\"] == \"solar_zenith_angle\"):\n                sun_zenith = dataset\n\n        # Check if the sun-zenith angle was provided:\n        if sun_zenith is None:\n            if sun_zenith_angle is None:\n                raise ImportError(\"No module named pyorbital.astronomy\")\n            lons, lats = _nir.attrs[\"area\"].get_lonlats_dask(CHUNK_SIZE)\n            sun_zenith = sun_zenith_angle(_nir.attrs['start_time'], lons, lats)\n\n        return self._refl3x.reflectance_from_tbs(sun_zenith, _nir, _tb11, tb_ir_co2=tb13_4)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating the colormap from the raw_palette and the valid_range.", "response": "def build_colormap(palette, dtype, info):\n        \"\"\"Create the colormap from the `raw_palette` and the valid_range.\"\"\"\n        from trollimage.colormap import Colormap\n        sqpalette = np.asanyarray(palette).squeeze() / 255.0\n        if hasattr(palette, 'attrs') and 'palette_meanings' in palette.attrs:\n            meanings = palette.attrs['palette_meanings']\n            iterator = zip(meanings, sqpalette)\n        else:\n            iterator = enumerate(sqpalette[:-1])\n\n        if dtype == np.dtype('uint8'):\n            tups = [(val, tuple(tup))\n                    for (val, tup) in iterator]\n            colormap = Colormap(*tups)\n\n        elif 'valid_range' in info:\n            tups = [(val, tuple(tup))\n                    for (val, tup) in iterator]\n            colormap = Colormap(*tups)\n\n            sf = info.get('scale_factor', np.array(1))\n            colormap.set_range(\n                *info['valid_range'] * sf + info.get('add_offset', 0))\n\n        return colormap, sqpalette"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_band(self, high_res, low_res, color, ratio):\n        if self.high_resolution_band == color:\n            ret = high_res\n        else:\n            ret = low_res * ratio\n            ret.attrs = low_res.attrs.copy()\n        return ret", "response": "Figure out what data should represent this color."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\naverages every 4 elements in a 2D array", "response": "def four_element_average_dask(d):\n        \"\"\"Average every 4 elements (2x2) in a 2D array\"\"\"\n        try:\n            offset = d.attrs['area'].crop_offset\n        except (KeyError, AttributeError):\n            offset = (0, 0)\n\n        res = d.data.map_blocks(_mean4, offset=offset, dtype=d.dtype)\n        return xr.DataArray(res, attrs=d.attrs, dims=d.dims, coords=d.coords)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_area_def(area_name):\n    try:\n        from pyresample import parse_area_file\n    except ImportError:\n        from pyresample.utils import parse_area_file\n    return parse_area_file(get_area_file(), area_name)[0]", "response": "Get the definition of area_name from file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prepare_resampler(source_area, destination_area, resampler=None, **resample_kwargs):\n    if resampler is None:\n        LOG.info(\"Using default KDTree resampler\")\n        resampler = 'kd_tree'\n\n    if isinstance(resampler, BaseResampler):\n        raise ValueError(\"Trying to create a resampler when one already \"\n                         \"exists.\")\n    elif isinstance(resampler, str):\n        resampler_class = RESAMPLERS[resampler]\n    else:\n        resampler_class = resampler\n\n    key = (resampler_class,\n           source_area, destination_area,\n           hash_dict(resample_kwargs))\n    try:\n        resampler_instance = resamplers_cache[key]\n    except KeyError:\n        resampler_instance = resampler_class(source_area, destination_area)\n        resamplers_cache[key] = resampler_instance\n    return key, resampler_instance", "response": "Instantiate and return a resampler."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the fill value of the dataset.", "response": "def get_fill_value(dataset):\n    \"\"\"Get the fill value of the *dataset*, defaulting to np.nan.\"\"\"\n    if np.issubdtype(dataset.dtype, np.integer):\n        return dataset.attrs.get('_FillValue', np.nan)\n    return np.nan"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef resample_dataset(dataset, destination_area, **kwargs):\n    # call the projection stuff here\n    try:\n        source_area = dataset.attrs[\"area\"]\n    except KeyError:\n        LOG.info(\"Cannot reproject dataset %s, missing area info\",\n                 dataset.attrs['name'])\n\n        return dataset\n\n    fill_value = kwargs.pop('fill_value', get_fill_value(dataset))\n    new_data = resample(source_area, dataset, destination_area, fill_value=fill_value, **kwargs)\n    new_attrs = new_data.attrs\n    new_data.attrs = dataset.attrs.copy()\n    new_data.attrs.update(new_attrs)\n    new_data.attrs.update(area=destination_area)\n\n    return new_data", "response": "Resample a dataset and return the resampled version."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the hash for the current resample with the given kwargs", "response": "def get_hash(self, source_geo_def=None, target_geo_def=None, **kwargs):\n        \"\"\"Get hash for the current resample with the given *kwargs*.\"\"\"\n        if source_geo_def is None:\n            source_geo_def = self.source_geo_def\n        if target_geo_def is None:\n            target_geo_def = self.target_geo_def\n        the_hash = source_geo_def.update_hash()\n        target_geo_def.update_hash(the_hash)\n        hash_dict(kwargs, the_hash)\n        return the_hash.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates filename for the cached resampling parameters", "response": "def _create_cache_filename(self, cache_dir=None, **kwargs):\n        \"\"\"Create filename for the cached resampling parameters\"\"\"\n        cache_dir = cache_dir or '.'\n        hash_str = self.get_hash(**kwargs)\n\n        return os.path.join(cache_dir, 'resample_lut-' + hash_str + '.npz')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a KDTree structure and store it for later use. Note: The `mask` keyword should be provided if geolocation may be valid where data points are invalid.", "response": "def precompute(self, mask=None, radius_of_influence=None, epsilon=0,\n                   cache_dir=None, **kwargs):\n        \"\"\"Create a KDTree structure and store it for later use.\n\n        Note: The `mask` keyword should be provided if geolocation may be valid\n        where data points are invalid.\n\n        \"\"\"\n        del kwargs\n        source_geo_def = self.source_geo_def\n\n        if mask is not None and cache_dir is not None:\n            LOG.warning(\"Mask and cache_dir both provided to nearest \"\n                        \"resampler. Cached parameters are affected by \"\n                        \"masked pixels. Will not cache results.\")\n            cache_dir = None\n        # TODO: move this to pyresample\n        if radius_of_influence is None:\n            try:\n                radius_of_influence = source_geo_def.lons.resolution * 3\n            except AttributeError:\n                try:\n                    radius_of_influence = max(abs(source_geo_def.pixel_size_x),\n                                              abs(source_geo_def.pixel_size_y)) * 3\n                except AttributeError:\n                    radius_of_influence = 1000\n\n            except TypeError:\n                radius_of_influence = 10000\n\n        kwargs = dict(source_geo_def=source_geo_def,\n                      target_geo_def=self.target_geo_def,\n                      radius_of_influence=radius_of_influence,\n                      neighbours=1,\n                      epsilon=epsilon)\n\n        if self.resampler is None:\n            # FIXME: We need to move all of this caching logic to pyresample\n            self.resampler = XArrayResamplerNN(**kwargs)\n\n        try:\n            self.load_neighbour_info(cache_dir, mask=mask, **kwargs)\n            LOG.debug(\"Read pre-computed kd-tree parameters\")\n        except IOError:\n            LOG.debug(\"Computing kd-tree parameters\")\n            self.resampler.get_neighbour_info(mask=mask)\n            self.save_neighbour_info(cache_dir, mask=mask, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreassigns various resampler index attributes.", "response": "def _apply_cached_indexes(self, cached_indexes, persist=False):\n        \"\"\"Reassign various resampler index attributes.\"\"\"\n        # cacheable_dict = {}\n        for elt in ['valid_input_index', 'valid_output_index',\n                    'index_array', 'distance_array']:\n            val = cached_indexes[elt]\n            if isinstance(val, tuple):\n                val = cached_indexes[elt][0]\n            elif isinstance(val, np.ndarray):\n                val = da.from_array(val, chunks=CHUNK_SIZE)\n            elif persist and isinstance(val, da.Array):\n                cached_indexes[elt] = val = val.persist()\n            setattr(self.resampler, elt, val)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_neighbour_info(self, cache_dir, mask=None, **kwargs):\n        mask_name = getattr(mask, 'name', None)\n        filename = self._create_cache_filename(cache_dir,\n                                               mask=mask_name, **kwargs)\n        if kwargs.get('mask') in self._index_caches:\n            self._apply_cached_indexes(self._index_caches[kwargs.get('mask')])\n        elif cache_dir:\n            cache = np.load(filename, mmap_mode='r')\n            # copy the dict so we can modify it's keys\n            new_cache = dict(cache.items())\n            cache.close()\n            self._apply_cached_indexes(new_cache)  # modifies cache dict in-place\n            self._index_caches[mask_name] = new_cache\n        else:\n            raise IOError", "response": "Read index arrays from either the in - memory or disk cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves the neighbour info to a file.", "response": "def save_neighbour_info(self, cache_dir, mask=None, **kwargs):\n        \"\"\"Cache resampler's index arrays if there is a cache dir.\"\"\"\n        if cache_dir:\n            mask_name = getattr(mask, 'name', None)\n            filename = self._create_cache_filename(\n                cache_dir, mask=mask_name, **kwargs)\n            LOG.info('Saving kd_tree neighbour info to %s', filename)\n            cache = self._read_resampler_attrs()\n            # update the cache in place with persisted dask arrays\n            self._apply_cached_indexes(cache, persist=True)\n            self._index_caches[mask_name] = cache\n            np.savez(filename, **cache)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef resample(self, *args, **kwargs):\n        kwargs.setdefault('mask_area', False)\n        return super(EWAResampler, self).resample(*args, **kwargs)", "response": "Run precompute and compute methods."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _call_ll2cr(self, lons, lats, target_geo_def, swath_usage=0):\n        new_src = SwathDefinition(lons, lats)\n\n        swath_points_in_grid, cols, rows = ll2cr(new_src, target_geo_def)\n        # FIXME: How do we check swath usage/coverage if we only do this\n        #        per-block\n        # # Determine if enough of the input swath was used\n        # grid_name = getattr(self.target_geo_def, \"name\", \"N/A\")\n        # fraction_in = swath_points_in_grid / float(lons.size)\n        # swath_used = fraction_in > swath_usage\n        # if not swath_used:\n        #     LOG.info(\"Data does not fit in grid %s because it only %f%% of \"\n        #              \"the swath is used\" %\n        #              (grid_name, fraction_in * 100))\n        #     raise RuntimeError(\"Data does not fit in grid %s\" % (grid_name,))\n        # else:\n        #     LOG.debug(\"Data fits in grid %s and uses %f%% of the swath\",\n        #               grid_name, fraction_in * 100)\n\n        return np.stack([cols, rows], axis=0)", "response": "Wrapper around ll2cr for handling dask delayed calls better."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating row and column arrays and store them for later use.", "response": "def precompute(self, cache_dir=None, swath_usage=0, **kwargs):\n        \"\"\"Generate row and column arrays and store it for later use.\"\"\"\n        if kwargs.get('mask') is not None:\n            LOG.warning(\"'mask' parameter has no affect during EWA \"\n                        \"resampling\")\n\n        del kwargs\n        source_geo_def = self.source_geo_def\n        target_geo_def = self.target_geo_def\n\n        if cache_dir:\n            LOG.warning(\"'cache_dir' is not used by EWA resampling\")\n\n        # Satpy/PyResample don't support dynamic grids out of the box yet\n        lons, lats = source_geo_def.get_lonlats()\n        if isinstance(lons, xr.DataArray):\n            # get dask arrays\n            lons = lons.data\n            lats = lats.data\n        # we are remapping to a static unchanging grid/area with all of\n        # its parameters specified\n        chunks = (2,) + lons.chunks\n        res = da.map_blocks(self._call_ll2cr, lons, lats,\n                            target_geo_def, swath_usage,\n                            dtype=lons.dtype, chunks=chunks, new_axis=[0])\n        cols = res[0]\n        rows = res[1]\n\n        # save the dask arrays in the class instance cache\n        # the on-disk cache will store the numpy arrays\n        self.cache = {\n            \"rows\": rows,\n            \"cols\": cols,\n        }\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrap to run fornav as a dask delayed.", "response": "def _call_fornav(self, cols, rows, target_geo_def, data,\n                     grid_coverage=0, **kwargs):\n        \"\"\"Wrapper to run fornav as a dask delayed.\"\"\"\n        num_valid_points, res = fornav(cols, rows, target_geo_def,\n                                       data, **kwargs)\n\n        if isinstance(data, tuple):\n            # convert 'res' from tuple of arrays to one array\n            res = np.stack(res)\n            num_valid_points = sum(num_valid_points)\n\n        grid_covered_ratio = num_valid_points / float(res.size)\n        grid_covered = grid_covered_ratio > grid_coverage\n        if not grid_covered:\n            msg = \"EWA resampling only found %f%% of the grid covered \" \\\n                  \"(need %f%%)\" % (grid_covered_ratio * 100,\n                                   grid_coverage * 100)\n            raise RuntimeError(msg)\n        LOG.debug(\"EWA resampling found %f%% of the grid covered\" %\n                  (grid_covered_ratio * 100))\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compute(self, data, cache_id=None, fill_value=0, weight_count=10000,\n                weight_min=0.01, weight_distance_max=1.0,\n                weight_delta_max=1.0, weight_sum_min=-1.0,\n                maximum_weight_mode=False, grid_coverage=0, **kwargs):\n        \"\"\"Resample the data according to the precomputed X/Y coordinates.\"\"\"\n        rows = self.cache[\"rows\"]\n        cols = self.cache[\"cols\"]\n\n        # if the data is scan based then check its metadata or the passed\n        # kwargs otherwise assume the entire input swath is one large\n        # \"scanline\"\n        rows_per_scan = kwargs.get('rows_per_scan',\n                                   data.attrs.get(\"rows_per_scan\",\n                                                  data.shape[0]))\n\n        if data.ndim == 3 and 'bands' in data.dims:\n            data_in = tuple(data.sel(bands=band).data\n                            for band in data['bands'])\n        elif data.ndim == 2:\n            data_in = data.data\n        else:\n            raise ValueError(\"Unsupported data shape for EWA resampling.\")\n\n        res = dask.delayed(self._call_fornav)(\n            cols, rows, self.target_geo_def, data_in,\n            grid_coverage=grid_coverage,\n            rows_per_scan=rows_per_scan, weight_count=weight_count,\n            weight_min=weight_min, weight_distance_max=weight_distance_max,\n            weight_delta_max=weight_delta_max, weight_sum_min=weight_sum_min,\n            maximum_weight_mode=maximum_weight_mode)\n        if isinstance(data_in, tuple):\n            new_shape = (len(data_in),) + self.target_geo_def.shape\n        else:\n            new_shape = self.target_geo_def.shape\n        data_arr = da.from_delayed(res, new_shape, data.dtype)\n        # from delayed creates one large chunk, break it up a bit if we can\n        data_arr = data_arr.rechunk([CHUNK_SIZE] * data_arr.ndim)\n        if data.ndim == 3 and data.dims[0] == 'bands':\n            dims = ('bands', 'y', 'x')\n        elif data.ndim == 2:\n            dims = ('y', 'x')\n        else:\n            dims = data.dims\n\n        return xr.DataArray(data_arr, dims=dims,\n                            attrs=data.attrs.copy())", "response": "Resample the data according to the precomputed X and Y coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates bilinear coefficients and store them for later use. Note: The `mask` keyword should be provided if geolocation may be valid where data points are invalid. This defaults to the `mask` attribute of the `data` numpy masked array passed to the `resample` method.", "response": "def precompute(self, mask=None, radius_of_influence=50000, epsilon=0,\n                   reduce_data=True, nprocs=1,\n                   cache_dir=False, **kwargs):\n        \"\"\"Create bilinear coefficients and store them for later use.\n\n        Note: The `mask` keyword should be provided if geolocation may be valid\n        where data points are invalid. This defaults to the `mask` attribute of\n        the `data` numpy masked array passed to the `resample` method.\n        \"\"\"\n\n        del kwargs\n\n        if self.resampler is None:\n            kwargs = dict(source_geo_def=self.source_geo_def,\n                          target_geo_def=self.target_geo_def,\n                          radius_of_influence=radius_of_influence,\n                          neighbours=32,\n                          epsilon=epsilon,\n                          reduce_data=reduce_data)\n\n            self.resampler = XArrayResamplerBilinear(**kwargs)\n            try:\n                self.load_bil_info(cache_dir, **kwargs)\n                LOG.debug(\"Loaded bilinear parameters\")\n            except IOError:\n                LOG.debug(\"Computing bilinear parameters\")\n                self.resampler.get_bil_info()\n                self.save_bil_info(cache_dir, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute(self, data, fill_value=None, **kwargs):\n        del kwargs\n\n        if fill_value is None:\n            fill_value = data.attrs.get('_FillValue')\n        target_shape = self.target_geo_def.shape\n\n        res = self.resampler.get_sample_from_bil_info(data,\n                                                      fill_value=fill_value,\n                                                      output_shape=target_shape)\n\n        return res", "response": "Resample the given data using bilinear interpolation"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef aggregate(d, y_size, x_size):\n        if d.ndim != 2:\n            # we can't guarantee what blocks we are getting and how\n            # it should be reshaped to do the averaging.\n            raise ValueError(\"Can't aggregrate (reduce) data arrays with \"\n                             \"more than 2 dimensions.\")\n        if not (x_size.is_integer() and y_size.is_integer()):\n            raise ValueError(\"Aggregation factors are not integers\")\n        for agg_size, chunks in zip([y_size, x_size], d.chunks):\n            for chunk_size in chunks:\n                if chunk_size % agg_size != 0:\n                    raise ValueError(\"Aggregation requires arrays with \"\n                                     \"shapes and chunks divisible by the \"\n                                     \"factor\")\n\n        new_chunks = (tuple(int(x / y_size) for x in d.chunks[0]),\n                      tuple(int(x / x_size) for x in d.chunks[1]))\n        return da.core.map_blocks(_mean, d, y_size, x_size, dtype=d.dtype, chunks=new_chunks)", "response": "Aggregate every 4 elements in a 2D array."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_xy_scaling_parameters(self):\n        gd = self.area_definition\n        bx = self.x.min()\n        mx = gd.pixel_size_x\n        by = self.y.min()\n        my = -abs(gd.pixel_size_y)\n        return mx, bx, my, by", "response": "Get the X and Y limits for the full resulting image"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_xy_scaling_parameters(self):\n        return self.mx, self.bx, self.my, self.by", "response": "Get the X Y and MX limits for the full resulting image"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef apply_attributes(self, nc, table, prefix=''):\n        for name, value in sorted(table.items()):\n            if name in nc.ncattrs():\n                LOG.debug('already have a value for %s' % name)\n                continue\n            if value is not None:\n                setattr(nc, name, value)\n            else:\n                funcname = prefix+name  # _global_ + product_tile_height\n                func = getattr(self, funcname, None)\n                if func is not None:\n                    value = func()\n                    if value is not None:\n                        setattr(nc, name, value)\n                else:\n                    LOG.info('no routine matching %s' % funcname)", "response": "Apply attributes needed by the user to the nc object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nassign projection attributes per GRB standard", "response": "def set_projection_attrs(self, area_id, proj4_info):\n        \"\"\"Assign projection attributes per GRB standard\"\"\"\n        proj4_info['a'], proj4_info['b'] = proj4_radius_parameters(proj4_info)\n        if proj4_info[\"proj\"] == \"geos\":\n            p = self.projection = self.nc.createVariable(\"fixedgrid_projection\", 'i4')\n            self.image_data.grid_mapping = \"fixedgrid_projection\"\n            p.short_name = area_id\n            p.grid_mapping_name = \"geostationary\"\n            p.sweep_angle_axis = proj4_info.get(\"sweep\", \"y\")\n            p.perspective_point_height = proj4_info['h']\n            p.latitude_of_projection_origin = np.float32(0.0)\n            p.longitude_of_projection_origin = np.float32(proj4_info.get('lon_0', 0.0))  # is the float32 needed?\n        elif proj4_info[\"proj\"] == \"lcc\":\n            p = self.projection = self.nc.createVariable(\"lambert_projection\", 'i4')\n            self.image_data.grid_mapping = \"lambert_projection\"\n            p.short_name = area_id\n            p.grid_mapping_name = \"lambert_conformal_conic\"\n            p.standard_parallel = proj4_info[\"lat_0\"]  # How do we specify two standard parallels?\n            p.longitude_of_central_meridian = proj4_info[\"lon_0\"]\n            p.latitude_of_projection_origin = proj4_info.get('lat_1', proj4_info['lat_0'])  # Correct?\n        elif proj4_info['proj'] == 'stere':\n            p = self.projection = self.nc.createVariable(\"polar_projection\", 'i4')\n            self.image_data.grid_mapping = \"polar_projection\"\n            p.short_name = area_id\n            p.grid_mapping_name = \"polar_stereographic\"\n            p.standard_parallel = proj4_info[\"lat_ts\"]\n            p.straight_vertical_longitude_from_pole = proj4_info.get(\"lon_0\", 0.0)\n            p.latitude_of_projection_origin = proj4_info[\"lat_0\"]  # ?\n        elif proj4_info['proj'] == 'merc':\n            p = self.projection = self.nc.createVariable(\"mercator_projection\", 'i4')\n            self.image_data.grid_mapping = \"mercator_projection\"\n            p.short_name = area_id\n            p.grid_mapping_name = \"mercator\"\n            p.standard_parallel = proj4_info.get('lat_ts', proj4_info.get('lat_0', 0.0))\n            p.longitude_of_projection_origin = proj4_info.get(\"lon_0\", 0.0)\n        else:\n            raise ValueError(\"SCMI can not handle projection '{}'\".format(proj4_info['proj']))\n\n        p.semi_major_axis = np.float64(proj4_info[\"a\"])\n        p.semi_minor_axis = np.float64(proj4_info[\"b\"])\n        p.false_easting = np.float32(proj4_info.get(\"x\", 0.0))\n        p.false_northing = np.float32(proj4_info.get(\"y\", 0.0))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngroup datasets by their area.", "response": "def _group_by_area(self, datasets):\n        \"\"\"Group datasets by their area.\"\"\"\n        def _area_id(area_def):\n            return area_def.name + str(area_def.area_extent) + str(area_def.shape)\n\n        # get all of the datasets stored by area\n        area_datasets = {}\n        for x in datasets:\n            area_id = _area_id(x.attrs['area'])\n            area, ds_list = area_datasets.setdefault(area_id, (x.attrs['area'], []))\n            ds_list.append(x)\n        return area_datasets"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndownloading GOES - 16 ABI data from 2019 - 03 - 14 00 : 00Z.", "response": "def get_us_midlatitude_cyclone_abi(base_dir='.', method=None, force=False):\n    \"\"\"Get GOES-16 ABI (CONUS sector) data from 2019-03-14 00:00Z.\n\n    Args:\n        base_dir (str): Base directory for downloaded files.\n        method (str): Force download method for the data if not already cached.\n            Allowed options are: 'gcsfs'. Default of ``None`` will\n            choose the best method based on environment settings.\n        force (bool): Force re-download of data regardless of its existence on\n            the local system. Warning: May delete non-demo files stored in\n            download directory.\n\n    Total size: ~110MB\n\n    \"\"\"\n    if method is None:\n        method = 'gcsfs'\n    if method not in ['gcsfs']:\n        raise NotImplementedError(\"Demo data download method '{}' not \"\n                                  \"implemented yet.\".format(method))\n\n    from ._google_cloud_platform import get_bucket_files\n    patterns = ['gs://gcp-public-data-goes-16/ABI-L1b-RadC/2019/073/00/*0002*.nc']\n    subdir = os.path.join(base_dir, 'abi_l1b', '20190314_us_midlatitude_cyclone')\n    _makedirs(subdir, exist_ok=True)\n    filenames = get_bucket_files(patterns, subdir, force=force)\n    assert len(filenames) == 16, \"Not all files could be downloaded\"\n    return filenames"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_hurricane_florence_abi(base_dir='.', method=None, force=False,\n                               channels=range(1, 17), num_frames=10):\n    \"\"\"Get GOES-16 ABI (Meso sector) data from 2018-09-11 13:00Z to 17:00Z.\n\n    Args:\n        base_dir (str): Base directory for downloaded files.\n        method (str): Force download method for the data if not already cached.\n            Allowed options are: 'gcsfs'. Default of ``None`` will\n            choose the best method based on environment settings.\n        force (bool): Force re-download of data regardless of its existence on\n            the local system. Warning: May delete non-demo files stored in\n            download directory.\n        channels (list): Channels to include in download. Defaults to all\n            16 channels.\n        num_frames (int or slice): Number of frames to download. Maximum\n            240 frames. Default 10 frames.\n\n    Size per frame (all channels): ~15MB\n\n    Total size (default 10 frames, all channels): ~124MB\n\n    Total size (240 frames, all channels): ~3.5GB\n\n    \"\"\"\n    if method is None:\n        method = 'gcsfs'\n    if method not in ['gcsfs']:\n        raise NotImplementedError(\"Demo data download method '{}' not \"\n                                  \"implemented yet.\".format(method))\n    if isinstance(num_frames, (int, float)):\n        frame_slice = slice(0, num_frames)\n    else:\n        frame_slice = num_frames\n\n    from ._google_cloud_platform import get_bucket_files\n\n    patterns = []\n    for channel in channels:\n        # patterns += ['gs://gcp-public-data-goes-16/ABI-L1b-RadM/2018/254/1[3456]/'\n        #              '*C{:02d}*s20182541[3456]*.nc'.format(channel)]\n        patterns += [(\n            'gs://gcp-public-data-goes-16/ABI-L1b-RadM/2018/254/13/*RadM1*C{:02d}*s201825413*.nc'.format(channel),\n            'gs://gcp-public-data-goes-16/ABI-L1b-RadM/2018/254/14/*RadM1*C{:02d}*s201825414*.nc'.format(channel),\n            'gs://gcp-public-data-goes-16/ABI-L1b-RadM/2018/254/15/*RadM1*C{:02d}*s201825415*.nc'.format(channel),\n            'gs://gcp-public-data-goes-16/ABI-L1b-RadM/2018/254/16/*RadM1*C{:02d}*s201825416*.nc'.format(channel),\n        )]\n    subdir = os.path.join(base_dir, 'abi_l1b', '20180911_hurricane_florence_abi_l1b')\n    _makedirs(subdir, exist_ok=True)\n    filenames = get_bucket_files(patterns, subdir, force=force, pattern_slice=frame_slice)\n\n    actual_slice = frame_slice.indices(240)  # 240 max frames\n    num_frames = int((actual_slice[1] - actual_slice[0]) / actual_slice[2])\n    assert len(filenames) == len(channels) * num_frames, \"Not all files could be downloaded\"\n    return filenames", "response": "Download the Hurricane - Florence ABI file from 2018 - 09 - 11 13 - 00Z to 17 - 00Z."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind all subdirectory configuration files.", "response": "def _config_data_files(base_dirs, extensions=(\".cfg\", )):\n    \"\"\"Find all subdirectory configuration files.\n\n    Searches each base directory relative to this setup.py file and finds\n    all files ending in the extensions provided.\n\n    :param base_dirs: iterable of relative base directories to search\n    :param extensions: iterable of file extensions to include (with '.' prefix)\n    :returns: list of 2-element tuples compatible with `setuptools.setup`\n    \"\"\"\n    data_files = []\n    pkg_root = os.path.realpath(os.path.dirname(__file__)) + \"/\"\n    for base_dir in base_dirs:\n        new_data_files = []\n        for ext in extensions:\n            configs = glob(os.path.join(pkg_root, base_dir, \"*\" + ext))\n            configs = [c.replace(pkg_root, \"\") for c in configs]\n            new_data_files.extend(configs)\n        data_files.append((base_dir, new_data_files))\n\n    return data_files"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a dask array from a SD dataset.", "response": "def from_sds(var, *args, **kwargs):\n    \"\"\"Create a dask array from a SD dataset.\"\"\"\n    var.__dict__['dtype'] = HTYPE_TO_DTYPE[var.info()[3]]\n    shape = var.info()[2]\n    var.__dict__['shape'] = shape if isinstance(shape, (tuple, list)) else tuple(shape)\n    return da.from_array(var, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the band in blocks and return a xarray. DataArray.", "response": "def _open_xarray_dataset(self, val, chunks=CHUNK_SIZE):\n        \"\"\"Read the band in blocks.\"\"\"\n        dask_arr = from_sds(val, chunks=chunks)\n        attrs = val.attributes()\n        return xr.DataArray(dask_arr, dims=('y', 'x'),\n                            attrs=attrs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef overlay(top, bottom, maxval=None):\n    if maxval is None:\n        maxval = np.maximum(top.max(), bottom.max())\n\n    res = ((2 * top / maxval - 1) * bottom + 2 * top) * bottom / maxval\n    return res.clip(min=0)", "response": "Blending two layers.\n\n    from: https://docs.gimp.org/en/gimp-concepts-layer-modes.html"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_writer_config(config_files, loader=UnsafeLoader):\n\n    conf = {}\n    LOG.debug('Reading %s', str(config_files))\n    for config_file in config_files:\n        with open(config_file) as fd:\n            conf.update(yaml.load(fd.read(), Loader=loader))\n\n    try:\n        writer_info = conf['writer']\n    except KeyError:\n        raise KeyError(\n            \"Malformed config file {}: missing writer 'writer'\".format(\n                config_files))\n    writer_info['config_files'] = config_files\n    return writer_info", "response": "Read the writer config_files and return the info extracted."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_writer_configs(writer_configs, ppp_config_dir,\n                        **writer_kwargs):\n    \"\"\"Load the writer from the provided `writer_configs`.\"\"\"\n    try:\n        writer_info = read_writer_config(writer_configs)\n        writer_class = writer_info['writer']\n    except (ValueError, KeyError, yaml.YAMLError):\n        raise ValueError(\"Invalid writer configs: \"\n                         \"'{}'\".format(writer_configs))\n    init_kwargs, kwargs = writer_class.separate_init_kwargs(writer_kwargs)\n    writer = writer_class(ppp_config_dir=ppp_config_dir,\n                          config_files=writer_configs,\n                          **init_kwargs)\n    return writer, kwargs", "response": "Load the writer from the provided writer_configs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_writer(writer, ppp_config_dir=None, **writer_kwargs):\n    if ppp_config_dir is None:\n        ppp_config_dir = get_environ_config_dir()\n\n    config_fn = writer + \".yaml\" if \".\" not in writer else writer\n    config_files = config_search_paths(\n        os.path.join(\"writers\", config_fn), ppp_config_dir)\n    writer_kwargs.setdefault(\"config_files\", config_files)\n    if not writer_kwargs['config_files']:\n        raise ValueError(\"Unknown writer '{}'\".format(writer))\n\n    try:\n        return load_writer_configs(writer_kwargs['config_files'],\n                                   ppp_config_dir=ppp_config_dir,\n                                   **writer_kwargs)\n    except ValueError:\n        raise ValueError(\"Writer '{}' does not exist or could not be \"\n                         \"loaded\".format(writer))", "response": "Find and load writer in the available configuration files."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef configs_for_writer(writer=None, ppp_config_dir=None):\n    search_paths = (ppp_config_dir,) if ppp_config_dir else tuple()\n    if writer is not None:\n        if not isinstance(writer, (list, tuple)):\n            writer = [writer]\n        # given a config filename or writer name\n        config_files = [w if w.endswith('.yaml') else w + '.yaml' for w in writer]\n    else:\n        writer_configs = glob_config(os.path.join('writers', '*.yaml'),\n                                     *search_paths)\n        config_files = set(writer_configs)\n\n    for config_file in config_files:\n        config_basename = os.path.basename(config_file)\n        writer_configs = config_search_paths(\n            os.path.join(\"writers\", config_basename), *search_paths)\n\n        if not writer_configs:\n            LOG.warning(\"No writer configs found for '%s'\", writer)\n            continue\n\n        yield writer_configs", "response": "Generator of configuration files for one or more writers"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of available writers based on current configuration.", "response": "def available_writers(as_dict=False):\n    \"\"\"Available writers based on current configuration.\n\n    Args:\n        as_dict (bool): Optionally return writer information as a dictionary.\n                        Default: False\n\n    Returns: List of available writer names. If `as_dict` is `True` then\n             a list of dictionaries including additionally writer information\n             is returned.\n\n    \"\"\"\n    writers = []\n    for writer_configs in configs_for_writer():\n        try:\n            writer_info = read_writer_config(writer_configs)\n        except (KeyError, IOError, yaml.YAMLError):\n            LOG.warning(\"Could not import writer config from: %s\", writer_configs)\n            LOG.debug(\"Error loading YAML\", exc_info=True)\n            continue\n        writers.append(writer_info if as_dict else writer_info['name'])\n    return writers"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_overlay(orig, area, coast_dir, color=(0, 0, 0), width=0.5, resolution=None,\n                level_coast=1, level_borders=1, fill_value=None,\n                grid=None):\n    \"\"\"Add coastline, political borders and grid(graticules) to image.\n\n    Uses ``color`` for feature colors where ``color`` is a 3-element tuple\n    of integers between 0 and 255 representing (R, G, B).\n\n    .. warning::\n\n        This function currently loses the data mask (alpha band).\n\n    ``resolution`` is chosen automatically if None (default), otherwise it should be one of:\n\n    +-----+-------------------------+---------+\n    | 'f' | Full resolution         | 0.04 km |\n    | 'h' | High resolution         | 0.2 km  |\n    | 'i' | Intermediate resolution | 1.0 km  |\n    | 'l' | Low resolution          | 5.0 km  |\n    | 'c' | Crude resolution        | 25  km  |\n    +-----+-------------------------+---------+\n\n    ``grid`` is a dictionary with key values as documented in detail in pycoast\n\n    eg. overlay={'grid': {'major_lonlat': (10, 10),\n                          'write_text': False,\n                          'outline': (224, 224, 224),\n                          'width': 0.5}}\n\n    Here major_lonlat is plotted every 10 deg for both longitude and latitude,\n    no labels for the grid lines are plotted, the color used for the grid lines\n    is light gray, and the width of the gratucules is 0.5 pixels.\n\n    For grid if aggdraw is used, font option is mandatory, if not write_text is set to False\n    eg. font = aggdraw.Font('black', '/usr/share/fonts/truetype/msttcorefonts/Arial.ttf',\n                            opacity=127, size=16)\n    \"\"\"\n\n    if area is None:\n        raise ValueError(\"Area of image is None, can't add overlay.\")\n\n    from pycoast import ContourWriterAGG\n    if isinstance(area, str):\n        area = get_area_def(area)\n    LOG.info(\"Add coastlines and political borders to image.\")\n\n    if resolution is None:\n\n        x_resolution = ((area.area_extent[2] -\n                         area.area_extent[0]) /\n                        area.x_size)\n        y_resolution = ((area.area_extent[3] -\n                         area.area_extent[1]) /\n                        area.y_size)\n        res = min(x_resolution, y_resolution)\n\n        if res > 25000:\n            resolution = \"c\"\n        elif res > 5000:\n            resolution = \"l\"\n        elif res > 1000:\n            resolution = \"i\"\n        elif res > 200:\n            resolution = \"h\"\n        else:\n            resolution = \"f\"\n\n        LOG.debug(\"Automagically choose resolution %s\", resolution)\n\n    if hasattr(orig, 'convert'):\n        # image must be in RGB space to work with pycoast/pydecorate\n        orig = orig.convert('RGBA' if orig.mode.endswith('A') else 'RGB')\n    elif not orig.mode.startswith('RGB'):\n        raise RuntimeError(\"'trollimage' 1.6+ required to support adding \"\n                           \"overlays/decorations to non-RGB data.\")\n    img = orig.pil_image(fill_value=fill_value)\n    cw_ = ContourWriterAGG(coast_dir)\n    cw_.add_coastlines(img, area, outline=color,\n                       resolution=resolution, width=width, level=level_coast)\n    cw_.add_borders(img, area, outline=color,\n                    resolution=resolution, width=width, level=level_borders)\n    # Only add grid if major_lonlat is given.\n    if grid and 'major_lonlat' in grid and grid['major_lonlat']:\n        major_lonlat = grid.pop('major_lonlat')\n        minor_lonlat = grid.pop('minor_lonlat', major_lonlat)\n\n        cw_.add_grid(img, area, major_lonlat, minor_lonlat, **grid)\n\n    arr = da.from_array(np.array(img) / 255.0, chunks=CHUNK_SIZE)\n\n    new_data = xr.DataArray(arr, dims=['y', 'x', 'bands'],\n                            coords={'y': orig.data.coords['y'],\n                                    'x': orig.data.coords['x'],\n                                    'bands': list(img.mode)},\n                            attrs=orig.data.attrs)\n    return XRImage(new_data)", "response": "Add an overlay to the image."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds text to an image using the pydecorate package.", "response": "def add_text(orig, dc, img, text=None):\n    \"\"\"Add text to an image using the pydecorate package.\n\n    All the features of pydecorate's ``add_text`` are available.\n    See documentation of :doc:`pydecorate:index` for more info.\n\n    \"\"\"\n    LOG.info(\"Add text to image.\")\n\n    dc.add_text(**text)\n\n    arr = da.from_array(np.array(img) / 255.0, chunks=CHUNK_SIZE)\n\n    new_data = xr.DataArray(arr, dims=['y', 'x', 'bands'],\n                            coords={'y': orig.data.coords['y'],\n                                    'x': orig.data.coords['x'],\n                                    'bands': list(img.mode)},\n                            attrs=orig.data.attrs)\n    return XRImage(new_data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_decorate(orig, fill_value=None, **decorate):\n    LOG.info(\"Decorate image.\")\n\n    # Need to create this here to possible keep the alignment\n    # when adding text and/or logo with pydecorate\n    if hasattr(orig, 'convert'):\n        # image must be in RGB space to work with pycoast/pydecorate\n        orig = orig.convert('RGBA' if orig.mode.endswith('A') else 'RGB')\n    elif not orig.mode.startswith('RGB'):\n        raise RuntimeError(\"'trollimage' 1.6+ required to support adding \"\n                           \"overlays/decorations to non-RGB data.\")\n    img_orig = orig.pil_image(fill_value=fill_value)\n    from pydecorate import DecoratorAGG\n    dc = DecoratorAGG(img_orig)\n\n    # decorate need to be a list to maintain the alignment\n    # as ordered in the list\n    img = orig\n    if 'decorate' in decorate:\n        for dec in decorate['decorate']:\n            if 'logo' in dec:\n                img = add_logo(img, dc, img_orig, logo=dec['logo'])\n            elif 'text' in dec:\n                img = add_text(img, dc, img_orig, text=dec['text'])\n    return img", "response": "Add an image with text and logos and images to the list of decorations."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an enhanced version of dataset as an XRImage instance.", "response": "def get_enhanced_image(dataset, ppp_config_dir=None, enhance=None, enhancement_config_file=None,\n                       overlay=None, decorate=None, fill_value=None):\n    \"\"\"Get an enhanced version of `dataset` as an :class:`~trollimage.xrimage.XRImage` instance.\n\n    Args:\n        dataset (xarray.DataArray): Data to be enhanced and converted to an image.\n        ppp_config_dir (str): Root configuration directory.\n        enhance (bool or Enhancer): Whether to automatically enhance\n            data to be more visually useful and to fit inside the file\n            format being saved to. By default this will default to using\n            the enhancement configuration files found using the default\n            :class:`~satpy.writers.Enhancer` class. This can be set to\n            `False` so that no enhancments are performed. This can also\n            be an instance of the :class:`~satpy.writers.Enhancer` class\n            if further custom enhancement is needed.\n        enhancement_config_file (str): Deprecated.\n        overlay (dict): Options for image overlays. See :func:`add_overlay`\n            for available options.\n        decorate (dict): Options for decorating the image. See\n            :func:`add_decorate` for available options.\n        fill_value (int or float): Value to use when pixels are masked or\n            invalid. Default of `None` means to create an alpha channel.\n            See :meth:`~trollimage.xrimage.XRImage.finalize` for more\n            details. Only used when adding overlays or decorations. Otherwise\n            it is up to the caller to \"finalize\" the image before using it\n            except if calling ``img.show()`` or providing the image to\n            a writer as these will finalize the image.\n\n    .. versionchanged:: 0.10\n\n        Deprecated `enhancement_config_file` and 'enhancer' in favor of\n        `enhance`. Pass an instance of the `Enhancer` class to `enhance`\n        instead.\n\n    \"\"\"\n    if ppp_config_dir is None:\n        ppp_config_dir = get_environ_config_dir()\n\n    if enhancement_config_file is not None:\n        warnings.warn(\"'enhancement_config_file' has been deprecated. Pass an instance of the \"\n                      \"'Enhancer' class to the 'enhance' keyword argument instead.\", DeprecationWarning)\n\n    if enhance is False:\n        # no enhancement\n        enhancer = None\n    elif enhance is None or enhance is True:\n        # default enhancement\n        enhancer = Enhancer(ppp_config_dir, enhancement_config_file)\n    else:\n        # custom enhancer\n        enhancer = enhance\n\n    # Create an image for enhancement\n    img = to_image(dataset)\n\n    if enhancer is None or enhancer.enhancement_tree is None:\n        LOG.debug(\"No enhancement being applied to dataset\")\n    else:\n        if dataset.attrs.get(\"sensor\", None):\n            enhancer.add_sensor_enhancements(dataset.attrs[\"sensor\"])\n\n        enhancer.apply(img, **dataset.attrs)\n\n    if overlay is not None:\n        img = add_overlay(img, dataset.attrs['area'], fill_value=fill_value, **overlay)\n\n    if decorate is not None:\n        img = add_decorate(img, fill_value=fill_value, **decorate)\n\n    return img"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef show(dataset, **kwargs):\n    img = get_enhanced_image(dataset.squeeze(), **kwargs)\n    img.show()\n    return img", "response": "Display the dataset as an image."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert the dataset into an image.", "response": "def to_image(dataset):\n    \"\"\"convert ``dataset`` into a :class:`~trollimage.xrimage.XRImage` instance.\n\n    Convert the ``dataset`` into an instance of the\n    :class:`~trollimage.xrimage.XRImage` class.  This function makes no other\n    changes.  To get an enhanced image, possibly with overlays and decoration,\n    see :func:`~get_enhanced_image`.\n\n    Args:\n        dataset (xarray.DataArray): Data to be converted to an image.\n\n    Returns:\n        Instance of :class:`~trollimage.xrimage.XRImage`.\n    \"\"\"\n    dataset = dataset.squeeze()\n    if dataset.ndim < 2:\n        raise ValueError(\"Need at least a 2D array to make an image.\")\n    else:\n        return XRImage(dataset)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget sources targets and delayed objects to separate lists from a list of results collected from multiple writer ( s ).", "response": "def split_results(results):\n    \"\"\"Get sources, targets and delayed objects to separate lists from a\n    list of results collected from (multiple) writer(s).\"\"\"\n    from dask.delayed import Delayed\n\n    def flatten(results):\n        out = []\n        if isinstance(results, (list, tuple)):\n            for itm in results:\n                out.extend(flatten(itm))\n            return out\n        return [results]\n\n    sources = []\n    targets = []\n    delayeds = []\n\n    for res in flatten(results):\n        if isinstance(res, da.Array):\n            sources.append(res)\n        elif isinstance(res, Delayed):\n            delayeds.append(res)\n        else:\n            targets.append(res)\n    return sources, targets, delayeds"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compute_writer_results(results):\n    if not results:\n        return\n\n    sources, targets, delayeds = split_results(results)\n\n    # one or more writers have targets that we need to close in the future\n    if targets:\n        delayeds.append(da.store(sources, targets, compute=False))\n\n    if delayeds:\n        da.compute(delayeds)\n\n    if targets:\n        for target in targets:\n            if hasattr(target, 'close'):\n                target.close()", "response": "Compute all the given dask graphs results so that the files are saved."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef separate_init_kwargs(cls, kwargs):\n        # FUTURE: Don't pass Scene.save_datasets kwargs to init and here\n        init_kwargs = {}\n        kwargs = kwargs.copy()\n        for kw in ['base_dir', 'filename', 'file_pattern']:\n            if kw in kwargs:\n                init_kwargs[kw] = kwargs.pop(kw)\n        return init_kwargs, kwargs", "response": "Helper method to separate keyword arguments between init and save methods."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a parser object for later use.", "response": "def create_filename_parser(self, base_dir):\n        \"\"\"Create a :class:`trollsift.parser.Parser` object for later use.\"\"\"\n        # just in case a writer needs more complex file patterns\n        # Set a way to create filenames if we were given a pattern\n        if base_dir and self.file_pattern:\n            file_pattern = os.path.join(base_dir, self.file_pattern)\n        else:\n            file_pattern = self.file_pattern\n        return parser.Parser(file_pattern) if file_pattern else None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_filename(self, **kwargs):\n        if self.filename_parser is None:\n            raise RuntimeError(\"No filename pattern or specific filename provided\")\n        output_filename = self.filename_parser.compose(kwargs)\n        dirname = os.path.dirname(output_filename)\n        if dirname and not os.path.isdir(dirname):\n            LOG.info(\"Creating output directory: {}\".format(dirname))\n            os.makedirs(dirname)\n        return output_filename", "response": "Create a filename where output data will be saved."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save_datasets(self, datasets, compute=True, **kwargs):\n        results = []\n        for ds in datasets:\n            results.append(self.save_dataset(ds, compute=False, **kwargs))\n\n        if compute:\n            LOG.info(\"Computing and writing results...\")\n            return compute_writer_results([results])\n\n        targets, sources, delayeds = split_results([results])\n        if delayeds:\n            # This writer had only delayed writes\n            return delayeds\n        else:\n            return targets, sources", "response": "Save all datasets to one or more files."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save_dataset(self, dataset, filename=None, fill_value=None,\n                     compute=True, **kwargs):\n        \"\"\"Saves the ``dataset`` to a given ``filename``.\n\n        This method must be overloaded by the subclass.\n\n        Args:\n            dataset (xarray.DataArray): Dataset to save using this writer.\n            filename (str): Optionally specify the filename to save this\n                            dataset to. If not provided then `filename`\n                            which can be provided to the init method will be\n                            used and formatted by dataset attributes.\n            fill_value (int or float): Replace invalid values in the dataset\n                                       with this fill value if applicable to\n                                       this writer.\n            compute (bool): If `True` (default), compute and save the dataset.\n                            If `False` return either a `dask.delayed.Delayed`\n                            object or tuple of (source, target). See the\n                            return values below for more information.\n            **kwargs: Other keyword arguments for this particular writer.\n\n        Returns:\n            Value returned depends on `compute`. If `compute` is `True` then\n            the return value is the result of computing a\n            `dask.delayed.Delayed` object or running `dask.array.store`. If\n            `compute` is `False` then the returned value is either a\n            `dask.delayed.Delayed` object that can be computed using\n            `delayed.compute()` or a tuple of (source, target) that should be\n            passed to `dask.array.store`. If target is provided the the caller\n            is responsible for calling `target.close()` if the target has\n            this method.\n\n        \"\"\"\n        raise NotImplementedError(\n            \"Writer '%s' has not implemented dataset saving\" % (self.name, ))", "response": "Saves the dataset to a given filename."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save_dataset(self, dataset, filename=None, fill_value=None,\n                     overlay=None, decorate=None, compute=True, **kwargs):\n        \"\"\"Saves the ``dataset`` to a given ``filename``.\n\n        This method creates an enhanced image using `get_enhanced_image`. The\n        image is then passed to `save_image`. See both of these functions for\n        more details on the arguments passed to this method.\n\n        \"\"\"\n        img = get_enhanced_image(dataset.squeeze(), enhance=self.enhancer, overlay=overlay,\n                                 decorate=decorate, fill_value=fill_value)\n        return self.save_image(img, filename=filename, compute=compute, fill_value=fill_value, **kwargs)", "response": "Saves the dataset to a given filename."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save_image(self, img, filename=None, compute=True, **kwargs):\n        raise NotImplementedError(\"Writer '%s' has not implemented image saving\" % (self.name,))", "response": "Save an image object to disk."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_dataset(self, key, info=None, out=None, xslice=None, yslice=None):\n        if key in self.cache:\n            return self.cache[key]\n        # Type dictionary\n        typedict = {\"af\": \"flash_accumulation\",\n                    \"afa\": \"accumulated_flash_area\",\n                    \"afr\": \"flash_radiance\",\n                    \"lgr\": \"radiance\",\n                    \"lef\": \"radiance\",\n                    \"lfl\": \"radiance\"}\n\n        # Get lightning data out of NetCDF container\n        logger.debug(\"Key: {}\".format(key.name))\n        # Create reference grid\n        grid = np.full((self.nlines, self.ncols), np.NaN)\n        # Set slices to full disc extent\n        if xslice is None:\n            xslice = slice(0, self.ncols, None)\n        if yslice is None:\n            yslice = slice(0, self.nlines, None)\n        logger.debug(\"Slices - x: {}, y: {}\".format(xslice, yslice))\n        # Get product values\n        values = self.nc[typedict[key.name]]\n        rows = self.nc['row']\n        cols = self.nc['column']\n        logger.debug('[ Number of values ] : {}'.format((len(values))))\n        logger.debug('[Min/Max] : <{}> / <{}>'.format(np.min(values),\n                                                      np.max(values)))\n        # Convert xy coordinates to flatten indices\n        ids = np.ravel_multi_index([rows, cols], grid.shape)\n        # Replace NaN values with data\n        np.put(grid, ids, values)\n\n        # Correct for bottom left origin in LI row/column indices.\n        rotgrid = np.flipud(grid)\n        logger.debug('Data shape: {}, {}'.format(yslice, xslice))\n        # Rotate the grid by 90 degree clockwise\n        rotgrid = np.rot90(rotgrid, 3)\n        logger.warning(\"LI data has been rotated to fit to reference grid. \\\n                        Works only for test dataset\")\n        # Slice the gridded lighting data\n        slicegrid = rotgrid[yslice, xslice]\n        # Mask invalid values\n        ds = np.ma.masked_where(np.isnan(slicegrid), slicegrid)\n        # Create dataset object\n        out.data[:] = np.ma.getdata(ds)\n        out.mask[:] = np.ma.getmask(ds)\n        out.info.update(key.to_dict())\n\n        return(out)", "response": "Load a dataset from the NetCDF file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates AreaDefinition for specified product.", "response": "def get_area_def(self, key, info=None):\n        \"\"\"Create AreaDefinition for specified product.\n\n        Projection information are hard coded for 0 degree geos projection\n        Test dataset doesn't provide the values in the file container.\n        Only fill values are inserted.\n\n        \"\"\"\n        # TODO Get projection information from input file\n        a = 6378169.\n        h = 35785831.\n        b = 6356583.8\n        lon_0 = 0.\n        # area_extent = (-5432229.9317116784, -5429229.5285458621,\n        #                5429229.5285458621, 5432229.9317116784)\n        area_extent = (-5570248.4773392612, -5567248.074173444,\n                       5567248.074173444, 5570248.4773392612)\n        proj_dict = {'a': float(a),\n                     'b': float(b),\n                     'lon_0': float(lon_0),\n                     'h': float(h),\n                     'proj': 'geos',\n                     'units': 'm'}\n\n        area = geometry.AreaDefinition(\n            'LI_area_name',\n            \"LI area\",\n            'geosli',\n            proj_dict,\n            self.ncols,\n            self.nlines,\n            area_extent)\n        self.area = area\n        logger.debug(\"Dataset area definition: \\n {}\".format(area))\n        return area"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef listify_string(something):\n    if isinstance(something, (str, six.text_type)):\n        return [something]\n    elif something is not None:\n        return list(something)\n    else:\n        return list()", "response": "Takes a string and makes it a list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the end of path of same length as pattern.", "response": "def get_filebase(path, pattern):\n    \"\"\"Get the end of *path* of same length as *pattern*.\"\"\"\n    # A pattern can include directories\n    tail_len = len(pattern.split(os.path.sep))\n    return os.path.join(*str(path).split(os.path.sep)[-tail_len:])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef match_filenames(filenames, pattern):\n    matching = []\n\n    for filename in filenames:\n        if fnmatch(get_filebase(filename, pattern), globify(pattern)):\n            matching.append(filename)\n\n    return matching", "response": "Get the filenames matching pattern."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if sensor is supported.", "response": "def supports_sensor(self, sensor):\n        \"\"\"Check if *sensor* is supported.\n\n        Returns True is *sensor* is None.\n        \"\"\"\n        if sensor and not (set(self.info.get(\"sensors\")) &\n                           set(listify_string(sensor))):\n            return False\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds files for this reader in directory.", "response": "def select_files_from_directory(self, directory=None):\n        \"\"\"Find files for this reader in *directory*.\n\n        If directory is None or '', look in the current directory.\n        \"\"\"\n        filenames = []\n        if directory is None:\n            directory = ''\n        for pattern in self.file_patterns:\n            matching = glob.iglob(os.path.join(directory, globify(pattern)))\n            filenames.extend(matching)\n        return filenames"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nselect the files from the given file names.", "response": "def select_files_from_pathnames(self, filenames):\n        \"\"\"Select the files from *filenames* this reader can handle.\"\"\"\n        selected_filenames = []\n\n        for pattern in self.file_patterns:\n            matching = match_filenames(filenames, pattern)\n            for fname in matching:\n                if fname not in selected_filenames:\n                    selected_filenames.append(fname)\n        if len(selected_filenames) == 0:\n            logger.warning(\"No filenames found for reader: %s\", self.name)\n        return selected_filenames"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_dataset_key(self, key, **kwargs):\n        return get_key(key, self.ids.keys(), **kwargs)", "response": "Get the fully qualified DatasetID matching key."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the dataset ids from the config file.", "response": "def load_ds_ids_from_config(self):\n        \"\"\"Get the dataset ids from the config.\"\"\"\n        ids = []\n        for dataset in self.datasets.values():\n            # xarray doesn't like concatenating attributes that are lists\n            # https://github.com/pydata/xarray/issues/2060\n            if 'coordinates' in dataset and \\\n                    isinstance(dataset['coordinates'], list):\n                dataset['coordinates'] = tuple(dataset['coordinates'])\n            # Build each permutation/product of the dataset\n            id_kwargs = []\n            for key in DATASET_KEYS:\n                val = dataset.get(key)\n                if key in [\"wavelength\", \"modifiers\"] and isinstance(val,\n                                                                     list):\n                    # special case: wavelength can be [min, nominal, max]\n                    # but is still considered 1 option\n                    # it also needs to be a tuple so it can be used in\n                    # a dictionary key (DatasetID)\n                    id_kwargs.append((tuple(val), ))\n                elif key == \"modifiers\" and val is None:\n                    # empty modifiers means no modifiers applied\n                    id_kwargs.append((tuple(), ))\n                elif isinstance(val, (list, tuple, set)):\n                    # this key has multiple choices\n                    # (ex. 250 meter, 500 meter, 1000 meter resolutions)\n                    id_kwargs.append(val)\n                elif isinstance(val, dict):\n                    id_kwargs.append(val.keys())\n                else:\n                    # this key only has one choice so make it a one\n                    # item iterable\n                    id_kwargs.append((val, ))\n            for id_params in itertools.product(*id_kwargs):\n                dsid = DatasetID(*id_params)\n                ids.append(dsid)\n\n                # create dataset infos specifically for this permutation\n                ds_info = dataset.copy()\n                for key in DATASET_KEYS:\n                    if isinstance(ds_info.get(key), dict):\n                        ds_info.update(ds_info[key][getattr(dsid, key)])\n                    # this is important for wavelength which was converted\n                    # to a tuple\n                    ds_info[key] = getattr(dsid, key)\n                self.ids[dsid] = ds_info\n\n        return ids"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_file_covers_area(file_handler, check_area):\n        try:\n            gbb = Boundary(*file_handler.get_bounding_box())\n        except NotImplementedError as err:\n            logger.debug(\"Bounding box computation not implemented: %s\",\n                         str(err))\n        else:\n            abb = AreaDefBoundary(get_area_def(check_area), frequency=1000)\n\n            intersection = gbb.contour_poly.intersection(abb.contour_poly)\n            if not intersection:\n                return False\n        return True", "response": "Checks if the file covers the current area."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_required_filehandlers(self, requirements, filename_info):\n        req_fh = []\n        filename_info = set(filename_info.items())\n        if requirements:\n            for requirement in requirements:\n                for fhd in self.file_handlers[requirement]:\n                    if set(fhd.filename_info.items()).issubset(filename_info):\n                        req_fh.append(fhd)\n                        break\n                else:\n                    raise RuntimeError(\"No matching requirement file of type \"\n                                       \"{}\".format(requirement))\n                    # break everything and continue to next\n                    # filetype!\n        return req_fh", "response": "Find the required file handlers for the given requirements."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsort the instance s filetypes in using order.", "response": "def sorted_filetype_items(self):\n        \"\"\"Sort the instance's filetypes in using order.\"\"\"\n        processed_types = []\n        file_type_items = deque(self.config['file_types'].items())\n        while len(file_type_items):\n            filetype, filetype_info = file_type_items.popleft()\n\n            requirements = filetype_info.get('requires')\n            if requirements is not None:\n                # requirements have not been processed yet -> wait\n                missing = [req for req in requirements\n                           if req not in processed_types]\n                if missing:\n                    file_type_items.append((filetype, filetype_info))\n                    continue\n\n            processed_types.append(filetype)\n            yield filetype, filetype_info"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef filename_items_for_filetype(filenames, filetype_info):\n        matched_files = []\n        for pattern in filetype_info['file_patterns']:\n            for filename in match_filenames(filenames, pattern):\n                if filename in matched_files:\n                    continue\n                try:\n                    filename_info = parse(\n                        pattern, get_filebase(filename, pattern))\n                except ValueError:\n                    logger.debug(\"Can't parse %s with %s.\", filename, pattern)\n                    continue\n                matched_files.append(filename)\n                yield filename, filename_info", "response": "Yields the filenames and the filename_info for a given filetype."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates new filehandler instances.", "response": "def new_filehandler_instances(self, filetype_info, filename_items, fh_kwargs=None):\n        \"\"\"Generate new filehandler instances.\"\"\"\n        requirements = filetype_info.get('requires')\n        filetype_cls = filetype_info['file_reader']\n\n        if fh_kwargs is None:\n            fh_kwargs = {}\n\n        for filename, filename_info in filename_items:\n            try:\n                req_fh = self.find_required_filehandlers(requirements,\n                                                         filename_info)\n            except KeyError as req:\n                msg = \"No handler for reading requirement {} for {}\".format(\n                    req, filename)\n                warnings.warn(msg)\n                continue\n            except RuntimeError as err:\n                warnings.warn(str(err) + ' for {}'.format(filename))\n                continue\n\n            yield filetype_cls(filename, filename_info, filetype_info, *req_fh, **fh_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfilter out filenames that are not metadata from the filenames.", "response": "def filter_filenames_by_info(self, filename_items):\n        \"\"\"Filter out file using metadata from the filenames.\n\n        Currently only uses start and end time. If only start time is available\n        from the filename, keep all the filename that have a start time before\n        the requested end time.\n        \"\"\"\n        for filename, filename_info in filename_items:\n            fend = filename_info.get('end_time')\n            fstart = filename_info.setdefault('start_time', fend)\n            if fend and fend < fstart:\n                # correct for filenames with 1 date and 2 times\n                fend = fend.replace(year=fstart.year,\n                                    month=fstart.month,\n                                    day=fstart.day)\n                filename_info['end_time'] = fend\n            if self.metadata_matches(filename_info):\n                yield filename, filename_info"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfilter out filehandlers using provide filter parameters.", "response": "def filter_fh_by_metadata(self, filehandlers):\n        \"\"\"Filter out filehandlers using provide filter parameters.\"\"\"\n        for filehandler in filehandlers:\n            filehandler.metadata['start_time'] = filehandler.start_time\n            filehandler.metadata['end_time'] = filehandler.end_time\n            if self.metadata_matches(filehandler.metadata, filehandler):\n                yield filehandler"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates filehandlers for a given filetype.", "response": "def new_filehandlers_for_filetype(self, filetype_info, filenames, fh_kwargs=None):\n        \"\"\"Create filehandlers for a given filetype.\"\"\"\n        filename_iter = self.filename_items_for_filetype(filenames,\n                                                         filetype_info)\n        if self.filter_filenames:\n            # preliminary filter of filenames based on start/end time\n            # to reduce the number of files to open\n            filename_iter = self.filter_filenames_by_info(filename_iter)\n        filehandler_iter = self.new_filehandler_instances(filetype_info,\n                                                          filename_iter,\n                                                          fh_kwargs=fh_kwargs)\n        filtered_iter = self.filter_fh_by_metadata(filehandler_iter)\n        return list(filtered_iter)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_filehandlers(self, filenames, fh_kwargs=None):\n        filenames = list(OrderedDict.fromkeys(filenames))\n        logger.debug(\"Assigning to %s: %s\", self.info['name'], filenames)\n\n        self.info.setdefault('filenames', []).extend(filenames)\n        filename_set = set(filenames)\n        created_fhs = {}\n        # load files that we know about by creating the file handlers\n        for filetype, filetype_info in self.sorted_filetype_items():\n            filehandlers = self.new_filehandlers_for_filetype(filetype_info,\n                                                              filename_set,\n                                                              fh_kwargs=fh_kwargs)\n\n            filename_set -= set([fhd.filename for fhd in filehandlers])\n            if filehandlers:\n                created_fhs[filetype] = filehandlers\n                self.file_handlers[filetype] = sorted(\n                    self.file_handlers.get(filetype, []) + filehandlers,\n                    key=lambda fhd: (fhd.start_time, fhd.filename))\n\n        # update existing dataset IDs with information from the file handler\n        self.update_ds_ids_from_file_handlers()\n\n        # load any additional dataset IDs determined dynamically from the file\n        self.add_ds_ids_from_files()\n        return created_fhs", "response": "Organize the filenames into file types and create file handlers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_ds_ids_from_file_handlers(self):\n        for file_handlers in self.file_handlers.values():\n            fh = file_handlers[0]\n            # update resolution in the dataset IDs for this files resolution\n            res = getattr(fh, 'resolution', None)\n            if res is None:\n                continue\n\n            for ds_id, ds_info in list(self.ids.items()):\n                file_types = ds_info['file_type']\n                if not isinstance(file_types, list):\n                    file_types = [file_types]\n                if fh.filetype_info['file_type'] not in file_types:\n                    continue\n                if ds_id.resolution is not None:\n                    continue\n                ds_info['resolution'] = res\n                new_id = DatasetID.from_dict(ds_info)\n                self.ids[new_id] = ds_info\n                del self.ids[ds_id]", "response": "Update DatasetIDs with information from loaded files."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck files for more dynamically discovered datasets.", "response": "def add_ds_ids_from_files(self):\n        \"\"\"Check files for more dynamically discovered datasets.\"\"\"\n        for file_handlers in self.file_handlers.values():\n            try:\n                fh = file_handlers[0]\n                avail_ids = fh.available_datasets()\n            except NotImplementedError:\n                continue\n\n            # dynamically discover other available datasets\n            for ds_id, ds_info in avail_ids:\n                # don't overwrite an existing dataset\n                # especially from the yaml config\n                coordinates = ds_info.get('coordinates')\n                if isinstance(coordinates, list):\n                    # xarray doesn't like concatenating attributes that are\n                    # lists: https://github.com/pydata/xarray/issues/2060\n                    ds_info['coordinates'] = tuple(ds_info['coordinates'])\n                self.ids.setdefault(ds_id, ds_info)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _load_dataset(dsid, ds_info, file_handlers, dim='y'):\n        slice_list = []\n        failure = True\n        for fh in file_handlers:\n            try:\n                projectable = fh.get_dataset(dsid, ds_info)\n                if projectable is not None:\n                    slice_list.append(projectable)\n                    failure = False\n            except KeyError:\n                logger.warning(\"Failed to load {} from {}\".format(dsid, fh),\n                               exc_info=True)\n\n        if failure:\n            raise KeyError(\n                \"Could not load {} from any provided files\".format(dsid))\n\n        if dim not in slice_list[0].dims:\n            return slice_list[0]\n        res = xr.concat(slice_list, dim=dim)\n\n        combined_info = file_handlers[0].combine_info(\n            [p.attrs for p in slice_list])\n\n        res.attrs = combined_info\n        return res", "response": "Load only a piece of the dataset."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _preferred_filetype(self, filetypes):\n        if not isinstance(filetypes, list):\n            filetypes = [filetypes]\n\n        # look through the file types and use the first one that we have loaded\n        for filetype in filetypes:\n            if filetype in self.file_handlers:\n                return filetype\n        return None", "response": "Get the preferred filetype out of the list of filetypes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _load_area_def(self, dsid, file_handlers):\n        area_defs = [fh.get_area_def(dsid) for fh in file_handlers]\n        area_defs = [area_def for area_def in area_defs\n                     if area_def is not None]\n\n        final_area = StackedAreaDefinition(*area_defs)\n        return final_area.squeeze()", "response": "Load the area definition of dsid."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the coordinate dataset keys for dsid.", "response": "def _get_coordinates_for_dataset_key(self, dsid):\n        \"\"\"Get the coordinate dataset keys for *dsid*.\"\"\"\n        ds_info = self.ids[dsid]\n        cids = []\n\n        for cinfo in ds_info.get('coordinates', []):\n            if not isinstance(cinfo, dict):\n                cinfo = {'name': cinfo}\n\n            cinfo['resolution'] = ds_info['resolution']\n            if 'polarization' in ds_info:\n                cinfo['polarization'] = ds_info['polarization']\n            cid = DatasetID(**cinfo)\n            cids.append(self.get_dataset_key(cid))\n\n        return cids"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting all coordinates for the given dataset keys.", "response": "def _get_coordinates_for_dataset_keys(self, dsids):\n        \"\"\"Get all coordinates.\"\"\"\n        coordinates = {}\n        for dsid in dsids:\n            cids = self._get_coordinates_for_dataset_key(dsid)\n            coordinates.setdefault(dsid, []).extend(cids)\n        return coordinates"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_file_handlers(self, dsid):\n        ds_info = self.ids[dsid]\n\n        filetype = self._preferred_filetype(ds_info['file_type'])\n        if filetype is None:\n            logger.warning(\"Required file type '%s' not found or loaded for \"\n                           \"'%s'\", ds_info['file_type'], dsid.name)\n        else:\n            return self.file_handlers[filetype]", "response": "Get the file handler to load this dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _make_area_from_coords(self, coords):\n        if len(coords) == 2:\n            lon_sn = coords[0].attrs.get('standard_name')\n            lat_sn = coords[1].attrs.get('standard_name')\n            if lon_sn == 'longitude' and lat_sn == 'latitude':\n                key = None\n                try:\n                    key = (coords[0].data.name, coords[1].data.name)\n                    sdef = self.coords_cache.get(key)\n                except AttributeError:\n                    sdef = None\n                if sdef is None:\n                    sdef = SwathDefinition(*coords)\n                    if key is not None:\n                        self.coords_cache[key] = sdef\n                sensor_str = '_'.join(self.info['sensors'])\n                shape_str = '_'.join(map(str, coords[0].shape))\n                sdef.name = \"{}_{}_{}_{}\".format(sensor_str, shape_str,\n                                                 coords[0].attrs['name'],\n                                                 coords[1].attrs['name'])\n                return sdef\n            else:\n                raise ValueError(\n                    'Coordinates info object missing standard_name key: ' +\n                    str(coords))\n        elif len(coords) != 0:\n            raise NameError(\"Don't know what to do with coordinates \" + str(\n                coords))", "response": "Create an appropriate area with the given coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload the area for the dataset.", "response": "def _load_dataset_area(self, dsid, file_handlers, coords):\n        \"\"\"Get the area for *dsid*.\"\"\"\n        try:\n            return self._load_area_def(dsid, file_handlers)\n        except NotImplementedError:\n            if any(x is None for x in coords):\n                logger.warning(\n                    \"Failed to load coordinates for '{}'\".format(dsid))\n                return None\n\n            area = self._make_area_from_coords(coords)\n            if area is None:\n                logger.debug(\"No coordinates found for %s\", str(dsid))\n            return area"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a dataset and its area if available.", "response": "def _load_dataset_with_area(self, dsid, coords):\n        \"\"\"Loads *dsid* and it's area if available.\"\"\"\n        file_handlers = self._get_file_handlers(dsid)\n        if not file_handlers:\n            return\n\n        area = self._load_dataset_area(dsid, file_handlers, coords)\n\n        try:\n            ds = self._load_dataset_data(file_handlers, dsid)\n        except (KeyError, ValueError) as err:\n            logger.exception(\"Could not load dataset '%s': %s\", dsid, str(err))\n            return None\n\n        if area is not None:\n            ds.attrs['area'] = area\n            calc_coords = (('x' not in ds.coords) or('y' not in ds.coords)) and hasattr(area, 'get_proj_vectors_dask')\n            if calc_coords and hasattr(area, 'get_proj_vectors'):\n                ds['x'], ds['y'] = area.get_proj_vectors()\n            elif calc_coords:\n                # older pyresample with dask-only method\n                ds['x'], ds['y'] = area.get_proj_vectors_dask(CHUNK_SIZE)\n        return ds"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _load_ancillary_variables(self, datasets):\n        all_av_ids = set()\n        for dataset in datasets.values():\n            ancillary_variables = dataset.attrs.get('ancillary_variables', [])\n            if not isinstance(ancillary_variables, (list, tuple, set)):\n                ancillary_variables = ancillary_variables.split(' ')\n            av_ids = []\n            for key in ancillary_variables:\n                try:\n                    av_ids.append(self.get_dataset_key(key))\n                except KeyError:\n                    logger.warning(\"Can't load ancillary dataset %s\", str(key))\n\n            all_av_ids |= set(av_ids)\n            dataset.attrs['ancillary_variables'] = av_ids\n        loadable_av_ids = [av_id for av_id in all_av_ids if av_id not in datasets]\n        if not all_av_ids:\n            return\n        if loadable_av_ids:\n            self.load(loadable_av_ids, previous_datasets=datasets)\n\n        for dataset in datasets.values():\n            new_vars = []\n            for av_id in dataset.attrs.get('ancillary_variables', []):\n                if isinstance(av_id, DatasetID):\n                    new_vars.append(datasets[av_id])\n                else:\n                    new_vars.append(av_id)\n            dataset.attrs['ancillary_variables'] = new_vars", "response": "Load the ancillary variables of datasets."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading the datasets with the given keys.", "response": "def load(self, dataset_keys, previous_datasets=None):\n        \"\"\"Load `dataset_keys`.\n\n        If `previous_datasets` is provided, do not reload those.\"\"\"\n        all_datasets = previous_datasets or DatasetDict()\n        datasets = DatasetDict()\n\n        # Include coordinates in the list of datasets to load\n        dsids = [self.get_dataset_key(ds_key) for ds_key in dataset_keys]\n        coordinates = self._get_coordinates_for_dataset_keys(dsids)\n        all_dsids = list(set().union(*coordinates.values())) + dsids\n\n        for dsid in all_dsids:\n            if dsid in all_datasets:\n                continue\n            coords = [all_datasets.get(cid, None)\n                      for cid in coordinates.get(dsid, [])]\n            ds = self._load_dataset_with_area(dsid, coords)\n            if ds is not None:\n                all_datasets[dsid] = ds\n                if dsid in dsids:\n                    datasets[dsid] = ds\n        self._load_ancillary_variables(all_datasets)\n\n        return datasets"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calibrate_counts(array, attributes, index):\n    offset = np.float32(attributes[\"corrected_counts_offsets\"][index])\n    scale = np.float32(attributes[\"corrected_counts_scales\"][index])\n    array = (array - offset) * scale\n    return array", "response": "Calibration for counts channels."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calibrate_bt(array, attributes, index, band_name):\n    offset = np.float32(attributes[\"radiance_offsets\"][index])\n    scale = np.float32(attributes[\"radiance_scales\"][index])\n\n    array = (array - offset) * scale\n\n    # Planck constant (Joule second)\n    h__ = np.float32(6.6260755e-34)\n\n    # Speed of light in vacuum (meters per second)\n    c__ = np.float32(2.9979246e+8)\n\n    # Boltzmann constant (Joules per Kelvin)\n    k__ = np.float32(1.380658e-23)\n\n    # Derived constants\n    c_1 = 2 * h__ * c__ * c__\n    c_2 = (h__ * c__) / k__\n\n    # Effective central wavenumber (inverse centimeters)\n    cwn = np.array([\n        2.641775E+3, 2.505277E+3, 2.518028E+3, 2.465428E+3,\n        2.235815E+3, 2.200346E+3, 1.477967E+3, 1.362737E+3,\n        1.173190E+3, 1.027715E+3, 9.080884E+2, 8.315399E+2,\n        7.483394E+2, 7.308963E+2, 7.188681E+2, 7.045367E+2],\n        dtype=np.float32)\n\n    # Temperature correction slope (no units)\n    tcs = np.array([\n        9.993411E-1, 9.998646E-1, 9.998584E-1, 9.998682E-1,\n        9.998819E-1, 9.998845E-1, 9.994877E-1, 9.994918E-1,\n        9.995495E-1, 9.997398E-1, 9.995608E-1, 9.997256E-1,\n        9.999160E-1, 9.999167E-1, 9.999191E-1, 9.999281E-1],\n        dtype=np.float32)\n\n    # Temperature correction intercept (Kelvin)\n    tci = np.array([\n        4.770532E-1, 9.262664E-2, 9.757996E-2, 8.929242E-2,\n        7.310901E-2, 7.060415E-2, 2.204921E-1, 2.046087E-1,\n        1.599191E-1, 8.253401E-2, 1.302699E-1, 7.181833E-2,\n        1.972608E-2, 1.913568E-2, 1.817817E-2, 1.583042E-2],\n        dtype=np.float32)\n\n    # Transfer wavenumber [cm^(-1)] to wavelength [m]\n    cwn = 1. / (cwn * 100)\n\n    # Some versions of the modis files do not contain all the bands.\n    emmissive_channels = [\"20\", \"21\", \"22\", \"23\", \"24\", \"25\", \"27\", \"28\", \"29\",\n                          \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\"]\n    global_index = emmissive_channels.index(band_name)\n\n    cwn = cwn[global_index]\n    tcs = tcs[global_index]\n    tci = tci[global_index]\n    array = c_2 / (cwn * xu.log(c_1 / (1000000 * array * cwn ** 5) + 1))\n    array = (array - tci) / tcs\n    return array", "response": "Calibration for the emissive channels."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_mda(attribute):\n        lines = attribute.split('\\n')\n        mda = {}\n        current_dict = mda\n        path = []\n        prev_line = None\n        for line in lines:\n            if not line:\n                continue\n            if line == 'END':\n                break\n            if prev_line:\n                line = prev_line + line\n            key, val = line.split('=')\n            key = key.strip()\n            val = val.strip()\n            try:\n                val = eval(val)\n            except NameError:\n                pass\n            except SyntaxError:\n                prev_line = line\n                continue\n            prev_line = None\n            if key in ['GROUP', 'OBJECT']:\n                new_dict = {}\n                path.append(val)\n                current_dict[val] = new_dict\n                current_dict = new_dict\n            elif key in ['END_GROUP', 'END_OBJECT']:\n                if val != path[-1]:\n                    raise SyntaxError\n                path = path[:-1]\n                current_dict = mda\n                for item in path:\n                    current_dict = current_dict[item]\n            elif key in ['CLASS', 'NUM_VAL']:\n                pass\n            else:\n                current_dict[key] = val\n        return mda", "response": "Read HDFEOS metadata and return a dict with all the key - value pairs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the dataset designated by key.", "response": "def get_dataset(self, key, info):\n        \"\"\"Get the dataset designated by *key*.\"\"\"\n        if key.name == 'solar_zenith_angle':\n            data = self.load('SolarZenith')\n        elif key.name == 'solar_azimuth_angle':\n            data = self.load('SolarAzimuth')\n        elif key.name == 'satellite_zenith_angle':\n            data = self.load('SensorZenith')\n        elif key.name == 'satellite_azimuth_angle':\n            data = self.load('SensorAzimuth')\n        elif key.name == 'longitude':\n            data = self.load('Longitude')\n        elif key.name == 'latitude':\n            data = self.load('Latitude')\n        else:\n            return\n        if key.resolution != self.georesolution:\n            # let's see if we have something in the cache\n            try:\n                data = self.cache[key.resolution][key.name]\n                data.attrs.update(info)\n                data.attrs['standard_name'] = data.attrs['name']\n                return data\n            except KeyError:\n                self.cache.setdefault(key.resolution, {})\n\n            # too bad, now we need to interpolate\n            satz = self.load('SensorZenith')\n            if key.name in ['longitude', 'latitude']:\n                data_a = self.load('Longitude')\n                data_b = self.load('Latitude')\n            elif key.name in ['satellite_azimuth_angle', 'satellite_zenith_angle']:\n                data_a = self.load('SensorAzimuth')\n                data_b = self.load('SensorZenith') - 90\n            elif key.name in ['solar_azimuth_angle', 'solar_zenith_angle']:\n                data_a = self.load('SolarAzimuth')\n                data_b = self.load('SolarZenith') - 90\n\n            data_a, data_b = self._interpolate(data_a, data_b, satz,\n                                               self.georesolution, key.resolution)\n\n            if key.name in ['longitude', 'latitude']:\n                self.cache[key.resolution]['longitude'] = data_a\n                self.cache[key.resolution]['latitude'] = data_b\n            elif key.name in ['satellite_azimuth_angle', 'satellite_zenith_angle']:\n                self.cache[key.resolution]['satellite_azimuth_angle'] = data_a\n                self.cache[key.resolution]['satellite_zenith_angle'] = data_b + 90\n            elif key.name in ['solar_azimuth_angle', 'solar_zenith_angle']:\n                self.cache[key.resolution]['solar_azimuth_angle'] = data_a\n                self.cache[key.resolution]['solar_zenith_angle'] = data_b + 90\n\n            data = self.cache[key.resolution][key.name]\n\n        data.attrs.update(info)\n        data.attrs['standard_name'] = data.attrs['name']\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load(self, file_key):\n        var = self.sd.select(file_key)\n        data = xr.DataArray(from_sds(var, chunks=CHUNK_SIZE),\n                            dims=['y', 'x']).astype(np.float32)\n        data = data.where(data != var._FillValue)\n        try:\n            data = data * np.float32(var.scale_factor)\n        except AttributeError:\n            pass\n        return data", "response": "Load the data from the file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading data from file and return the corresponding projectables.", "response": "def get_dataset(self, key, info):\n        \"\"\"Read data from file and return the corresponding projectables.\"\"\"\n        datadict = {\n            1000: ['EV_250_Aggr1km_RefSB',\n                   'EV_500_Aggr1km_RefSB',\n                   'EV_1KM_RefSB',\n                   'EV_1KM_Emissive'],\n            500: ['EV_250_Aggr500_RefSB',\n                  'EV_500_RefSB'],\n            250: ['EV_250_RefSB']}\n\n        platform_name = self.metadata['INVENTORYMETADATA']['ASSOCIATEDPLATFORMINSTRUMENTSENSOR'][\n            'ASSOCIATEDPLATFORMINSTRUMENTSENSORCONTAINER']['ASSOCIATEDPLATFORMSHORTNAME']['VALUE']\n\n        info.update({'platform_name': 'EOS-' + platform_name})\n        info.update({'sensor': 'modis'})\n\n        if self.resolution != key.resolution:\n            return\n\n        datasets = datadict[self.resolution]\n        for dataset in datasets:\n            subdata = self.sd.select(dataset)\n            var_attrs = subdata.attributes()\n            band_names = var_attrs[\"band_names\"].split(\",\")\n\n            # get the relative indices of the desired channel\n            try:\n                index = band_names.index(key.name)\n            except ValueError:\n                continue\n            uncertainty = self.sd.select(dataset + \"_Uncert_Indexes\")\n            array = xr.DataArray(from_sds(subdata, chunks=CHUNK_SIZE)[index, :, :],\n                                 dims=['y', 'x']).astype(np.float32)\n            valid_range = var_attrs['valid_range']\n\n            # Fill values:\n            # Data Value Meaning\n            # 65535 Fill Value (includes reflective band data at night mode\n            # and completely missing L1A scans)\n            # 65534 L1A DN is missing within a scan\n            # 65533 Detector is saturated\n            # 65532 Cannot compute zero point DN, e.g., SV is saturated\n            # 65531 Detector is dead (see comments below)\n            # 65530 RSB dn** below the minimum of the scaling range\n            # 65529 TEB radiance or RSB dn** exceeds the maximum of the\n            # scaling range\n            # 65528 Aggregation algorithm failure\n            # 65527 Rotation of Earth view Sector from nominal science\n            # collection position\n            # 65526 Calibration coefficient b1 could not be computed\n            # 65525 Subframe is dead\n            # 65524 Both sides of the PCLW electronics on simultaneously\n            # 65501 - 65523 (reserved for future use)\n            # 65500 NAD closed upper limit\n\n            array = array.where(array >= np.float32(valid_range[0]))\n            array = array.where(array <= np.float32(valid_range[1]))\n            array = array.where(from_sds(uncertainty, chunks=CHUNK_SIZE)[index, :, :] < 15)\n\n            if key.calibration == 'brightness_temperature':\n                projectable = calibrate_bt(array, var_attrs, index, key.name)\n                info.setdefault('units', 'K')\n                info.setdefault('standard_name', 'toa_brightness_temperature')\n            elif key.calibration == 'reflectance':\n                projectable = calibrate_refl(array, var_attrs, index)\n                info.setdefault('units', '%')\n                info.setdefault('standard_name',\n                                'toa_bidirectional_reflectance')\n            elif key.calibration == 'radiance':\n                projectable = calibrate_radiance(array, var_attrs, index)\n                info.setdefault('units', var_attrs.get('radiance_units'))\n                info.setdefault('standard_name',\n                                'toa_outgoing_radiance_per_unit_wavelength')\n            elif key.calibration == 'counts':\n                projectable = calibrate_counts(array, var_attrs, index)\n                info.setdefault('units', 'counts')\n                info.setdefault('standard_name', 'counts')  # made up\n            else:\n                raise ValueError(\"Unknown calibration for \"\n                                 \"key: {}\".format(key))\n            projectable.attrs = info\n\n            # if ((platform_name == 'Aqua' and key.name in [\"6\", \"27\", \"36\"]) or\n            #         (platform_name == 'Terra' and key.name in [\"29\"])):\n            #     height, width = projectable.shape\n            #     row_indices = projectable.mask.sum(1) == width\n            #     if row_indices.sum() != height:\n            #         projectable.mask[row_indices, :] = True\n\n            # Get the orbit number\n            # if not satscene.orbit:\n            #     mda = self.data.attributes()[\"CoreMetadata.0\"]\n            #     orbit_idx = mda.index(\"ORBITNUMBER\")\n            #     satscene.orbit = mda[orbit_idx + 111:orbit_idx + 116]\n\n            # Trimming out dead sensor lines (detectors) on terra:\n            # (in addition channel 27, 30, 34, 35, and 36 are nosiy)\n            # if satscene.satname == \"terra\":\n            #     for band in [\"29\"]:\n            #         if not satscene[band].is_loaded() or satscene[band].data.mask.all():\n            #             continue\n            #         width = satscene[band].data.shape[1]\n            #         height = satscene[band].data.shape[0]\n            #         indices = satscene[band].data.mask.sum(1) < width\n            #         if indices.sum() == height:\n            #             continue\n            #         satscene[band] = satscene[band].data[indices, :]\n            #         satscene[band].area = geometry.SwathDefinition(\n            #             lons=satscene[band].area.lons[indices, :],\n            #             lats=satscene[band].area.lats[indices, :])\n            return projectable"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread dataset from file", "response": "def read_dataset(fid, key):\n    \"\"\"Read dataset\"\"\"\n    dsid = DSET_NAMES[key.name]\n    dset = fid[\"/PWLR/\" + dsid]\n    if dset.ndim == 3:\n        dims = ['y', 'x', 'level']\n    else:\n        dims = ['y', 'x']\n    data = xr.DataArray(da.from_array(dset.value, chunks=CHUNK_SIZE),\n                        name=key.name, dims=dims).astype(np.float32)\n    data = xr.where(data > 1e30, np.nan, data)\n\n    dset_attrs = dict(dset.attrs)\n    data.attrs.update(dset_attrs)\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_geo(fid, key):\n    dsid = GEO_NAMES[key.name]\n    add_epoch = False\n    if \"time\" in key.name:\n        days = fid[\"/L1C/\" + dsid[\"day\"]].value\n        msecs = fid[\"/L1C/\" + dsid[\"msec\"]].value\n        data = _form_datetimes(days, msecs)\n        add_epoch = True\n        dtype = np.float64\n    else:\n        data = fid[\"/L1C/\" + dsid].value\n        dtype = np.float32\n    data = xr.DataArray(da.from_array(data, chunks=CHUNK_SIZE),\n                        name=key.name, dims=['y', 'x']).astype(dtype)\n\n    if add_epoch:\n        data.attrs['sensing_time_epoch'] = EPOCH\n\n    return data", "response": "Read geolocation and related datasets."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates seconds since EPOCH from days and milliseconds for each of IASI scan.", "response": "def _form_datetimes(days, msecs):\n    \"\"\"Calculate seconds since EPOCH from days and milliseconds for each of IASI scan.\"\"\"\n\n    all_datetimes = []\n    for i in range(days.size):\n        day = int(days[i])\n        msec = msecs[i]\n        scanline_datetimes = []\n        for j in range(int(VALUES_PER_SCAN_LINE / 4)):\n            usec = 1000 * (j * VIEW_TIME_ADJUSTMENT + msec)\n            delta = (dt.timedelta(days=day, microseconds=usec))\n            for k in range(4):\n                scanline_datetimes.append(delta.total_seconds())\n        all_datetimes.append(scanline_datetimes)\n\n    return np.array(all_datetimes, dtype=np.float64)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_dataset(self, key, info):\n        with h5py.File(self.filename, 'r') as fid:\n            LOGGER.debug('Reading %s.', key.name)\n            if key.name in DSET_NAMES:\n                m_data = read_dataset(fid, key)\n            else:\n                m_data = read_geo(fid, key)\n        m_data.attrs.update(info)\n        m_data.attrs['sensor'] = self.sensor\n\n        return m_data", "response": "Load a dataset from file"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves the image to the given filename in ninjotiff_ format.", "response": "def save_image(self, img, filename=None, **kwargs):  # floating_point=False,\n        \"\"\"Save the image to the given *filename* in ninjotiff_ format.\n\n        .. _ninjotiff: http://www.ssec.wisc.edu/~davidh/polar2grid/misc/NinJo_Satellite_Import_Formats.html\n        \"\"\"\n        filename = filename or self.get_filename(**img.data.attrs)\n        nt.save(img, filename, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_config(filename, *search_dirs, **kwargs):\n    config = kwargs.get(\"config_reader_class\", configparser.ConfigParser)()\n\n    paths = config_search_paths(filename, *search_dirs)\n    successes = config.read(reversed(paths))\n    if successes:\n        LOG.debug(\"Read config from %s\", str(successes))\n        return config, successes\n\n    LOG.warning(\"Couldn't file any config file matching %s\", filename)\n    return None, []", "response": "Blends the different configs from package defaults to."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef glob_config(pattern, *search_dirs):\n    patterns = config_search_paths(pattern, *search_dirs, check_exists=False)\n\n    for pattern in patterns:\n        for path in glob.iglob(pattern):\n            yield path", "response": "Return a generator that yields all possible configuration locations that match the pattern."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the appropriate path for a filename in that order.", "response": "def get_config_path(filename, *search_dirs):\n    \"\"\"Get the appropriate path for a filename, in that order: filename, ., PPP_CONFIG_DIR, package's etc dir.\"\"\"\n    paths = config_search_paths(filename, *search_dirs)\n\n    for path in paths[::-1]:\n        if os.path.exists(path):\n            return path"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_yaml_configs(configs, key):\n    diagnostic = {}\n    for i in configs:\n        for fname in i:\n            with open(fname) as stream:\n                try:\n                    res = yaml.load(stream, Loader=UnsafeLoader)\n                    msg = 'ok'\n                except yaml.YAMLError as err:\n                    stream.seek(0)\n                    res = yaml.load(stream, Loader=BaseLoader)\n                    if err.context == 'while constructing a Python object':\n                        msg = err.problem\n                    else:\n                        msg = 'error'\n                finally:\n                    try:\n                        diagnostic[res[key]['name']] = msg\n                    except (KeyError, TypeError):\n                        # this object doesn't have a 'name'\n                        pass\n    return diagnostic", "response": "Get a diagnostic for the yaml configs."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nimports the specified modules and provide status.", "response": "def _check_import(module_names):\n    \"\"\"Import the specified modules and provide status.\"\"\"\n    diagnostics = {}\n    for module_name in module_names:\n        try:\n            __import__(module_name)\n            res = 'ok'\n        except ImportError as err:\n            res = str(err)\n        diagnostics[module_name] = res\n    return diagnostics"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_satpy(readers=None, writers=None, extras=None):\n    from satpy.readers import configs_for_reader\n    from satpy.writers import configs_for_writer\n\n    print('Readers')\n    print('=======')\n    for reader, res in sorted(check_yaml_configs(configs_for_reader(reader=readers), 'reader').items()):\n        print(reader + ': ', res)\n    print()\n\n    print('Writers')\n    print('=======')\n    for writer, res in sorted(check_yaml_configs(configs_for_writer(writer=writers), 'writer').items()):\n        print(writer + ': ', res)\n    print()\n\n    print('Extras')\n    print('======')\n    module_names = extras if extras is not None else ('cartopy', 'geoviews')\n    for module_name, res in sorted(_check_import(module_names).items()):\n        print(module_name + ': ', res)\n    print()", "response": "Checks the satpy readers and writers for correct installation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the dir of f exists otherwise create it.", "response": "def ensure_dir(filename):\n    \"\"\"Checks if the dir of f exists, otherwise create it.\"\"\"\n    directory = os.path.dirname(filename)\n    if directory and not os.path.isdir(directory):\n        os.makedirs(directory)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a logger with null handler added if needed.", "response": "def get_logger(name):\n    \"\"\"Return logger with null handler added if needed.\"\"\"\n    if not hasattr(logging.Logger, 'trace'):\n        logging.addLevelName(TRACE_LEVEL, 'TRACE')\n\n        def trace(self, message, *args, **kwargs):\n            if self.isEnabledFor(TRACE_LEVEL):\n                # Yes, logger takes its '*args' as 'args'.\n                self._log(TRACE_LEVEL, message, args, **kwargs)\n\n        logging.Logger.trace = trace\n\n    log = logging.getLogger(name)\n    if not log.handlers:\n        log.addHandler(logging.NullHandler())\n    return log"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef lonlat2xyz(lon, lat):\n    lat = xu.deg2rad(lat)\n    lon = xu.deg2rad(lon)\n    x = xu.cos(lat) * xu.cos(lon)\n    y = xu.cos(lat) * xu.sin(lon)\n    z = xu.sin(lat)\n    return x, y, z", "response": "Convert lon lat to cartesian."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef xyz2lonlat(x, y, z):\n    lon = xu.rad2deg(xu.arctan2(y, x))\n    lat = xu.rad2deg(xu.arctan2(z, xu.sqrt(x**2 + y**2)))\n    return lon, lat", "response": "Convert cartesian to lon lat."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef angle2xyz(azi, zen):\n    azi = xu.deg2rad(azi)\n    zen = xu.deg2rad(zen)\n    x = xu.sin(zen) * xu.sin(azi)\n    y = xu.sin(zen) * xu.cos(azi)\n    z = xu.cos(zen)\n    return x, y, z", "response": "Convert azimuth and zenith to cartesian."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef xyz2angle(x, y, z):\n    azi = xu.rad2deg(xu.arctan2(x, y))\n    zen = 90 - xu.rad2deg(xu.arctan2(z, xu.sqrt(x**2 + y**2)))\n    return azi, zen", "response": "Convert cartesian to azimuth and zenith."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert projection units from kilometers to meters.", "response": "def proj_units_to_meters(proj_str):\n    \"\"\"Convert projection units from kilometers to meters.\"\"\"\n    proj_parts = proj_str.split()\n    new_parts = []\n    for itm in proj_parts:\n        key, val = itm.split('=')\n        key = key.strip('+')\n        if key in ['a', 'b', 'h']:\n            val = float(val)\n            if val < 6e6:\n                val *= 1000.\n                val = '%.3f' % val\n\n        if key == 'units' and val == 'km':\n            continue\n\n        new_parts.append('+%s=%s' % (key, val))\n\n    return ' '.join(new_parts)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sunzen_corr_cos(data, cos_zen, limit=88., max_sza=95.):\n\n    # Convert the zenith angle limit to cosine of zenith angle\n    limit_rad = np.deg2rad(limit)\n    limit_cos = np.cos(limit_rad)\n    max_sza_rad = np.deg2rad(max_sza) if max_sza is not None else max_sza\n\n    # Cosine correction\n    corr = 1. / cos_zen\n    if max_sza is not None:\n        # gradually fall off for larger zenith angle\n        grad_factor = (np.arccos(cos_zen) - limit_rad) / (max_sza_rad - limit_rad)\n        # invert the factor so maximum correction is done at `limit` and falls off later\n        grad_factor = 1. - np.log(grad_factor + 1) / np.log(2)\n        # make sure we don't make anything negative\n        grad_factor = grad_factor.clip(0.)\n    else:\n        # Use constant value (the limit) for larger zenith angles\n        grad_factor = 1.\n    corr = corr.where(cos_zen > limit_cos, grad_factor / limit_cos)\n    # Force \"night\" pixels to 0 (where SZA is invalid)\n    corr = corr.where(cos_zen.notnull(), 0)\n\n    return data * corr", "response": "Perform sunzenith angle correction on the provided cos_zen."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves an image object to disk.", "response": "def save_image(self, img, filename=None, compute=True, **kwargs):\n        \"\"\"Save Image object to a given ``filename``.\n\n        Args:\n            img (trollimage.xrimage.XRImage): Image object to save to disk.\n            filename (str): Optionally specify the filename to save this\n                            dataset to. It may include string formatting\n                            patterns that will be filled in by dataset\n                            attributes.\n            compute (bool): If `True` (default), compute and save the dataset.\n                            If `False` return either a `dask.delayed.Delayed`\n                            object or tuple of (source, target). See the\n                            return values below for more information.\n            **kwargs: Keyword arguments to pass to the images `save` method.\n\n        Returns:\n            Value returned depends on `compute`. If `compute` is `True` then\n            the return value is the result of computing a\n            `dask.delayed.Delayed` object or running `dask.array.store`. If\n            `compute` is `False` then the returned value is either a\n            `dask.delayed.Delayed` object that can be computed using\n            `delayed.compute()` or a tuple of (source, target) that should be\n            passed to `dask.array.store`. If target is provided the the caller\n            is responsible for calling `target.close()` if the target has\n            this method.\n\n        \"\"\"\n        filename = filename or self.get_filename(**img.data.attrs)\n\n        LOG.debug(\"Saving to image: %s\", filename)\n        return img.save(filename, compute=compute, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine the sensor for this file.", "response": "def _get_sensor(self):\n        \"\"\"Determine the sensor for this file.\"\"\"\n        # sometimes Himawari-8 (or 9) data is stored in SCMI format\n        is_h8 = 'H8' in self.platform_name\n        is_h9 = 'H9' in self.platform_name\n        is_ahi = is_h8 or is_h9\n        return 'ahi' if is_ahi else 'abi'"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_dataset(self, key, info):\n        logger.debug('Reading in get_dataset %s.', key.name)\n        var_name = info.get('file_key', self.filetype_info.get('file_key'))\n        if var_name:\n            data = self[var_name]\n        elif 'Sectorized_CMI' in self.nc:\n            data = self['Sectorized_CMI']\n        elif 'data' in self.nc:\n            data = self['data']\n        # NetCDF doesn't support multi-threaded reading, trick it by opening\n        # as one whole chunk then split it up before we do any calculations\n        data = data.chunk({'x': CHUNK_SIZE, 'y': CHUNK_SIZE})\n\n        # convert to satpy standard units\n        factor = data.attrs.pop('scale_factor', 1)\n        offset = data.attrs.pop('add_offset', 0)\n        units = data.attrs.get('units', 1)\n        # the '*1' unit is some weird convention added/needed by AWIPS\n        if units in ['1', '*1'] and key.calibration == 'reflectance':\n            data *= 100\n            factor *= 100  # used for valid_min/max\n            data.attrs['units'] = '%'\n\n        # set up all the attributes that might be useful to the user/satpy\n        data.attrs.update({'platform_name': self.platform_name,\n                           'sensor': data.attrs.get('sensor', self.sensor),\n                           })\n        if 'satellite_longitude' in self.nc.attrs:\n            data.attrs['satellite_longitude'] = self.nc.attrs['satellite_longitude']\n            data.attrs['satellite_latitude'] = self.nc.attrs['satellite_latitude']\n            data.attrs['satellite_altitude'] = self.nc.attrs['satellite_altitude']\n\n        scene_id = self.nc.attrs.get('scene_id')\n        if scene_id is not None:\n            data.attrs['scene_id'] = scene_id\n        data.attrs.update(key.to_dict())\n        data.attrs.pop('_FillValue', None)\n        if 'valid_min' in data.attrs:\n            vmin = data.attrs.pop('valid_min')\n            vmax = data.attrs.pop('valid_max')\n            vmin = vmin * factor + offset\n            vmax = vmax * factor + offset\n            data.attrs['valid_min'] = vmin\n            data.attrs['valid_max'] = vmax\n        return data", "response": "Load a dataset from NetCDF file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfigure out which grid mapping should be used", "response": "def _get_cf_grid_mapping_var(self):\n        \"\"\"Figure out which grid mapping should be used\"\"\"\n        gmaps = ['fixedgrid_projection', 'goes_imager_projection',\n                 'lambert_projection', 'polar_projection',\n                 'mercator_projection']\n        if 'grid_mapping' in self.filename_info:\n            gmaps = [self.filename_info.get('grid_mapping')] + gmaps\n        for grid_mapping in gmaps:\n            if grid_mapping in self.nc:\n                return self.nc[grid_mapping]\n        raise KeyError(\"Can't find grid mapping variable in SCMI file\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_proj4_name(self, projection):\n        gmap_name = projection.attrs['grid_mapping_name']\n        proj = {\n            'geostationary': 'geos',\n            'lambert_conformal_conic': 'lcc',\n            'polar_stereographic': 'stere',\n            'mercator': 'merc',\n        }.get(gmap_name, gmap_name)\n        return proj", "response": "Map CF projection name to PROJ. 4 name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_proj_specific_params(self, projection):\n        proj = self._get_proj4_name(projection)\n        proj_dict = {\n            'proj': proj,\n            'a': float(projection.attrs['semi_major_axis']),\n            'b': float(projection.attrs['semi_minor_axis']),\n            'units': 'm',\n        }\n        if proj == 'geos':\n            proj_dict['h'] = float(projection.attrs['perspective_point_height'])\n            proj_dict['sweep'] = projection.attrs.get('sweep_angle_axis', 'y')\n            proj_dict['lon_0'] = float(projection.attrs['longitude_of_projection_origin'])\n            proj_dict['lat_0'] = float(projection.attrs.get('latitude_of_projection_origin', 0.0))\n        elif proj == 'lcc':\n            proj_dict['lat_0'] = float(projection.attrs['standard_parallel'])\n            proj_dict['lon_0'] = float(projection.attrs['longitude_of_central_meridian'])\n            proj_dict['lat_1'] = float(projection.attrs['latitude_of_projection_origin'])\n        elif proj == 'stere':\n            proj_dict['lat_ts'] = float(projection.attrs['standard_parallel'])\n            proj_dict['lon_0'] = float(projection.attrs['straight_vertical_longitude_from_pole'])\n            proj_dict['lat_0'] = float(projection.attrs['latitude_of_projection_origin'])\n        elif proj == 'merc':\n            proj_dict['lat_ts'] = float(projection.attrs['standard_parallel'])\n            proj_dict['lat_0'] = proj_dict['lat_ts']\n            proj_dict['lon_0'] = float(projection.attrs['longitude_of_projection_origin'])\n        else:\n            raise ValueError(\"Can't handle projection '{}'\".format(proj))\n        return proj_dict", "response": "Convert CF projection parameters to PROJ. 4 dict."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _calc_extents(self, proj_dict):\n        h = float(proj_dict.get('h', 1.))  # force to 64-bit float\n        x = self['x']\n        y = self['y']\n        x_units = x.attrs.get('units', 'rad')\n        if x_units == 'meters':\n            h_factor = 1.\n            factor = 1.\n        elif x_units == 'microradian':\n            h_factor = h\n            factor = 1e6\n        else:  # radians\n            h_factor = h\n            factor = 1.\n        x_l = h_factor * x[0] / factor\n        x_r = h_factor * x[-1] / factor\n        y_l = h_factor * y[-1] / factor\n        y_u = h_factor * y[0] / factor\n        x_half = (x_r - x_l) / (self.ncols - 1) / 2.\n        y_half = (y_u - y_l) / (self.nlines - 1) / 2.\n        return x_l - x_half, y_l - y_half, x_r + x_half, y_u + y_half", "response": "Calculate area extents from x and y variables."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the area definition of the data at hand.", "response": "def get_area_def(self, key):\n        \"\"\"Get the area definition of the data at hand.\"\"\"\n        # FIXME: Can't we pass dataset info to the get_area_def?\n        projection = self._get_cf_grid_mapping_var()\n        proj_dict = self._get_proj_specific_params(projection)\n        area_extent = self._calc_extents(proj_dict)\n        area_name = '{}_{}'.format(self.sensor, proj_dict['proj'])\n        return geometry.AreaDefinition(\n            area_name,\n            \"SCMI file area\",\n            area_name,\n            proj_dict,\n            self.ncols,\n            self.nlines,\n            np.asarray(area_extent))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_datasets(self, datasets, filename=None, fill_value=None,\n                      compute=True, **kwargs):\n        \"\"\"Save all datasets to one or more files.\n        \"\"\"\n        LOG.debug(\"Starting in mitiff save_datasets ... \")\n\n        def _delayed_create(create_opts, datasets):\n            LOG.debug(\"create_opts: %s\", create_opts)\n            try:\n                if 'platform_name' not in kwargs:\n                    kwargs['platform_name'] = datasets.attrs['platform_name']\n                if 'name' not in kwargs:\n                    kwargs['name'] = datasets[0].attrs['name']\n                if 'start_time' not in kwargs:\n                    kwargs['start_time'] = datasets[0].attrs['start_time']\n                if 'sensor' not in kwargs:\n                    kwargs['sensor'] = datasets[0].attrs['sensor']\n\n                try:\n                    self.mitiff_config[kwargs['sensor']] = datasets['metadata_requirements']['config']\n                    self.translate_channel_name[kwargs['sensor']] = datasets['metadata_requirements']['translate']\n                    self.channel_order[kwargs['sensor']] = datasets['metadata_requirements']['order']\n                    self.file_pattern = datasets['metadata_requirements']['file_pattern']\n                except KeyError:\n                    # For some mitiff products this info is needed, for others not.\n                    # If needed you should know how to fix this\n                    pass\n\n                image_description = self._make_image_description(datasets, **kwargs)\n                LOG.debug(\"File pattern %s\", self.file_pattern)\n                if isinstance(datasets, list):\n                    kwargs['start_time'] = datasets[0].attrs['start_time']\n                else:\n                    kwargs['start_time'] = datasets.attrs['start_time']\n                gen_filename = filename or self.get_filename(**kwargs)\n                LOG.info(\"Saving mitiff to: %s ...\", gen_filename)\n                self._save_datasets_as_mitiff(datasets, image_description, gen_filename, **kwargs)\n            except (KeyError, ValueError, RuntimeError):\n                raise\n\n        create_opts = ()\n        delayed = dask.delayed(_delayed_create)(create_opts, datasets)\n        LOG.debug(\"About to call delayed compute ...\")\n        if compute:\n            return delayed.compute()\n        return delayed", "response": "Save all datasets to one or more files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating image description for a given set of datasets.", "response": "def _make_image_description(self, datasets, **kwargs):\n        \"\"\"\n        generate image description for mitiff.\n\n        Satellite: NOAA 18\n        Date and Time: 06:58 31/05-2016\n        SatDir: 0\n        Channels:   6 In this file: 1-VIS0.63 2-VIS0.86 3(3B)-IR3.7\n        4-IR10.8 5-IR11.5 6(3A)-VIS1.6\n        Xsize:  4720\n        Ysize:  5544\n        Map projection: Stereographic\n        Proj string: +proj=stere +lon_0=0 +lat_0=90 +lat_ts=60\n        +ellps=WGS84 +towgs84=0,0,0 +units=km\n        +x_0=2526000.000000 +y_0=5806000.000000\n        TrueLat: 60 N\n        GridRot: 0\n        Xunit:1000 m Yunit: 1000 m\n        NPX: 0.000000 NPY: 0.000000\n        Ax: 1.000000 Ay: 1.000000 Bx: -2526.000000 By: -262.000000\n\n        Satellite: <satellite name>\n        Date and Time: <HH:MM dd/mm-yyyy>\n        SatDir: 0\n        Channels:   <number of chanels> In this file: <channels names in order>\n        Xsize:  <number of pixels x>\n        Ysize:  <number of pixels y>\n        Map projection: Stereographic\n        Proj string: <proj4 string with +x_0 and +y_0 which is the positive\n        distance from proj origo\n        to the lower left corner of the image data>\n        TrueLat: 60 N\n        GridRot: 0\n        Xunit:1000 m Yunit: 1000 m\n        NPX: 0.000000 NPY: 0.000000\n        Ax: <pixels size x in km> Ay: <pixel size y in km> Bx: <left corner of\n        upper right pixel in km>\n        By: <upper corner of upper right pixel in km>\n\n\n        if palette image write special palette\n        if normal channel write table calibration:\n        Table_calibration: <channel name>, <calibration type>, [<unit>],\n        <no of bits of data>,\n        [<calibration values space separated>]\\n\\n\n        \"\"\"\n\n        translate_platform_name = {'metop01': 'Metop-B',\n                                   'metop02': 'Metop-A',\n                                   'metop03': 'Metop-C',\n                                   'noaa15': 'NOAA-15',\n                                   'noaa16': 'NOAA-16',\n                                   'noaa17': 'NOAA-17',\n                                   'noaa18': 'NOAA-18',\n                                   'noaa19': 'NOAA-19'}\n\n        first_dataset = datasets\n        if isinstance(datasets, list):\n            LOG.debug(\"Datasets is a list of dataset\")\n            first_dataset = datasets[0]\n\n        if 'platform_name' in first_dataset.attrs:\n            _platform_name = translate_platform_name.get(\n                first_dataset.attrs['platform_name'],\n                first_dataset.attrs['platform_name'])\n        elif 'platform_name' in kwargs:\n            _platform_name = translate_platform_name.get(\n                kwargs['platform_name'], kwargs['platform_name'])\n        else:\n            _platform_name = None\n\n        _image_description = ''\n        _image_description.encode('utf-8')\n\n        _image_description += ' Satellite: '\n        if _platform_name is not None:\n            _image_description += _platform_name\n\n        _image_description += '\\n'\n\n        _image_description += ' Date and Time: '\n        # Select earliest start_time\n        first = True\n        earliest = 0\n        for dataset in datasets:\n            if first:\n                earliest = dataset.attrs['start_time']\n            else:\n                if dataset.attrs['start_time'] < earliest:\n                    earliest = dataset.attrs['start_time']\n            first = False\n        LOG.debug(\"earliest start_time: %s\", earliest)\n        _image_description += earliest.strftime(\"%H:%M %d/%m-%Y\\n\")\n\n        _image_description += ' SatDir: 0\\n'\n\n        _image_description += ' Channels: '\n\n        if isinstance(datasets, list):\n            LOG.debug(\"len datasets: %s\", len(datasets))\n            _image_description += str(len(datasets))\n        elif 'bands' in datasets.sizes:\n            LOG.debug(\"len datasets: %s\", datasets.sizes['bands'])\n            _image_description += str(datasets.sizes['bands'])\n        elif len(datasets.sizes) == 2:\n            LOG.debug(\"len datasets: 1\")\n            _image_description += '1'\n\n        _image_description += ' In this file: '\n\n        channels = self._make_channel_list(datasets, **kwargs)\n\n        try:\n            cns = self.translate_channel_name.get(kwargs['sensor'], {})\n        except KeyError:\n            pass\n\n        _image_description += self._channel_names(channels, cns, **kwargs)\n\n        _image_description += self._add_sizes(datasets, first_dataset)\n\n        _image_description += ' Map projection: Stereographic\\n'\n\n        _image_description += self._add_proj4_string(datasets, first_dataset)\n\n        _image_description += ' TrueLat: 60N\\n'\n        _image_description += ' GridRot: 0\\n'\n\n        _image_description += ' Xunit:1000 m Yunit: 1000 m\\n'\n\n        _image_description += ' NPX: %.6f' % (0)\n        _image_description += ' NPY: %.6f' % (0) + '\\n'\n\n        _image_description += self._add_pixel_sizes(datasets, first_dataset)\n        _image_description += self._add_corners(datasets, first_dataset)\n\n        if isinstance(datasets, list):\n            LOG.debug(\"Area extent: %s\", first_dataset.attrs['area'].area_extent)\n        else:\n            LOG.debug(\"Area extent: %s\", datasets.attrs['area'].area_extent)\n\n        _image_description += self._add_calibration(channels, cns, datasets, **kwargs)\n\n        return _image_description"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave all togehter and save as a mitiff file with the special tag making it a mitiff file.", "response": "def _save_datasets_as_mitiff(self, datasets, image_description,\n                                 gen_filename, **kwargs):\n        \"\"\"Put all togehter and save as a tiff file with the special tag\n           making it a mitiff file.\n        \"\"\"\n        from libtiff import TIFF\n\n        tif = TIFF.open(gen_filename, mode='w')\n\n        tif.SetField(IMAGEDESCRIPTION, (image_description).encode('utf-8'))\n\n        cns = self.translate_channel_name.get(kwargs['sensor'], {})\n        if isinstance(datasets, list):\n            LOG.debug(\"Saving datasets as list\")\n\n            for _cn in self.channel_order[kwargs['sensor']]:\n                for dataset in datasets:\n                    if dataset.attrs['name'] == _cn:\n                        reverse_offset = 0.\n                        reverse_scale = 1.\n                        if dataset.attrs['calibration'] == 'brightness_temperature':\n                            reverse_offset = 255.\n                            reverse_scale = -1.\n                            dataset.data += KELVIN_TO_CELSIUS\n\n                        # Need to possible translate channels names from satpy to mitiff\n                        cn = cns.get(dataset.attrs['name'], dataset.attrs['name'])\n                        _data = reverse_offset + reverse_scale * (((dataset.data - float(self.mitiff_config[\n                            kwargs['sensor']][cn]['min-val'])) /\n                            (float(self.mitiff_config[kwargs['sensor']][cn]['max-val']) -\n                             float(self.mitiff_config[kwargs['sensor']][cn]['min-val']))) * 255.)\n                        data = _data.clip(0, 255)\n\n                        tif.write_image(data.astype(np.uint8), compression='deflate')\n                        break\n        elif 'dataset' in datasets.attrs['name']:\n            LOG.debug(\"Saving %s as a dataset.\", datasets.attrs['name'])\n            for _cn in self.channel_order[kwargs['sensor']]:\n                for i, band in enumerate(datasets['bands']):\n                    if band == _cn:\n                        chn = datasets.sel(bands=band)\n                        reverse_offset = 0.\n                        reverse_scale = 1.\n                        if chn.attrs['prerequisites'][i][4] == 'brightness_temperature':\n                            reverse_offset = 255.\n                            reverse_scale = -1.\n                            chn.data += KELVIN_TO_CELSIUS\n\n                        # Need to possible translate channels names from satpy to mitiff\n                        cn = cns.get(chn.attrs['prerequisites'][i][0],\n                                     chn.attrs['prerequisites'][i][0])\n                        _data = reverse_offset + reverse_scale * (((chn.data - float(self.mitiff_config[\n                            kwargs['sensor']][cn]['min-val'])) /\n                            (float(self.mitiff_config[kwargs['sensor']][cn]['max-val']) -\n                             float(self.mitiff_config[kwargs['sensor']][cn]['min-val']))) * 255.)\n                        data = _data.clip(0, 255)\n\n                        tif.write_image(data.astype(np.uint8), compression='deflate')\n                        break\n\n        else:\n            LOG.debug(\"Saving datasets as enhanced image\")\n            img = get_enhanced_image(datasets.squeeze(), enhance=self.enhancer)\n            for i, band in enumerate(img.data['bands']):\n                chn = img.data.sel(bands=band)\n                data = chn.values.clip(0, 1) * 254. + 1\n                data = data.clip(0, 255)\n                tif.write_image(data.astype(np.uint8), compression='deflate')\n\n        del tif"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef timecds2datetime(tcds):\n\n    days = int(tcds['Days'])\n    milliseconds = int(tcds['Milliseconds'])\n    try:\n        microseconds = int(tcds['Microseconds'])\n    except (KeyError, ValueError):\n        microseconds = 0\n    try:\n        microseconds += int(tcds['Nanoseconds']) / 1000.\n    except (KeyError, ValueError):\n        pass\n\n    reference = datetime(1958, 1, 1)\n    delta = timedelta(days=days, milliseconds=milliseconds,\n                      microseconds=microseconds)\n\n    return reference + delta", "response": "Method for converting time_cds - variables to datetime - objectsself."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload a dataset from the file.", "response": "def get_dataset(self, key, info):\n        \"\"\"Load a dataset.\"\"\"\n        if key.name in ['owiLat', 'owiLon']:\n            if self.lons is None or self.lats is None:\n                self.lons = self.nc['owiLon']\n                self.lats = self.nc['owiLat']\n            if key.name == 'owiLat':\n                res = self.lats\n            else:\n                res = self.lons\n            res.attrs = info\n        else:\n            res = self.nc[key.name]\n            if key.name in ['owiHs', 'owiWl', 'owiDirmet']:\n                res = xr.DataArray(res, dims=['y', 'x', 'oswPartitions'])\n            elif key.name in ['owiNrcs', 'owiNesz', 'owiNrcsNeszCorr']:\n                res = xr.DataArray(res, dims=['y', 'x', 'oswPolarisation'])\n            elif key.name in ['owiPolarisationName']:\n                res = xr.DataArray(res, dims=['owiPolarisation'])\n            elif key.name in ['owiCalConstObsi', 'owiCalConstInci']:\n                res = xr.DataArray(res, dims=['owiIncSize'])\n            elif key.name.startswith('owi'):\n                res = xr.DataArray(res, dims=['y', 'x'])\n            else:\n                res = xr.DataArray(res, dims=['y', 'x'])\n            res.attrs.update(info)\n            if '_FillValue' in res.attrs:\n                res = res.where(res != res.attrs['_FillValue'])\n                res.attrs['_FillValue'] = np.nan\n\n        if 'missionName' in self.nc.attrs:\n            res.attrs.update({'platform_name': self.nc.attrs['missionName']})\n\n        res.attrs.update({'fstart_time': self._fstart_time})\n        res.attrs.update({'fend_time': self._fend_time})\n\n        if not self._shape:\n            self._shape = res.shape\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef show(data, negate=False):\n    from PIL import Image as pil\n    data = np.array((data - data.min()) * 255.0 /\n                    (data.max() - data.min()), np.uint8)\n    if negate:\n        data = 255 - data\n    img = pil.fromarray(data)\n    img.show()", "response": "Show the stretched data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_prologue(self):\n\n        with open(self.filename, \"rb\") as fp_:\n            fp_.seek(self.mda['total_header_length'])\n            data = np.fromfile(fp_, dtype=hrit_prologue, count=1)\n            self.prologue.update(recarray2dict(data))\n            try:\n                impf = np.fromfile(fp_, dtype=impf_configuration, count=1)[0]\n            except IndexError:\n                logger.info('No IMPF configuration field found in prologue.')\n            else:\n                self.prologue.update(recarray2dict(impf))", "response": "Read the prologue metadata."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_satpos(self):\n        if self.satpos is None:\n            logger.debug(\"Computing actual satellite position\")\n\n            try:\n                # Get satellite position in cartesian coordinates\n                x, y, z = self._get_satpos_cart()\n\n                # Transform to geodetic coordinates\n                geocent = pyproj.Proj(proj='geocent')\n                a, b = self.get_earth_radii()\n                latlong = pyproj.Proj(proj='latlong', a=a, b=b, units='m')\n                lon, lat, alt = pyproj.transform(geocent, latlong, x, y, z)\n            except NoValidNavigationCoefs as err:\n                logger.warning(err)\n                lon = lat = alt = None\n\n            # Cache results\n            self.satpos = lon, lat, alt\n\n        return self.satpos", "response": "Get actual satellite position in geodetic coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_satpos_cart(self):\n        orbit_polynomial = self.prologue['SatelliteStatus']['Orbit']['OrbitPolynomial']\n\n        # Find Chebyshev coefficients for the given time\n        coef_idx = self._find_navigation_coefs()\n        tstart = orbit_polynomial['StartTime'][0, coef_idx]\n        tend = orbit_polynomial['EndTime'][0, coef_idx]\n\n        # Obtain cartesian coordinates (x, y, z) of the satellite by evaluating the Chebyshev polynomial at the\n        # start time of the scan. Express timestamps in microseconds since 1970-01-01 00:00.\n        time = self.prologue['ImageAcquisition']['PlannedAcquisitionTime']['TrueRepeatCycleStart']\n        time64 = np.datetime64(time).astype('int64')\n        domain = [np.datetime64(tstart).astype('int64'),\n                  np.datetime64(tend).astype('int64')]\n        x = chebyshev(coefs=orbit_polynomial['X'][coef_idx], time=time64, domain=domain)\n        y = chebyshev(coefs=orbit_polynomial['Y'][coef_idx], time=time64, domain=domain)\n        z = chebyshev(coefs=orbit_polynomial['Z'][coef_idx], time=time64, domain=domain)\n\n        return x*1000, y*1000, z*1000", "response": "Determine satellite position in earth - centered cartesion coordinates"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds the navigation coefficients for the current time.", "response": "def _find_navigation_coefs(self):\n        \"\"\"Find navigation coefficients for the current time\n\n        The navigation Chebyshev coefficients are only valid for a certain time interval. The header entry\n        SatelliteStatus/Orbit/OrbitPolynomial contains multiple coefficients for multiple time intervals. Find the\n        coefficients which are valid for the nominal timestamp of the scan.\n\n        Returns: Corresponding index in the coefficient list.\n        \"\"\"\n        # Find index of interval enclosing the nominal timestamp of the scan\n        time = np.datetime64(self.prologue['ImageAcquisition']['PlannedAcquisitionTime']['TrueRepeatCycleStart'])\n        intervals_tstart = self.prologue['SatelliteStatus']['Orbit']['OrbitPolynomial']['StartTime'][0].astype(\n            'datetime64[us]')\n        intervals_tend = self.prologue['SatelliteStatus']['Orbit']['OrbitPolynomial']['EndTime'][0].astype(\n            'datetime64[us]')\n        try:\n            return np.where(np.logical_and(time >= intervals_tstart, time < intervals_tend))[0][0]\n        except IndexError:\n            raise NoValidNavigationCoefs('Unable to find navigation coefficients valid for {}'.format(time))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting earth radii from prologue", "response": "def get_earth_radii(self):\n        \"\"\"Get earth radii from prologue\n\n        Returns:\n            Equatorial radius, polar radius [m]\n        \"\"\"\n        earth_model = self.prologue['GeometricProcessing']['EarthModel']\n        a = earth_model['EquatorialRadius'] * 1000\n        b = (earth_model['NorthPolarRadius'] +\n             earth_model['SouthPolarRadius']) / 2.0 * 1000\n        return a, b"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the epilogue metadata.", "response": "def read_epilogue(self):\n        \"\"\"Read the epilogue metadata.\"\"\"\n\n        with open(self.filename, \"rb\") as fp_:\n            fp_.seek(self.mda['total_header_length'])\n            data = np.fromfile(fp_, dtype=hrit_epilogue, count=1)\n            self.epilogue.update(recarray2dict(data))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_header(self):\n\n        earth_model = self.prologue['GeometricProcessing']['EarthModel']\n        self.mda['offset_corrected'] = earth_model['TypeOfEarthModel'] == 2\n\n        # Projection\n        a, b = self.prologue_.get_earth_radii()\n        self.mda['projection_parameters']['a'] = a\n        self.mda['projection_parameters']['b'] = b\n        ssp = self.prologue['ImageDescription'][\n            'ProjectionDescription']['LongitudeOfSSP']\n        self.mda['projection_parameters']['SSP_longitude'] = ssp\n        self.mda['projection_parameters']['SSP_latitude'] = 0.0\n\n        # Navigation\n        actual_lon, actual_lat, actual_alt = self.prologue_.get_satpos()\n        self.mda['navigation_parameters']['satellite_nominal_longitude'] = self.prologue['SatelliteStatus'][\n            'SatelliteDefinition']['NominalLongitude']\n        self.mda['navigation_parameters']['satellite_nominal_latitude'] = 0.0\n        self.mda['navigation_parameters']['satellite_actual_longitude'] = actual_lon\n        self.mda['navigation_parameters']['satellite_actual_latitude'] = actual_lat\n        self.mda['navigation_parameters']['satellite_actual_altitude'] = actual_alt\n\n        # Misc\n        self.platform_id = self.prologue[\"SatelliteStatus\"][\n            \"SatelliteDefinition\"][\"SatelliteId\"]\n        self.platform_name = \"Meteosat-\" + SATNUM[self.platform_id]\n        self.mda['platform_name'] = self.platform_name\n        service = self._filename_info['service']\n        if service == '':\n            self.mda['service'] = '0DEG'\n        else:\n            self.mda['service'] = service\n        self.channel_name = CHANNEL_NAMES[self.mda['spectral_channel_id']]", "response": "Read the header info and fill the metadata dictionary with the information from the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_area_extent(self, size, offsets, factors, platform_height):\n        nlines, ncols = size\n        h = platform_height\n\n        loff, coff = offsets\n        loff -= nlines\n        offsets = loff, coff\n        # count starts at 1\n        cols = 1 - 0.5\n        lines = 0.5 - 1\n        ll_x, ll_y = self.get_xy_from_linecol(-lines, cols, offsets, factors)\n\n        cols += ncols\n        lines += nlines\n        ur_x, ur_y = self.get_xy_from_linecol(-lines, cols, offsets, factors)\n\n        aex = (np.deg2rad(ll_x) * h, np.deg2rad(ll_y) * h,\n               np.deg2rad(ur_x) * h, np.deg2rad(ur_y) * h)\n\n        if not self.mda['offset_corrected']:\n            # Geo-referencing offset present. Adjust area extent to match the shifted data. Note that we have to adjust\n            # the corners in the *opposite* direction, i.e. S-E. Think of it as if the coastlines were fixed and you\n            # dragged the image to S-E until coastlines and data area aligned correctly.\n            #\n            # Although the image is flipped upside-down and left-right, the projection coordinates retain their\n            # properties, i.e. positive x/y is East/North, respectively.\n            xadj = 1500\n            yadj = -1500\n            aex = (aex[0] + xadj, aex[1] + yadj,\n                   aex[2] + xadj, aex[3] + yadj)\n\n        return aex", "response": "Get the area extent of the file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_area_def(self, dsid):\n        if dsid.name != 'HRV':\n            return super(HRITMSGFileHandler, self).get_area_def(dsid)\n\n        cfac = np.int32(self.mda['cfac'])\n        lfac = np.int32(self.mda['lfac'])\n        loff = np.float32(self.mda['loff'])\n\n        a = self.mda['projection_parameters']['a']\n        b = self.mda['projection_parameters']['b']\n        h = self.mda['projection_parameters']['h']\n        lon_0 = self.mda['projection_parameters']['SSP_longitude']\n\n        nlines = int(self.mda['number_of_lines'])\n        ncols = int(self.mda['number_of_columns'])\n\n        segment_number = self.mda['segment_sequence_number']\n\n        current_first_line = (segment_number -\n                              self.mda['planned_start_segment_number']) * nlines\n        bounds = self.epilogue['ImageProductionStats']['ActualL15CoverageHRV']\n\n        upper_south_line = bounds[\n            'LowerNorthLineActual'] - current_first_line - 1\n        upper_south_line = min(max(upper_south_line, 0), nlines)\n\n        lower_coff = (5566 - bounds['LowerEastColumnActual'] + 1)\n        upper_coff = (5566 - bounds['UpperEastColumnActual'] + 1)\n\n        lower_area_extent = self.get_area_extent((upper_south_line, ncols),\n                                                 (loff, lower_coff),\n                                                 (lfac, cfac),\n                                                 h)\n\n        upper_area_extent = self.get_area_extent((nlines - upper_south_line,\n                                                  ncols),\n                                                 (loff - upper_south_line,\n                                                  upper_coff),\n                                                 (lfac, cfac),\n                                                 h)\n\n        proj_dict = {'a': float(a),\n                     'b': float(b),\n                     'lon_0': float(lon_0),\n                     'h': float(h),\n                     'proj': 'geos',\n                     'units': 'm'}\n\n        lower_area = geometry.AreaDefinition(\n            'some_area_name',\n            \"On-the-fly area\",\n            'geosmsg',\n            proj_dict,\n            ncols,\n            upper_south_line,\n            lower_area_extent)\n\n        upper_area = geometry.AreaDefinition(\n            'some_area_name',\n            \"On-the-fly area\",\n            'geosmsg',\n            proj_dict,\n            ncols,\n            nlines - upper_south_line,\n            upper_area_extent)\n\n        area = geometry.StackedAreaDefinition(lower_area, upper_area)\n\n        self.area = area.squeeze()\n        return area", "response": "Get the area definition of the band."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the cf grid mapping for the omerc projection.", "response": "def omerc2cf(area):\n    \"\"\"Return the cf grid mapping for the omerc projection.\"\"\"\n    proj_dict = area.proj_dict\n\n    args = dict(azimuth_of_central_line=proj_dict.get('alpha'),\n                latitude_of_projection_origin=proj_dict.get('lat_0'),\n                longitude_of_projection_origin=proj_dict.get('lonc'),\n                grid_mapping_name='oblique_mercator',\n                reference_ellipsoid_name=proj_dict.get('ellps', 'WGS84'),\n                false_easting=0.,\n                false_northing=0.\n                )\n    if \"no_rot\" in proj_dict:\n        args['no_rotation'] = 1\n    if \"gamma\" in proj_dict:\n        args['gamma'] = proj_dict['gamma']\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the cf grid mapping for the geos projection.", "response": "def geos2cf(area):\n    \"\"\"Return the cf grid mapping for the geos projection.\"\"\"\n    proj_dict = area.proj_dict\n    args = dict(perspective_point_height=proj_dict.get('h'),\n                latitude_of_projection_origin=proj_dict.get('lat_0'),\n                longitude_of_projection_origin=proj_dict.get('lon_0'),\n                grid_mapping_name='geostationary',\n                semi_major_axis=proj_dict.get('a'),\n                semi_minor_axis=proj_dict.get('b'),\n                sweep_axis=proj_dict.get('sweep'),\n                )\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the cf grid mapping for the laea projection.", "response": "def laea2cf(area):\n    \"\"\"Return the cf grid mapping for the laea projection.\"\"\"\n    proj_dict = area.proj_dict\n    args = dict(latitude_of_projection_origin=proj_dict.get('lat_0'),\n                longitude_of_projection_origin=proj_dict.get('lon_0'),\n                grid_mapping_name='lambert_azimuthal_equal_area',\n                )\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_grid_mapping(area):\n    try:\n        grid_mapping = mappings[area.proj_dict['proj']](area)\n        grid_mapping['name'] = area.proj_dict['proj']\n    except KeyError:\n        raise NotImplementedError\n\n    return grid_mapping", "response": "Create the grid mapping instance for area."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_extra_ds(dataset):\n    ds_collection = {}\n    for ds in dataset.attrs.get('ancillary_variables', []):\n        ds_collection.update(get_extra_ds(ds))\n    ds_collection[dataset.attrs['name']] = dataset\n\n    return ds_collection", "response": "Get the extra datasets associated to dataset."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting an area to longitudes and latitudes.", "response": "def area2lonlat(dataarray):\n    \"\"\"Convert an area to longitudes and latitudes.\"\"\"\n    area = dataarray.attrs['area']\n    lons, lats = area.get_lonlats_dask()\n    lons = xr.DataArray(lons, dims=['y', 'x'],\n                        attrs={'name': \"longitude\",\n                               'standard_name': \"longitude\",\n                               'units': 'degrees_east'},\n                        name='longitude')\n    lats = xr.DataArray(lats, dims=['y', 'x'],\n                        attrs={'name': \"latitude\",\n                               'standard_name': \"latitude\",\n                               'units': 'degrees_north'},\n                        name='latitude')\n    dataarray.attrs['coordinates'] = 'longitude latitude'\n\n    return [dataarray, lons, lats]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts the dataarray to something cf - compatible.", "response": "def da2cf(dataarray, epoch=EPOCH):\n        \"\"\"Convert the dataarray to something cf-compatible.\"\"\"\n        new_data = dataarray.copy()\n\n        # Remove the area\n        new_data.attrs.pop('area', None)\n\n        anc = [ds.attrs['name']\n               for ds in new_data.attrs.get('ancillary_variables', [])]\n        if anc:\n            new_data.attrs['ancillary_variables'] = ' '.join(anc)\n        # TODO: make this a grid mapping or lon/lats\n        # new_data.attrs['area'] = str(new_data.attrs.get('area'))\n        for key, val in new_data.attrs.copy().items():\n            if val is None:\n                new_data.attrs.pop(key)\n        new_data.attrs.pop('_last_resampler', None)\n\n        if 'time' in new_data.coords:\n            new_data['time'].encoding['units'] = epoch\n            new_data['time'].attrs['standard_name'] = 'time'\n            new_data['time'].attrs.pop('bounds', None)\n            if 'time' not in new_data.dims:\n                new_data = new_data.expand_dims('time')\n\n        if 'x' in new_data.coords:\n            new_data['x'].attrs['standard_name'] = 'projection_x_coordinate'\n            new_data['x'].attrs['units'] = 'm'\n\n        if 'y' in new_data.coords:\n            new_data['y'].attrs['standard_name'] = 'projection_y_coordinate'\n            new_data['y'].attrs['units'] = 'm'\n\n        new_data.attrs.setdefault('long_name', new_data.attrs.pop('name'))\n        if 'prerequisites' in new_data.attrs:\n            new_data.attrs['prerequisites'] = [np.string_(str(prereq)) for prereq in new_data.attrs['prerequisites']]\n        return new_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves the dataset to a given filename.", "response": "def save_dataset(self, dataset, filename=None, fill_value=None, **kwargs):\n        \"\"\"Save the *dataset* to a given *filename*.\"\"\"\n        return self.save_datasets([dataset], filename, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_datasets(self, datasets, filename=None, **kwargs):\n        logger.info('Saving datasets to NetCDF4/CF.')\n        # XXX: Should we combine the info of all datasets?\n        filename = filename or self.get_filename(**datasets[0].attrs)\n        datas, start_times, end_times = self._collect_datasets(datasets, kwargs)\n\n        dataset = xr.Dataset(datas)\n        try:\n            dataset['time_bnds'] = make_time_bounds(dataset,\n                                                    start_times,\n                                                    end_times)\n            dataset['time'].attrs['bounds'] = \"time_bnds\"\n        except KeyError:\n            logger.warning('No time dimension in datasets, skipping time bounds creation.')\n\n        header_attrs = kwargs.pop('header_attrs', None)\n\n        if header_attrs is not None:\n            dataset.attrs.update({k: v for k, v in header_attrs.items() if v})\n\n        dataset.attrs['history'] = (\"Created by pytroll/satpy on \" +\n                                    str(datetime.utcnow()))\n        dataset.attrs['conventions'] = 'CF-1.7'\n        engine = kwargs.pop(\"engine\", 'h5netcdf')\n        for key in list(kwargs.keys()):\n            if key not in ['mode', 'format', 'group', 'encoding', 'unlimited_dims', 'compute']:\n                kwargs.pop(key, None)\n        return dataset.to_netcdf(filename, engine=engine, **kwargs)", "response": "Save all datasets to one or more files."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_platform(self):\n        try:\n            return PLATFORMS[self.projection_name]\n        except KeyError:\n            logger.error('Unable to determine platform: Unknown projection '\n                         'name \"{}\"'.format(self.projection_name))\n            return UNKNOWN_PLATFORM", "response": "Get the platform name of the current HRIT entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks that sensor and platform are consistent with the current configuration.", "response": "def _check_sensor_platform_consistency(self, sensor):\n        \"\"\"Make sure sensor and platform are consistent\n\n        Args:\n            sensor (str) : Sensor name from YAML dataset definition\n\n        Raises:\n            ValueError if they don't match\n        \"\"\"\n        ref_sensor = SENSORS.get(self.platform, None)\n        if ref_sensor and not sensor == ref_sensor:\n            logger.error('Sensor-Platform mismatch: {} is not a payload '\n                         'of {}. Did you choose the correct reader?'\n                         .format(sensor, self.platform))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_line_offset(self):\n        # Get line offset from the file\n        nlines = int(self.mda['number_of_lines'])\n        loff = np.float32(self.mda['loff'])\n\n        # Adapt it to the current segment\n        if self.is_segmented:\n            # loff in the file specifies the offset of the full disk image\n            # centre (1375/2750 for VIS/IR)\n            segment_number = self.mda['segment_sequence_number'] - 1\n            loff -= (self.mda['total_no_image_segm'] - segment_number - 1) * nlines\n        elif self.area_id in (NORTH_HEMIS, SOUTH_HEMIS):\n            # loff in the file specifies the start line of the half disk image\n            # in the full disk image\n            loff = nlines - loff\n        elif self.area_id == UNKNOWN_AREA:\n            logger.error('Cannot compute line offset for unknown area')\n\n        return loff", "response": "Get the line offset for the current segment and adapt it to the current segment."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_area_def(self):\n        cfac = np.int32(self.mda['cfac'])\n        lfac = np.int32(self.mda['lfac'])\n        coff = np.float32(self.mda['coff'])\n        loff = self._get_line_offset()\n\n        a = self.mda['projection_parameters']['a']\n        b = self.mda['projection_parameters']['b']\n        h = self.mda['projection_parameters']['h']\n        lon_0 = self.mda['projection_parameters']['SSP_longitude']\n\n        nlines = int(self.mda['number_of_lines'])\n        ncols = int(self.mda['number_of_columns'])\n\n        area_extent = self.get_area_extent((nlines, ncols),\n                                           (loff, coff),\n                                           (lfac, cfac),\n                                           h)\n\n        proj_dict = {'a': float(a),\n                     'b': float(b),\n                     'lon_0': float(lon_0),\n                     'h': float(h),\n                     'proj': 'geos',\n                     'units': 'm'}\n\n        area = geometry.AreaDefinition(\n            AREA_NAMES[self.area_id]['short'],\n            AREA_NAMES[self.area_id]['long'],\n            'geosmsg',\n            proj_dict,\n            ncols,\n            nlines,\n            area_extent)\n\n        return area", "response": "Get the area definition of the band."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the dataset designated by key.", "response": "def get_dataset(self, key, info):\n        \"\"\"Get the dataset designated by *key*.\"\"\"\n        res = super(HRITJMAFileHandler, self).get_dataset(key, info)\n\n        # Filenames of segmented data is identical for MTSAT-1R, MTSAT-2\n        # and Himawari-8/9. Make sure we have the correct reader for the data\n        # at hand.\n        self._check_sensor_platform_consistency(info['sensor'])\n\n        # Calibrate and mask space pixels\n        res = self._mask_space(self.calibrate(res, key.calibration))\n\n        # Update attributes\n        res.attrs.update(info)\n        res.attrs['platform_name'] = self.platform\n        res.attrs['satellite_longitude'] = float(self.mda['projection_parameters']['SSP_longitude'])\n        res.attrs['satellite_latitude'] = 0.\n        res.attrs['satellite_altitude'] = float(self.mda['projection_parameters']['h'])\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef combine_info(self, all_infos):\n        combined_info = combine_metadata(*all_infos)\n\n        new_dict = self._combine(all_infos, min, 'start_time', 'start_orbit')\n        new_dict.update(self._combine(all_infos, max, 'end_time', 'end_orbit'))\n        new_dict.update(self._combine(all_infos, np.mean,\n                                      'satellite_longitude',\n                                      'satellite_latitude',\n                                      'satellite_altitude'))\n\n        try:\n            area = SwathDefinition(lons=np.ma.vstack([info['area'].lons for info in all_infos]),\n                                   lats=np.ma.vstack([info['area'].lats for info in all_infos]))\n            area.name = '_'.join([info['area'].name for info in all_infos])\n            combined_info['area'] = area\n        except KeyError:\n            pass\n\n        new_dict.update(combined_info)\n        return new_dict", "response": "Combine metadata for multiple datasets."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_available_channels(header):\n\n    chlist_str = header['15_SECONDARY_PRODUCT_HEADER'][\n        'SelectedBandIDs']['Value']\n    retv = {}\n\n    for idx, char in zip(range(12), chlist_str):\n        retv[CHANNEL_NAMES[idx + 1]] = (char == 'X')\n\n    return retv", "response": "Get the available channels from the header information"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the dtype of the data for the current ISO - 8601 record.", "response": "def _get_data_dtype(self):\n        \"\"\"Get the dtype of the file based on the actual available channels\"\"\"\n\n        pkhrec = [\n            ('GP_PK_HEADER', GSDTRecords.gp_pk_header),\n            ('GP_PK_SH1', GSDTRecords.gp_pk_sh1)\n        ]\n        pk_head_dtype = np.dtype(pkhrec)\n\n        def get_lrec(cols):\n            lrec = [\n                (\"gp_pk\", pk_head_dtype),\n                (\"version\", np.uint8),\n                (\"satid\", np.uint16),\n                (\"time\", (np.uint16, 5)),\n                (\"lineno\", np.uint32),\n                (\"chan_id\", np.uint8),\n                (\"acq_time\", (np.uint16, 3)),\n                (\"line_validity\", np.uint8),\n                (\"line_rquality\", np.uint8),\n                (\"line_gquality\", np.uint8),\n                (\"line_data\", (np.uint8, cols))\n            ]\n\n            return lrec\n\n        # each pixel is 10-bits -> one line of data has 25% more bytes\n        # than the number of columns suggest (10/8 = 1.25)\n        visir_rec = get_lrec(int(self.mda['number_of_columns'] * 1.25))\n        number_of_visir_channels = len(\n            [s for s in self.mda['channel_list'] if not s == 'HRV'])\n        drec = [('visir', (visir_rec, number_of_visir_channels))]\n\n        if self.mda['available_channels']['HRV']:\n            hrv_rec = get_lrec(int(self.mda['hrv_number_of_columns'] * 1.25))\n            drec.append(('hrv', (hrv_rec, 3)))\n\n        return np.dtype(drec)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_memmap(self):\n\n        with open(self.filename) as fp:\n\n            data_dtype = self._get_data_dtype()\n            hdr_size = native_header.itemsize\n\n            return np.memmap(fp, dtype=data_dtype,\n                             shape=(self.mda['number_of_lines'],),\n                             offset=hdr_size, mode=\"r\")", "response": "Get the memory map for the SEVIRI data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the header info from the ISO.", "response": "def _read_header(self):\n        \"\"\"Read the header info\"\"\"\n\n        data = np.fromfile(self.filename,\n                           dtype=native_header, count=1)\n\n        self.header.update(recarray2dict(data))\n\n        data15hd = self.header['15_DATA_HEADER']\n        sec15hd = self.header['15_SECONDARY_PRODUCT_HEADER']\n\n        # Set the list of available channels:\n        self.mda['available_channels'] = get_available_channels(self.header)\n        self.mda['channel_list'] = [i for i in CHANNEL_NAMES.values()\n                                    if self.mda['available_channels'][i]]\n\n        self.platform_id = data15hd[\n            'SatelliteStatus']['SatelliteDefinition']['SatelliteId']\n        self.mda['platform_name'] = \"Meteosat-\" + SATNUM[self.platform_id]\n\n        equator_radius = data15hd['GeometricProcessing'][\n            'EarthModel']['EquatorialRadius'] * 1000.\n        north_polar_radius = data15hd[\n            'GeometricProcessing']['EarthModel']['NorthPolarRadius'] * 1000.\n        south_polar_radius = data15hd[\n            'GeometricProcessing']['EarthModel']['SouthPolarRadius'] * 1000.\n        polar_radius = (north_polar_radius + south_polar_radius) * 0.5\n        ssp_lon = data15hd['ImageDescription'][\n            'ProjectionDescription']['LongitudeOfSSP']\n\n        self.mda['projection_parameters'] = {'a': equator_radius,\n                                             'b': polar_radius,\n                                             'h': 35785831.00,\n                                             'ssp_longitude': ssp_lon}\n\n        north = int(sec15hd['NorthLineSelectedRectangle']['Value'])\n        east = int(sec15hd['EastColumnSelectedRectangle']['Value'])\n        south = int(sec15hd['SouthLineSelectedRectangle']['Value'])\n        west = int(sec15hd['WestColumnSelectedRectangle']['Value'])\n\n        ncolumns = west - east + 1\n        nrows = north - south + 1\n\n        # check if the file has less rows or columns than\n        # the maximum, if so it is an area of interest file\n        if (nrows < VISIR_NUM_LINES) or (ncolumns < VISIR_NUM_COLUMNS):\n            self.mda['is_full_disk'] = False\n\n        # If the number of columns in the file is not divisible by 4,\n        # UMARF will add extra columns to the file\n        modulo = ncolumns % 4\n        padding = 0\n        if modulo > 0:\n            padding = 4 - modulo\n        cols_visir = ncolumns + padding\n\n        # Check the VISIR calculated column dimension against\n        # the header information\n        cols_visir_hdr = int(sec15hd['NumberColumnsVISIR']['Value'])\n        if cols_visir_hdr != cols_visir:\n            logger.warning(\n                \"Number of VISIR columns from the header is incorrect!\")\n            logger.warning(\"Header: %d\", cols_visir_hdr)\n            logger.warning(\"Calculated: = %d\", cols_visir)\n\n        # HRV Channel - check if the area is reduced in east west\n        # direction as this affects the number of columns in the file\n        cols_hrv_hdr = int(sec15hd['NumberColumnsHRV']['Value'])\n        if ncolumns < VISIR_NUM_COLUMNS:\n            cols_hrv = cols_hrv_hdr\n        else:\n            cols_hrv = int(cols_hrv_hdr / 2)\n\n        # self.mda represents the 16bit dimensions not 10bit\n        self.mda['number_of_lines'] = int(sec15hd['NumberLinesVISIR']['Value'])\n        self.mda['number_of_columns'] = cols_visir\n        self.mda['hrv_number_of_lines'] = int(sec15hd[\"NumberLinesHRV\"]['Value'])\n        self.mda['hrv_number_of_columns'] = cols_hrv"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef radiance_to_bt(arr, wc_, a__, b__):\n    return a__ + b__ * (C2 * wc_ / (da.log(1 + (C1 * (wc_ ** 3) / arr))))", "response": "Convert radiance to BT."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_raw(filename):\n\n    form = XMLFormat(os.path.join(CONFIG_PATH, \"eps_avhrrl1b_6.5.xml\"))\n\n    grh_dtype = np.dtype([(\"record_class\", \"|i1\"),\n                          (\"INSTRUMENT_GROUP\", \"|i1\"),\n                          (\"RECORD_SUBCLASS\", \"|i1\"),\n                          (\"RECORD_SUBCLASS_VERSION\", \"|i1\"),\n                          (\"RECORD_SIZE\", \">u4\"),\n                          (\"RECORD_START_TIME\", \"S6\"),\n                          (\"RECORD_STOP_TIME\", \"S6\")])\n\n    dtypes = []\n    cnt = 0\n    with open(filename, \"rb\") as fdes:\n        while True:\n            grh = np.fromfile(fdes, grh_dtype, 1)\n            if not grh:\n                break\n            rec_class = record_class[int(grh[\"record_class\"])]\n            sub_class = grh[\"RECORD_SUBCLASS\"][0]\n\n            expected_size = int(grh[\"RECORD_SIZE\"])\n            bare_size = expected_size - grh_dtype.itemsize\n            try:\n                the_type = form.dtype((rec_class, sub_class))\n                the_descr = grh_dtype.descr + the_type.descr\n            except KeyError:\n                the_type = np.dtype([('unknown', 'V%d' % bare_size)])\n            the_descr = grh_dtype.descr + the_type.descr\n            the_type = np.dtype(the_descr)\n            if the_type.itemsize < expected_size:\n                padding = [('unknown%d' % cnt, 'V%d' % (expected_size - the_type.itemsize))]\n                cnt += 1\n                the_descr += padding\n            dtypes.append(np.dtype(the_descr))\n            fdes.seek(expected_size - grh_dtype.itemsize, 1)\n\n        file_dtype = np.dtype([(str(num), the_dtype) for num, the_dtype in enumerate(dtypes)])\n        records = np.memmap(fdes, mode='r', dtype=file_dtype, shape=1)[0]\n\n    return records, form", "response": "Read *filename* without scaling it afterwards."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef keys(self):\n        keys = []\n        for val in self.form.scales.values():\n            keys += val.dtype.fields.keys()\n        return keys", "response": "List of reader s keys."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the interpolated lons and lats.", "response": "def get_full_lonlats(self):\n        \"\"\"Get the interpolated lons/lats.\n        \"\"\"\n        if self.lons is not None and self.lats is not None:\n            return self.lons, self.lats\n\n        self.lons, self.lats = self._get_full_lonlats()\n        self.lons = da.from_delayed(self.lons, dtype=self[\"EARTH_LOCATIONS\"].dtype,\n                                    shape=(self.scanlines, self.pixels))\n        self.lats = da.from_delayed(self.lats, dtype=self[\"EARTH_LOCATIONS\"].dtype,\n                                    shape=(self.scanlines, self.pixels))\n        return self.lons, self.lats"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_full_angles(self):\n        if (self.sun_azi is not None and self.sun_zen is not None and\n                self.sat_azi is not None and self.sat_zen is not None):\n            return self.sun_azi, self.sun_zen, self.sat_azi, self.sat_zen\n\n        self.sun_azi, self.sun_zen, self.sat_azi, self.sat_zen = self._get_full_angles()\n        self.sun_azi = da.from_delayed(self.sun_azi, dtype=self[\"ANGULAR_RELATIONS\"].dtype,\n                                       shape=(self.scanlines, self.pixels))\n        self.sun_zen = da.from_delayed(self.sun_zen, dtype=self[\"ANGULAR_RELATIONS\"].dtype,\n                                       shape=(self.scanlines, self.pixels))\n        self.sat_azi = da.from_delayed(self.sat_azi, dtype=self[\"ANGULAR_RELATIONS\"].dtype,\n                                       shape=(self.scanlines, self.pixels))\n        self.sat_zen = da.from_delayed(self.sat_zen, dtype=self[\"ANGULAR_RELATIONS\"].dtype,\n                                       shape=(self.scanlines, self.pixels))\n        return self.sun_azi, self.sun_zen, self.sat_azi, self.sat_zen", "response": "Get the interpolated lons and longs."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets calibrated channel data.", "response": "def get_dataset(self, key, info):\n        \"\"\"Get calibrated channel data.\"\"\"\n        if self.mdrs is None:\n            self._read_all(self.filename)\n\n        if key.name in ['longitude', 'latitude']:\n            lons, lats = self.get_full_lonlats()\n            if key.name == 'longitude':\n                dataset = create_xarray(lons)\n            else:\n                dataset = create_xarray(lats)\n\n        elif key.name in ['solar_zenith_angle', 'solar_azimuth_angle',\n                          'satellite_zenith_angle', 'satellite_azimuth_angle']:\n            sun_azi, sun_zen, sat_azi, sat_zen = self.get_full_angles()\n            if key.name == 'solar_zenith_angle':\n                dataset = create_xarray(sun_zen)\n            elif key.name == 'solar_azimuth_angle':\n                dataset = create_xarray(sun_azi)\n            if key.name == 'satellite_zenith_angle':\n                dataset = create_xarray(sat_zen)\n            elif key.name == 'satellite_azimuth_angle':\n                dataset = create_xarray(sat_azi)\n        else:\n            mask = None\n            if key.calibration == 'counts':\n                raise ValueError('calibration=counts is not supported! ' +\n                                 'This reader cannot return counts')\n            elif key.calibration not in ['reflectance', 'brightness_temperature', 'radiance']:\n                raise ValueError('calibration type ' + str(key.calibration) +\n                                 ' is not supported!')\n\n            if key.name in ['3A', '3a'] and self.three_a_mask is None:\n                self.three_a_mask = ((self[\"FRAME_INDICATOR\"] & 2 ** 16) != 2 ** 16)\n\n            if key.name in ['3B', '3b'] and self.three_b_mask is None:\n                self.three_b_mask = ((self[\"FRAME_INDICATOR\"] & 2 ** 16) != 0)\n\n            if key.name not in [\"1\", \"2\", \"3a\", \"3A\", \"3b\", \"3B\", \"4\", \"5\"]:\n                LOG.info(\"Can't load channel in eps_l1b: \" + str(key.name))\n                return\n\n            if key.name == \"1\":\n                if key.calibration == 'reflectance':\n                    array = radiance_to_refl(self[\"SCENE_RADIANCES\"][:, 0, :],\n                                             self[\"CH1_SOLAR_FILTERED_IRRADIANCE\"])\n                else:\n                    array = self[\"SCENE_RADIANCES\"][:, 0, :]\n\n            if key.name == \"2\":\n                if key.calibration == 'reflectance':\n                    array = radiance_to_refl(self[\"SCENE_RADIANCES\"][:, 1, :],\n                                             self[\"CH2_SOLAR_FILTERED_IRRADIANCE\"])\n                else:\n                    array = self[\"SCENE_RADIANCES\"][:, 1, :]\n\n            if key.name.lower() == \"3a\":\n                if key.calibration == 'reflectance':\n                    array = radiance_to_refl(self[\"SCENE_RADIANCES\"][:, 2, :],\n                                             self[\"CH3A_SOLAR_FILTERED_IRRADIANCE\"])\n                else:\n                    array = self[\"SCENE_RADIANCES\"][:, 2, :]\n\n                mask = np.empty(array.shape, dtype=bool)\n                mask[:, :] = self.three_a_mask[:, np.newaxis]\n\n            if key.name.lower() == \"3b\":\n                if key.calibration == 'brightness_temperature':\n                    array = radiance_to_bt(self[\"SCENE_RADIANCES\"][:, 2, :],\n                                           self[\"CH3B_CENTRAL_WAVENUMBER\"],\n                                           self[\"CH3B_CONSTANT1\"],\n                                           self[\"CH3B_CONSTANT2_SLOPE\"])\n                else:\n                    array = self[\"SCENE_RADIANCES\"][:, 2, :]\n                mask = np.empty(array.shape, dtype=bool)\n                mask[:, :] = self.three_b_mask[:, np.newaxis]\n\n            if key.name == \"4\":\n                if key.calibration == 'brightness_temperature':\n                    array = radiance_to_bt(self[\"SCENE_RADIANCES\"][:, 3, :],\n                                           self[\"CH4_CENTRAL_WAVENUMBER\"],\n                                           self[\"CH4_CONSTANT1\"],\n                                           self[\"CH4_CONSTANT2_SLOPE\"])\n                else:\n                    array = self[\"SCENE_RADIANCES\"][:, 3, :]\n\n            if key.name == \"5\":\n                if key.calibration == 'brightness_temperature':\n                    array = radiance_to_bt(self[\"SCENE_RADIANCES\"][:, 4, :],\n                                           self[\"CH5_CENTRAL_WAVENUMBER\"],\n                                           self[\"CH5_CONSTANT1\"],\n                                           self[\"CH5_CONSTANT2_SLOPE\"])\n                else:\n                    array = self[\"SCENE_RADIANCES\"][:, 4, :]\n\n            dataset = create_xarray(array)\n            if mask is not None:\n                dataset = dataset.where(~mask)\n\n        dataset.attrs['platform_name'] = self.platform_name\n        dataset.attrs['sensor'] = self.sensor_name\n        dataset.attrs.update(info)\n        dataset.attrs.update(key.to_dict())\n        return dataset"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload a single dataset.", "response": "def get_dataset(self, dataset_id, ds_info, out=None):\n        \"\"\"Load a dataset.\"\"\"\n        var_path = ds_info.get('file_key', '{}'.format(dataset_id.name))\n        dtype = ds_info.get('dtype', np.float32)\n        if var_path + '/shape' not in self:\n            # loading a scalar value\n            shape = 1\n        else:\n            shape = self[var_path + '/shape']\n            if shape[0] == 1:\n                # Remove the time dimenstion from dataset\n                shape = shape[1], shape[2]\n\n        file_units = ds_info.get('file_units')\n        if file_units is None:\n            try:\n                file_units = self[var_path + '/attr/units']\n                # they were almost completely CF compliant...\n                if file_units == \"none\":\n                    file_units = \"1\"\n            except KeyError:\n                # no file units specified\n                file_units = None\n\n        if out is None:\n            out = np.ma.empty(shape, dtype=dtype)\n            out.mask = np.zeros(shape, dtype=np.bool)\n\n        out.data[:] = np.require(self[var_path][0][::-1], dtype=dtype)\n        valid_min = self[var_path + '/attr/valid_min']\n        valid_max = self[var_path + '/attr/valid_max']\n        try:\n            scale_factor = self[var_path + '/attr/scale_factor']\n            scale_offset = self[var_path + '/attr/add_offset']\n        except KeyError:\n            scale_factor = scale_offset = None\n\n        if valid_min is not None and valid_max is not None:\n            out.mask[:] |= (out.data < valid_min) | (out.data > valid_max)\n\n        factors = (scale_factor, scale_offset)\n        if factors[0] != 1 or factors[1] != 0:\n            out.data[:] *= factors[0]\n            out.data[:] += factors[1]\n\n        ds_info.update({\n            \"units\": ds_info.get(\"units\", file_units),\n            \"platform_name\": PLATFORM_NAME.get(self['/attr/platform'], self['/attr/platform']),\n            \"sensor\": SENSOR_NAME.get(self['/attr/sensor'], self['/attr/sensor']),\n        })\n        ds_info.update(dataset_id.to_dict())\n        cls = ds_info.pop(\"container\", Dataset)\n        return cls(out, **ds_info)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _vis_calibrate(data,\n                   chn,\n                   calib_type,\n                   pre_launch_coeffs=False,\n                   calib_coeffs=None,\n                   mask=False):\n    \"\"\"Visible channel calibration only.\n\n    *calib_type* in count, reflectance, radiance\n    \"\"\"\n    # Calibration count to albedo, the calibration is performed separately for\n    # two value ranges.\n\n    if calib_type not in ['counts', 'radiance', 'reflectance']:\n        raise ValueError('Calibration ' + calib_type + ' unknown!')\n\n    arr = data[\"hrpt\"][:, :, chn]\n    mask |= arr == 0\n\n    channel = arr.astype(np.float)\n    if calib_type == 'counts':\n        return channel\n\n    if calib_type == 'radiance':\n        logger.info(\"Radiances are not yet supported for \" +\n                    \"the VIS/NIR channels!\")\n\n    if pre_launch_coeffs:\n        coeff_idx = 2\n    else:\n        # check that coeffs are valid\n        if np.all(data[\"calvis\"][:, chn, 0, 4] == 0):\n            logger.info(\n                \"No valid operational coefficients, fall back to pre-launch\")\n            coeff_idx = 2\n        else:\n            coeff_idx = 0\n\n    intersection = data[\"calvis\"][:, chn, coeff_idx, 4]\n\n    if calib_coeffs is not None:\n        logger.info(\"Updating from external calibration coefficients.\")\n        # intersection = np.expand_dims\n        slope1 = np.expand_dims(calib_coeffs[0], 1)\n        intercept1 = np.expand_dims(calib_coeffs[1], 1)\n        slope2 = np.expand_dims(calib_coeffs[2], 1)\n        intercept2 = np.expand_dims(calib_coeffs[3], 1)\n    else:\n        slope1 = np.expand_dims(data[\"calvis\"][:, chn, coeff_idx, 0] * 1e-10,\n                                1)\n        intercept1 = np.expand_dims(data[\"calvis\"][:, chn, coeff_idx, 1] *\n                                    1e-7, 1)\n        slope2 = np.expand_dims(data[\"calvis\"][:, chn, coeff_idx, 2] * 1e-10,\n                                1)\n        intercept2 = np.expand_dims(data[\"calvis\"][:, chn, coeff_idx, 3] *\n                                    1e-7, 1)\n\n        if chn == 2:\n            slope2[slope2 < 0] += 0.4294967296\n\n    mask1 = channel <= np.expand_dims(intersection, 1)\n    mask2 = channel > np.expand_dims(intersection, 1)\n\n    channel[mask1] = (channel * slope1 + intercept1)[mask1]\n\n    channel[mask2] = (channel * slope2 + intercept2)[mask2]\n\n    channel = channel.clip(min=0)\n\n    return np.where(mask, np.nan, channel)", "response": "Calibrate the VIS channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ir_calibrate(header, data, irchn, calib_type, mask=False):\n\n    count = data[\"hrpt\"][:, :, irchn + 2].astype(np.float)\n\n    if calib_type == 0:\n        return count\n\n    # Mask unnaturally low values\n    mask |= count == 0.0\n\n    k1_ = np.expand_dims(data['calir'][:, irchn, 0, 0] / 1.0e9, 1)\n    k2_ = np.expand_dims(data['calir'][:, irchn, 0, 1] / 1.0e6, 1)\n    k3_ = np.expand_dims(data['calir'][:, irchn, 0, 2] / 1.0e6, 1)\n\n    # Count to radiance conversion:\n    rad = k1_ * count * count + k2_ * count + k3_\n\n    all_zero = np.logical_and(\n        np.logical_and(\n            np.equal(k1_, 0), np.equal(k2_, 0)), np.equal(k3_, 0))\n    idx = np.indices((all_zero.shape[0], ))\n    suspect_line_nums = np.repeat(idx[0], all_zero[:, 0])\n    if suspect_line_nums.any():\n        logger.info(\"Suspicious scan lines: %s\", str(suspect_line_nums))\n\n    if calib_type == 2:\n        mask |= rad <= 0.0\n        return np.where(mask, np.nan, rad)\n\n    # Central wavenumber:\n    cwnum = header['radtempcnv'][0, irchn, 0]\n    if irchn == 0:\n        cwnum = cwnum / 1.0e2\n    else:\n        cwnum = cwnum / 1.0e3\n\n    bandcor_2 = header['radtempcnv'][0, irchn, 1] / 1e5\n    bandcor_3 = header['radtempcnv'][0, irchn, 2] / 1e6\n\n    ir_const_1 = 1.1910659e-5\n    ir_const_2 = 1.438833\n\n    t_planck = (ir_const_2 * cwnum) / \\\n        np.log(1 + ir_const_1 * cwnum * cwnum * cwnum / rad)\n\n    # Band corrections applied to t_planck to get correct\n    # brightness temperature for channel:\n    if bandcor_2 < 0:  # Post AAPP-v4\n        tb_ = bandcor_2 + bandcor_3 * t_planck\n    else:  # AAPP 1 to 4\n        tb_ = (t_planck - bandcor_2) / bandcor_3\n\n    # Mask unnaturally low values\n    # mask |= tb_ < 0.1\n\n    return np.where(mask, np.nan, tb_)", "response": "Internal function to calibrate the IR calibration of the given calendar type."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a dataset from the file.", "response": "def get_dataset(self, key, info):\n        \"\"\"Get a dataset from the file.\"\"\"\n\n        if key.name in CHANNEL_NAMES:\n            dataset = self.calibrate(key)\n        elif key.name in ['longitude', 'latitude']:\n            if self.lons is None or self.lats is None:\n                self.navigate()\n            if key.name == 'longitude':\n                dataset = create_xarray(self.lons)\n            else:\n                dataset = create_xarray(self.lats)\n            dataset.attrs = info\n        else:  # Get sun-sat angles\n            if key.name in ANGLES:\n                if isinstance(getattr(self, ANGLES[key.name]), np.ndarray):\n                    dataset = create_xarray(getattr(self, ANGLES[key.name]))\n                else:\n                    dataset = self.get_angles(key.name)\n            else:\n                raise ValueError(\"Not a supported sun-sensor viewing angle: %s\", key.name)\n\n        dataset.attrs.update({'platform_name': self.platform_name,\n                              'sensor': self.sensor})\n        dataset.attrs.update(key.to_dict())\n\n        if not self._shape:\n            self._shape = dataset.shape\n\n        return dataset"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_angles(self, angle_id):\n\n        tic = datetime.now()\n\n        sunz40km = self._data[\"ang\"][:, :, 0] * 1e-2\n        satz40km = self._data[\"ang\"][:, :, 1] * 1e-2\n        azidiff40km = self._data[\"ang\"][:, :, 2] * 1e-2\n\n        try:\n            from geotiepoints.interpolator import Interpolator\n        except ImportError:\n            logger.warning(\"Could not interpolate sun-sat angles, \"\n                           \"python-geotiepoints missing.\")\n            self.sunz, self.satz, self.azidiff = sunz40km, satz40km, azidiff40km\n        else:\n            cols40km = np.arange(24, 2048, 40)\n            cols1km = np.arange(2048)\n            lines = sunz40km.shape[0]\n            rows40km = np.arange(lines)\n            rows1km = np.arange(lines)\n\n            along_track_order = 1\n            cross_track_order = 3\n\n            satint = Interpolator(\n                [sunz40km, satz40km, azidiff40km], (rows40km, cols40km),\n                (rows1km, cols1km), along_track_order, cross_track_order)\n            self.sunz, self.satz, self.azidiff = satint.interpolate()\n\n            logger.debug(\"Interpolate sun-sat angles: time %s\",\n                         str(datetime.now() - tic))\n\n        return create_xarray(getattr(self, ANGLES[angle_id]))", "response": "Get sun - satellite viewing angles"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the longitudes and latitudes of the scene.", "response": "def navigate(self):\n        \"\"\"Return the longitudes and latitudes of the scene.\n        \"\"\"\n        tic = datetime.now()\n        lons40km = self._data[\"pos\"][:, :, 1] * 1e-4\n        lats40km = self._data[\"pos\"][:, :, 0] * 1e-4\n\n        try:\n            from geotiepoints import SatelliteInterpolator\n        except ImportError:\n            logger.warning(\"Could not interpolate lon/lats, \"\n                           \"python-geotiepoints missing.\")\n            self.lons, self.lats = lons40km, lats40km\n        else:\n            cols40km = np.arange(24, 2048, 40)\n            cols1km = np.arange(2048)\n            lines = lons40km.shape[0]\n            rows40km = np.arange(lines)\n            rows1km = np.arange(lines)\n\n            along_track_order = 1\n            cross_track_order = 3\n\n            satint = SatelliteInterpolator(\n                (lons40km, lats40km), (rows40km, cols40km), (rows1km, cols1km),\n                along_track_order, cross_track_order)\n            self.lons, self.lats = satint.interpolate()\n            logger.debug(\"Navigation time %s\", str(datetime.now() - tic))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calibrate(self,\n                  dataset_id,\n                  pre_launch_coeffs=False,\n                  calib_coeffs=None):\n        \"\"\"Calibrate the data\n        \"\"\"\n        tic = datetime.now()\n\n        if calib_coeffs is None:\n            calib_coeffs = {}\n\n        units = {'reflectance': '%',\n                 'brightness_temperature': 'K',\n                 'counts': '',\n                 'radiance': 'W*m-2*sr-1*cm ?'}\n\n        if dataset_id.name in (\"3a\", \"3b\") and self._is3b is None:\n            # Is it 3a or 3b:\n            is3b = np.expand_dims(\n                np.bitwise_and(\n                    np.right_shift(self._data['scnlinbit'], 0), 1) == 1, 1)\n            self._is3b = np.repeat(is3b,\n                                   self._data['hrpt'][0].shape[0], axis=1)\n\n        try:\n            vis_idx = ['1', '2', '3a'].index(dataset_id.name)\n            ir_idx = None\n        except ValueError:\n            vis_idx = None\n            ir_idx = ['3b', '4', '5'].index(dataset_id.name)\n\n        if vis_idx is not None:\n            coeffs = calib_coeffs.get('ch' + dataset_id.name)\n            ds = create_xarray(\n                _vis_calibrate(self._data,\n                               vis_idx,\n                               dataset_id.calibration,\n                               pre_launch_coeffs,\n                               coeffs,\n                               mask=(dataset_id.name == '3a' and self._is3b)))\n        else:\n            ds = create_xarray(\n                _ir_calibrate(self._header,\n                              self._data,\n                              ir_idx,\n                              dataset_id.calibration,\n                              mask=(dataset_id.name == '3b' and\n                                    np.logical_not(self._is3b))))\n\n        if dataset_id.name == '3a' and np.all(np.isnan(ds)):\n            raise ValueError(\"Empty dataset for channel 3A\")\n        if dataset_id.name == '3b' and np.all(np.isnan(ds)):\n            raise ValueError(\"Empty dataset for channel 3B\")\n\n        ds.attrs['units'] = units[dataset_id.calibration]\n        ds.attrs.update(dataset_id._asdict())\n\n        logger.debug(\"Calibration time %s\", str(datetime.now() - tic))\n\n        return ds", "response": "Calibrates the data for the specified calendar."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncollecting all the attributes for the provided file object.", "response": "def _collect_attrs(self, name, obj):\n        \"\"\"Collect all the attributes for the provided file object.\n        \"\"\"\n        for key in obj.ncattrs():\n            value = getattr(obj, key)\n            fc_key = \"{}/attr/{}\".format(name, key)\n            try:\n                self.file_content[fc_key] = np2str(value)\n            except ValueError:\n                self.file_content[fc_key] = value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncollecting all file variables and attributes for the provided file object.", "response": "def collect_metadata(self, name, obj):\n        \"\"\"Collect all file variables and attributes for the provided file object.\n\n        This method also iterates through subgroups of the provided object.\n        \"\"\"\n        # Look through each subgroup\n        base_name = name + \"/\" if name else \"\"\n        for group_name, group_obj in obj.groups.items():\n            self.collect_metadata(base_name + group_name, group_obj)\n        for var_name, var_obj in obj.variables.items():\n            var_name = base_name + var_name\n            self.file_content[var_name] = var_obj\n            self.file_content[var_name + \"/dtype\"] = var_obj.dtype\n            self.file_content[var_name + \"/shape\"] = var_obj.shape\n            self._collect_attrs(var_name, var_obj)\n        self._collect_attrs(name, obj)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading data array and metadata from file on disk.", "response": "def get_dataset(self, dataset_id, ds_info, xslice=slice(None), yslice=slice(None)):\n        \"\"\"Load data array and metadata from file on disk.\"\"\"\n        var_path = ds_info.get('file_key', '{}'.format(dataset_id.name))\n        metadata = self.get_metadata(dataset_id, ds_info)\n        shape = metadata['shape']\n        file_shape = self[var_path + '/shape']\n        if isinstance(shape, tuple) and len(shape) == 2:\n            # 2D array\n            if xslice.start is not None:\n                shape = (shape[0], xslice.stop - xslice.start)\n            if yslice.start is not None:\n                shape = (yslice.stop - yslice.start, shape[1])\n        elif isinstance(shape, tuple) and len(shape) == 1 and yslice.start is not None:\n            shape = ((yslice.stop - yslice.start) / yslice.step,)\n        metadata['shape'] = shape\n\n        valid_min = self[var_path + '/attr/valid_min']\n        valid_max = self[var_path + '/attr/valid_max']\n        # no need to check fill value since we are using valid min/max\n        scale_factor = self.get(var_path + '/attr/scale_factor')\n        add_offset = self.get(var_path + '/attr/add_offset')\n\n        if isinstance(file_shape, tuple) and len(file_shape) == 3:\n            data = self[var_path][0, yslice, xslice]\n        elif isinstance(file_shape, tuple) and len(file_shape) == 2:\n            data = self[var_path][yslice, xslice]\n        elif isinstance(file_shape, tuple) and len(file_shape) == 1:\n            data = self[var_path][yslice]\n        else:\n            data = self[var_path]\n        data = data.where((data >= valid_min) & (data <= valid_max))\n        if scale_factor is not None:\n            data = data * scale_factor + add_offset\n\n        if ds_info.get('cloud_clear', False):\n            # clear-sky if bit 15-16 are 00\n            clear_sky_mask = (self['l2p_flags'][0] & 0b1100000000000000) != 0\n            data = data.where(~clear_sky_mask)\n\n        data.attrs.update(metadata)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_dataset(self, dsid, dsinfo):\n        data = self[dsinfo.get('file_key', dsid.name)]\n        data.attrs.update(dsinfo)\n\n        data.attrs[\"platform_name\"] = self['/attr/satellite_name']\n        data.attrs[\"sensor\"] = self['/attr/instrument_name']\n\n        return data", "response": "Get the data array for a specific dataset"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting an numpy. string_ to str.", "response": "def np2str(value):\n    \"\"\"Convert an `numpy.string_` to str.\n\n    Args:\n        value (ndarray): scalar or 1-element numpy array to convert\n\n    Raises:\n        ValueError: if value is array larger than 1-element or it is not of\n                    type `numpy.string_` or it is not a numpy array\n\n    \"\"\"\n    if hasattr(value, 'dtype') and \\\n            issubclass(value.dtype.type, (np.string_, np.object_)) and value.size == 1:\n        value = np.asscalar(value)\n        if not isinstance(value, str):\n            # python 3 - was scalar numpy array of bytes\n            # otherwise python 2 - scalar numpy array of 'str'\n            value = value.decode()\n        return value\n    else:\n        raise ValueError(\"Array is not a string type or is larger than 1\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the max earth viewing angles in x and y.", "response": "def get_geostationary_angle_extent(geos_area):\n    \"\"\"Get the max earth (vs space) viewing angles in x and y.\"\"\"\n    # TODO: take into account sweep_axis_angle parameter\n\n    # get some projection parameters\n    req = geos_area.proj_dict['a'] / 1000\n    rp = geos_area.proj_dict['b'] / 1000\n    h = geos_area.proj_dict['h'] / 1000 + req\n\n    # compute some constants\n    aeq = 1 - req**2 / (h ** 2)\n    ap_ = 1 - rp**2 / (h ** 2)\n\n    # generate points around the north hemisphere in satellite projection\n    # make it a bit smaller so that we stay inside the valid area\n    xmax = np.arccos(np.sqrt(aeq))\n    ymax = np.arccos(np.sqrt(ap_))\n    return xmax, ymax"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes a mask of the earth s shape as seen by a geostationary satellite.", "response": "def get_geostationary_mask(area):\n    \"\"\"Compute a mask of the earth's shape as seen by a geostationary satellite\n\n    Args:\n        area (pyresample.geometry.AreaDefinition) : Corresponding area\n                                                    definition\n\n    Returns:\n        Boolean mask, True inside the earth's shape, False outside.\n    \"\"\"\n    # Compute projection coordinates at the earth's limb\n    h = area.proj_dict['h']\n    xmax, ymax = get_geostationary_angle_extent(area)\n    xmax *= h\n    ymax *= h\n\n    # Compute projection coordinates at the centre of each pixel\n    x, y = area.get_proj_coords_dask()\n\n    # Compute mask of the earth's elliptical shape\n    return ((x / xmax) ** 2 + (y / ymax) ** 2) <= 1"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets lons and lats from x y in projection coordinates.", "response": "def _lonlat_from_geos_angle(x, y, geos_area):\n    \"\"\"Get lons and lats from x, y in projection coordinates.\"\"\"\n    h = (geos_area.proj_dict['h'] + geos_area.proj_dict['a']) / 1000\n    b__ = (geos_area.proj_dict['a'] / geos_area.proj_dict['b']) ** 2\n\n    sd = np.sqrt((h * np.cos(x) * np.cos(y)) ** 2 -\n                 (np.cos(y)**2 + b__ * np.sin(y)**2) *\n                 (h**2 - (geos_area.proj_dict['a'] / 1000)**2))\n    # sd = 0\n\n    sn = (h * np.cos(x) * np.cos(y) - sd) / (np.cos(y)**2 + b__ * np.sin(y)**2)\n    s1 = h - sn * np.cos(x) * np.cos(y)\n    s2 = sn * np.sin(x) * np.cos(y)\n    s3 = -sn * np.sin(y)\n    sxy = np.sqrt(s1**2 + s2**2)\n\n    lons = np.rad2deg(np.arctan2(s2, s1)) + geos_area.proj_dict.get('lon_0', 0)\n    lats = np.rad2deg(-np.arctan2(b__ * s3, sxy))\n\n    return lons, lats"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the bounding box of the valid pixels inside a geospatial polygon.", "response": "def get_geostationary_bounding_box(geos_area, nb_points=50):\n    \"\"\"Get the bbox in lon/lats of the valid pixels inside *geos_area*.\n\n    Args:\n      nb_points: Number of points on the polygon\n    \"\"\"\n    xmax, ymax = get_geostationary_angle_extent(geos_area)\n\n    # generate points around the north hemisphere in satellite projection\n    # make it a bit smaller so that we stay inside the valid area\n    x = np.cos(np.linspace(-np.pi, 0, nb_points / 2)) * (xmax - 0.001)\n    y = -np.sin(np.linspace(-np.pi, 0, nb_points / 2)) * (ymax - 0.001)\n\n    # clip the projection coordinates to fit the area extent of geos_area\n    ll_x, ll_y, ur_x, ur_y = (np.array(geos_area.area_extent) /\n                              geos_area.proj_dict['h'])\n\n    x = np.clip(np.concatenate([x, x[::-1]]), min(ll_x, ur_x), max(ll_x, ur_x))\n    y = np.clip(np.concatenate([y, -y]), min(ll_y, ur_y), max(ll_y, ur_y))\n\n    return _lonlat_from_geos_angle(x, y, geos_area)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_area_slices(data_area, area_to_cover):\n\n    if data_area.proj_dict['proj'] != 'geos':\n        raise NotImplementedError('Only geos supported')\n\n    # Intersection only required for two different projections\n    if area_to_cover.proj_dict['proj'] == data_area.proj_dict['proj']:\n        LOGGER.debug('Projections for data and slice areas are'\n                     ' identical: {}'.format(area_to_cover.proj_dict['proj']))\n        # Get xy coordinates\n        llx, lly, urx, ury = area_to_cover.area_extent\n        x, y = data_area.get_xy_from_proj_coords([llx, urx], [lly, ury])\n\n        return slice(x[0], x[1] + 1), slice(y[1], y[0] + 1)\n\n    data_boundary = Boundary(*get_geostationary_bounding_box(data_area))\n\n    area_boundary = AreaDefBoundary(area_to_cover, 100)\n    intersection = data_boundary.contour_poly.intersection(\n        area_boundary.contour_poly)\n\n    x, y = data_area.get_xy_from_lonlat(np.rad2deg(intersection.lon),\n                                        np.rad2deg(intersection.lat))\n\n    return slice(min(x), max(x) + 1), slice(min(y), max(y) + 1)", "response": "Compute the slice to read from an area based on an area_to_cover."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\napplies slices to the area_extent and size of the area.", "response": "def get_sub_area(area, xslice, yslice):\n    \"\"\"Apply slices to the area_extent and size of the area.\"\"\"\n    new_area_extent = ((area.pixel_upper_left[0] +\n                        (xslice.start - 0.5) * area.pixel_size_x),\n                       (area.pixel_upper_left[1] -\n                        (yslice.stop - 0.5) * area.pixel_size_y),\n                       (area.pixel_upper_left[0] +\n                        (xslice.stop - 0.5) * area.pixel_size_x),\n                       (area.pixel_upper_left[1] -\n                        (yslice.start - 0.5) * area.pixel_size_y))\n\n    return AreaDefinition(area.area_id, area.name,\n                          area.proj_id, area.proj_dict,\n                          xslice.stop - xslice.start,\n                          yslice.stop - yslice.start,\n                          new_area_extent)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unzip_file(filename):\n\n    if filename.endswith('bz2'):\n        bz2file = bz2.BZ2File(filename)\n        fdn, tmpfilepath = tempfile.mkstemp()\n        with closing(os.fdopen(fdn, 'wb')) as ofpt:\n            try:\n                ofpt.write(bz2file.read())\n            except IOError:\n                import traceback\n                traceback.print_exc()\n                LOGGER.info(\"Failed to read bzipped file %s\", str(filename))\n                os.remove(tmpfilepath)\n                return None\n\n        return tmpfilepath\n\n    return None", "response": "Unzip the file if it is bzipped"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bbox(img):\n    rows = np.any(img, axis=1)\n    cols = np.any(img, axis=0)\n    rmin, rmax = np.where(rows)[0][[0, -1]]\n    cmin, cmax = np.where(cols)[0][[0, -1]]\n\n    return rmin, rmax, cmin, cmax", "response": "Find the bounding box around nonzero elements in the given array\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute pieces of metadata from the readers loaded.", "response": "def _compute_metadata_from_readers(self):\n        \"\"\"Determine pieces of metadata from the readers loaded.\"\"\"\n        mda = {'sensor': self._get_sensor_names()}\n\n        # overwrite the request start/end times with actual loaded data limits\n        if self.readers:\n            mda['start_time'] = min(x.start_time\n                                    for x in self.readers.values())\n            mda['end_time'] = max(x.end_time\n                                  for x in self.readers.values())\n        return mda"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\njoining the sensors from all loaded readers.", "response": "def _get_sensor_names(self):\n        \"\"\"Join the sensors from all loaded readers.\"\"\"\n        # if the user didn't tell us what sensors to work with, let's figure it\n        # out\n        if not self.attrs.get('sensor'):\n            # reader finder could return multiple readers\n            return set([sensor for reader_instance in self.readers.values()\n                        for sensor in reader_instance.sensor_names])\n        elif not isinstance(self.attrs['sensor'], (set, tuple, list)):\n            return set([self.attrs['sensor']])\n        else:\n            return set(self.attrs['sensor'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_reader_instances(self,\n                                filenames=None,\n                                reader=None,\n                                reader_kwargs=None):\n        \"\"\"Find readers and return their instances.\"\"\"\n        return load_readers(filenames=filenames,\n                            reader=reader,\n                            reader_kwargs=reader_kwargs,\n                            ppp_config_dir=self.ppp_config_dir)", "response": "Find readers and return their instances."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _compare_areas(self, datasets=None, compare_func=max):\n        if datasets is None:\n            datasets = list(self.values())\n\n        areas = []\n        for ds in datasets:\n            if isinstance(ds, BaseDefinition):\n                areas.append(ds)\n                continue\n            elif not isinstance(ds, DataArray):\n                ds = self[ds]\n            area = ds.attrs.get('area')\n            areas.append(area)\n\n        areas = [x for x in areas if x is not None]\n        if not areas:\n            raise ValueError(\"No dataset areas available\")\n\n        if not all(isinstance(x, type(areas[0]))\n                   for x in areas[1:]):\n            raise ValueError(\"Can't compare areas of different types\")\n        elif isinstance(areas[0], AreaDefinition):\n            first_pstr = areas[0].proj_str\n            if not all(ad.proj_str == first_pstr for ad in areas[1:]):\n                raise ValueError(\"Can't compare areas with different \"\n                                 \"projections.\")\n\n            def key_func(ds):\n                return 1. / ds.pixel_size_x\n        else:\n            def key_func(ds):\n                return ds.shape\n\n        # find the highest/lowest area among the provided\n        return compare_func(areas, key=key_func)", "response": "Get the list of areas for the provided datasets."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets names of available datasets globally or just for reader_name if specified.", "response": "def available_dataset_ids(self, reader_name=None, composites=False):\n        \"\"\"Get names of available datasets, globally or just for *reader_name*\n        if specified, that can be loaded.\n\n        Available dataset names are determined by what each individual reader\n        can load. This is normally determined by what files are needed to load\n        a dataset and what files have been provided to the scene/reader.\n\n        :return: list of available dataset names\n        \"\"\"\n        try:\n            if reader_name:\n                readers = [self.readers[reader_name]]\n            else:\n                readers = self.readers.values()\n        except (AttributeError, KeyError):\n            raise KeyError(\"No reader '%s' found in scene\" % reader_name)\n\n        available_datasets = sorted([dataset_id\n                                     for reader in readers\n                                     for dataset_id in reader.available_dataset_ids])\n        if composites:\n            available_datasets += sorted(self.available_composite_ids(\n                available_datasets))\n        return available_datasets"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef available_dataset_names(self, reader_name=None, composites=False):\n        return sorted(set(x.name for x in self.available_dataset_ids(\n            reader_name=reader_name, composites=composites)))", "response": "Get the list of the names of the available datasets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget names of all datasets from loaded readers or reader_name", "response": "def all_dataset_ids(self, reader_name=None, composites=False):\n        \"\"\"Get names of all datasets from loaded readers or `reader_name` if\n        specified..\n\n        :return: list of all dataset names\n        \"\"\"\n        try:\n            if reader_name:\n                readers = [self.readers[reader_name]]\n            else:\n                readers = self.readers.values()\n        except (AttributeError, KeyError):\n            raise KeyError(\"No reader '%s' found in scene\" % reader_name)\n\n        all_datasets = [dataset_id\n                        for reader in readers\n                        for dataset_id in reader.all_dataset_ids]\n        if composites:\n            all_datasets += self.all_composite_ids()\n        return all_datasets"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef available_composite_ids(self, available_datasets=None):\n        if available_datasets is None:\n            available_datasets = self.available_dataset_ids(composites=False)\n        else:\n            if not all(isinstance(ds_id, DatasetID) for ds_id in available_datasets):\n                raise ValueError(\n                    \"'available_datasets' must all be DatasetID objects\")\n\n        all_comps = self.all_composite_ids()\n        # recreate the dependency tree so it doesn't interfere with the user's\n        # wishlist\n        comps, mods = self.cpl.load_compositors(self.attrs['sensor'])\n        dep_tree = DependencyTree(self.readers, comps, mods)\n        dep_tree.find_dependencies(set(available_datasets + all_comps))\n        available_comps = set(x.name for x in dep_tree.trunk())\n        # get rid of modified composites that are in the trunk\n        return sorted(available_comps & set(all_comps))", "response": "Get names of compositors that can be generated from the available datasets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of all composites known to this Scene.", "response": "def available_composite_names(self, available_datasets=None):\n        \"\"\"All configured composites known to this Scene.\"\"\"\n        return sorted(set(x.name for x in self.available_composite_ids(\n            available_datasets=available_datasets)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets all composite IDs that are configured.", "response": "def all_composite_ids(self, sensor_names=None):\n        \"\"\"Get all composite IDs that are configured.\n\n        Returns: generator of configured composite names\n        \"\"\"\n        if sensor_names is None:\n            sensor_names = self.attrs['sensor']\n        compositors = []\n        # Note if we get compositors from the dep tree then it will include\n        # modified composites which we don't want\n        for sensor_name in sensor_names:\n            sensor_comps = self.cpl.compositors.get(sensor_name, {}).keys()\n            # ignore inline compositor dependencies starting with '_'\n            compositors.extend(c for c in sensor_comps if not c.name.startswith('_'))\n        return sorted(set(compositors))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iter_by_area(self):\n        datasets_by_area = {}\n        for ds in self:\n            a = ds.attrs.get('area')\n            datasets_by_area.setdefault(a, []).append(\n                DatasetID.from_dict(ds.attrs))\n\n        return datasets_by_area.items()", "response": "Generate datasets grouped by Area."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef copy(self, datasets=None):\n        new_scn = self.__class__()\n        new_scn.attrs = self.attrs.copy()\n        new_scn.dep_tree = self.dep_tree.copy()\n\n        for ds_id in (datasets or self.keys()):\n            # NOTE: Must use `.datasets` or side effects of `__setitem__`\n            #       could hurt us with regards to the wishlist\n            new_scn.datasets[ds_id] = self[ds_id]\n\n        if not datasets:\n            new_scn.wishlist = self.wishlist.copy()\n        else:\n            new_scn.wishlist = set([DatasetID.from_dict(ds.attrs)\n                                    for ds in new_scn])\n        return new_scn", "response": "Create a copy of the Scene including dependency information."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef all_same_area(self):\n        all_areas = [x.attrs.get('area', None) for x in self.values()]\n        all_areas = [x for x in all_areas if x is not None]\n        return all(all_areas[0] == x for x in all_areas[1:])", "response": "All contained data arrays are on the same area."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef all_same_proj(self):\n        all_areas = [x.attrs.get('area', None) for x in self.values()]\n        all_areas = [x for x in all_areas if x is not None]\n        return all(all_areas[0].proj_str == x.proj_str for x in all_areas[1:])", "response": "All contained data array are in the same projection."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _slice_area_from_bbox(self, src_area, dst_area, ll_bbox=None,\n                              xy_bbox=None):\n        \"\"\"Slice the provided area using the bounds provided.\"\"\"\n        if ll_bbox is not None:\n            dst_area = AreaDefinition(\n                'crop_area', 'crop_area', 'crop_latlong',\n                {'proj': 'latlong'}, 100, 100, ll_bbox)\n        elif xy_bbox is not None:\n            dst_area = AreaDefinition(\n                'crop_area', 'crop_area', 'crop_xy',\n                src_area.proj_dict, src_area.x_size, src_area.y_size,\n                xy_bbox)\n        x_slice, y_slice = src_area.get_area_slices(dst_area)\n        return src_area[y_slice, x_slice], y_slice, x_slice", "response": "Slice the provided area using the bounds provided."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nslice the scene in - place for the datasets specified.", "response": "def _slice_datasets(self, dataset_ids, slice_key, new_area, area_only=True):\n        \"\"\"Slice scene in-place for the datasets specified.\"\"\"\n        new_datasets = {}\n        datasets = (self[ds_id] for ds_id in dataset_ids)\n        for ds, parent_ds in dataset_walker(datasets):\n            ds_id = DatasetID.from_dict(ds.attrs)\n            # handle ancillary variables\n            pres = None\n            if parent_ds is not None:\n                pres = new_datasets[DatasetID.from_dict(parent_ds.attrs)]\n            if ds_id in new_datasets:\n                replace_anc(ds, pres)\n                continue\n            if area_only and ds.attrs.get('area') is None:\n                new_datasets[ds_id] = ds\n                replace_anc(ds, pres)\n                continue\n\n            if not isinstance(slice_key, dict):\n                # match dimension name to slice object\n                key = dict(zip(ds.dims, slice_key))\n            else:\n                key = slice_key\n            new_ds = ds.isel(**key)\n            if new_area is not None:\n                new_ds.attrs['area'] = new_area\n\n            new_datasets[ds_id] = new_ds\n            if parent_ds is None:\n                # don't use `__setitem__` because we don't want this to\n                # affect the existing wishlist/dep tree\n                self.datasets[ds_id] = new_ds\n            else:\n                replace_anc(new_ds, pres)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nslice by dataset index.", "response": "def slice(self, key):\n        \"\"\"Slice Scene by dataset index.\n\n        .. note::\n\n            DataArrays that do not have an ``area`` attribute will not be\n            sliced.\n\n        \"\"\"\n        if not self.all_same_area:\n            raise RuntimeError(\"'Scene' has different areas and cannot \"\n                               \"be usefully sliced.\")\n        # slice\n        new_scn = self.copy()\n        new_scn.wishlist = self.wishlist\n        for area, dataset_ids in self.iter_by_area():\n            if area is not None:\n                # assume dimensions for area are y and x\n                one_ds = self[dataset_ids[0]]\n                area_key = tuple(sl for dim, sl in zip(one_ds.dims, key) if dim in ['y', 'x'])\n                new_area = area[area_key]\n            else:\n                new_area = None\n            new_scn._slice_datasets(dataset_ids, key, new_area)\n        return new_scn"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncrop the current Scene to a specific area boundary or bounding box.", "response": "def crop(self, area=None, ll_bbox=None, xy_bbox=None, dataset_ids=None):\n        \"\"\"Crop Scene to a specific Area boundary or bounding box.\n\n        Args:\n            area (AreaDefinition): Area to crop the current Scene to\n            ll_bbox (tuple, list): 4-element tuple where values are in\n                                   lon/lat degrees. Elements are\n                                   ``(xmin, ymin, xmax, ymax)`` where X is\n                                   longitude and Y is latitude.\n            xy_bbox (tuple, list): Same as `ll_bbox` but elements are in\n                                   projection units.\n            dataset_ids (iterable): DatasetIDs to include in the returned\n                                 `Scene`. Defaults to all datasets.\n\n        This method will attempt to intelligently slice the data to preserve\n        relationships between datasets. For example, if we are cropping two\n        DataArrays of 500m and 1000m pixel resolution then this method will\n        assume that exactly 4 pixels of the 500m array cover the same\n        geographic area as a single 1000m pixel. It handles these cases based\n        on the shapes of the input arrays and adjusting slicing indexes\n        accordingly. This method will have trouble handling cases where data\n        arrays seem related but don't cover the same geographic area or if the\n        coarsest resolution data is not related to the other arrays which are\n        related.\n\n        It can be useful to follow cropping with a call to the native\n        resampler to resolve all datasets to the same resolution and compute\n        any composites that could not be generated previously::\n\n        >>> cropped_scn = scn.crop(ll_bbox=(-105., 40., -95., 50.))\n        >>> remapped_scn = cropped_scn.resample(resampler='native')\n\n        .. note::\n\n            The `resample` method automatically crops input data before\n            resampling to save time/memory.\n\n        \"\"\"\n        if len([x for x in [area, ll_bbox, xy_bbox] if x is not None]) != 1:\n            raise ValueError(\"One and only one of 'area', 'll_bbox', \"\n                             \"or 'xy_bbox' can be specified.\")\n\n        new_scn = self.copy(datasets=dataset_ids)\n        if not new_scn.all_same_proj and xy_bbox is not None:\n            raise ValueError(\"Can't crop when dataset_ids are not all on the \"\n                             \"same projection.\")\n\n        # get the lowest resolution area, use it as the base of the slice\n        # this makes sure that the other areas *should* be a consistent factor\n        min_area = new_scn.min_area()\n        if isinstance(area, str):\n            area = get_area_def(area)\n        new_min_area, min_y_slice, min_x_slice = self._slice_area_from_bbox(\n            min_area, area, ll_bbox, xy_bbox)\n        new_target_areas = {}\n        for src_area, dataset_ids in new_scn.iter_by_area():\n            if src_area is None:\n                for ds_id in dataset_ids:\n                    new_scn.datasets[ds_id] = self[ds_id]\n                continue\n\n            y_factor, y_remainder = np.divmod(float(src_area.shape[0]),\n                                              min_area.shape[0])\n            x_factor, x_remainder = np.divmod(float(src_area.shape[1]),\n                                              min_area.shape[1])\n            y_factor = int(y_factor)\n            x_factor = int(x_factor)\n            if y_remainder == 0 and x_remainder == 0:\n                y_slice = slice(min_y_slice.start * y_factor,\n                                min_y_slice.stop * y_factor)\n                x_slice = slice(min_x_slice.start * x_factor,\n                                min_x_slice.stop * x_factor)\n                new_area = src_area[y_slice, x_slice]\n                slice_key = {'y': y_slice, 'x': x_slice}\n                new_scn._slice_datasets(dataset_ids, slice_key, new_area)\n            else:\n                new_target_areas[src_area] = self._slice_area_from_bbox(\n                    src_area, area, ll_bbox, xy_bbox\n                )\n\n        return new_scn"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef aggregate(self, dataset_ids=None, boundary='exact', side='left', func='mean', **dim_kwargs):\n        new_scn = self.copy(datasets=dataset_ids)\n\n        for src_area, ds_ids in new_scn.iter_by_area():\n            if src_area is None:\n                for ds_id in ds_ids:\n                    new_scn.datasets[ds_id] = self[ds_id]\n                continue\n\n            if boundary != 'exact':\n                raise NotImplementedError(\"boundary modes appart from 'exact' are not implemented yet.\")\n            target_area = src_area.aggregate(**dim_kwargs)\n            resolution = max(target_area.pixel_size_x, target_area.pixel_size_y)\n            for ds_id in ds_ids:\n                res = self[ds_id].coarsen(boundary=boundary, side=side, func=func, **dim_kwargs)\n\n                new_scn.datasets[ds_id] = getattr(res, func)()\n                new_scn.datasets[ds_id].attrs['area'] = target_area\n                new_scn.datasets[ds_id].attrs['resolution'] = resolution\n\n        return new_scn", "response": "Create an aggregated version of the Scene."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the given datasets from file.", "response": "def _read_datasets(self, dataset_nodes, **kwargs):\n        \"\"\"Read the given datasets from file.\"\"\"\n        # Sort requested datasets by reader\n        reader_datasets = {}\n\n        for node in dataset_nodes:\n            ds_id = node.name\n            # if we already have this node loaded or the node was assigned\n            # by the user (node data is None) then don't try to load from a\n            # reader\n            if ds_id in self.datasets or not isinstance(node.data, dict):\n                continue\n            reader_name = node.data.get('reader_name')\n            if reader_name is None:\n                # This shouldn't be possible\n                raise RuntimeError(\"Dependency tree has a corrupt node.\")\n            reader_datasets.setdefault(reader_name, set()).add(ds_id)\n\n        # load all datasets for one reader at a time\n        loaded_datasets = DatasetDict()\n        for reader_name, ds_ids in reader_datasets.items():\n            reader_instance = self.readers[reader_name]\n            new_datasets = reader_instance.load(ds_ids, **kwargs)\n            loaded_datasets.update(new_datasets)\n        self.datasets.update(loaded_datasets)\n        return loaded_datasets"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a composite s prerequisites generating them if needed.", "response": "def _get_prereq_datasets(self, comp_id, prereq_nodes, keepables, skip=False):\n        \"\"\"Get a composite's prerequisites, generating them if needed.\n\n        Args:\n            comp_id (DatasetID): DatasetID for the composite whose\n                                 prerequisites are being collected.\n            prereq_nodes (sequence of Nodes): Prerequisites to collect\n            keepables (set): `set` to update if any prerequisites can't\n                             be loaded at this time (see\n                             `_generate_composite`).\n            skip (bool): If True, consider prerequisites as optional and\n                         only log when they are missing. If False,\n                         prerequisites are considered required and will\n                         raise an exception and log a warning if they can't\n                         be collected. Defaults to False.\n\n        Raises:\n            KeyError: If required (skip=False) prerequisite can't be collected.\n\n        \"\"\"\n        prereq_datasets = []\n        delayed_gen = False\n        for prereq_node in prereq_nodes:\n            prereq_id = prereq_node.name\n            if prereq_id not in self.datasets and prereq_id not in keepables \\\n                    and not prereq_node.is_leaf:\n                self._generate_composite(prereq_node, keepables)\n\n            if prereq_id in self.datasets:\n                prereq_datasets.append(self.datasets[prereq_id])\n            elif not prereq_node.is_leaf and prereq_id in keepables:\n                delayed_gen = True\n                continue\n            elif not skip:\n                LOG.debug(\"Missing prerequisite for '{}': '{}'\".format(comp_id, prereq_id))\n                raise KeyError(\"Missing composite prerequisite\")\n            else:\n                LOG.debug(\"Missing optional prerequisite for {}: {}\".format(comp_id, prereq_id))\n\n        if delayed_gen:\n            keepables.add(comp_id)\n            keepables.update([x.name for x in prereq_nodes])\n            LOG.debug(\"Delaying generation of %s because of dependency's delayed generation: %s\", comp_id, prereq_id)\n            if not skip:\n                LOG.debug(\"Missing prerequisite for '{}': '{}'\".format(comp_id, prereq_id))\n                raise KeyError(\"Missing composite prerequisite\")\n            else:\n                LOG.debug(\"Missing optional prerequisite for {}: {}\".format(comp_id, prereq_id))\n\n        return prereq_datasets"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncollect all composite prereqs and create a composite Dataset.", "response": "def _generate_composite(self, comp_node, keepables):\n        \"\"\"Collect all composite prereqs and create the specified composite.\n\n        Args:\n            comp_node (Node): Composite Node to generate a Dataset for\n            keepables (set): `set` to update if any datasets are needed\n                             when generation is continued later. This can\n                             happen if generation is delayed to incompatible\n                             areas which would require resampling first.\n\n        \"\"\"\n        if comp_node.name in self.datasets:\n            # already loaded\n            return\n        compositor, prereqs, optional_prereqs = comp_node.data\n\n        try:\n            prereq_datasets = self._get_prereq_datasets(\n                comp_node.name,\n                prereqs,\n                keepables,\n            )\n        except KeyError:\n            return\n\n        optional_datasets = self._get_prereq_datasets(\n            comp_node.name,\n            optional_prereqs,\n            keepables,\n            skip=True\n        )\n\n        try:\n            composite = compositor(prereq_datasets,\n                                   optional_datasets=optional_datasets,\n                                   **self.attrs)\n\n            cid = DatasetID.from_dict(composite.attrs)\n\n            self.datasets[cid] = composite\n            # update the node with the computed DatasetID\n            if comp_node.name in self.wishlist:\n                self.wishlist.remove(comp_node.name)\n                self.wishlist.add(cid)\n            comp_node.name = cid\n        except IncompatibleAreas:\n            LOG.debug(\"Delaying generation of %s because of incompatible areas\", str(compositor.id))\n            preservable_datasets = set(self.datasets.keys())\n            prereq_ids = set(p.name for p in prereqs)\n            opt_prereq_ids = set(p.name for p in optional_prereqs)\n            keepables |= preservable_datasets & (prereq_ids | opt_prereq_ids)\n            # even though it wasn't generated keep a list of what\n            # might be needed in other compositors\n            keepables.add(comp_node.name)\n            return"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_composites(self, compositor_nodes):\n        keepables = set()\n        for item in compositor_nodes:\n            self._generate_composite(item, keepables)\n        return keepables", "response": "Read ( generate ) composites."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading datasets from the necessary reader.", "response": "def read(self, nodes=None, **kwargs):\n        \"\"\"Load datasets from the necessary reader.\n\n        Args:\n            nodes (iterable): DependencyTree Node objects\n            **kwargs: Keyword arguments to pass to the reader's `load` method.\n\n        Returns:\n            DatasetDict of loaded datasets\n\n        \"\"\"\n        if nodes is None:\n            required_nodes = self.wishlist - set(self.datasets.keys())\n            nodes = self.dep_tree.leaves(nodes=required_nodes)\n        return self._read_datasets(nodes, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes all the composites contained in requirements.", "response": "def generate_composites(self, nodes=None):\n        \"\"\"Compute all the composites contained in `requirements`.\n        \"\"\"\n        if nodes is None:\n            required_nodes = self.wishlist - set(self.datasets.keys())\n            nodes = set(self.dep_tree.trunk(nodes=required_nodes)) - \\\n                set(self.datasets.keys())\n        return self._read_composites(nodes)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nunloading all unneeded datasets.", "response": "def unload(self, keepables=None):\n        \"\"\"Unload all unneeded datasets.\n\n        Datasets are considered unneeded if they weren't directly requested\n        or added to the Scene by the user or they are no longer needed to\n        generate composites that have yet to be generated.\n\n        Args:\n            keepables (iterable): DatasetIDs to keep whether they are needed\n                                  or not.\n\n        \"\"\"\n        to_del = [ds_id for ds_id, projectable in self.datasets.items()\n                  if ds_id not in self.wishlist and (not keepables or ds_id\n                                                     not in keepables)]\n        for ds_id in to_del:\n            LOG.debug(\"Unloading dataset: %r\", ds_id)\n            del self.datasets[ds_id]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading and generate a set of available datasets.", "response": "def load(self, wishlist, calibration=None, resolution=None,\n             polarization=None, level=None, generate=True, unload=True,\n             **kwargs):\n        \"\"\"Read and generate requested datasets.\n\n        When the `wishlist` contains `DatasetID` objects they can either be\n        fully-specified `DatasetID` objects with every parameter specified\n        or they can not provide certain parameters and the \"best\" parameter\n        will be chosen. For example, if a dataset is available in multiple\n        resolutions and no resolution is specified in the wishlist's DatasetID\n        then the highest (smallest number) resolution will be chosen.\n\n        Loaded `DataArray` objects are created and stored in the Scene object.\n\n        Args:\n            wishlist (iterable): Names (str), wavelengths (float), or\n                                 DatasetID objects of the requested datasets\n                                 to load. See `available_dataset_ids()` for\n                                 what datasets are available.\n            calibration (list, str): Calibration levels to limit available\n                                      datasets. This is a shortcut to\n                                      having to list each DatasetID in\n                                      `wishlist`.\n            resolution (list | float): Resolution to limit available datasets.\n                                       This is a shortcut similar to\n                                       calibration.\n            polarization (list | str): Polarization ('V', 'H') to limit\n                                       available datasets. This is a shortcut\n                                       similar to calibration.\n            level (list | str): Pressure level to limit available datasets.\n                                Pressure should be in hPa or mb. If an\n                                altitude is used it should be specified in\n                                inverse meters (1/m). The units of this\n                                parameter ultimately depend on the reader.\n            generate (bool): Generate composites from the loaded datasets\n                             (default: True)\n            unload (bool): Unload datasets that were required to generate\n                           the requested datasets (composite dependencies)\n                           but are no longer needed.\n\n        \"\"\"\n        dataset_keys = set(wishlist)\n        needed_datasets = (self.wishlist | dataset_keys) - \\\n            set(self.datasets.keys())\n        unknown = self.dep_tree.find_dependencies(needed_datasets,\n                                                  calibration=calibration,\n                                                  polarization=polarization,\n                                                  resolution=resolution,\n                                                  level=level)\n        self.wishlist |= needed_datasets\n        if unknown:\n            unknown_str = \", \".join(map(str, unknown))\n            raise KeyError(\"Unknown datasets: {}\".format(unknown_str))\n\n        self.read(**kwargs)\n        if generate:\n            keepables = self.generate_composites()\n        else:\n            # don't lose datasets we loaded to try to generate composites\n            keepables = set(self.datasets.keys()) | self.wishlist\n        if self.missing_datasets:\n            # copy the set of missing datasets because they won't be valid\n            # after they are removed in the next line\n            missing = self.missing_datasets.copy()\n            self._remove_failed_datasets(keepables)\n            missing_str = \", \".join(str(x) for x in missing)\n            LOG.warning(\"The following datasets were not created and may require \"\n                        \"resampling to be generated: {}\".format(missing_str))\n        if unload:\n            self.unload(keepables=keepables)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _slice_data(self, source_area, slices, dataset):\n        slice_x, slice_y = slices\n        dataset = dataset.isel(x=slice_x, y=slice_y)\n        assert ('x', source_area.x_size) in dataset.sizes.items()\n        assert ('y', source_area.y_size) in dataset.sizes.items()\n        dataset.attrs['area'] = source_area\n\n        return dataset", "response": "Slice the data to reduce it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef resample(self, destination=None, datasets=None, generate=True,\n                 unload=True, resampler=None, reduce_data=True,\n                 **resample_kwargs):\n        \"\"\"Resample datasets and return a new scene.\n\n        Args:\n            destination (AreaDefinition, GridDefinition): area definition to\n                resample to. If not specified then the area returned by\n                `Scene.max_area()` will be used.\n            datasets (list): Limit datasets to resample to these specified\n                `DatasetID` objects . By default all currently loaded\n                datasets are resampled.\n            generate (bool): Generate any requested composites that could not\n                be previously due to incompatible areas (default: True).\n            unload (bool): Remove any datasets no longer needed after\n                requested composites have been generated (default: True).\n            resampler (str): Name of resampling method to use. By default,\n                this is a nearest neighbor KDTree-based resampling\n                ('nearest'). Other possible values include 'native', 'ewa',\n                etc. See the :mod:`~satpy.resample` documentation for more\n                information.\n            reduce_data (bool): Reduce data by matching the input and output\n                areas and slicing the data arrays (default: True)\n            resample_kwargs: Remaining keyword arguments to pass to individual\n                resampler classes. See the individual resampler class\n                documentation :mod:`here <satpy.resample>` for available\n                arguments.\n\n        \"\"\"\n        to_resample_ids = [dsid for (dsid, dataset) in self.datasets.items()\n                           if (not datasets) or dsid in datasets]\n\n        if destination is None:\n            destination = self.max_area(to_resample_ids)\n        new_scn = self.copy(datasets=to_resample_ids)\n        # we may have some datasets we asked for but don't exist yet\n        new_scn.wishlist = self.wishlist.copy()\n        self._resampled_scene(new_scn, destination, resampler=resampler,\n                              reduce_data=reduce_data, **resample_kwargs)\n\n        # regenerate anything from the wishlist that needs it (combining\n        # multiple resolutions, etc.)\n        if generate:\n            keepables = new_scn.generate_composites()\n        else:\n            # don't lose datasets that we may need later for generating\n            # composites\n            keepables = set(new_scn.datasets.keys()) | new_scn.wishlist\n\n        if new_scn.missing_datasets:\n            # copy the set of missing datasets because they won't be valid\n            # after they are removed in the next line\n            missing = new_scn.missing_datasets.copy()\n            new_scn._remove_failed_datasets(keepables)\n            missing_str = \", \".join(str(x) for x in missing)\n            LOG.warning(\n                \"The following datasets \"\n                \"were not created: {}\".format(missing_str))\n        if unload:\n            new_scn.unload(keepables)\n\n        return new_scn", "response": "Resample datasets and return a new scene."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef show(self, dataset_id, overlay=None):\n        from satpy.writers import get_enhanced_image\n        from satpy.utils import in_ipynb\n        img = get_enhanced_image(self[dataset_id].squeeze(), overlay=overlay)\n        if not in_ipynb():\n            img.show()\n        return img", "response": "Show the dataset on screen as an image."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_geoviews(self, gvtype=None, datasets=None, kdims=None, vdims=None, dynamic=False):\n        try:\n            import geoviews as gv\n            from cartopy import crs  # noqa\n        except ImportError:\n            import warnings\n            warnings.warn(\"This method needs the geoviews package installed.\")\n\n        if gvtype is None:\n            gvtype = gv.Image\n\n        ds = self.to_xarray_dataset(datasets)\n\n        if vdims is None:\n            # by default select first data variable as display variable\n            vdims = ds.data_vars[list(ds.data_vars.keys())[0]].name\n\n        if hasattr(ds, \"area\") and hasattr(ds.area, 'to_cartopy_crs'):\n            dscrs = ds.area.to_cartopy_crs()\n            gvds = gv.Dataset(ds, crs=dscrs)\n        else:\n            gvds = gv.Dataset(ds)\n\n        if \"latitude\" in ds.coords.keys():\n            gview = gvds.to(gv.QuadMesh, kdims=[\"longitude\", \"latitude\"], vdims=vdims, dynamic=dynamic)\n        else:\n            gview = gvds.to(gvtype, kdims=[\"x\", \"y\"], vdims=vdims, dynamic=dynamic)\n\n        return gview", "response": "Convert satpy. Scene to geoviews."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_xarray_dataset(self, datasets=None):\n        if datasets is not None:\n            datasets = [self[ds] for ds in datasets]\n        else:\n            datasets = [self.datasets.get(ds) for ds in self.wishlist]\n            datasets = [ds for ds in datasets if ds is not None]\n\n        ds_dict = {i.attrs['name']: i.rename(i.attrs['name']) for i in datasets if i.attrs.get('area') is not None}\n        mdata = combine_metadata(*tuple(i.attrs for i in datasets))\n        if mdata.get('area') is None or not isinstance(mdata['area'], SwathDefinition):\n            # either don't know what the area is or we have an AreaDefinition\n            ds = xr.merge(ds_dict.values())\n        else:\n            # we have a swath definition and should use lon/lat values\n            lons, lats = mdata['area'].get_lonlats()\n            if not isinstance(lons, DataArray):\n                lons = DataArray(lons, dims=('y', 'x'))\n                lats = DataArray(lats, dims=('y', 'x'))\n            # ds_dict['longitude'] = lons\n            # ds_dict['latitude'] = lats\n            ds = xr.Dataset(ds_dict, coords={\"latitude\": ([\"y\", \"x\"], lats),\n                                             \"longitude\": ([\"y\", \"x\"], lons)})\n\n        ds.attrs = mdata\n        return ds", "response": "Merge all xr. DataArrays of a scene to a xr. Dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef images(self):\n        for ds_id, projectable in self.datasets.items():\n            if ds_id in self.wishlist:\n                yield projectable.to_image()", "response": "Generate all the images from the scene."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsave the dataset_id to file using the specified writer.", "response": "def save_dataset(self, dataset_id, filename=None, writer=None, overlay=None, compute=True, **kwargs):\n        \"\"\"Save the *dataset_id* to file using *writer* (default: geotiff).\"\"\"\n        if writer is None and filename is None:\n            writer = 'geotiff'\n        elif writer is None:\n            writer = self.get_writer_by_ext(os.path.splitext(filename)[1])\n\n        writer, save_kwargs = load_writer(writer,\n                                          ppp_config_dir=self.ppp_config_dir,\n                                          filename=filename,\n                                          **kwargs)\n        return writer.save_dataset(self[dataset_id],\n                                   overlay=overlay, compute=compute,\n                                   **save_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave all the datasets present in a scene to disk using the specified writer.", "response": "def save_datasets(self, writer=\"geotiff\", datasets=None, compute=True, **kwargs):\n        \"\"\"Save all the datasets present in a scene to disk using *writer*.\"\"\"\n        if datasets is not None:\n            datasets = [self[ds] for ds in datasets]\n        else:\n            datasets = [self.datasets.get(ds) for ds in self.wishlist]\n            datasets = [ds for ds in datasets if ds is not None]\n        if not datasets:\n            raise RuntimeError(\"None of the requested datasets have been \"\n                               \"generated or could not be loaded. Requested \"\n                               \"composite inputs may need to have matching \"\n                               \"dimensions (eg. through resampling).\")\n        writer, save_kwargs = load_writer(writer, ppp_config_dir=self.ppp_config_dir, **kwargs)\n        return writer.save_datasets(datasets, compute=compute, **save_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts an ElementTree into a dict.", "response": "def dictify(r, root=True):\n    \"\"\"Convert an ElementTree into a dict.\"\"\"\n    if root:\n        return {r.tag: dictify(r, False)}\n    d = {}\n    if r.text and r.text.strip():\n        try:\n            return int(r.text)\n        except ValueError:\n            try:\n                return float(r.text)\n            except ValueError:\n                return r.text\n    for x in r.findall(\"./*\"):\n        if x.tag in d and not isinstance(d[x.tag], list):\n            d[x.tag] = [d[x.tag]]\n            d[x.tag].append(dictify(x, False))\n        else:\n            d[x.tag] = dictify(x, False)\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninterpolate the given slice of the larger array.", "response": "def interpolate_slice(slice_rows, slice_cols, interpolator):\n    \"\"\"Interpolate the given slice of the larger array.\"\"\"\n    fine_rows = np.arange(slice_rows.start, slice_rows.stop, slice_rows.step)\n    fine_cols = np.arange(slice_cols.start, slice_cols.stop, slice_cols.step)\n    return interpolator(fine_cols, fine_rows)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninterpolates a dask array.", "response": "def interpolate_xarray(xpoints, ypoints, values, shape, kind='cubic',\n                       blocksize=CHUNK_SIZE):\n    \"\"\"Interpolate, generating a dask array.\"\"\"\n    vchunks = range(0, shape[0], blocksize)\n    hchunks = range(0, shape[1], blocksize)\n\n    token = tokenize(blocksize, xpoints, ypoints, values, kind, shape)\n    name = 'interpolate-' + token\n\n    from scipy.interpolate import interp2d\n    interpolator = interp2d(xpoints, ypoints, values, kind=kind)\n\n    dskx = {(name, i, j): (interpolate_slice,\n                           slice(vcs, min(vcs + blocksize, shape[0])),\n                           slice(hcs, min(hcs + blocksize, shape[1])),\n                           interpolator)\n            for i, vcs in enumerate(vchunks)\n            for j, hcs in enumerate(hchunks)\n            }\n\n    res = da.Array(dskx, name, shape=list(shape),\n                   chunks=(blocksize, blocksize),\n                   dtype=values.dtype)\n    return DataArray(res, dims=('y', 'x'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninterpolating linearly generating a dask array.", "response": "def interpolate_xarray_linear(xpoints, ypoints, values, shape, chunks=CHUNK_SIZE):\n    \"\"\"Interpolate linearly, generating a dask array.\"\"\"\n    from scipy.interpolate.interpnd import (LinearNDInterpolator,\n                                            _ndim_coords_from_arrays)\n\n    if isinstance(chunks, (list, tuple)):\n        vchunks, hchunks = chunks\n    else:\n        vchunks, hchunks = chunks, chunks\n\n    points = _ndim_coords_from_arrays(np.vstack((np.asarray(ypoints),\n                                                 np.asarray(xpoints))).T)\n\n    interpolator = LinearNDInterpolator(points, values)\n\n    grid_x, grid_y = da.meshgrid(da.arange(shape[1], chunks=hchunks),\n                                 da.arange(shape[0], chunks=vchunks))\n\n    # workaround for non-thread-safe first call of the interpolator:\n    interpolator((0, 0))\n    res = da.map_blocks(intp, grid_x, grid_y, interpolator=interpolator)\n\n    return DataArray(res, dims=('y', 'x'))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread an array from an xml elements *elts*.", "response": "def read_xml_array(elts, variable_name):\n        \"\"\"Read an array from an xml elements *elts*.\"\"\"\n        y = []\n        x = []\n        data = []\n        for elt in elts:\n            newx = elt.find('pixel').text.split()\n            y += [int(elt.find('line').text)] * len(newx)\n            x += [int(val) for val in newx]\n            data += [float(val)\n                     for val in elt.find(variable_name).text.split()]\n\n        return np.asarray(data), (x, y)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_azimuth_noise_array(elts):\n        y = []\n        x = []\n        data = []\n        for elt in elts:\n            first_pixel = int(elt.find('firstRangeSample').text)\n            last_pixel = int(elt.find('lastRangeSample').text)\n            lines = elt.find('line').text.split()\n            lut = elt.find('noiseAzimuthLut').text.split()\n            pixels = [first_pixel, last_pixel]\n            swath = elt.find('swath').text\n            corr = 1\n            if swath == 'EW1':\n                corr = 1.5\n            if swath == 'EW4':\n                corr = 1.2\n            if swath == 'EW5':\n                corr = 1.5\n\n            for pixel in pixels:\n                y += [int(val) for val in lines]\n                x += [pixel] * len(lines)\n                data += [float(val) * corr for val in lut]\n\n        return np.asarray(data), (x, y)", "response": "Read the azimuth noise vectors."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninterpolate arbitrary size dataset to a full sized grid.", "response": "def interpolate_xml_array(data, low_res_coords, shape, chunks):\n        \"\"\"Interpolate arbitrary size dataset to a full sized grid.\"\"\"\n        xpoints, ypoints = low_res_coords\n\n        return interpolate_xarray_linear(xpoints, ypoints, data, shape, chunks=chunks)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the noise correction array.", "response": "def get_noise_correction(self, shape, chunks=None):\n        \"\"\"Get the noise correction array.\"\"\"\n        data_items = self.root.findall(\".//noiseVector\")\n        data, low_res_coords = self.read_xml_array(data_items, 'noiseLut')\n        if not data_items:\n            data_items = self.root.findall(\".//noiseRangeVector\")\n            data, low_res_coords = self.read_xml_array(data_items, 'noiseRangeLut')\n            range_noise = self.interpolate_xml_array(data, low_res_coords, shape, chunks=chunks)\n            data_items = self.root.findall(\".//noiseAzimuthVector\")\n            data, low_res_coords = self.read_azimuth_noise_array(data_items)\n            azimuth_noise = self.interpolate_xml_array(data, low_res_coords, shape, chunks=chunks)\n            noise = range_noise * azimuth_noise\n        else:\n            noise = self.interpolate_xml_array(data, low_res_coords, shape, chunks=chunks)\n        return noise"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the calibration array.", "response": "def get_calibration(self, name, shape, chunks=None):\n        \"\"\"Get the calibration array.\"\"\"\n        data_items = self.root.findall(\".//calibrationVector\")\n        data, low_res_coords = self.read_xml_array(data_items, name)\n        return self.interpolate_xml_array(data, low_res_coords, shape, chunks=chunks)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading a dataset from the data.", "response": "def get_dataset(self, key, info):\n        \"\"\"Load a dataset.\"\"\"\n        if self._polarization != key.polarization:\n            return\n\n        logger.debug('Reading %s.', key.name)\n\n        if key.name in ['longitude', 'latitude']:\n            logger.debug('Constructing coordinate arrays.')\n\n            if self.lons is None or self.lats is None:\n                self.lons, self.lats, self.alts = self.get_lonlatalts()\n\n            if key.name == 'latitude':\n                data = self.lats\n            else:\n                data = self.lons\n            data.attrs.update(info)\n\n        else:\n            calibration = key.calibration or 'gamma'\n            if calibration == 'sigma_nought':\n                calibration = 'sigmaNought'\n            elif calibration == 'beta_nought':\n                calibration = 'betaNought'\n\n            data = self.read_band()\n            # chunks = data.chunks  # This seems to be slower for some reason\n            chunks = CHUNK_SIZE\n            logger.debug('Reading noise data.')\n            noise = self.noise.get_noise_correction(data.shape, chunks=chunks).fillna(0)\n\n            logger.debug('Reading calibration data.')\n\n            cal = self.calibration.get_calibration(calibration, data.shape, chunks=chunks)\n            cal_constant = self.calibration.get_calibration_constant()\n\n            logger.debug('Calibrating.')\n            data = data.where(data > 0)\n            data = data.astype(np.float64)\n            dn = data * data\n            data = ((dn - noise).clip(min=0) + cal_constant)\n\n            data = (np.sqrt(data) / cal).clip(min=0)\n            data.attrs.update(info)\n\n            del noise, cal\n\n            data.attrs['units'] = calibration\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the band in native blocks.", "response": "def read_band_blocks(self, blocksize=CHUNK_SIZE):\n        \"\"\"Read the band in native blocks.\"\"\"\n        # For sentinel 1 data, the block are 1 line, and dask seems to choke on that.\n        band = self.filehandle\n\n        shape = band.shape\n        token = tokenize(blocksize, band)\n        name = 'read_band-' + token\n        dskx = dict()\n        if len(band.block_shapes) != 1:\n            raise NotImplementedError('Bands with multiple shapes not supported.')\n        else:\n            chunks = band.block_shapes[0]\n\n        def do_read(the_band, the_window, the_lock):\n            with the_lock:\n                return the_band.read(1, None, window=the_window)\n\n        for ji, window in band.block_windows(1):\n            dskx[(name, ) + ji] = (do_read, band, window, self.read_lock)\n\n        res = da.Array(dskx, name, shape=list(shape),\n                       chunks=chunks,\n                       dtype=band.dtypes[0])\n        return DataArray(res, dims=('y', 'x'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_band(self, blocksize=CHUNK_SIZE):\n        band = self.filehandle\n\n        shape = band.shape\n        if len(band.block_shapes) == 1:\n            total_size = blocksize * blocksize * 1.0\n            lines, cols = band.block_shapes[0]\n            if cols > lines:\n                hblocks = cols\n                vblocks = int(total_size / cols / lines)\n            else:\n                hblocks = int(total_size / cols / lines)\n                vblocks = lines\n        else:\n            hblocks = blocksize\n            vblocks = blocksize\n        vchunks = range(0, shape[0], vblocks)\n        hchunks = range(0, shape[1], hblocks)\n\n        token = tokenize(hblocks, vblocks, band)\n        name = 'read_band-' + token\n\n        def do_read(the_band, the_window, the_lock):\n            with the_lock:\n                return the_band.read(1, None, window=the_window)\n\n        dskx = {(name, i, j): (do_read, band,\n                               Window(hcs, vcs,\n                                      min(hblocks,  shape[1] - hcs),\n                                      min(vblocks,  shape[0] - vcs)),\n                               self.read_lock)\n                for i, vcs in enumerate(vchunks)\n                for j, hcs in enumerate(hchunks)\n                }\n\n        res = da.Array(dskx, name, shape=list(shape),\n                       chunks=(vblocks, hblocks),\n                       dtype=band.dtypes[0])\n        return DataArray(res, dims=('y', 'x'))", "response": "Read the band in chunks."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nobtains GCPs and construct latitude and longitude arrays.", "response": "def get_lonlatalts(self):\n        \"\"\"Obtain GCPs and construct latitude and longitude arrays.\n\n        Args:\n           band (gdal band): Measurement band which comes with GCP's\n           array_shape (tuple) : The size of the data array\n        Returns:\n           coordinates (tuple): A tuple with longitude and latitude arrays\n        \"\"\"\n        band = self.filehandle\n\n        (xpoints, ypoints), (gcp_lons, gcp_lats, gcp_alts), (gcps, crs) = self.get_gcps()\n\n        # FIXME: do interpolation on cartesion coordinates if the area is\n        # problematic.\n\n        longitudes = interpolate_xarray(xpoints, ypoints, gcp_lons, band.shape)\n        latitudes = interpolate_xarray(xpoints, ypoints, gcp_lats, band.shape)\n        altitudes = interpolate_xarray(xpoints, ypoints, gcp_alts, band.shape)\n\n        longitudes.attrs['gcps'] = gcps\n        longitudes.attrs['crs'] = crs\n        latitudes.attrs['gcps'] = gcps\n        latitudes.attrs['crs'] = crs\n        altitudes.attrs['gcps'] = gcps\n        altitudes.attrs['crs'] = crs\n\n        return longitudes, latitudes, altitudes"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads GCP from GDAL band. coordinates and return the points ypoints and gcp_coords arrays.", "response": "def get_gcps(self):\n        \"\"\"Read GCP from the GDAL band.\n\n        Args:\n           band (gdal band): Measurement band which comes with GCP's\n           coordinates (tuple): A tuple with longitude and latitude arrays\n\n        Returns:\n           points (tuple): Pixel and Line indices 1d arrays\n           gcp_coords (tuple): longitude and latitude 1d arrays\n\n        \"\"\"\n        gcps = self.filehandle.gcps\n\n        gcp_array = np.array([(p.row, p.col, p.x, p.y, p.z) for p in gcps[0]])\n\n        ypoints = np.unique(gcp_array[:, 0])\n        xpoints = np.unique(gcp_array[:, 1])\n\n        gcp_lons = gcp_array[:, 2].reshape(ypoints.shape[0], xpoints.shape[0])\n        gcp_lats = gcp_array[:, 3].reshape(ypoints.shape[0], xpoints.shape[0])\n        gcp_alts = gcp_array[:, 4].reshape(ypoints.shape[0], xpoints.shape[0])\n\n        return (xpoints, ypoints), (gcp_lons, gcp_lats, gcp_alts), gcps"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a detailed report of the missing data.", "response": "def _get_invalid_info(granule_data):\n    \"\"\"Get a detailed report of the missing data.\n        N/A: not applicable\n        MISS: required value missing at time of processing\n        OBPT: onboard pixel trim (overlapping/bow-tie pixel removed during\n            SDR processing)\n        OGPT: on-ground pixel trim (overlapping/bow-tie pixel removed\n            during EDR processing)\n        ERR: error occurred during processing / non-convergence\n        ELINT: ellipsoid intersect failed / instrument line-of-sight does\n            not intersect the Earth\u2019s surface\n        VDNE: value does not exist / processing algorithm did not execute\n        SOUB: scaled out-of-bounds / solution not within allowed range\n    \"\"\"\n    if issubclass(granule_data.dtype.type, np.integer):\n        msg = (\"na:\" + str((granule_data == 65535).sum()) +\n               \" miss:\" + str((granule_data == 65534).sum()) +\n               \" obpt:\" + str((granule_data == 65533).sum()) +\n               \" ogpt:\" + str((granule_data == 65532).sum()) +\n               \" err:\" + str((granule_data == 65531).sum()) +\n               \" elint:\" + str((granule_data == 65530).sum()) +\n               \" vdne:\" + str((granule_data == 65529).sum()) +\n               \" soub:\" + str((granule_data == 65528).sum()))\n    elif issubclass(granule_data.dtype.type, np.floating):\n        msg = (\"na:\" + str((granule_data == -999.9).sum()) +\n               \" miss:\" + str((granule_data == -999.8).sum()) +\n               \" obpt:\" + str((granule_data == -999.7).sum()) +\n               \" ogpt:\" + str((granule_data == -999.6).sum()) +\n               \" err:\" + str((granule_data == -999.5).sum()) +\n               \" elint:\" + str((granule_data == -999.4).sum()) +\n               \" vdne:\" + str((granule_data == -999.3).sum()) +\n               \" soub:\" + str((granule_data == -999.2).sum()))\n    return msg"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsplit the provided filehandlers *fhs* into desired filehandlers and others.", "response": "def split_desired_other(fhs, req_geo, rem_geo):\n    \"\"\"Split the provided filehandlers *fhs* into desired filehandlers and others.\"\"\"\n    desired = []\n    other = []\n    for fh in fhs:\n        if req_geo in fh.datasets:\n            desired.append(fh)\n        elif rem_geo in fh.datasets:\n            other.append(fh)\n    return desired, other"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef scale_swath_data(self, data, scaling_factors):\n        num_grans = len(scaling_factors) // 2\n        gran_size = data.shape[0] // num_grans\n        factors = scaling_factors.where(scaling_factors > -999)\n        factors = factors.data.reshape((-1, 2))\n        factors = xr.DataArray(da.repeat(factors, gran_size, axis=0),\n                               dims=(data.dims[0], 'factors'))\n        data = data * factors[:, 0] + factors[:, 1]\n        return data", "response": "Scale swath data using scaling factors and offsets."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef expand_single_values(var, scans):\n        if scans.size == 1:\n            return var\n        else:\n            expanded = np.repeat(var, scans)\n            expanded.attrs = var.attrs\n            expanded.rename({expanded.dims[0]: 'y'})\n            return expanded", "response": "Expand single valued variable to full scan lengths."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the dataset corresponding to dataset_id.", "response": "def get_dataset(self, dataset_id, ds_info):\n        \"\"\"Get the dataset corresponding to *dataset_id*.\n\n        The size of the return DataArray will be dependent on the number of\n        scans actually sensed, and not necessarily the regular 768 scanlines\n        that the file contains for each granule. To that end, the number of\n        scans for each granule is read from:\n        ``Data_Products/...Gran_x/N_Number_Of_Scans``.\n        \"\"\"\n        dataset_group = [ds_group for ds_group in ds_info['dataset_groups'] if ds_group in self.datasets]\n        if not dataset_group:\n            return\n        else:\n            dataset_group = dataset_group[0]\n            ds_info['dataset_group'] = dataset_group\n        var_path = self._generate_file_key(dataset_id, ds_info)\n        factor_var_path = ds_info.get(\"factors_key\", var_path + \"Factors\")\n\n        data = self.concatenate_dataset(dataset_group, var_path)\n        data = self.mask_fill_values(data, ds_info)\n        factors = self.get(factor_var_path)\n        if factors is None:\n            LOG.debug(\"No scaling factors found for %s\", dataset_id)\n\n        file_units = self.get_file_units(dataset_id, ds_info)\n        output_units = ds_info.get(\"units\", file_units)\n        factors = self.adjust_scaling_factors(factors, file_units, output_units)\n\n        if factors is not None:\n            data = self.scale_swath_data(data, factors)\n\n        i = getattr(data, 'attrs', {})\n        i.update(ds_info)\n        i.update({\n            \"units\": ds_info.get(\"units\", file_units),\n            \"platform_name\": self.platform_name,\n            \"sensor\": self.sensor_name,\n            \"start_orbit\": self.start_orbit_number,\n            \"end_orbit\": self.end_orbit_number,\n        })\n        i.update(dataset_id.to_dict())\n        data.attrs.update(i)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_bounding_box(self):\n        from pyproj import Geod\n        geod = Geod(ellps='WGS84')\n        dataset_group = DATASET_KEYS[self.datasets[0]]\n        idx = 0\n        lons_ring = None\n        lats_ring = None\n        while True:\n            path = 'Data_Products/{dataset_group}/{dataset_group}_Gran_{idx}/attr/'\n            prefix = path.format(dataset_group=dataset_group, idx=idx)\n            try:\n                lats = self.file_content[prefix + 'G-Ring_Latitude']\n                lons = self.file_content[prefix + 'G-Ring_Longitude']\n                if lons_ring is None:\n                    lons_ring = lons\n                    lats_ring = lats\n                else:\n                    prev_lon = lons_ring[0]\n                    prev_lat = lats_ring[0]\n                    dists = list(geod.inv(lon, lat, prev_lon, prev_lat)[2] for lon, lat in zip(lons, lats))\n                    first_idx = np.argmin(dists)\n                    if first_idx == 2 and len(lons) == 8:\n                        lons_ring = np.hstack((lons[:3], lons_ring[:-2], lons[4:]))\n                        lats_ring = np.hstack((lats[:3], lats_ring[:-2], lats[4:]))\n                    else:\n                        raise NotImplementedError(\"Don't know how to handle G-Rings of length %d\" % len(lons))\n\n            except KeyError:\n                break\n            idx += 1\n\n        return lons_ring, lats_ring", "response": "Get the bounding box of this file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfilters out files based on metadata from the filenames.", "response": "def filter_filenames_by_info(self, filename_items):\n        \"\"\"Filter out file using metadata from the filenames.\n\n        This sorts out the different lon and lat datasets depending on TC is\n        desired or not.\n        \"\"\"\n        filename_items = list(filename_items)\n        geo_keep = []\n        geo_del = []\n        for filename, filename_info in filename_items:\n            filename_info['datasets'] = datasets = filename_info['datasets'].split('-')\n            if ('GITCO' in datasets) or ('GMTCO' in datasets):\n                if self.use_tc is False:\n                    geo_del.append(filename)\n                else:\n                    geo_keep.append(filename)\n            elif ('GIMGO' in datasets) or ('GMODO' in datasets):\n                if self.use_tc is True:\n                    geo_del.append(filename)\n                else:\n                    geo_keep.append(filename)\n        if geo_keep:\n            fdict = dict(filename_items)\n            for to_del in geo_del:\n                for dataset in ['GITCO', 'GMTCO', 'GIMGO', 'GMODO']:\n                    try:\n                        fdict[to_del]['datasets'].remove(dataset)\n                    except ValueError:\n                        pass\n                if not fdict[to_del]['datasets']:\n                    del fdict[to_del]\n            filename_items = fdict.items()\n        for filename, filename_info in filename_items:\n            filename_info['datasets'] = '-'.join(filename_info['datasets'])\n        return super(VIIRSSDRReader, self).filter_filenames_by_info(filename_items)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _load_from_geo_ref(self, dsid):\n        file_handlers = self._get_file_handlers(dsid)\n        if not file_handlers:\n            return None\n\n        fns = []\n        for fh in file_handlers:\n            base_dir = os.path.dirname(fh.filename)\n            try:\n                # get the filename and remove the creation time\n                # which is often wrong\n                fn = fh['/attr/N_GEO_Ref'][:46] + '*.h5'\n                fns.extend(glob(os.path.join(base_dir, fn)))\n\n                # usually is non-terrain corrected file, add the terrain\n                # corrected file too\n                if fn[:5] == 'GIMGO':\n                    fn = 'GITCO' + fn[5:]\n                elif fn[:5] == 'GMODO':\n                    fn = 'GMTCO' + fn[5:]\n                else:\n                    continue\n                fns.extend(glob(os.path.join(base_dir, fn)))\n            except KeyError:\n                LOG.debug(\"Could not load geo-reference information from {}\".format(fh.filename))\n\n        return fns", "response": "Load filenames from the N_GEO_Ref attribute of a dataset s file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_req_rem_geo(self, ds_info):\n        if ds_info['dataset_groups'][0].startswith('GM'):\n            if self.use_tc is False:\n                req_geo = 'GMODO'\n                rem_geo = 'GMTCO'\n            else:\n                req_geo = 'GMTCO'\n                rem_geo = 'GMODO'\n        elif ds_info['dataset_groups'][0].startswith('GI'):\n            if self.use_tc is False:\n                req_geo = 'GIMGO'\n                rem_geo = 'GITCO'\n            else:\n                req_geo = 'GITCO'\n                rem_geo = 'GIMGO'\n        else:\n            raise ValueError('Unknown dataset group %s' % ds_info['dataset_groups'][0])\n        return req_geo, rem_geo", "response": "Find out which geolocation files are needed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_right_geo_fhs(self, dsid, fhs):\n        ds_info = self.ids[dsid]\n        req_geo, rem_geo = self._get_req_rem_geo(ds_info)\n        desired, other = split_desired_other(fhs, req_geo, rem_geo)\n        if desired:\n            try:\n                ds_info['dataset_groups'].remove(rem_geo)\n            except ValueError:\n                pass\n            return desired\n        else:\n            return other", "response": "Find the right geographical file handlers for given dataset ID dsid."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the file handler to load this dataset.", "response": "def _get_file_handlers(self, dsid):\n        \"\"\"Get the file handler to load this dataset.\"\"\"\n        ds_info = self.ids[dsid]\n\n        fhs = [fh for fh in self.file_handlers['generic_file']\n               if set(fh.datasets) & set(ds_info['dataset_groups'])]\n        if not fhs:\n            LOG.warning(\"Required file type '%s' not found or loaded for \"\n                        \"'%s'\", ds_info['file_type'], dsid.name)\n        else:\n            if len(set(ds_info['dataset_groups']) & set(['GITCO', 'GIMGO', 'GMTCO', 'GMODO'])) > 1:\n                fhs = self.get_right_geo_fhs(dsid, fhs)\n            return fhs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the coordinate dataset keys for dsid.", "response": "def _get_coordinates_for_dataset_key(self, dsid):\n        \"\"\"Get the coordinate dataset keys for `dsid`.\n\n        Wraps the base class method in order to load geolocation files\n        from the geo reference attribute in the datasets file.\n        \"\"\"\n        coords = super(VIIRSSDRReader, self)._get_coordinates_for_dataset_key(dsid)\n        for c_id in coords:\n            c_info = self.ids[c_id]  # c_info['dataset_groups'] should be a list of 2 elements\n            self._get_file_handlers(c_id)\n            if len(c_info['dataset_groups']) == 1:  # filtering already done\n                continue\n            try:\n                req_geo, rem_geo = self._get_req_rem_geo(c_info)\n            except ValueError:  # DNB\n                continue\n\n            # check the dataset file for the geolocation filename\n            geo_filenames = self._load_from_geo_ref(dsid)\n            if not geo_filenames:\n                c_info['dataset_groups'] = [rem_geo]\n            else:\n                # concatenate all values\n                new_fhs = sum(self.create_filehandlers(geo_filenames).values(), [])\n                desired, other = split_desired_other(new_fhs, req_geo, rem_geo)\n                if desired:\n                    c_info['dataset_groups'].remove(rem_geo)\n                else:\n                    c_info['dataset_groups'].remove(req_geo)\n\n        return coords"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite datasets in a gdal raster structure dts_ds", "response": "def _gdal_write_datasets(self, dst_ds, datasets):\n        \"\"\"Write datasets in a gdal raster structure dts_ds\"\"\"\n        for i, band in enumerate(datasets['bands']):\n            chn = datasets.sel(bands=band)\n            bnd = dst_ds.GetRasterBand(i + 1)\n            bnd.SetNoDataValue(0)\n            bnd.WriteArray(chn.values)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving the image to the given filename in geotiff_ format.", "response": "def save_image(self, img, filename=None, dtype=None, fill_value=None,\n                   floating_point=None, compute=True, **kwargs):\n        \"\"\"Save the image to the given ``filename`` in geotiff_ format.\n\n        Note for faster output and reduced memory usage the ``rasterio``\n        library must be installed. This writer currently falls back to\n        using ``gdal`` directly, but that will be deprecated in the future.\n\n        Args:\n            img (xarray.DataArray): Data to save to geotiff.\n            filename (str): Filename to save the image to. Defaults to\n                ``filename`` passed during writer creation. Unlike the\n                creation ``filename`` keyword argument, this filename does not\n                get formatted with data attributes.\n            dtype (numpy.dtype): Numpy data type to save the image as.\n                Defaults to 8-bit unsigned integer (``np.uint8``). If the\n                ``dtype`` argument is provided during writer creation then\n                that will be used as the default.\n            fill_value (int or float): Value to use where data values are\n                NaN/null. If this is specified in the writer configuration\n                file that value will be used as the default.\n            floating_point (bool): Deprecated. Use ``dtype=np.float64``\n                instead.\n            compute (bool): Compute dask arrays and save the image\n                immediately. If ``False`` then the return value can be passed\n                to :func:`~satpy.writers.compute_writer_results` to do the\n                computation. This is useful when multiple images may share\n                input calculations where dask can benefit from not repeating\n                them multiple times. Defaults to ``True`` in the writer by\n                itself, but is typically passed as ``False`` by callers where\n                calculations can be combined.\n\n        .. _geotiff: http://trac.osgeo.org/geotiff/\n\n        \"\"\"\n        filename = filename or self.get_filename(**img.data.attrs)\n\n        # Update global GDAL options with these specific ones\n        gdal_options = self.gdal_options.copy()\n        for k in kwargs.keys():\n            if k in self.GDAL_OPTIONS:\n                gdal_options[k] = kwargs[k]\n        if fill_value is None:\n            # fall back to fill_value from configuration file\n            fill_value = self.info.get('fill_value')\n\n        if floating_point is not None:\n            import warnings\n            warnings.warn(\"'floating_point' is deprecated, use\"\n                          \"'dtype=np.float64' instead.\",\n                          DeprecationWarning)\n            dtype = np.float64\n        dtype = dtype if dtype is not None else self.dtype\n        if dtype is None:\n            dtype = np.uint8\n\n        if \"alpha\" in kwargs:\n            raise ValueError(\n                \"Keyword 'alpha' is automatically set based on 'fill_value' \"\n                \"and should not be specified\")\n        if np.issubdtype(dtype, np.floating):\n            if img.mode != \"L\":\n                raise ValueError(\"Image must be in 'L' mode for floating \"\n                                 \"point geotiff saving\")\n            if fill_value is None:\n                LOG.debug(\"Alpha band not supported for float geotiffs, \"\n                          \"setting fill value to 'NaN'\")\n                fill_value = np.nan\n\n        try:\n            import rasterio  # noqa\n            # we can use the faster rasterio-based save\n            return img.save(filename, fformat='tif', fill_value=fill_value,\n                            dtype=dtype, compute=compute, **gdal_options)\n        except ImportError:\n            LOG.warning(\"Using legacy/slower geotiff save method, install \"\n                        \"'rasterio' for faster saving.\")\n            warnings.warn(\"Using legacy/slower geotiff save method with 'gdal'.\"\n                          \"This will be deprecated in the future. Install \"\n                          \"'rasterio' for faster saving and future \"\n                          \"compatibility.\", PendingDeprecationWarning)\n\n            # Map numpy data types to GDAL data types\n            NP2GDAL = {\n                np.float32: gdal.GDT_Float32,\n                np.float64: gdal.GDT_Float64,\n                np.uint8: gdal.GDT_Byte,\n                np.uint16: gdal.GDT_UInt16,\n                np.uint32: gdal.GDT_UInt32,\n                np.int16: gdal.GDT_Int16,\n                np.int32: gdal.GDT_Int32,\n                np.complex64: gdal.GDT_CFloat32,\n                np.complex128: gdal.GDT_CFloat64,\n            }\n\n            # force to numpy dtype object\n            dtype = np.dtype(dtype)\n            gformat = NP2GDAL[dtype.type]\n\n            gdal_options['nbits'] = int(gdal_options.get('nbits',\n                                                         dtype.itemsize * 8))\n            datasets, mode = img._finalize(fill_value=fill_value, dtype=dtype)\n            LOG.debug(\"Saving to GeoTiff: %s\", filename)\n            g_opts = [\"{0}={1}\".format(k.upper(), str(v))\n                      for k, v in gdal_options.items()]\n\n            ensure_dir(filename)\n            delayed = self._create_file(filename, img, gformat, g_opts,\n                                        datasets, mode)\n            if compute:\n                return delayed.compute()\n            return delayed"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_end_time(self):\n        mda_dict = self.filehandle.attributes()\n        core_mda = mda_dict['coremetadata']\n        end_time_str = self.parse_metadata_string(core_mda)\n        self._end_time = datetime.strptime(end_time_str, \"%Y-%m-%dT%H:%M:%SZ\")", "response": "Get observation end time from file metadata."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngrabs end time with regular expression.", "response": "def parse_metadata_string(metadata_string):\n        \"\"\"Grab end time with regular expression.\"\"\"\n        regex = r\"STOP_DATE.+?VALUE\\s*=\\s*\\\"(.+?)\\\"\"\n        match = re.search(regex, metadata_string, re.DOTALL)\n        end_time_str = match.group(1)\n        return end_time_str"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_dataset(self, key, info):\n        if key.name in ['longitude', 'latitude']:\n            logger.debug('Reading coordinate arrays.')\n\n            if self.lons is None or self.lats is None:\n                self.lons, self.lats = self.get_lonlats()\n\n            if key.name == 'latitude':\n                proj = Dataset(self.lats, id=key, **info)\n            else:\n                proj = Dataset(self.lons, id=key, **info)\n\n        else:\n            data = self.get_sds_variable(key.name)\n            proj = Dataset(data, id=key, **info)\n\n        return proj", "response": "Read data from file and return the corresponding projectables."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_sds_variable(self, name):\n        sds_obj = self.filehandle.select(name)\n        data = sds_obj.get()\n        return data", "response": "Read variable from the HDF4 file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget longitude and latitude arrays from the file.", "response": "def get_lonlats(self):\n        \"\"\"Get longitude and latitude arrays from the file.\"\"\"\n        longitudes = self.get_sds_variable('Longitude')\n        latitudes = self.get_sds_variable('Latitude')\n        return longitudes, latitudes"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nflatten tree structure to a one level dictionary.", "response": "def flatten(self, d=None):\n        \"\"\"Flatten tree structure to a one level dictionary.\n\n\n        Args:\n            d (dict, optional): output dictionary to update\n\n        Returns:\n            dict: Node.name -> Node. The returned dictionary includes the\n                  current Node and all its children.\n        \"\"\"\n        if d is None:\n            d = {}\n        if self.name is not None:\n            d[self.name] = self\n        for child in self.children:\n            child.flatten(d=d)\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_child(self, obj):\n        self.children.append(obj)\n        obj.parents.append(self)", "response": "Add a child to the node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef leaves(self, unique=True):\n        if not self.children:\n            return [self]\n        else:\n            res = list()\n            for child in self.children:\n                for sub_child in child.leaves(unique=unique):\n                    if not unique or sub_child not in res:\n                        res.append(sub_child)\n            return res", "response": "Get the leaves of the tree starting at this root."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the trunk of the tree starting at this root.", "response": "def trunk(self, unique=True):\n        \"\"\"Get the trunk of the tree starting at this root.\"\"\"\n        # uniqueness is not correct in `trunk` yet\n        unique = False\n        res = []\n        if self.children:\n            if self.name is not None:\n                res.append(self)\n            for child in self.children:\n                for sub_child in child.trunk(unique=unique):\n                    if not unique or sub_child not in res:\n                        res.append(sub_child)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef leaves(self, nodes=None, unique=True):\n        if nodes is None:\n            return super(DependencyTree, self).leaves(unique=unique)\n\n        res = list()\n        for child_id in nodes:\n            for sub_child in self._all_nodes[child_id].leaves(unique=unique):\n                if not unique or sub_child not in res:\n                    res.append(sub_child)\n        return res", "response": "Returns the leaves of the dependency tree starting at this root."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef copy(self):\n        new_tree = DependencyTree({}, self.compositors, self.modifiers)\n        for c in self.children:\n            c = c.copy(node_cache=new_tree._all_nodes)\n            new_tree.add_child(new_tree, c)\n        return new_tree", "response": "Copy the this node tree into a new tree."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _find_reader_dataset(self, dataset_key, **dfilter):\n        too_many = False\n        for reader_name, reader_instance in self.readers.items():\n            try:\n                ds_id = reader_instance.get_dataset_key(dataset_key, **dfilter)\n            except TooManyResults:\n                LOG.trace(\"Too many datasets matching key {} in reader {}\".format(dataset_key, reader_name))\n                too_many = True\n                continue\n            except KeyError:\n                LOG.trace(\"Can't find dataset %s in reader %s\", str(dataset_key), reader_name)\n                continue\n            LOG.trace(\"Found {} in reader {} when asking for {}\".format(str(ds_id), reader_name, repr(dataset_key)))\n            try:\n                # now that we know we have the exact DatasetID see if we have already created a Node for it\n                return self.getitem(ds_id)\n            except KeyError:\n                # we haven't created a node yet, create it now\n                return Node(ds_id, {'reader_name': reader_name})\n        if too_many:\n            raise TooManyResults(\"Too many keys matching: {}\".format(dataset_key))", "response": "Attempt to find a DatasetID in the available readers."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_compositor_prereqs(self, parent, prereq_names, skip=False,\n                                **dfilter):\n        \"\"\"Determine prerequisite Nodes for a composite.\n\n        Args:\n            parent (Node): Compositor node to add these prerequisites under\n            prereq_names (sequence): Strings (names), floats (wavelengths), or\n                                     DatasetIDs to analyze.\n            skip (bool, optional): If True, prerequisites are considered\n                                   optional if they can't be found and a\n                                   debug message is logged. If False (default),\n                                   the missing prerequisites are not logged\n                                   and are expected to be handled by the\n                                   caller.\n\n        \"\"\"\n        prereq_ids = []\n        unknowns = set()\n        for prereq in prereq_names:\n            n, u = self._find_dependencies(prereq, **dfilter)\n            if u:\n                unknowns.update(u)\n                if skip:\n                    u_str = \", \".join([str(x) for x in u])\n                    LOG.debug('Skipping optional %s: Unknown dataset %s',\n                              str(prereq), u_str)\n            else:\n                prereq_ids.append(n)\n                self.add_child(parent, n)\n        return prereq_ids, unknowns", "response": "Determine the prerequisite Nodes for a composite node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the key based on the dataset it will modified.", "response": "def _update_modifier_key(self, orig_key, dep_key):\n        \"\"\"Update a key based on the dataset it will modified (dep).\n\n        Typical use case is requesting a modified dataset (orig_key). This\n        modified dataset most likely depends on a less-modified\n        dataset (dep_key). The less-modified dataset must come from a reader\n        (at least for now) or will eventually depend on a reader dataset.\n        The original request key may be limited like\n        (wavelength=0.67, modifiers=('a', 'b')) while the reader-based key\n        should have all of its properties specified. This method updates the\n        original request key so it is fully specified and should reduce the\n        chance of Node's not being unique.\n\n        \"\"\"\n        orig_dict = orig_key._asdict()\n        dep_dict = dep_key._asdict()\n        # don't change the modifiers\n        for k in DATASET_KEYS[:-1]:\n            orig_dict[k] = dep_dict[k]\n        return DatasetID.from_dict(orig_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _find_compositor(self, dataset_key, **dfilter):\n        # NOTE: This function can not find a modifier that performs\n        # one or more modifications if it has modifiers see if we can find\n        # the unmodified version first\n        src_node = None\n        if isinstance(dataset_key, DatasetID) and dataset_key.modifiers:\n            new_prereq = DatasetID(\n                *dataset_key[:-1] + (dataset_key.modifiers[:-1],))\n            src_node, u = self._find_dependencies(new_prereq, **dfilter)\n            # Update the requested DatasetID with information from the src\n            if src_node is not None:\n                dataset_key = self._update_modifier_key(dataset_key,\n                                                        src_node.name)\n            if u:\n                return None, u\n\n        try:\n            compositor = self.get_compositor(dataset_key)\n        except KeyError:\n            raise KeyError(\"Can't find anything called {}\".format(str(dataset_key)))\n        dataset_key = compositor.id\n        root = Node(dataset_key, data=(compositor, [], []))\n        if src_node is not None:\n            self.add_child(root, src_node)\n            root.data[1].append(src_node)\n\n        # 2.1 get the prerequisites\n        LOG.trace(\"Looking for composite prerequisites for: {}\".format(dataset_key))\n        prereqs, unknowns = self._get_compositor_prereqs(root, compositor.attrs['prerequisites'], **dfilter)\n        if unknowns:\n            # Should we remove all of the unknown nodes that were found\n            # if there is an unknown prerequisite are we in trouble?\n            return None, unknowns\n        root.data[1].extend(prereqs)\n\n        LOG.trace(\"Looking for optional prerequisites for: {}\".format(dataset_key))\n        optional_prereqs, _ = self._get_compositor_prereqs(\n            root, compositor.attrs['optional_prerequisites'], skip=True, **dfilter)\n        root.data[2].extend(optional_prereqs)\n\n        return root, set()", "response": "Find the compositor object for the given dataset_key."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind the dependencies for a specific dataset_key.", "response": "def _find_dependencies(self, dataset_key, **dfilter):\n        \"\"\"Find the dependencies for *dataset_key*.\n\n        Args:\n            dataset_key (str, float, DatasetID): Dataset identifier to locate\n                                                 and find any additional\n                                                 dependencies for.\n            **dfilter (dict): Additional filter parameters. See\n                              `satpy.readers.get_key` for more details.\n\n        \"\"\"\n        # 0 check if the *exact* dataset is already loaded\n        try:\n            node = self.getitem(dataset_key)\n            LOG.trace(\"Found exact dataset already loaded: {}\".format(node.name))\n            return node, set()\n        except KeyError:\n            # exact dataset isn't loaded, let's load it below\n            LOG.trace(\"Exact dataset {} isn't loaded, will try reader...\".format(dataset_key))\n\n        # 1 try to get *best* dataset from reader\n        try:\n            node = self._find_reader_dataset(dataset_key, **dfilter)\n        except TooManyResults:\n            LOG.warning(\"Too many possible datasets to load for {}\".format(dataset_key))\n            return None, set([dataset_key])\n        if node is not None:\n            LOG.trace(\"Found reader provided dataset:\\n\\tRequested: {}\\n\\tFound: {}\".format(dataset_key, node.name))\n            return node, set()\n        LOG.trace(\"Could not find dataset in reader: {}\".format(dataset_key))\n\n        # 2 try to find a composite by name (any version of it is good enough)\n        try:\n            # assume that there is no such thing as a \"better\" composite\n            # version so if we find any DatasetIDs already loaded then\n            # we want to use them\n            node = self[dataset_key]\n            LOG.trace(\"Composite already loaded:\\n\\tRequested: {}\\n\\tFound: {}\".format(dataset_key, node.name))\n            return node, set()\n        except KeyError:\n            # composite hasn't been loaded yet, let's load it below\n            LOG.trace(\"Composite hasn't been loaded yet, will load: {}\".format(dataset_key))\n\n        # 3 try to find a composite that matches\n        try:\n            node, unknowns = self._find_compositor(dataset_key, **dfilter)\n            LOG.trace(\"Found composite:\\n\\tRequested: {}\\n\\tFound: {}\".format(dataset_key, node and node.name))\n        except KeyError:\n            node = None\n            unknowns = set([dataset_key])\n            LOG.trace(\"Composite not found: {}\".format(dataset_key))\n\n        return node, unknowns"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_dependencies(self, dataset_keys, **dfilter):\n        unknown_datasets = set()\n        for key in dataset_keys.copy():\n            n, unknowns = self._find_dependencies(key, **dfilter)\n\n            dataset_keys.discard(key)  # remove old non-DatasetID\n            if n is not None:\n                dataset_keys.add(n.name)  # add equivalent DatasetID\n            if unknowns:\n                unknown_datasets.update(unknowns)\n                continue\n\n            self.add_child(self, n)\n\n        return unknown_datasets", "response": "Create the dependency tree."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\naveraging a series of datetime objects.", "response": "def average_datetimes(dt_list):\n    \"\"\"Average a series of datetime objects.\n\n    .. note::\n\n        This function assumes all datetime objects are naive and in the same\n        time zone (UTC).\n\n    Args:\n        dt_list (iterable): Datetime objects to average\n\n    Returns: Average datetime as a datetime object\n\n    \"\"\"\n    if sys.version_info < (3, 3):\n        # timestamp added in python 3.3\n        import time\n\n        def timestamp_func(dt):\n            return time.mktime(dt.timetuple())\n    else:\n        timestamp_func = datetime.timestamp\n\n    total = [timestamp_func(dt) for dt in dt_list]\n    return datetime.fromtimestamp(sum(total) / len(total))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncombining the metadata of two or more Datasets.", "response": "def combine_metadata(*metadata_objects, **kwargs):\n    \"\"\"Combine the metadata of two or more Datasets.\n\n    If any keys are not equal or do not exist in all provided dictionaries\n    then they are not included in the returned dictionary.\n    By default any keys with the word 'time' in them and consisting\n    of datetime objects will be averaged. This is to handle cases where\n    data were observed at almost the same time but not exactly.\n\n    Args:\n        *metadata_objects: MetadataObject or dict objects to combine\n        average_times (bool): Average any keys with 'time' in the name\n\n    Returns:\n        dict: the combined metadata\n\n    \"\"\"\n    average_times = kwargs.get('average_times', True)  # python 2 compatibility (no kwarg after *args)\n    shared_keys = None\n    info_dicts = []\n    # grab all of the dictionary objects provided and make a set of the shared keys\n    for metadata_object in metadata_objects:\n        if isinstance(metadata_object, dict):\n            metadata_dict = metadata_object\n        elif hasattr(metadata_object, \"attrs\"):\n            metadata_dict = metadata_object.attrs\n        else:\n            continue\n        info_dicts.append(metadata_dict)\n\n        if shared_keys is None:\n            shared_keys = set(metadata_dict.keys())\n        else:\n            shared_keys &= set(metadata_dict.keys())\n\n    # combine all of the dictionaries\n    shared_info = {}\n    for k in shared_keys:\n        values = [nfo[k] for nfo in info_dicts]\n        any_arrays = any([isinstance(val, np.ndarray) for val in values])\n        if any_arrays:\n            if all(np.all(val == values[0]) for val in values[1:]):\n                shared_info[k] = values[0]\n        elif 'time' in k and isinstance(values[0], datetime) and average_times:\n            shared_info[k] = average_datetimes(values)\n        elif all(val == values[0] for val in values[1:]):\n            shared_info[k] = values[0]\n\n    return shared_info"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dataset_walker(datasets):\n    for dataset in datasets:\n        yield dataset, None\n        for anc_ds in dataset.attrs.get('ancillary_variables', []):\n            try:\n                anc_ds.attrs\n                yield anc_ds, dataset\n            except AttributeError:\n                continue", "response": "Yields datasets and ancillary data."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreplace dataset the parent_dataset s ancillary_variables field.", "response": "def replace_anc(dataset, parent_dataset):\n    \"\"\"Replace *dataset* the *parent_dataset*'s `ancillary_variables` field.\"\"\"\n    if parent_dataset is None:\n        return\n    current_dsid = DatasetID.from_dict(dataset.attrs)\n    for idx, ds in enumerate(parent_dataset.attrs['ancillary_variables']):\n        if current_dsid == DatasetID.from_dict(ds.attrs):\n            parent_dataset.attrs['ancillary_variables'][idx] = dataset\n            return"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wavelength_match(a, b):\n        if type(a) == (type(b) or\n                       isinstance(a, numbers.Number) and\n                       isinstance(b, numbers.Number)):\n            return a == b\n        elif a is None or b is None:\n            return False\n        elif isinstance(a, (list, tuple)) and len(a) == 3:\n            return a[0] <= b <= a[2]\n        elif isinstance(b, (list, tuple)) and len(b) == 3:\n            return b[0] <= a <= b[2]\n        else:\n            raise ValueError(\"Can only compare wavelengths of length 1 or 3\")", "response": "Return True if two wavelengths are equal."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _comparable(self):\n        return self._replace(\n            name='' if self.name is None else self.name,\n            wavelength=tuple() if self.wavelength is None else self.wavelength,\n            resolution=0 if self.resolution is None else self.resolution,\n            polarization='' if self.polarization is None else self.polarization,\n            calibration='' if self.calibration is None else self.calibration,\n        )", "response": "Return a comparable version of the DatasetID."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_dict(cls, d, **kwargs):\n        args = []\n        for k in DATASET_KEYS:\n            val = kwargs.get(k, d.get(k))\n            # force modifiers to tuple\n            if k == 'modifiers' and val is not None:\n                val = tuple(val)\n            args.append(val)\n\n        return cls(*args)", "response": "Convert a dict to an ID."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_dict(self, trim=True):\n        if trim:\n            return self._to_trimmed_dict()\n        else:\n            return dict(zip(DATASET_KEYS, self))", "response": "Convert the ID to a dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_dataset(self, key, info):\n\n        logger.debug('Reading %s.', key.name)\n\n        try:\n            variable = self.nc[info['file_key']]\n        except KeyError:\n            return\n\n        info.update(variable.attrs)\n\n        variable.attrs = info\n        return variable", "response": "Load a dataset from the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_dataset(self, key, info):\n        logger.debug('Reading %s.', key.name)\n\n        # Check if file_key is specified in the yaml\n        file_key = info.get('file_key', key.name)\n\n        variable = self.nc[file_key]\n\n        l_step = self.nc.attrs.get('al_subsampling_factor', 1)\n        c_step = self.nc.attrs.get('ac_subsampling_factor', 16)\n\n        if c_step != 1 or l_step != 1:\n            logger.debug('Interpolating %s.', key.name)\n\n            # TODO: do it in cartesian coordinates ! pbs at date line and\n            # possible\n            tie_x = self.cartx['x_tx'].data[0, :][::-1]\n            tie_y = self.cartx['y_tx'].data[:, 0]\n            full_x = self.cart['x_i' + self.view].data\n            full_y = self.cart['y_i' + self.view].data\n\n            variable = variable.fillna(0)\n\n            from scipy.interpolate import RectBivariateSpline\n            spl = RectBivariateSpline(\n                tie_y, tie_x, variable.data[:, ::-1])\n\n            valid = np.isfinite(full_y)\n\n            interpolated = spl.ev(full_y[valid],\n                                  full_x[valid])\n\n            values = np.full_like(full_y, np.nan,\n                                  dtype=variable.dtype)\n\n            values[valid] = interpolated\n            values = np.ma.masked_invalid(values, copy=False)\n\n            variable = xr.DataArray(da.from_array(values, chunks=(CHUNK_SIZE, CHUNK_SIZE)),\n                                    dims=['y', 'x'], attrs=variable.attrs)\n\n        variable.attrs['platform_name'] = self.platform_name\n        variable.attrs['sensor'] = self.sensor\n\n        if 'units' not in variable.attrs:\n            variable.attrs['units'] = 'degrees'\n\n        variable.attrs.update(key.to_dict())\n\n        return variable", "response": "Load a dataset from the file and the attributes of the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a dataset from the file.", "response": "def get_dataset(self, key, info, out=None):\n        \"\"\"Get a dataset from the file.\"\"\"\n\n        logger.debug(\"Reading %s.\", key.name)\n        values = self.file_content[key.name]\n        selected = np.array(self.selected)\n        if key.name in (\"Latitude\", \"Longitude\"):\n            values = values / 10000.\n        if key.name in ('Tsurf', 'CloudTopPres', 'CloudTopTemp'):\n            goods = values > -9998.\n            selected = np.array(selected & goods)\n            if key.name in ('Tsurf', \"Alt_surface\", \"CloudTopTemp\"):\n                values = values / 100.\n            if key.name in (\"CloudTopPres\"):\n                values = values / 10.\n        else:\n            selected = self.selected\n        info.update(self.finfo)\n\n        fill_value = np.nan\n\n        if key.name == 'ct':\n            fill_value = 0\n            info['_FillValue'] = 0\n        ds = DataArray(values, dims=['y', 'x'], attrs=info).where(selected, fill_value)\n\n        # update dataset info with file_info\n        return ds"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sensor_names(self):\n        res = self['/attr/instrument_name']\n        if isinstance(res, np.ndarray):\n            res = str(res.astype(str))\n        res = [x.strip() for x in res.split(',')]\n        if len(res) == 1:\n            return res[0]\n        return res", "response": "Return standard sensor or instrument name for the file s data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_shape(self, ds_id, ds_info):\n        var_path = ds_info.get('file_key', '{}'.format(ds_id.name))\n        if var_path + '/shape' not in self:\n            # loading a scalar value\n            shape = 1\n        else:\n            shape = self[var_path + \"/shape\"]\n            if \"index\" in ds_info:\n                shape = shape[1:]\n            if \"pressure_index\" in ds_info:\n                shape = shape[:-1]\n        return shape", "response": "Return data array shape for item specified."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_dataset(self, dataset_id, ds_info):\n        var_path = ds_info.get('file_key', '{}'.format(dataset_id.name))\n        metadata = self.get_metadata(dataset_id, ds_info)\n        valid_min, valid_max = self[var_path + '/attr/valid_range']\n        fill_value = self.get(var_path + '/attr/_FillValue')\n\n        d_tmp = self[var_path]\n        if \"index\" in ds_info:\n            d_tmp = d_tmp[int(ds_info[\"index\"])]\n        if \"pressure_index\" in ds_info:\n            d_tmp = d_tmp[..., int(ds_info[\"pressure_index\"])]\n            # this is a pressure based field\n            # include surface_pressure as metadata\n            sp = self['Surface_Pressure']\n            if 'surface_pressure' in ds_info:\n                ds_info['surface_pressure'] = xr.concat((ds_info['surface_pressure'], sp))\n            else:\n                ds_info['surface_pressure'] = sp\n            # include all the pressure levels\n            ds_info.setdefault('pressure_levels', self['Pressure'][0])\n        data = d_tmp\n\n        if valid_min is not None and valid_max is not None:\n            # the original .cfg/INI based reader only checked valid_max\n            data = data.where((data <= valid_max))  # | (data >= valid_min))\n        if fill_value is not None:\n            data = data.where(data != fill_value)\n\n        data.attrs.update(metadata)\n        return data", "response": "Load data array and metadata for specified dataset"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_ds_ids_from_config(self):\n        super(NUCAPSReader, self).load_ds_ids_from_config()\n        for ds_id in list(self.ids.keys()):\n            ds_info = self.ids[ds_id]\n            if ds_info.get('pressure_based', False):\n                for idx, lvl_num in enumerate(ALL_PRESSURE_LEVELS):\n                    if lvl_num < 5.0:\n                        suffix = \"_{:0.03f}mb\".format(lvl_num)\n                    else:\n                        suffix = \"_{:0.0f}mb\".format(lvl_num)\n\n                    new_info = ds_info.copy()\n                    new_info['pressure_level'] = lvl_num\n                    new_info['pressure_index'] = idx\n                    new_info['file_key'] = '{}'.format(ds_id.name)\n                    new_info['name'] = ds_id.name + suffix\n                    new_ds_id = ds_id._replace(name=new_info['name'])\n                    new_info['id'] = new_ds_id\n                    self.ids[new_ds_id] = new_info\n                    self.pressure_dataset_names[ds_id.name].append(new_info['name'])", "response": "Convert config dataset entries to DatasetIDs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load(self, dataset_keys, previous_datasets=None, pressure_levels=None):\n        dataset_keys = set(self.get_dataset_key(x) for x in dataset_keys)\n        if pressure_levels is not None:\n            # Filter out datasets that don't fit in the correct pressure level\n            for ds_id in dataset_keys.copy():\n                ds_info = self.ids[ds_id]\n                ds_level = ds_info.get(\"pressure_level\")\n                if ds_level is not None:\n                    if pressure_levels is True:\n                        # they want all pressure levels\n                        continue\n                    elif len(pressure_levels) == 2 and pressure_levels[0] <= ds_level <= pressure_levels[1]:\n                        # given a min and a max pressure level\n                        continue\n                    elif np.isclose(pressure_levels, ds_level).any():\n                        # they asked for this specific pressure level\n                        continue\n                    else:\n                        # they don't want this dataset at this pressure level\n                        LOG.debug(\"Removing dataset to load: %s\", ds_id)\n                        dataset_keys.remove(ds_id)\n                        continue\n\n            # Add pressure levels to the datasets to load if needed so\n            # we can do further filtering after loading\n            plevels_ds_id = self.get_dataset_key('Pressure_Levels')\n            remove_plevels = False\n            if plevels_ds_id not in dataset_keys:\n                dataset_keys.add(plevels_ds_id)\n                remove_plevels = True\n\n        datasets_loaded = super(NUCAPSReader, self).load(\n            dataset_keys, previous_datasets=previous_datasets)\n\n        if pressure_levels is not None:\n            if remove_plevels:\n                plevels_ds = datasets_loaded.pop(plevels_ds_id)\n                dataset_keys.remove(plevels_ds_id)\n            else:\n                plevels_ds = datasets_loaded[plevels_ds_id]\n\n            if pressure_levels is True:\n                cond = None\n            elif len(pressure_levels) == 2:\n                cond = (plevels_ds >= pressure_levels[0]) & (plevels_ds <= pressure_levels[1])\n            else:\n                cond = plevels_ds == pressure_levels\n            if cond is not None:\n                new_plevels = plevels_ds.where(cond, drop=True)\n            else:\n                new_plevels = plevels_ds\n\n            for ds_id in datasets_loaded.keys():\n                ds_obj = datasets_loaded[ds_id]\n                if plevels_ds.dims[0] not in ds_obj.dims:\n                    continue\n\n                if cond is not None:\n                    datasets_loaded[ds_id] = ds_obj.where(cond, drop=True)\n                datasets_loaded[ds_id].attrs['pressure_levels'] = new_plevels\n\n        if self.mask_surface:\n            LOG.debug(\"Filtering pressure levels at or below the surface pressure\")\n            for ds_id in sorted(dataset_keys):\n                ds = datasets_loaded[ds_id]\n                if \"surface_pressure\" not in ds.attrs or \"pressure_levels\" not in ds.attrs:\n                    continue\n                data_pressure = ds.attrs[\"pressure_levels\"]\n                surface_pressure = ds.attrs[\"surface_pressure\"]\n                if isinstance(surface_pressure, float):\n                    # scalar needs to become array for each record\n                    surface_pressure = np.repeat(surface_pressure, ds.shape[0])\n                if surface_pressure.ndim == 1 and surface_pressure.shape[0] == ds.shape[0]:\n                    # surface is one element per record\n                    LOG.debug(\"Filtering %s at and below the surface pressure\", ds_id)\n                    if ds.ndim == 2:\n                        surface_pressure = np.repeat(surface_pressure[:, None], data_pressure.shape[0], axis=1)\n                        data_pressure = np.repeat(data_pressure[None, :], surface_pressure.shape[0], axis=0)\n                        datasets_loaded[ds_id] = ds.where(data_pressure < surface_pressure)\n                    else:\n                        # entire dataset represents one pressure level\n                        data_pressure = ds.attrs[\"pressure_level\"]\n                        datasets_loaded[ds_id] = ds.where(data_pressure < surface_pressure)\n                else:\n                    LOG.warning(\"Not sure how to handle shape of 'surface_pressure' metadata\")\n\n        if self.mask_quality:\n            LOG.debug(\"Filtering data based on quality flags\")\n            for ds_id in sorted(dataset_keys):\n                ds = datasets_loaded[ds_id]\n                quality_flag = [\n                    x for x in ds.attrs.get('ancillary_variables', [])\n                    if x.attrs.get('name') == 'Quality_Flag']\n                if not quality_flag:\n                    continue\n\n                quality_flag = quality_flag[0]\n                if quality_flag.dims[0] not in ds.dims:\n                    continue\n                LOG.debug(\"Masking %s where quality flag doesn't equal 1\", ds_id)\n                datasets_loaded[ds_id] = ds.where(quality_flag == 0)\n\n        return datasets_loaded", "response": "Load data from one or more set of files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef scheduled_time(self):\n        timeline = \"{:04d}\".format(self.basic_info['observation_timeline'][0])\n        return self.start_time.replace(hour=int(timeline[:2]), minute=int(timeline[2:4]), second=0, microsecond=0)", "response": "Time this band was scheduled to be recorded."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _check_fpos(self, fp_, fpos, offset, block):\n        if (fp_.tell() + offset != fpos):\n            warnings.warn(\"Actual \"+block+\" header size does not match expected\")\n        return", "response": "Check file position matches blocksize"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read_header(self, fp_):\n        header = {}\n\n        fpos = 0\n        header['block1'] = np.fromfile(\n            fp_, dtype=_BASIC_INFO_TYPE, count=1)\n        fpos = fpos + int(header['block1']['blocklength'])\n        self._check_fpos(fp_, fpos, 0, 'block1')\n        fp_.seek(fpos, 0)\n        header[\"block2\"] = np.fromfile(fp_, dtype=_DATA_INFO_TYPE, count=1)\n        fpos = fpos + int(header['block2']['blocklength'])\n        self._check_fpos(fp_, fpos, 0, 'block2')\n        fp_.seek(fpos, 0)\n        header[\"block3\"] = np.fromfile(fp_, dtype=_PROJ_INFO_TYPE, count=1)\n        fpos = fpos + int(header['block3']['blocklength'])\n        self._check_fpos(fp_, fpos, 0, 'block3')\n        fp_.seek(fpos, 0)\n        header[\"block4\"] = np.fromfile(fp_, dtype=_NAV_INFO_TYPE, count=1)\n        fpos = fpos + int(header['block4']['blocklength'])\n        self._check_fpos(fp_, fpos, 0, 'block4')\n        fp_.seek(fpos, 0)\n        header[\"block5\"] = np.fromfile(fp_, dtype=_CAL_INFO_TYPE, count=1)\n        logger.debug(\"Band number = \" +\n                     str(header[\"block5\"]['band_number'][0]))\n        logger.debug('Time_interval: %s - %s',\n                     str(self.start_time), str(self.end_time))\n        band_number = header[\"block5\"]['band_number'][0]\n        if band_number < 7:\n            cal = np.fromfile(fp_, dtype=_VISCAL_INFO_TYPE, count=1)\n        else:\n            cal = np.fromfile(fp_, dtype=_IRCAL_INFO_TYPE, count=1)\n        fpos = fpos + int(header['block5']['blocklength'])\n        self._check_fpos(fp_, fpos, 0, 'block5')\n        fp_.seek(fpos, 0)\n\n        header['calibration'] = cal\n\n        header[\"block6\"] = np.fromfile(\n            fp_, dtype=_INTER_CALIBRATION_INFO_TYPE, count=1)\n        fpos = fpos + int(header['block6']['blocklength'])\n        self._check_fpos(fp_, fpos, 0, 'block6')\n        fp_.seek(fpos, 0)\n        header[\"block7\"] = np.fromfile(\n            fp_, dtype=_SEGMENT_INFO_TYPE, count=1)\n        fpos = fpos + int(header['block7']['blocklength'])\n        self._check_fpos(fp_, fpos, 0, 'block7')\n        fp_.seek(fpos, 0)\n        header[\"block8\"] = np.fromfile(\n            fp_, dtype=_NAVIGATION_CORRECTION_INFO_TYPE, count=1)\n        # 8 The navigation corrections:\n        ncorrs = header[\"block8\"]['numof_correction_info_data'][0]\n        dtype = np.dtype([\n            (\"line_number_after_rotation\", \"<u2\"),\n            (\"shift_amount_for_column_direction\", \"f4\"),\n            (\"shift_amount_for_line_direction\", \"f4\"),\n        ])\n        corrections = []\n        for i in range(ncorrs):\n            corrections.append(np.fromfile(fp_, dtype=dtype, count=1))\n        fpos = fpos + int(header['block8']['blocklength'])\n        self._check_fpos(fp_, fpos, 40, 'block8')\n        fp_.seek(fpos, 0)\n        header['navigation_corrections'] = corrections\n        header[\"block9\"] = np.fromfile(fp_,\n                                       dtype=_OBS_TIME_INFO_TYPE,\n                                       count=1)\n        numobstimes = header[\"block9\"]['number_of_observation_times'][0]\n\n        dtype = np.dtype([\n            (\"line_number\", \"<u2\"),\n            (\"observation_time\", \"f8\"),\n        ])\n        lines_and_times = []\n        for i in range(numobstimes):\n            lines_and_times.append(np.fromfile(fp_,\n                                               dtype=dtype,\n                                               count=1))\n        header['observation_time_information'] = lines_and_times\n        fpos = fpos + int(header['block9']['blocklength'])\n        self._check_fpos(fp_, fpos, 40, 'block9')\n        fp_.seek(fpos, 0)\n\n        header[\"block10\"] = np.fromfile(fp_,\n                                        dtype=_ERROR_INFO_TYPE,\n                                        count=1)\n        dtype = np.dtype([\n            (\"line_number\", \"<u2\"),\n            (\"numof_error_pixels_per_line\", \"<u2\"),\n        ])\n        num_err_info_data = header[\"block10\"][\n            'number_of_error_info_data'][0]\n        err_info_data = []\n        for i in range(num_err_info_data):\n            err_info_data.append(np.fromfile(fp_, dtype=dtype, count=1))\n        header['error_information_data'] = err_info_data\n        fpos = fpos + int(header['block10']['blocklength'])\n        self._check_fpos(fp_, fpos, 40, 'block10')\n        fp_.seek(fpos, 0)\n\n        header[\"block11\"] = np.fromfile(fp_, dtype=_SPARE_TYPE, count=1)\n        fpos = fpos + int(header['block11']['blocklength'])\n        self._check_fpos(fp_, fpos, 0, 'block11')\n        fp_.seek(fpos, 0)\n\n        return header", "response": "Read the header of the log entry."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread data block from file", "response": "def _read_data(self, fp_, header):\n        \"\"\"Read data block\"\"\"\n        nlines = int(header[\"block2\"]['number_of_lines'][0])\n        ncols = int(header[\"block2\"]['number_of_columns'][0])\n        return da.from_array(np.memmap(self.filename, offset=fp_.tell(),\n                                       dtype='<u2', shape=(nlines, ncols), mode='r'),\n                             chunks=CHUNK_SIZE)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading the data for the given key.", "response": "def read_band(self, key, info):\n        \"\"\"Read the data.\"\"\"\n        # Read data\n        tic = datetime.now()\n        with open(self.filename, \"rb\") as fp_:\n            header = self._read_header(fp_)\n            res = self._read_data(fp_, header)\n        res = self._mask_invalid(data=res, header=header)\n        self._header = header\n        logger.debug(\"Reading time \" + str(datetime.now() - tic))\n\n        # Calibrate\n        res = self.calibrate(res, key.calibration)\n\n        # Update metadata\n        new_info = dict(units=info['units'],\n                        standard_name=info['standard_name'],\n                        wavelength=info['wavelength'],\n                        resolution='resolution',\n                        id=key,\n                        name=key.name,\n                        scheduled_time=self.scheduled_time,\n                        platform_name=self.platform_name,\n                        sensor=self.sensor,\n                        satellite_longitude=float(\n                            self.nav_info['SSP_longitude']),\n                        satellite_latitude=float(\n                            self.nav_info['SSP_latitude']),\n                        satellite_altitude=float(self.nav_info['distance_earth_center_to_satellite'] -\n                                                 self.proj_info['earth_equatorial_radius']) * 1000)\n        res = xr.DataArray(res, attrs=new_info, dims=['y', 'x'])\n\n        # Mask space pixels\n        if self.mask_space:\n            res = self._mask_space(res)\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_field(elt, ascii=False):\n\n    # NOTE: if there is a variable defined in this field and it is different\n    # from the default, we could change the value and restart.\n\n    scale = np.uint8(1)\n    if elt.get(\"type\") == \"bitfield\" and not ascii:\n        current_type = \">u\" + str(int(elt.get(\"length\")) // 8)\n        scale = np.dtype(current_type).type(1)\n    elif (elt.get(\"length\") is not None):\n        if ascii:\n            add = 33\n        else:\n            add = 0\n        current_type = \"S\" + str(int(elt.get(\"length\")) + add)\n    else:\n        current_type = TYPEC[elt.get(\"type\")]\n        try:\n            scale = (10 /\n                     float(elt.get(\"scaling-factor\", \"10\").replace(\"^\", \"e\")))\n        except ValueError:\n            scale = (10 / np.array(\n                elt.get(\"scaling-factor\").replace(\"^\", \"e\").split(\",\"),\n                dtype=np.float))\n\n    return ((elt.get(\"name\"), current_type, scale))", "response": "Process a field tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_array(elt, ascii=False):\n    del ascii\n    chld = elt.getchildren()\n    if len(chld) > 1:\n        raise ValueError()\n    chld = chld[0]\n    try:\n        name, current_type, scale = CASES[chld.tag](chld)\n        size = None\n    except ValueError:\n        name, current_type, size, scale = CASES[chld.tag](chld)\n    del name\n    myname = elt.get(\"name\") or elt.get(\"label\")\n    if elt.get(\"length\").startswith(\"$\"):\n        length = int(VARIABLES[elt.get(\"length\")[1:]])\n    else:\n        length = int(elt.get(\"length\"))\n    if size is not None:\n        return (myname, current_type, (length, ) + size, scale)\n    else:\n        return (myname, current_type, (length, ), scale)", "response": "Process an array tag."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a list of strings to return a dtype.", "response": "def to_scaled_dtype(val):\n    \"\"\"Parse *val* to return a dtype.\n    \"\"\"\n    res = []\n    for i in val:\n        if i[1].startswith(\"S\"):\n            res.append((i[0], i[1]) + i[2:-1])\n        else:\n            try:\n                res.append((i[0], i[-1].dtype) + i[2:-1])\n            except AttributeError:\n                res.append((i[0], type(i[-1])) + i[2:-1])\n\n    return np.dtype(res)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a list of scale factors.", "response": "def to_scales(val):\n    \"\"\"Parse *val* to return an array of scale factors.\n    \"\"\"\n    res = []\n    for i in val:\n        if len(i) == 3:\n            res.append((i[0], type(i[2])))\n        else:\n            try:\n                res.append((i[0], i[3].dtype, i[2]))\n            except AttributeError:\n                res.append((i[0], type(i[3]), i[2]))\n\n    dtype = np.dtype(res)\n\n    scales = np.zeros((1, ), dtype=dtype)\n\n    for i in val:\n        try:\n            scales[i[0]] = i[-1]\n        except ValueError:\n            scales[i[0]] = np.repeat(np.array(i[-1]), i[2][1]).reshape(i[2])\n\n    return scales"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the xml file to create types scaling factor types and scales.", "response": "def parse_format(xml_file):\n    \"\"\"Parse the xml file to create types, scaling factor types, and scales.\n    \"\"\"\n    tree = ElementTree()\n    tree.parse(xml_file)\n\n    for param in tree.find(\"parameters\").getchildren():\n        VARIABLES[param.get(\"name\")] = param.get(\"value\")\n\n    types_scales = {}\n\n    for prod in tree.find(\"product\"):\n        ascii = (prod.tag in [\"mphr\", \"sphr\"])\n        res = []\n        for i in prod:\n            lres = CASES[i.tag](i, ascii)\n            if lres is not None:\n                res.append(lres)\n        types_scales[(prod.tag, int(prod.get(\"subclass\")))] = res\n\n    types = {}\n    stypes = {}\n    scales = {}\n\n    for key, val in types_scales.items():\n        types[key] = to_dtype(val)\n        stypes[key] = to_scaled_dtype(val)\n        scales[key] = to_scales(val)\n\n    return types, stypes, scales"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _apply_scales(array, scales, dtype):\n    new_array = np.empty(array.shape, dtype)\n    for i in array.dtype.names:\n        try:\n            new_array[i] = array[i] * scales[i]\n        except TypeError:\n            if np.all(scales[i] == 1):\n                new_array[i] = array[i]\n            else:\n                raise\n    return new_array", "response": "Apply scales to the array."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses coefficients from the page.", "response": "def get_coeffs(page):\n    '''Parse coefficients from the page.'''\n    coeffs = {}\n    coeffs['datetime'] = []\n    coeffs['slope1'] = []\n    coeffs['intercept1'] = []\n    coeffs['slope2'] = []\n    coeffs['intercept2'] = []\n\n    slope1_idx, intercept1_idx, slope2_idx, intercept2_idx = \\\n        None, None, None, None\n\n    date_idx = 0\n    for row in page.lower().split('\\n'):\n        row = row.split()\n        if len(row) == 0:\n            continue\n        if row[0] == 'update':\n            # Get the column indices from the header line\n            slope1_idx = row.index('slope_lo')\n            intercept1_idx = row.index('int_lo')\n            slope2_idx = row.index('slope_hi')\n            intercept2_idx = row.index('int_hi')\n            continue\n\n        if slope1_idx is None:\n            continue\n\n        # In some cases the fields are connected, skip those rows\n        if max([slope1_idx, intercept1_idx,\n                slope2_idx, intercept2_idx]) >= len(row):\n            continue\n\n        try:\n            dat = dt.datetime.strptime(row[date_idx], \"%m/%d/%Y\")\n        except ValueError:\n            continue\n\n        coeffs['datetime'].append([dat.year, dat.month, dat.day])\n        coeffs['slope1'].append(float(row[slope1_idx]))\n        coeffs['intercept1'].append(float(row[intercept1_idx]))\n        coeffs['slope2'].append(float(row[slope2_idx]))\n        coeffs['intercept2'].append(float(row[intercept2_idx]))\n\n    return coeffs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_all_coeffs():\n    '''Get all available calibration coefficients for the satellites.'''\n    coeffs = {}\n\n    for platform in URLS.keys():\n        if platform not in coeffs:\n            coeffs[platform] = {}\n        for chan in URLS[platform].keys():\n            url = URLS[platform][chan]\n            print url\n            page = get_page(url)\n            coeffs[platform][chan] = get_coeffs(page)\n\n    return coeffs", "response": "Get all available calibration coefficients for the satellites."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving calibration coefficients to HDF5 files.", "response": "def save_coeffs(coeffs, out_dir=''):\n    '''Save calibration coefficients to HDF5 files.'''\n    for platform in coeffs.keys():\n        fname = os.path.join(out_dir, \"%s_calibration_data.h5\" % platform)\n        fid = h5py.File(fname, 'w')\n        \n        for chan in coeffs[platform].keys():\n            fid.create_group(chan)\n            fid[chan]['datetime'] = coeffs[platform][chan]['datetime']\n            fid[chan]['slope1'] = coeffs[platform][chan]['slope1']\n            fid[chan]['intercept1'] = coeffs[platform][chan]['intercept1']\n            fid[chan]['slope2'] = coeffs[platform][chan]['slope2']\n            fid[chan]['intercept2'] = coeffs[platform][chan]['intercept2']\n\n        fid.close()\n        print \"Calibration coefficients saved for %s\" % platform"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating calibration coefficient files for AVHRR", "response": "def main():\n    '''Create calibration coefficient files for AVHRR'''\n    out_dir = sys.argv[1]\n    coeffs = get_all_coeffs()\n    save_coeffs(coeffs, out_dir=out_dir)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves empty objects from the variable s attrs.", "response": "def remove_empties(variable):\n    \"\"\"Remove empty objects from the *variable*'s attrs.\"\"\"\n    import h5py\n    for key, val in variable.attrs.items():\n        if isinstance(val, h5py._hl.base.Empty):\n            variable.attrs.pop(key)\n\n    return variable"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves time dimension from dataset", "response": "def remove_timedim(self, var):\n        \"\"\"Remove time dimension from dataset\"\"\"\n        if self.pps and var.dims[0] == 'time':\n            data = var[0, :, :]\n            data.attrs = var.attrs\n            var = data\n\n        return var"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload a dataset from the cache.", "response": "def get_dataset(self, dsid, info):\n        \"\"\"Load a dataset.\"\"\"\n\n        dsid_name = dsid.name\n        if dsid_name in self.cache:\n            logger.debug('Get the data set from cache: %s.', dsid_name)\n            return self.cache[dsid_name]\n        if dsid_name in ['lon', 'lat'] and dsid_name not in self.nc:\n            dsid_name = dsid_name + '_reduced'\n\n        logger.debug('Reading %s.', dsid_name)\n        variable = self.nc[dsid_name]\n        variable = self.remove_timedim(variable)\n        variable = self.scale_dataset(dsid, variable, info)\n\n        if dsid_name.endswith('_reduced'):\n            # Get full resolution lon,lat from the reduced (tie points) grid\n            self.upsample_geolocation(dsid, info)\n\n            return self.cache[dsid.name]\n\n        return variable"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nscale the data set according to the attributes from the netCDF file", "response": "def scale_dataset(self, dsid, variable, info):\n        \"\"\"Scale the data set, applying the attributes from the netCDF file\"\"\"\n\n        variable = remove_empties(variable)\n        scale = variable.attrs.get('scale_factor', np.array(1))\n        offset = variable.attrs.get('add_offset', np.array(0))\n        if np.issubdtype((scale + offset).dtype, np.floating) or np.issubdtype(variable.dtype, np.floating):\n            if '_FillValue' in variable.attrs:\n                variable = variable.where(\n                    variable != variable.attrs['_FillValue'])\n                variable.attrs['_FillValue'] = np.nan\n            if 'valid_range' in variable.attrs:\n                variable = variable.where(\n                    variable <= variable.attrs['valid_range'][1])\n                variable = variable.where(\n                    variable >= variable.attrs['valid_range'][0])\n            if 'valid_max' in variable.attrs:\n                variable = variable.where(\n                    variable <= variable.attrs['valid_max'])\n            if 'valid_min' in variable.attrs:\n                variable = variable.where(\n                    variable >= variable.attrs['valid_min'])\n        attrs = variable.attrs\n        variable = variable * scale + offset\n        variable.attrs = attrs\n\n        variable.attrs.update({'platform_name': self.platform_name,\n                               'sensor': self.sensor})\n\n        variable.attrs.setdefault('units', '1')\n\n        ancillary_names = variable.attrs.get('ancillary_variables', '')\n        try:\n            variable.attrs['ancillary_variables'] = ancillary_names.split()\n        except AttributeError:\n            pass\n\n        if 'palette_meanings' in variable.attrs:\n            variable.attrs['palette_meanings'] = [int(val)\n                                                  for val in variable.attrs['palette_meanings'].split()]\n            if variable.attrs['palette_meanings'][0] == 1:\n                variable.attrs['palette_meanings'] = [0] + variable.attrs['palette_meanings']\n                variable = xr.DataArray(da.vstack((np.array(variable.attrs['fill_value_color']), variable.data)),\n                                        coords=variable.coords, dims=variable.dims, attrs=variable.attrs)\n\n            val, idx = np.unique(variable.attrs['palette_meanings'], return_index=True)\n            variable.attrs['palette_meanings'] = val\n            variable = variable[idx]\n\n        if 'standard_name' in info:\n            variable.attrs.setdefault('standard_name', info['standard_name'])\n        if self.sw_version == 'NWC/PPS version v2014' and dsid.name == 'ctth_alti':\n            # pps 2014 valid range and palette don't match\n            variable.attrs['valid_range'] = (0., 9000.)\n        if self.sw_version == 'NWC/PPS version v2014' and dsid.name == 'ctth_alti_pal':\n            # pps 2014 palette has the nodata color (black) first\n            variable = variable[1:, :]\n\n        return variable"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef upsample_geolocation(self, dsid, info):\n        from geotiepoints import SatelliteInterpolator\n        # Read the fields needed:\n        col_indices = self.nc['nx_reduced'].values\n        row_indices = self.nc['ny_reduced'].values\n        lat_reduced = self.scale_dataset(dsid, self.nc['lat_reduced'], info)\n        lon_reduced = self.scale_dataset(dsid, self.nc['lon_reduced'], info)\n\n        shape = (self.nc['y'].shape[0], self.nc['x'].shape[0])\n        cols_full = np.arange(shape[1])\n        rows_full = np.arange(shape[0])\n\n        satint = SatelliteInterpolator((lon_reduced.values, lat_reduced.values),\n                                       (row_indices,\n                                        col_indices),\n                                       (rows_full, cols_full))\n\n        lons, lats = satint.interpolate()\n        self.cache['lon'] = xr.DataArray(lons, attrs=lon_reduced.attrs, dims=['y', 'x'])\n        self.cache['lat'] = xr.DataArray(lats, attrs=lat_reduced.attrs, dims=['y', 'x'])\n\n        return", "response": "Upsample the geolocation from the tiepoint grid"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the area definition of the datasets in the file. Only applicable for MSG products!", "response": "def get_area_def(self, dsid):\n        \"\"\"Get the area definition of the datasets in the file.\n\n        Only applicable for MSG products!\n        \"\"\"\n        if self.pps:\n            # PPS:\n            raise NotImplementedError\n\n        if dsid.name.endswith('_pal'):\n            raise NotImplementedError\n\n        proj_str, area_extent = self._get_projection()\n\n        nlines, ncols = self.nc[dsid.name].shape\n\n        area = get_area_def('some_area_name',\n                            \"On-the-fly area\",\n                            'geosmsg',\n                            proj_str,\n                            ncols,\n                            nlines,\n                            area_extent)\n\n        return area"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef end_time(self):\n        try:\n            # MSG:\n            try:\n                return datetime.strptime(self.nc.attrs['time_coverage_end'],\n                                         '%Y-%m-%dT%H:%M:%SZ')\n            except TypeError:\n                return datetime.strptime(self.nc.attrs['time_coverage_end'].astype(str),\n                                         '%Y-%m-%dT%H:%M:%SZ')\n        except ValueError:\n            # PPS:\n            return datetime.strptime(self.nc.attrs['time_coverage_end'],\n                                     '%Y%m%dT%H%M%S%fZ')", "response": "Return the end time of the object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the projection from the NetCDF4 attributes", "response": "def _get_projection(self):\n        \"\"\"Get projection from the NetCDF4 attributes\"\"\"\n        try:\n            proj_str = self.nc.attrs['gdal_projection']\n        except TypeError:\n            proj_str = self.nc.attrs['gdal_projection'].decode()\n\n        # Check the a/b/h units\n        radius_a = proj_str.split('+a=')[-1].split()[0]\n        if float(radius_a) > 10e3:\n            units = 'm'\n            scale = 1.0\n        else:\n            units = 'km'\n            scale = 1e3\n\n        if 'units' not in proj_str:\n            proj_str = proj_str + ' +units=' + units\n\n        area_extent = (float(self.nc.attrs['gdal_xgeo_up_left']) / scale,\n                       float(self.nc.attrs['gdal_ygeo_low_right']) / scale,\n                       float(self.nc.attrs['gdal_xgeo_low_right']) / scale,\n                       float(self.nc.attrs['gdal_ygeo_up_left']) / scale)\n\n        return proj_str, area_extent"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_geotiff_area_def(filename, crs):\n    from osgeo import gdal\n    from pyresample.geometry import AreaDefinition\n    from pyresample.utils import proj4_str_to_dict\n    fid = gdal.Open(filename)\n    geo_transform = fid.GetGeoTransform()\n    pcs_id = fid.GetProjection().split('\"')[1]\n    min_x = geo_transform[0]\n    max_y = geo_transform[3]\n    x_size = fid.RasterXSize\n    y_size = fid.RasterYSize\n    max_x = min_x + geo_transform[1] * x_size\n    min_y = max_y + geo_transform[5] * y_size\n    area_extent = [min_x, min_y, max_x, max_y]\n    area_def = AreaDefinition('geotiff_area', pcs_id, pcs_id,\n                              proj4_str_to_dict(crs),\n                              x_size, y_size, area_extent)\n    return area_def", "response": "Read area definition from a geotiff file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmasking image data if alpha channel is present.", "response": "def mask_image_data(data):\n    \"\"\"Mask image data if alpha channel is present.\"\"\"\n    if data.bands.size in (2, 4):\n        if not np.issubdtype(data.dtype, np.integer):\n            raise ValueError(\"Only integer datatypes can be used as a mask.\")\n        mask = data.data[-1, :, :] == np.iinfo(data.dtype).min\n        data = data.astype(np.float64)\n        masked_data = da.stack([da.where(mask, np.nan, data.data[i, :, :])\n                                for i in range(data.shape[0])])\n        data.data = masked_data\n        data = data.sel(bands=BANDS[data.bands.size - 1])\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading the image and store it in the file_content attribute.", "response": "def read(self):\n        \"\"\"Read the image\"\"\"\n        data = xr.open_rasterio(self.finfo[\"filename\"],\n                                chunks=(1, CHUNK_SIZE, CHUNK_SIZE))\n        attrs = data.attrs.copy()\n        # Create area definition\n        if hasattr(data, 'crs'):\n            self.area = self.get_geotiff_area_def(data.crs)\n\n        # Rename to Satpy convention\n        data = data.rename({'band': 'bands'})\n\n        # Rename bands to [R, G, B, A], or a subset of those\n        data['bands'] = BANDS[data.bands.size]\n\n        # Mask data if alpha channel is present\n        try:\n            data = mask_image_data(data)\n        except ValueError as err:\n            logger.warning(err)\n\n        data.attrs = attrs\n        self.file_content['image'] = data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a dataset from the file.", "response": "def get_dataset(self, key, info):\n        \"\"\"Get a dataset from the file.\"\"\"\n        logger.debug(\"Reading %s.\", key)\n        return self.file_content[key.name]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_area_def(self, dsid):\n        msg = self._get_message(self._msg_datasets[dsid])\n        try:\n            return self._area_def_from_msg(msg)\n        except (RuntimeError, KeyError):\n            raise RuntimeError(\"Unknown GRIB projection information\")", "response": "Get area definition for message."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_dataset(self, dataset_id, ds_info):\n        msg = self._get_message(ds_info)\n        ds_info = self.get_metadata(msg, ds_info)\n        fill = msg['missingValue']\n        data = msg.values.astype(np.float32)\n        if msg.valid_key('jScansPositively') and msg['jScansPositively'] == 1:\n            data = data[::-1]\n\n        if isinstance(data, np.ma.MaskedArray):\n            data = data.filled(np.nan)\n            data = da.from_array(data, chunks=CHUNK_SIZE)\n        else:\n            data[data == fill] = np.nan\n            data = da.from_array(data, chunks=CHUNK_SIZE)\n\n        return xr.DataArray(data, attrs=ds_info, dims=('y', 'x'))", "response": "Read a GRIB message into an xarray DataArray."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a copy of the datasets in the order they appear.", "response": "def stack(datasets):\n    \"\"\"First dataset at the bottom.\"\"\"\n    base = datasets[0].copy()\n    for dataset in datasets[1:]:\n        base = base.where(dataset.isnull(), dataset)\n    return base"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef timeseries(datasets):\n    expanded_ds = []\n    for ds in datasets:\n        tmp = ds.expand_dims(\"time\")\n        tmp.coords[\"time\"] = pd.DatetimeIndex([ds.attrs[\"start_time\"]])\n        expanded_ds.append(tmp)\n\n    res = xr.concat(expanded_ds, dim=\"time\")\n    res.attrs = combine_metadata(*[x.attrs for x in expanded_ds])\n    return res", "response": "Expands dataset with and concats by time dimension"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _create_cached_iter(self):\n        for scn in self._scene_gen:\n            self._scene_cache.append(scn)\n            yield scn", "response": "Iterate over the provided scenes and store them in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_files(cls, files_to_sort, reader=None, **kwargs):\n        from satpy.readers import group_files\n        file_groups = group_files(files_to_sort, reader=reader, **kwargs)\n        scenes = (Scene(filenames=fg) for fg in file_groups)\n        return cls(scenes)", "response": "Create multiple Scene objects from multiple files."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget list of Scenes contained in this MultiScene.", "response": "def scenes(self):\n        \"\"\"Get list of Scene objects contained in this MultiScene.\n\n        .. note::\n\n            If the Scenes contained in this object are stored in a\n            generator (not list or tuple) then accessing this property\n            will load/iterate through the generator possibly\n\n        \"\"\"\n        if self.is_generator:\n            log.debug(\"Forcing iteration of generator-like object of Scenes\")\n            self._scenes = list(self._scenes)\n        return self._scenes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef shared_dataset_ids(self):\n        shared_ids = set(self.scenes[0].keys())\n        for scene in self.scenes[1:]:\n            shared_ids &= set(scene.keys())\n        return shared_ids", "response": "Dataset IDs shared by all children."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if all areas for the provided IDs are equal.", "response": "def _all_same_area(self, dataset_ids):\n        \"\"\"Return True if all areas for the provided IDs are equal.\"\"\"\n        all_areas = []\n        for ds_id in dataset_ids:\n            for scn in self.scenes:\n                ds = scn.get(ds_id)\n                if ds is None:\n                    continue\n                all_areas.append(ds.attrs.get('area'))\n        all_areas = [area for area in all_areas if area is not None]\n        return all(all_areas[0] == area for area in all_areas[1:])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nabstracts method for running a Scene method on each Scene.", "response": "def _call_scene_func(gen, func_name, create_new_scene, *args, **kwargs):\n        \"\"\"Abstract method for running a Scene method on each Scene.\"\"\"\n        for scn in gen:\n            new_scn = getattr(scn, func_name)(*args, **kwargs)\n            if create_new_scene:\n                yield new_scn\n            else:\n                yield scn"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _generate_scene_func(self, gen, func_name, create_new_scene, *args, **kwargs):\n        new_gen = self._call_scene_func(gen, func_name, create_new_scene, *args, **kwargs)\n        new_gen = new_gen if self.is_generator else list(new_gen)\n        if create_new_scene:\n            return self.__class__(new_gen)\n        self._scene_gen = _SceneGenerator(new_gen)\n        self._scenes = iter(self._scene_gen)", "response": "Abstract method for running a Scene method on each Scene."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load(self, *args, **kwargs):\n        self._generate_scene_func(self._scenes, 'load', False, *args, **kwargs)", "response": "Load the required datasets from the multiple scenes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef crop(self, *args, **kwargs):\n        return self._generate_scene_func(self._scenes, 'crop', True, *args, **kwargs)", "response": "Crop the multiscene and return a new cropped multiscene."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef blend(self, blend_function=stack):\n        new_scn = Scene()\n        common_datasets = self.shared_dataset_ids\n        for ds_id in common_datasets:\n            datasets = [scn[ds_id] for scn in self.scenes if ds_id in scn]\n            new_scn[ds_id] = blend_function(datasets)\n\n        return new_scn", "response": "Blends the datasets into one scene."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _distribute_save_datasets(self, scenes_iter, client, batch_size=1, **kwargs):\n        def load_data(q):\n            idx = 0\n            while True:\n                future_list = q.get()\n                if future_list is None:\n                    break\n\n                # save_datasets shouldn't be returning anything\n                for future in future_list:\n                    future.result()\n                    log.info(\"Finished saving %d scenes\", idx)\n                    idx += 1\n                q.task_done()\n\n        input_q = Queue(batch_size if batch_size is not None else 1)\n        load_thread = Thread(target=load_data, args=(input_q,))\n        load_thread.start()\n\n        for scene in scenes_iter:\n            delayed = scene.save_datasets(compute=False, **kwargs)\n            if isinstance(delayed, (list, tuple)) and len(delayed) == 2:\n                # TODO Make this work for (source, target) datasets\n                # given a target, source combination\n                raise NotImplementedError(\"Distributed save_datasets does not support writers \"\n                                          \"that return (source, target) combinations at this time. Use \"\n                                          \"the non-distributed save_datasets instead.\")\n            future = client.compute(delayed)\n            input_q.put(future)\n        input_q.put(None)\n\n        log.debug(\"Waiting for child thread to get saved results...\")\n        load_thread.join()\n        log.debug(\"Child thread died successfully\")", "response": "Distribute save_datasets across a cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_datasets(self, client=True, batch_size=1, **kwargs):\n        if 'compute' in kwargs:\n            raise ValueError(\"The 'compute' keyword argument can not be provided.\")\n\n        client = self._get_client(client=client)\n\n        scenes = iter(self._scenes)\n        if client is not None:\n            self._distribute_save_datasets(scenes, client, batch_size=batch_size, **kwargs)\n        else:\n            self._simple_save_datasets(scenes, **kwargs)", "response": "Runs save_datasets on each Scene."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetermines filename shape and fill value of animation to be created.", "response": "def _get_animation_info(self, all_datasets, filename, fill_value=None):\n        \"\"\"Determine filename and shape of animation to be created.\"\"\"\n        valid_datasets = [ds for ds in all_datasets if ds is not None]\n        first_dataset = valid_datasets[0]\n        last_dataset = valid_datasets[-1]\n        first_img = get_enhanced_image(first_dataset)\n        first_img_data = first_img.finalize(fill_value=fill_value)[0]\n        shape = tuple(first_img_data.sizes.get(dim_name)\n                      for dim_name in ('y', 'x', 'bands'))\n        if fill_value is None and filename.endswith('gif'):\n            log.warning(\"Forcing fill value to '0' for GIF Luminance images\")\n            fill_value = 0\n            shape = shape[:2]\n\n        attrs = first_dataset.attrs.copy()\n        if 'end_time' in last_dataset.attrs:\n            attrs['end_time'] = last_dataset.attrs['end_time']\n        this_fn = filename.format(**attrs)\n        return this_fn, shape, fill_value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_animation_frames(self, all_datasets, shape, fill_value=None,\n                              ignore_missing=False):\n        \"\"\"Create enhanced image frames to save to a file.\"\"\"\n        for idx, ds in enumerate(all_datasets):\n            if ds is None and ignore_missing:\n                continue\n            elif ds is None:\n                log.debug(\"Missing frame: %d\", idx)\n                data = da.zeros(shape, dtype=np.uint8, chunks=shape)\n                data = xr.DataArray(data)\n            else:\n                img = get_enhanced_image(ds)\n                data, mode = img.finalize(fill_value=fill_value)\n                if data.ndim == 3:\n                    # assume all other shapes are (y, x)\n                    # we need arrays grouped by pixel so\n                    # transpose if needed\n                    data = data.transpose('y', 'x', 'bands')\n            yield data.data", "response": "Create enhanced image frames to save to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_client(self, client=True):\n        client = client or None  # convert False/None to None\n        if client is True and get_client is None:\n            log.debug(\"'dask.distributed' library was not found, will \"\n                      \"use simple serial processing.\")\n            client = None\n        elif client is True:\n            try:\n                # get existing client\n                client = get_client()\n            except ValueError:\n                log.warning(\"No dask distributed client was provided or found, \"\n                            \"but distributed features were requested. Will use simple serial processing.\")\n                client = None\n        return client", "response": "Determine what dask distributed client to use."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nuse dask. distributed to compute multiple frames at a time.", "response": "def _distribute_frame_compute(self, writers, frame_keys, frames_to_write, client, batch_size=1):\n        \"\"\"Use ``dask.distributed`` to compute multiple frames at a time.\"\"\"\n        def load_data(frame_gen, q):\n            for frame_arrays in frame_gen:\n                future_list = client.compute(frame_arrays)\n                for frame_key, arr_future in zip(frame_keys, future_list):\n                    q.put({frame_key: arr_future})\n            q.put(None)\n\n        input_q = Queue(batch_size if batch_size is not None else 1)\n        load_thread = Thread(target=load_data, args=(frames_to_write, input_q,))\n        remote_q = client.gather(input_q)\n        load_thread.start()\n\n        while True:\n            future_dict = remote_q.get()\n            if future_dict is None:\n                break\n\n            # write the current frame\n            # this should only be one element in the dictionary, but this is\n            # also the easiest way to get access to the data\n            for frame_key, result in future_dict.items():\n                # frame_key = rev_future_dict[future]\n                w = writers[frame_key]\n                w.append_data(result)\n            input_q.task_done()\n\n        log.debug(\"Waiting for child thread...\")\n        load_thread.join(10)\n        if load_thread.is_alive():\n            import warnings\n            warnings.warn(\"Background thread still alive after failing to die gracefully\")\n        else:\n            log.debug(\"Child thread died successfully\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _simple_frame_compute(self, writers, frame_keys, frames_to_write):\n        for frame_arrays in frames_to_write:\n            for frame_key, product_frame in zip(frame_keys, frame_arrays):\n                w = writers[frame_key]\n                w.append_data(product_frame.compute())", "response": "Compute frames the plain dask way."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsaves an animation to a file.", "response": "def save_animation(self, filename, datasets=None, fps=10, fill_value=None,\n                       batch_size=1, ignore_missing=False, client=True, **kwargs):\n        \"\"\"Helper method for saving to movie (MP4) or GIF formats.\n\n        Supported formats are dependent on the `imageio` library and are\n        determined by filename extension by default.\n\n        .. note::\n\n            Starting with ``imageio`` 2.5.0, the use of FFMPEG depends on\n            a separate ``imageio-ffmpeg`` package.\n\n        By default all datasets available will be saved to individual files\n        using the first Scene's datasets metadata to format the filename\n        provided. If a dataset is not available from a Scene then a black\n        array is used instead (np.zeros(shape)).\n\n        This function can use the ``dask.distributed`` library for improved\n        performance by computing multiple frames at a time (see `batch_size`\n        option below). If the distributed library is not available then frames\n        will be generated one at a time, one product at a time.\n\n        Args:\n            filename (str): Filename to save to. Can include python string\n                            formatting keys from dataset ``.attrs``\n                            (ex. \"{name}_{start_time:%Y%m%d_%H%M%S.gif\")\n            datasets (list): DatasetIDs to save (default: all datasets)\n            fps (int): Frames per second for produced animation\n            fill_value (int): Value to use instead creating an alpha band.\n            batch_size (int): Number of frames to compute at the same time.\n                This only has effect if the `dask.distributed` package is\n                installed. This will default to 1. Setting this to 0 or less\n                will attempt to process all frames at once. This option should\n                be used with care to avoid memory issues when trying to\n                improve performance. Note that this is the total number of\n                frames for all datasets, so when saving 2 datasets this will\n                compute ``(batch_size / 2)`` frames for the first dataset and\n                ``(batch_size / 2)`` frames for the second dataset.\n            ignore_missing (bool): Don't include a black frame when a dataset\n                                   is missing from a child scene.\n            client (bool or dask.distributed.Client): Dask distributed client\n                to use for computation. If this is ``True`` (default) then\n                any existing clients will be used.\n                If this is ``False`` or ``None`` then a client will not be\n                created and ``dask.distributed`` will not be used. If this\n                is a dask ``Client`` object then it will be used for\n                distributed computation.\n            kwargs: Additional keyword arguments to pass to\n                   `imageio.get_writer`.\n\n        \"\"\"\n        if imageio is None:\n            raise ImportError(\"Missing required 'imageio' library\")\n\n        scene_gen = self._scene_gen\n        first_scene = self.first_scene\n        scenes = iter(self._scene_gen)\n        info_scenes = [first_scene]\n        if 'end_time' in filename:\n            # if we need the last scene to generate the filename\n            # then compute all the scenes so we can figure it out\n            log.debug(\"Generating scenes to compute end_time for filename\")\n            scenes = list(scenes)\n            info_scenes.append(scenes[-1])\n\n        available_ds = [first_scene.datasets.get(ds) for ds in first_scene.wishlist]\n        available_ds = [DatasetID.from_dict(ds.attrs) for ds in available_ds if ds is not None]\n        dataset_ids = datasets or available_ds\n\n        if not dataset_ids:\n            raise RuntimeError(\"No datasets found for saving (resampling may be needed to generate composites)\")\n\n        writers = {}\n        frames = {}\n        for dataset_id in dataset_ids:\n            if not self.is_generator and not self._all_same_area([dataset_id]):\n                raise ValueError(\"Sub-scene datasets must all be on the same \"\n                                 \"area (see the 'resample' method).\")\n\n            all_datasets = scene_gen[dataset_id]\n            info_datasets = [scn.get(dataset_id) for scn in info_scenes]\n            this_fn, shape, this_fill = self._get_animation_info(info_datasets, filename, fill_value=fill_value)\n            data_to_write = self._get_animation_frames(all_datasets, shape, this_fill, ignore_missing)\n\n            writer = imageio.get_writer(this_fn, fps=fps, **kwargs)\n            frames[dataset_id] = data_to_write\n            writers[dataset_id] = writer\n\n        client = self._get_client(client=client)\n        # get an ordered list of frames\n        frame_keys, frames_to_write = list(zip(*frames.items()))\n        frames_to_write = zip(*frames_to_write)\n        if client is not None:\n            self._distribute_frame_compute(writers, frame_keys, frames_to_write, client, batch_size=batch_size)\n        else:\n            self._simple_frame_compute(writers, frame_keys, frames_to_write)\n\n        for writer in writers.values():\n            writer.close()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_prologue(self):\n\n        for key in ['TCurr', 'TCHED', 'TCTRL', 'TLHED', 'TLTRL', 'TIPFS',\n                    'TINFS', 'TISPC', 'TIECL', 'TIBBC', 'TISTR', 'TLRAN',\n                    'TIIRT', 'TIVIT', 'TCLMT', 'TIONA']:\n            try:\n                self.prologue[key] = make_sgs_time(self.prologue[key])\n            except ValueError:\n                self.prologue.pop(key, None)\n                logger.debug(\"Invalid data for %s\", key)\n\n        for key in ['SubSatLatitude', \"SubSatLongitude\", \"ReferenceLongitude\",\n                    \"ReferenceDistance\", \"ReferenceLatitude\"]:\n            self.prologue[key] = make_gvar_float(self.prologue[key])", "response": "Reprocess prologue to correct types."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_dataset(self, key, info):\n        logger.debug(\"Getting raw data\")\n        res = super(HRITGOESFileHandler, self).get_dataset(key, info)\n\n        self.mda['calibration_parameters'] = self._get_calibration_params()\n\n        res = self.calibrate(res, key.calibration)\n        new_attrs = info.copy()\n        new_attrs.update(res.attrs)\n        res.attrs = new_attrs\n        res.attrs['platform_name'] = self.platform_name\n        res.attrs['sensor'] = 'goes_imager'\n        return res", "response": "Get the data from the files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the calibration parameters from the metadata.", "response": "def _get_calibration_params(self):\n        \"\"\"Get the calibration parameters from the metadata.\"\"\"\n        params = {}\n        idx_table = []\n        val_table = []\n        for elt in self.mda['image_data_function'].split(b'\\r\\n'):\n            try:\n                key, val = elt.split(b':=')\n                try:\n                    idx_table.append(int(key))\n                    val_table.append(float(val))\n                except ValueError:\n                    params[key] = val\n            except ValueError:\n                pass\n        params['indices'] = np.array(idx_table)\n        params['values'] = np.array(val_table, dtype=np.float32)\n        return params"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_prologue(self):\n        with open(self.filename, \"rb\") as fp_:\n            fp_.seek(self.mda['total_header_length'])\n            data = np.fromfile(fp_, dtype=prologue, count=1)[0]\n\n            self.prologue.update(recarray2dict(data))\n\n        self.process_prologue()", "response": "Read the prologue metadata."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the data from the files.", "response": "def get_dataset(self, key, info):\n        \"\"\"Get the data  from the files.\"\"\"\n        res = super(HRITGOMSFileHandler, self).get_dataset(key, info)\n\n        res = self.calibrate(res, key.calibration)\n        res.attrs['units'] = info['units']\n        res.attrs['standard_name'] = info['standard_name']\n        res.attrs['wavelength'] = info['wavelength']\n        res.attrs['platform_name'] = self.platform_name\n        res.attrs['sensor'] = 'msu-gs'\n        res.attrs['satellite_longitude'] = self.mda['projection_parameters']['SSP_longitude']\n        res.attrs['satellite_latitude'] = 0\n        res.attrs['satellite_altitude'] = 35785831.00\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_solar_flux(self, band):\n        solar_flux = self.cal['solar_flux'].isel(bands=band).values\n        d_index = self.cal['detector_index'].fillna(0).astype(int)\n\n        return da.map_blocks(self._get_items, d_index.data, solar_flux=solar_flux, dtype=solar_flux.dtype)", "response": "Get the solar flux for the band."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_dataset(self, key, info):\n        if self.channel is not None and self.channel != key.name:\n            return\n        logger.debug('Reading %s.', key.name)\n\n        if self.channel is not None and self.channel.startswith('Oa'):\n            dataset = self.nc[self.channel + '_reflectance']\n        else:\n            dataset = self.nc[info['nc_key']]\n\n        if key.name == 'wqsf':\n            dataset.attrs['_FillValue'] = 1\n        elif key.name == 'mask':\n            mask = self.getbitmask(dataset.to_masked_array().data)\n            dataset = dataset * np.nan\n            dataset = dataset.where(~ mask, True)\n\n        dataset.attrs['platform_name'] = self.platform_name\n        dataset.attrs['sensor'] = self.sensor\n        dataset.attrs.update(key.to_dict())\n        return dataset", "response": "Load a dataset from the NetCDF file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine the platform name of the user s current user", "response": "def _get_platform_name(ncattr):\n        \"\"\"Determine name of the platform\"\"\"\n        match = re.match(r'G-(\\d+)', ncattr)\n        if match:\n            return SPACECRAFTS.get(int(match.groups()[0]))\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_sector(self, channel, nlines, ncols):\n        if self._is_vis(channel):\n            margin = 100\n            sectors_ref = self.vis_sectors\n        else:\n            margin = 50\n            sectors_ref = self.ir_sectors\n\n        for (nlines_ref, ncols_ref), sector in sectors_ref.items():\n            if np.fabs(ncols - ncols_ref) < margin and \\\n                    np.fabs(nlines - nlines_ref) < margin:\n                return sector\n\n        return UNKNOWN_SECTOR", "response": "Determine which sector was scanned"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _is_vis(channel):\n        if isinstance(channel, str):\n            return channel == '00_7'\n        elif isinstance(channel, int):\n            return channel == 1\n        else:\n            raise ValueError('Invalid channel')", "response": "Determine whether the given channel is a visible channel"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the nadir pixel of the nadir in the image.", "response": "def _get_nadir_pixel(earth_mask, sector):\n        \"\"\"Find the nadir pixel\n\n        Args:\n            earth_mask: Mask identifying earth and space pixels\n            sector: Specifies the scanned sector\n        Returns:\n            nadir row, nadir column\n        \"\"\"\n        if sector == FULL_DISC:\n            logger.debug('Computing nadir pixel')\n\n            # The earth is not centered in the image, compute bounding box\n            # of the earth disc first\n            rmin, rmax, cmin, cmax = bbox(earth_mask)\n\n            # The nadir pixel is approximately at the centre of the earth disk\n            nadir_row = rmin + (rmax - rmin) // 2\n            nadir_col = cmin + (cmax - cmin) // 2\n\n            return nadir_row, nadir_col\n\n        return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _is_yaw_flip(lat, delta=10):\n        logger.debug('Computing yaw flip flag')\n        # In case of yaw-flip the data and coordinates in the netCDF files are\n        # also flipped. Just check whether the latitude increases or decrases\n        # with the line number.\n        crow, ccol = np.array(lat.shape) // 2\n        return (lat[crow+delta, ccol] - lat[crow, ccol]).values > 0", "response": "Determine whether the satellite is yaw - flipped."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget area definition with uniform sampling", "response": "def _get_area_def_uniform_sampling(self, lon0, channel):\n        \"\"\"Get area definition with uniform sampling\"\"\"\n        logger.debug('Computing area definition')\n\n        if lon0 is not None:\n            # Define proj4 projection parameters\n            proj_dict = {'a': EQUATOR_RADIUS,\n                         'b': POLE_RADIUS,\n                         'lon_0': lon0,\n                         'h': ALTITUDE,\n                         'proj': 'geos',\n                         'units': 'm'}\n\n            # Calculate maximum scanning angles\n            xmax, ymax = get_geostationary_angle_extent(\n                namedtuple('area', ['proj_dict'])(proj_dict))\n\n            # Derive area extent using small angle approximation (maximum\n            # scanning angle is ~8.6 degrees)\n            llx, lly, urx, ury = ALTITUDE * np.array([-xmax, -ymax, xmax, ymax])\n            area_extent = [llx, lly, urx, ury]\n\n            # Original image is oversampled. Create pyresample area definition\n            # with uniform sampling in N-S and E-W direction\n            if self._is_vis(channel):\n                sampling = SAMPLING_NS_VIS\n            else:\n                sampling = SAMPLING_NS_IR\n            pix_size = ALTITUDE * sampling\n            area_def = pyresample.geometry.AreaDefinition(\n                'goes_geos_uniform',\n                '{} geostationary projection (uniform sampling)'.format(self.platform_name),\n                'goes_geos_uniform',\n                proj_dict,\n                np.rint((urx - llx) / pix_size).astype(int),\n                np.rint((ury - lly) / pix_size).astype(int),\n                area_extent)\n\n            return area_def\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts timestamp of the dataset", "response": "def start_time(self):\n        \"\"\"Start timestamp of the dataset\"\"\"\n        dt = self.nc['time'].dt\n        return datetime(year=dt.year, month=dt.month, day=dt.day,\n                        hour=dt.hour, minute=dt.minute,\n                        second=dt.second, microsecond=dt.microsecond)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef end_time(self):\n        try:\n            return self.start_time + SCAN_DURATION[self.sector]\n        except KeyError:\n            return self.start_time", "response": "End timestamp of the dataset"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nderiving metadata from the coordinates", "response": "def meta(self):\n        \"\"\"Derive metadata from the coordinates\"\"\"\n        # Use buffered data if available\n        if self._meta is None:\n            lat = self.geo_data['lat']\n            earth_mask = self._get_earth_mask(lat)\n            crow, ccol = self._get_nadir_pixel(earth_mask=earth_mask,\n                                               sector=self.sector)\n            lat0 = lat.values[crow, ccol] if crow is not None else None\n            yaw_flip = self._is_yaw_flip(lat)\n            del lat\n\n            lon = self.geo_data['lon']\n            lon0 = lon.values[crow, ccol] if crow is not None else None\n            area_def_uni = self._get_area_def_uniform_sampling(\n                lon0=lon0, channel=self.gvar_channel)\n            del lon\n\n            self._meta = {'earth_mask': earth_mask,\n                          'yaw_flip': yaw_flip,\n                          'lat0': lat0,\n                          'lon0': lon0,\n                          'nadir_row': crow,\n                          'nadir_col': ccol,\n                          'area_def_uni': area_def_uni}\n        return self._meta"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _counts2radiance(self, counts, coefs, channel):\n        logger.debug('Converting counts to radiance')\n\n        if self._is_vis(channel):\n            # Since the scanline-detector assignment is unknown, use the average\n            # coefficients for all scanlines.\n            slope = np.array(coefs['slope']).mean()\n            offset = np.array(coefs['offset']).mean()\n            return self._viscounts2radiance(counts=counts, slope=slope,\n                                            offset=offset)\n\n        return self._ircounts2radiance(counts=counts, scale=coefs['scale'],\n                                       offset=coefs['offset'])", "response": "Convert raw detector counts to radiance"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _calibrate(self, radiance, coefs, channel, calibration):\n        if self._is_vis(channel):\n            if not calibration == 'reflectance':\n                raise ValueError('Cannot calibrate VIS channel to '\n                                 '{}'.format(calibration))\n            return self._calibrate_vis(radiance=radiance, k=coefs['k'])\n        else:\n            if not calibration == 'brightness_temperature':\n                raise ValueError('Cannot calibrate IR channel to '\n                                 '{}'.format(calibration))\n\n            # Since the scanline-detector assignment is unknown, use the average\n            # coefficients for all scanlines.\n            mean_coefs = {'a': np.array(coefs['a']).mean(),\n                          'b': np.array(coefs['b']).mean(),\n                          'n': np.array(coefs['n']).mean(),\n                          'btmin': coefs['btmin'],\n                          'btmax': coefs['btmax']}\n            return self._calibrate_ir(radiance=radiance, coefs=mean_coefs)", "response": "Convert radiance to reflectance or brightness temperature"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts IR counts to radiance", "response": "def _ircounts2radiance(counts, scale, offset):\n        \"\"\"Convert IR counts to radiance\n\n        Reference: [IR].\n\n        Args:\n            counts: Raw detector counts\n            scale: Scale [mW-1 m2 cm sr]\n            offset: Offset [1]\n\n        Returns:\n            Radiance [mW m-2 cm-1 sr-1]\n        \"\"\"\n        rad = (counts - offset) / scale\n        return rad.clip(min=0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts IR radiance to brightness temperature.", "response": "def _calibrate_ir(radiance, coefs):\n        \"\"\"Convert IR radiance to brightness temperature\n\n        Reference: [IR]\n\n        Args:\n            radiance: Radiance [mW m-2 cm-1 sr-1]\n            coefs: Dictionary of calibration coefficients. Keys:\n                   n: The channel's central wavenumber [cm-1]\n                   a: Offset [K]\n                   b: Slope [1]\n                   btmin: Minimum brightness temperature threshold [K]\n                   btmax: Maximum brightness temperature threshold [K]\n\n        Returns:\n            Brightness temperature [K]\n        \"\"\"\n        logger.debug('Calibrating to brightness temperature')\n\n        # Compute brightness temperature using inverse Planck formula\n        n = coefs['n']\n        bteff = C2 * n / xu.log(1 + C1 * n**3 / radiance.where(radiance > 0))\n        bt = xr.DataArray(bteff * coefs['b'] + coefs['a'])\n\n        # Apply BT threshold\n        return bt.where(xu.logical_and(bt >= coefs['btmin'],\n                                       bt <= coefs['btmax']))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _viscounts2radiance(counts, slope, offset):\n        rad = counts * slope + offset\n        return rad.clip(min=0)", "response": "Convert VIS counts to radiance."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert VIS radiance to reflectance.", "response": "def _calibrate_vis(radiance, k):\n        \"\"\"Convert VIS radiance to reflectance\n\n        Note: Angle of incident radiation and annual variation of the\n        earth-sun distance is not taken into account. A value of 100%\n        corresponds to the radiance of a perfectly reflecting diffuse surface\n        illuminated at normal incidence when the sun is at its annual-average\n        distance from the Earth.\n\n        TODO: Take angle of incident radiation (cos sza) and annual variation\n        of the earth-sun distance into account.\n\n        Reference: [VIS]\n\n        Args:\n            radiance: Radiance [mW m-2 cm-1 sr-1]\n            k: pi / H, where H is the solar spectral irradiance at\n               annual-average sun-earth distance, averaged over the spectral\n               response function of the detector). Units of k: [m2 um sr W-1]\n        Returns:\n            Reflectance [%]\n        \"\"\"\n        logger.debug('Calibrating to reflectance')\n        refl = 100 * k * radiance\n        return refl.clip(min=0)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _update_metadata(self, data, ds_info):\n        # Metadata from the dataset definition\n        data.attrs.update(ds_info)\n\n        # If the file_type attribute is a list and the data is xarray\n        # the concat of the dataset will not work. As the file_type is\n        # not needed this will be popped here.\n        if 'file_type' in data.attrs:\n            data.attrs.pop('file_type')\n\n        # Metadata discovered from the file\n        data.attrs.update(\n            {'platform_name': self.platform_name,\n             'sensor': self.sensor,\n             'sector': self.sector,\n             'yaw_flip': self.meta['yaw_flip']}\n        )\n        if self.meta['lon0'] is not None:\n            # Attributes only available for full disc images. YAML reader\n            # doesn't like it if satellite_* is present but None\n            data.attrs.update(\n                {'satellite_longitude': self.meta['lon0'],\n                 'satellite_latitude': self.meta['lat0'],\n                 'satellite_altitude': ALTITUDE,\n                 'nadir_row': self.meta['nadir_row'],\n                 'nadir_col': self.meta['nadir_col'],\n                 'area_def_uniform_sampling': self.meta['area_def_uni']}\n            )", "response": "Update the metadata of the given DataArray with the given data."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload the dataset designated by the given key from file and return it as a numpy array.", "response": "def get_dataset(self, key, info):\n        \"\"\"Load dataset designated by the given key from file\"\"\"\n        logger.debug('Reading dataset {}'.format(key.name))\n\n        # Read data from file and calibrate if necessary\n        if 'longitude' in key.name:\n            data = self.geo_data['lon']\n        elif 'latitude' in key.name:\n            data = self.geo_data['lat']\n        else:\n            tic = datetime.now()\n            data = self.calibrate(self.nc['data'].isel(time=0),\n                                  calibration=key.calibration,\n                                  channel=key.name)\n            logger.debug('Calibration time: {}'.format(datetime.now() - tic))\n\n        # Mask space pixels\n        data = data.where(self.meta['earth_mask'])\n\n        # Set proper dimension names\n        data = data.rename({'xc': 'x', 'yc': 'y'})\n\n        # Update metadata\n        self._update_metadata(data, ds_info=info)\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calibrate(self, counts, calibration, channel):\n        # Convert 16bit counts from netCDF4 file to the original 10bit\n        # GVAR counts by dividing by 32. See [FAQ].\n        counts = counts / 32.\n\n        coefs = CALIB_COEFS[self.platform_name][channel]\n        if calibration == 'counts':\n            return counts\n        elif calibration in ['radiance', 'reflectance',\n                             'brightness_temperature']:\n            radiance = self._counts2radiance(counts=counts, coefs=coefs,\n                                             channel=channel)\n            if calibration == 'radiance':\n                return radiance\n\n            return self._calibrate(radiance=radiance, coefs=coefs,\n                                   channel=channel, calibration=calibration)\n        else:\n            raise ValueError('Unsupported calibration for channel {}: {}'\n                             .format(channel, calibration))", "response": "Perform calibration on a given channel."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming calibration on the data", "response": "def calibrate(self, data, calibration, channel):\n        \"\"\"Perform calibration\"\"\"\n        coefs = CALIB_COEFS[self.platform_name][channel]\n        is_vis = self._is_vis(channel)\n\n        # IR files provide radiances, VIS file provides reflectances\n        if is_vis and calibration == 'reflectance':\n            return data\n        elif not is_vis and calibration == 'radiance':\n            return data\n        elif not is_vis and calibration == 'brightness_temperature':\n            return self._calibrate(radiance=data, calibration=calibration,\n                                   coefs=coefs, channel=channel)\n        else:\n            raise ValueError('Unsupported calibration for channel {}: {}'\n                             .format(channel, calibration))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the dataset designated by the given key from file and update metadata with the given info.", "response": "def get_dataset(self, key, info):\n        \"\"\"Load dataset designated by the given key from file\"\"\"\n        logger.debug('Reading dataset {}'.format(key.name))\n\n        # Read data from file and calibrate if necessary\n        if 'longitude' in key.name:\n            data = self.nc['lon']\n        elif 'latitude' in key.name:\n            data = self.nc['lat']\n        else:\n            raise KeyError(\"Unknown dataset: {}\".format(key.name))\n\n        # Set proper dimension names\n        data = data.rename({'xc': 'x', 'yc': 'y'})\n\n        # Update metadata\n        data.attrs.update(info)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _float(self, string):\n        string = self._denoise(string)\n        exp_match = re.match(r'^[-.\\d]+x10-(\\d)$', string)\n        if exp_match:\n            exp = int(exp_match.groups()[0])\n            fac = 10 ** -exp\n            string = string.replace('x10-{}'.format(exp), '')\n        else:\n            fac = 1\n\n        return fac * float(string)", "response": "Convert string to float"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a single dataset from the DQF file.", "response": "def get_dataset(self, key, info):\n        \"\"\"Load a dataset.\"\"\"\n        logger.debug('Reading in get_dataset %s.', key.name)\n        radiances = self['Rad']\n\n        if key.calibration == 'reflectance':\n            logger.debug(\"Calibrating to reflectances\")\n            res = self._vis_calibrate(radiances)\n        elif key.calibration == 'brightness_temperature':\n            logger.debug(\"Calibrating to brightness temperatures\")\n            res = self._ir_calibrate(radiances)\n        elif key.calibration != 'radiance':\n            raise ValueError(\"Unknown calibration '{}'\".format(key.calibration))\n        else:\n            res = radiances\n\n        # convert to satpy standard units\n        if res.attrs['units'] == '1':\n            res *= 100\n            res.attrs['units'] = '%'\n\n        res.attrs.update({'platform_name': self.platform_name,\n                          'sensor': self.sensor,\n                          'satellite_latitude': float(self['nominal_satellite_subpoint_lat']),\n                          'satellite_longitude': float(self['nominal_satellite_subpoint_lon']),\n                          'satellite_altitude': float(self['nominal_satellite_height'])})\n        res.attrs.update(key.to_dict())\n        # remove attributes that could be confusing later\n        res.attrs.pop('_FillValue', None)\n        res.attrs.pop('scale_factor', None)\n        res.attrs.pop('add_offset', None)\n        res.attrs.pop('ancillary_variables', None)  # Can't currently load DQF\n        # add in information from the filename that may be useful to the user\n        for key in ('observation_type', 'scene_abbr', 'scan_mode', 'platform_shortname'):\n            res.attrs[key] = self.filename_info[key]\n        # copy global attributes to metadata\n        for key in ('scene_id', 'orbital_slot', 'instrument_ID', 'production_site', 'timeline_ID'):\n            res.attrs[key] = self.nc.attrs.get(key)\n        # only include these if they are present\n        for key in ('fusion_args',):\n            if key in self.nc.attrs:\n                res.attrs[key] = self.nc.attrs[key]\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the area definition of the data at hand.", "response": "def get_area_def(self, key):\n        \"\"\"Get the area definition of the data at hand.\"\"\"\n        projection = self.nc[\"goes_imager_projection\"]\n        a = projection.attrs['semi_major_axis']\n        h = projection.attrs['perspective_point_height']\n        b = projection.attrs['semi_minor_axis']\n\n        lon_0 = projection.attrs['longitude_of_projection_origin']\n        sweep_axis = projection.attrs['sweep_angle_axis'][0]\n\n        # x and y extents in m\n        h = float(h)\n        x = self['x']\n        y = self['y']\n        x_l = h * x[0]\n        x_r = h * x[-1]\n        y_l = h * y[-1]\n        y_u = h * y[0]\n        x_half = (x_r - x_l) / (self.ncols - 1) / 2.\n        y_half = (y_u - y_l) / (self.nlines - 1) / 2.\n        area_extent = (x_l - x_half, y_l - y_half, x_r + x_half, y_u + y_half)\n\n        proj_dict = {'a': float(a),\n                     'b': float(b),\n                     'lon_0': float(lon_0),\n                     'h': h,\n                     'proj': 'geos',\n                     'units': 'm',\n                     'sweep': sweep_axis}\n\n        area = geometry.AreaDefinition(\n            self.nc.attrs.get('orbital_slot', 'abi_geos'),\n            self.nc.attrs.get('spatial_resolution', 'ABI L1B file area'),\n            'abi_geos',\n            proj_dict,\n            self.ncols,\n            self.nlines,\n            np.asarray(area_extent))\n\n        return area"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _vis_calibrate(self, data):\n        solar_irradiance = self['esun']\n        esd = self[\"earth_sun_distance_anomaly_in_AU\"].astype(float)\n\n        factor = np.pi * esd * esd / solar_irradiance\n\n        res = data * factor\n        res.attrs = data.attrs\n        res.attrs['units'] = '1'\n        res.attrs['standard_name'] = 'toa_bidirectional_reflectance'\n        return res", "response": "Calibrate visible channels to reflectance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _ir_calibrate(self, data):\n        fk1 = float(self[\"planck_fk1\"])\n        fk2 = float(self[\"planck_fk2\"])\n        bc1 = float(self[\"planck_bc1\"])\n        bc2 = float(self[\"planck_bc2\"])\n\n        res = (fk2 / xu.log(fk1 / data + 1) - bc1) / bc2\n        res.attrs = data.attrs\n        res.attrs['units'] = 'K'\n        res.attrs['standard_name'] = 'toa_brightness_temperature'\n        return res", "response": "Calibrate IR channels to BT."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef time_seconds(tc_array, year):\n    tc_array = np.array(tc_array, copy=True)\n    word = tc_array[:, 0]\n    day = word >> 1\n    word = tc_array[:, 1].astype(np.uint64)\n    msecs = ((127) & word) * 1024\n    word = tc_array[:, 2]\n    msecs += word & 1023\n    msecs *= 1024\n    word = tc_array[:, 3]\n    msecs += word & 1023\n    return (np.datetime64(\n        str(year) + '-01-01T00:00:00Z', 's') +\n        msecs[:].astype('timedelta64[ms]') +\n        (day - 1)[:].astype('timedelta64[D]'))", "response": "Return the time object from the timecodes\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_colormap(palette, info):\n\n        from trollimage.colormap import Colormap\n        if 'palette_meanings' in palette.attrs:\n            palette_indices = palette.attrs['palette_meanings']\n        else:\n            palette_indices = range(len(palette))\n\n        sqpalette = np.asanyarray(palette).squeeze() / 255.0\n        tups = [(val, tuple(tup))\n                for (val, tup) in zip(palette_indices, sqpalette)]\n        colormap = Colormap(*tups)\n        if 'palette_meanings' not in palette.attrs:\n            sf = info.get('scale_factor', np.array(1))\n            colormap.set_range(\n                *(np.array(info['valid_range']) * sf + info.get('add_offset', 0)))\n\n        return colormap, sqpalette", "response": "Create the colormap from the raw_palette and the valid_range."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apply_enhancement(data, func, exclude=None, separate=False,\n                      pass_dask=False):\n    \"\"\"Apply `func` to the provided data.\n\n    Args:\n        data (xarray.DataArray): Data to be modified inplace.\n        func (callable): Function to be applied to an xarray\n        exclude (iterable): Bands in the 'bands' dimension to not include\n                            in the calculations.\n        separate (bool): Apply `func` one band at a time. Default is False.\n        pass_dask (bool): Pass the underlying dask array instead of the\n                          xarray.DataArray.\n\n    \"\"\"\n    attrs = data.attrs\n    bands = data.coords['bands'].values\n    if exclude is None:\n        exclude = ['A'] if 'A' in bands else []\n\n    if separate:\n        data_arrs = []\n        for idx, band_name in enumerate(bands):\n            band_data = data.sel(bands=[band_name])\n            if band_name in exclude:\n                # don't modify alpha\n                data_arrs.append(band_data)\n                continue\n\n            if pass_dask:\n                dims = band_data.dims\n                coords = band_data.coords\n                d_arr = func(band_data.data, index=idx)\n                band_data = xr.DataArray(d_arr, dims=dims, coords=coords)\n            else:\n                band_data = func(band_data, index=idx)\n            data_arrs.append(band_data)\n            # we assume that the func can add attrs\n            attrs.update(band_data.attrs)\n\n        data.data = xr.concat(data_arrs, dim='bands').data\n        data.attrs = attrs\n        return data\n    else:\n        band_data = data.sel(bands=[b for b in bands\n                                    if b not in exclude])\n        if pass_dask:\n            dims = band_data.dims\n            coords = band_data.coords\n            d_arr = func(band_data.data)\n            band_data = xr.DataArray(d_arr, dims=dims, coords=coords)\n        else:\n            band_data = func(band_data)\n\n        attrs.update(band_data.attrs)\n        # combine the new data with the excluded data\n        new_data = xr.concat([band_data, data.sel(bands=exclude)],\n                             dim='bands')\n        data.data = new_data.sel(bands=bands).data\n        data.attrs = attrs\n\n    return data", "response": "Apply func to the data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nassign values to channels based on a table.", "response": "def lookup(img, **kwargs):\n    \"\"\"Assign values to channels based on a table.\"\"\"\n    luts = np.array(kwargs['luts'], dtype=np.float32) / 255.0\n\n    def func(band_data, luts=luts, index=-1):\n        # NaN/null values will become 0\n        lut = luts[:, index] if len(luts.shape) == 2 else luts\n        band_data = band_data.clip(0, lut.size - 1).astype(np.uint8)\n\n        new_delay = dask.delayed(_lookup_delayed)(lut, band_data)\n        new_data = da.from_delayed(new_delay, shape=band_data.shape,\n                                   dtype=luts.dtype)\n        return new_data\n\n    return apply_enhancement(img.data, func, separate=True, pass_dask=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmerges colormaps listed in kwargs.", "response": "def _merge_colormaps(kwargs):\n    \"\"\"Merge colormaps listed in kwargs.\"\"\"\n    from trollimage.colormap import Colormap\n    full_cmap = None\n\n    palette = kwargs['palettes']\n    if isinstance(palette, Colormap):\n        full_cmap = palette\n    else:\n        for itm in palette:\n            cmap = create_colormap(itm)\n            cmap.set_range(itm[\"min_value\"], itm[\"max_value\"])\n            if full_cmap is None:\n                full_cmap = cmap\n            else:\n                full_cmap = full_cmap + cmap\n\n    return full_cmap"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating colormap of the given numpy file color vector or colormap.", "response": "def create_colormap(palette):\n    \"\"\"Create colormap of the given numpy file, color vector or colormap.\"\"\"\n    from trollimage.colormap import Colormap\n    fname = palette.get('filename', None)\n    if fname:\n        data = np.load(fname)\n        cmap = []\n        num = 1.0 * data.shape[0]\n        for i in range(int(num)):\n            cmap.append((i / num, (data[i, 0] / 255., data[i, 1] / 255.,\n                                   data[i, 2] / 255.)))\n        return Colormap(*cmap)\n\n    colors = palette.get('colors', None)\n    if isinstance(colors, list):\n        cmap = []\n        values = palette.get('values', None)\n        for idx, color in enumerate(colors):\n            if values:\n                value = values[idx]\n            else:\n                value = idx / float(len(colors) - 1)\n            cmap.append((value, tuple(color)))\n        return Colormap(*cmap)\n\n    if isinstance(colors, str):\n        from trollimage import colormap\n        import copy\n        return copy.copy(getattr(colormap, colors))\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate 3D effect using convolution", "response": "def three_d_effect(img, **kwargs):\n    \"\"\"Create 3D effect using convolution\"\"\"\n    w = kwargs.get('weight', 1)\n    LOG.debug(\"Applying 3D effect with weight %.2f\", w)\n    kernel = np.array([[-w, 0, w],\n                       [-w, 1, w],\n                       [-w, 0, w]])\n    mode = kwargs.get('convolve_mode', 'same')\n\n    def func(band_data, kernel=kernel, mode=mode, index=None):\n        del index\n\n        delay = dask.delayed(_three_d_effect_delayed)(band_data, kernel, mode)\n        new_data = da.from_delayed(delay, shape=band_data.shape, dtype=band_data.dtype)\n        return new_data\n\n    return apply_enhancement(img.data, func, separate=True, pass_dask=True)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef btemp_threshold(img, min_in, max_in, threshold, threshold_out=None, **kwargs):\n    threshold_out = threshold_out if threshold_out is not None else (176 / 255.0)\n    low_factor = (threshold_out - 1.) / (min_in - threshold)\n    low_offset = 1. + (low_factor * min_in)\n    high_factor = threshold_out / (max_in - threshold)\n    high_offset = high_factor * max_in\n\n    def _bt_threshold(band_data):\n        # expects dask array to be passed\n        return da.where(band_data >= threshold,\n                        high_offset - high_factor * band_data,\n                        low_offset - low_factor * band_data)\n\n    return apply_enhancement(img.data, _bt_threshold, pass_dask=True)", "response": "This function scales the input data linearly in two separate regions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _make_request(self, path, method='GET', params_=None):\n        uri = self.api_root + path\n        if params_:\n            params_['text_format'] = self.response_format\n        else:\n            params_ = {'text_format': self.response_format}\n\n        # Make the request\n        response = None\n        try:\n            response = self._session.request(method, uri,\n                                            timeout=self.timeout,\n                                            params=params_)\n        except Timeout as e:\n            print(\"Timeout raised and caught:\\n{e}\".format(e=e))\n\n        # Enforce rate limiting\n        time.sleep(max(self._SLEEP_MIN, self.sleep_time))\n        return response.json()['response'] if response else None", "response": "Make a request to the API"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a specific song.", "response": "def get_song(self, id_):\n        \"\"\"Data for a specific song.\"\"\"\n        endpoint = \"songs/{id}\".format(id=id_)\n        return self._make_request(endpoint)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_artist(self, id_):\n        endpoint = \"artists/{id}\".format(id=id_)\n        return self._make_request(endpoint)", "response": "Get a specific artist."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_artist_songs(self, id_, sort='title', per_page=20, page=1):\n        endpoint = \"artists/{id}/songs\".format(id=id_)\n        params = {'sort': sort, 'per_page': per_page, 'page': page}\n        return self._make_request(endpoint, params_=params)", "response": "Get the artist s songs for the artist specified."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef search_genius(self, search_term):\n        endpoint = \"search/\"\n        params = {'q': search_term}\n        return self._make_request(endpoint, params_=params)", "response": "Search documents hosted on Genius."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef search_genius_web(self, search_term, per_page=5):\n        endpoint = \"search/multi?\"\n        params = {'per_page': per_page, 'q': search_term}\n\n        # This endpoint is not part of the API, requires different formatting\n        url = \"https://genius.com/api/\" + endpoint + urlencode(params)\n        response = requests.get(url, timeout=self.timeout)\n        time.sleep(max(self._SLEEP_MIN, self.sleep_time))\n        return response.json()['response'] if response else None", "response": "Use the web - version of Genius search"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_annotation(self, id_):\n        endpoint = \"annotations/{id}\".format(id=id_)\n        return self._make_request(endpoint)", "response": "Get a specific annotation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _scrape_song_lyrics_from_url(self, url):\n        page = requests.get(url)\n        if page.status_code == 404:\n            return None\n\n        # Scrape the song lyrics from the HTML\n        html = BeautifulSoup(page.text, \"html.parser\")\n        div = html.find(\"div\", class_=\"lyrics\")\n        if not div:\n            return None # Sometimes the lyrics section isn't found\n\n        # Scrape lyrics if proper section was found on page\n        lyrics = div.get_text()\n        if self.remove_section_headers:  # Remove [Verse], [Bridge], etc.\n            lyrics = re.sub('(\\[.*?\\])*', '', lyrics)\n            lyrics = re.sub('\\n{2}', '\\n', lyrics)  # Gaps between verses\n        return lyrics.strip(\"\\n\")", "response": "Scrape the lyrics from a Genius song URL"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a lowercase string with punctuation and bad chars removed", "response": "def _clean_str(self, s):\n        \"\"\" Returns a lowercase string with punctuation and bad chars removed\n        :param s: string to clean\n        \"\"\"\n        return s.translate(str.maketrans('', '', punctuation)).replace('\\u200b', \" \").strip().lower()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning False if result from Genius is not actually song lyrics", "response": "def _result_is_lyrics(self, song_title):\n        \"\"\" Returns False if result from Genius is not actually song lyrics\n            Set the `excluded_terms` and `replace_default_terms` as\n            instance variables within the Genius class.\n        \"\"\"\n\n        default_terms = ['track\\\\s?list', 'album art(work)?', 'liner notes',\n                         'booklet', 'credits', 'interview', 'skit',\n                         'instrumental', 'setlist']\n        if self.excluded_terms:\n            if self.replace_default_terms:\n                default_terms = self.excluded_terms\n            else:\n                default_terms.extend(self.excluded_terms)\n\n        expression = r\"\".join([\"({})|\".format(term) for term in default_terms]).strip('|')\n        regex = re.compile(expression, re.IGNORECASE)\n        return not regex.search(self._clean_str(song_title))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns either a Song or Artist result from search_genius_web", "response": "def _get_item_from_search_response(self, response, type_):\n        \"\"\" Returns either a Song or Artist result from search_genius_web \"\"\"\n        sections = sorted(response['sections'],\n                          key=lambda sect: sect['type'] == type_,\n                          reverse=True)\n        for section in sections:\n            hits = [hit for hit in section['hits'] if hit['type'] == type_]\n            if hits:\n                return hits[0]['result']"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if search result matches searched song", "response": "def _result_is_match(self, result, title, artist=None):\n        \"\"\" Returns True if search result matches searched song \"\"\"\n        result_title = self._clean_str(result['title'])\n        title_is_match = result_title == self._clean_str(title)\n        if not artist:\n            return title_is_match\n        result_artist = self._clean_str(result['primary_artist']['name'])\n        return title_is_match and result_artist == self._clean_str(artist)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsearch Genius. com for lyrics to a specific song.", "response": "def search_song(self, title, artist=\"\", get_full_info=True):\n        \"\"\" Search Genius.com for lyrics to a specific song\n        :param title: Song title to search for\n        :param artist: Name of the artist\n        :param get_full_info: Get full info for each song (slower)\n        \"\"\"\n\n        # Search the Genius API for the specified song\n        if self.verbose:\n            if artist:\n                print('Searching for \"{s}\" by {a}...'.format(s=title, a=artist))\n            else:\n                print('Searching for \"{s}\"...'.format(s=title))\n        search_term = \"{s} {a}\".format(s=title, a=artist).strip()\n        response = self.search_genius_web(search_term)\n\n        # Otherwise, move forward with processing the search results\n        result = self._get_item_from_search_response(response, type_=\"song\")\n\n        # Exit search if there were no results returned from API\n        if not result:\n            if self.verbose:\n                print(\"No results found for: '{s}'\".format(s=search_term))\n            return None\n\n        # Reject non-songs (Liner notes, track lists, etc.)\n        valid = self._result_is_lyrics(result['title']) if self.skip_non_songs else True\n        if not valid:\n            if self.verbose:\n                print('Specified song does not contain lyrics. Rejecting.')\n            return None\n\n        # Download full song info (an API call) unless told not to by user\n        song_info = result.copy()\n        if get_full_info:\n            song_info.update(self.get_song(result['id'])['song'])\n        lyrics = self._scrape_song_lyrics_from_url(song_info['url'])\n\n        # Skip results when URL is a 404 or lyrics are missing\n        if not lyrics:\n            if self.verbose:\n                print('Specified song does not have a valid URL with lyrics. Rejecting.')\n            return None\n\n        # Return a Song object with lyrics if we've made it this far\n        song = Song(song_info, lyrics)\n        if self.verbose:\n            print('Done.')\n        return song"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsearching Genius. com for songs by the specified artist.", "response": "def search_artist(self, artist_name, max_songs=None,\n                      sort='popularity', per_page=20, get_full_info=True):\n        \"\"\"Search Genius.com for songs by the specified artist.\n        Returns an Artist object containing artist's songs.\n        :param artist_name: Name of the artist to search for\n        :param max_songs: Maximum number of songs to search for\n        :param sort: Sort by 'title' or 'popularity'\n        :param per_page: Number of results to return per search page\n        :param get_full_info: Get full info for each song (slower)\n        \"\"\"\n\n        if self.verbose:\n            print('Searching for songs by {0}...\\n'.format(artist_name))\n\n        # Perform a Genius API search for the artist\n        found_artist = None\n        response = self.search_genius_web(artist_name)\n        found_artist = self._get_item_from_search_response(response, type_=\"artist\")\n\n        # Exit the search if we couldn't find an artist by the given name\n        if not found_artist:\n            if self.verbose:\n                print(\"No results found for '{a}'.\".format(a=artist_name))\n            return None\n\n        # Assume the top search result is the intended artist\n        artist_id = found_artist['id']\n        artist_info = self.get_artist(artist_id)\n        found_name = artist_info['artist']['name']\n        if found_name != artist_name:\n            if self.verbose:\n                print(\"Changing artist name to '{a}'\".format(a=found_name))\n            artist_name = found_name\n\n        # Create the Artist object\n        artist = Artist(artist_info)\n\n        # Download each song by artist, stored as Song objects in Artist object\n        page = 1\n        reached_max_songs = False\n        while not reached_max_songs:\n            songs_on_page = self.get_artist_songs(artist_id, sort, per_page, page)\n\n            # Loop through each song on page of search results\n            for song_info in songs_on_page['songs']:\n                # Check if song is valid (e.g. has title, contains lyrics)\n                has_title = ('title' in song_info)\n                has_lyrics = self._result_is_lyrics(song_info['title'])\n                valid = has_title and (has_lyrics or (not self.skip_non_songs))\n\n                # Reject non-song results (e.g. Linear Notes, Tracklists, etc.)\n                if not valid:\n                    if self.verbose:\n                        s = song_info['title'] if has_title else \"MISSING TITLE\"\n                        print('\"{s}\" is not valid. Skipping.'.format(s=s))\n                    continue\n\n                # Create the Song object from lyrics and metadata\n                lyrics = self._scrape_song_lyrics_from_url(song_info['url'])\n                if get_full_info:\n                    info = self.get_song(song_info['id'])\n                else:\n                    info = {'song': song_info}\n                song = Song(info, lyrics)\n\n                # Attempt to add the Song to the Artist\n                result = artist.add_song(song, verbose=False)\n                if result == 0 and self.verbose:\n                    print('Song {n}: \"{t}\"'.format(n=artist.num_songs,\n                                                   t=song.title))\n\n                # Exit search if the max number of songs has been met\n                reached_max_songs = max_songs and artist.num_songs >= max_songs\n                if reached_max_songs:\n                    if self.verbose:\n                        print('\\nReached user-specified song limit ({m}).'.format(m=max_songs))\n                    break\n\n            # Move on to next page of search results\n            page = songs_on_page['next_page']\n            if page is None:\n                break  # Exit search when last page is reached\n\n        if self.verbose:\n            print('Done. Found {n} songs.'.format(n=artist.num_songs))\n        return artist"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_artists(self, artists, filename=\"artist_lyrics\", overwrite=False):\n        if isinstance(artists, Artist):\n            artists = [artists]\n\n        # Create a temporary directory for lyrics\n        start = time.time()\n        tmp_dir = 'tmp_lyrics'\n        if not os.path.isdir(tmp_dir):\n            os.mkdir(tmp_dir)\n            count = 0\n        else:\n            count = len(os.listdir(tmp_dir))\n\n        # Check if file already exists\n        if os.path.isfile(filename + \".json\") and not overwrite:\n            msg = \"{f} already exists. Overwrite?\\n(y/n): \".format(f=filename)\n            if input(msg).lower() != \"y\":\n                print(\"Leaving file in place. Exiting.\")\n                os.rmdir(tmp_dir)\n                return\n\n        # Extract each artist's lyrics in json format\n        all_lyrics = {'artists': []}\n        for n, artist in enumerate(artists):\n            if isinstance(artist, Artist):\n                all_lyrics['artists'].append({})\n                f = \"tmp_{n}_{a}\".format(n=count + n,\n                                         a=artist.name.replace(\" \", \"\"))\n                tmp_file = os.path.join(tmp_dir, f)\n                if self.verbose:\n                    print(tmp_file)\n                all_lyrics['artists'][-1] = artist.save_lyrics(overwrite=True)\n\n        # Save all of the lyrics\n        with open(filename + '.json', 'w') as outfile:\n            json.dump(all_lyrics, outfile)\n\n        # Delete the temporary directory\n        shutil.rmtree(tmp_dir)\n        elapsed = (time.time() - start) / 60 / 60\n        print(\"Time elapsed: {t} hours\".format(t=elapsed))", "response": "Save lyrics from multiple Artist objects as JSON object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_dict(self):\n        return dict({'title': self.title,\n                     'album': self.album,\n                     'year': self.year,\n                     'lyrics': self.lyrics,\n                     'image': self.song_art_image_url})", "response": "Create a dictionary from the song object"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nallow user to save song lyrics to a. json or. txt file.", "response": "def save_lyrics(self, filename=None, extension='json', verbose=True,\n                    overwrite=None, binary_encoding=False):\n        \"\"\"Allows user to save song lyrics from Song object to a .json or .txt file.\"\"\"\n        extension = extension.lstrip(\".\")\n        assert (extension == 'json') or (extension == 'txt'), \"format_ must be JSON or TXT\"\n\n        # Determine the filename\n        if filename:\n            for ext in [\"txt\", \"TXT\", \"json\", \"JSON\"]:\n                filename = filename.replace(\".\" + ext, \"\")\n            filename += \".\" + extension\n        else:\n            filename = \"Lyrics_{}_{}.{}\".format(self.artist.replace(\" \", \"\"),\n                                                self.title.replace(\" \", \"\"),\n                                                extension).lower()\n            filename = self._sanitize_filename(filename)\n\n        # Check if file already exists\n        write_file = False\n        if not os.path.isfile(filename):\n            write_file = True\n        elif overwrite:\n            write_file = True\n        else:\n            if input(\"{} already exists. Overwrite?\\n(y/n): \".format(filename)).lower() == 'y':\n                write_file = True\n\n        # Format lyrics as either .txt or .json\n        if extension == 'json':\n            lyrics_to_write = {'songs': [], 'artist': self.artist}\n            lyrics_to_write['songs'].append(self.to_dict())\n        else:\n            lyrics_to_write = self.lyrics\n\n        if binary_encoding:\n            lyrics_to_write = lyrics_to_write.encode('utf8')\n\n        # Write the lyrics to either a .json or .txt file\n        if write_file:\n            with open(filename, 'wb' if binary_encoding else 'w') as lyrics_file:\n                if extension == 'json':\n                    json.dump(lyrics_to_write, lyrics_file)\n                else:\n                    lyrics_file.write(lyrics_to_write)\n            if verbose:\n                print('Wrote {} to {}.'.format(self.title, filename))\n        else:\n            if verbose:\n                print('Skipping file save.\\n')\n        return lyrics_to_write"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_song(self, new_song, verbose=True):\n\n        if any([song.title == new_song.title for song in self._songs]):\n            if verbose:\n                print('{s} already in {a}, not adding song.'.format(s=new_song.title,\n                                                                    a=self.name))\n            return 1  # Failure\n        if new_song.artist == self.name:\n            self._songs.append(new_song)\n            self._num_songs += 1\n            return 0  # Success\n        if verbose:\n            print(\"Can't add song by {b}, artist must be {a}.\".format(b=new_song.artist,\n                                                                      a=self.name))\n        return 1", "response": "Add a Song object to the Artist object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_lyrics(self, extension='json', overwrite=False,\n                    verbose=True, binary_encoding=False):\n        \"\"\"Allows user to save all lyrics within an Artist object\"\"\"\n        extension = extension.lstrip(\".\")\n        assert (extension == 'json') or (extension == 'txt'), \"format_ must be JSON or TXT\"\n\n        for song in self.songs:\n            song.save_lyrics(extension=extension, overwrite=overwrite, verbose=verbose, binary_encoding=binary_encoding)", "response": "Allows user to save all lyrics within an Artist object"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_file_contents(cls, file_contents, seed_values=None):\n\n    @contextmanager\n    def opener(file_content):\n      with io.BytesIO(file_content.content) as fh:\n        yield fh\n\n    return cls._meta_load(opener, file_contents, seed_values)", "response": "Loads a config from a list of file contents."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load(cls, config_paths, seed_values=None):\n\n    @contextmanager\n    def opener(f):\n      with open(f, 'rb') as fh:\n        yield fh\n\n    return cls._meta_load(opener, config_paths, seed_values)", "response": "Loads a config file from the given paths."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_parser(cls, seed_values=None):\n    seed_values = seed_values or {}\n    buildroot = seed_values.get('buildroot', get_buildroot())\n\n    all_seed_values = {\n      'buildroot': buildroot,\n      'homedir': os.path.expanduser('~'),\n      'user': getpass.getuser(),\n      'pants_bootstrapdir': get_pants_cachedir(),\n      'pants_configdir': get_pants_configdir(),\n    }\n\n    def update_dir_from_seed_values(key, default):\n      all_seed_values[key] = seed_values.get(key, os.path.join(buildroot, default))\n\n    update_dir_from_seed_values('pants_workdir', '.pants.d')\n    update_dir_from_seed_values('pants_supportdir', 'build-support')\n    update_dir_from_seed_values('pants_distdir', 'dist')\n\n    return configparser.ConfigParser(all_seed_values)", "response": "Creates a configparser. ConfigParser that supports %([key - name ) value substitution."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves an option from the specified section or DEFAULT section and attempts to parse it as type.", "response": "def get(self, section, option, type_=six.string_types, default=None):\n    \"\"\"Retrieves option from the specified section (or 'DEFAULT') and attempts to parse it as type.\n\n    If the specified section does not exist or is missing a definition for the option, the value is\n    looked up in the DEFAULT section.  If there is still no definition found, the default value\n    supplied is returned.\n    \"\"\"\n    return self._getinstance(section, option, type_, default)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dlog(msg, log_path=DEFAULT_LOG_PATH):\n  with open(log_path, 'a') as f:\n    f.write('\\n{}@{}: {}'.format(os.getpid(), threading.current_thread().name, msg))", "response": "A handy log utility for debugging multi - process multi - threaded activities."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_passthru_args(self):\n    if not self.supports_passthru_args():\n      raise TaskError('{0} Does not support passthru args.'.format(self.stable_name()))\n    else:\n      return self.context.options.passthru_args_for_scope(self.options_scope)", "response": "Returns the passthru args for this task."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_targets(self, predicate=None):\n    initial_targets = (self.context.targets(predicate) if self.act_transitively\n                       else list(filter(predicate, self.context.target_roots)))\n\n    if not self.target_filtering_enabled:\n      return initial_targets\n    else: \n      return self._filter_targets(initial_targets)", "response": "Returns the candidate targets that this task should act on."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef versioned_workdir(self):\n    versioned_workdir = os.path.join(self.workdir, self.implementation_version_slug())\n    safe_mkdir(versioned_workdir)\n    return versioned_workdir", "response": "The Task. workdir suffixed with a fingerprint of the Task implementation version."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a fingerprint for the identity of the task.", "response": "def fingerprint(self):\n    \"\"\"Returns a fingerprint for the identity of the task.\n\n    A task fingerprint is composed of the options the task is currently running under.\n    Useful for invalidating unchanging targets being executed beneath changing task\n    options that affect outputted artifacts.\n\n    A task's fingerprint is only valid after the task has been fully initialized.\n    \"\"\"\n    hasher = sha1()\n    hasher.update(self.stable_name().encode('utf-8'))\n    hasher.update(self._options_fingerprint(self.options_scope).encode('utf-8'))\n    hasher.update(self.implementation_version_str().encode('utf-8'))\n    for dep in self.subsystem_closure_iter():\n      hasher.update(self._options_fingerprint(dep.options_scope).encode('utf-8'))\n    return hasher.hexdigest() if PY3 else hasher.hexdigest().decode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef invalidated(self,\n                  targets,\n                  invalidate_dependents=False,\n                  silent=False,\n                  fingerprint_strategy=None,\n                  topological_order=False):\n    \"\"\"Checks targets for invalidation, first checking the artifact cache.\n\n    Subclasses call this to figure out what to work on.\n\n    :API: public\n\n    :param targets: The targets to check for changes.\n    :param invalidate_dependents: If True then any targets depending on changed targets are\n                                  invalidated.\n    :param silent: If true, suppress logging information about target invalidation.\n    :param fingerprint_strategy: A FingerprintStrategy instance, which can do per task,\n                                finer grained fingerprinting of a given Target.\n    :param topological_order: Whether to invalidate in dependency order.\n\n    If no exceptions are thrown by work in the block, the build cache is updated for the targets.\n    Note: the artifact cache is not updated. That must be done manually.\n\n    :returns: Yields an InvalidationCheck object reflecting the targets.\n    :rtype: InvalidationCheck\n    \"\"\"\n    invalidation_check = self._do_invalidation_check(fingerprint_strategy,\n                                                     invalidate_dependents,\n                                                     targets,\n                                                     topological_order)\n\n    self._maybe_create_results_dirs(invalidation_check.all_vts)\n\n    if invalidation_check.invalid_vts and self.artifact_cache_reads_enabled():\n      with self.context.new_workunit('cache'):\n        cached_vts, uncached_vts, uncached_causes = \\\n          self.check_artifact_cache(self.check_artifact_cache_for(invalidation_check))\n      if cached_vts:\n        cached_targets = [vt.target for vt in cached_vts]\n        self.context.run_tracker.artifact_cache_stats.add_hits(self._task_name, cached_targets)\n        if not silent:\n          self._report_targets('Using cached artifacts for ', cached_targets, '.')\n      if uncached_vts:\n        uncached_targets = [vt.target for vt in uncached_vts]\n        self.context.run_tracker.artifact_cache_stats.add_misses(self._task_name,\n                                                                 uncached_targets,\n                                                                 uncached_causes)\n        if not silent:\n          self._report_targets('No cached artifacts for ', uncached_targets, '.')\n      # Now that we've checked the cache, re-partition whatever is still invalid.\n      invalidation_check = InvalidationCheck(invalidation_check.all_vts, uncached_vts)\n\n    if not silent:\n      targets = []\n      for vt in invalidation_check.invalid_vts:\n        targets.extend(vt.targets)\n\n      if len(targets):\n        target_address_references = [t.address.reference() for t in targets]\n        msg_elements = [\n          'Invalidated ',\n          items_to_report_element(target_address_references, 'target'),\n          '.',\n        ]\n        self.context.log.info(*msg_elements)\n\n    self._update_invalidation_report(invalidation_check, 'pre-check')\n\n    # Cache has been checked to create the full list of invalid VTs.\n    # Only copy previous_results for this subset of VTs.\n    if self.incremental:\n      for vts in invalidation_check.invalid_vts:\n        vts.copy_previous_results()\n\n    # This may seem odd: why would we need to invalidate a VersionedTargetSet that is already\n    # invalid?  But the name force_invalidate() is slightly misleading in this context - what it\n    # actually does is delete the key file created at the end of the last successful task run.\n    # This is necessary to avoid the following scenario:\n    #\n    # 1) In state A: Task suceeds and writes some output.  Key is recorded by the invalidator.\n    # 2) In state B: Task fails, but writes some output.  Key is not recorded.\n    # 3) After reverting back to state A: The current key is the same as the one recorded at the\n    #    end of step 1), so it looks like no work needs to be done, but actually the task\n    #   must re-run, to overwrite the output written in step 2.\n    #\n    # Deleting the file ensures that if a task fails, there is no key for which we might think\n    # we're in a valid state.\n    for vts in invalidation_check.invalid_vts:\n      vts.force_invalidate()\n\n    # Yield the result, and then mark the targets as up to date.\n    yield invalidation_check\n\n    self._update_invalidation_report(invalidation_check, 'post-check')\n\n    for vt in invalidation_check.invalid_vts:\n      vt.update()\n\n    # Background work to clean up previous builds.\n    if self.context.options.for_global_scope().workdir_max_build_entries is not None:\n      self._launch_background_workdir_cleanup(invalidation_check.all_vts)", "response": "Checks the artifact cache for the given targets and creates a new InvalidationCheck object for the targets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn true if the given vt should be written to a cache ( if configured and if not incremental or incremental cache.", "response": "def _should_cache_target_dir(self, vt):\n    \"\"\"Return true if the given vt should be written to a cache (if configured).\"\"\"\n    return (\n      self.cache_target_dirs and\n      vt.cacheable and\n      (not vt.is_incremental or self.cache_incremental) and\n      self.artifact_cache_writes_enabled()\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_check_artifact_cache(self, vts, post_process_cached_vts=None):\n    if not vts:\n      return [], [], []\n\n    read_cache = self._cache_factory.get_read_cache()\n    items = [(read_cache, vt.cache_key, vt.current_results_dir if self.cache_target_dirs else None)\n             for vt in vts]\n    res = self.context.subproc_map(call_use_cached_files, items)\n\n    cached_vts = []\n    uncached_vts = []\n    uncached_causes = []\n\n    # Note that while the input vts may represent multiple targets (for tasks that overrride\n    # check_artifact_cache_for), the ones we return must represent single targets.\n    # Once flattened, cached/uncached vts are in separate lists. Each uncached vts is paired\n    # with why it is missed for stat reporting purpose.\n    for vt, was_in_cache in zip(vts, res):\n      if was_in_cache:\n        cached_vts.extend(vt.versioned_targets)\n      else:\n        uncached_vts.extend(vt.versioned_targets)\n        uncached_causes.extend(repeat(was_in_cache, len(vt.versioned_targets)))\n        if isinstance(was_in_cache, UnreadableArtifact):\n          self._cache_key_errors.update(was_in_cache.key)\n\n    if post_process_cached_vts:\n      post_process_cached_vts(cached_vts)\n    for vt in cached_vts:\n      vt.update()\n    return cached_vts, uncached_vts, uncached_causes", "response": "Checks the artifact cache for the specified list of VersionedTargetSets."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_artifact_cache(self, vts_artifactfiles_pairs):\n    update_artifact_cache_work = self._get_update_artifact_cache_work(vts_artifactfiles_pairs)\n    if update_artifact_cache_work:\n      self.context.submit_background_work_chain([update_artifact_cache_work],\n                                                parent_workunit_name='cache')", "response": "Write to the artifact cache if we re configured to."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a Work instance to update an artifact cache.", "response": "def _get_update_artifact_cache_work(self, vts_artifactfiles_pairs):\n    \"\"\"Create a Work instance to update an artifact cache, if we're configured to.\n\n    vts_artifactfiles_pairs - a list of pairs (vts, artifactfiles) where\n      - vts is single VersionedTargetSet.\n      - artifactfiles is a list of paths to artifacts for the VersionedTargetSet.\n    \"\"\"\n    cache = self._cache_factory.get_write_cache()\n    if cache:\n      if len(vts_artifactfiles_pairs) == 0:\n        return None\n        # Do some reporting.\n      targets = set()\n      for vts, _ in vts_artifactfiles_pairs:\n        targets.update(vts.targets)\n\n      self._report_targets(\n        'Caching artifacts for ',\n        list(targets),\n        '.',\n        logger=self.context.log.debug,\n      )\n\n      always_overwrite = self._cache_factory.overwrite()\n\n      # Cache the artifacts.\n      args_tuples = []\n      for vts, artifactfiles in vts_artifactfiles_pairs:\n        overwrite = always_overwrite or vts.cache_key in self._cache_key_errors\n        args_tuples.append((cache, vts.cache_key, artifactfiles, overwrite))\n\n      return Work(lambda x: self.context.subproc_map(call_insert, x), [(args_tuples,)], 'insert')\n    else:\n      return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef require_single_root_target(self):\n    target_roots = self.context.target_roots\n    if len(target_roots) == 0:\n      raise TaskError('No target specified.')\n    elif len(target_roots) > 1:\n      raise TaskError('Multiple targets specified: {}'\n                      .format(', '.join([repr(t) for t in target_roots])))\n    return target_roots[0]", "response": "Returns the single root target if one is specified."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef determine_target_roots(self, goal_name):\n    if not self.context.target_roots:\n      print('WARNING: No targets were matched in goal `{}`.'.format(goal_name), file=sys.stderr)\n\n    # For the v2 path, e.g. `./pants list` is a functional no-op. This matches the v2 mode behavior\n    # of e.g. `./pants --changed-parent=HEAD list` (w/ no changes) returning an empty result.\n    return self.context.target_roots", "response": "Helper for tasks that scan for default target roots."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new archiver based on the given typename.", "response": "def create_archiver(typename):\n  \"\"\"Returns Archivers in common configurations.\n\n  :API: public\n\n  The typename must correspond to one of the following:\n  'tar'   Returns a tar archiver that applies no compression and emits .tar files.\n  'tgz'   Returns a tar archiver that applies gzip compression and emits .tar.gz files.\n  'tbz2'  Returns a tar archiver that applies bzip2 compression and emits .tar.bz2 files.\n  'zip'   Returns a zip archiver that applies standard compression and emits .zip files.\n  'jar'   Returns a jar archiver that applies no compression and emits .jar files.\n    Note this is provided as a light way of zipping input files into a jar, without the\n    need to prepare Manifest etc. For more advanced usages, please refer to :class:\n    `pants.backend.jvm.subsystems.jar_tool.JarTool` or :class:\n    `pants.backend.jvm.tasks.jar_task.JarTask`.\n  \"\"\"\n  archiver = _ARCHIVER_BY_TYPE.get(typename)\n  if not archiver:\n    raise ValueError('No archiver registered for {!r}'.format(typename))\n  return archiver"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an Archiver for the given path name.", "response": "def archiver_for_path(path_name):\n  \"\"\"Returns an Archiver for the given path name.\n\n  :API: public\n\n  :param string path_name: The path name of the archive - need not exist.\n  :raises: :class:`ValueError` If the path name does not uniquely identify a supported archive type.\n  \"\"\"\n  if path_name.endswith('.tar.gz'):\n    return TGZ\n  elif path_name.endswith('.tar.bz2'):\n    return TBZ2\n  else:\n    _, ext = os.path.splitext(path_name)\n    if ext:\n      ext = ext[1:]  # Trim leading '.'.\n    if not ext:\n      raise ValueError('Could not determine archive type of path {}'.format(path_name))\n    return create_archiver(ext)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract(self, path, outdir, concurrency_safe=False, **kwargs):\n    if concurrency_safe:\n      with temporary_dir() as temp_dir:\n        self._extract(path, temp_dir, **kwargs)\n        safe_concurrent_rename(temp_dir, outdir)\n    else:\n      # Leave the existing default behavior unchanged and allows overlay of contents.\n      self._extract(path, outdir, **kwargs)", "response": "Extracts an archive s contents to the specified outdir."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create(self, basedir, outdir, name, prefix=None, dereference=True):\n\n    basedir = ensure_text(basedir)\n    tarpath = os.path.join(outdir, '{}.{}'.format(ensure_text(name), self.extension))\n    with open_tar(tarpath, self.mode, dereference=dereference, errorlevel=1) as tar:\n      tar.add(basedir, arcname=prefix or '.')\n    return tarpath", "response": "Create a new tar file with the given name and directory."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _invoke_xz(self, xz_input_file):\n    # TODO: --threads=0 is supposed to use \"the number of processor cores on the machine\", but I\n    # see no more than 100% cpu used at any point. This seems like it could be a bug? If performance\n    # is an issue, investigate further.\n    cmd = [self._xz_binary_path, '--decompress', '--stdout', '--keep', '--threads=0', xz_input_file]\n\n    try:\n      # Pipe stderr to our own stderr, but leave stdout open so we can yield it.\n      process = subprocess.Popen(\n        cmd,\n        stdout=subprocess.PIPE,\n        stderr=sys.stderr)\n    except OSError as e:\n      raise self.XZArchiverError(\n        \"Error invoking xz with command {} for input file {}: {}\"\n        .format(cmd, xz_input_file, e),\n        e)\n\n    # This is a file object.\n    yield process.stdout\n\n    rc = process.wait()\n    if rc != 0:\n      raise self.XZArchiverError(\n        \"Error decompressing xz input with command {} for input file {}. Exit code was: {}. \"\n        .format(cmd, xz_input_file, rc))", "response": "Invoke the xz command and yield a file object for its stdout."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _extract(self, path, outdir, filter_func=None):\n    with open_zip(path) as archive_file:\n      for name in archive_file.namelist():\n        # While we're at it, we also perform this safety test.\n        if name.startswith('/') or name.startswith('..'):\n          raise ValueError('Zip file contains unsafe path: {}'.format(name))\n        if (not filter_func or filter_func(name)):\n          archive_file.extract(name, outdir)", "response": "Extract from a zip file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new public .", "response": "def create(self, basedir, outdir, name, prefix=None):\n    \"\"\"\n    :API: public\n    \"\"\"\n    zippath = os.path.join(outdir, '{}.{}'.format(name, self.extension))\n    with open_zip(zippath, 'w', compression=self.compression) as zip:\n      # For symlinks, we want to archive the actual content of linked files but\n      # under the relpath derived from symlink.\n      for root, _, files in safe_walk(basedir, followlinks=True):\n        root = ensure_text(root)\n        for file in files:\n          file = ensure_text(file)\n          full_path = os.path.join(root, file)\n          relpath = os.path.relpath(full_path, basedir)\n          if prefix:\n            relpath = os.path.join(ensure_text(prefix), relpath)\n          zip.write(full_path, relpath)\n    return zippath"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register_options(cls, register):\n    super(JvmResolverBase, cls).register_options(register)\n    # TODO This flag should be defaulted to True when we are doing hermetic execution,\n    # and should probably go away as we move forward into that direction.\n    register('--capture-snapshots', type=bool, default=False,\n      help='Enable capturing snapshots to add directory digests to dependency jars.'\n           'Note that this is necessary when hermetic execution is enabled.')", "response": "Register an option to make capturing snapshots optional."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_directory_digests_for_jars(self, targets_and_jars):\n\n    targets_and_jars=list(targets_and_jars)\n\n    if not targets_and_jars or not self.get_options().capture_snapshots:\n      return targets_and_jars\n\n    jar_paths = []\n    for target, jars_to_snapshot in targets_and_jars:\n      for jar in jars_to_snapshot:\n        jar_paths.append(fast_relpath(jar.pants_path, get_buildroot()))\n\n    snapshots = self.context._scheduler.capture_snapshots(\n      tuple(\n        PathGlobsAndRoot(PathGlobs([jar]), get_buildroot()) for jar in jar_paths\n      ))\n\n    # We want to map back the list[Snapshot] to targets_and_jars\n    # We assume that (1) jars_to_snapshot has the same number of ResolveJars as snapshots does Snapshots,\n    # and that (2) capture_snapshots preserves ordering.\n    digests = [snapshot.directory_digest for snapshot in snapshots]\n    digest_iterator = iter(digests)\n\n    snapshotted_targets_and_jars = []\n    for target, jars_to_snapshot in targets_and_jars:\n      snapshotted_jars = [ResolvedJar(coordinate=jar.coordinate,\n                                      cache_path=jar.cache_path,\n                                      pants_path=jar.pants_path,\n                                      directory_digest=next(digest_iterator)) for jar in jars_to_snapshot]\n      snapshotted_targets_and_jars.append((target, snapshotted_jars))\n\n    return snapshotted_targets_and_jars", "response": "Given a list of targets and jars add the directory digests for each target."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_info(self, key, val, ignore_errors=False):\n    self.add_infos((key, val), ignore_errors=ignore_errors)", "response": "Adds the given info and returns a dict composed of just this added info."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_infos(self, *keyvals, **kwargs):\n    kv_pairs = []\n    for key, val in keyvals:\n      key = key.strip()\n      val = str(val).strip()\n      if ':' in key:\n        raise ValueError('info key \"{}\" must not contain a colon.'.format(key))\n      kv_pairs.append((key, val))\n\n    for k, v in kv_pairs:\n      if k in self._info:\n        raise ValueError('info key \"{}\" already exists with value {}. '\n                         'Cannot add it again with value {}.'.format(k, self._info[k], v))\n      self._info[k] = v\n\n    try:\n      with open(self._info_file, 'a') as outfile:\n        for k, v in kv_pairs:\n          outfile.write('{}: {}\\n'.format(k, v))\n    except IOError:\n      if not kwargs.get('ignore_errors', False):\n        raise", "response": "Adds the given info and returns a dict composed of just this added info."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds basic build info.", "response": "def add_basic_info(self, run_id, timestamp):\n    \"\"\"Adds basic build info.\"\"\"\n    datetime = time.strftime('%A %b %d, %Y %H:%M:%S', time.localtime(timestamp))\n    user = getpass.getuser()\n    machine = socket.gethostname()\n    buildroot = get_buildroot()\n    # TODO: Get rid of the redundant 'path' key once everyone is off it.\n    self.add_infos(('id', run_id), ('timestamp', timestamp), ('datetime', datetime),\n                   ('user', user), ('machine', machine), ('path', buildroot),\n                   ('buildroot', buildroot), ('version', version.VERSION))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_scm_info(self):\n    scm = get_scm()\n    if scm:\n      revision = scm.commit_id\n      branch = scm.branch_name or revision\n    else:\n      revision, branch = 'none', 'none'\n    self.add_infos(('revision', revision), ('branch', branch))", "response": "Adds SCM - related info."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_python_thrift_library_sources(self, py_thrift_targets):\n    target_snapshots = OrderedDict(\n      (t, t.sources_snapshot(scheduler=self.context._scheduler).directory_digest)\n      for t in py_thrift_targets)\n    filescontent_by_target = OrderedDict(zip(\n      target_snapshots.keys(),\n      self.context._scheduler.product_request(FilesContent, target_snapshots.values())))\n    thrift_file_sources_by_target = OrderedDict(\n      (t, [(file_content.path, file_content.content) for file_content in all_content.dependencies])\n      for t, all_content in filescontent_by_target.items())\n    return thrift_file_sources_by_target", "response": "Get file contents for python thrift library targets."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _extract_all_python_namespaces(self, thrift_file_sources_by_target):\n    py_namespaces_by_target = OrderedDict()\n    failing_py_thrift_by_target = defaultdict(list)\n    for t, all_content in thrift_file_sources_by_target.items():\n      py_namespaces_by_target[t] = []\n      for (path, content) in all_content:\n        try:\n          py_namespaces_by_target[t].append(\n            # File content is provided as a binary string, so we have to decode it.\n            (path, self._extract_py_namespace_from_content(t, path, content.decode('utf-8'))))\n        except self.NamespaceParseFailure:\n          failing_py_thrift_by_target[t].append(path)\n\n    if failing_py_thrift_by_target:\n      # We dump the output to a file here because the output can be very long in some repos.\n      no_py_namespace_output_file = os.path.join(self.workdir, 'no-python-namespace-output.txt')\n\n      pretty_printed_failures = '\\n'.join(\n        '{}: [{}]'.format(t.address.spec, ', '.join(paths))\n        for t, paths in failing_py_thrift_by_target.items())\n      error = self.NamespaceExtractionError(no_py_namespace_output_file, \"\"\"\\\nPython namespaces could not be extracted from some thrift sources. Declaring a `namespace py` in\nthrift sources for python thrift library targets will soon become required.\n\n{} python library target(s) contained thrift sources not declaring a python namespace. The targets\nand/or files which need to be edited will be dumped to: {}\n\"\"\".format(len(failing_py_thrift_by_target), no_py_namespace_output_file))\n\n      safe_file_dump(no_py_namespace_output_file, '{}\\n'.format(pretty_printed_failures))\n\n      if self.get_options().strict:\n        raise error\n      else:\n        self.context.log.warn(error)\n    return py_namespaces_by_target", "response": "Extracts the python namespace from each thrift source file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking for any overlapping namespaces.", "response": "def _determine_clashing_namespaces(self, py_namespaces_by_target):\n    \"\"\"Check for any overlapping namespaces.\"\"\"\n    namespaces_by_files = defaultdict(list)\n    for target, all_namespaces in py_namespaces_by_target.items():\n      for (path, namespace) in all_namespaces:\n        namespaces_by_files[namespace].append((target, path))\n\n    clashing_namespaces = {\n      namespace: all_paths\n      for namespace, all_paths in namespaces_by_files.items()\n      if len(all_paths) > 1\n    }\n    if clashing_namespaces:\n      pretty_printed_clashing = '\\n'.join(\n        '{}: [{}]'\n        .format(\n          namespace,\n          ', '.join('({}, {})'.format(t.address.spec, path) for (t, path) in all_paths))\n        for namespace, all_paths in clashing_namespaces.items())\n      error = self.ClashingNamespaceError(\"\"\"\\\nClashing namespaces for python thrift library sources detected in build graph. This will silently\noverwrite previously generated python sources with generated sources from thrift files declaring the\nsame python namespace. This is an upstream WONTFIX in thrift, see:\n      https://issues.apache.org/jira/browse/THRIFT-515\nErrors:\n{}\n\"\"\".format(pretty_printed_clashing))\n      if self.get_options().strict:\n        raise error\n      else:\n        self.context.log.warn(error)\n    return namespaces_by_files"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef close(self):\n    self.emit(self.generate_epilog(self.settings), dest=ReporterDestination.ERR)", "response": "Implementation of Reporter callback."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start_workunit(self, workunit):\n    if not self.is_under_main_root(workunit):\n      return\n\n    label_format = self._get_label_format(workunit)\n\n    if label_format == LabelFormat.FULL:\n      if not WorkUnitLabel.SUPPRESS_LABEL in workunit.labels:\n        self._emit_indented_workunit_label(workunit)\n      # Start output on a new line.\n      tool_output_format = self._get_tool_output_format(workunit)\n      if tool_output_format == ToolOutputFormat.INDENT:\n        self.emit(self._prefix(workunit, '\\n'))\n      elif tool_output_format == ToolOutputFormat.UNINDENTED:\n        self.emit('\\n')\n    elif label_format == LabelFormat.DOT:\n      self.emit('.')\n\n    self.flush()", "response": "Implementation of Reporter callback."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef end_workunit(self, workunit):\n    if not self.is_under_main_root(workunit):\n      return\n\n    if workunit.outcome() != WorkUnit.SUCCESS and not self._show_output(workunit):\n      # Emit the suppressed workunit output, if any, to aid in debugging the problem.\n      if self._get_label_format(workunit) != LabelFormat.FULL:\n        self._emit_indented_workunit_label(workunit)\n      for name, outbuf in workunit.outputs().items():\n        self.emit(self._prefix(workunit, '\\n==== {} ====\\n'.format(name)))\n        self.emit(self._prefix(workunit, outbuf.read_from(0).decode('utf-8')))\n        self.flush()", "response": "Implementation of Reporter callback."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_handle_log(self, workunit, level, *msg_elements):\n    if not self.is_under_main_root(workunit):\n      return\n\n    # If the element is a (msg, detail) pair, we ignore the detail. There's no\n    # useful way to display it on the console.\n    elements = [e if isinstance(e, six.string_types) else e[0] for e in msg_elements]\n    msg = '\\n' + ''.join(elements)\n    if self.use_color_for_workunit(workunit, self.settings.color):\n      msg = self._COLOR_BY_LEVEL.get(level, lambda x: x)(msg)\n\n    self.emit(self._prefix(workunit, msg))\n    self.flush()", "response": "Implementation of Reporter callback."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_output(self, workunit, label, s):\n    if not self.is_under_main_root(workunit):\n      return\n    tool_output_format = self._get_tool_output_format(workunit)\n    if tool_output_format == ToolOutputFormat.INDENT:\n      self.emit(self._prefix(workunit, s))\n    elif tool_output_format == ToolOutputFormat.UNINDENTED:\n      self.emit(s)\n    self.flush()", "response": "Implementation of Reporter callback."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nguessing the package name from the path.", "response": "def _guess_package(self, path):\n    \"\"\"Used in execute_codegen to actually invoke the compiler with the proper arguments, and in\n    _sources_to_be_generated to declare what the generated files will be.\n    \"\"\"\n    supported_prefixes = ('com', 'org', 'net',)\n    package = ''\n    slash = path.rfind(os.path.sep)\n    prefix_with_slash = max(path.rfind(os.path.join('', prefix, ''))\n                            for prefix in supported_prefixes)\n    if prefix_with_slash < 0:\n      package = path[:slash]\n    elif prefix_with_slash >= 0:\n      package = path[prefix_with_slash:slash]\n    package = package.replace(os.path.sep, ' ')\n    package = package.strip().replace(' ', '.')\n    return package"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload a Digest from a. digest file adjacent to the given directory.", "response": "def load(cls, directory):\n    \"\"\"Load a Digest from a `.digest` file adjacent to the given directory.\n\n    :return: A Digest, or None if the Digest did not exist.\n    \"\"\"\n    read_file = maybe_read_file(cls._path(directory))\n    if read_file:\n      fingerprint, length = read_file.split(':')\n      return Digest(fingerprint, int(length))\n    else:\n      return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndump this Digest object to the given directory.", "response": "def dump(self, directory):\n    \"\"\"Dump this Digest object adjacent to the given directory.\"\"\"\n    payload = '{}:{}'.format(self.fingerprint, self.serialized_bytes_length)\n    safe_file_dump(self._path(directory), payload=payload)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a Snapshot containing the sources relative to the build root.", "response": "def snapshot(self, scheduler=None):\n    \"\"\"\n    Returns a Snapshot containing the sources, relative to the build root.\n\n    This API is experimental, and subject to change.\n    \"\"\"\n    if isinstance(self._sources, EagerFilesetWithSpec):\n      snapshot = self._sources.snapshot\n      if snapshot is not None:\n        return snapshot\n    input_pathglobs = PathGlobs(tuple(self.relative_to_buildroot()))\n    return scheduler.product_request(Snapshot, [input_pathglobs])[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef combine_hashes(hashes):\n  hasher = sha1()\n  for h in sorted(hashes):\n    h = ensure_binary(h)\n    hasher.update(h)\n  return hasher.hexdigest() if PY3 else hasher.hexdigest().decode('utf-8')", "response": "A simple helper function to combine other hashes. Sorts the hashes before rolling them in."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fingerprint(self):\n    if self._fingerprint_memo is None:\n      self._fingerprint_memo = self._compute_fingerprint()\n    return self._fingerprint_memo", "response": "A memoized sha1 hexdigest hashing the contents of this PayloadField returning either a string or None."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_failed_targets(test_registry, junit_xml_path, error_handler):\n  failed_targets = defaultdict(set)\n\n  def parse_junit_xml_file(path):\n    try:\n      xml = XmlParser.from_file(path)\n      failures = int(xml.get_attribute('testsuite', 'failures'))\n      errors = int(xml.get_attribute('testsuite', 'errors'))\n      if failures or errors:\n        for testcase in xml.parsed.getElementsByTagName('testcase'):\n          test_failed = testcase.getElementsByTagName('failure')\n          test_errored = testcase.getElementsByTagName('error')\n          if test_failed or test_errored:\n            test = Test(classname=testcase.getAttribute('classname'),\n                        methodname=testcase.getAttribute('name'))\n            target = test_registry.get_owning_target(test)\n            failed_targets[target].add(test)\n    except (XmlParser.XmlError, ValueError) as e:\n      error_handler(ParseError(path, e))\n\n  if os.path.isdir(junit_xml_path):\n    for root, _, files in safe_walk(junit_xml_path):\n      for junit_xml_file in fnmatch.filter(files, 'TEST-*.xml'):\n        parse_junit_xml_file(os.path.join(root, junit_xml_file))\n  else:\n    parse_junit_xml_file(junit_xml_path)\n\n  return dict(failed_targets)", "response": "Parses the junit xml reports and maps targets to sets of failed tests."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef install(self, goal=None, first=False, replace=False, before=None, after=None):\n    goal = Goal.by_name(goal or self.name)\n    goal.install(self, first, replace, before, after)\n    return goal", "response": "Installs the task in the specified goal."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef injectables_specs_for_key(self, key):\n    mapping = self.injectables_spec_mapping\n    if key not in mapping:\n      raise self.NoMappingForKey(key)\n    specs = mapping[key]\n    assert isinstance(specs, list), (\n      'invalid `injectables_spec_mapping` on {!r} for key \"{}\". '\n      'expected a `list` but instead found a `{}`: {}'\n    ).format(self, key, type(specs), specs)\n    return [Address.parse(s).spec for s in specs]", "response": "Given a key yield all relevant injectable spec addresses."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a key yield a singular spec representing that key.", "response": "def injectables_spec_for_key(self, key):\n    \"\"\"Given a key, yield a singular spec representing that key.\n\n    :API: public\n    \"\"\"\n    specs = self.injectables_specs_for_key(key)\n    specs_len = len(specs)\n    if specs_len == 0:\n      return None\n    if specs_len != 1:\n      raise self.TooManySpecsForKey('injectables spec mapping for key included {} elements, '\n                                    'expected 1'.format(specs_len))\n    return specs[0]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn true if the task should be entirely skipped.", "response": "def _skip(options):\n    \"\"\"Return true if the task should be entirely skipped, and thus have no product requirements.\"\"\"\n    values = [options.missing_direct_deps, options.unnecessary_deps]\n    return all(v == 'off' for v in values)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check(self, src_tgt, actual_deps):\n    if self._check_missing_direct_deps or self._check_unnecessary_deps:\n      missing_file_deps, missing_direct_tgt_deps = \\\n        self._compute_missing_deps(src_tgt, actual_deps)\n\n      buildroot = get_buildroot()\n\n      def shorten(path):  # Make the output easier to read.\n        if path.startswith(buildroot):\n          return os.path.relpath(path, buildroot)\n        return path\n\n      def filter_whitelisted(missing_deps):\n        # Removing any targets that exist in the whitelist from the list of dependency issues.\n        return [(tgt_pair, evidence) for (tgt_pair, evidence) in missing_deps\n                            if tgt_pair[0].address not in self._target_whitelist]\n\n      missing_direct_tgt_deps = filter_whitelisted(missing_direct_tgt_deps)\n\n      if self._check_missing_direct_deps and missing_direct_tgt_deps:\n        log_fn = (self.context.log.error if self._check_missing_direct_deps == 'fatal'\n                  else self.context.log.warn)\n        for (tgt_pair, evidence) in missing_direct_tgt_deps:\n          evidence_str = '\\n'.join(['  {} uses {}'.format(shorten(e[0]), shorten(e[1]))\n                                    for e in evidence])\n          log_fn('Missing direct BUILD dependency {} -> {} because:\\n{}'\n                 .format(tgt_pair[0].address.spec, tgt_pair[1].address.spec, evidence_str))\n        if self._check_missing_direct_deps == 'fatal':\n          raise TaskError('Missing direct deps.')\n\n      if self._check_unnecessary_deps:\n        log_fn = (self.context.log.error if self._check_unnecessary_deps == 'fatal'\n                  else self.context.log.warn)\n        had_unused = self._do_check_unnecessary_deps(src_tgt, actual_deps, log_fn)\n        if had_unused and self._check_unnecessary_deps == 'fatal':\n          raise TaskError('Unnecessary deps.')", "response": "Check for missing deps."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_missing_deps(self, src_tgt, actual_deps):\n    analyzer = self._analyzer\n    def must_be_explicit_dep(dep):\n      # We don't require explicit deps on the java runtime, so we shouldn't consider that\n      # a missing dep.\n      return (dep not in analyzer.bootstrap_jar_classfiles\n              and not dep.startswith(DistributionLocator.cached().real_home))\n\n    def target_or_java_dep_in_targets(target, targets):\n      # We want to check if the target is in the targets collection\n      #\n      # However, for the special case of scala_library that has a java_sources\n      # reference we're ok if that exists in targets even if the scala_library does not.\n\n      if target in targets:\n        return True\n      elif isinstance(target, ScalaLibrary):\n        return any(t in targets for t in target.java_sources)\n      else:\n        return False\n\n    # Find deps that are actual but not specified.\n    missing_file_deps = OrderedSet()  # (src, src).\n    missing_direct_tgt_deps_map = defaultdict(list)  # The same, but for direct deps.\n\n    targets_by_file = analyzer.targets_by_file(self.context.targets())\n    buildroot = get_buildroot()\n    abs_srcs = [os.path.join(buildroot, src) for src in src_tgt.sources_relative_to_buildroot()]\n    for src in abs_srcs:\n      for actual_dep in filter(must_be_explicit_dep, actual_deps.get(src, [])):\n        actual_dep_tgts = targets_by_file.get(actual_dep)\n        # actual_dep_tgts is usually a singleton. If it's not, we only need one of these\n        # to be in our declared deps to be OK.\n        if actual_dep_tgts is None:\n          missing_file_deps.add((src_tgt, actual_dep))\n        elif not target_or_java_dep_in_targets(src_tgt, actual_dep_tgts):\n          # Obviously intra-target deps are fine.\n          canonical_actual_dep_tgt = next(iter(actual_dep_tgts))\n          if canonical_actual_dep_tgt not in src_tgt.dependencies:\n            # The canonical dep is the only one a direct dependency makes sense on.\n            missing_direct_tgt_deps_map[(src_tgt, canonical_actual_dep_tgt)].append(\n                (src, actual_dep))\n\n    return (list(missing_file_deps),\n            list(missing_direct_tgt_deps_map.items()))", "response": "Computes missing deps that are not specified in a BUILD file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compute_unnecessary_deps(self, target, actual_deps):\n    # Flatten the product deps of this target.\n    product_deps = set()\n    for dep_entries in actual_deps.values():\n      product_deps.update(dep_entries)\n\n    # Determine which of the DEFAULT deps in the declared set of this target were used.\n    used = set()\n    unused = set()\n    for dep, _ in self._analyzer.resolve_aliases(target, scope=Scopes.DEFAULT):\n      if dep in used or dep in unused:\n        continue\n      # TODO: What's a better way to accomplish this check? Filtering by `has_sources` would\n      # incorrectly skip \"empty\" `*_library` targets, which could then be used as a loophole.\n      if isinstance(dep, (Resources, UnpackedJars)):\n        continue\n      # If any of the target's jars or classfiles were used, consider it used.\n      if product_deps.isdisjoint(self._analyzer.files_for_target(dep)):\n        unused.add(dep)\n      else:\n        used.add(dep)\n\n    # If there were no unused deps, break.\n    if not unused:\n      return {}\n\n    # For any deps that were used, count their derived-from targets used as well.\n    # TODO: Refactor to do some of this above once tests are in place.\n    for dep in list(used):\n      for derived_from in dep.derived_from_chain:\n        if derived_from in unused:\n          unused.remove(derived_from)\n          used.add(derived_from)\n\n    # Prune derived targets that would be in the set twice.\n    for dep in list(unused):\n      if set(dep.derived_from_chain) & unused:\n        unused.remove(dep)\n\n    if not unused:\n      return {}\n\n    # For any deps that were not used, determine whether their transitive deps were used, and\n    # recommend those as replacements.\n    replacements = {}\n    for dep in unused:\n      replacements[dep] = set()\n      for t in dep.closure():\n        if t in used or t in unused:\n          continue\n        if not product_deps.isdisjoint(self._analyzer.files_for_target(t)):\n          replacements[dep].add(t.concrete_derived_from)\n\n    return replacements", "response": "Computes the unnecessary dependencies for the given Target."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind all thrift files included by the given thrift source file.", "response": "def find_includes(basedirs, source, log=None):\n  \"\"\"Finds all thrift files included by the given thrift source.\n\n  :basedirs: A set of thrift source file base directories to look for includes in.\n  :source: The thrift source file to scan for includes.\n  :log: An optional logger\n  \"\"\"\n\n  all_basedirs = [os.path.dirname(source)]\n  all_basedirs.extend(basedirs)\n\n  includes = set()\n  with open(source, 'r') as thrift:\n    for line in thrift.readlines():\n      match = INCLUDE_PARSER.match(line)\n      if match:\n        capture = match.group(1)\n        added = False\n        for basedir in all_basedirs:\n          include = os.path.join(basedir, capture)\n          if os.path.exists(include):\n            if log:\n              log.debug('{} has include {}'.format(source, include))\n            includes.add(include)\n            added = True\n        if not added:\n          raise ValueError(\"{} included in {} not found in bases {}\"\n                           .format(include, source, all_basedirs))\n  return includes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the root thrift files in the graph formed by sources and their recursive includes.", "response": "def find_root_thrifts(basedirs, sources, log=None):\n  \"\"\"Finds the root thrift files in the graph formed by sources and their recursive includes.\n\n  :basedirs: A set of thrift source file base directories to look for includes in.\n  :sources: Seed thrift files to examine.\n  :log: An optional logger.\n  \"\"\"\n\n  root_sources = set(sources)\n  for source in sources:\n    root_sources.difference_update(find_includes(basedirs, source, log=log))\n  return root_sources"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calculate_include_paths(targets, is_thrift_target):\n\n  basedirs = set()\n  def collect_paths(target):\n    basedirs.add(target.target_base)\n\n  for target in targets:\n    target.walk(collect_paths, predicate=is_thrift_target)\n  return basedirs", "response": "Calculates the set of import paths for the given targets."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlooking for jars in the distribution lib folder ( s.", "response": "def find_libs(self, names):\n    \"\"\"Looks for jars in the distribution lib folder(s).\n\n    If the distribution is a JDK, both the `lib` and `jre/lib` dirs will be scanned.\n    The endorsed and extension dirs are not checked.\n\n    :param list names: jar file names\n    :return: list of paths to requested libraries\n    :raises: `Distribution.Error` if any of the jars could not be found.\n    \"\"\"\n    def collect_existing_libs():\n      def lib_paths():\n        yield os.path.join(self.home, 'lib')\n        if self.jdk:\n          yield os.path.join(self.home, 'jre', 'lib')\n\n      for name in names:\n        for path in lib_paths():\n          lib_path = os.path.join(path, name)\n          if os.path.exists(lib_path):\n            yield lib_path\n            break\n        else:\n          raise Distribution.Error('Failed to locate {} library'.format(name))\n\n    return list(collect_existing_libs())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef home(self):\n    if not self._home:\n      home = self._get_system_properties(self.java)['java.home']\n      # The `jre/bin/java` executable in a JDK distribution will report `java.home` as the jre dir,\n      # so we check for this and re-locate to the containing jdk dir when present.\n      if os.path.basename(home) == 'jre':\n        jdk_dir = os.path.dirname(home)\n        if self._is_executable(os.path.join(jdk_dir, 'bin', 'javac')):\n          home = jdk_dir\n      self._home = home\n    return text_type(self._home)", "response": "Returns the distribution JAVA_HOME."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef binary(self, name):\n    if not isinstance(name, str):\n      raise ValueError('name must be a binary name, given {} of type {}'.format(name, type(name)))\n    self.validate()\n    return self._validated_executable(name)", "response": "Returns the path to the executable of the given name for this distribution."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate this distribution against its configured constraints.", "response": "def validate(self):\n    \"\"\"Validates this distribution against its configured constraints.\n\n    Raises Distribution.Error if this distribution is not valid according to the configured\n    constraints.\n    \"\"\"\n    if self._validated_binaries:\n      return\n\n    with self._valid_executable('java') as java:\n      if self._minimum_version:\n        version = self._get_version(java)\n        if version < self._minimum_version:\n          raise self.Error('The java distribution at {} is too old; expecting at least {} and'\n                           ' got {}'.format(java, self._minimum_version, version))\n      if self._maximum_version:\n        version = self._get_version(java)\n        if version > self._maximum_version:\n          raise self.Error('The java distribution at {} is too new; expecting no older than'\n                           ' {} and got {}'.format(java, self._maximum_version, version))\n\n    # We might be a JDK discovered by the embedded jre `java` executable.\n    # If so reset the bin path to the true JDK home dir for full access to all binaries.\n    self._bin_path = os.path.join(self.home, 'bin')\n\n    try:\n      self._validated_executable('javac')  # Calling purely for the check and cache side effects\n      self._is_jdk = True\n    except self.Error as e:\n      if self._jdk:\n        logger.debug('Failed to validate javac executable. Please check you have a JDK '\n                      'installed. Original error: {}'.format(e))\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _scan_constraint_match(self, minimum_version, maximum_version, jdk):\n\n    for dist in self._cache.values():\n      if minimum_version and dist.version < minimum_version:\n        continue\n      if maximum_version and dist.version > maximum_version:\n        continue\n      if jdk and not dist.jdk:\n        continue\n      return dist", "response": "Searches the cache for a distribution that matches the constraints."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlocates a java distribution that meets the given constraints and returns it.", "response": "def locate(self, minimum_version=None, maximum_version=None, jdk=False):\n    \"\"\"Finds a java distribution that meets the given constraints and returns it.\n\n    First looks for a cached version that was previously located, otherwise calls locate().\n    :param minimum_version: minimum jvm version to look for (eg, 1.7).\n                            The stricter of this and `--jvm-distributions-minimum-version` is used.\n    :param maximum_version: maximum jvm version to look for (eg, 1.7.9999).\n                            The stricter of this and `--jvm-distributions-maximum-version` is used.\n    :param bool jdk: whether the found java distribution is required to have a jdk.\n    :return: the Distribution.\n    :rtype: :class:`Distribution`\n    :raises: :class:`Distribution.Error` if no suitable java distribution could be found.\n    \"\"\"\n\n    def _get_stricter_version(a, b, name, stricter):\n      version_a = _parse_java_version(name, a)\n      version_b = _parse_java_version(name, b)\n      if version_a is None:\n        return version_b\n      if version_b is None:\n        return version_a\n      return stricter(version_a, version_b)\n\n    # Take the tighter constraint of method args and subsystem options.\n    minimum_version = _get_stricter_version(minimum_version,\n                                            self._minimum_version,\n                                            \"minimum_version\",\n                                            max)\n    maximum_version = _get_stricter_version(maximum_version,\n                                            self._maximum_version,\n                                            \"maximum_version\",\n                                            min)\n\n    key = (minimum_version, maximum_version, jdk)\n    dist = self._cache.get(key)\n    if not dist:\n      dist = self._scan_constraint_match(minimum_version, maximum_version, jdk)\n      if not dist:\n        dist = self._locate(minimum_version=minimum_version,\n                            maximum_version=maximum_version,\n                            jdk=jdk)\n      self._cache[key] = dist\n    return dist"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlocates a java distribution that meets any given constraints and returns it.", "response": "def _locate(self, minimum_version=None, maximum_version=None, jdk=False):\n    \"\"\"Finds a java distribution that meets any given constraints and returns it.\n\n    :param minimum_version: minimum jvm version to look for (eg, 1.7).\n    :param maximum_version: maximum jvm version to look for (eg, 1.7.9999).\n    :param bool jdk: whether the found java distribution is required to have a jdk.\n    :return: the located Distribution.\n    :rtype: :class:`Distribution`\n    :raises: :class:`Distribution.Error` if no suitable java distribution could be found.\n    \"\"\"\n    for location in itertools.chain(self._distribution_environment.jvm_locations):\n      try:\n        dist = Distribution(home_path=location.home_path,\n                            bin_path=location.bin_path,\n                            minimum_version=minimum_version,\n                            maximum_version=maximum_version,\n                            jdk=jdk)\n        dist.validate()\n        logger.debug('Located {} for constraints: minimum_version {}, maximum_version {}, jdk {}'\n                     .format(dist, minimum_version, maximum_version, jdk))\n        return dist\n      except (ValueError, Distribution.Error) as e:\n        logger.debug('{} is not a valid distribution because: {}'\n                     .format(location.home_path, str(e)))\n        pass\n\n    if (minimum_version is not None\n        and maximum_version is not None\n        and maximum_version < minimum_version):\n      error_format = ('Pants configuration/options led to impossible constraints for {} '\n                      'distribution: minimum_version {}, maximum_version {}')\n    else:\n      error_format = ('Failed to locate a {} distribution with minimum_version {}, '\n                      'maximum_version {}')\n    raise self.Error(error_format.format('JDK' if jdk else 'JRE', minimum_version, maximum_version))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cached(cls, minimum_version=None, maximum_version=None, jdk=False):\n    try:\n      return cls.global_instance()._locator().locate(\n          minimum_version=minimum_version,\n          maximum_version=maximum_version,\n          jdk=jdk)\n    except _Locator.Error as e:\n      raise cls.Error('Problem locating a java distribution: {}'.format(e))", "response": "Returns a cached version of the java distribution that meets the given constraints and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_build_flags(cls, build_flags_from_option, is_flagged, target):\n    # If self.get_options().build_flags returns a quoted string, remove the outer quotes,\n    # which happens for flags passed from the command-line.\n    if (build_flags_from_option.startswith('\\'') and build_flags_from_option.endswith('\\'')) or \\\n        (build_flags_from_option.startswith('\"') and build_flags_from_option.endswith('\"')):\n      bfo = build_flags_from_option[1:-1]\n    else:\n      bfo = build_flags_from_option\n    global_build_flags, ephemeral_build_flags = ('', bfo) if is_flagged else (bfo, '')\n    target_build_flags = target.build_flags if getattr(target, 'build_flags', None) else ''\n    joined_build_flags = ' '.join([global_build_flags, target_build_flags, ephemeral_build_flags])\n    return cls._split_build_flags(joined_build_flags)", "response": "Merge build flags with global < target < command - line order\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating and execute a go install command.", "response": "def _go_install(self, target, gopath, build_flags):\n    \"\"\"Create and execute a `go install` command.\"\"\"\n    args = build_flags + [target.import_path]\n    result, go_cmd = self.go_dist.execute_go_cmd(\n      'install', gopath=gopath, args=args,\n      workunit_factory=self.context.new_workunit,\n      workunit_name='install {}'.format(target.import_path),\n      workunit_labels=[WorkUnitLabel.COMPILER])\n    if result != 0:\n      raise TaskError('{} failed with exit code {}'.format(go_cmd, result))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef target_types_by_alias(self):\n    target_types_by_alias = defaultdict(set)\n    for alias, target_type in self.target_types.items():\n      target_types_by_alias[alias].add(target_type)\n    for alias, target_macro_factory in self.target_macro_factories.items():\n      target_types_by_alias[alias].update(target_macro_factory.target_types)\n    return dict(target_types_by_alias)", "response": "Returns a mapping from target alias to the target types produced for that alias."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef merge(self, other):\n    if not isinstance(other, BuildFileAliases):\n      raise TypeError('Can only merge other BuildFileAliases, given {0}'.format(other))\n\n    def merge(*items):\n      merged = {}\n      for item in items:\n        merged.update(item)\n      return merged\n\n    targets = merge(self.target_types, self.target_macro_factories,\n                    other.target_types, other.target_macro_factories)\n    objects = merge(self.objects, other.objects)\n    context_aware_object_factories=merge(self.context_aware_object_factories,\n                                         other.context_aware_object_factories)\n    return BuildFileAliases(targets=targets,\n                            objects=objects,\n                            context_aware_object_factories=context_aware_object_factories)", "response": "Merges two sets of build file aliases and returns a new set of aliases containing both."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncompiles a python target.", "response": "def _compile_target(self, vt):\n    \"\"\"'Compiles' a python target.\n\n    'Compiling' means forming an isolated chroot of its sources and transitive deps and then\n    attempting to import each of the target's sources in the case of a python library or else the\n    entry point in the case of a python binary.\n\n    For a library with sources lib/core.py and lib/util.py a \"compiler\" main file would look like:\n\n      if __name__ == '__main__':\n        import lib.core\n        import lib.util\n\n    For a binary with entry point lib.bin:main the \"compiler\" main file would look like:\n\n      if __name__ == '__main__':\n        from lib.bin import main\n\n    In either case the main file is executed within the target chroot to reveal missing BUILD\n    dependencies.\n    \"\"\"\n    target = vt.target\n    with self.context.new_workunit(name=target.address.spec):\n      modules = self._get_modules(target)\n      if not modules:\n        # Nothing to eval, so a trivial compile success.\n        return 0\n\n      interpreter = self._get_interpreter_for_target_closure(target)\n      reqs_pex = self._resolve_requirements_for_versioned_target_closure(interpreter, vt)\n      srcs_pex = self._source_pex_for_versioned_target_closure(interpreter, vt)\n\n      # Create the executable pex.\n      exec_pex_parent = os.path.join(self.workdir, 'executable_pex')\n      executable_file_content = self._get_executable_file_content(exec_pex_parent, modules)\n\n      hasher = hashlib.sha1()\n      hasher.update(reqs_pex.path().encode('utf-8'))\n      hasher.update(srcs_pex.path().encode('utf-8'))\n      hasher.update(executable_file_content.encode('utf-8'))\n      exec_file_hash = hasher.hexdigest()\n      exec_pex_path = os.path.realpath(os.path.join(exec_pex_parent, exec_file_hash))\n      if not os.path.isdir(exec_pex_path):\n        with safe_concurrent_creation(exec_pex_path) as safe_path:\n          # Write the entry point.\n          safe_mkdir(safe_path)\n          with open(os.path.join(safe_path, '{}.py'.format(self._EXEC_NAME)), 'w') as outfile:\n            outfile.write(executable_file_content)\n          pex_info = (target.pexinfo if isinstance(target, PythonBinary) else None) or PexInfo()\n          # Override any user-specified entry point, under the assumption that the\n          # executable_file_content does what the user intends (including, probably, calling that\n          # underlying entry point).\n          pex_info.entry_point = self._EXEC_NAME\n          pex_info.pex_path = ':'.join(pex.path() for pex in (reqs_pex, srcs_pex) if pex)\n          builder = PEXBuilder(safe_path, interpreter, pex_info=pex_info)\n          builder.freeze()\n\n      pex = PEX(exec_pex_path, interpreter)\n\n      with self.context.new_workunit(name='eval',\n                                     labels=[WorkUnitLabel.COMPILER, WorkUnitLabel.RUN,\n                                             WorkUnitLabel.TOOL],\n                                     cmd=' '.join(pex.cmdline())) as workunit:\n        returncode = pex.run(stdout=workunit.output('stdout'), stderr=workunit.output('stderr'))\n        workunit.set_outcome(WorkUnit.SUCCESS if returncode == 0 else WorkUnit.FAILURE)\n        if returncode != 0:\n          self.context.log.error('Failed to eval {}'.format(target.address.spec))\n        return returncode"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a xargs engine that uses subprocess. call to execute the given cmd array with extra arg chunks.", "response": "def subprocess(cls, cmd, **kwargs):\n    \"\"\"Creates an xargs engine that uses subprocess.call to execute the given cmd array with extra\n    arg chunks.\n    \"\"\"\n    def call(args):\n      return subprocess.call(cmd + args, **kwargs)\n    return cls(call)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef execute(self, args):\n    all_args = list(args)\n    try:\n      return self._cmd(all_args)\n    except OSError as e:\n      if errno.E2BIG == e.errno:\n        args1, args2 = self._split_args(all_args)\n        result = self.execute(args1)\n        if result != 0:\n          return result\n        return self.execute(args2)\n      else:\n        raise e", "response": "Executes the configured command passing args in one or more rounds xargs style."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef start_workunit(self, workunit):\n    if workunit.has_label(WorkUnitLabel.GOAL):\n      service_name = \"pants goal\"\n    elif workunit.has_label(WorkUnitLabel.TASK):\n      service_name = \"pants task\"\n    else:\n      service_name = \"pants workunit\"\n\n    # Check if it is the first workunit\n    first_span = not self._workunits_to_spans\n    if first_span:\n      # If trace_id and parent_id are given as flags create zipkin_attrs\n      if self.trace_id is not None and self.parent_id is not None:\n        zipkin_attrs = ZipkinAttrs(\n          # trace_id and parent_id are passed to Pants by another process that collects\n          # Zipkin trace\n          trace_id=self.trace_id,\n          span_id=generate_random_64bit_string(),\n          parent_span_id=self.parent_id,\n          flags='0', # flags: stores flags header. Currently unused\n          is_sampled=True,\n        )\n      else:\n        zipkin_attrs =  create_attrs_for_span(\n          # trace_id is the same as run_uuid that is created in run_tracker and is the part of\n          # pants_run id\n          trace_id=self.trace_id,\n          sample_rate=self.sample_rate, # Value between 0.0 and 100.0\n        )\n        self.trace_id = zipkin_attrs.trace_id\n        # TODO delete this line when parent_id will be passed in v2 engine:\n        #  - with ExecutionRequest when Nodes from v2 engine are called by a workunit;\n        #  - when a v2 engine Node is called by another v2 engine Node.\n        self.parent_id = zipkin_attrs.span_id\n\n      span = zipkin_span(\n        service_name=service_name,\n        span_name=workunit.name,\n        transport_handler=self.handler,\n        encoding=Encoding.V1_THRIFT,\n        zipkin_attrs=zipkin_attrs,\n        span_storage=self.span_storage,\n      )\n    else:\n      span = zipkin_span(\n        service_name=service_name,\n        span_name=workunit.name,\n        span_storage=self.span_storage,\n      )\n    self._workunits_to_spans[workunit] = span\n    span.start()\n    # Goals and tasks save their start time at the beginning of their run.\n    # This start time is passed to workunit, because the workunit may be created much later.\n    span.start_timestamp = workunit.start_time\n    if first_span and span.zipkin_attrs.is_sampled:\n      span.logging_context.start_timestamp = workunit.start_time", "response": "Implementation of Reporter callback."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_field_value(self, key, default=None):\n    if key in self._fields:\n      payload_field = self._fields[key]\n      if payload_field:\n        return payload_field.value\n    return default", "response": "Retrieves the value in the payload field if the field exists otherwise returns the default."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_fields(self, field_dict):\n    for key, field in field_dict.items():\n      self.add_field(key, field)", "response": "Add a mapping of field names to PayloadField instances."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a field to the Payload.", "response": "def add_field(self, key, field):\n    \"\"\"Add a field to the Payload.\n\n    :API: public\n\n    :param string key:  The key for the field.  Fields can be accessed using attribute access as\n      well as `get_field` using `key`.\n    :param PayloadField field:  A PayloadField instance.  None is an allowable value for `field`,\n      in which case it will be skipped during hashing.\n    \"\"\"\n    if key in self._fields:\n      raise PayloadFieldAlreadyDefinedError(\n        'Key {key} is already set on this payload. The existing field was {existing_field}.'\n        ' Tried to set new field {field}.'\n        .format(key=key, existing_field=self._fields[key], field=field))\n    elif self._frozen:\n      raise PayloadFrozenError(\n        'Payload is frozen, field with key {key} cannot be added to it.'\n        .format(key=key))\n    else:\n      self._fields[key] = field\n      self._fingerprint_memo = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninvalidates memoized fingerprints for this payload.", "response": "def mark_dirty(self):\n    \"\"\"Invalidates memoized fingerprints for this payload.\n\n    Exposed for testing.\n\n    :API: public\n    \"\"\"\n    self._fingerprint_memo_map = {}\n    for field in self._fields.values():\n      field.mark_dirty()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(self):\n    while not self._state.is_terminating:\n      self._maybe_garbage_collect()\n      self._maybe_extend_lease()\n      # Waiting with a timeout in maybe_pause has the effect of waiting until:\n      # 1) we are paused and then resumed\n      # 2) we are terminated (which will break the loop)\n      # 3) the timeout is reached, which will cause us to wake up and check gc/leases\n      self._state.maybe_pause(timeout=10)", "response": "Main service entrypoint. Called via Thread.start() via PantsDaemon.run()."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_canonical_classpath(cls, classpath_products, targets, basedir,\n                                 save_classpath_file=False,\n                                 internal_classpath_only=True,\n                                 excludes=None):\n    \"\"\"Create a stable classpath of symlinks with standardized names.\n\n    By default symlinks are created for each target under `basedir` based on its `target.id`.\n    Unique suffixes are added to further disambiguate classpath products from the same target.\n\n    It also optionally saves the classpath products to be used externally (by intellij plugin),\n    one output file for each target.\n\n    Note calling this function will refresh the symlinks and output files for the target under\n    `basedir` if they exist, but it will NOT delete/cleanup the contents for *other* targets.\n    Caller wants that behavior can make the similar calls for other targets or just remove\n    the `basedir` first.\n\n    :param classpath_products: Classpath products.\n    :param targets: Targets to create canonical classpath for.\n    :param basedir: Directory to create symlinks.\n    :param save_classpath_file: An optional file with original classpath entries that symlinks\n      are created from.\n    :param internal_classpath_only: whether to create symlinks just for internal classpath or\n       all classpath.\n    :param excludes: classpath entries should be excluded.\n\n    :returns: Converted canonical classpath.\n    :rtype: list of strings\n    \"\"\"\n    def delete_old_target_output_files(classpath_prefix):\n      \"\"\"Delete existing output files or symlinks for target.\"\"\"\n      directory, basename = os.path.split(classpath_prefix)\n      pattern = re.compile(r'^{basename}(([0-9]+)(\\.jar)?|classpath\\.txt)$'\n                           .format(basename=re.escape(basename)))\n      files = [filename for filename in os.listdir(directory) if pattern.match(filename)]\n      for rel_path in files:\n        path = os.path.join(directory, rel_path)\n        if os.path.islink(path) or os.path.isfile(path):\n          safe_delete(path)\n\n    def prepare_target_output_folder(basedir, target):\n      \"\"\"Prepare directory that will contain canonical classpath for the target.\n\n      This includes creating directories if it does not already exist, cleaning up\n      previous classpath output related to the target.\n      \"\"\"\n      output_dir = basedir\n      # TODO(peiyu) improve readability once we deprecate the old naming style.\n      # For example, `-` is commonly placed in string format as opposed to here.\n      classpath_prefix_for_target = '{basedir}/{target_id}-'.format(basedir=basedir,\n                                                                    target_id=target.id)\n\n      if os.path.exists(output_dir):\n        delete_old_target_output_files(classpath_prefix_for_target)\n      else:\n        os.makedirs(output_dir)\n      return classpath_prefix_for_target\n\n    excludes = excludes or set()\n    canonical_classpath = []\n    target_to_classpath = ClasspathUtil.classpath_by_targets(targets, classpath_products)\n\n    processed_entries = set()\n    for target, classpath_entries_for_target in target_to_classpath.items():\n      if internal_classpath_only:\n        classpath_entries_for_target = [entry for entry in classpath_entries_for_target\n                                        if ClasspathEntry.is_internal_classpath_entry(entry)]\n      if len(classpath_entries_for_target) > 0:\n        classpath_prefix_for_target = prepare_target_output_folder(basedir, target)\n\n        # Note: for internal targets pants has only one classpath entry, but user plugins\n        # might generate additional entries, for example, build.properties for the target.\n        # Also it's common to have multiple classpath entries associated with 3rdparty targets.\n        for (index, entry) in enumerate(classpath_entries_for_target):\n          if entry.is_excluded_by(excludes):\n            continue\n\n          # Avoid creating symlink for the same entry twice, only the first entry on\n          # classpath will get a symlink. The resulted symlinks as a whole are still stable,\n          # but may have non-consecutive suffixes because the 'missing' ones are those\n          # have already been created symlinks by previous targets.\n          if entry in processed_entries:\n            continue\n          processed_entries.add(entry)\n\n          # Create a unique symlink path by prefixing the base file name with a monotonic\n          # increasing `index` to avoid name collisions.\n          _, ext = os.path.splitext(entry.path)\n          symlink_path = '{}{}{}'.format(classpath_prefix_for_target, index, ext)\n          real_entry_path = os.path.realpath(entry.path)\n          if not os.path.exists(real_entry_path):\n            raise MissingClasspathEntryError('Could not find {realpath} when attempting to link '\n                                             '{src} into {dst}'\n                                             .format(realpath=real_entry_path, src=entry.path, dst=symlink_path))\n\n          os.symlink(real_entry_path, symlink_path)\n          canonical_classpath.append(symlink_path)\n\n        if save_classpath_file:\n          classpath = [entry.path for entry in classpath_entries_for_target]\n          with safe_open('{}classpath.txt'.format(classpath_prefix_for_target), 'w') as classpath_file:\n            classpath_file.write(os.pathsep.join(classpath))\n            classpath_file.write('\\n')\n\n    return canonical_classpath", "response": "Creates a canonical classpath for the given classpath products."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a shallow copy of this ClasspathProducts.", "response": "def copy(self):\n    \"\"\"Returns a copy of this ClasspathProducts.\n\n    Edits to the copy's classpaths or exclude associations will not affect the classpaths or\n    excludes in the original. The copy is shallow though, so edits to the copy's product values\n    will mutate the original's product values.  See `UnionProducts.copy`.\n\n    :API: public\n\n    :rtype: :class:`ClasspathProducts`\n    \"\"\"\n    return ClasspathProducts(pants_workdir=self._pants_workdir,\n                             classpaths=self._classpaths.copy(),\n                             excludes=self._excludes.copy())"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds classpath path elements to the products of all the provided targets.", "response": "def add_for_targets(self, targets, classpath_elements):\n    \"\"\"Adds classpath path elements to the products of all the provided targets.\"\"\"\n    for target in targets:\n      self.add_for_target(target, classpath_elements)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_for_target(self, target, classpath_elements):\n    self._add_elements_for_target(target, self._wrap_path_elements(classpath_elements))", "response": "Adds classpath path elements to the products of the provided target."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding jar classpath elements to the products of the provided targets.", "response": "def add_jars_for_targets(self, targets, conf, resolved_jars):\n    \"\"\"Adds jar classpath elements to the products of the provided targets.\n\n    The resolved jars are added in a way that works with excludes.\n    :param targets: The targets to add the jars for.\n    :param conf: The configuration.\n    :param resolved_jars: A list of ResolvedJars.\n    \"\"\"\n    classpath_entries = []\n    for jar in resolved_jars:\n      if not jar.pants_path:\n        raise TaskError('Jar: {!s} has no specified path.'.format(jar.coordinate))\n      cp_entry = ArtifactClasspathEntry(jar.pants_path, jar.coordinate, jar.cache_path, jar.directory_digest)\n      classpath_entries.append((conf, cp_entry))\n\n    for target in targets:\n      self._add_elements_for_target(target, classpath_entries)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_for_target(self, target, classpath_elements):\n    self._classpaths.remove_for_target(target, self._wrap_path_elements(classpath_elements))", "response": "Removes the given entries for the given target."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_for_targets(self, targets):\n    cp_entries = self.get_classpath_entries_for_targets(targets)\n    return [(conf, cp_entry.path) for conf, cp_entry in cp_entries]", "response": "Gets the classpath products for the given targets."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the classpath entries for the given targets.", "response": "def get_classpath_entries_for_targets(self, targets, respect_excludes=True):\n    \"\"\"Gets the classpath products for the given targets.\n\n    Products are returned in order, optionally respecting target excludes.\n\n    :param targets: The targets to lookup classpath products for.\n    :param bool respect_excludes: `True` to respect excludes; `False` to ignore them.\n    :returns: The ordered (conf, classpath entry) tuples.\n    :rtype: list of (string, :class:`ClasspathEntry`)\n    \"\"\"\n\n    # remove the duplicate, preserve the ordering.\n    return list(OrderedSet([cp for cp, target in self.get_product_target_mappings_for_targets(\n                            targets, respect_excludes)]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the classpath products - target associations for the given targets.", "response": "def get_product_target_mappings_for_targets(self, targets, respect_excludes=True):\n    \"\"\"Gets the classpath products-target associations for the given targets.\n\n    Product-target tuples are returned in order, optionally respecting target excludes.\n\n    :param targets: The targets to lookup classpath products for.\n    :param bool respect_excludes: `True` to respect excludes; `False` to ignore them.\n    :returns: The ordered (classpath products, target) tuples.\n    \"\"\"\n    classpath_target_tuples = self._classpaths.get_product_target_mappings_for_targets(targets)\n    if respect_excludes:\n      return self._filter_by_excludes(classpath_target_tuples, targets)\n    else:\n      return classpath_target_tuples"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the artifact classpath products for the given targets.", "response": "def get_artifact_classpath_entries_for_targets(self, targets, respect_excludes=True):\n    \"\"\"Gets the artifact classpath products for the given targets.\n\n    Products are returned in order, optionally respecting target excludes, and the products only\n    include external artifact classpath elements (ie: resolved jars).\n\n    :param targets: The targets to lookup classpath products for.\n    :param bool respect_excludes: `True` to respect excludes; `False` to ignore them.\n    :returns: The ordered (conf, classpath entry) tuples.\n    :rtype: list of (string, :class:`ArtifactClasspathEntry`)\n    \"\"\"\n    classpath_tuples = self.get_classpath_entries_for_targets(targets,\n                                                              respect_excludes=respect_excludes)\n    return [(conf, cp_entry) for conf, cp_entry in classpath_tuples\n            if ClasspathEntry.is_artifact_classpath_entry(cp_entry)]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_internal_classpath_entries_for_targets(self, targets, respect_excludes=True):\n    classpath_tuples = self.get_classpath_entries_for_targets(targets,\n                                                              respect_excludes=respect_excludes)\n    return [(conf, cp_entry) for conf, cp_entry in classpath_tuples\n            if ClasspathEntry.is_internal_classpath_entry(cp_entry)]", "response": "Returns the internal classpath products for the given targets."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd the contents of other to this ClasspathProducts.", "response": "def update(self, other):\n    \"\"\"Adds the contents of other to this ClasspathProducts.\"\"\"\n    if self._pants_workdir != other._pants_workdir:\n      raise ValueError('Other ClasspathProducts from a different pants workdir {}'.format(other._pants_workdir))\n    for target, products in other._classpaths._products_by_target.items():\n      self._classpaths.add_for_target(target, products)\n    for target, products in other._excludes._products_by_target.items():\n      self._excludes.add_for_target(target, products)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates that all files in the classpath are located within the working directory.", "response": "def _validate_classpath_tuples(self, classpath, target):\n    \"\"\"Validates that all files are located within the working directory, to simplify relativization.\n\n    :param classpath: The list of classpath tuples. Each tuple is a 2-tuple of ivy_conf and\n                      ClasspathEntry.\n    :param target: The target that the classpath tuple is being registered for.\n    :raises: `TaskError` when the path is outside the work directory\n    \"\"\"\n    for classpath_tuple in classpath:\n      conf, classpath_entry = classpath_tuple\n      path = classpath_entry.path\n      if os.path.relpath(path, self._pants_workdir).startswith(os.pardir):\n        raise TaskError(\n          'Classpath entry {} for target {} is located outside the working directory \"{}\".'\n          .format(path, target.address.spec, self._pants_workdir))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a subclass of this registrar suitable for registering on the specified goal.", "response": "def registrar_for_scope(cls, goal):\n    \"\"\"Returns a subclass of this registrar suitable for registering on the specified goal.\n\n    Allows reuse of the same registrar for multiple goals, and also allows us to decouple task\n    code from knowing which goal(s) the task is to be registered in.\n    \"\"\"\n    type_name = '{}_{}'.format(cls.__name__, goal)\n    if PY2:\n      type_name = type_name.encode('utf-8')\n    return type(type_name, (cls, ), {'options_scope': goal})"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _calculate_python_sources(self, targets):\n    python_eval_targets = filter(self.is_non_synthetic_python_target, targets)\n    sources = set()\n    for target in python_eval_targets:\n      sources.update(\n        source for source in target.sources_relative_to_buildroot()\n        if os.path.splitext(source)[1] == self._PYTHON_SOURCE_EXTENSION\n      )\n    return list(sources)", "response": "Generate a set of source files from the given targets."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef teardown_socket(s):\n  try:\n    s.shutdown(socket.SHUT_WR)\n  except socket.error:\n    pass\n  finally:\n    s.close()", "response": "Shuts down and closes a socket."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a byte string of length bufsize.", "response": "def recv(self, bufsize):\n    \"\"\"Buffers up to _chunk_size bytes when the internal buffer has less than `bufsize` bytes.\"\"\"\n    assert bufsize > 0, 'a positive bufsize is required'\n\n    if len(self._buffer) < bufsize:\n      readable, _, _ = safe_select([self._socket], [], [], self._select_timeout)\n      if readable:\n        recvd = self._socket.recv(max(self._chunk_size, bufsize))\n        self._buffer = self._buffer + recvd\n    return_buf, self._buffer = self._buffer[:bufsize], self._buffer[bufsize:]\n    return return_buf"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _install_eslint(self, bootstrap_dir):\n    with pushd(bootstrap_dir):\n      result, install_command = self.install_module(\n        package_manager=self.node_distribution.get_package_manager(package_manager=PACKAGE_MANAGER_YARNPKG),\n        workunit_name=self.INSTALL_JAVASCRIPTSTYLE_TARGET_NAME,\n        workunit_labels=[WorkUnitLabel.PREP])\n      if result != 0:\n        raise TaskError('Failed to install ESLint\\n'\n                        '\\t{} failed with exit code {}'.format(install_command, result))\n\n    self.context.log.debug('Successfully installed ESLint to {}'.format(bootstrap_dir))\n    return bootstrap_dir", "response": "Install the ESLint distribution."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the list of globs that can be used to invalidate the project.", "response": "def compute_invalidation_globs(bootstrap_options):\n    \"\"\"\n    Combine --pythonpath and --pants_config_files(pants.ini) files that are in {buildroot} dir\n    with those invalidation_globs provided by users\n    :param bootstrap_options:\n    :return: A list of invalidation_globs\n    \"\"\"\n    buildroot = get_buildroot()\n    invalidation_globs = []\n    globs = bootstrap_options.pythonpath + \\\n      bootstrap_options.pants_config_files + \\\n      bootstrap_options.pantsd_invalidation_globs\n\n    for glob in globs:\n      glob_relpath = os.path.relpath(glob, buildroot)\n      if glob_relpath and (not glob_relpath.startswith(\"../\")):\n        invalidation_globs.extend([glob_relpath, glob_relpath + '/**'])\n      else:\n        logging.getLogger(__name__).warning(\"Changes to {}, outside of the buildroot\"\n                                            \", will not be invalidated.\".format(glob))\n\n    return invalidation_globs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef shutdown(self, service_thread_map):\n    with self._services.lifecycle_lock:\n      for service, service_thread in service_thread_map.items():\n        self._logger.info('terminating pantsd service: {}'.format(service))\n        service.terminate()\n        service_thread.join(self.JOIN_TIMEOUT_SECONDS)\n      self._logger.info('terminating pantsd')\n      self._kill_switch.set()", "response": "Gracefully terminate all services and kill the main PantsDaemon loop."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclose stdio streams to avoid output in the tty that launched pantsd.", "response": "def _close_stdio():\n    \"\"\"Close stdio streams to avoid output in the tty that launched pantsd.\"\"\"\n    for fd in (sys.stdin, sys.stdout, sys.stderr):\n      file_no = fd.fileno()\n      fd.flush()\n      fd.close()\n      os.close(file_no)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _run_services(self, pants_services):\n    if not pants_services.services:\n      self._logger.critical('no services to run, bailing!')\n      return\n\n    service_thread_map = {service: self._make_thread(service)\n                          for service in pants_services.services}\n\n    # Start services.\n    for service, service_thread in service_thread_map.items():\n      self._logger.info('starting service {}'.format(service))\n      try:\n        service_thread.start()\n      except (RuntimeError, FSEventService.ServiceError):\n        self.shutdown(service_thread_map)\n        raise PantsDaemon.StartupFailure('service {} failed to start, shutting down!'.format(service))\n\n    # Once all services are started, write our pid.\n    self.write_pid()\n    self.write_metadata_by_name('pantsd', self.FINGERPRINT_KEY, ensure_text(self.options_fingerprint))\n\n    # Monitor services.\n    while not self.is_killed:\n      for service, service_thread in service_thread_map.items():\n        if not service_thread.is_alive():\n          self.shutdown(service_thread_map)\n          raise PantsDaemon.RuntimeFailure('service failure for {}, shutting down!'.format(service))\n        else:\n          # Avoid excessive CPU utilization.\n          service_thread.join(self.JOIN_TIMEOUT_SECONDS)", "response": "Main loop for the services."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites multiple named sockets using a socket mapping.", "response": "def _write_named_sockets(self, socket_map):\n    \"\"\"Write multiple named sockets using a socket mapping.\"\"\"\n    for socket_name, socket_info in socket_map.items():\n      self.write_named_socket(socket_name, socket_info)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npost - fork child callback for ProcessManager. daemon_spawn.", "response": "def post_fork_child(self):\n    \"\"\"Post-fork() child callback for ProcessManager.daemon_spawn().\"\"\"\n    entry_point = '{}:launch'.format(__name__)\n    exec_env = combined_dict(os.environ, dict(PANTS_ENTRYPOINT=entry_point))\n    # Pass all of sys.argv so that we can proxy arg flags e.g. `-ldebug`.\n    cmd = [sys.executable] + sys.argv\n    self._logger.debug('cmd is: PANTS_ENTRYPOINT={} {}'.format(entry_point, ' '.join(cmd)))\n    # TODO: Improve error handling on launch failures.\n    os.spawnve(os.P_NOWAIT, sys.executable, cmd, env=exec_env)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef needs_launch(self):\n    new_fingerprint = self.options_fingerprint\n    self._logger.debug('pantsd: is_alive={} new_fingerprint={} current_fingerprint={}'\n                       .format(self.is_alive(), new_fingerprint, self.fingerprint))\n    return self.needs_restart(new_fingerprint)", "response": "Determines if pantsd needs to be launched."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef launch(self):\n    self.terminate(include_watchman=False)\n    self.watchman_launcher.maybe_launch()\n    self._logger.debug('launching pantsd')\n    self.daemon_spawn()\n    # Wait up to 60 seconds for pantsd to write its pidfile.\n    pantsd_pid = self.await_pid(60)\n    listening_port = self.read_named_socket('pailgun', int)\n    self._logger.debug('pantsd is running at pid {}, pailgun port is {}'\n                       .format(self.pid, listening_port))\n    return self.Handle(pantsd_pid, listening_port, text_type(self._metadata_base_dir))", "response": "Launches pantsd in a subprocess."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nterminating pantsd and watchman.", "response": "def terminate(self, include_watchman=True):\n    \"\"\"Terminates pantsd and watchman.\n\n    N.B. This should always be called under care of the `lifecycle_lock`.\n    \"\"\"\n    super(PantsDaemon, self).terminate()\n    if include_watchman:\n      self.watchman_launcher.terminate()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\noverride ProcessManager. needs_restart to account for the case where pantsd is running and we want to shutdown after this run.", "response": "def needs_restart(self, option_fingerprint):\n    \"\"\"\n    Overrides ProcessManager.needs_restart, to account for the case where pantsd is running\n    but we want to shutdown after this run.\n    :param option_fingerprint: A fingeprint of the global bootstrap options.\n    :return: True if the daemon needs to restart.\n    \"\"\"\n    should_shutdown_after_run = self._bootstrap_options.for_global_scope().shutdown_pantsd_after_run\n    return super(PantsDaemon, self).needs_restart(option_fingerprint) or \\\n           (self.is_alive() and should_shutdown_after_run)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a Get node to return its two legal form of product type id and subject type id.", "response": "def extract_constraints(call_node):\n    \"\"\"Parses a `Get(..)` call in one of its two legal forms to return its type constraints.\n\n    :param call_node: An `ast.Call` node representing a call to `Get(..)`.\n    :return: A tuple of product type id and subject type id.\n    \"\"\"\n    def render_args():\n      return ', '.join(\n        # Dump the Name's id to simplify output when available, falling back to the name of the\n        # node's class.\n        getattr(a, 'id', type(a).__name__)\n        for a in call_node.args)\n\n    if len(call_node.args) == 2:\n      product_type, subject_constructor = call_node.args\n      if not isinstance(product_type, ast.Name) or not isinstance(subject_constructor, ast.Call):\n        # TODO(#7114): describe what types of objects are expected in the get call, not just the\n        # argument names. After #7114 this will be easier because they will just be types!\n        raise ValueError(\n          'Two arg form of {} expected (product_type, subject_type(subject)), but '\n                        'got: ({})'.format(Get.__name__, render_args()))\n      return (product_type.id, subject_constructor.func.id)\n    elif len(call_node.args) == 3:\n      product_type, subject_declared_type, _ = call_node.args\n      if not isinstance(product_type, ast.Name) or not isinstance(subject_declared_type, ast.Name):\n        raise ValueError(\n          'Three arg form of {} expected (product_type, subject_declared_type, subject), but '\n                        'got: ({})'.format(Get.__name__, render_args()))\n      return (product_type.id, subject_declared_type.id)\n    else:\n      raise ValueError('Invalid {}; expected either two or three args, but '\n                      'got: ({})'.format(Get.__name__, render_args()))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the input scope into a set of strings.", "response": "def _parse(cls, scope):\n    \"\"\"Parses the input scope into a normalized set of strings.\n\n    :param scope: A string or tuple containing zero or more scope names.\n    :return: A set of scope name strings, or a tuple with the default scope name.\n    :rtype: set\n    \"\"\"\n    if not scope:\n      return ('default',)\n    if isinstance(scope, string_types):\n      scope = scope.split(' ')\n    scope = {str(s).lower() for s in scope if s}\n    return scope or ('default',)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef in_scope(self, exclude_scopes=None, include_scopes=None):\n    if include_scopes is not None and not isinstance(include_scopes, Scope):\n      raise ValueError('include_scopes must be a Scope instance but was {}.'.format(\n        type(include_scopes)\n      ))\n    if exclude_scopes is not None and not isinstance(exclude_scopes, Scope):\n      raise ValueError('exclude_scopes must be a Scope instance but was {}.'.format(\n        type(exclude_scopes)\n      ))\n    if exclude_scopes and any(s in exclude_scopes for s in self):\n      return False\n    if include_scopes and not any(s in include_scopes for s in self):\n      return False\n    return True", "response": "Returns True if this resource is included by the given inclusion and exclusion rules."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef execute(self, jvm_options=None, args=None, executor=None,\n              workunit_factory=None, workunit_name=None, workunit_labels=None):\n    \"\"\"Executes the ivy commandline client with the given args.\n\n    Raises Ivy.Error if the command fails for any reason.\n    :param executor: Java executor to run ivy with.\n    \"\"\"\n    # NB(gmalmquist): It should be OK that we can't declare a subsystem_dependency in this file\n    # (because it's just a plain old object), because Ivy is only constructed by Bootstrapper, which\n    # makes an explicit call to IvySubsystem.global_instance() in its constructor, which in turn has\n    # a declared dependency on DistributionLocator.\n    executor = executor or SubprocessExecutor(DistributionLocator.cached())\n    runner = self.runner(jvm_options=jvm_options, args=args, executor=executor)\n    try:\n      with self.resolution_lock:\n        result = util.execute_runner(runner, workunit_factory, workunit_name, workunit_labels)\n      if result != 0:\n        raise self.Error('Ivy command failed with exit code {}{}'.format(\n                           result, ': ' + ' '.join(args) if args else ''))\n    except executor.Error as e:\n      raise self.Error('Problem executing ivy: {}'.format(e))", "response": "Executes the ivy commandline client with the given args."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an ivy commandline client runner for the given args.", "response": "def runner(self, jvm_options=None, args=None, executor=None):\n    \"\"\"Creates an ivy commandline client runner for the given args.\"\"\"\n    args = args or []\n    if self._ivy_settings and '-settings' not in args:\n      args = ['-settings', self._ivy_settings] + args\n\n    options = list(jvm_options) if jvm_options else []\n    if self._ivy_resolution_cache_dir and '-cache' not in args:\n      # TODO(John Sirois): Currently this is a magic property to support hand-crafted <caches/> in\n      # ivysettings.xml.  Ideally we'd support either simple -caches or these hand-crafted cases\n      # instead of just hand-crafted.  Clean this up by taking over ivysettings.xml and generating\n      # it from BUILD constructs.\n      options += ['-Divy.cache.dir={}'.format(self._ivy_resolution_cache_dir)]\n    options += self._extra_jvm_options\n\n    executor = executor or SubprocessExecutor(DistributionLocator.cached())\n    if not isinstance(executor, Executor):\n      raise ValueError('The executor argument must be an Executor instance, given {} of type {}'.format(\n                         executor, type(executor)))\n\n    return executor.runner(classpath=self._classpath, main='org.apache.ivy.Main',\n                           jvm_options=options, args=args)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the java runner for execute_java and execute_java_async.", "response": "def _get_runner(classpath, main, jvm_options, args, executor,\n               cwd, distribution,\n               create_synthetic_jar, synthetic_jar_dir):\n  \"\"\"Gets the java runner for execute_java and execute_java_async.\"\"\"\n\n  executor = executor or SubprocessExecutor(distribution)\n\n  safe_cp = classpath\n  if create_synthetic_jar:\n    safe_cp = safe_classpath(classpath, synthetic_jar_dir)\n    logger.debug('Bundling classpath {} into {}'.format(':'.join(classpath), safe_cp))\n\n  return executor.runner(safe_cp, main, args=args, jvm_options=jvm_options, cwd=cwd)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef execute_java(classpath, main, jvm_options=None, args=None, executor=None,\n                 workunit_factory=None, workunit_name=None, workunit_labels=None,\n                 cwd=None, workunit_log_config=None, distribution=None,\n                 create_synthetic_jar=True, synthetic_jar_dir=None, stdin=None):\n  \"\"\"Executes the java program defined by the classpath and main.\n\n  If `workunit_factory` is supplied, does so in the context of a workunit.\n\n  :param list classpath: the classpath for the java program\n  :param string main: the fully qualified class name of the java program's entry point\n  :param list jvm_options: an optional sequence of options for the underlying jvm\n  :param list args: an optional sequence of args to pass to the java program\n  :param executor: an optional java executor to use to launch the program; defaults to a subprocess\n    spawn of the default java distribution\n  :param workunit_factory: an optional callable that can produce a workunit context\n  :param string workunit_name: an optional name for the work unit; defaults to the main\n  :param list workunit_labels: an optional sequence of labels for the work unit\n  :param string cwd: optionally set the working directory\n  :param WorkUnit.LogConfig workunit_log_config: an optional tuple of options affecting reporting\n  :param bool create_synthetic_jar: whether to create a synthentic jar that includes the original\n    classpath in its manifest.\n  :param string synthetic_jar_dir: an optional directory to store the synthetic jar, if `None`\n    a temporary directory will be provided and cleaned up upon process exit.\n  :param file stdin: The stdin handle to use: by default None, meaning that stdin will\n    not be propagated into the process.\n\n  Returns the exit code of the java program.\n  Raises `pants.java.Executor.Error` if there was a problem launching java itself.\n  \"\"\"\n\n  runner = _get_runner(classpath, main, jvm_options, args, executor, cwd, distribution,\n                       create_synthetic_jar, synthetic_jar_dir)\n  workunit_name = workunit_name or main\n\n  return execute_runner(runner,\n                        workunit_factory=workunit_factory,\n                        workunit_name=workunit_name,\n                        workunit_labels=workunit_labels,\n                        workunit_log_config=workunit_log_config,\n                        stdin=stdin)", "response": "Executes the java program defined by the classpath and main."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef execute_java_async(classpath, main, jvm_options=None, args=None, executor=None,\n                       workunit_factory=None, workunit_name=None, workunit_labels=None,\n                       cwd=None, workunit_log_config=None, distribution=None,\n                       create_synthetic_jar=True, synthetic_jar_dir=None):\n  \"\"\"This is just like execute_java except that it returns a ProcessHandler rather than a return code.\n\n\n  If `workunit_factory` is supplied, does so in the context of a workunit.\n\n  :param list classpath: the classpath for the java program\n  :param string main: the fully qualified class name of the java program's entry point\n  :param list jvm_options: an optional sequence of options for the underlying jvm\n  :param list args: an optional sequence of args to pass to the java program\n  :param executor: an optional java executor to use to launch the program; defaults to a subprocess\n    spawn of the default java distribution\n  :param workunit_factory: an optional callable that can produce a workunit context\n  :param string workunit_name: an optional name for the work unit; defaults to the main\n  :param list workunit_labels: an optional sequence of labels for the work unit\n  :param string cwd: optionally set the working directory\n  :param WorkUnit.LogConfig workunit_log_config: an optional tuple of options affecting reporting\n  :param bool create_synthetic_jar: whether to create a synthentic jar that includes the original\n    classpath in its manifest.\n  :param string synthetic_jar_dir: an optional directory to store the synthetic jar, if `None`\n    a temporary directory will be provided and cleaned up upon process exit.\n\n  Returns a ProcessHandler to the java program.\n  Raises `pants.java.Executor.Error` if there was a problem launching java itself.\n  \"\"\"\n\n  runner = _get_runner(classpath, main, jvm_options, args, executor, cwd, distribution,\n                       create_synthetic_jar, synthetic_jar_dir)\n  workunit_name = workunit_name or main\n\n  return execute_runner_async(runner,\n                              workunit_factory=workunit_factory,\n                              workunit_name=workunit_name,\n                              workunit_labels=workunit_labels,\n                              workunit_log_config=workunit_log_config)", "response": "This function is just like execute_java except that it returns a ProcessHandler that will be returned when the java program exits."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting the given java runner.", "response": "def execute_runner(runner, workunit_factory=None, workunit_name=None, workunit_labels=None,\n                   workunit_log_config=None, stdin=None):\n  \"\"\"Executes the given java runner.\n\n  If `workunit_factory` is supplied, does so in the context of a workunit.\n\n  :param runner: the java runner to run\n  :param workunit_factory: an optional callable that can produce a workunit context\n  :param string workunit_name: an optional name for the work unit; defaults to the main\n  :param list workunit_labels: an optional sequence of labels for the work unit\n  :param WorkUnit.LogConfig workunit_log_config: an optional tuple of task options affecting reporting\n  :param file stdin: The stdin handle to use: by default None, meaning that stdin will\n    not be propagated into the process.\n\n  Returns the exit code of the java runner.\n  Raises `pants.java.Executor.Error` if there was a problem launching java itself.\n  \"\"\"\n  if not isinstance(runner, Executor.Runner):\n    raise ValueError('The runner argument must be a java Executor.Runner instance, '\n                     'given {} of type {}'.format(runner, type(runner)))\n\n  if workunit_factory is None:\n    return runner.run(stdin=stdin)\n  else:\n    workunit_labels = [\n        WorkUnitLabel.TOOL,\n        WorkUnitLabel.NAILGUN if isinstance(runner.executor, NailgunExecutor) else WorkUnitLabel.JVM\n    ] + (workunit_labels or [])\n\n    with workunit_factory(name=workunit_name, labels=workunit_labels,\n                          cmd=runner.cmd, log_config=workunit_log_config) as workunit:\n      ret = runner.run(stdout=workunit.output('stdout'),\n                       stderr=workunit.output('stderr'),\n                       stdin=stdin)\n      workunit.set_outcome(WorkUnit.FAILURE if ret else WorkUnit.SUCCESS)\n      return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexecute the given java runner asynchronously.", "response": "def execute_runner_async(runner, workunit_factory=None, workunit_name=None, workunit_labels=None,\n                         workunit_log_config=None):\n  \"\"\"Executes the given java runner asynchronously.\n\n  We can't use 'with' here because the workunit_generator's __exit__ function\n  must be called after the process exits, in the return_code_handler.\n  The wrapper around process.wait() needs to handle the same exceptions\n  as the contextmanager does, so we have code duplication.\n\n  We're basically faking the 'with' call to deal with asynchronous\n  results.\n\n  If `workunit_factory` is supplied, does so in the context of a workunit.\n\n  :param runner: the java runner to run\n  :param workunit_factory: an optional callable that can produce a workunit context\n  :param string workunit_name: an optional name for the work unit; defaults to the main\n  :param list workunit_labels: an optional sequence of labels for the work unit\n  :param WorkUnit.LogConfig workunit_log_config: an optional tuple of task options affecting reporting\n\n  Returns a ProcessHandler to the java process that is spawned.\n  Raises `pants.java.Executor.Error` if there was a problem launching java itself.\n  \"\"\"\n\n  if not isinstance(runner, Executor.Runner):\n    raise ValueError('The runner argument must be a java Executor.Runner instance, '\n                     'given {} of type {}'.format(runner, type(runner)))\n\n  if workunit_factory is None:\n    return SubprocessProcessHandler(runner.spawn())\n  else:\n    workunit_labels = [\n                        WorkUnitLabel.TOOL,\n                        WorkUnitLabel.NAILGUN if isinstance(runner.executor, NailgunExecutor) else WorkUnitLabel.JVM\n                      ] + (workunit_labels or [])\n\n    workunit_generator = workunit_factory(name=workunit_name, labels=workunit_labels,\n                                cmd=runner.cmd, log_config=workunit_log_config)\n    workunit = workunit_generator.__enter__()\n    process = runner.spawn(stdout=workunit.output('stdout'), stderr=workunit.output('stderr'))\n\n    class WorkUnitProcessHandler(ProcessHandler):\n      def wait(_, timeout=None):\n        try:\n          ret = process.wait(timeout=timeout)\n          workunit.set_outcome(WorkUnit.FAILURE if ret else WorkUnit.SUCCESS)\n          workunit_generator.__exit__(None, None, None)\n          return ret\n        except BaseException:\n          if not workunit_generator.__exit__(*sys.exc_info()):\n            raise\n\n      def kill(_):\n        return process.kill()\n\n      def terminate(_):\n        return process.terminate()\n\n      def poll(_):\n        return process.poll()\n\n    return WorkUnitProcessHandler()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef relativize_classpath(classpath, root_dir, followlinks=True):\n  def relativize_url(url, root_dir):\n    # When symlink is involed, root_dir concatenated with the returned relpath may not exist.\n    # Consider on mac `/var` is a symlink of `/private/var`, the relative path of subdirectories\n    # under /var to any other directories under `/` computed by os.path.relpath misses one level\n    # of `..`. Use os.path.realpath to guarantee returned relpath can always be located.\n    # This is not needed only when path are all relative.\n    url = os.path.realpath(url) if followlinks else url\n    root_dir = os.path.realpath(root_dir) if followlinks else root_dir\n    url_in_bundle = os.path.relpath(url, root_dir)\n    # Append '/' for directories, those not ending with '/' are assumed to be jars.\n    # Note isdir does what we need here to follow symlinks.\n    if os.path.isdir(url):\n      url_in_bundle += '/'\n    return url_in_bundle\n\n  return [relativize_url(url, root_dir) for url in classpath]", "response": "Convert a list of classpath relative to a directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbundle classpath into one synthetic jar that includes original classpath in its manifest.", "response": "def safe_classpath(classpath, synthetic_jar_dir, custom_name=None):\n  \"\"\"Bundles classpath into one synthetic jar that includes original classpath in its manifest.\n\n  This is to ensure classpath length never exceeds platform ARG_MAX.\n\n  :param list classpath: Classpath to be bundled.\n  :param string synthetic_jar_dir: directory to store the synthetic jar, if `None`\n    a temp directory will be provided and cleaned up upon process exit. Otherwise synthetic\n    jar will remain in the supplied directory, only for debugging purpose.\n  :param custom_name: filename of the synthetic jar to be created.\n\n  :returns: A classpath (singleton list with just the synthetic jar).\n  :rtype: list of strings\n  \"\"\"\n  if synthetic_jar_dir:\n    safe_mkdir(synthetic_jar_dir)\n  else:\n    synthetic_jar_dir = safe_mkdtemp()\n\n  # Quote the paths so that if they contain reserved characters can be safely passed to JVM classloader.\n  bundled_classpath = [parse.quote(cp) for cp in relativize_classpath(classpath, synthetic_jar_dir)]\n\n  manifest = Manifest()\n  manifest.addentry(Manifest.CLASS_PATH, ' '.join(bundled_classpath))\n\n  with temporary_file(root_dir=synthetic_jar_dir, cleanup=False, suffix='.jar') as jar_file:\n    with open_zip(jar_file, mode='w', compression=ZIP_STORED) as jar:\n      jar.writestr(Manifest.PATH, manifest.contents())\n\n    if custom_name:\n      custom_path = os.path.join(synthetic_jar_dir, custom_name)\n      safe_concurrent_rename(jar_file.name, custom_path)\n      return [custom_path]\n    else:\n      return [jar_file.name]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a rule which shades jar entries from one pattern to another.", "response": "def create_relocate(cls, from_pattern, shade_pattern=None, shade_prefix=None):\n    \"\"\"Creates a rule which shades jar entries from one pattern to another.\n\n    Examples: ::\n\n        # Rename everything in the org.foobar.example package\n        # to __shaded_by_pants__.org.foobar.example.\n        shading_relocate('org.foobar.example.**')\n\n        # Rename org.foobar.example.Main to __shaded_by_pants__.org.foobar.example.Main\n        shading_relocate('org.foobar.example.Main')\n\n        # Rename org.foobar.example.Main to org.foobar.example.NotMain\n        shading_relocate('org.foobar.example.Main', 'org.foobar.example.NotMain')\n\n        # Rename all 'Main' classes under any direct subpackage of org.foobar.\n        shading_relocate('org.foobar.*.Main')\n\n        # Rename org.foobar package to com.barfoo package\n        shading_relocate('org.foobar.**', 'com.barfoo.@1')\n\n        # Rename everything in org.foobar.example package to __hello__.org.foobar.example\n        shading_relocate('org.foobar.example.**', shade_prefix='__hello__')\n\n    :param string from_pattern: Any fully-qualified classname which matches this pattern will be\n      shaded. '*' is a wildcard that matches any individual package component, and '**' is a\n      wildcard that matches any trailing pattern (ie the rest of the string).\n    :param string shade_pattern: The shaded pattern to use, where ``@1``, ``@2``, ``@3``, etc are\n      references to the groups matched by wildcards (groups are numbered from left to right). If\n      omitted, this pattern is inferred from the input pattern, prefixed by the ``shade_prefix``\n      (if provided). (Eg, a ``from_pattern`` of ``com.*.foo.bar.**`` implies a default\n      ``shade_pattern`` of ``__shaded_by_pants__.com.@1.foo.@2``)\n    :param string shade_prefix: Prefix to prepend when generating a ``shade_pattern`` (if a\n      ``shade_pattern`` is not provided by the user). Defaults to '``__shaded_by_pants__.``'.\n    \"\"\"\n    # NB(gmalmquist): Have have to check \"is None\" rather than using an or statement, because the\n    # empty-string is a valid prefix which should not be replaced by the default prefix.\n    shade_prefix = Shading.SHADE_PREFIX if shade_prefix is None else shade_prefix\n    return RelocateRule.new(from_pattern, shade_pattern, shade_prefix)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_keep_package(cls, package_name, recursive=True):\n    return cls.create_keep(cls._format_package_glob(package_name, recursive))", "response": "Convenience constructor for a package keep rule."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_relocate_package(cls, package_name, shade_prefix=None, recursive=True):\n    return cls.create_relocate(from_pattern=cls._format_package_glob(package_name, recursive),\n                               shade_prefix=shade_prefix)", "response": "Convenience constructor for a package relocation rule."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a Shading. Rule describing the shading exclusion of the given package name.", "response": "def exclude_package(cls, package_name=None, recursive=False):\n    \"\"\"Excludes the given fully qualified package name from shading.\n\n    :param unicode package_name: A fully qualified package_name; eg: `org.pantsbuild`; `None` for\n                                the java default (root) package.\n    :param bool recursive: `True` to exclude any package with `package_name` as a proper prefix;\n                           `False` by default.\n    :returns: A `Shader.Rule` describing the shading exclusion.\n    \"\"\"\n    if not package_name:\n      return Shading.create_exclude('**' if recursive else '*')\n    return Shading.create_exclude_package(package_name, recursive=recursive)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninclude the given fully qualified package name in shading.", "response": "def shade_package(cls, package_name=None, recursive=False):\n    \"\"\"Includes the given fully qualified package name in shading.\n\n    :param unicode package_name: A fully qualified package_name; eg: `org.pantsbuild`; `None` for\n                                 the java default (root) package.\n    :param bool recursive: `True` to include any package with `package_name` as a proper prefix;\n                           `False` by default.\n    :returns: A `Shader.Rule` describing the packages to be shaded.\n    \"\"\"\n    if not package_name:\n      return Shading.create_relocate('**' if recursive else '*')\n    return Shading.create_relocate_package(package_name, recursive=recursive)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nassemble a list of shader. Rule objects suitable for fully shading the given main class.", "response": "def assemble_binary_rules(self, main, jar, custom_rules=None):\n    \"\"\"Creates an ordered list of rules suitable for fully shading the given binary.\n\n    The default rules will ensure the `main` class name is un-changed along with a minimal set of\n    support classes but that everything else will be shaded.\n\n    Any `custom_rules` are given highest precedence and so they can interfere with this automatic\n    binary shading.  In general it's safe to add exclusion rules to open up classes that need to be\n    shared between the binary and the code it runs over.  An example would be excluding the\n    `org.junit.Test` annotation class from shading since a tool running junit needs to be able\n    to scan for this annotation inside the user code it tests.\n\n    :param unicode main: The main class to preserve as the entry point.\n    :param unicode jar: The path of the binary jar the `main` class lives in.\n    :param list custom_rules: An optional list of custom `Shader.Rule`s.\n    :returns: a precedence-ordered list of `Shader.Rule`s\n    \"\"\"\n    # If a class is matched by multiple rules, the 1st lexical match wins (see:\n    # https://code.google.com/p/jarjar/wiki/CommandLineDocs#Rules_file_format).\n    # As such we 1st ensure the `main` package and the jre packages have exclusion rules and\n    # then apply a final set of shading rules to everything else at lowest precedence.\n\n    # Custom rules take precedence.\n    rules = list(custom_rules or [])\n\n    # Exclude the main entrypoint's package from shading. There may be package-private classes that\n    # the main class accesses so we must preserve the whole package).\n    parts = main.rsplit('.', 1)\n    if len(parts) == 2:\n      main_package = parts[0]\n    else:\n      # There is no package component, so the main class is in the root (default) package.\n      main_package = None\n    rules.append(self.exclude_package(main_package))\n\n    rules.extend(self.exclude_package(system_pkg, recursive=True)\n                 for system_pkg in self._binary_package_excludes)\n\n    # Shade everything else.\n    #\n    # NB: A simpler way to do this jumps out - just emit 1 wildcard rule:\n    #\n    #   rule **.* _shaded_.@1.@2\n    #\n    # Unfortunately, as of jarjar 1.4 this wildcard catch-all technique improperly transforms\n    # resources in the `main_package`.  The jarjar binary jar itself has its command line help text\n    # stored as a resource in its main's package and so using a catch-all like this causes\n    # recursively shading jarjar with itself using this class to fail!\n    #\n    # As a result we explicitly shade all the non `main_package` packages in the binary jar instead\n    # which does support recursively shading jarjar.\n    rules.extend(self.shade_package(pkg) for pkg in sorted(self._iter_jar_packages(jar))\n                 if pkg != main_package)\n\n    return rules"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a new binary jar that will shade the given jar using the given rules.", "response": "def binary_shader_for_rules(self, output_jar, jar, rules, jvm_options=None):\n    \"\"\"Yields an `Executor.Runner` that will perform shading of the binary `jar` when `run()`.\n\n    No default rules are applied; only the rules passed in as a parameter will be used.\n\n    :param unicode output_jar: The path to dump the shaded jar to; will be over-written if it\n                               exists.\n    :param unicode jar: The path to the jar file to shade.\n    :param list rules: The rules to apply for shading.\n    :param list jvm_options: an optional sequence of options for the underlying jvm\n    :returns: An `Executor.Runner` that can be `run()` to shade the given `jar`.\n    :rtype: :class:`pants.java.executor.Executor.Runner`\n    \"\"\"\n    with self.temporary_rules_file(rules) as rules_file:\n      logger.debug('Running jarjar with rules:\\n{}'.format(' '.join(rule.render() for rule in rules)))\n      yield self._executor.runner(classpath=self._jarjar_classpath,\n                                  main='org.pantsbuild.jarjar.Main',\n                                  jvm_options=jvm_options,\n                                  args=['process', rules_file, jar, output_jar])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an Executor. Runner that can be used to shade the given jar.", "response": "def binary_shader(self, output_jar, main, jar, custom_rules=None, jvm_options=None):\n    \"\"\"Yields an `Executor.Runner` that will perform shading of the binary `jar` when `run()`.\n\n    The default rules will ensure the `main` class name is un-changed along with a minimal set of\n    support classes but that everything else will be shaded.\n\n    Any `custom_rules` are given highest precedence and so they can interfere with this automatic\n    binary shading.  In general its safe to add exclusion rules to open up classes that need to be\n    shared between the binary and the code it runs over.  An example would be excluding the\n    `org.junit.Test` annotation class from shading since both a tool running junit needs to be able\n    to scan for this annotation applied to the user code it tests.\n\n    :param unicode output_jar: The path to dump the shaded jar to; will be over-written if it\n                               exists.\n    :param unicode main: The main class in the `jar` to preserve as the entry point.\n    :param unicode jar: The path to the jar file to shade.\n    :param list custom_rules: An optional list of custom `Shader.Rule`s.\n    :param list jvm_options: an optional sequence of options for the underlying jvm\n    :returns: An `Executor.Runner` that can be `run()` to shade the given `jar`.\n    :rtype: :class:`pants.java.executor.Executor.Runner`\n    \"\"\"\n    all_rules = self.assemble_binary_rules(main, jar, custom_rules=custom_rules)\n    return self.binary_shader_for_rules(output_jar, jar, all_rules, jvm_options=jvm_options)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives an Options object produce a ChangedRequest object.", "response": "def from_options(cls, options):\n    \"\"\"Given an `Options` object, produce a `ChangedRequest`.\"\"\"\n    return cls(options.changes_since,\n               options.diffspec,\n               options.include_dependees,\n               options.fast)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ensure_arg(args, arg, param=None):\n  for idx, found_arg in enumerate(args):\n    if found_arg == arg:\n      if param is not None:\n        args[idx + 1] = param\n      return args\n\n  args.append(arg)\n  if param is not None:\n    args.append(param)\n  return args", "response": "Make sure the argument is present in the list of args."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving the first instance of the specified arg from the list of args.", "response": "def remove_arg(args, arg, has_param=False):\n  \"\"\"Removes the first instance of the specified arg from the list of args.\n\n  If the arg is present and has_param is set, also removes the parameter that follows\n  the arg.\n  :param list args: strings representing an argument list.\n  :param staring arg: argument to remove from the list.\n  :param bool has_param: if true, also remove the parameter that follows arg in the list.\n  :return: possibly modified list of args.\n  \"\"\"\n  for idx, found_arg in enumerate(args):\n    if found_arg == arg:\n      if has_param:\n        slice_idx = idx + 2\n      else:\n        slice_idx = idx + 1\n      args = args[:idx] + args[slice_idx:]\n      break\n  return args"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setup_logging_to_stderr(python_logger, level):\n  native = Native()\n  levelno = get_numeric_level(level)\n  handler = create_native_stderr_log_handler(levelno, native, stream=sys.stderr)\n  python_logger.addHandler(handler)\n  # Let the rust side filter levels; try to have the python side send everything to the rust logger.\n  python_logger.setLevel(\"TRACE\")", "response": "Setup logging to stderr."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconfigures logging for a given scope.", "response": "def setup_logging(level, console_stream=None, log_dir=None, scope=None, log_name=None, native=None):\n  \"\"\"Configures logging for a given scope, by default the global scope.\n\n  :param str level: The logging level to enable, must be one of the level names listed here:\n                    https://docs.python.org/2/library/logging.html#levels\n  :param file console_stream: The stream to use for default (console) logging. If None (default),\n                              this will disable console logging.\n  :param str log_dir: An optional directory to emit logs files in.  If unspecified, no disk logging\n                      will occur.  If supplied, the directory will be created if it does not already\n                      exist and all logs will be tee'd to a rolling set of log files in that\n                      directory.\n  :param str scope: A logging scope to configure.  The scopes are hierarchichal logger names, with\n                    The '.' separator providing the scope hierarchy.  By default the root logger is\n                    configured.\n  :param str log_name: The base name of the log file (defaults to 'pants.log').\n  :param Native native: An instance of the Native FFI lib, to register rust logging.\n  :returns: The full path to the main log file if file logging is configured or else `None`.\n  :rtype: str\n  \"\"\"\n\n  # TODO(John Sirois): Consider moving to straight python logging.  The divide between the\n  # context/work-unit logging and standard python logging doesn't buy us anything.\n\n  # TODO(John Sirois): Support logging.config.fileConfig so a site can setup fine-grained\n  # logging control and we don't need to be the middleman plumbing an option for each python\n  # standard logging knob.\n\n  log_filename = None\n  file_handler = None\n\n  # A custom log handler for sub-debug trace logging.\n  def trace(self, message, *args, **kwargs):\n    if self.isEnabledFor(TRACE):\n      self._log(TRACE, message, args, **kwargs)\n\n  logging.Logger.trace = trace\n\n  logger = logging.getLogger(scope)\n  for handler in logger.handlers:\n    logger.removeHandler(handler)\n\n\n  if console_stream:\n    native_handler = create_native_stderr_log_handler(level, native, stream=console_stream)\n    logger.addHandler(native_handler)\n\n  if log_dir:\n    safe_mkdir(log_dir)\n    log_filename = os.path.join(log_dir, log_name or 'pants.log')\n\n    native_handler = create_native_pantsd_file_log_handler(level, native, log_filename)\n    file_handler = native_handler\n    logger.addHandler(native_handler)\n\n\n  logger.setLevel(level)\n\n  # This routes warnings through our loggers instead of straight to raw stderr.\n  logging.captureWarnings(True)\n\n  _maybe_configure_extended_logging(logger)\n\n  return LoggingSetupResult(log_filename, file_handler)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract and maps jars to directories containing their contents.", "response": "def _jars_to_directories(self, target):\n    \"\"\"Extracts and maps jars to directories containing their contents.\n\n    :returns: a set of filepaths to directories containing the contents of jar.\n    \"\"\"\n    files = set()\n    jar_import_products = self.context.products.get_data(JarImportProducts)\n    imports = jar_import_products.imports(target)\n    for coordinate, jar in imports:\n      files.add(self._extract_jar(coordinate, jar))\n    return files"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _extract_jar(self, coordinate, jar_path):\n    with open(jar_path, 'rb') as f:\n      sha = sha1(f.read()).hexdigest()\n      outdir = os.path.join(self.workdir, 'extracted', sha)\n    if not os.path.exists(outdir):\n      ZIP.extract(jar_path, outdir)\n      self.context.log.debug('Extracting jar {jar} at {jar_path}.'\n                             .format(jar=coordinate, jar_path=jar_path))\n    else:\n      self.context.log.debug('Jar {jar} already extracted at {jar_path}.'\n                             .format(jar=coordinate, jar_path=jar_path))\n    return outdir", "response": "Extracts the jar to a subfolder of workdir and returns the path to it."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef partition_targets_by_compatibility(self, targets):\n    tgts_by_compatibilities = defaultdict(list)\n    filters = set()\n\n    for target in targets:\n      if isinstance(target, PythonTarget):\n        c = self.python_setup.compatibility_or_constraints(target)\n        tgts_by_compatibilities[c].append(target)\n        filters.update(c)\n    return tgts_by_compatibilities, filters", "response": "Partition targets by their compatibility constraints."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef select_interpreter_for_targets(self, targets):\n\n    tgts_by_compatibilities, total_filter_set = self.partition_targets_by_compatibility(targets)\n    allowed_interpreters = set(self.setup(filters=total_filter_set))\n\n    # Constrain allowed_interpreters based on each target's compatibility requirements.\n    for compatibility in tgts_by_compatibilities:\n      compatible_with_target = set(self._matching(allowed_interpreters, compatibility))\n      allowed_interpreters &= compatible_with_target\n\n    if not allowed_interpreters:\n      # Create a helpful error message.\n      all_interpreter_version_strings = sorted({\n        interpreter.version_string\n        # NB: self.setup() requires filters to be passed, or else it will use the global interpreter\n        # constraints. We allow any interpreter other than CPython 3.0-3.3, which is known to choke\n        # with Pants.\n        for interpreter in self.setup(filters=(\"CPython<3\", \"CPython>=3.3\", \"PyPy\"))\n      })\n      unique_compatibilities = {tuple(c) for c in tgts_by_compatibilities.keys()}\n      unique_compatibilities_strs = [','.join(x) for x in unique_compatibilities if x]\n      tgts_by_compatibilities_strs = [t[0].address.spec for t in tgts_by_compatibilities.values()]\n      raise self.UnsatisfiableInterpreterConstraintsError(dedent(\"\"\"\\\n        Unable to detect a suitable interpreter for compatibilities: {} (Conflicting targets: {})\n\n        Pants detected these interpreter versions on your system: {}\n\n        Possible ways to fix this:\n        * Modify your Python interpreter constraints by following https://www.pantsbuild.org/python_readme.html#configure-the-python-version.\n        * Ensure the targeted Python version is installed and discoverable.\n        * Modify Pants' interpreter search paths via --pants-setup-interpreter-search-paths.\"\"\".format(\n          ' && '.join(sorted(unique_compatibilities_strs)),\n          ', '.join(tgts_by_compatibilities_strs),\n          ', '.join(all_interpreter_version_strings)\n        )))\n    # Return the lowest compatible interpreter.\n    return min(allowed_interpreters)", "response": "Pick an interpreter that can handle the given targets."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding all currently - cached interpreters.", "response": "def _setup_cached(self, filters=()):\n    \"\"\"Find all currently-cached interpreters.\"\"\"\n    for interpreter_dir in os.listdir(self._cache_dir):\n      pi = self._interpreter_from_relpath(interpreter_dir, filters=filters)\n      if pi:\n        logger.debug('Detected interpreter {}: {}'.format(pi.binary, str(pi.identity)))\n        yield pi"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _setup_paths(self, paths, filters=()):\n    for interpreter in self._matching(PythonInterpreter.all(paths), filters=filters):\n      identity_str = str(interpreter.identity)\n      pi = self._interpreter_from_relpath(identity_str, filters=filters)\n      if pi is None:\n        self._setup_interpreter(interpreter, identity_str)\n        pi = self._interpreter_from_relpath(identity_str, filters=filters)\n      if pi:\n        yield pi", "response": "Find interpreters under paths and cache them."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets up a cache of python interpreters.", "response": "def setup(self, filters=()):\n    \"\"\"Sets up a cache of python interpreters.\n\n    :param filters: A sequence of strings that constrain the interpreter compatibility for this\n      cache, using the Requirement-style format, e.g. ``'CPython>=3', or just ['>=2.7','<3']``\n      for requirements agnostic to interpreter class.\n    :returns: A list of cached interpreters\n    :rtype: list of :class:`pex.interpreter.PythonInterpreter`\n    \"\"\"\n    # We filter the interpreter cache itself (and not just the interpreters we pull from it)\n    # because setting up some python versions (e.g., 3<=python<3.3) crashes, and this gives us\n    # an escape hatch.\n    filters = filters if any(filters) else self.python_setup.interpreter_constraints\n    setup_paths = self.python_setup.interpreter_search_paths\n    logger.debug(\n      'Initializing Python interpreter cache matching filters `{}` from paths `{}`'.format(\n        ':'.join(filters), ':'.join(setup_paths)))\n\n    interpreters = []\n    def unsatisfied_filters():\n      return [f for f in filters if len(list(self._matching(interpreters, [f]))) == 0]\n\n    with OwnerPrintingInterProcessFileLock(path=os.path.join(self._cache_dir, '.file_lock')):\n      interpreters.extend(self._setup_cached(filters=filters))\n      if not interpreters or unsatisfied_filters():\n        interpreters.extend(self._setup_paths(setup_paths, filters=filters))\n\n    for filt in unsatisfied_filters():\n      logger.debug('No valid interpreters found for {}!'.format(filt))\n\n    matches = list(self._matching(interpreters, filters=filters))\n    if len(matches) == 0:\n      logger.debug('Found no valid interpreters!')\n\n    logger.debug(\n      'Initialized Python interpreter cache with {}'.format(', '.join([x.binary for x in matches])))\n    return matches"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_arguments(log, whitelisted_args, args):\n    valid_patterns = {re.compile(p): v for p, v in whitelisted_args.items()}\n\n    def validate(idx):\n      arg = args[idx]\n      for pattern, has_argument in valid_patterns.items():\n        if pattern.match(arg):\n          return 2 if has_argument else 1\n      log.warn(\"Zinc argument '{}' is not supported, and is subject to change/removal!\".format(arg))\n      return 1\n\n    arg_index = 0\n    while arg_index < len(args):\n      arg_index += validate(arg_index)", "response": "Validate that all arguments match whitelisted regexes."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts and formats the zinc arguments given in the jvm - platform settings.", "response": "def _format_zinc_arguments(settings, distribution):\n    \"\"\"Extracts and formats the zinc arguments given in the jvm platform settings.\n\n    This is responsible for the symbol substitution which replaces $JAVA_HOME with the path to an\n    appropriate jvm distribution.\n\n    :param settings: The jvm platform settings from which to extract the arguments.\n    :type settings: :class:`JvmPlatformSettings`\n    \"\"\"\n    zinc_args = [\n      '-C-source', '-C{}'.format(settings.source_level),\n      '-C-target', '-C{}'.format(settings.target_level),\n    ]\n    if settings.args:\n      settings_args = settings.args\n      if any('$JAVA_HOME' in a for a in settings.args):\n        logger.debug('Substituting \"$JAVA_HOME\" with \"{}\" in jvm-platform args.'\n                     .format(distribution.home))\n        settings_args = (a.replace('$JAVA_HOME', distribution.home) for a in settings.args)\n      zinc_args.extend(settings_args)\n    return zinc_args"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scalac_classpath_entries(self):\n    return ScalaPlatform.global_instance().compiler_classpath_entries(\n      self.context.products, self.context._scheduler)", "response": "Returns the classpath entries for the scalac classpath."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\noverride write_extra_resources to produce plugin and annotation processor files.", "response": "def write_extra_resources(self, compile_context):\n    \"\"\"Override write_extra_resources to produce plugin and annotation processor files.\"\"\"\n    target = compile_context.target\n    if isinstance(target, ScalacPlugin):\n      self._write_scalac_plugin_info(compile_context.classes_dir.path, target)\n    elif isinstance(target, JavacPlugin):\n      self._write_javac_plugin_info(compile_context.classes_dir.path, target)\n    elif isinstance(target, AnnotationProcessor) and target.processors:\n      processor_info_file = os.path.join(compile_context.classes_dir.path, _PROCESSOR_INFO_FILE)\n      self._write_processor_info(processor_info_file, target.processors)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind scalac plugins and return a map from plugin name to list of classpath entries.", "response": "def _find_scalac_plugins(self, scalac_plugins, classpath):\n    \"\"\"Returns a map from plugin name to list of plugin classpath entries.\n\n    The first entry in each list is the classpath entry containing the plugin metadata.\n    The rest are the internal transitive deps of the plugin.\n\n    This allows us to have in-repo plugins with dependencies (unlike javac, scalac doesn't load\n    plugins or their deps from the regular classpath, so we have to provide these entries\n    separately, in the -Xplugin: flag).\n\n    Note that we don't currently support external plugins with dependencies, as we can't know which\n    external classpath elements are required, and we'd have to put the entire external classpath\n    on each -Xplugin: flag, which seems excessive.\n    Instead, external plugins should be published as \"fat jars\" (which appears to be the norm,\n    since SBT doesn't support plugins with dependencies anyway).\n    \"\"\"\n    # Allow multiple flags and also comma-separated values in a single flag.\n    plugin_names = {p for val in scalac_plugins for p in val.split(',')}\n    if not plugin_names:\n      return {}\n\n    active_plugins = {}\n    buildroot = get_buildroot()\n\n    cp_product = self.context.products.get_data('runtime_classpath')\n    for classpath_element in classpath:\n      name = self._maybe_get_plugin_name(classpath_element)\n      if name in plugin_names:\n        plugin_target_closure = self._plugin_targets('scalac').get(name, [])\n        # It's important to use relative paths, as the compiler flags get embedded in the zinc\n        # analysis file, and we port those between systems via the artifact cache.\n        rel_classpath_elements = [\n          os.path.relpath(cpe, buildroot) for cpe in\n          ClasspathUtil.internal_classpath(plugin_target_closure, cp_product, self._confs)]\n        # If the plugin is external then rel_classpath_elements will be empty, so we take\n        # just the external jar itself.\n        rel_classpath_elements = rel_classpath_elements or [classpath_element]\n        # Some classpath elements may be repeated, so we allow for that here.\n        if active_plugins.get(name, rel_classpath_elements) != rel_classpath_elements:\n          raise TaskError('Plugin {} defined in {} and in {}'.format(name, active_plugins[name],\n                                                                     classpath_element))\n        active_plugins[name] = rel_classpath_elements\n        if len(active_plugins) == len(plugin_names):\n          # We've found all the plugins, so return now to spare us from processing\n          # of the rest of the classpath for no reason.\n          return active_plugins\n\n    # If we get here we must have unresolved plugins.\n    unresolved_plugins = plugin_names - set(active_plugins.keys())\n    raise TaskError('Could not find requested plugins: {}'.format(list(unresolved_plugins)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _maybe_get_plugin_name(cls, classpath_element):\n    def process_info_file(cp_elem, info_file):\n      plugin_info = ElementTree.parse(info_file).getroot()\n      if plugin_info.tag != 'plugin':\n        raise TaskError('File {} in {} is not a valid scalac plugin descriptor'.format(\n            _SCALAC_PLUGIN_INFO_FILE, cp_elem))\n      return plugin_info.find('name').text\n\n    if os.path.isdir(classpath_element):\n      try:\n        with open(os.path.join(classpath_element, _SCALAC_PLUGIN_INFO_FILE), 'r') as plugin_info_file:\n          return process_info_file(classpath_element, plugin_info_file)\n      except IOError as e:\n        if e.errno != errno.ENOENT:\n          raise\n    else:\n      with open_zip(classpath_element, 'r') as jarfile:\n        try:\n          with closing(jarfile.open(_SCALAC_PLUGIN_INFO_FILE, 'r')) as plugin_info_file:\n            return process_info_file(classpath_element, plugin_info_file)\n        except KeyError:\n          pass\n    return None", "response": "If classpath_element is a scalac plugin returns its name. Otherwise returns None."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nresetting the global runtime state of a pants runtime for cleaner forking.", "response": "def clean_global_runtime_state(reset_subsystem=False):\n  \"\"\"Resets the global runtime state of a pants runtime for cleaner forking.\n\n  :param bool reset_subsystem: Whether or not to clean Subsystem global state.\n  \"\"\"\n  if reset_subsystem:\n    # Reset subsystem state.\n    Subsystem.reset()\n\n  # Reset Goals and Tasks.\n  Goal.clear()\n\n  # Reset global plugin state.\n  BuildConfigInitializer.reset()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nyield the paths between from_target to to_target if they exist.", "response": "def find_paths_breadth_first(from_target, to_target, log):\n  \"\"\"Yields the paths between from_target to to_target if they exist.\n\n  The paths are returned ordered by length, shortest first.\n  If there are cycles, it checks visited edges to prevent recrossing them.\"\"\"\n  log.debug('Looking for all paths from {} to {}'.format(from_target.address.reference(),\n                                                         to_target.address.reference()))\n\n  if from_target == to_target:\n    yield [from_target]\n    return\n\n  visited_edges = set()\n  to_walk_paths = deque([[from_target]])\n  while len(to_walk_paths) > 0:\n    cur_path = to_walk_paths.popleft()\n    target = cur_path[-1]\n\n    if len(cur_path) > 1:\n      prev_target = cur_path[-2]\n    else:\n      prev_target = None\n    current_edge = (prev_target, target)\n\n    if current_edge not in visited_edges:\n      for dep in target.dependencies:\n        dep_path = cur_path + [dep]\n        if dep == to_target:\n          yield dep_path\n        else:\n          to_walk_paths.append(dep_path)\n      visited_edges.add(current_edge)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninverts the dependencies of the given targets.", "response": "def invert_dependencies(targets):\n  \"\"\"\n  :API: public\n\n  :return: the full graph of dependencies for `targets` and the list of roots.\n  \"\"\"\n  roots = set()\n  inverted_deps = defaultdict(OrderedSet)  # target -> dependent targets\n  visited = set()\n  path = OrderedSet()\n\n  def invert(tgt):\n    if tgt in path:\n      path_list = list(path)\n      cycle_head = path_list.index(tgt)\n      cycle = path_list[cycle_head:] + [tgt]\n      raise CycleException(cycle)\n    path.add(tgt)\n    if tgt not in visited:\n      visited.add(tgt)\n      if tgt.dependencies:\n        for dependency in tgt.dependencies:\n          inverted_deps[dependency].add(tgt)\n          invert(dependency)\n      else:\n        roots.add(tgt)\n\n    path.remove(tgt)\n\n  for target in targets:\n    invert(target)\n\n  return roots, inverted_deps"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sort_targets(targets):\n\n  roots, inverted_deps = invert_dependencies(targets)\n  ordered = []\n  visited = set()\n\n  def topological_sort(target):\n    if target not in visited:\n      visited.add(target)\n      if target in inverted_deps:\n        for dep in inverted_deps[target]:\n          topological_sort(dep)\n      ordered.append(target)\n\n  for root in roots:\n    topological_sort(root)\n\n  return ordered", "response": "Sort the targets in order of most dependent to least."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives an iterable of Target instances apply their transitive injectables.", "response": "def apply_injectables(self, targets):\n    \"\"\"Given an iterable of `Target` instances, apply their transitive injectables.\"\"\"\n    target_types = {type(t) for t in targets}\n    target_subsystem_deps = {s for s in itertools.chain(*(t.subsystems() for t in target_types))}\n    for subsystem in target_subsystem_deps:\n      # TODO: The is_initialized() check is primarily for tests and would be nice to do away with.\n      if issubclass(subsystem, InjectablesMixin) and subsystem.is_initialized():\n        subsystem.global_instance().injectables(self)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreset the state of the build graph.", "response": "def reset(self):\n    \"\"\"Clear out the state of the BuildGraph, in particular Target mappings and dependencies.\n\n    :API: public\n    \"\"\"\n    self._target_by_address = OrderedDict()\n    self._target_dependencies_by_address = defaultdict(OrderedSet)\n    self._target_dependees_by_address = defaultdict(OrderedSet)\n    self._derived_from_by_derivative = {}  # Address -> Address.\n    self._derivatives_by_derived_from = defaultdict(list)   # Address -> list of Address.\n    self.synthetic_addresses = set()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a spec into an address and returns the result of get_target", "response": "def get_target_from_spec(self, spec, relative_to=''):\n    \"\"\"Converts `spec` into an address and returns the result of `get_target`\n\n    :API: public\n    \"\"\"\n    return self.get_target(Address.parse(spec, relative_to=relative_to))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the dependencies of the Target at address.", "response": "def dependencies_of(self, address):\n    \"\"\"Returns the dependencies of the Target at `address`.\n\n    This method asserts that the address given is actually in the BuildGraph.\n\n    :API: public\n    \"\"\"\n    assert address in self._target_by_address, (\n      'Cannot retrieve dependencies of {address} because it is not in the BuildGraph.'\n      .format(address=address)\n    )\n    return self._target_dependencies_by_address[address]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dependents_of(self, address):\n    assert address in self._target_by_address, (\n      'Cannot retrieve dependents of {address} because it is not in the BuildGraph.'\n      .format(address=address)\n    )\n    return self._target_dependees_by_address[address]", "response": "Returns the addresses of the targets that depend on the target at address."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the target that the specified target was derived from.", "response": "def get_derived_from(self, address):\n    \"\"\"Get the target the specified target was derived from.\n\n    If a Target was injected programmatically, e.g. from codegen, this allows us to trace its\n    ancestry.  If a Target is not derived, default to returning itself.\n\n    :API: public\n    \"\"\"\n    parent_address = self._derived_from_by_derivative.get(address, address)\n    return self.get_target(parent_address)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the concrete target the specified target was directly or indirectly derived from.", "response": "def get_concrete_derived_from(self, address):\n    \"\"\"Get the concrete target the specified target was (directly or indirectly) derived from.\n\n    The returned target is guaranteed to not have been derived from any other target.\n\n    :API: public\n    \"\"\"\n    current_address = address\n    next_address = self._derived_from_by_derivative.get(current_address, current_address)\n    while next_address != current_address:\n      current_address = next_address\n      next_address = self._derived_from_by_derivative.get(current_address, current_address)\n    return self.get_target(current_address)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets all targets derived directly from the specified target.", "response": "def get_direct_derivatives(self, address):\n    \"\"\"Get all targets derived directly from the specified target.\n\n    Note that the specified target itself is not returned.\n\n    :API: public\n    \"\"\"\n    derivative_addrs = self._derivatives_by_derived_from.get(address, [])\n    return [self.get_target(addr) for addr in derivative_addrs]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_all_derivatives(self, address):\n    ret = []\n    direct = self.get_direct_derivatives(address)\n    ret.extend(direct)\n    for t in direct:\n      ret.extend(self.get_all_derivatives(t.address))\n    return ret", "response": "Get all targets derived directly or indirectly from the specified target."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninjects a fully realized Target into the BuildGraph.", "response": "def inject_target(self, target, dependencies=None, derived_from=None, synthetic=False):\n    \"\"\"Injects a fully realized Target into the BuildGraph.\n\n    :API: public\n\n    :param Target target: The Target to inject.\n    :param list<Address> dependencies: The Target addresses that `target` depends on.\n    :param Target derived_from: The Target that `target` was derived from, usually as a result\n      of codegen.\n    :param bool synthetic: Whether to flag this target as synthetic, even if it isn't derived\n      from another target.\n    \"\"\"\n    if self.contains_address(target.address):\n      raise ValueError('Attempted to inject synthetic {target} derived from {derived_from}'\n                       ' into the BuildGraph with address {address}, but there is already a Target'\n                       ' {existing_target} with that address'\n                       .format(target=target,\n                               derived_from=derived_from,\n                               address=target.address,\n                               existing_target=self.get_target(target.address)))\n\n    dependencies = dependencies or frozenset()\n    address = target.address\n\n    if address in self._target_by_address:\n      raise ValueError('A Target {existing_target} already exists in the BuildGraph at address'\n                       ' {address}.  Failed to insert {target}.'\n                       .format(existing_target=self._target_by_address[address],\n                               address=address,\n                               target=target))\n\n    if derived_from:\n      if not self.contains_address(derived_from.address):\n        raise ValueError('Attempted to inject synthetic {target} derived from {derived_from}'\n                         ' into the BuildGraph, but {derived_from} was not in the BuildGraph.'\n                         ' Synthetic Targets must be derived from no Target (None) or from a'\n                         ' Target already in the BuildGraph.'\n                         .format(target=target,\n                                 derived_from=derived_from))\n      self._derived_from_by_derivative[target.address] = derived_from.address\n      self._derivatives_by_derived_from[derived_from.address].append(target.address)\n\n    if derived_from or synthetic:\n      self.synthetic_addresses.add(address)\n\n    self._target_by_address[address] = target\n\n    for dependency_address in dependencies:\n      self.inject_dependency(dependent=address, dependency=dependency_address)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef inject_dependency(self, dependent, dependency):\n    if dependent not in self._target_by_address:\n      raise ValueError('Cannot inject dependency from {dependent} on {dependency} because the'\n                       ' dependent is not in the BuildGraph.'\n                       .format(dependent=dependent, dependency=dependency))\n\n    # TODO(pl): Unfortunately this is an unhelpful time to error due to a cycle.  Instead, we warn\n    # and allow the cycle to appear.  It is the caller's responsibility to call sort_targets on the\n    # entire graph to generate a friendlier CycleException that actually prints the cycle.\n    # Alternatively, we could call sort_targets after every inject_dependency/inject_target, but\n    # that could have nasty performance implications.  Alternative 2 would be to have an internal\n    # data structure of the topologically sorted graph which would have acceptable amortized\n    # performance for inserting new nodes, and also cycle detection on each insert.\n\n    if dependency not in self._target_by_address:\n      logger.warning('Injecting dependency from {dependent} on {dependency}, but the dependency'\n                     ' is not in the BuildGraph.  This probably indicates a dependency cycle, but'\n                     ' it is not an error until sort_targets is called on a subgraph containing'\n                     ' the cycle.'\n                     .format(dependent=dependent, dependency=dependency))\n\n    if dependency in self.dependencies_of(dependent):\n      logger.debug('{dependent} already depends on {dependency}'\n                   .format(dependent=dependent, dependency=dependency))\n    else:\n      self._target_dependencies_by_address[dependent].add(dependency)\n      self._target_dependees_by_address[dependency].add(dependent)", "response": "Injects a dependency from dependent onto dependency."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconstructs the right context object for managing state during a transitive walk.", "response": "def _walk_factory(self, dep_predicate):\n    \"\"\"Construct the right context object for managing state during a transitive walk.\"\"\"\n    walk = None\n    if dep_predicate:\n      walk = self.DepPredicateWalk(dep_predicate)\n    else:\n      walk = self.NoDepPredicateWalk()\n    return walk"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef walk_transitive_dependency_graph(self,\n                                       addresses,\n                                       work,\n                                       predicate=None,\n                                       postorder=False,\n                                       dep_predicate=None,\n                                       prelude=None,\n                                       epilogue=None):\n    \"\"\"Given a work function, walks the transitive dependency closure of `addresses` using DFS.\n\n    :API: public\n\n    :param list<Address> addresses: The closure of `addresses` will be walked.\n    :param function work: The function that will be called on every target in the closure using\n      the specified traversal order.\n    :param bool postorder: When ``True``, the traversal order is postorder (children before\n      parents), else it is preorder (parents before children).\n    :param function predicate: If this parameter is not given, no Targets will be filtered\n      out of the closure.  If it is given, any Target which fails the predicate will not be\n      walked, nor will its dependencies.  Thus predicate effectively trims out any subgraph\n      that would only be reachable through Targets that fail the predicate.\n    :param function dep_predicate: Takes two parameters, the current target and the dependency of\n      the current target. If this parameter is not given, no dependencies will be filtered\n      when traversing the closure. If it is given, when the predicate fails, the edge to the dependency\n      will not be expanded.\n    :param function prelude: Function to run before any dependency expansion.\n      It takes the currently explored target as an argument.\n      If ``postorder`` is ``False``, it will run after the current node is visited.\n      It is not affected by ``dep_predicate``.\n      It is not run if ``predicate`` does not succeed.\n    :param function epilogue: Function to run after children nodes are visited.\n      It takes the currently explored target as an argument.\n      If ``postorder`` is ``True``, it runs before visiting the current node.\n      It is not affected by ``dep_predicate``.\n      It is not run if ``predicate`` is not passed.\n    \"\"\"\n    walk = self._walk_factory(dep_predicate)\n\n    def _walk_rec(addr, level=0):\n      # If we've followed an edge to this address, stop recursing.\n      if not walk.expand_once(addr, level):\n        return\n\n      target = self._target_by_address[addr]\n\n      if predicate and not predicate(target):\n        return\n\n      if not postorder and walk.do_work_once(addr):\n        work(target)\n\n      if prelude:\n        prelude(target)\n\n      for dep_address in self._target_dependencies_by_address[addr]:\n        if walk.expanded_or_worked(dep_address):\n          continue\n        if walk.dep_predicate(target, self._target_by_address[dep_address], level):\n          _walk_rec(dep_address, level + 1)\n\n      if epilogue:\n        epilogue(target)\n\n      if postorder and walk.do_work_once(addr):\n        work(target)\n\n    for address in addresses:\n      _walk_rec(address)", "response": "Given a function that will be called on every target in the transitive dependency closure of addresses and walks the transitive dependency closure of addresses."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef walk_transitive_dependee_graph(self,\n                                     addresses,\n                                     work,\n                                     predicate=None,\n                                     postorder=False,\n                                     prelude=None,\n                                     epilogue=None):\n    \"\"\"Identical to `walk_transitive_dependency_graph`, but walks dependees preorder (or postorder\n    if the postorder parameter is True).\n\n    This is identical to reversing the direction of every arrow in the DAG, then calling\n    `walk_transitive_dependency_graph`.\n\n    :API: public\n    \"\"\"\n    walked = set()\n\n    def _walk_rec(addr):\n      if addr not in walked:\n        walked.add(addr)\n        target = self._target_by_address[addr]\n        if not predicate or predicate(target):\n          if not postorder:\n            work(target)\n          if prelude:\n            prelude(target)\n          for dep_address in self._target_dependees_by_address[addr]:\n            _walk_rec(dep_address)\n          if epilogue:\n            epilogue(target)\n          if postorder:\n            work(target)\n    for address in addresses:\n      _walk_rec(address)", "response": "This function walks the transitive dependency graph of the given addresses and work and returns a tree of nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns all transitive dependees of addresses.", "response": "def transitive_dependees_of_addresses(self, addresses, predicate=None, postorder=False):\n    \"\"\"Returns all transitive dependees of `addresses`.\n\n    Note that this uses `walk_transitive_dependee_graph` and the predicate is passed through,\n    hence it trims graphs rather than just filtering out Targets that do not match the predicate.\n    See `walk_transitive_dependee_graph for more detail on `predicate`.\n\n    :API: public\n\n    :param list<Address> addresses: The root addresses to transitively close over.\n    :param function predicate: The predicate passed through to `walk_transitive_dependee_graph`.\n    \"\"\"\n    ret = OrderedSet()\n    self.walk_transitive_dependee_graph(addresses, ret.add, predicate=predicate,\n                                        postorder=postorder)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns all transitive dependencies of addresses.", "response": "def transitive_subgraph_of_addresses(self, addresses, *vargs, **kwargs):\n    \"\"\"Returns all transitive dependencies of `addresses`.\n\n    Note that this uses `walk_transitive_dependencies_graph` and the predicate is passed through,\n    hence it trims graphs rather than just filtering out Targets that do not match the predicate.\n    See `walk_transitive_dependency_graph for more detail on `predicate`.\n\n    :API: public\n\n    :param list<Address> addresses: The root addresses to transitively close over.\n    :param function predicate: The predicate passed through to\n      `walk_transitive_dependencies_graph`.\n    :param bool postorder: When ``True``, the traversal order is postorder (children before\n      parents), else it is preorder (parents before children).\n    :param function predicate: If this parameter is not given, no Targets will be filtered\n      out of the closure.  If it is given, any Target which fails the predicate will not be\n      walked, nor will its dependencies.  Thus predicate effectively trims out any subgraph\n      that would only be reachable through Targets that fail the predicate.\n    :param function dep_predicate: Takes two parameters, the current target and the dependency of\n      the current target. If this parameter is not given, no dependencies will be filtered\n      when traversing the closure. If it is given, when the predicate fails, the edge to the dependency\n      will not be expanded.\n    \"\"\"\n    ret = OrderedSet()\n    self.walk_transitive_dependency_graph(addresses, ret.add,\n                                          *vargs,\n                                          **kwargs)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef transitive_subgraph_of_addresses_bfs(self,\n                                           addresses,\n                                           predicate=None,\n                                           dep_predicate=None):\n    \"\"\"Returns the transitive dependency closure of `addresses` using BFS.\n\n    :API: public\n\n    :param list<Address> addresses: The closure of `addresses` will be walked.\n    :param function predicate: If this parameter is not given, no Targets will be filtered\n      out of the closure.  If it is given, any Target which fails the predicate will not be\n      walked, nor will its dependencies.  Thus predicate effectively trims out any subgraph\n      that would only be reachable through Targets that fail the predicate.\n    :param function dep_predicate: Takes two parameters, the current target and the dependency of\n      the current target. If this parameter is not given, no dependencies will be filtered\n      when traversing the closure. If it is given, when the predicate fails, the edge to the dependency\n      will not be expanded.\n    \"\"\"\n    walk = self._walk_factory(dep_predicate)\n\n    ordered_closure = OrderedSet()\n    to_walk = deque((0, addr) for addr in addresses)\n    while len(to_walk) > 0:\n      level, address = to_walk.popleft()\n\n      if not walk.expand_once(address, level):\n        continue\n\n      target = self._target_by_address[address]\n      if predicate and not predicate(target):\n        continue\n      if walk.do_work_once(address):\n        ordered_closure.add(target)\n      for dep_address in self._target_dependencies_by_address[address]:\n        if walk.expanded_or_worked(dep_address):\n          continue\n        if walk.dep_predicate(target, self._target_by_address[dep_address], level):\n          to_walk.append((level + 1, dep_address))\n    return ordered_closure", "response": "Returns the transitive dependency closure of addresses using BFS."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resolve(self, spec):\n    address = Address.parse(spec)\n    # NB: This is an idempotent, short-circuiting call.\n    self.inject_address_closure(address)\n    return self.transitive_subgraph_of_addresses([address])", "response": "Returns an iterator over the target s the given address points to."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the indexable targets in the given context.", "response": "def get(self, context):\n    \"\"\"Return the indexable targets in the given context.\n\n    Computes them lazily from the given context.  They are then fixed for the duration\n    of the run, even if this method is called again with a different context.\n    \"\"\"\n    if self.get_options().recursive:\n      requested_targets = context.targets(exclude_scopes=Scope(self.get_options().exclude_scopes))\n    else:\n      requested_targets = list(context.target_roots)\n\n    expanded_targets = list(requested_targets)\n    # We want to act on targets derived from the specified, e.g., if acting on a binary\n    # jar_library we actually want to act on the derived java_library wrapping the decompiled\n    # sources.\n    for t in requested_targets:\n      expanded_targets.extend(context.build_graph.get_all_derivatives(t.address))\n\n    return tuple(sorted(\n      [t for t in expanded_targets if isinstance(t, JvmTarget) and t.has_sources('.java')],\n      key=lambda t: t.address.spec))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_builtin_name(name):\n  if name.startswith('__') and name.endswith('__'):\n    return ALL_LOWER_CASE_RE.match(name[2:-2]) is not None\n  return False", "response": "Check if a name is a builtin name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the location of the config files.", "response": "def get_config_file_paths(env, args):\n    \"\"\"Get the location of the config files.\n\n    The locations are specified by the --pants-config-files option.  However we need to load the\n    config in order to process the options.  This method special-cases --pants-config-files\n    in order to solve this chicken-and-egg problem.\n\n    Note that, obviously, it's not possible to set the location of config files in a config file.\n    Doing so will have no effect.\n    \"\"\"\n    # This exactly mirrors the logic applied in Option to all regular options.  Note that we'll\n    # also parse --pants-config as a regular option later, but there's no harm in that.  In fact,\n    # it's preferable, so that any code that happens to want to know where we read config from\n    # can inspect the option.\n    flag = '--pants-config-files='\n    evars = ['PANTS_GLOBAL_PANTS_CONFIG_FILES', 'PANTS_PANTS_CONFIG_FILES', 'PANTS_CONFIG_FILES']\n\n    path_list_values = []\n    if os.path.isfile(get_default_pants_config_file()):\n      path_list_values.append(ListValueComponent.create(get_default_pants_config_file()))\n    for var in evars:\n      if var in env:\n        path_list_values.append(ListValueComponent.create(env[var]))\n        break\n\n    for arg in args:\n      # Technically this is very slightly incorrect, as we don't check scope.  But it's\n      # very unlikely that any task or subsystem will have an option named --pants-config-files.\n      # TODO: Enforce a ban on options with a --pants- prefix outside our global options?\n      if arg.startswith(flag):\n        path_list_values.append(ListValueComponent.create(arg[len(flag):]))\n\n    return ListValueComponent.merge(path_list_values).val"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create(cls, env=None, args=None):\n    env = {k: v for k, v in (os.environ if env is None else env).items()\n           if k.startswith('PANTS_')}\n    args = tuple(sys.argv if args is None else args)\n\n    flags = set()\n    short_flags = set()\n\n    def filecontent_for(path):\n      return FileContent(ensure_text(path), read_file(path, binary_mode=True))\n\n    def capture_the_flags(*args, **kwargs):\n      for arg in args:\n        flags.add(arg)\n        if len(arg) == 2:\n          short_flags.add(arg)\n        elif kwargs.get('type') == bool:\n          flags.add('--no-{}'.format(arg[2:]))\n\n    GlobalOptionsRegistrar.register_bootstrap_options(capture_the_flags)\n\n    def is_bootstrap_option(arg):\n      components = arg.split('=', 1)\n      if components[0] in flags:\n        return True\n      for flag in short_flags:\n        if arg.startswith(flag):\n          return True\n      return False\n\n    # Take just the bootstrap args, so we don't choke on other global-scope args on the cmd line.\n    # Stop before '--' since args after that are pass-through and may have duplicate names to our\n    # bootstrap options.\n    bargs = tuple(filter(is_bootstrap_option, itertools.takewhile(lambda arg: arg != '--', args)))\n\n    config_file_paths = cls.get_config_file_paths(env=env, args=args)\n    config_files_products = [filecontent_for(p) for p in config_file_paths]\n    pre_bootstrap_config = Config.load_file_contents(config_files_products)\n\n    initial_bootstrap_options = cls.parse_bootstrap_options(env, bargs, pre_bootstrap_config)\n    bootstrap_option_values = initial_bootstrap_options.for_global_scope()\n\n    # Now re-read the config, post-bootstrapping. Note the order: First whatever we bootstrapped\n    # from (typically pants.ini), then config override, then rcfiles.\n    full_configpaths = pre_bootstrap_config.sources()\n    if bootstrap_option_values.pantsrc:\n      rcfiles = [os.path.expanduser(str(rcfile)) for rcfile in bootstrap_option_values.pantsrc_files]\n      existing_rcfiles = list(filter(os.path.exists, rcfiles))\n      full_configpaths.extend(existing_rcfiles)\n\n    full_config_files_products = [filecontent_for(p) for p in full_configpaths]\n    post_bootstrap_config = Config.load_file_contents(\n      full_config_files_products,\n      seed_values=bootstrap_option_values\n    )\n\n    env_tuples = tuple(sorted(iteritems(env), key=lambda x: x[0]))\n    return cls(env_tuples=env_tuples, bootstrap_args=bargs, args=args, config=post_bootstrap_config)", "response": "Create an OptionsBootstrapper object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nverifies all loaded configs have correct scopes and options.", "response": "def verify_configs_against_options(self, options):\n    \"\"\"Verify all loaded configs have correct scopes and options.\n\n    :param options: Fully bootstrapped valid options.\n    :return: None.\n    \"\"\"\n    error_log = []\n    for config in self.config.configs():\n      for section in config.sections():\n        if section == GLOBAL_SCOPE_CONFIG_SECTION:\n          scope = GLOBAL_SCOPE\n        else:\n          scope = section\n        try:\n          valid_options_under_scope = set(options.for_scope(scope))\n        # Only catch ConfigValidationError. Other exceptions will be raised directly.\n        except Config.ConfigValidationError:\n          error_log.append(\"Invalid scope [{}] in {}\".format(section, config.configpath))\n        else:\n          # All the options specified under [`section`] in `config` excluding bootstrap defaults.\n          all_options_under_scope = (set(config.configparser.options(section)) -\n                                     set(config.configparser.defaults()))\n          for option in all_options_under_scope:\n            if option not in valid_options_under_scope:\n              error_log.append(\"Invalid option '{}' under [{}] in {}\".format(option, section, config.configpath))\n\n    if error_log:\n      for error in error_log:\n        logger.error(error)\n      raise Config.ConfigValidationError(\"Invalid config entries detected. \"\n                              \"See log for details on which entries to update or remove.\\n\"\n                              \"(Specify --no-verify-config to disable this check.)\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_spec(self, spec):\n\n    if spec.endswith('::'):\n      spec_path = spec[:-len('::')]\n      return DescendantAddresses(self._normalize_spec_path(spec_path))\n    elif spec.endswith(':'):\n      spec_path = spec[:-len(':')]\n      return SiblingAddresses(self._normalize_spec_path(spec_path))\n    else:\n      spec_parts = spec.rsplit(':', 1)\n      spec_path = self._normalize_spec_path(spec_parts[0]) \n      name = spec_parts[1] if len(spec_parts) > 1 else os.path.basename(spec_path)\n      return SingleAddress(spec_path, name)", "response": "Parse the given spec into a single specs. Spec object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _require_homogeneous_roots(self, accept_predicate, reject_predicate):\n    if len(self.context.target_roots) == 0:\n      raise self.NoActivationsError('No target specified.')\n\n    def resolve(targets):\n      # Recursively resolve target aliases.\n      for t in targets:\n        if type(t) == Target:\n          for r in resolve(t.dependencies):\n            yield r\n        else:\n          yield t\n\n    expanded_roots = list(resolve(self.context.target_roots))\n\n    accepted = list(filter(accept_predicate, expanded_roots))\n    rejected = list(filter(reject_predicate, expanded_roots))\n    if len(accepted) == 0:\n      # no targets were accepted, regardless of rejects\n      return None\n    elif len(rejected) == 0:\n      # we have at least one accepted target, and no rejected targets\n      return accepted\n    else:\n      # both accepted and rejected targets\n      # TODO: once https://github.com/pantsbuild/pants/issues/425 lands, we should add\n      # language-specific flags that would resolve the ambiguity here\n      def render_target(target):\n        return '{} (a {})'.format(target.address.reference(), target.type_alias)\n      raise self.IncompatibleActivationsError('Mutually incompatible targets specified: {} vs {} '\n                                              '(and {} others)'\n                                              .format(render_target(accepted[0]),\n                                                      render_target(rejected[0]),\n                                                      len(accepted) + len(rejected) - 2))", "response": "Ensures that there are no ambiguity in the context according to the given predicates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_header(filename, is_newly_created=False):\n  try:\n    with open(filename, 'r') as pyfile:\n      buf = \"\"\n      for lineno in range(1,7):\n        line = pyfile.readline()\n        # Skip shebang line\n        if lineno == 1 and line.startswith('#!'):\n          line = pyfile.readline()\n        # Check if the copyright year can be parsed as within the current century, or the current\n        # year if it is a new file.\n        if line.startswith(\"# Copyright\"):\n          year = line[12:16]\n          if is_newly_created:\n            if not year == _current_year:\n              raise HeaderCheckFailure('{}: copyright year must be {} (was {})'\n                                       .format(filename, _current_year, year))\n          else:\n            if not _current_century_regex.match(year):\n              raise HeaderCheckFailure(\n                \"{}: copyright year must match '{}' (was {}): current year is {}\"\n                .format(filename, _current_century_regex.pattern, year, _current_year))\n          line = \"# Copyright YYYY\" + line[16:]\n        buf += line\n      if buf != EXPECTED_HEADER:\n        raise HeaderCheckFailure('{}: failed to parse header at all'\n                                 .format(filename))\n  except IOError as e:\n    raise HeaderCheckFailure('{}: error while reading input ({})'\n                             .format(filename, str(e)))", "response": "Raises HeaderCheckFailure if the header doesn t match."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck the directory for header files that fail the check.", "response": "def check_dir(directory, newly_created_files):\n  \"\"\"Returns list of files that fail the check.\"\"\"\n  header_parse_failures = []\n  for root, dirs, files in os.walk(directory):\n    for f in files:\n      if f.endswith('.py') and os.path.basename(f) != '__init__.py':\n        filename = os.path.join(root, f)\n        try:\n          check_header(filename, filename in newly_created_files)\n        except HeaderCheckFailure as e:\n          header_parse_failures.append(e.message)\n  return header_parse_failures"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the package manager for the given target argument or global config.", "response": "def get_package_manager(self, target=None):\n    \"\"\"Returns package manager for target argument or global config.\"\"\"\n    package_manager = None\n    if target:\n      target_package_manager_field = target.payload.get_field('package_manager')\n      if target_package_manager_field:\n        package_manager = target_package_manager_field.value\n    return self.node_distribution.get_package_manager(package_manager=package_manager)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef execute_node(self, args, workunit_name, workunit_labels=None, node_paths=None):\n    node_command = self.node_distribution.node_command(args=args, node_paths=node_paths)\n    return self._execute_command(node_command,\n                                 workunit_name=workunit_name,\n                                 workunit_labels=workunit_labels)", "response": "Executes a node command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_package(\n    self, target=None, package_manager=None, \n    package=None, type_option=None, version_option=None,\n    node_paths=None, workunit_name=None, workunit_labels=None):\n    \"\"\"Add an additional package using requested package_manager.\"\"\"\n    package_manager = package_manager or self.get_package_manager(target=target)\n    command = package_manager.add_package(\n      package,\n      type_option=type_option,\n      version_option=version_option,\n      node_paths=node_paths,\n    )\n    return self._execute_command(\n      command, workunit_name=workunit_name, workunit_labels=workunit_labels)", "response": "Add an additional package to the current object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninstall node module using requested package_manager.", "response": "def install_module(\n    self, target=None, package_manager=None, \n    install_optional=False, production_only=False, force=False, \n    node_paths=None, frozen_lockfile=None, workunit_name=None, workunit_labels=None):\n    \"\"\"Installs node module using requested package_manager.\"\"\"\n    package_manager = package_manager or self.get_package_manager(target=target)\n    command = package_manager.install_module(\n      install_optional=install_optional,\n      force=force,\n      production_only=production_only,\n      node_paths=node_paths,\n      frozen_lockfile=frozen_lockfile\n    )\n    return self._execute_command(\n      command, workunit_name=workunit_name, workunit_labels=workunit_labels)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute a node or npm command via self. _run_node_distribution_command.", "response": "def _execute_command(self, command, workunit_name=None, workunit_labels=None):\n    \"\"\"Executes a node or npm command via self._run_node_distribution_command.\n\n    :param NodeDistribution.Command command: The command to run.\n    :param string workunit_name: A name for the execution's work unit; default command.executable.\n    :param list workunit_labels: Any extra :class:`pants.base.workunit.WorkUnitLabel`s to apply.\n    :returns: A tuple of (returncode, command).\n    :rtype: A tuple of (int,\n            :class:`pants.contrib.node.subsystems.node_distribution.NodeDistribution.Command`)\n    \"\"\"\n    workunit_name = workunit_name or command.executable\n    workunit_labels = {WorkUnitLabel.TOOL} | set(workunit_labels or ())\n    with self.context.new_workunit(name=workunit_name,\n                                   labels=workunit_labels,\n                                   cmd=str(command)) as workunit:\n      returncode = self._run_node_distribution_command(command, workunit)\n      workunit.set_outcome(WorkUnit.SUCCESS if returncode == 0 else WorkUnit.FAILURE)\n      return returncode, command"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning a NodeDistribution. Command for _execute_command and returns its return code.", "response": "def _run_node_distribution_command(self, command, workunit):\n    \"\"\"Runs a NodeDistribution.Command for _execute_command and returns its return code.\n\n    Passes any additional kwargs to command.run (which passes them, modified, to subprocess.Popen).\n    Override this in a Task subclass to do something more complicated than just calling\n    command.run() and returning the result of wait().\n\n    :param NodeDistribution.Command command: The command to run.\n    :param WorkUnit workunit: The WorkUnit the command is running under.\n    :returns: returncode\n    :rtype: int\n    \"\"\"\n    process = command.run(stdout=workunit.output('stdout'),\n                          stderr=workunit.output('stderr'))\n    return process.wait()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads config info from a. json file and return it.", "response": "def load_config(json_path):\n  \"\"\"Load config info from a .json file and return it.\"\"\"\n  with open(json_path, 'r') as json_file:\n    config = json.loads(json_file.read())\n  # sanity-test the config:\n  assert(config['tree'][0]['page'] == 'index')\n  return config"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_soups(config):\n  soups = {}\n  for page, path in config['sources'].items():\n    with open(path, 'r') as orig_file:\n      soups[page] = beautiful_soup(orig_file.read(), features='html.parser')\n  return soups", "response": "Generate BeautifulSoup AST for each page listed in config."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef precompute_pantsrefs(soups):\n  accumulator = {}\n  for (page, soup) in soups.items():\n    existing_anchors = find_existing_anchors(soup)\n    count = 100\n    for tag in soup.find_all('a'):\n      if not tag.has_attr('pantsmark'):\n        continue\n      pantsmark = tag['pantsmark']\n      if pantsmark in accumulator:\n        raise TaskError('pantsmarks are unique but \"{}\" appears in {} and {}'\n                        .format(pantsmark, page, accumulator[pantsmark]))\n\n      # To link to a place \"mid-page\", we need an HTML anchor.\n      # If this tag already has such an anchor, use it.\n      # Else, make one up.\n      anchor = tag.get('id') or tag.get('name')\n      if not anchor:\n        anchor = pantsmark\n        while anchor in existing_anchors:\n          count += 1\n          anchor = '{}_{}'.format(pantsmark, count)\n        tag['id'] = anchor\n        existing_anchors = find_existing_anchors(soup)\n\n      link = '{}.html#{}'.format(page, anchor)\n      accumulator[pantsmark] = link\n  return accumulator", "response": "Precompute the pantsrefs for the tags."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef precompute(config, soups):\n  show_toc = config.get('show_toc', {})\n  page = {}\n  pantsrefs = precompute_pantsrefs(soups)\n  for p, soup in soups.items():\n    title = get_title(soup) or p\n    page[p] = PrecomputedPageInfo(title=title, show_toc=show_toc.get(p, True))\n  return Precomputed(page=page, pantsref=pantsrefs)", "response": "Return info we want to compute and preserve before we mutate things."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fixup_internal_links(config, soups):\n  # Pages can come from different dirs; they can go to different dirs.\n  # Thus, there's some relative-path-computing here.\n  reverse_directory = {}\n  for d, s in config['sources'].items():\n    reverse_directory[s] = d\n  for name, soup in soups.items():\n    old_src_dir = os.path.dirname(config['sources'][name])\n    for tag in soup.find_all(True):\n      if not 'href' in tag.attrs:\n        continue\n      old_rel_path = tag['href'].split('#')[0]\n      old_dst = os.path.normpath(os.path.join(old_src_dir, old_rel_path))\n      if not old_dst in reverse_directory:\n        continue\n      new_dst = reverse_directory[old_dst] + '.html'\n      new_rel_path = rel_href(name, new_dst)\n      # string replace instead of assign to not loose anchor in foo.html#anchor\n      tag['href'] = tag['href'].replace(old_rel_path, new_rel_path, 1)", "response": "Find href =... links that link to pages in our docset ; fix them up."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rel_href(src, dst):\n  src_dir = os.path.dirname(src)\n  return os.path.relpath(dst, src_dir)", "response": "For src = foo. html dst = bar. html return relative link '../garply. html#frotz."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_existing_anchors(soup):\n  existing_anchors = set()\n  for tag in soup.find_all(True):\n    for attr in ['id', 'name']:\n      if tag.has_attr(attr):\n        existing_anchors.add(tag.get(attr))\n  return existing_anchors", "response": "Return a set of ids and names from a soup."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef link_pantsrefs(soups, precomputed):\n  for (page, soup) in soups.items():\n    for a in soup.find_all('a'):\n      if not a.has_attr('pantsref'):\n        continue\n      pantsref = a['pantsref']\n      if not pantsref in precomputed.pantsref:\n        raise TaskError('Page {} has pantsref \"{}\" and I cannot find pantsmark for'\n                        ' it'.format(page, pantsref))\n      a['href'] = rel_href(page, precomputed.pantsref[pantsref])", "response": "Transorm soups with pantsrefs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef transform_soups(config, soups, precomputed):\n  fixup_internal_links(config, soups)\n  ensure_headings_linkable(soups)\n\n  # Do this after ensure_headings_linkable so that there will be links.\n  generate_page_tocs(soups, precomputed)\n  link_pantsrefs(soups, precomputed)", "response": "Mutate our soups to be better when we write them out later."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a soup pick out a title", "response": "def get_title(soup):\n  \"\"\"Given a soup, pick out a title\"\"\"\n  if soup.title:\n    return soup.title.string\n  if soup.h1:\n    return soup.h1.string\n  return ''"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing an h tag s outline depth.", "response": "def hdepth(tag):\n  \"\"\"Compute an h tag's \"outline depth\".\n\n  E.g., h1 at top level is 1, h1 in a section is 2, h2 at top level is 2.\n  \"\"\"\n  if not _heading_re.search(tag.name):\n    raise TaskError(\"Can't compute heading depth of non-heading {}\".format(tag))\n  depth = int(tag.name[1], 10)  # get the 2 from 'h2'\n  cursor = tag\n  while cursor:\n    if cursor.name == 'section':\n      depth += 1\n    cursor = cursor.parent\n  return depth"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_page_toc(soup):\n  # Maybe we don't want to show all the headings. E.g., it's common for a page\n  # to have just one H1, a title at the top. Our heuristic: if a page has just\n  # one heading of some outline level, don't show it.\n  found_depth_counts = collections.defaultdict(int)\n  for tag in soup.find_all(_heading_re):\n    if (tag.get('id') or tag.get('name')):\n      found_depth_counts[hdepth(tag)] += 1\n\n  depth_list = [i for i in range(100) if 1 < found_depth_counts[i]]\n  depth_list = depth_list[:4]\n  toc = []\n  for tag in soup.find_all(_heading_re):\n    depth = hdepth(tag)\n    if depth in depth_list:\n      toc.append(dict(depth=depth_list.index(depth) + 1,\n                      link=tag.get('id') or tag.get('name'),\n                      text=tag.text))\n  return toc", "response": "Generate a list of TOC template data for a page."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncopying over extra files named in config. json", "response": "def copy_extras(config):\n  \"\"\"copy over \"extra\" files named in config json: stylesheets, logos, ...\"\"\"\n  outdir = config['outdir']\n  for dst, src in config['extras'].items():\n    dst_path = os.path.join(outdir, dst)\n    dst_dir = os.path.dirname(dst_path)\n    if not os.path.isdir(dst_dir):\n      os.makedirs(dst_dir)\n    shutil.copy(src, dst_path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the list of jars to resolve and pin.", "response": "def _compute_jars_to_resolve_and_pin(raw_jars, artifact_set, manager):\n    \"\"\"\n    This method provides settled lists of jar dependencies and coordinates\n    based on conflict management.\n\n    :param raw_jars: a collection of `JarDependencies`\n    :param artifact_set: PinnedJarArtifactSet\n    :param manager: JarDependencyManagement\n    :return: (list of settled `JarDependency`, set of pinned `M2Coordinate`)\n    \"\"\"\n    if artifact_set is None:\n      artifact_set = PinnedJarArtifactSet()\n\n    untouched_pinned_artifact = {M2Coordinate.create(x) for x in artifact_set}\n    jar_list = list(raw_jars)\n    for i, dep in enumerate(jar_list):\n      direct_coord = M2Coordinate.create(dep)\n      # Portion to manage pinned jars in case of conflict\n      if direct_coord in artifact_set:\n        managed_coord = artifact_set[direct_coord]\n        untouched_pinned_artifact.remove(managed_coord)\n\n        if direct_coord.rev != managed_coord.rev:\n          # It may be necessary to actually change the version number of the jar we want to resolve\n          # here, because overrides do not apply directly (they are exclusively transitive). This is\n          # actually a good thing, because it gives us more control over what happens.\n          coord = manager.resolve_version_conflict(managed_coord, direct_coord, force=dep.force)\n\n          # Once a version is settled, we force it anyway\n          jar_list[i] = dep.copy(rev=coord.rev, force=True)\n\n    return jar_list, untouched_pinned_artifact"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef resolve(self, targets, compile_classpath, sources, javadoc, executor):\n    manager = JarDependencyManagement.global_instance()\n\n    jar_targets = manager.targets_by_artifact_set(targets)\n\n    executor = executor or SubprocessExecutor(DistributionLocator.cached())\n    if not isinstance(executor, Executor):\n      raise ValueError('The executor argument must be an Executor instance, given {} of type {}'.format(\n        executor, type(executor)))\n\n    for artifact_set, target_subset in jar_targets.items():\n      # TODO(wisechengyi): this is the only place we are using IvyUtil method, which isn't specific to ivy really.\n      raw_jar_deps, global_excludes = IvyUtils.calculate_classpath(target_subset)\n\n      # ['sources'] * False = [], ['sources'] * True = ['sources']\n      confs_for_fingerprint = ['sources'] * sources + ['javadoc'] * javadoc\n      fp_strategy = CoursierResolveFingerprintStrategy(confs_for_fingerprint)\n\n      compile_classpath.add_excludes_for_targets(target_subset)\n\n      with self.invalidated(target_subset,\n                            invalidate_dependents=False,\n                            silent=False,\n                            fingerprint_strategy=fp_strategy) as invalidation_check:\n\n        if not invalidation_check.all_vts:\n          continue\n\n        resolve_vts = VersionedTargetSet.from_versioned_targets(invalidation_check.all_vts)\n\n        vt_set_results_dir = self._prepare_vts_results_dir(resolve_vts)\n        pants_jar_base_dir = self._prepare_workdir()\n        coursier_cache_dir = CoursierSubsystem.global_instance().get_options().cache_dir\n\n        # If a report is requested, do not proceed with loading validated result.\n        if not self.get_options().report:\n          # Check each individual target without context first\n          # If the individuals are valid, check them as a VersionedTargetSet\n          if not invalidation_check.invalid_vts and resolve_vts.valid:\n            # Load up from the results dir\n            success = self._load_from_results_dir(compile_classpath, vt_set_results_dir,\n                                                  coursier_cache_dir, invalidation_check, pants_jar_base_dir)\n            if success:\n              return\n\n        jars_to_resolve, pinned_coords = self._compute_jars_to_resolve_and_pin(raw_jar_deps,\n                                                                               artifact_set,\n                                                                               manager)\n\n        results = self._get_result_from_coursier(jars_to_resolve, global_excludes, pinned_coords,\n                                                 coursier_cache_dir, sources, javadoc, executor)\n\n        for conf, result_list in results.items():\n          for result in result_list:\n            self._load_json_result(conf, compile_classpath, coursier_cache_dir, invalidation_check,\n                                   pants_jar_base_dir, result, self._override_classifiers_for_conf(conf))\n\n        self._populate_results_dir(vt_set_results_dir, results)\n        resolve_vts.update()", "response": "This method is used to resolve targets against a coursier target set."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprepare the results dir for the given VergetTargetSet.", "response": "def _prepare_vts_results_dir(self, vts):\n    \"\"\"\n    Given a `VergetTargetSet`, prepare its results dir.\n    \"\"\"\n    vt_set_results_dir = os.path.join(self.versioned_workdir, 'results', vts.cache_key.hash)\n    safe_mkdir(vt_set_results_dir)\n    return vt_set_results_dir"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprepares the location in our task workdir to store all the hardlinks to coursier cache dir.", "response": "def _prepare_workdir(self):\n    \"\"\"Prepare the location in our task workdir to store all the hardlinks to coursier cache dir.\"\"\"\n    pants_jar_base_dir = os.path.join(self.versioned_workdir, 'cache')\n    safe_mkdir(pants_jar_base_dir)\n    return pants_jar_base_dir"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls coursier and return the result per invocation. If coursier was called once for classifier '' and once for classifier 'tests', then the return value would be: {'default': [<first coursier output>, <second coursier output>]} :param jars_to_resolve: List of `JarDependency`s to resolve :param global_excludes: List of `M2Coordinate`s to exclude globally :param pinned_coords: List of `M2Coordinate`s that need to be pinned. :param coursier_cache_path: path to where coursier cache is stored. :param executor: An instance of `pants.java.executor.Executor` :return: The aggregation of results by conf from coursier. Each coursier call could return the following: { \"conflict_resolution\": { \"org:name:version\" (requested): \"org:name:version\" (reconciled) }, \"dependencies\": [ { \"coord\": \"orgA:nameA:versionA\", \"file\": <path>, \"dependencies\": [ // coodinates for its transitive dependencies <orgX:nameX:versionX>, <orgY:nameY:versionY>, ] }, { \"coord\": \"orgB:nameB:jar:classifier:versionB\", \"file\": <path>, \"dependencies\": [ // coodinates for its transitive dependencies <orgX:nameX:versionX>, <orgZ:nameZ:versionZ>, ] }, ... // more about orgX:nameX:versionX, orgY:nameY:versionY, orgZ:nameZ:versionZ ] } Hence the aggregation of the results will be in the following format, for example when default classifier and sources are fetched: { 'default': [<result from coursier call with default conf with classifier X>, <result from coursier call with default conf with classifier Y>], 'src_doc': [<result from coursier call with --sources and/or --javadoc>], }", "response": "def _get_result_from_coursier(self, jars_to_resolve, global_excludes, pinned_coords,\n                                coursier_cache_path, sources, javadoc, executor):\n    \"\"\"\n    Calling coursier and return the result per invocation.\n\n    If coursier was called once for classifier '' and once for classifier 'tests', then the return value\n    would be: {'default': [<first coursier output>, <second coursier output>]}\n\n    :param jars_to_resolve: List of `JarDependency`s to resolve\n    :param global_excludes: List of `M2Coordinate`s to exclude globally\n    :param pinned_coords: List of `M2Coordinate`s that need to be pinned.\n    :param coursier_cache_path: path to where coursier cache is stored.\n    :param executor: An instance of `pants.java.executor.Executor`\n\n    :return: The aggregation of results by conf from coursier. Each coursier call could return\n    the following:\n        {\n          \"conflict_resolution\": {\n            \"org:name:version\" (requested): \"org:name:version\" (reconciled)\n          },\n          \"dependencies\": [\n            {\n              \"coord\": \"orgA:nameA:versionA\",\n              \"file\": <path>,\n              \"dependencies\": [ // coodinates for its transitive dependencies\n                <orgX:nameX:versionX>,\n                <orgY:nameY:versionY>,\n              ]\n            },\n            {\n              \"coord\": \"orgB:nameB:jar:classifier:versionB\",\n              \"file\": <path>,\n              \"dependencies\": [ // coodinates for its transitive dependencies\n                <orgX:nameX:versionX>,\n                <orgZ:nameZ:versionZ>,\n              ]\n            },\n            ... // more about orgX:nameX:versionX, orgY:nameY:versionY, orgZ:nameZ:versionZ\n          ]\n        }\n    Hence the aggregation of the results will be in the following format, for example when default classifier\n    and sources are fetched:\n    {\n      'default': [<result from coursier call with default conf with classifier X>,\n                  <result from coursier call with default conf with classifier Y>],\n      'src_doc': [<result from coursier call with --sources and/or --javadoc>],\n    }\n    \"\"\"\n    # Prepare coursier args\n    coursier_subsystem_instance = CoursierSubsystem.global_instance()\n    coursier_jar = coursier_subsystem_instance.bootstrap_coursier(self.context.new_workunit)\n\n    repos = coursier_subsystem_instance.get_options().repos\n    # make [repoX, repoY] -> ['-r', repoX, '-r', repoY]\n    repo_args = list(itertools.chain(*list(zip(['-r'] * len(repos), repos))))\n    artifact_types_arg = ['-A', ','.join(coursier_subsystem_instance.get_options().artifact_types)]\n    advanced_options = coursier_subsystem_instance.get_options().fetch_options\n    common_args = ['fetch',\n                   # Print the resolution tree\n                   '-t',\n                   '--cache', coursier_cache_path\n                   ] + repo_args + artifact_types_arg + advanced_options\n\n    coursier_work_temp_dir = os.path.join(self.versioned_workdir, 'tmp')\n    safe_mkdir(coursier_work_temp_dir)\n\n    results_by_conf = self._get_default_conf_results(common_args, coursier_jar, global_excludes, jars_to_resolve,\n                                                     coursier_work_temp_dir,\n                                                     pinned_coords, executor)\n    if sources or javadoc:\n      non_default_conf_results = self._get_non_default_conf_results(common_args, coursier_jar, global_excludes,\n                                                                    jars_to_resolve, coursier_work_temp_dir,\n                                                                    pinned_coords, sources, javadoc, executor)\n      results_by_conf.update(non_default_conf_results)\n\n    return results_by_conf"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _load_json_result(self, conf, compile_classpath, coursier_cache_path, invalidation_check,\n                        pants_jar_path_base, result, override_classifiers=None):\n    \"\"\"\n    Given a coursier run result, load it into compile_classpath by target.\n\n    :param compile_classpath: `ClasspathProducts` that will be modified\n    :param coursier_cache_path: cache location that is managed by coursier\n    :param invalidation_check: InvalidationCheck\n    :param pants_jar_path_base: location under pants workdir that contains all the hardlinks to coursier cache\n    :param result: result dict converted from the json produced by one coursier run\n    :return: n/a\n    \"\"\"\n    # Parse the coursier result\n    flattened_resolution = self._extract_dependencies_by_root(result)\n\n    coord_to_resolved_jars = self._map_coord_to_resolved_jars(result, coursier_cache_path, pants_jar_path_base)\n\n    # Construct a map from org:name to the reconciled org:name:version coordinate\n    # This is used when there is won't be a conflict_resolution entry because the conflict\n    # was resolved in pants.\n    org_name_to_org_name_rev = {}\n    for coord in coord_to_resolved_jars.keys():\n      org_name_to_org_name_rev['{}:{}'.format(coord.org, coord.name)] = coord\n\n    jars_per_target = []\n\n    for vt in invalidation_check.all_vts:\n      t = vt.target\n      jars_to_digest = []\n      if isinstance(t, JarLibrary):\n        def get_transitive_resolved_jars(my_coord, resolved_jars):\n          transitive_jar_path_for_coord = []\n          coord_str = str(my_coord)\n          if coord_str in flattened_resolution and my_coord in resolved_jars:\n            transitive_jar_path_for_coord.append(resolved_jars[my_coord])\n\n            for c in flattened_resolution[coord_str]:\n              j = resolved_jars.get(self.to_m2_coord(c))\n              if j:\n                transitive_jar_path_for_coord.append(j)\n\n          return transitive_jar_path_for_coord\n\n        for jar in t.jar_dependencies:\n          # if there are override classifiers, then force use of those.\n          coord_candidates = []\n          if override_classifiers:\n            coord_candidates = [jar.coordinate.copy(classifier=c) for c in override_classifiers]\n          else:\n            coord_candidates = [jar.coordinate]\n\n          # if conflict resolution entries, then update versions to the resolved ones.\n          if jar.coordinate.simple_coord in result['conflict_resolution']:\n            parsed_conflict = self.to_m2_coord(\n              result['conflict_resolution'][jar.coordinate.simple_coord])\n            coord_candidates = [c.copy(rev=parsed_conflict.rev) for c in coord_candidates]\n          elif '{}:{}'.format(jar.coordinate.org, jar.coordinate.name) in org_name_to_org_name_rev:\n            parsed_conflict = org_name_to_org_name_rev['{}:{}'.format(jar.coordinate.org, jar.coordinate.name)]\n            coord_candidates = [c.copy(rev=parsed_conflict.rev) for c in coord_candidates]\n\n          for coord in coord_candidates:\n            transitive_resolved_jars = get_transitive_resolved_jars(coord, coord_to_resolved_jars)\n            if transitive_resolved_jars:\n              for jar in transitive_resolved_jars:\n                jars_to_digest.append(jar)\n\n        jars_per_target.append((t, jars_to_digest))\n\n    for target, jars_to_add in self.add_directory_digests_for_jars(jars_per_target):\n      compile_classpath.add_jars_for_targets([target], conf, jars_to_add)", "response": "Given a coursier run result load it into compile_classpath by target."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload the results from the results directory into the compile_classpath.", "response": "def _load_from_results_dir(self, compile_classpath, vts_results_dir,\n                             coursier_cache_path, invalidation_check, pants_jar_path_base):\n    \"\"\"\n    Given vts_results_dir, load the results which can be from multiple runs of coursier into compile_classpath.\n\n    :return: True if success; False if any of the classpath is not valid anymore.\n    \"\"\"\n    result_file_path = os.path.join(vts_results_dir, self.RESULT_FILENAME)\n    if not os.path.exists(result_file_path):\n      return\n\n    with open(result_file_path, 'r') as f:\n      results = json.load(f)\n      for conf, result_list in results.items():\n        for result in result_list:\n          try:\n            self._load_json_result(conf,\n                                   compile_classpath,\n                                   coursier_cache_path,\n                                   invalidation_check,\n                                   pants_jar_path_base,\n                                   result,\n                                   self._override_classifiers_for_conf(conf))\n          except CoursierResultNotFound:\n            return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmaps maven coordinate to resolved jars.", "response": "def _map_coord_to_resolved_jars(cls, result, coursier_cache_path, pants_jar_path_base):\n    \"\"\"\n    Map resolved files to each org:name:version\n\n    Example:\n    {\n      \"conflict_resolution\": {},\n      \"dependencies\": [\n        {\n          \"coord\": \"a\",\n          \"dependencies\": [\"b\", \"c\"],\n          \"file\": \"a.jar\"\n        },\n        {\n          \"coord\": \"b\",\n          \"dependencies\": [],\n          \"file\": \"b.jar\"\n        },\n        {\n          \"coord\": \"c\",\n          \"dependencies\": [],\n          \"file\": \"c.jar\"\n        },\n        {\n          \"coord\": \"a:sources\",\n          \"dependencies\": [\"b\", \"c\"],\n          \"file\": \"a-sources.jar\"\n        },\n      ]\n    }\n\n    Should return:\n    {\n      M2Coordinate(\"a\", ...):                             ResolvedJar(classifier='', path/cache_path=\"a.jar\"),\n      M2Coordinate(\"a\", ..., classifier=\"sources\"):       ResolvedJar(classifier='sources', path/cache_path=\"a-sources.jar\"),\n      M2Coordinate(\"b\", ...):                             ResolvedJar(classifier='', path/cache_path=\"b.jar\"),\n      M2Coordinate(\"c\", ...):                             ResolvedJar(classifier='', path/cache_path=\"c.jar\"),\n    }\n\n    :param result: coursier json output\n    :param coursier_cache_path: coursier cache location\n    :param pants_jar_path_base: location under pants workdir to store the hardlink to the coursier cache\n    :return: a map from maven coordinate to a resolved jar.\n    \"\"\"\n\n    coord_to_resolved_jars = dict()\n\n    for dep in result['dependencies']:\n      coord = dep['coord']\n      jar_path = dep.get('file', None)\n      if not jar_path:\n        # NB: Not all coordinates will have associated files.\n        #     This is fine. Some coordinates will just have dependencies.\n        continue\n\n      if not os.path.exists(jar_path):\n        raise CoursierResultNotFound(\"Jar path not found: {}\".format(jar_path))\n\n      pants_path = cls._get_path_to_jar(coursier_cache_path, pants_jar_path_base, jar_path)\n\n      if not os.path.exists(pants_path):\n        safe_mkdir(os.path.dirname(pants_path))\n        safe_hardlink_or_copy(jar_path, pants_path)\n\n      coord = cls.to_m2_coord(coord)\n      resolved_jar = ResolvedJar(coord,\n                                 cache_path=jar_path,\n                                 pants_path=pants_path)\n      coord_to_resolved_jars[coord] = resolved_jar\n    return coord_to_resolved_jars"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_path_to_jar(cls, coursier_cache_path, pants_jar_path_base, jar_path):\n    if os.path.abspath(coursier_cache_path) not in os.path.abspath(jar_path):\n      # Appending the string 'absolute' to the jar_path and joining that is a hack to work around\n      # python's os.path.join behavior of throwing away all components that come before an\n      # absolute path. See https://docs.python.org/3.3/library/os.path.html#os.path.join\n      return os.path.join(pants_jar_path_base, os.path.normpath('absolute/' + jar_path))\n    else:\n      return os.path.join(pants_jar_path_base, 'relative', os.path.relpath(jar_path, coursier_cache_path))", "response": "Create the path to the jar that will live in the coursier cache."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef execute(self):\n\n    jvm_resolve_subsystem = JvmResolveSubsystem.global_instance()\n    if jvm_resolve_subsystem.get_options().resolver != 'coursier':\n      return\n\n    # executor = self.create_java_executor()\n    classpath_products = self.context.products.get_data('compile_classpath',\n                                                        init_func=ClasspathProducts.init_func(\n                                                          self.get_options().pants_workdir))\n    executor = self.create_java_executor()\n    self.resolve(self.context.targets(), classpath_products, sources=False, javadoc=False, executor=executor)", "response": "Resolves the specified confs for the configured targets and returns an iterator over tuples of conf jar path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _determine_v1_goals(self, address_mapper, options):\n    v1_goals, ambiguous_goals, _ = options.goals_by_version\n    requested_goals = v1_goals + ambiguous_goals\n\n    spec_parser = CmdLineSpecParser(self._root_dir)\n\n    for goal in requested_goals:\n      if address_mapper.is_valid_single_address(spec_parser.parse_spec(goal)):\n        logger.warning(\"Command-line argument '{0}' is ambiguous and was assumed to be \"\n                       \"a goal. If this is incorrect, disambiguate it with ./{0}.\".format(goal))\n\n    return [Goal.by_name(goal) for goal in requested_goals]", "response": "Check and populate the requested goals for a given run."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npopulate the BuildGraph and target list from a set of input TargetRoots.", "response": "def _roots_to_targets(self, build_graph, target_roots):\n    \"\"\"Populate the BuildGraph and target list from a set of input TargetRoots.\"\"\"\n    with self._run_tracker.new_workunit(name='parse', labels=[WorkUnitLabel.SETUP]):\n      return [\n        build_graph.get_target(address)\n        for address\n        in build_graph.inject_roots_closure(target_roots, self._fail_fast)\n      ]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlog a message. Each element of msg_elements is either a message string or a (message, detail) pair.", "response": "def log(self, workunit, level, *msg_elements):\n    \"\"\"Log a message.\n\n    Each element of msg_elements is either a message string or a (message, detail) pair.\n    \"\"\"\n    with self._lock:\n      for reporter in self._reporters.values():\n        reporter.handle_log(workunit, level, *msg_elements)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nclones the existing runtime_classpath and corresponding binaries to instrumentation specific paths.", "response": "def initialize_instrument_classpath(output_dir, settings, targets, instrumentation_classpath):\n    \"\"\"Clones the existing runtime_classpath and corresponding binaries to instrumentation specific\n    paths.\n\n    :param targets: the targets for which we should create an instrumentation_classpath entry based\n    on their runtime_classpath entry.\n    \"\"\"\n    instrument_dir = os.path.join(output_dir, 'coverage', 'classes')\n    settings.safe_makedir(instrument_dir, clean=True)\n\n    for target in targets:\n      if not Cobertura.is_coverage_target(target):\n        continue\n      # Do not instrument transitive dependencies.\n      paths = instrumentation_classpath.get_for_target(target)\n      target_instrumentation_path = os.path.join(instrument_dir, target.id)\n      for (index, (config, path)) in enumerate(paths):\n        # There are two sorts of classpath entries we see in the compile classpath: jars and dirs.\n        # The branches below handle the cloning of those respectively.\n        entry_instrumentation_path = os.path.join(target_instrumentation_path, str(index))\n        if settings.is_file(path):\n          settings.safe_makedir(entry_instrumentation_path, clean=True)\n          settings.copy2(path, entry_instrumentation_path)\n          new_path = os.path.join(entry_instrumentation_path, os.path.basename(path))\n        else:\n          settings.copytree(path, entry_instrumentation_path)\n          new_path = entry_instrumentation_path\n\n        instrumentation_classpath.remove_for_target(target, [(config, path)])\n        instrumentation_classpath.add_for_target(target, [(config, new_path)])\n        settings.log.debug(\n          \"runtime_classpath ({}) cloned to instrument_classpath ({})\".format(path, new_path))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_request(cls, sock, working_dir, command, *arguments, **environment):\n    for argument in arguments:\n      cls.write_chunk(sock, ChunkType.ARGUMENT, argument)\n\n    for item_tuple in environment.items():\n      cls.write_chunk(sock,\n                      ChunkType.ENVIRONMENT,\n                      cls.ENVIRON_SEP.join(cls._decode_unicode_seq(item_tuple)))\n\n    cls.write_chunk(sock, ChunkType.WORKING_DIR, working_dir)\n    cls.write_chunk(sock, ChunkType.COMMAND, command)", "response": "Send a request over the specified socket."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_request(cls, sock):\n\n    command = None\n    working_dir = None\n    arguments = []\n    environment = {}\n\n    while not all((working_dir, command)):\n      chunk_type, payload = cls.read_chunk(sock)\n\n      if chunk_type == ChunkType.ARGUMENT:\n        arguments.append(payload)\n      elif chunk_type == ChunkType.ENVIRONMENT:\n        key, val = payload.split(cls.ENVIRON_SEP, 1)\n        environment[key] = val\n      elif chunk_type == ChunkType.WORKING_DIR:\n        working_dir = payload\n      elif chunk_type == ChunkType.COMMAND:\n        command = payload\n      else:\n        raise cls.ProtocolError('received non-request chunk before header was fully received!')\n\n    return working_dir, command, arguments, environment", "response": "Parse the request section of the nailgun protocol from the given socket."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting a single chunk to the connected client.", "response": "def write_chunk(cls, sock, chunk_type, payload=b''):\n    \"\"\"Write a single chunk to the connected client.\"\"\"\n    chunk = cls.construct_chunk(chunk_type, payload)\n    sock.sendall(chunk)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef construct_chunk(cls, chunk_type, payload, encoding='utf-8'):\n    if isinstance(payload, str):\n      payload = payload.encode(encoding)\n    elif not isinstance(payload, bytes):\n      raise TypeError('cannot encode type: {}'.format(type(payload)))\n\n    header = struct.pack(cls.HEADER_FMT, len(payload), chunk_type)\n    return header + payload", "response": "Construct and return a single chunk."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read_until(cls, sock, desired_size):\n    buf = b''\n    while len(buf) < desired_size:\n      recv_bytes = sock.recv(desired_size - len(buf))\n      if not recv_bytes:\n        raise cls.TruncatedRead('Expected {} bytes before socket shutdown, instead received {}'\n                                .format(desired_size, len(buf)))\n      buf += recv_bytes\n    return buf", "response": "Read a certain amount of content from a socket before returning."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_chunk(cls, sock, return_bytes=False):\n    try:\n      # Read the chunk header from the socket.\n      header = cls._read_until(sock, cls.HEADER_BYTES)\n    except cls.TruncatedRead as e:\n      raise cls.TruncatedHeaderError('Failed to read nailgun chunk header ({!r}).'.format(e))\n\n    # Unpack the chunk header.\n    payload_len, chunk_type = struct.unpack(cls.HEADER_FMT, header)\n\n    try:\n      # Read the chunk payload.\n      payload = cls._read_until(sock, payload_len)\n    except cls.TruncatedRead as e:\n      raise cls.TruncatedPayloadError('Failed to read nailgun chunk payload ({!r}).'.format(e))\n\n    # In the case we get an otherwise well-formed chunk, check the chunk_type for validity _after_\n    # we've drained the payload from the socket to avoid subsequent reads of a stale payload.\n    if chunk_type not in ChunkType.VALID_TYPES:\n      raise cls.ProtocolError('invalid chunk type: {}'.format(chunk_type))\n    if not return_bytes:\n      payload = payload.decode('utf-8')\n\n    return chunk_type, payload", "response": "Read a single chunk from the connected client."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef iter_chunks(cls, sock, return_bytes=False, timeout_object=None):\n    assert(timeout_object is None or isinstance(timeout_object, cls.TimeoutProvider))\n    orig_timeout_time = None\n    timeout_interval = None\n    while 1:\n      if orig_timeout_time is not None:\n        remaining_time = time.time() - (orig_timeout_time + timeout_interval)\n        if remaining_time > 0:\n          original_timestamp = datetime.datetime.fromtimestamp(orig_timeout_time).isoformat()\n          raise cls.ProcessStreamTimeout(\n            \"iterating over bytes from nailgun timed out with timeout interval {} starting at {}, \"\n            \"overtime seconds: {}\"\n            .format(timeout_interval, original_timestamp, remaining_time))\n      elif timeout_object is not None:\n        opts = timeout_object.maybe_timeout_options()\n        if opts:\n          orig_timeout_time = opts.start_time\n          timeout_interval = opts.interval\n          continue\n        remaining_time = None\n      else:\n        remaining_time = None\n\n      with cls._set_socket_timeout(sock, timeout=remaining_time):\n        chunk_type, payload = cls.read_chunk(sock, return_bytes)\n        yield chunk_type, payload\n        if chunk_type == ChunkType.EXIT:\n          break", "response": "Yields a sequence of chunks from a connected socket."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend the Stdout chunk over the specified socket.", "response": "def send_stdout(cls, sock, payload):\n    \"\"\"Send the Stdout chunk over the specified socket.\"\"\"\n    cls.write_chunk(sock, ChunkType.STDOUT, payload)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_stderr(cls, sock, payload):\n    cls.write_chunk(sock, ChunkType.STDERR, payload)", "response": "Send the stderr chunk over the specified socket."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending the Exit chunk over the specified socket.", "response": "def send_exit(cls, sock, payload=b''):\n    \"\"\"Send the Exit chunk over the specified socket.\"\"\"\n    cls.write_chunk(sock, ChunkType.EXIT, payload)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending an Exit chunk over the specified socket containing the specified return code.", "response": "def send_exit_with_code(cls, sock, code):\n    \"\"\"Send an Exit chunk over the specified socket, containing the specified return code.\"\"\"\n    encoded_exit_status = cls.encode_int(code)\n    cls.send_exit(sock, payload=encoded_exit_status)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send_pid(cls, sock, pid):\n    assert(isinstance(pid, IntegerForPid) and pid > 0)\n    encoded_int = cls.encode_int(pid)\n    cls.write_chunk(sock, ChunkType.PID, encoded_int)", "response": "Send the PID chunk over the specified socket."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_pgrp(cls, sock, pgrp):\n    assert(isinstance(pgrp, IntegerForPid) and pgrp < 0)\n    encoded_int = cls.encode_int(pgrp)\n    cls.write_chunk(sock, ChunkType.PGRP, encoded_int)", "response": "Send the PGRP chunk over the specified socket."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef encode_int(cls, obj):\n    if not isinstance(obj, int):\n      raise TypeError(\"cannot encode non-integer object in encode_int(): object was {} (type '{}').\"\n                      .format(obj, type(obj)))\n    return str(obj).encode('ascii')", "response": "Verify the object is an int and ASCII - encode it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef isatty_to_env(cls, stdin, stdout, stderr):\n    def gen_env_vars():\n      for fd_id, fd in zip(STDIO_DESCRIPTORS, (stdin, stdout, stderr)):\n        is_atty = fd.isatty()\n        yield (cls.TTY_ENV_TMPL.format(fd_id), cls.encode_env_var_value(int(is_atty)))\n        if is_atty:\n          yield (cls.TTY_PATH_ENV.format(fd_id), os.ttyname(fd.fileno()) or b'')\n    return dict(gen_env_vars())", "response": "Generate a dict containing the nailgun tty capability environment variables based on checking a set of fds."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef isatty_from_env(cls, env):\n    def str_int_bool(i):\n      return i.isdigit() and bool(int(i))  # Environment variable values should always be strings.\n\n    return tuple(\n      str_int_bool(env.get(cls.TTY_ENV_TMPL.format(fd_id), '0')) for fd_id in STDIO_DESCRIPTORS\n    )", "response": "Determine whether remote file descriptors are tty capable using std nailgunned env variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ttynames_from_env(cls, env):\n    return tuple(env.get(cls.TTY_PATH_ENV.format(fd_id)) for fd_id in STDIO_DESCRIPTORS)", "response": "Determines the ttynames for remote file descriptors in the environment."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef execute(self, context, goals):\n    try:\n      self.attempt(context, goals)\n      return 0\n    except TaskError as e:\n      message = str(e)\n      if message:\n        print('\\nFAILURE: {0}\\n'.format(message))\n      else:\n        print('\\nFAILURE\\n')\n      return e.exit_code", "response": "Executes the supplied goals and their dependencies against the given context."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a copy of this JarDependency with the given replacements kwargs overlaid.", "response": "def copy(self, **replacements):\n    \"\"\"Returns a clone of this JarDependency with the given replacements kwargs overlaid.\"\"\"\n    cls = type(self)\n    kwargs = self._asdict()\n    for key, val in replacements.items():\n      if key == 'excludes':\n        val = JarDependency._prepare_excludes(val)\n      kwargs[key] = val\n    org = kwargs.pop('org')\n    base_name = kwargs.pop('base_name')\n    return cls(org, base_name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the maven coordinate of this jar.", "response": "def coordinate(self):\n    \"\"\"Returns the maven coordinate of this jar.\n\n    :rtype: :class:`pants.java.jar.M2Coordinate`\n    \"\"\"\n    return M2Coordinate(org=self.org, name=self.name, rev=self.rev, classifier=self.classifier,\n                        ext=self.ext)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_products_if_should_run(self):\n\n    should_run = False\n    if self.context.products.is_required_data('classes_by_source'):\n      should_run = True\n      make_products = lambda: defaultdict(MultipleRootedProducts)\n      self.context.products.safe_create_data('classes_by_source', make_products)\n\n    if self.context.products.is_required_data('product_deps_by_src'):\n      should_run = True\n      self.context.products.safe_create_data('product_deps_by_src', dict)\n    return should_run", "response": "Returns true if the task should run."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef expand(self, name=None, target=None, **kwargs):\n    if name is None:\n      raise TargetDefinitionException('{}:?'.format(self._parse_context.rel_path, name),\n                                      'The alias() must have a name!')\n    if target is None:\n      raise TargetDefinitionException('{}:{}'.format(self._parse_context.rel_path, name),\n                                      'The alias() must have a \"target\" parameter.')\n    self._parse_context.create_object(\n      AliasTarget,\n      type_alias='alias',\n      name=name,\n      dependencies=[target] if target else [],\n      **kwargs\n    )", "response": ":param string name: The name for this alias.\n    :param string target: The address of the destination target."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms a sanity check on the SCM publishing constraints.", "response": "def check_clean_master(self, commit=False):\n    \"\"\"Perform a sanity check on SCM publishing constraints.\n\n    Checks for uncommitted tracked files and ensures we're on an allowed branch configured to push\n    to an allowed server if `commit` is `True`.\n\n    :param bool commit: `True` if a commit is in progress.\n    :raise TaskError: on failure\n    \"\"\"\n    if commit:\n      if self.restrict_push_branches:\n        branch = self.scm.branch_name\n        if branch not in self.restrict_push_branches:\n          raise self.InvalidBranchError('Can only push from {}, currently on branch: {}'\n                                        .format(' '.join(sorted(self.restrict_push_branches)),\n                                                branch))\n\n      if self.restrict_push_urls:\n        url = self.scm.server_url\n        if url not in self.restrict_push_urls:\n          raise self.InvalidRemoteError('Can only push to {}, currently the remote url is: {}'\n                                        .format(' '.join(sorted(self.restrict_push_urls)), url))\n\n      changed_files = self.scm.changed_files()\n      if changed_files:\n        raise self.DirtyWorkspaceError('Can only push from a clean branch, found : {}'\n                                       .format(' '.join(changed_files)))\n    elif self.scm:\n      self.log.info('Skipping check for a clean {} branch in test mode.'\n                    .format(self.scm.branch_name))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef commit_pushdb(self, coordinates, postscript=None):\n    self.scm.commit('pants build committing publish data for push of {coordinates}'\n                    '{postscript}'.format(coordinates=coordinates, postscript=postscript or ''),\n                    verify=self.get_options().verify_commit)", "response": "Commit changes to the pushdb with a message containing the provided coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef publish_pushdb_changes_to_remote_scm(self, pushdb_file, coordinate, tag_name, tag_message,\n                                           postscript=None):\n    \"\"\"Push pushdb changes to the remote scm repository, and then tag the commit if it succeeds.\"\"\"\n\n    self._add_pushdb(pushdb_file)\n    self.commit_pushdb(coordinate, postscript=postscript)\n    self._push_and_tag_changes(\n      tag_name=tag_name,\n      tag_message='{message}{postscript}'.format(message=tag_message, postscript=postscript or '')\n    )", "response": "Push pushdb changes to the remote scm repository and then tag the commit if it succeeds."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a normalized package path from the given path and its root.", "response": "def package_path(cls, root, path):\n    \"\"\"Returns a normalized package path constructed from the given path and its root.\n\n    A remote package path is the portion of the remote Go package's import path after the remote\n    root path.\n\n    For example, the remote import path 'https://github.com/bitly/go-simplejson' has\n    a remote root of 'https://github.com/bitly/go-simplejson' and there is only 1 package\n    in that remote root.  The package path in this case is '' or '.' and is normalized\n    to ''.\n\n    Some remote roots have no root package and others have both a root and sub-packages.  The\n    remote root of 'github.com/docker/docker' is an example of the former.  One of the packages\n    you might import from it is 'github.com/docker/docker/daemon/events' and that package has a\n    normalized remote package path of 'daemon/events'.\n\n    :param string root: The portion of `path` pointing to the go source code root.  This would be\n                        `src` under a standard $GOPATH root.\n    :param string path: The full path to the package.  This would be the full path to the package\n                        directory relative to the $GOPATH.\n    :raises: `ValueError` if the path does not lay within the root.\n    \"\"\"\n    package_path = os.path.relpath(path, root)\n    return cls.normalize_package_path(package_path)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef normalize_package_path(cls, package_path):\n    if package_path.startswith(os.pardir + os.sep):\n      raise ValueError('Relative package paths are not allowed. Given: {!r}'.format(package_path))\n    if os.path.isabs(package_path):\n      raise ValueError('Absolute package paths are not allowed. Given: {!r}'.format(package_path))\n    return '' if not package_path or package_path == os.curdir else package_path.lstrip('/')", "response": "Normalizes the given package path."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef join(self, timeout=None):\n    return super(_StoppableDaemonThread, self).join(timeout or self.JOIN_TIMEOUT)", "response": "Joins with a default timeout exposed on the class."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef open(cls, sock, chunk_type, isatty, chunk_eof_type=None, buf_size=None, select_timeout=None):\n    with cls.open_multi(sock,\n                        (chunk_type,),\n                        (isatty,),\n                        chunk_eof_type,\n                        buf_size,\n                        select_timeout) as ctx:\n      yield ctx", "response": "Yields the write side of a pipe that will copy appropriately chunked values to a socket."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef open_multi(cls, sock, chunk_types, isattys, chunk_eof_type=None, buf_size=None,\n                 select_timeout=None):\n    \"\"\"Yields the write sides of pipes that will copy appropriately chunked values to the socket.\"\"\"\n    cls._assert_aligned(chunk_types, isattys)\n\n    # N.B. This is purely to permit safe handling of a dynamic number of contextmanagers.\n    with ExitStack() as stack:\n      read_fds, write_fds = list(zip(\n        # Allocate one pipe pair per chunk type provided.\n        *(stack.enter_context(_pipe(isatty)) for isatty in isattys)\n      ))\n      writer = NailgunStreamWriter(\n        read_fds,\n        sock,\n        chunk_types,\n        chunk_eof_type,\n        buf_size=buf_size,\n        select_timeout=select_timeout\n      )\n      with writer.running():\n        yield write_fds, writer", "response": "Yields the read sides of pipes that will copy appropriately chunked values to the socket."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef combine_cache_keys(cls, cache_keys):\n    if len(cache_keys) == 1:\n      return cache_keys[0]\n    else:\n      combined_id = Target.maybe_readable_combine_ids(cache_key.id for cache_key in cache_keys)\n      combined_hash = hash_all(sorted(cache_key.hash for cache_key in cache_keys))\n      return cls(combined_id, combined_hash)", "response": "Returns a new cache key for a list of target sets that already have cache keys."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the previous key for the given key.", "response": "def previous_key(self, cache_key):\n    \"\"\"If there was a previous successful build for the given key, return the previous key.\n\n    :param cache_key: A CacheKey object (as returned by CacheKeyGenerator.key_for().\n    :returns: The previous cache_key, or None if there was not a previous build.\n    \"\"\"\n    if not self.cacheable(cache_key):\n      # We should never successfully cache an uncacheable CacheKey.\n      return None\n\n    previous_hash = self._read_sha(cache_key)\n    if not previous_hash:\n      return None\n    return CacheKey(cache_key.id, previous_hash)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the given cached item is invalid.", "response": "def needs_update(self, cache_key):\n    \"\"\"Check if the given cached item is invalid.\n\n    :param cache_key: A CacheKey object (as returned by CacheKeyGenerator.key_for().\n    :returns: True if the cached version of the item is out of date.\n    \"\"\"\n    if not self.cacheable(cache_key):\n      # An uncacheable CacheKey is always out of date.\n      return True\n\n    return self._read_sha(cache_key) != cache_key.hash"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef force_invalidate(self, cache_key):\n    try:\n      if self.cacheable(cache_key):\n        os.unlink(self._sha_file(cache_key))\n    except OSError as e:\n      if e.errno != errno.ENOENT:\n        raise", "response": "Force - invalidate the cached item."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_command(self, args=None, node_paths=None):\n    return command_gen(\n      self.tool_installations,\n      self.name,\n      args=args,\n      node_paths=node_paths\n    )", "response": "Returns a command that when executed will run an arbitury command via package manager."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a command that when executed will install node package.", "response": "def install_module(\n    self,\n    install_optional=False,\n    production_only=False,\n    force=False,\n    frozen_lockfile=True,\n    node_paths=None):\n    \"\"\"Returns a command that when executed will install node package.\n\n    :param install_optional: True to install optional dependencies.\n    :param production_only: True to only install production dependencies, i.e.\n      ignore devDependencies.\n    :param force: True to force re-download dependencies.\n    :param frozen_lockfile: True to disallow automatic update of lock files.\n    :param node_paths: A list of path that should be included in $PATH when\n      running installation.\n    \"\"\"\n    args=self._get_installation_args(\n      install_optional=install_optional,\n      production_only=production_only,\n      force=force,\n      frozen_lockfile=frozen_lockfile)\n    return self.run_command(args=args, node_paths=node_paths)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_script(self, script_name, script_args=None, node_paths=None):\n    # TODO: consider add a pants.util function to manipulate command line.\n    package_manager_args = self._get_run_script_args()\n    package_manager_args.append(script_name)\n    if script_args:\n      package_manager_args.append('--')\n      package_manager_args.extend(script_args)\n    return self.run_command(args=package_manager_args, node_paths=node_paths)", "response": "Returns a command to execute a package. json script."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_package(\n    self,\n    package,\n    node_paths=None, \n    type_option=PackageInstallationTypeOption.PROD,\n    version_option=None):\n    \"\"\"Returns a command that when executed will add a node package to current node module.\n\n    :param package: string.  A valid npm/yarn package description.  The accepted forms are\n      package-name, package-name@version, package-name@tag, file:/folder, file:/path/to.tgz\n      https://url/to.tgz\n    :param node_paths: A list of path that should be included in $PATH when\n      running the script.\n    :param type_option: A value from PackageInstallationTypeOption that indicates the type\n      of package to be installed. Default to 'prod', which is a production dependency.\n    :param version_option: A value from PackageInstallationVersionOption that indicates how\n      to match version. Default to None, which uses package manager default.\n    \"\"\"\n    args=self._get_add_package_args(\n      package,\n      type_option=type_option,\n      version_option=version_option)\n    return self.run_command(args=args, node_paths=node_paths)", "response": "Returns a command that will add a node package to current node module."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_cli(self, cli, args=None, node_paths=None):\n    cli_args = [cli]\n    if args:\n      cli_args.append('--')\n      cli_args.extend(args)\n    return self.run_command(args=cli_args, node_paths=node_paths)", "response": "Returns a command that when executed will run an installed cli via package manager."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef scan_build_files(project_tree, base_relpath, build_ignore_patterns=None):\n    if base_relpath and os.path.isabs(base_relpath):\n      raise BuildFile.BadPathError('base_relpath parameter ({}) should be a relative path.'\n                                   .format(base_relpath))\n    if base_relpath and not project_tree.isdir(base_relpath):\n      raise BuildFile.BadPathError('Can only scan directories and {0} is not a valid dir.'\n                                   .format(base_relpath))\n    if build_ignore_patterns and not isinstance(build_ignore_patterns, PathSpec):\n      raise TypeError(\"build_ignore_patterns should be pathspec.pathspec.PathSpec instance, \"\n                      \"instead {} was given.\".format(type(build_ignore_patterns)))\n\n    build_files = set()\n    for root, dirs, files in project_tree.walk(base_relpath or '', topdown=True):\n      excluded_dirs = list(build_ignore_patterns.match_files('{}/'.format(os.path.join(root, dirname))\n                                                          for dirname in dirs))\n      for subdir in excluded_dirs:\n        # Remove trailing '/' from paths which were added to indicate that paths are paths to directories.\n        dirs.remove(fast_relpath(subdir, root)[:-1])\n      for filename in files:\n        if BuildFile._is_buildfile_name(filename):\n          build_files.add(os.path.join(root, filename))\n\n    return BuildFile._build_files_from_paths(project_tree, build_files, build_ignore_patterns)", "response": "Scan a project tree for BUILD files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn all the BUILD files on a path", "response": "def get_build_files_family(project_tree, dir_relpath, build_ignore_patterns=None):\n    \"\"\"Returns all the BUILD files on a path\"\"\"\n    build_files = set()\n    for build in sorted(project_tree.glob1(dir_relpath, '{prefix}*'.format(prefix=BuildFile._BUILD_FILE_PREFIX))):\n      if BuildFile._is_buildfile_name(build) and project_tree.isfile(os.path.join(dir_relpath, build)):\n        build_files.add(os.path.join(dir_relpath, build))\n    return BuildFile._build_files_from_paths(project_tree, build_files, build_ignore_patterns)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the code object for this BUILD file.", "response": "def code(self):\n    \"\"\"Returns the code object for this BUILD file.\"\"\"\n    return compile(self.source(), self.full_path, 'exec', flags=0, dont_inherit=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef walk(self, callback):\n    callback(self)\n    for child in self._child_parsers:\n      child.walk(callback)", "response": "Invoke callback on this parser and its descendants in depth - first order."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_flag_value_map(self, flags):\n    flag_value_map = defaultdict(list)\n    for flag in flags:\n      key, has_equals_sign, flag_val = flag.partition('=')\n      if not has_equals_sign:\n        if not flag.startswith('--'):  # '-xfoo' style.\n          key = flag[0:2]\n          flag_val = flag[2:]\n        if not flag_val:\n          # Either a short option with no value or a long option with no equals sign.\n          # Important so we can distinguish between no value ('--foo') and setting to an empty\n          # string ('--foo='), for options with an implicit_value.\n          flag_val = None\n      flag_value_map[key].append(flag_val)\n    return flag_value_map", "response": "Create a map of flag - > value based on the given flags."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the command line arguments and return a list of values.", "response": "def parse_args(self, flags, namespace):\n    \"\"\"Set values for this parser's options on the namespace object.\"\"\"\n    flag_value_map = self._create_flag_value_map(flags)\n\n    mutex_map = defaultdict(list)\n    for args, kwargs in self._unnormalized_option_registrations_iter():\n      self._validate(args, kwargs)\n      dest = self.parse_dest(*args, **kwargs)\n\n      # Compute the values provided on the command line for this option.  Note that there may be\n      # multiple values, for any combination of the following reasons:\n      #   - The user used the same flag multiple times.\n      #   - The user specified a boolean flag (--foo) and its inverse (--no-foo).\n      #   - The option has multiple names, and the user used more than one of them.\n      #\n      # We also check if the option is deprecated, but we only do so if the option is explicitly\n      # specified as a command-line flag, so we don't spam users with deprecated option values\n      # specified in config, which isn't something they control.\n      implicit_value = kwargs.get('implicit_value')\n      if implicit_value is None and kwargs.get('type') == bool:\n        implicit_value = True  # Allows --foo to mean --foo=true.\n\n      flag_vals = []\n\n      def add_flag_val(v):\n        if v is None:\n          if implicit_value is None:\n            raise ParseError('Missing value for command line flag {} in {}'.format(\n              arg, self._scope_str()))\n          else:\n            flag_vals.append(implicit_value)\n        else:\n          flag_vals.append(v)\n\n      for arg in args:\n        # If the user specified --no-foo on the cmd line, treat it as if the user specified\n        # --foo, but with the inverse value.\n        if kwargs.get('type') == bool:\n          inverse_arg = self._inverse_arg(arg)\n          if inverse_arg in flag_value_map:\n            flag_value_map[arg] = [self._invert(v) for v in flag_value_map[inverse_arg]]\n            implicit_value = self._invert(implicit_value)\n            del flag_value_map[inverse_arg]\n\n        if arg in flag_value_map:\n          for v in flag_value_map[arg]:\n            add_flag_val(v)\n          del flag_value_map[arg]\n\n      # Get the value for this option, falling back to defaults as needed.\n      try:\n        val = self._compute_value(dest, kwargs, flag_vals)\n      except ParseError as e:\n        # Reraise a new exception with context on the option being processed at the time of error.\n        # Note that other exception types can be raised here that are caught by ParseError (e.g.\n        # BooleanConversionError), hence we reference the original exception type as type(e).\n        raise type(e)(\n          'Error computing value for {} in {} (may also be from PANTS_* environment variables).'\n          '\\nCaused by:\\n{}'.format(', '.join(args), self._scope_str(), traceback.format_exc())\n        )\n\n      # If the option is explicitly given, check deprecation and mutual exclusion.\n      if val.rank > RankedValue.HARDCODED:\n        self._check_deprecated(dest, kwargs)\n\n        mutex_dest = kwargs.get('mutually_exclusive_group')\n        if mutex_dest:\n          mutex_map[mutex_dest].append(dest)\n          dest = mutex_dest\n        else:\n          mutex_map[dest].append(dest)\n\n        if len(mutex_map[dest]) > 1:\n          raise self.MutuallyExclusiveOptionError(\n            \"Can only provide one of the mutually exclusive options {}\".format(mutex_map[dest]))\n\n      setattr(namespace, dest, val)\n\n    # See if there are any unconsumed flags remaining.\n    if flag_value_map:\n      raise ParseError('Unrecognized command line flags on {}: {}'.format(\n        self._scope_str(), ', '.join(flag_value_map.keys())))\n\n    return namespace"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef option_registrations_iter(self):\n    def normalize_kwargs(args, orig_kwargs):\n      nkwargs = copy.copy(orig_kwargs)\n      dest = self.parse_dest(*args, **nkwargs)\n      nkwargs['dest'] = dest\n      if not ('default' in nkwargs and isinstance(nkwargs['default'], RankedValue)):\n        nkwargs['default'] = self._compute_value(dest, nkwargs, [])\n      return nkwargs\n\n    # First yield any recursive options we inherit from our parent.\n    if self._parent_parser:\n      for args, kwargs in self._parent_parser._recursive_option_registration_args():\n        yield args, normalize_kwargs(args, kwargs)\n\n    # Then yield our directly-registered options.\n    # This must come after yielding inherited recursive options, so we can detect shadowing.\n    for args, kwargs in self._option_registrations:\n      normalized_kwargs = normalize_kwargs(args, kwargs)\n      if 'recursive' in normalized_kwargs:\n        # If we're the original registrar, make sure we can distinguish that.\n        normalized_kwargs['recursive_root'] = True\n      yield args, normalized_kwargs", "response": "Returns an iterator over the registration arguments of each option in this parser."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an iterator over the raw registration arguments of each option in this parser.", "response": "def _unnormalized_option_registrations_iter(self):\n    \"\"\"Returns an iterator over the raw registration arguments of each option in this parser.\n\n    Each yielded item is an (args, kwargs) pair, exactly as passed to register(), except for\n    substituting list and dict types with list_option/dict_option.\n\n    Note that recursive options we inherit from a parent will also be yielded here.\n    \"\"\"\n    # First yield any recursive options we inherit from our parent.\n    if self._parent_parser:\n      for args, kwargs in self._parent_parser._recursive_option_registration_args():\n        yield args, kwargs\n    # Then yield our directly-registered options.\n    for args, kwargs in self._option_registrations:\n      if 'recursive' in kwargs and self._scope_info.category == ScopeInfo.SUBSYSTEM:\n        raise RecursiveSubsystemOption(self.scope, args[0])\n      yield args, kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nyields args kwargs pairs for just our recursive options.", "response": "def _recursive_option_registration_args(self):\n    \"\"\"Yield args, kwargs pairs for just our recursive options.\n\n    Includes all the options we inherit recursively from our ancestors.\n    \"\"\"\n    if self._parent_parser:\n      for args, kwargs in self._parent_parser._recursive_option_registration_args():\n        yield args, kwargs\n    for args, kwargs in self._option_registrations:\n      # Note that all subsystem options are implicitly recursive: a subscope of a subsystem\n      # scope is another (optionable-specific) instance of the same subsystem, so it needs\n      # all the same options.\n      if self._scope_info.category == ScopeInfo.SUBSYSTEM or 'recursive' in kwargs:\n        yield args, kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking option for deprecation and issues a warning or error if necessary.", "response": "def _check_deprecated(self, dest, kwargs):\n    \"\"\"Checks option for deprecation and issues a warning/error if necessary.\"\"\"\n    removal_version = kwargs.get('removal_version', None)\n    if removal_version is not None:\n      warn_or_error(\n        removal_version=removal_version,\n        deprecated_entity_description=\"option '{}' in {}\".format(dest, self._scope_str()),\n        deprecation_start_version=kwargs.get('deprecation_start_version', None),\n        hint=kwargs.get('removal_hint', None),\n        stacklevel=9999)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate the arguments for the option registration.", "response": "def _validate(self, args, kwargs):\n    \"\"\"Validate option registration arguments.\"\"\"\n    def error(exception_type, arg_name=None, **msg_kwargs):\n      if arg_name is None:\n        arg_name = args[0] if args else '<unknown>'\n      raise exception_type(self.scope, arg_name, **msg_kwargs)\n\n    if not args:\n      error(NoOptionNames)\n    # validate args.\n    for arg in args:\n      if not arg.startswith('-'):\n        error(OptionNameDash, arg_name=arg)\n      if not arg.startswith('--') and len(arg) > 2:\n        error(OptionNameDoubleDash, arg_name=arg)\n\n    # Validate kwargs.\n    if 'implicit_value' in kwargs and kwargs['implicit_value'] is None:\n      error(ImplicitValIsNone)\n\n    # Note: we check for list here, not list_option, because we validate the provided kwargs,\n    # not the ones we modified.  However we temporarily also allow list_option, until the\n    # deprecation is complete.\n    if 'member_type' in kwargs and kwargs.get('type', str) not in [list, list_option]:\n      error(MemberTypeNotAllowed, type_=kwargs.get('type', str).__name__)\n\n    if kwargs.get('member_type', str) not in self._allowed_member_types:\n      error(InvalidMemberType, member_type=kwargs.get('member_type', str).__name__)\n\n    for kwarg in kwargs:\n      if kwarg not in self._allowed_registration_kwargs:\n        error(InvalidKwarg, kwarg=kwarg)\n\n      # Ensure `daemon=True` can't be passed on non-global scopes (except for `recursive=True`).\n      if (kwarg == 'daemon' and self._scope != GLOBAL_SCOPE and kwargs.get('recursive') is False):\n        error(InvalidKwargNonGlobalScope, kwarg=kwarg)\n\n    removal_version = kwargs.get('removal_version')\n    if removal_version is not None:\n      validate_deprecation_semver(removal_version, 'removal version')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_dest(*args, **kwargs):\n    explicit_dest = kwargs.get('dest')\n    if explicit_dest:\n      return explicit_dest\n\n    arg = next((a for a in args if a.startswith('--')), args[0])\n    return arg.lstrip('-').replace('-', '_')", "response": "Select the dest name for an option registration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the value to use for an option.", "response": "def _compute_value(self, dest, kwargs, flag_val_strs):\n    \"\"\"Compute the value to use for an option.\n\n    The source of the default value is chosen according to the ranking in RankedValue.\n    \"\"\"\n    # Helper function to convert a string to a value of the option's type.\n    def to_value_type(val_str):\n      if val_str is None:\n        return None\n      elif kwargs.get('type') == bool:\n        return self._ensure_bool(val_str)\n      else:\n        type_arg = kwargs.get('type', str)\n        try:\n          return self._wrap_type(type_arg)(val_str)\n        except TypeError as e:\n          raise ParseError(\n            \"Error applying type '{}' to option value '{}', for option '--{}' in {}: {}\"\n            .format(type_arg.__name__, val_str, dest, self._scope_str(), e))\n\n    # Helper function to expand a fromfile=True value string, if needed.\n    # May return a string or a dict/list decoded from a json/yaml file.\n    def expand(val_or_str):\n      if (kwargs.get('fromfile', True) and val_or_str and\n          isinstance(val_or_str, str) and val_or_str.startswith('@')):\n        if val_or_str.startswith('@@'):   # Support a literal @ for fromfile values via @@.\n          return val_or_str[1:]\n        else:\n          fromfile = val_or_str[1:]\n          try:\n            with open(fromfile, 'r') as fp:\n              s = fp.read().strip()\n              if fromfile.endswith('.json'):\n                return json.loads(s)\n              elif fromfile.endswith('.yml') or fromfile.endswith('.yaml'):\n                return yaml.safe_load(s)\n              else:\n                return s\n          except (IOError, ValueError, yaml.YAMLError) as e:\n            raise self.FromfileError('Failed to read {} in {} from file {}: {}'.format(\n                dest, self._scope_str(), fromfile, e))\n      else:\n        return val_or_str\n\n    # Get value from config files, and capture details about its derivation.\n    config_details = None\n    config_section = GLOBAL_SCOPE_CONFIG_SECTION if self._scope == GLOBAL_SCOPE else self._scope\n    config_default_val_or_str = expand(self._config.get(Config.DEFAULT_SECTION, dest, default=None))\n    config_val_or_str = expand(self._config.get(config_section, dest, default=None))\n    config_source_file = (self._config.get_source_for_option(config_section, dest) or\n        self._config.get_source_for_option(Config.DEFAULT_SECTION, dest))\n    if config_source_file is not None:\n      config_source_file = os.path.relpath(config_source_file)\n      config_details = 'in {}'.format(config_source_file)\n\n    # Get value from environment, and capture details about its derivation.\n    udest = dest.upper()\n    if self._scope == GLOBAL_SCOPE:\n      # For convenience, we allow three forms of env var for global scope options.\n      # The fully-specified env var is PANTS_GLOBAL_FOO, which is uniform with PANTS_<SCOPE>_FOO\n      # for all the other scopes.  However we also allow simply PANTS_FOO. And if the option name\n      # itself starts with 'pants-' then we also allow simply FOO. E.g., PANTS_WORKDIR instead of\n      # PANTS_PANTS_WORKDIR or PANTS_GLOBAL_PANTS_WORKDIR. We take the first specified value we\n      # find, in this order: PANTS_GLOBAL_FOO, PANTS_FOO, FOO.\n      env_vars = ['PANTS_GLOBAL_{0}'.format(udest), 'PANTS_{0}'.format(udest)]\n      if udest.startswith('PANTS_'):\n        env_vars.append(udest)\n    else:\n      sanitized_env_var_scope = self._ENV_SANITIZER_RE.sub('_', self._scope.upper())\n      env_vars = ['PANTS_{0}_{1}'.format(sanitized_env_var_scope, udest)]\n\n    env_val_or_str = None\n    env_details = None\n    if self._env:\n      for env_var in env_vars:\n        if env_var in self._env:\n          env_val_or_str = expand(self._env.get(env_var))\n          env_details = 'from env var {}'.format(env_var)\n          break\n\n    # Get value from cmd-line flags.\n    flag_vals = [to_value_type(expand(x)) for x in flag_val_strs]\n    if is_list_option(kwargs):\n      # Note: It's important to set flag_val to None if no flags were specified, so we can\n      # distinguish between no flags set vs. explicit setting of the value to [].\n      flag_val = ListValueComponent.merge(flag_vals) if flag_vals else None\n    elif is_dict_option(kwargs):\n      # Note: It's important to set flag_val to None if no flags were specified, so we can\n      # distinguish between no flags set vs. explicit setting of the value to {}.\n      flag_val = DictValueComponent.merge(flag_vals) if flag_vals else None\n    elif len(flag_vals) > 1:\n      raise ParseError('Multiple cmd line flags specified for option {} in {}'.format(\n          dest, self._scope_str()))\n    elif len(flag_vals) == 1:\n      flag_val = flag_vals[0]\n    else:\n      flag_val = None\n\n    # Rank all available values.\n    # Note that some of these values may already be of the value type, but type conversion\n    # is idempotent, so this is OK.\n\n    values_to_rank = [to_value_type(x) for x in\n                      [flag_val, env_val_or_str, config_val_or_str,\n                       config_default_val_or_str, kwargs.get('default'), None]]\n    # Note that ranked_vals will always have at least one element, and all elements will be\n    # instances of RankedValue (so none will be None, although they may wrap a None value).\n    ranked_vals = list(reversed(list(RankedValue.prioritized_iter(*values_to_rank))))\n\n    def record_option(value, rank, option_details=None):\n      deprecation_version = kwargs.get('removal_version')\n      self._option_tracker.record_option(scope=self._scope,\n                                         option=dest,\n                                         value=value,\n                                         rank=rank,\n                                         deprecation_version=deprecation_version,\n                                         details=option_details)\n\n    # Record info about the derivation of each of the contributing values.\n    detail_history = []\n    for ranked_val in ranked_vals:\n      if ranked_val.rank in (RankedValue.CONFIG, RankedValue.CONFIG_DEFAULT):\n        details = config_details\n      elif ranked_val.rank == RankedValue.ENVIRONMENT:\n        details = env_details\n      else:\n        details = None\n      if details:\n        detail_history.append(details)\n      record_option(value=ranked_val.value, rank=ranked_val.rank, option_details=details)\n\n    # Helper function to check various validity constraints on final option values.\n    def check(val):\n      if val is not None:\n        choices = kwargs.get('choices')\n        # If the `type` argument has an `all_variants` attribute, use that as `choices` if not\n        # already set. Using an attribute instead of checking a subclass allows `type` arguments\n        # which are functions to have an implicit fallback `choices` set as well.\n        if choices is None and 'type' in kwargs:\n          type_arg = kwargs.get('type')\n          if hasattr(type_arg, 'all_variants'):\n            choices = list(type_arg.all_variants)\n        # TODO: convert this into an enum() pattern match!\n        if choices is not None and val not in choices:\n          raise ParseError('`{}` is not an allowed value for option {} in {}. '\n                           'Must be one of: {}'.format(val, dest, self._scope_str(), choices))\n        elif kwargs.get('type') == dir_option and not os.path.isdir(val):\n          raise ParseError('Directory value `{}` for option {} in {} does not exist.'.format(\n              val, dest, self._scope_str()))\n        elif kwargs.get('type') == file_option and not os.path.isfile(val):\n          raise ParseError('File value `{}` for option {} in {} does not exist.'.format(\n              val, dest, self._scope_str()))\n\n    # Generate the final value from all available values, and check that it (or its members,\n    # if a list) are in the set of allowed choices.\n    if is_list_option(kwargs):\n      merged_rank = ranked_vals[-1].rank\n      merged_val = ListValueComponent.merge(\n          [rv.value for rv in ranked_vals if rv.value is not None]).val\n      # TODO: run `check()` for all elements of a list option too!!!\n      merged_val = [self._convert_member_type(kwargs.get('member_type', str), x)\n                    for x in merged_val]\n      for val in merged_val:\n        check(val)\n      ret = RankedValue(merged_rank, merged_val)\n    elif is_dict_option(kwargs):\n      # TODO: convert `member_type` for dict values too!\n      merged_rank = ranked_vals[-1].rank\n      merged_val = DictValueComponent.merge(\n          [rv.value for rv in ranked_vals if rv.value is not None]).val\n      for val in merged_val:\n        check(val)\n      ret = RankedValue(merged_rank, merged_val)\n    else:\n      ret = ranked_vals[-1]\n      check(ret.value)\n\n    # Record info about the derivation of the final value.\n    merged_details = ', '.join(detail_history) if detail_history else None\n    record_option(value=ret.value, rank=ret.rank, option_details=merged_details)\n\n    # All done!\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _generate_snapshot_bdist_wheel_argv(self, snapshot_fingerprint, is_platform_specific):\n    egg_info_snapshot_tag_args = ['egg_info', '--tag-build=+{}'.format(snapshot_fingerprint)]\n    bdist_whl_args = ['bdist_wheel']\n    if is_platform_specific:\n      platform_args = ['--plat-name', pep425tags.get_platform()]\n    else:\n      platform_args = []\n\n    dist_dir_args = ['--dist-dir', self._DIST_OUTPUT_DIR]\n\n    return (['setup.py'] +\n            egg_info_snapshot_tag_args +\n            bdist_whl_args +\n            platform_args +\n            dist_dir_args)", "response": "Generate the command line to pass to setup. py."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_dist(self,\n                   dist_tgt,\n                   dist_target_dir,\n                   setup_requires_pex,\n                   snapshot_fingerprint,\n                   is_platform_specific):\n    \"\"\"Create a .whl file for the specified python_distribution target.\"\"\"\n    self._copy_sources(dist_tgt, dist_target_dir)\n\n    setup_py_snapshot_version_argv = self._generate_snapshot_bdist_wheel_argv(\n      snapshot_fingerprint, is_platform_specific)\n\n    cmd = safe_shlex_join(setup_requires_pex.cmdline(setup_py_snapshot_version_argv))\n    with self.context.new_workunit('setup.py', cmd=cmd, labels=[WorkUnitLabel.TOOL]) as workunit:\n      with pushd(dist_target_dir):\n        result = setup_requires_pex.run(args=setup_py_snapshot_version_argv,\n                                        stdout=workunit.output('stdout'),\n                                        stderr=workunit.output('stderr'))\n        if result != 0:\n          raise self.BuildLocalPythonDistributionsError(\n            \"Installation of python distribution from target {target} into directory {into_dir} \"\n            \"failed (return value of run() was: {rc!r}).\\n\"\n            \"The pex with any requirements is located at: {interpreter}.\\n\"\n            \"The host system's compiler and linker were used.\\n\"\n            \"The setup command was: {command}.\"\n            .format(target=dist_tgt,\n                    into_dir=dist_target_dir,\n                    rc=result,\n                    interpreter=setup_requires_pex.path(),\n                    command=setup_py_snapshot_version_argv))", "response": "Create a. whl file for the specified python_distribution target."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninjects a synthetic requirements library that references a locally built wheel.", "response": "def _inject_synthetic_dist_requirements(self, dist, req_lib_addr):\n    \"\"\"Inject a synthetic requirements library that references a local wheel.\n\n    :param dist: Path of the locally built wheel to reference.\n    :param req_lib_addr:  :class: `Address` to give to the synthetic target.\n    :return: a :class: `PythonRequirementLibrary` referencing the locally-built wheel.\n    \"\"\"\n    whl_dir, base = split_basename_and_dirname(dist)\n    whl_metadata = base.split('-')\n    req_name = '=='.join([whl_metadata[0], whl_metadata[1]])\n    req = PythonRequirement(req_name, repository=whl_dir)\n    self.context.build_graph.inject_synthetic_target(req_lib_addr, PythonRequirementLibrary,\n                                                     requirements=[req])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the absolute path of the whl in a setup. py install directory.", "response": "def _get_whl_from_dir(cls, install_dir):\n    \"\"\"Return the absolute path of the whl in a setup.py install directory.\"\"\"\n    dist_dir = cls._get_dist_dir(install_dir)\n    dists = glob.glob(os.path.join(dist_dir, '*.whl'))\n    if len(dists) == 0:\n      raise cls.BuildLocalPythonDistributionsError(\n        'No distributions were produced by python_create_distribution task.\\n'\n        'dist_dir: {}, install_dir: {}'\n        .format(dist_dir, install_dir))\n    if len(dists) > 1:\n      # TODO: is this ever going to happen?\n      raise cls.BuildLocalPythonDistributionsError('Ambiguous local python distributions found: {}'\n                                                   .format(dists))\n    return dists[0]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nregister options for this optionable.", "response": "def register_options(cls, register):\n    \"\"\"Register options for this optionable.\n\n    In this case, there are no special options, but we want to use this opportunity to setup\n    goal validation in JvmPrepCommand before the build graph is parsed.\n    \"\"\"\n    super(RunJvmPrepCommandBase, cls).register_options(register)\n    JvmPrepCommand.add_goal(cls.goal)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prepare(cls, options, round_manager):\n    super(RunJvmPrepCommandBase, cls).prepare(options, round_manager)\n    round_manager.require_data('compile_classpath')\n    if not cls.classpath_product_only:\n      round_manager.require_data('runtime_classpath')", "response": "Prepare the JVM for execution."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a processhandler to a process executing java.", "response": "def _spawn(self, distribution, executor=None, *args, **kwargs):\n    \"\"\"Returns a processhandler to a process executing java.\n\n    :param Executor executor: the java subprocess executor to use. If not specified, construct\n      using the distribution.\n    :param Distribution distribution: The JDK or JRE installed.\n    :rtype: ProcessHandler\n    \"\"\"\n\n    actual_executor = executor or SubprocessExecutor(distribution)\n    return distribution.execute_java_async(*args,\n                                           executor=actual_executor,\n                                           **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes java for the given targets.", "response": "def execute_java_for_coverage(self, targets, *args, **kwargs):\n    \"\"\"Execute java for targets directly and don't use the test mixin.\n\n    This execution won't be wrapped with timeouts and other test mixin code common\n    across test targets. Used for coverage instrumentation.\n    \"\"\"\n\n    distribution = self.preferred_jvm_distribution_for_targets(targets)\n    actual_executor = SubprocessExecutor(distribution)\n    return distribution.execute_java(*args, executor=actual_executor, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a test specification string into an object that can yield corresponding tests.", "response": "def _parse(self, test_spec_str):\n    \"\"\"Parses a test specification string into an object that can yield corresponding tests.\n\n    Tests can be specified in one of four forms:\n\n    * [classname]\n    * [classname]#[methodname]\n    * [fully qualified classname]#[methodname]\n    * [fully qualified classname]#[methodname]\n\n    :param string test_spec: A test specification.\n    :returns: A Test object.\n    :rtype: :class:`Test`\n    \"\"\"\n    components = test_spec_str.split('#', 2)\n    classname = components[0]\n    methodname = components[1] if len(components) == 2 else None\n\n    return Test(classname=classname, methodname=methodname)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nregistering a jvm tool under the key for lazy classpath resolution.", "response": "def register_jvm_tool(cls,\n                        register,\n                        key,\n                        classpath_spec=None,\n                        main=None,\n                        custom_rules=None,\n                        fingerprint=True,\n                        classpath=None,\n                        help=None,\n                        removal_version=None,\n                        removal_hint=None):\n    \"\"\"Registers a jvm tool under `key` for lazy classpath resolution.\n\n    Classpaths can be retrieved in `execute` scope via `tool_classpath_from_products`.\n\n    NB: If the tool's `main` class name is supplied the tool classpath will be shaded.\n\n    :param register: A function that can register options with the option system.\n    :param unicode key: The key the tool configuration should be registered under.\n    :param unicode classpath_spec: The tool classpath target address spec that can be used to\n                                   override this tool's classpath; by default, `//:[key]`.\n    :param unicode main: The fully qualified class name of the tool's main class if shading of the\n                         tool classpath is desired.\n    :param list custom_rules: An optional list of `Shader.Rule`s to apply before the automatically\n                              generated binary jar shading rules.  This is useful for excluding\n                              classes shared between the tool and the code it runs over.  The\n                              canonical example is the `org.junit.Test` annotation read by junit\n                              runner tools from user code. In this sort of case the shared code must\n                              have a uniform name between the tool and the user code and so the\n                              shared code must be excluded from shading.\n    :param bool fingerprint: Indicates whether to include the jvm tool in the task's fingerprint.\n                             Note that unlike for other options, fingerprinting is enabled for tools\n                             by default.\n    :param list classpath: A list of one or more `JarDependency` objects that form this tool's\n                           default classpath.  If the classpath is optional, supply an empty list;\n                           otherwise the default classpath of `None` indicates the `classpath_spec`\n                           must point to a target defined in a BUILD file that provides the tool\n                           classpath.\n    :param unicode help: An optional custom help string; otherwise a reasonable one is generated.\n    :param string removal_version: A semver at which this tool will be removed.\n    :param unicode removal_hint: A hint on how to migrate away from this tool.\n    \"\"\"\n    def formulate_help():\n      if classpath:\n        return ('Target address spec for overriding the classpath of the {} jvm tool which is, '\n                'by default: {}'.format(key, classpath))\n      else:\n        return 'Target address spec for specifying the classpath of the {} jvm tool.'.format(key)\n    help = help or formulate_help()\n\n    register('--{}'.format(key),\n             advanced=True,\n             type=target_option,\n             default='//:{}'.format(key) if classpath_spec is None else classpath_spec,\n             help=help,\n             fingerprint=fingerprint,\n             removal_version=removal_version,\n             removal_hint=removal_hint)\n\n    # TODO(John Sirois): Move towards requiring tool specs point to jvm_binary targets.\n    # These already have a main and are a natural place to house any custom shading rules.  That\n    # would eliminate the need to pass main and custom_rules here.\n    # It is awkward that jars can no longer be inlined as dependencies - this will require 2 targets\n    # for every tool - the jvm_binary, and a jar_library for its dependencies to point to.  It may\n    # be worth creating a JarLibrary subclass - say JarBinary, or else mixing in a Binary interface\n    # to JarLibrary to endow it with main and shade_rules attributes to allow for single-target\n    # definition of resolvable jvm binaries.\n    jvm_tool = cls.JvmTool(register.scope, key, classpath, main, custom_rules)\n    JvmToolMixin._jvm_tools.append(jvm_tool)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tool_jar_from_products(cls, products, key, scope):\n    classpath = cls.tool_classpath_from_products(products, key, scope)\n    if len(classpath) != 1:\n      params = dict(tool=key, scope=scope, count=len(classpath), classpath='\\n\\t'.join(classpath))\n      raise cls.InvalidToolClasspath('Expected tool {tool} in scope {scope} to resolve to one '\n                                     'jar, instead found {count}:\\n\\t{classpath}'.format(**params))\n    return classpath[0]", "response": "Returns the path to the jar for the given tool configuration in the given scope."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tool_classpath_from_products(products, key, scope):\n    callback_product_map = products.get_data('jvm_build_tools_classpath_callbacks') or {}\n    callback = callback_product_map.get(scope, {}).get(key)\n    if not callback:\n      raise TaskError('No bootstrap callback registered for {key} in {scope}'\n                      .format(key=key, scope=scope))\n    return callback()", "response": "Get a classpath for the tool previously registered under key in the given scope."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef target_types_from_build_file_aliases(aliases):\n  target_types = dict(aliases.target_types)\n  for alias, factory in aliases.target_macro_factories.items():\n    target_type, = factory.target_types\n    target_types[alias] = target_type\n  return target_types", "response": "Given BuildFileAliases return the concrete target types constructed for each alias."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef transitive_hydrated_targets(build_file_addresses):\n\n  transitive_hydrated_targets = yield [Get(TransitiveHydratedTarget, Address, a)\n                                       for a in build_file_addresses.addresses]\n\n  closure = OrderedSet()\n  to_visit = deque(transitive_hydrated_targets)\n\n  while to_visit:\n    tht = to_visit.popleft()\n    if tht.root in closure:\n      continue\n    closure.add(tht.root)\n    to_visit.extend(tht.dependencies)\n\n  yield TransitiveHydratedTargets(tuple(tht.root for tht in transitive_hydrated_targets), closure)", "response": "Given BuildFileAddresses kicks off recursion on expansion of TransitiveHydratedTargets."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hydrated_targets(build_file_addresses):\n  targets = yield [Get(HydratedTarget, Address, a) for a in build_file_addresses.addresses]\n  yield HydratedTargets(targets)", "response": "Requests HydratedTarget instances for BuildFileAddresses."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconstructing a HydratedTarget from a TargetAdaptor and hydrated versions of its adapted fields.", "response": "def hydrate_target(hydrated_struct):\n  target_adaptor = hydrated_struct.value\n  \"\"\"Construct a HydratedTarget from a TargetAdaptor and hydrated versions of its adapted fields.\"\"\"\n  # Hydrate the fields of the adaptor and re-construct it.\n  hydrated_fields = yield [Get(HydratedField, HydrateableField, fa)\n                           for fa in target_adaptor.field_adaptors]\n  kwargs = target_adaptor.kwargs()\n  for field in hydrated_fields:\n    kwargs[field.name] = field.value\n  yield HydratedTarget(target_adaptor.address,\n                        TargetAdaptor(**kwargs),\n                        tuple(target_adaptor.dependencies))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hydrate_sources(sources_field, glob_match_error_behavior):\n  # TODO(#5864): merge the target's selection of --glob-expansion-failure (which doesn't exist yet)\n  # with the global default!\n  path_globs = sources_field.path_globs.copy(glob_match_error_behavior=glob_match_error_behavior)\n  snapshot = yield Get(Snapshot, PathGlobs, path_globs)\n  fileset_with_spec = _eager_fileset_with_spec(\n    sources_field.address.spec_path,\n    sources_field.filespecs,\n    snapshot)\n  sources_field.validate_fn(fileset_with_spec)\n  yield HydratedField(sources_field.arg, fileset_with_spec)", "response": "Given a SourcesField request a Snapshot for its path_globs and create an EagerFilesetWithSpec."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hydrate_bundles(bundles_field, glob_match_error_behavior):\n  path_globs_with_match_errors = [\n    pg.copy(glob_match_error_behavior=glob_match_error_behavior)\n    for pg in bundles_field.path_globs_list\n  ]\n  snapshot_list = yield [Get(Snapshot, PathGlobs, pg) for pg in path_globs_with_match_errors]\n\n  spec_path = bundles_field.address.spec_path\n\n  bundles = []\n  zipped = zip(bundles_field.bundles,\n               bundles_field.filespecs_list,\n               snapshot_list)\n  for bundle, filespecs, snapshot in zipped:\n    rel_spec_path = getattr(bundle, 'rel_path', spec_path)\n    kwargs = bundle.kwargs()\n    # NB: We `include_dirs=True` because bundle filesets frequently specify directories in order\n    # to trigger a (deprecated) default inclusion of their recursive contents. See the related\n    # deprecation in `pants.backend.jvm.tasks.bundle_create`.\n    kwargs['fileset'] = _eager_fileset_with_spec(rel_spec_path,\n                                                 filespecs,\n                                                 snapshot,\n                                                 include_dirs=True)\n    bundles.append(BundleAdaptor(**kwargs))\n  yield HydratedField('bundles', bundles)", "response": "Given a BundlesField request Snapshots for each of its filesets and create BundleAdaptors."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_legacy_graph_tasks():\n  return [\n    transitive_hydrated_targets,\n    transitive_hydrated_target,\n    hydrated_targets,\n    hydrate_target,\n    find_owners,\n    hydrate_sources,\n    hydrate_bundles,\n    RootRule(OwnersRequest),\n  ]", "response": "Create tasks to recursively parse the legacy graph."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nindex from the given roots into the storage provided by the base class.", "response": "def _index(self, hydrated_targets):\n    \"\"\"Index from the given roots into the storage provided by the base class.\n\n    This is an additive operation: any existing connections involving these nodes are preserved.\n    \"\"\"\n    all_addresses = set()\n    new_targets = list()\n\n    # Index the ProductGraph.\n    for hydrated_target in hydrated_targets:\n      target_adaptor = hydrated_target.adaptor\n      address = target_adaptor.address\n      all_addresses.add(address)\n      if address not in self._target_by_address:\n        new_targets.append(self._index_target(target_adaptor))\n\n    # Once the declared dependencies of all targets are indexed, inject their\n    # additional \"traversable_(dependency_)?specs\".\n    deps_to_inject = OrderedSet()\n    addresses_to_inject = set()\n    def inject(target, dep_spec, is_dependency):\n      address = Address.parse(dep_spec, relative_to=target.address.spec_path)\n      if not any(address == t.address for t in target.dependencies):\n        addresses_to_inject.add(address)\n        if is_dependency:\n          deps_to_inject.add((target.address, address))\n\n    self.apply_injectables(new_targets)\n\n    for target in new_targets:\n      for spec in target.compute_dependency_specs(payload=target.payload):\n        inject(target, spec, is_dependency=True)\n\n      for spec in target.compute_injectable_specs(payload=target.payload):\n        inject(target, spec, is_dependency=False)\n\n    # Inject all addresses, then declare injected dependencies.\n    self.inject_addresses_closure(addresses_to_inject)\n    for target_address, dep_address in deps_to_inject:\n      self.inject_dependency(dependent=target_address, dependency=dep_address)\n\n    return all_addresses"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _instantiate_target(self, target_adaptor):\n    target_cls = self._target_types[target_adaptor.type_alias]\n    try:\n      # Pop dependencies, which were already consumed during construction.\n      kwargs = target_adaptor.kwargs()\n      kwargs.pop('dependencies')\n\n      # Instantiate.\n      if issubclass(target_cls, AppBase):\n        return self._instantiate_app(target_cls, kwargs)\n      elif target_cls is RemoteSources:\n        return self._instantiate_remote_sources(kwargs)\n      return target_cls(build_graph=self, **kwargs)\n    except TargetDefinitionException:\n      raise\n    except Exception as e:\n      raise TargetDefinitionException(\n          target_adaptor.address,\n          'Failed to instantiate Target with type {}: {}'.format(target_cls, e))", "response": "Given a TargetAdaptor struct previously parsed from a BUILD file instantiate a Target."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _inject_addresses(self, subjects):\n    logger.debug('Injecting addresses to %s: %s', self, subjects)\n    with self._resolve_context():\n      addresses = tuple(subjects)\n      thts, = self._scheduler.product_request(TransitiveHydratedTargets,\n                                              [BuildFileAddresses(addresses)])\n\n    self._index(thts.closure)\n\n    yielded_addresses = set()\n    for address in subjects:\n      if address not in yielded_addresses:\n        yielded_addresses.add(address)\n        yield address", "response": "Injects targets into the graph for each of the given Address objects and yields them."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninjecting targets into the graph for the given Specs object.", "response": "def _inject_specs(self, specs):\n    \"\"\"Injects targets into the graph for the given `Specs` object.\n\n    Yields the resulting addresses.\n    \"\"\"\n    if not specs:\n      return\n\n    logger.debug('Injecting specs to %s: %s', self, specs)\n    with self._resolve_context():\n      thts, = self._scheduler.product_request(TransitiveHydratedTargets,\n                                              [specs])\n\n    self._index(thts.closure)\n\n    for hydrated_target in thts.roots:\n      yield hydrated_target.address"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_iterable(cls, target_types, address_mapper, adaptor_iter):\n    inst = cls(target_types, address_mapper)\n    all_valid_addresses = set()\n    for target_adaptor in adaptor_iter:\n      inst._inject_target(target_adaptor)\n      all_valid_addresses.add(target_adaptor.address)\n    inst._validate(all_valid_addresses)\n    return inst", "response": "Create a new DependentGraph from an iterable of TargetAdaptor subclasses."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _validate(self, all_valid_addresses):\n    for dependency, dependents in iteritems(self._dependent_address_map):\n      if dependency not in all_valid_addresses:\n        raise AddressLookupError(\n            'Dependent graph construction failed: {} did not exist. Was depended on by:\\n  {}'.format(\n             dependency.spec,\n             '\\n  '.join(d.spec for d in dependents)\n          )\n        )", "response": "Validate that all of the dependencies in the graph exist in the given addresses set."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _inject_target(self, target_adaptor):\n    target_cls = self._target_types[target_adaptor.type_alias]\n\n    declared_deps = target_adaptor.dependencies\n    implicit_deps = (Address.parse(s,\n                                   relative_to=target_adaptor.address.spec_path,\n                                   subproject_roots=self._address_mapper.subproject_roots)\n                     for s in target_cls.compute_dependency_specs(kwargs=target_adaptor.kwargs()))\n    for dep in declared_deps:\n      self._dependent_address_map[dep].add(target_adaptor.address)\n    for dep in implicit_deps:\n      self._implicit_dependent_address_map[dep].add(target_adaptor.address)", "response": "Inject a target respecting all sources of dependencies."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dependents_of_addresses(self, addresses):\n    seen = OrderedSet(addresses)\n    for address in addresses:\n      seen.update(self._dependent_address_map[address])\n      seen.update(self._implicit_dependent_address_map[address])\n    return seen", "response": "Given an iterable of addresses yield all of those addresses dependents."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving an iterable of addresses yield all of those addresses dependents transitively.", "response": "def transitive_dependents_of_addresses(self, addresses):\n    \"\"\"Given an iterable of addresses, yield all of those addresses dependents, transitively.\"\"\"\n    closure = set()\n    result = []\n    to_visit = deque(addresses)\n\n    while to_visit:\n      address = to_visit.popleft()\n      if address in closure:\n        continue\n\n      closure.add(address)\n      result.append(address)\n      to_visit.extend(self._dependent_address_map[address])\n      to_visit.extend(self._implicit_dependent_address_map[address])\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconstruct a SymbolTable for the given BuildFileAliases.", "response": "def _legacy_symbol_table(build_file_aliases):\n  \"\"\"Construct a SymbolTable for the given BuildFileAliases.\n\n  :param build_file_aliases: BuildFileAliases to register.\n  :type build_file_aliases: :class:`pants.build_graph.build_file_aliases.BuildFileAliases`\n\n  :returns: A SymbolTable.\n  \"\"\"\n  table = {\n    alias: _make_target_adaptor(TargetAdaptor, target_type)\n    for alias, target_type in build_file_aliases.target_types.items()\n  }\n\n  for alias, factory in build_file_aliases.target_macro_factories.items():\n    # TargetMacro.Factory with more than one target type is deprecated.\n    # For default sources, this means that TargetMacro Factories with more than one target_type\n    # will not parse sources through the engine, and will fall back to the legacy python sources\n    # parsing.\n    # Conveniently, multi-target_type TargetMacro.Factory, and legacy python source parsing, are\n    # targeted to be removed in the same version of pants.\n    if len(factory.target_types) == 1:\n      table[alias] = _make_target_adaptor(\n        TargetAdaptor,\n        tuple(factory.target_types)[0],\n      )\n\n  # TODO: The alias replacement here is to avoid elevating \"TargetAdaptors\" into the public\n  # API until after https://github.com/pantsbuild/pants/issues/3560 has been completed.\n  # These should likely move onto Target subclasses as the engine gets deeper into beta\n  # territory.\n  table['python_library'] = _make_target_adaptor(PythonTargetAdaptor, PythonLibrary)\n\n  table['jvm_app'] = _make_target_adaptor(AppAdaptor, JvmApp)\n  table['jvm_binary'] = _make_target_adaptor(JvmBinaryAdaptor, JvmBinary)\n  table['python_app'] = _make_target_adaptor(AppAdaptor, PythonApp)\n  table['python_tests'] = _make_target_adaptor(PythonTestsAdaptor, PythonTests)\n  table['python_binary'] = _make_target_adaptor(PythonBinaryAdaptor, PythonBinary)\n  table['remote_sources'] = _make_target_adaptor(RemoteSourcesAdaptor, RemoteSources)\n  table['page'] = _make_target_adaptor(PageAdaptor, Page)\n\n  # Note that these don't call _make_target_adaptor because we don't have a handy reference to the\n  # types being constructed. They don't have any default_sources behavior, so this should be ok,\n  # but if we end up doing more things in _make_target_adaptor, we should make sure they're\n  # applied here too.\n  table['pants_plugin'] = PantsPluginAdaptor\n  table['contrib_plugin'] = PantsPluginAdaptor\n\n  return SymbolTable(table)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _make_target_adaptor(base_class, target_type):\n  if not target_type.supports_default_sources() or target_type.default_sources_globs is None:\n    return base_class\n\n  globs = _tuplify(target_type.default_sources_globs)\n  excludes = _tuplify(target_type.default_sources_exclude_globs)\n\n  class GlobsHandlingTargetAdaptor(base_class):\n    @property\n    def default_sources_globs(self):\n      if globs is None:\n        return super(GlobsHandlingTargetAdaptor, self).default_sources_globs\n      else:\n        return globs\n\n    @property\n    def default_sources_exclude_globs(self):\n      if excludes is None:\n        return super(GlobsHandlingTargetAdaptor, self).default_sources_exclude_globs\n      else:\n        return excludes\n\n  return GlobsHandlingTargetAdaptor", "response": "Create a new target adaptor for the given target type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef warm_product_graph(self, target_roots):\n    logger.debug('warming target_roots for: %r', target_roots)\n    subjects = [target_roots.specs]\n    request = self.scheduler_session.execution_request([TransitiveHydratedTargets], subjects)\n    self.scheduler_session.execute(request)", "response": "Warm the scheduler s ProductGraph with TransitiveHydratedTargets products."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_console_rules(self, options_bootstrapper, goals, target_roots):\n    subject = target_roots.specs\n    console = Console()\n    for goal in goals:\n      goal_product = self.goal_map[goal]\n      params = Params(subject, options_bootstrapper, console)\n      logger.debug('requesting {} to satisfy execution of `{}` goal'.format(goal_product, goal))\n      try:\n        exit_code = self.scheduler_session.run_console_rule(goal_product, params)\n      finally:\n        console.flush()\n\n      if exit_code != PANTS_SUCCEEDED_EXIT_CODE:\n        return exit_code\n    return PANTS_SUCCEEDED_EXIT_CODE", "response": "Runs the console rules sequentially and interactively by requesting implicit Goal products."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconstructs and return a BuildGraph given a set of target roots.", "response": "def create_build_graph(self, target_roots, build_root=None):\n    \"\"\"Construct and return a `BuildGraph` given a set of input specs.\n\n    :param TargetRoots target_roots: The targets root of the request.\n    :param string build_root: The build root.\n    :returns: A tuple of (BuildGraph, AddressMapper).\n    \"\"\"\n    logger.debug('target_roots are: %r', target_roots)\n    graph = LegacyBuildGraph.create(self.scheduler_session, self.build_file_aliases)\n    logger.debug('build_graph is: %s', graph)\n    # Ensure the entire generator is unrolled.\n    for _ in graph.inject_roots_closure(target_roots):\n      pass\n\n    address_mapper = LegacyAddressMapper(self.scheduler_session, build_root or get_buildroot())\n    logger.debug('address_mapper is: %s', address_mapper)\n    return graph, address_mapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconstructs and return the components necessary for LegacyBuildGraph construction.", "response": "def setup_legacy_graph(native, options_bootstrapper, build_configuration):\n    \"\"\"Construct and return the components necessary for LegacyBuildGraph construction.\"\"\"\n    bootstrap_options = options_bootstrapper.bootstrap_options.for_global_scope()\n    return EngineInitializer.setup_legacy_graph_extended(\n      bootstrap_options.pants_ignore,\n      bootstrap_options.pants_workdir,\n      bootstrap_options.local_store_dir,\n      bootstrap_options.build_file_imports,\n      options_bootstrapper,\n      build_configuration,\n      native=native,\n      glob_match_error_behavior=bootstrap_options.glob_expansion_failure,\n      build_ignore_patterns=bootstrap_options.build_ignore,\n      exclude_target_regexps=bootstrap_options.exclude_target_regexp,\n      subproject_roots=bootstrap_options.subproject_roots,\n      include_trace_on_error=bootstrap_options.print_exception_stacktrace,\n      execution_options=ExecutionOptions.from_bootstrap_options(bootstrap_options),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setup_legacy_graph_extended(\n    pants_ignore_patterns,\n    workdir,\n    local_store_dir,\n    build_file_imports_behavior,\n    options_bootstrapper,\n    build_configuration,\n    build_root=None,\n    native=None,\n    glob_match_error_behavior=None,\n    build_ignore_patterns=None,\n    exclude_target_regexps=None,\n    subproject_roots=None,\n    include_trace_on_error=True,\n    execution_options=None,\n  ):\n    \"\"\"Construct and return the components necessary for LegacyBuildGraph construction.\n\n    :param list pants_ignore_patterns: A list of path ignore patterns for FileSystemProjectTree,\n                                       usually taken from the '--pants-ignore' global option.\n    :param str workdir: The pants workdir.\n    :param local_store_dir: The directory to use for storing the engine's LMDB store in.\n    :param build_file_imports_behavior: How to behave if a BUILD file being parsed tries to use\n      import statements. Valid values: \"allow\", \"warn\", \"error\".\n    :type build_file_imports_behavior: string\n    :param str build_root: A path to be used as the build root. If None, then default is used.\n    :param Native native: An instance of the native-engine subsystem.\n    :param options_bootstrapper: A `OptionsBootstrapper` object containing bootstrap options.\n    :type options_bootstrapper: :class:`pants.options.options_bootstrapper.OptionsBootstrapper`\n    :param build_configuration: The `BuildConfiguration` object to get build file aliases from.\n    :type build_configuration: :class:`pants.build_graph.build_configuration.BuildConfiguration`\n    :param glob_match_error_behavior: How to behave if a glob specified for a target's sources or\n                                      bundles does not expand to anything.\n    :type glob_match_error_behavior: :class:`pants.option.global_options.GlobMatchErrorBehavior`\n    :param list build_ignore_patterns: A list of paths ignore patterns used when searching for BUILD\n                                       files, usually taken from the '--build-ignore' global option.\n    :param list exclude_target_regexps: A list of regular expressions for excluding targets.\n    :param list subproject_roots: Paths that correspond with embedded build roots\n                                  under the current build root.\n    :param bool include_trace_on_error: If True, when an error occurs, the error message will\n                include the graph trace.\n    :param execution_options: Option values for (remote) process execution.\n    :type execution_options: :class:`pants.option.global_options.ExecutionOptions`\n    :returns: A LegacyGraphScheduler.\n    \"\"\"\n\n    build_root = build_root or get_buildroot()\n    build_configuration = build_configuration or BuildConfigInitializer.get(options_bootstrapper)\n    bootstrap_options = options_bootstrapper.bootstrap_options.for_global_scope()\n\n    build_file_aliases = build_configuration.registered_aliases()\n    rules = build_configuration.rules()\n\n    symbol_table = _legacy_symbol_table(build_file_aliases)\n\n    project_tree = FileSystemProjectTree(build_root, pants_ignore_patterns)\n\n    execution_options = execution_options or DEFAULT_EXECUTION_OPTIONS\n\n    # Register \"literal\" subjects required for these rules.\n    parser = LegacyPythonCallbacksParser(\n      symbol_table,\n      build_file_aliases,\n      build_file_imports_behavior\n    )\n    address_mapper = AddressMapper(parser=parser,\n                                   build_ignore_patterns=build_ignore_patterns,\n                                   exclude_target_regexps=exclude_target_regexps,\n                                   subproject_roots=subproject_roots)\n\n    @rule(GlobMatchErrorBehavior, [])\n    def glob_match_error_behavior_singleton():\n      return glob_match_error_behavior or GlobMatchErrorBehavior.ignore\n\n    @rule(BuildConfiguration, [])\n    def build_configuration_singleton():\n      return build_configuration\n\n    @rule(SymbolTable, [])\n    def symbol_table_singleton():\n      return symbol_table\n\n    # Create a Scheduler containing graph and filesystem rules, with no installed goals. The\n    # LegacyBuildGraph will explicitly request the products it needs.\n    rules = (\n      [\n        RootRule(Console),\n        glob_match_error_behavior_singleton,\n        build_configuration_singleton,\n        symbol_table_singleton,\n      ] +\n      create_legacy_graph_tasks() +\n      create_fs_rules() +\n      create_process_rules() +\n      create_graph_rules(address_mapper) +\n      create_options_parsing_rules() +\n      structs_rules() +\n      # TODO: This should happen automatically, but most tests (e.g. tests/python/pants_test/auth) fail if it's not here:\n      python_test_runner.rules() +\n      rules\n    )\n\n    goal_map = EngineInitializer._make_goal_map_from_rules(rules)\n\n    union_rules = build_configuration.union_rules()\n\n    scheduler = Scheduler(\n      native,\n      project_tree,\n      workdir,\n      local_store_dir,\n      rules,\n      union_rules,\n      execution_options,\n      include_trace_on_error=include_trace_on_error,\n      visualize_to_dir=bootstrap_options.native_engine_visualize_to,\n    )\n\n    return LegacyGraphScheduler(scheduler, build_file_aliases, goal_map)", "response": "This function is used to setup the legacy build graph for the legacy build graph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nregistering a callback to execute on each filesystem event.", "response": "def register_all_files_handler(self, callback, name='all_files'):\n    \"\"\"Registers a subscription for all files under a given watch path.\n\n    :param func callback: the callback to execute on each filesystem event\n    :param str name:      the subscription name as used by watchman\n    \"\"\"\n    self.register_handler(\n      name,\n      dict(\n        fields=['name'],\n        # Request events for all file types.\n        # NB: Touching a file invalidates its parent directory due to:\n        #   https://github.com/facebook/watchman/issues/305\n        # ...but if we were to skip watching directories, we'd still have to invalidate\n        # the parents of any changed files, and we wouldn't see creation/deletion of\n        # empty directories.\n        expression=[\n          'allof',  # All of the below rules must be true to match.\n          ['not', ['dirname', 'dist', self.ZERO_DEPTH]],  # Exclude the ./dist dir.\n          # N.B. 'wholename' ensures we match against the absolute ('x/y/z') vs base path ('z').\n          ['not', ['pcre', r'^\\..*', 'wholename']],  # Exclude files in hidden dirs (.pants.d etc).\n          ['not', ['match', '*.pyc']]  # Exclude .pyc files.\n          # TODO(kwlzn): Make exclusions here optionable.\n          # Related: https://github.com/pantsbuild/pants/issues/2956\n        ]\n      ),\n      callback\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nregisters a handler for a given subscription.", "response": "def register_handler(self, name, metadata, callback):\n    \"\"\"Register subscriptions and their event handlers.\n\n    :param str name:      the subscription name as used by watchman\n    :param dict metadata: a dictionary of metadata to be serialized and passed to the watchman\n                          subscribe command. this should include the match expression as well\n                          as any required callback fields.\n    :param func callback: the callback to execute on each matching filesystem event\n    \"\"\"\n    assert name not in self._handlers, 'duplicate handler name: {}'.format(name)\n    assert (\n      isinstance(metadata, dict) and 'fields' in metadata and 'expression' in metadata\n    ), 'invalid handler metadata!'\n    self._handlers[name] = Watchman.EventHandler(name=name, metadata=metadata, callback=callback)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(self):\n\n    if not (self._watchman and self._watchman.is_alive()):\n      raise PantsService.ServiceError('watchman is not running, bailing!')\n\n    # Enable watchman for the build root.\n    self._watchman.watch_project(self._build_root)\n\n    subscriptions = list(self._handlers.values())\n\n    # Setup subscriptions and begin the main event firing loop.\n    for handler_name, event_data in self._watchman.subscribed(self._build_root, subscriptions):\n      self._state.maybe_pause()\n      if self._state.is_terminating:\n        break\n\n      if event_data:\n        # As we receive events from watchman, trigger the relevant handlers.\n        self.fire_callback(handler_name, event_data)", "response": "Main service entrypoint. Called via Thread.start() via PantsDaemon.run()."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef default_subsystem_for_plugin(plugin_type):\n  if not issubclass(plugin_type, CheckstylePlugin):\n    raise ValueError('Can only create a default plugin subsystem for subclasses of {}, given: {}'\n                     .format(CheckstylePlugin, plugin_type))\n\n  return type(str('{}Subsystem'.format(plugin_type.__name__)),\n              (PluginSubsystemBase,),\n              {\n                str('options_scope'): 'pycheck-{}'.format(plugin_type.name()),\n                str('plugin_type'): classmethod(lambda cls: plugin_type),\n                str('register_plugin_options'): classmethod(lambda cls, register: None),\n              })", "response": "Create a default plugin subsystem for a given plugin type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_expression(val, acceptable_types, name=None, raise_type=ValueError):\n  def format_type(typ):\n    return typ.__name__\n\n  if not isinstance(val, string_types):\n    raise raise_type('The raw `val` is not a string.  Given {} of type {}.'\n                     .format(val, format_type(type(val))))\n\n  def get_name():\n    return repr(name) if name else 'value'\n\n  def format_raw_value():\n    lines = val.splitlines()\n    for line_number in range(0, len(lines)):\n      lines[line_number] = \"{line_number:{width}}: {line}\".format(\n        line_number=line_number + 1,\n        line=lines[line_number],\n        width=len(str(len(lines))))\n    return '\\n'.join(lines)\n\n  try:\n    parsed_value = eval(val)\n  except Exception as e:\n    raise raise_type(dedent(\"\"\"\\\n      The {name} cannot be evaluated as a literal expression: {error}\n      Given raw value:\n      {value}\n      \"\"\".format(name=get_name(),\n                 error=e,\n                 value=format_raw_value())))\n\n  if not isinstance(parsed_value, acceptable_types):\n    def iter_types(types):\n      if isinstance(types, type):\n        yield types\n      elif isinstance(types, tuple):\n        for item in types:\n          for typ in iter_types(item):\n            yield typ\n      else:\n        raise ValueError('The given acceptable_types is not a valid type (tuple): {}'\n                         .format(acceptable_types))\n\n    raise raise_type(dedent(\"\"\"\\\n      The {name} is not of the expected type(s): {types}:\n      Given the following raw value that evaluated to type {type}:\n      {value}\n      \"\"\".format(name=get_name(),\n                 types=', '.join(format_type(t) for t in iter_types(acceptable_types)),\n                 type=format_type(type(parsed_value)),\n                 value=format_raw_value())))\n  return parsed_value", "response": "Attempts to parse the given python expression of the specified acceptable_types."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the read cache for this setup creating it if necessary.", "response": "def get_read_cache(self):\n    \"\"\"Returns the read cache for this setup, creating it if necessary.\n\n    Returns None if no read cache is configured.\n    \"\"\"\n    if self._options.read_from and not self._read_cache:\n      cache_spec = self._resolve(self._sanitize_cache_spec(self._options.read_from))\n      if cache_spec:\n        with self._cache_setup_lock:\n          self._read_cache = self._do_create_artifact_cache(cache_spec, 'will read from')\n    return self._read_cache"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the write cache for this setup creating it if necessary.", "response": "def get_write_cache(self):\n    \"\"\"Returns the write cache for this setup, creating it if necessary.\n\n    Returns None if no write cache is configured.\n    \"\"\"\n    if self._options.write_to and not self._write_cache:\n      cache_spec = self._resolve(self._sanitize_cache_spec(self._options.write_to))\n      if cache_spec:\n        with self._cache_setup_lock:\n          self._write_cache = self._do_create_artifact_cache(cache_spec, 'will write to')\n    return self._write_cache"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _resolve(self, spec):\n    if not spec.remote:\n      return spec\n\n    try:\n      resolved_urls = self._resolver.resolve(spec.remote)\n      if resolved_urls:\n        # keep the bar separated list of URLs convention\n        return CacheSpec(local=spec.local, remote='|'.join(resolved_urls))\n      # no-op\n      return spec\n    except Resolver.ResolverError as e:\n      self._log.warn('Error while resolving from {0}: {1}'.format(spec.remote, str(e)))\n      # If for some reason resolver fails we continue to use local cache\n      if spec.local:\n        return CacheSpec(local=spec.local, remote=None)\n      # resolver fails but there is no local cache\n      return None", "response": "Attempt resolving cache URIs when a remote spec is provided."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_available_urls(self, urls):\n    baseurl_to_urls = {self._baseurl(url): url for url in urls}\n    pingtimes = self._pinger.pings(list(baseurl_to_urls.keys()))  # List of pairs (host, time in ms).\n    self._log.debug('Artifact cache server ping times: {}'\n                    .format(', '.join(['{}: {:.6f} secs'.format(*p) for p in pingtimes])))\n\n    sorted_pingtimes = sorted(pingtimes, key=lambda x: x[1])\n    available_urls = [baseurl_to_urls[baseurl] for baseurl, pingtime in sorted_pingtimes\n                      if pingtime < Pinger.UNREACHABLE]\n    self._log.debug('Available cache servers: {0}'.format(available_urls))\n\n    return available_urls", "response": "Return reachable urls sorted by their ping times."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate an artifact cache for the specified spec.", "response": "def _do_create_artifact_cache(self, spec, action):\n    \"\"\"Returns an artifact cache for the specified spec.\n\n    spec can be:\n      - a path to a file-based cache root.\n      - a URL of a RESTful cache root.\n      - a bar-separated list of URLs, where we'll pick the one with the best ping times.\n      - A list or tuple of two specs, local, then remote, each as described above\n    \"\"\"\n    compression = self._options.compression_level\n    if compression not in range(1, 10):\n      raise ValueError('compression_level must be an integer 1-9: {}'.format(compression))\n\n    artifact_root = self._options.pants_workdir\n\n    def create_local_cache(parent_path):\n      path = os.path.join(parent_path, self._cache_dirname)\n      self._log.debug('{0} {1} local artifact cache at {2}'\n                      .format(self._task.stable_name(), action, path))\n      return LocalArtifactCache(artifact_root, path, compression,\n                                self._options.max_entries_per_target,\n                                permissions=self._options.write_permissions,\n                                dereference=self._options.dereference_symlinks)\n\n    def create_remote_cache(remote_spec, local_cache):\n      urls = self.get_available_urls(remote_spec.split('|'))\n\n      if len(urls) > 0:\n        best_url_selector = BestUrlSelector(\n          ['{}/{}'.format(url.rstrip('/'), self._cache_dirname) for url in urls]\n        )\n        local_cache = local_cache or TempLocalArtifactCache(artifact_root, compression)\n        return RESTfulArtifactCache(\n          artifact_root,\n          best_url_selector,\n          local_cache,\n          read_timeout=self._options.read_timeout,\n          write_timeout=self._options.write_timeout,\n        )\n\n    local_cache = create_local_cache(spec.local) if spec.local else None\n    remote_cache = create_remote_cache(spec.remote, local_cache) if spec.remote else None\n    if remote_cache:\n      return remote_cache\n    return local_cache"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nresolving any configured plugins and adds them to the global set.", "response": "def resolve(self, working_set=None):\n    \"\"\"Resolves any configured plugins and adds them to the global working set.\n\n    :param working_set: The working set to add the resolved plugins to instead of the global\n                        working set (for testing).\n    :type: :class:`pkg_resources.WorkingSet`\n    \"\"\"\n    working_set = working_set or global_working_set\n    if self._plugin_requirements:\n      for plugin_location in self._resolve_plugin_locations():\n        if self._is_wheel(plugin_location):\n          plugin_location = self._activate_wheel(plugin_location)\n        working_set.add_entry(plugin_location)\n    return working_set"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_deprecation_semver(version_string, version_description):\n  if version_string is None:\n    raise MissingSemanticVersionError('The {} must be provided.'.format(version_description))\n  if not isinstance(version_string, six.string_types):\n    raise BadSemanticVersionError('The {} must be a version string.'.format(version_description))\n  try:\n    # NB: packaging will see versions like 1.a.0 as 1a0, and are \"valid\"\n    # We explicitly want our versions to be of the form x.y.z.\n    v = Version(version_string)\n    if len(v.base_version.split('.')) != 3:\n      raise BadSemanticVersionError('The given {} is not a valid version: '\n                                   '{}'.format(version_description, version_string))\n    if not v.is_prerelease:\n      raise NonDevSemanticVersionError('The given {} is not a dev version: {}\\n'\n                                      'Features should generally be removed in the first `dev` release '\n                                      'of a release cycle.'.format(version_description, version_string))\n    return v\n  except InvalidVersion as e:\n    raise BadSemanticVersionError('The given {} {} is not a valid version: '\n                                 '{}'.format(version_description, version_string, e))", "response": "Validates that version_string is a valid semver."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a Traceback for the given stacklevel.", "response": "def _get_frame_info(stacklevel, context=1):\n  \"\"\"Get a Traceback for the given `stacklevel`.\n\n  For example:\n  `stacklevel=0` means this function's frame (_get_frame_info()).\n  `stacklevel=1` means the calling function's frame.\n  See https://docs.python.org/2/library/inspect.html#inspect.getouterframes for more info.\n\n  NB: If `stacklevel` is greater than the number of actual frames, the outermost frame is used\n  instead.\n  \"\"\"\n  frame_list = inspect.getouterframes(inspect.currentframe(), context=context)\n  frame_stack_index = stacklevel if stacklevel < len(frame_list) else len(frame_list) - 1\n  return frame_list[frame_stack_index]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _greater_warnings_context(context_lines_string):\n  prev_showwarning = warnings.showwarning\n  def wrapped(message, category, filename, lineno, file=None, line=None):\n    return prev_showwarning(\n      message=message,\n      category=category,\n      filename=filename,\n      lineno=lineno,\n      file=file,\n      line=(line or context_lines_string))\n  warnings.showwarning = wrapped\n  yield\n  warnings.showwarning = prev_showwarning", "response": "A contextmanager that provides a more context than the number of context lines."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef warn_or_error(removal_version, deprecated_entity_description, hint=None,\n                  deprecation_start_version=None,\n                  stacklevel=3, frame_info=None, context=1, ensure_stderr=False):\n  \"\"\"Check the removal_version against the current pants version.\n\n  Issues a warning if the removal version is > current pants version, or an error otherwise.\n\n  :param string removal_version: The pantsbuild.pants version at which the deprecated entity\n                                 will be/was removed.\n  :param string deprecated_entity_description: A short description of the deprecated entity, that\n                                            we can embed in warning/error messages.\n  :param string hint: A message describing how to migrate from the removed entity.\n  :param string deprecation_start_version: The pantsbuild.pants version at which the entity will\n                                           begin to display a deprecation warning. This must be less\n                                           than the `removal_version`. If not provided, the\n                                           deprecation warning is always displayed.\n  :param int stacklevel: The stacklevel to pass to warnings.warn.\n  :param FrameInfo frame_info: If provided, use this frame info instead of getting one from\n                               `stacklevel`.\n  :param int context: The number of lines of source code surrounding the selected frame to display\n                      in a warning message.\n  :param bool ensure_stderr: Whether use warnings.warn, or use warnings.showwarning to print\n                             directly to stderr.\n  :raises DeprecationApplicationError: if the removal_version parameter is invalid.\n  :raises CodeRemovedError: if the current version is later than the version marked for removal.\n  \"\"\"\n  removal_semver = validate_deprecation_semver(removal_version, 'removal version')\n  if deprecation_start_version:\n    deprecation_start_semver = validate_deprecation_semver(\n      deprecation_start_version, 'deprecation start version')\n    if deprecation_start_semver >= removal_semver:\n      raise InvalidSemanticVersionOrderingError(\n        'The deprecation start version {} must be less than the end version {}.'\n        .format(deprecation_start_version, removal_version))\n    elif PANTS_SEMVER < deprecation_start_semver:\n      return\n\n  msg = 'DEPRECATED: {} {} removed in version {}.'.format(deprecated_entity_description,\n      get_deprecated_tense(removal_version), removal_version)\n  if hint:\n    msg += '\\n  {}'.format(hint)\n\n  # We need to have filename and line_number for warnings.formatwarning, which appears to be the only\n  # way to get a warning message to display to stderr. We get that from frame_info -- it's too bad\n  # we have to reconstruct the `stacklevel` logic ourselves, but we do also gain the ability to have\n  # multiple lines of context, which is neat.\n  if frame_info is None:\n    frame_info = _get_frame_info(stacklevel, context=context)\n  _, filename, line_number, _, code_context, _ = frame_info\n  if code_context:\n    context_lines = ''.join(code_context)\n  else:\n    context_lines = '<no code context available>'\n\n  if removal_semver > PANTS_SEMVER:\n    if ensure_stderr:\n      # No warning filters can stop us from printing this message directly to stderr.\n      warning_msg = warnings.formatwarning(\n        msg, DeprecationWarning, filename, line_number, line=context_lines)\n      print(warning_msg, file=sys.stderr)\n    else:\n      # This output is filtered by warning filters.\n      with _greater_warnings_context(context_lines):\n        warnings.warn_explicit(\n          message=DeprecationWarning(msg) if PY2 else msg,\n          category=DeprecationWarning,\n          filename=filename,\n          lineno=line_number)\n    return msg\n  else:\n    raise CodeRemovedError(msg)", "response": "Issue a warning or error if the removal_version is less than the current version."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef deprecated_conditional(predicate,\n                           removal_version,\n                           entity_description,\n                           hint_message=None,\n                           stacklevel=4):\n  \"\"\"Marks a certain configuration as deprecated.\n\n  The predicate is used to determine if that configuration is deprecated. It is a function that\n  will be called, if true, then the deprecation warning will issue.\n\n  :param () -> bool predicate: A function that returns True if the deprecation warning should be on.\n  :param string removal_version: The pants version which will remove the deprecated functionality.\n  :param string entity_description: A description of the deprecated entity.\n  :param string hint_message: An optional hint pointing to alternatives to the deprecation.\n  :param int stacklevel: How far up in the stack do we go to find the calling fn to report\n  :raises DeprecationApplicationError if the deprecation is applied improperly.\n  \"\"\"\n  validate_deprecation_semver(removal_version, 'removal version')\n  if predicate():\n    warn_or_error(removal_version, entity_description, hint_message, stacklevel=stacklevel)", "response": "Mark a certain configuration as deprecated."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmark a function or method as deprecated.", "response": "def deprecated(removal_version, hint_message=None, subject=None, ensure_stderr=False):\n  \"\"\"Marks a function or method as deprecated.\n\n  A removal version must be supplied and it must be greater than the current 'pantsbuild.pants'\n  version.\n\n  When choosing a removal version there is a natural tension between the code-base, which benefits\n  from short deprecation cycles, and the user-base which may prefer to deal with deprecations less\n  frequently.  As a rule of thumb, if the hint message can fully convey corrective action\n  succinctly and you judge the impact to be on the small side (effects custom tasks as opposed to\n  effecting BUILD files), lean towards the next release version as the removal version; otherwise,\n  consider initiating a discussion to win consensus on a reasonable removal version.\n\n  :param str removal_version: The pantsbuild.pants version which will remove the deprecated\n                              function.\n  :param str hint_message: An optional hint pointing to alternatives to the deprecation.\n  :param str subject: The name of the subject that has been deprecated for logging clarity. Defaults\n                      to the name of the decorated function/method.\n  :param bool ensure_stderr: Forwarded to `ensure_stderr` in warn_or_error().\n  :raises DeprecationApplicationError if the @deprecation is applied improperly.\n  \"\"\"\n  validate_deprecation_semver(removal_version, 'removal version')\n  def decorator(func):\n    if not inspect.isfunction(func):\n      raise BadDecoratorNestingError('The @deprecated decorator must be applied innermost of all '\n                                     'decorators.')\n\n    func_full_name = '{}.{}'.format(func.__module__, func.__name__)\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n      warn_or_error(removal_version, subject or func_full_name, hint_message,\n                    ensure_stderr=ensure_stderr)\n      return func(*args, **kwargs)\n    return wrapper\n  return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute_default(kwargs):\n    ranked_default = kwargs.get('default')\n    typ = kwargs.get('type', str)\n\n    default = ranked_default.value if ranked_default else None\n    if default is None:\n      return 'None'\n\n    if typ == list:\n      default_str = '[{}]'.format(','.join([\"'{}'\".format(s) for s in default]))\n    elif typ == dict:\n      if default:\n        default_str = '{{ {} }}'.format(\n          ','.join([\"'{}':'{}'\".format(k, v) for k, v in default.items()]))\n      else:\n        default_str = '{}'\n    elif typ == str:\n      default_str = \"'{}'\".format(default).replace('\\n', ' ')\n    else:\n      default_str = str(default)\n    return default_str", "response": "Compute the default value to display in help for an option registered with these kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the metavar to display in help for an option registered with these kwargs.", "response": "def compute_metavar(kwargs):\n    \"\"\"Compute the metavar to display in help for an option registered with these kwargs.\"\"\"\n    metavar = kwargs.get('metavar')\n    if not metavar:\n      typ = kwargs.get('type', str)\n      if typ == list:\n        typ = kwargs.get('member_type', str)\n\n      if typ == dict:\n        metavar = '\"{\\'key1\\':val1,\\'key2\\':val2,...}\"'\n      else:\n        type_name = typ.__name__ if typ != newstr else 'str'  # TODO(#6071): drop special case once Py2 removed\n        metavar = '<{}>'.format(type_name)\n\n    return metavar"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_option_scope_help_info(self, option_registrations_iter):\n    basic_options = []\n    recursive_options = []\n    advanced_options = []\n    # Sort the arguments, so we display the help in alphabetical order.\n    for args, kwargs in sorted(option_registrations_iter):\n      ohi = self.get_option_help_info(args, kwargs)\n      if kwargs.get('advanced'):\n        advanced_options.append(ohi)\n      elif kwargs.get('recursive') and not kwargs.get('recursive_root'):\n        recursive_options.append(ohi)\n      else:\n        basic_options.append(ohi)\n\n    return OptionScopeHelpInfo(scope=self._scope,\n                               basic=basic_options,\n                               recursive=recursive_options,\n                               advanced=advanced_options)", "response": "Returns an OptionScopeHelpInfo for the options registered with the arguments and kwargs pairs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_option_help_info(self, args, kwargs):\n    display_args = []\n    scoped_cmd_line_args = []\n    unscoped_cmd_line_args = []\n\n    for arg in args:\n      is_short_arg = len(arg) == 2\n      unscoped_cmd_line_args.append(arg)\n      if self._scope_prefix:\n        scoped_arg = '--{}-{}'.format(self._scope_prefix, arg.lstrip('-'))\n      else:\n        scoped_arg = arg\n      scoped_cmd_line_args.append(scoped_arg)\n\n      if kwargs.get('type') == bool:\n        if is_short_arg:\n          display_args.append(scoped_arg)\n        else:\n          unscoped_cmd_line_args.append('--no-{}'.format(arg[2:]))\n          scoped_cmd_line_args.append('--no-{}'.format(scoped_arg[2:]))\n          display_args.append('--[no-]{}'.format(scoped_arg[2:]))\n      else:\n        metavar = self.compute_metavar(kwargs)\n        display_arg = '{}={}'.format(scoped_arg, metavar)\n        if is_list_option(kwargs):\n          # Show the multi-arg append form.\n          display_args.append('{arg_str} ({arg_str}) ...'.format(arg_str=display_arg))\n          # Also show the list literal form, both with and without the append operator.\n          if metavar.startswith('\"') and metavar.endswith('\"'):\n            # We quote the entire list literal, so we shouldn't quote the individual members.\n            metavar = metavar[1:-1]\n          display_args.append('{arg}=\"[{metavar}, {metavar}, ...]\"'.format(arg=scoped_arg,\n                                                                           metavar=metavar))\n          display_args.append('{arg}=\"+[{metavar}, {metavar}, ...]\"'.format(arg=scoped_arg,\n                                                                            metavar=metavar))\n        else:\n          display_args.append(display_arg)\n\n    typ = kwargs.get('type', str)\n    default = self.compute_default(kwargs)\n    help_msg = kwargs.get('help', 'No help available.')\n    removal_version = kwargs.get('removal_version')\n    deprecated_message = None\n    if removal_version:\n      deprecated_tense = deprecated.get_deprecated_tense(removal_version)\n      deprecated_message = 'DEPRECATED. {} removed in version: {}'.format(deprecated_tense,\n                                                                          removal_version)\n    removal_hint = kwargs.get('removal_hint')\n    choices = ', '.join(str(choice) for choice in kwargs.get('choices', [])) or None\n\n    ret = OptionHelpInfo(registering_class=kwargs.get('registering_class', type(None)),\n                         display_args=display_args,\n                         scoped_cmd_line_args=scoped_cmd_line_args,\n                         unscoped_cmd_line_args=unscoped_cmd_line_args,\n                         typ=typ,\n                         default=default,\n                         help=help_msg,\n                         deprecated_message=deprecated_message,\n                         removal_version=removal_version,\n                         removal_hint=removal_hint,\n                         choices=choices)\n    return ret", "response": "Returns an OptionHelpInfo for the option registered with the given args and kwargs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef register_bootstrap_options(cls, register):\n    buildroot = get_buildroot()\n    default_distdir_name = 'dist'\n    default_distdir = os.path.join(buildroot, default_distdir_name)\n    default_rel_distdir = '/{}/'.format(default_distdir_name)\n\n    register('-l', '--level', choices=['trace', 'debug', 'info', 'warn'], default='info',\n             recursive=True, help='Set the logging level.')\n    register('-q', '--quiet', type=bool, recursive=True, daemon=False,\n             help='Squelches most console output. NOTE: Some tasks default to behaving quietly: '\n                  'inverting this option supports making them noisier than they would be otherwise.')\n    register('--log-show-rust-3rdparty', type=bool, default=False, advanced=True,\n             help='Whether to show/hide logging done by 3rdparty rust crates used by the pants '\n                  'engine.')\n\n    # Not really needed in bootstrap options, but putting it here means it displays right\n    # after -l and -q in help output, which is conveniently contextual.\n    register('--colors', type=bool, default=sys.stdout.isatty(), recursive=True, daemon=False,\n             help='Set whether log messages are displayed in color.')\n    # TODO(#7203): make a regexp option type!\n    register('--ignore-pants-warnings', type=list, member_type=str, default=[],\n             help='Regexps matching warning strings to ignore, e.g. '\n                  '[\"DEPRECATED: scope some_scope will be removed\"]. The regexps will be matched '\n                  'from the start of the warning string, and will always be case-insensitive. '\n                  'See the `warnings` module documentation for more background on these are used.')\n\n    register('--pants-version', advanced=True, default=pants_version(),\n             help='Use this pants version. Note Pants code only uses this to verify that you are '\n                  'using the requested version, as Pants cannot dynamically change the version it '\n                  'is using once the program is already running. This option is useful to set in '\n                  'your pants.ini, however, and then you can grep the value to select which '\n                  'version to use for setup scripts (e.g. `./pants`), runner scripts, IDE plugins, '\n                  'etc. For example, the setup script we distribute at https://www.pantsbuild.org/install.html#recommended-installation '\n                  'uses this value to determine which Python version to run with. You may find the '\n                  'version of the pants instance you are running using -v, -V, or --version.')\n\n    register('--pants-runtime-python-version', advanced=True,\n             removal_version='1.19.0.dev0',\n             deprecation_start_version='1.17.0.dev0',\n             removal_hint=dedent(\"\"\"\n                  This option was only used to help with Pants' migration to run on Python 3. \\\n                  Pants will now correctly default to whichever Python versions are supported for \\\n                  the current `pants_version` you are using. Please make sure you are using the \\\n                  most up-to-date version of the `./pants` script with:\n\n                    curl -L -O https://pantsbuild.github.io/setup/pants\n\n                  and then unset this option.\"\"\"),\n             help='Use this Python version to run Pants. The option expects the major and minor '\n                  'version, e.g. 2.7 or 3.6. Note Pants code only uses this to verify that you are '\n                  'using the requested interpreter, as Pants cannot dynamically change the '\n                  'interpreter it is using once the program is already running. This option is '\n                  'useful to set in your pants.ini, however, and then you can grep the value to '\n                  'select which interpreter to use for setup scripts (e.g. `./pants`), runner '\n                  'scripts, IDE plugins, etc. For example, the setup script we distribute at '\n                  'https://www.pantsbuild.org/install.html#recommended-installation uses this '\n                  'value to determine which Python version to run with. Also note this does not mean '\n                  'your own code must use this Python version. See '\n                  'https://www.pantsbuild.org/python_readme.html#configure-the-python-version '\n                  'for how to configure your code\\'s compatibility.')\n\n    register('--plugins', advanced=True, type=list, help='Load these plugins.')\n    register('--plugin-cache-dir', advanced=True,\n             default=os.path.join(get_pants_cachedir(), 'plugins'),\n             help='Cache resolved plugin requirements here.')\n\n    register('--backend-packages', advanced=True, type=list,\n             default=['pants.backend.graph_info',\n                      'pants.backend.python',\n                      'pants.backend.jvm',\n                      'pants.backend.native',\n                      # TODO: Move into the graph_info backend.\n                      'pants.rules.core',\n                      'pants.backend.codegen.antlr.java',\n                      'pants.backend.codegen.antlr.python',\n                      'pants.backend.codegen.jaxb',\n                      'pants.backend.codegen.protobuf.java',\n                      'pants.backend.codegen.ragel.java',\n                      'pants.backend.codegen.thrift.java',\n                      'pants.backend.codegen.thrift.python',\n                      'pants.backend.codegen.grpcio.python',\n                      'pants.backend.codegen.wire.java',\n                      'pants.backend.project_info'],\n             help='Load backends from these packages that are already on the path. '\n                  'Add contrib and custom backends to this list.')\n\n    register('--pants-bootstrapdir', advanced=True, metavar='<dir>', default=get_pants_cachedir(),\n             help='Use this dir for global cache.')\n    register('--pants-configdir', advanced=True, metavar='<dir>', default=get_pants_configdir(),\n             help='Use this dir for global config files.')\n    register('--pants-workdir', advanced=True, metavar='<dir>',\n             default=os.path.join(buildroot, '.pants.d'),\n             help='Write intermediate output files to this dir.')\n    register('--pants-supportdir', advanced=True, metavar='<dir>',\n             default=os.path.join(buildroot, 'build-support'),\n             help='Use support files from this dir.')\n    register('--pants-distdir', advanced=True, metavar='<dir>',\n             default=default_distdir,\n             help='Write end-product artifacts to this dir. If you modify this path, you '\n                  'should also update --build-ignore and --pants-ignore to include the '\n                  'custom dist dir path as well.')\n    register('--pants-subprocessdir', advanced=True, default=os.path.join(buildroot, '.pids'),\n             help='The directory to use for tracking subprocess metadata, if any. This should '\n                  'live outside of the dir used by `--pants-workdir` to allow for tracking '\n                  'subprocesses that outlive the workdir data (e.g. `./pants server`).')\n    register('--pants-config-files', advanced=True, type=list, daemon=False,\n             default=[get_default_pants_config_file()], help='Paths to Pants config files.')\n    # TODO: Deprecate the --pantsrc/--pantsrc-files options?  This would require being able\n    # to set extra config file locations in an initial bootstrap config file.\n    register('--pantsrc', advanced=True, type=bool, default=True,\n             help='Use pantsrc files.')\n    register('--pantsrc-files', advanced=True, type=list, metavar='<path>', daemon=False,\n             default=['/etc/pantsrc', '~/.pants.rc'],\n             help='Override config with values from these files. '\n                  'Later files override earlier ones.')\n    register('--pythonpath', advanced=True, type=list,\n             help='Add these directories to PYTHONPATH to search for plugins.')\n    register('--target-spec-file', type=list, dest='target_spec_files', daemon=False,\n             help='Read additional specs from this file, one per line')\n    register('--verify-config', type=bool, default=True, daemon=False,\n             advanced=True,\n             help='Verify that all config file values correspond to known options.')\n\n    register('--build-ignore', advanced=True, type=list,\n             default=['.*/', default_rel_distdir, 'bower_components/',\n                      'node_modules/', '*.egg-info/'],\n             help='Paths to ignore when identifying BUILD files. '\n                  'This does not affect any other filesystem operations. '\n                  'Patterns use the gitignore pattern syntax (https://git-scm.com/docs/gitignore).')\n    register('--pants-ignore', advanced=True, type=list,\n             default=['.*/', default_rel_distdir],\n             help='Paths to ignore for all filesystem operations performed by pants '\n                  '(e.g. BUILD file scanning, glob matching, etc). '\n                  'Patterns use the gitignore syntax (https://git-scm.com/docs/gitignore).')\n    register('--glob-expansion-failure', advanced=True,\n             default=GlobMatchErrorBehavior.warn, type=GlobMatchErrorBehavior,\n             help=\"Raise an exception if any targets declaring source files \"\n                  \"fail to match any glob provided in the 'sources' argument.\")\n\n    # TODO(#7203): make a regexp option type!\n    register('--exclude-target-regexp', advanced=True, type=list, default=[], daemon=False,\n             metavar='<regexp>', help='Exclude target roots that match these regexes.')\n    register('--subproject-roots', type=list, advanced=True, default=[],\n             help='Paths that correspond with build roots for any subproject that this '\n                  'project depends on.')\n    register('--owner-of', type=list, member_type=file_option, default=[], daemon=False, metavar='<path>',\n             help='Select the targets that own these files. '\n                  'This is the third target calculation strategy along with the --changed-* '\n                  'options and specifying the targets directly. These three types of target '\n                  'selection are mutually exclusive.')\n\n    # These logging options are registered in the bootstrap phase so that plugins can log during\n    # registration and not so that their values can be interpolated in configs.\n    register('-d', '--logdir', advanced=True, metavar='<dir>',\n             help='Write logs to files under this directory.')\n\n    # This facilitates bootstrap-time configuration of pantsd usage such that we can\n    # determine whether or not to use the Pailgun client to invoke a given pants run\n    # without resorting to heavier options parsing.\n    register('--enable-pantsd', advanced=True, type=bool, default=False,\n             help='Enables use of the pants daemon (and implicitly, the v2 engine). (Beta)')\n\n    # Shutdown pantsd after the current run.\n    # This needs to be accessed at the same time as enable_pantsd,\n    # so we register it at bootstrap time.\n    register('--shutdown-pantsd-after-run', advanced=True, type=bool, default=False,\n      help='Create a new pantsd server, and use it, and shut it down immediately after. '\n           'If pantsd is already running, it will shut it down and spawn a new instance (Beta)')\n\n    # These facilitate configuring the native engine.\n    register('--native-engine-visualize-to', advanced=True, default=None, type=dir_option, daemon=False,\n             help='A directory to write execution and rule graphs to as `dot` files. The contents '\n                  'of the directory will be overwritten if any filenames collide.')\n    register('--print-exception-stacktrace', advanced=True, type=bool,\n             help='Print to console the full exception stack trace if encountered.')\n\n    # BinaryUtil options.\n    register('--binaries-baseurls', type=list, advanced=True,\n             default=['https://binaries.pantsbuild.org'],\n             help='List of URLs from which binary tools are downloaded. URLs are '\n                  'searched in order until the requested path is found.')\n    register('--binaries-fetch-timeout-secs', type=int, default=30, advanced=True, daemon=False,\n             help='Timeout in seconds for URL reads when fetching binary tools from the '\n                  'repos specified by --baseurls.')\n    register('--binaries-path-by-id', type=dict, advanced=True,\n             help=(\"Maps output of uname for a machine to a binary search path: \"\n                   \"(sysname, id) -> (os, arch), e.g. {('darwin', '15'): ('mac', '10.11'), \"\n                   \"('linux', 'arm32'): ('linux', 'arm32')}.\"))\n    register('--allow-external-binary-tool-downloads', type=bool, default=True, advanced=True,\n             help=\"If False, require BinaryTool subclasses to download their contents from urls \"\n                  \"generated from --binaries-baseurls, even if the tool has an external url \"\n                  \"generator. This can be necessary if using Pants in an environment which cannot \"\n                  \"contact the wider Internet.\")\n\n    # Pants Daemon options.\n    register('--pantsd-pailgun-host', advanced=True, default='127.0.0.1',\n             help='The host to bind the pants nailgun server to.')\n    register('--pantsd-pailgun-port', advanced=True, type=int, default=0,\n             help='The port to bind the pants nailgun server to. Defaults to a random port.')\n    # TODO(#7514): Make this default to 1.0 seconds if stdin is a tty!\n    register('--pantsd-pailgun-quit-timeout', advanced=True, type=float, default=5.0,\n             help='The length of time (in seconds) to wait for further output after sending a '\n                  'signal to the remote pantsd-runner process before killing it.')\n    register('--pantsd-log-dir', advanced=True, default=None,\n             help='The directory to log pantsd output to.')\n    register('--pantsd-invalidation-globs', advanced=True, type=list, default=[],\n             help='Filesystem events matching any of these globs will trigger a daemon restart.')\n\n    # Watchman options.\n    register('--watchman-version', advanced=True, default='4.9.0-pants1', help='Watchman version.')\n    register('--watchman-supportdir', advanced=True, default='bin/watchman',\n             help='Find watchman binaries under this dir. Used as part of the path to lookup '\n                  'the binary with --binaries-baseurls and --pants-bootstrapdir.')\n    register('--watchman-startup-timeout', type=float, advanced=True, default=30.0,\n             help='The watchman socket timeout (in seconds) for the initial `watch-project` command. '\n                  'This may need to be set higher for larger repos due to watchman startup cost.')\n    register('--watchman-socket-timeout', type=float, advanced=True, default=0.1,\n             help='The watchman client socket timeout in seconds. Setting this to too high a '\n                  'value can negatively impact the latency of runs forked by pantsd.')\n    register('--watchman-socket-path', type=str, advanced=True, default=None,\n             help='The path to the watchman UNIX socket. This can be overridden if the default '\n                  'absolute path length exceeds the maximum allowed by the OS.')\n\n    # This option changes the parser behavior in a fundamental way (which currently invalidates\n    # all caches), and needs to be parsed out early, so we make it a bootstrap option.\n    register('--build-file-imports', choices=['allow', 'warn', 'error'], default='warn',\n             advanced=True,\n             help='Whether to allow import statements in BUILD files')\n\n    register('--local-store-dir', advanced=True,\n             help=\"Directory to use for engine's local file store.\",\n             # This default is also hard-coded into the engine's rust code in\n             # fs::Store::default_path\n             default=os.path.expanduser('~/.cache/pants/lmdb_store'))\n    register('--remote-store-server', advanced=True, type=list, default=[],\n             help='host:port of grpc server to use as remote execution file store.')\n    register('--remote-store-thread-count', type=int, advanced=True,\n             default=DEFAULT_EXECUTION_OPTIONS.remote_store_thread_count,\n             help='Thread count to use for the pool that interacts with the remote file store.')\n    register('--remote-execution-server', advanced=True,\n             help='host:port of grpc server to use as remote execution scheduler.')\n    register('--remote-store-chunk-bytes', type=int, advanced=True,\n             default=DEFAULT_EXECUTION_OPTIONS.remote_store_chunk_bytes,\n             help='Size in bytes of chunks transferred to/from the remote file store.')\n    register('--remote-store-chunk-upload-timeout-seconds', type=int, advanced=True,\n             default=DEFAULT_EXECUTION_OPTIONS.remote_store_chunk_upload_timeout_seconds,\n             help='Timeout (in seconds) for uploads of individual chunks to the remote file store.')\n    register('--remote-store-rpc-retries', type=int, advanced=True,\n             default=DEFAULT_EXECUTION_OPTIONS.remote_store_rpc_retries,\n             help='Number of times to retry any RPC to the remote store before giving up.')\n    register('--remote-execution-process-cache-namespace', advanced=True,\n             help=\"The cache namespace for remote process execution. \"\n                  \"Bump this to invalidate every artifact's remote execution. \"\n                  \"This is the remote execution equivalent of the legacy cache-key-gen-version \"\n                  \"flag.\")\n    register('--remote-instance-name', advanced=True,\n             help='Name of the remote execution instance to use. Used for routing within '\n                  '--remote-execution-server and --remote-store-server.')\n    register('--remote-ca-certs-path', advanced=True,\n             help='Path to a PEM file containing CA certificates used for verifying secure '\n                  'connections to --remote-execution-server and --remote-store-server. '\n                  'If not specified, TLS will not be used.')\n    register('--remote-oauth-bearer-token-path', advanced=True,\n             help='Path to a file containing an oauth token to use for grpc connections to '\n                  '--remote-execution-server and --remote-store-server. If not specified, no '\n                  'authorization will be performed.')\n\n    # This should eventually deprecate the RunTracker worker count, which is used for legacy cache\n    # lookups via CacheSetup in TaskBase.\n    register('--process-execution-parallelism', type=int, default=multiprocessing.cpu_count(),\n             advanced=True,\n             help='Number of concurrent processes that may be executed either locally and remotely.')\n    register('--process-execution-cleanup-local-dirs', type=bool, default=True, advanced=True,\n             help='Whether or not to cleanup directories used for local process execution '\n                  '(primarily useful for e.g. debugging).')", "response": "Register bootstrap options.\n\n    \"Bootstrap options\" are a small set of options whose values are useful when registering other\n    options. Therefore we must bootstrap them early, before other options are registered, let\n    alone parsed.\n\n    Bootstrap option values can be interpolated into the config file, and can be referenced\n    programatically in registration code, e.g., as register.bootstrap.pants_workdir.\n\n    Note that regular code can also access these options as normal global-scope options. Their\n    status as \"bootstrap options\" is only pertinent during option registration."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef register_options(cls, register):\n    # The bootstrap options need to be registered on the post-bootstrap Options instance, so it\n    # won't choke on them on the command line, and also so we can access their values as regular\n    # global-scope options, for convenience.\n    cls.register_bootstrap_options(register)\n\n    register('-x', '--time', type=bool,\n             help='Output a timing report at the end of the run.')\n    register('-e', '--explain', type=bool,\n             help='Explain the execution of goals.')\n    register('--tag', type=list, metavar='[+-]tag1,tag2,...',\n             help=\"Include only targets with these tags (optional '+' prefix) or without these \"\n                  \"tags ('-' prefix).  Useful with ::, to find subsets of targets \"\n                  \"(e.g., integration tests.)\")\n\n    # Toggles v1/v2 `Task` vs `@rule` pipelines on/off.\n    register('--v1', advanced=True, type=bool, default=True,\n             help='Enables execution of v1 Tasks.')\n    register('--v2', advanced=True, type=bool, default=False,\n             help='Enables execution of v2 @console_rules.')\n    register('--v2-ui', default=False, type=bool, daemon=False,\n             help='Whether to show v2 engine execution progress. '\n                  'This requires the --v2 flag to take effect.')\n\n    loop_flag = '--loop'\n    register(loop_flag, type=bool,\n             help='Run v2 @console_rules continuously as file changes are detected. Requires '\n                  '`--v2`, and is best utilized with `--v2 --no-v1`.')\n    register('--loop-max', type=int, default=2**32, advanced=True,\n             help='The maximum number of times to loop when `{}` is specified.'.format(loop_flag))\n\n    register('-t', '--timeout', advanced=True, type=int, metavar='<seconds>',\n             help='Number of seconds to wait for http connections.')\n    # TODO: After moving to the new options system these abstraction leaks can go away.\n    register('-k', '--kill-nailguns', advanced=True, type=bool,\n             help='Kill nailguns before exiting')\n    register('--fail-fast', advanced=True, type=bool, recursive=True,\n             help='Exit as quickly as possible on error, rather than attempting to continue '\n                  'to process the non-erroneous subset of the input.')\n    register('--cache-key-gen-version', advanced=True, default='200', recursive=True,\n             help='The cache key generation. Bump this to invalidate every artifact for a scope.')\n    register('--workdir-max-build-entries', advanced=True, type=int, default=8,\n             help='Maximum number of previous builds to keep per task target pair in workdir. '\n             'If set, minimum 2 will always be kept to support incremental compilation.')\n    register('--max-subprocess-args', advanced=True, type=int, default=100, recursive=True,\n             help='Used to limit the number of arguments passed to some subprocesses by breaking '\n             'the command up into multiple invocations.')\n    register('--lock', advanced=True, type=bool, default=True,\n             help='Use a global lock to exclude other versions of pants from running during '\n                  'critical operations.')", "response": "Register options not tied to any particular task or subsystem."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvalidating an instance of the options.", "response": "def validate_instance(cls, opts):\n    \"\"\"Validates an instance of global options for cases that are not prohibited via registration.\n\n    For example: mutually exclusive options may be registered by passing a `mutually_exclusive_group`,\n    but when multiple flags must be specified together, it can be necessary to specify post-parse\n    checks.\n\n    Raises pants.option.errors.OptionsError on validation failure.\n    \"\"\"\n    if opts.loop and (not opts.v2 or opts.v1):\n      raise OptionsError('The --loop option only works with @console_rules, and thus requires '\n                         '`--v2 --no-v1` to function as expected.')\n    if opts.loop and not opts.enable_pantsd:\n      raise OptionsError('The --loop option requires `--enable-pantsd`, in order to watch files.')\n\n    if opts.v2_ui and not opts.v2:\n      raise OptionsError('The --v2-ui option requires --v2 to be enabled together.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tool_jar(self, key, scope=None):\n    return self.tool_jar_from_products(self.context.products, key, scope=self._scope(scope))", "response": "Returns the path to the jar for the tool previously registered under key in the given scope."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tool_classpath(self, key, scope=None):\n    return self.tool_classpath_from_products(self.context.products, key, scope=self._scope(scope))", "response": "Get a list of paths to the classpath for the tool previously registered under key in the given scope."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef combined_dict(*dicts):\n  return {k: v for d in dicts for k, v in d.items()}", "response": "Combine one or more dicts into a new unified dict."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef factory_dict(value_factory, *args, **kwargs):\n  class FactoryDict(collections.defaultdict):\n    @staticmethod\n    def __never_called():\n      raise AssertionError('The default factory should never be called since we override '\n                           '__missing__.')\n\n    def __init__(self):\n      super(FactoryDict, self).__init__(self.__never_called, *args, **kwargs)\n\n    def __missing__(self, key):\n      value = value_factory(key)\n      self[key] = value\n      return value\n\n  return FactoryDict()", "response": "A dict factory that creates a lazy copy of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef recursively_update(d, d2):\n  for k, v in d2.items():\n    if k in d:\n      if isinstance(v, dict):\n        recursively_update(d[k], v)\n        continue\n    d[k] = v", "response": "dict. update but which merges child dicts"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef assert_single_element(iterable):\n  it = iter(iterable)\n  first_item = next(it)\n\n  try:\n    next(it)\n  except StopIteration:\n    return first_item\n\n  raise ValueError(\"iterable {!r} has more than one element.\".format(iterable))", "response": "Get the single element of iterable or raise an error."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef checkstyle(self, interpreter, sources):\n    checker = self.checker_pex(interpreter)\n\n    args = [\n      '--root-dir={}'.format(get_buildroot()),\n      '--severity={}'.format(self.get_options().severity),\n    ]\n    if self.get_options().suppress:\n      args.append('--suppress={}'.format(self.get_options().suppress))\n    if self.get_options().strict:\n      args.append('--strict')\n\n    with temporary_file(binary_mode=False) as argfile:\n      for plugin_subsystem in self.plugin_subsystems:\n        options_blob = plugin_subsystem.global_instance().options_blob()\n        if options_blob:\n          argfile.write('--{}-options={}\\n'.format(plugin_subsystem.plugin_type().name(),\n                                                   options_blob))\n      argfile.write('\\n'.join(sources))\n      argfile.close()\n\n      args.append('@{}'.format(argfile.name))\n\n      with self.context.new_workunit(name='pythonstyle',\n                                     labels=[WorkUnitLabel.TOOL, WorkUnitLabel.LINT],\n                                     cmd=' '.join(checker.cmdline(args))) as workunit:\n        return checker.run(args=args,\n                                    stdout=workunit.output('stdout'),\n                                    stderr=workunit.output('stderr'))", "response": "Iterate over sources and run checker on each file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine whether a tuple of compatibility constraints matches constraints imposed by PythonSetup and a user - supplied whitelist.", "response": "def _constraints_are_whitelisted(self, constraint_tuple):\n    \"\"\"\n    Detect whether a tuple of compatibility constraints\n    matches constraints imposed by the merged list of the global\n    constraints from PythonSetup and a user-supplied whitelist.\n    \"\"\"\n    if self._acceptable_interpreter_constraints == []:\n      # The user wants to lint everything.\n      return True\n    return all(version.parse(constraint) in self._acceptable_interpreter_constraints\n           for constraint in constraint_tuple)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun Checkstyle on all found non - synthetic source files.", "response": "def execute(self):\n    \"\"\"\"Run Checkstyle on all found non-synthetic source files.\"\"\"\n    python_tgts = self.context.targets(\n      lambda tgt: isinstance(tgt, (PythonTarget))\n    )\n    if not python_tgts:\n      return 0\n    interpreter_cache = PythonInterpreterCache.global_instance()\n    with self.invalidated(self.get_targets(self._is_checked)) as invalidation_check:\n      failure_count = 0\n      tgts_by_compatibility, _ = interpreter_cache.partition_targets_by_compatibility(\n        [vt.target for vt in invalidation_check.invalid_vts]\n      )\n      for filters, targets in tgts_by_compatibility.items():\n        sources = self.calculate_sources([tgt for tgt in targets])\n        if sources:\n          allowed_interpreters = set(interpreter_cache.setup(filters=filters))\n          if not allowed_interpreters:\n            raise TaskError('No valid interpreters found for targets: {}\\n(filters: {})'\n                            .format(targets, filters))\n          interpreter = min(allowed_interpreters)\n          failure_count += self.checkstyle(interpreter, sources)\n      if failure_count > 0 and self.get_options().fail:\n        raise TaskError('{} Python Style issues found. You may try `./pants fmt <targets>`'\n                        .format(failure_count))\n      return failure_count"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calculate_sources(self, targets):\n    sources = set()\n    for target in targets:\n      sources.update(\n        source for source in target.sources_relative_to_buildroot()\n        if source.endswith(self._PYTHON_SOURCE_EXTENSION)\n      )\n    return sources", "response": "Generate a set of source files from the given targets."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef items_to_report_element(items, item_type):\n  n = len(items)\n  text = pluralize(n, item_type)\n  if n == 0:\n    return text\n  else:\n    detail = '\\n'.join(str(x) for x in items)\n    return text, detail", "response": "Converts an iterable of items to a report element."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a byte source of serialized objects and returns a new object mapping each object to its corresponding symbol table.", "response": "def parse(cls, filepath, filecontent, parser):\n    \"\"\"Parses a source for addressable Serializable objects.\n\n    No matter the parser used, the parsed and mapped addressable objects are all 'thin'; ie: any\n    objects they point to in other namespaces or even in the same namespace but from a seperate\n    source are left as unresolved pointers.\n\n    :param string filepath: The path to the byte source containing serialized objects.\n    :param string filecontent: The content of byte source containing serialized objects to be parsed.\n    :param symbol_table: The symbol table cls to expose a symbol table dict.\n    :type symbol_table: Instance of :class:`pants.engine.parser.SymbolTable`.\n    :param parser: The parser cls to use.\n    :type parser: A :class:`pants.engine.parser.Parser`.\n    \"\"\"\n    try:\n      objects = parser.parse(filepath, filecontent)\n    except Exception as e:\n      raise MappingError('Failed to parse {}:\\n{}'.format(filepath, e))\n    objects_by_name = {}\n    for obj in objects:\n      if not Serializable.is_serializable(obj):\n        raise UnaddressableObjectError('Parsed a non-serializable object: {!r}'.format(obj))\n      attributes = obj._asdict()\n\n      name = attributes.get('name')\n      if not name:\n        raise UnaddressableObjectError('Parsed a non-addressable object: {!r}'.format(obj))\n\n      if name in objects_by_name:\n        raise DuplicateNameError('An object already exists at {!r} with name {!r}: {!r}.  Cannot '\n                                 'map {!r}'.format(filepath, name, objects_by_name[name], obj))\n      objects_by_name[name] = obj\n    return cls(filepath, OrderedDict(sorted(objects_by_name.items())))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate an address family from the given set of address maps.", "response": "def create(cls, spec_path, address_maps):\n    \"\"\"Creates an address family from the given set of address maps.\n\n    :param spec_path: The directory prefix shared by all address_maps.\n    :param address_maps: The family of maps that form this namespace.\n    :type address_maps: :class:`collections.Iterable` of :class:`AddressMap`\n    :returns: a new address family.\n    :rtype: :class:`AddressFamily`\n    :raises: :class:`MappingError` if the given address maps do not form a family.\n    \"\"\"\n    if spec_path == '.':\n      spec_path = ''\n    for address_map in address_maps:\n      if not address_map.path.startswith(spec_path):\n        raise DifferingFamiliesError('Expected AddressMaps to share the same parent directory {}, '\n                                     'but received: {}'\n                                     .format(spec_path, address_map.path))\n\n\n    objects_by_name = {}\n    for address_map in address_maps:\n      current_path = address_map.path\n      for name, obj in address_map.objects_by_name.items():\n        previous = objects_by_name.get(name)\n        if previous:\n          previous_path, _ = previous\n          raise DuplicateNameError('An object with name {name!r} is already defined in '\n                                   '{previous_path!r}, will not overwrite with {obj!r} from '\n                                   '{current_path!r}.'\n                                   .format(name=name,\n                                           previous_path=previous_path,\n                                           obj=obj,\n                                           current_path=current_path))\n        objects_by_name[name] = (current_path, obj)\n    return AddressFamily(namespace=spec_path,\n                         objects_by_name=OrderedDict((name, (path, obj)) for name, (path, obj)\n                                                      in sorted(objects_by_name.items())))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a mapping from BuildFileAddress to thin addressable objects in this namespace.", "response": "def addressables(self):\n    \"\"\"Return a mapping from BuildFileAddress to thin addressable objects in this namespace.\n\n    :rtype: dict from :class:`pants.build_graph.address.BuildFileAddress` to thin addressable\n            objects.\n    \"\"\"\n    return {\n      BuildFileAddress(rel_path=path, target_name=name): obj\n      for name, (path, obj) in self.objects_by_name.items()\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute_fingerprint(self, target):\n    if isinstance(target, UnpackedJars):\n      hasher = sha1()\n      for cache_key in sorted(jar.cache_key() for jar in target.all_imported_jar_deps):\n        hasher.update(cache_key.encode('utf-8'))\n      hasher.update(target.payload.fingerprint().encode('utf-8'))\n      return hasher.hexdigest() if PY3 else hasher.hexdigest().decode('utf-8')\n    return None", "response": "Compute the fingerprint of the target."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_build_file(self, build_file):\n\n    def _format_context_msg(lineno, offset, error_type, message):\n      \"\"\"Show the line of the BUILD file that has the error along with a few line of context\"\"\"\n      build_contents = build_file.source().decode('utf-8')\n      context = \"While parsing {build_file}:\\n\".format(build_file=build_file)\n      curr_lineno = 0\n      for line in build_contents.split('\\n'):\n        line = line.encode('ascii', 'backslashreplace')\n        curr_lineno += 1\n        if curr_lineno == lineno:\n          highlight = '*'\n        else:\n          highlight = ' '\n        if curr_lineno >= lineno - 3:\n          context += \"{highlight}{curr_lineno:4d}: {line}\\n\".format(\n            highlight=highlight, line=line, curr_lineno=curr_lineno)\n          if lineno == curr_lineno:\n            if offset:\n              context += (\"       {caret:>{width}} {error_type}: {message}\\n\\n\"\n                          .format(caret=\"^\", width=int(offset), error_type=error_type,\n                                  message=message))\n            else:\n              context += (\"        {error_type}: {message}\\n\\n\"\n                          .format(error_type=error_type, message=message))\n        if curr_lineno > lineno + 3:\n          break\n      return context\n\n    logger.debug(\"Parsing BUILD file {build_file}.\"\n                 .format(build_file=build_file))\n\n    try:\n      build_file_code = build_file.code()\n    except SyntaxError as e:\n      raise self.ParseError(_format_context_msg(e.lineno, e.offset, e.__class__.__name__, e))\n    except Exception as e:\n      raise self.ParseError(\"{error_type}: {message}\\n while parsing BUILD file {build_file}\"\n                            .format(error_type=e.__class__.__name__,\n                                    message=e, build_file=build_file))\n\n    parse_state = self._build_configuration.initialize_parse_state(build_file)\n    try:\n      with warnings.catch_warnings(record=True) as warns:\n        six.exec_(build_file_code, parse_state.parse_globals)\n        for warn in warns:\n          logger.warning(_format_context_msg(lineno=warn.lineno,\n                                             offset=None,\n                                             error_type=warn.category.__name__,\n                                             message=warn.message))\n    except Exception as e:\n      raise self.ExecuteError(\"{message}\\n while executing BUILD file {build_file}\"\n                              .format(message=e, build_file=build_file))\n\n    name_map = {}\n    for addressable in parse_state.objects:\n      name = addressable.addressed_name\n      logger.debug('Adding {addressable} to the BuildFileParser address map for {build_file} with {name}'\n                   .format(addressable=addressable,\n                           build_file=build_file,\n                           name=name))\n      if name in name_map:\n        raise self.AddressableConflictException(\n          \"File {conflicting_file} defines address '{target_name}' more than once.\"\n          .format(conflicting_file=build_file,\n                  target_name=name))\n      name_map[name] = addressable\n\n    logger.debug(\"{build_file} produced the following Addressables:\"\n                 .format(build_file=build_file))\n    address_map = {}\n    for name, addressable in name_map.items():\n      address_map[BuildFileAddress(build_file=build_file, target_name=name)] = addressable\n      logger.debug(\"  * {name}: {addressable}\"\n                   .format(name=name,\n                           addressable=addressable))\n\n    return address_map", "response": "Parse a BUILD file and return a list of Addressable instances."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates synthetic targets with populated sources from remote_sources targets.", "response": "def process_remote_sources(self):\n    \"\"\"Create synthetic targets with populated sources from remote_sources targets.\"\"\"\n    unpacked_sources = self.context.products.get_data(UnpackedArchives)\n    remote_sources_targets = self.context.targets(predicate=lambda t: isinstance(t, RemoteSources))\n    if not remote_sources_targets:\n      return\n\n    snapshot_specs = []\n    filespecs = []\n    unpack_dirs = []\n    for target in remote_sources_targets:\n      unpacked_archive = unpacked_sources[target.sources_target]\n      sources = unpacked_archive.found_files\n      rel_unpack_dir = unpacked_archive.rel_unpack_dir\n      self.context.log.debug('target: {}, rel_unpack_dir: {}, sources: {}'\n                             .format(target, rel_unpack_dir, sources))\n      sources_in_dir = tuple(os.path.join(rel_unpack_dir, source) for source in sources)\n      snapshot_specs.append(PathGlobsAndRoot(\n        PathGlobs(sources_in_dir),\n        get_buildroot(),\n      ))\n      filespecs.append({'globs': sources_in_dir})\n      unpack_dirs.append(rel_unpack_dir)\n\n    snapshots = self.context._scheduler.capture_snapshots(tuple(snapshot_specs))\n    for target, snapshot, filespec, rel_unpack_dir in \\\n      zip(remote_sources_targets, snapshots, filespecs, unpack_dirs):\n      synthetic_target = self.context.add_new_target(\n        address=Address(os.path.relpath(self.workdir, get_buildroot()), target.id),\n        target_type=target.destination_target_type,\n        dependencies=target.dependencies,\n        sources=EagerFilesetWithSpec(rel_unpack_dir, filespec, snapshot),\n        derived_from=target,\n        **target.destination_target_args\n      )\n      self.context.log.debug('synthetic_target: {}'.format(synthetic_target))\n      for dependent in self.context.build_graph.dependents_of(target.address):\n        self.context.build_graph.inject_dependency(dependent, synthetic_target.address)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef equal_args(*args, **kwargs):\n  key = args\n  if kwargs:\n    key += _kwargs_separator + tuple(sorted(kwargs.items()))\n  return key", "response": "A memoized key factory that compares the equality of the arguments of a stable sort of the parameters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef memoized(func=None, key_factory=equal_args, cache_factory=dict):\n  if func is None:\n    # We're being applied as a decorator factory; ie: the user has supplied args, like so:\n    # >>> @memoized(cache_factory=lru_cache)\n    # ... def expensive_operation(user):\n    # ...   pass\n    # So we return a decorator with the user-supplied args curried in for the python decorator\n    # machinery to use to wrap the upcoming func.\n    #\n    # NB: This is just a tricky way to allow for both `@memoized` and `@memoized(...params...)`\n    # application forms.  Without this trick, ie: using a decorator class or nested decorator\n    # function, the no-params application would have to be `@memoized()`.  It still can, but need\n    # not be and a bare `@memoized` will work as well as a `@memoized()`.\n    return functools.partial(memoized, key_factory=key_factory, cache_factory=cache_factory)\n\n  if not inspect.isfunction(func):\n    raise ValueError('The @memoized decorator must be applied innermost of all decorators.')\n\n  key_func = key_factory or equal_args\n  memoized_results = cache_factory() if cache_factory else {}\n\n  @functools.wraps(func)\n  def memoize(*args, **kwargs):\n    key = key_func(*args, **kwargs)\n    if key in memoized_results:\n      return memoized_results[key]\n    result = func(*args, **kwargs)\n    memoized_results[key] = result\n    return result\n\n  @contextmanager\n  def put(*args, **kwargs):\n    key = key_func(*args, **kwargs)\n    yield functools.partial(memoized_results.__setitem__, key)\n  memoize.put = put\n\n  def forget(*args, **kwargs):\n    key = key_func(*args, **kwargs)\n    if key in memoized_results:\n      del memoized_results[key]\n  memoize.forget = forget\n\n  def clear():\n    memoized_results.clear()\n  memoize.clear = clear\n\n  return memoize", "response": "A decorator that memoizes the results of a function call."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef memoized_property(func=None, key_factory=per_instance, **kwargs):\n  getter = memoized_method(func=func, key_factory=key_factory, **kwargs)\n  return property(fget=getter, fdel=lambda self: getter.forget(self))", "response": "A convenience wrapper for memoizing properties."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef combined_options_fingerprint_for_scope(cls, scope, options,\n                                             build_graph=None, **kwargs):\n    \"\"\"Given options and a scope, compute a combined fingerprint for the scope.\n\n    :param string scope: The scope to fingerprint.\n    :param Options options: The `Options` object to fingerprint.\n    :param BuildGraph build_graph: A `BuildGraph` instance, only needed if fingerprinting\n                                   target options.\n    :param dict **kwargs: Keyword parameters passed on to\n                          `Options#get_fingerprintable_for_scope`.\n    :return: Hexadecimal string representing the fingerprint for all `options`\n             values in `scope`.\n    \"\"\"\n    fingerprinter = cls(build_graph)\n    hasher = sha1()\n    pairs = options.get_fingerprintable_for_scope(scope, **kwargs)\n    for (option_type, option_value) in pairs:\n      fingerprint = fingerprinter.fingerprint(option_type, option_value)\n      if fingerprint is None:\n        # This isn't necessarily a good value to be using here, but it preserves behavior from\n        # before the commit which added it. I suspect that using the empty string would be\n        # reasonable too, but haven't done any archaeology to check.\n        fingerprint = 'None'\n      hasher.update(fingerprint.encode('utf-8'))\n    return hasher.hexdigest()", "response": "Given a list of options and a scope compute a fingerprint for the options."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a hash of the given option_val based on the option_type.", "response": "def fingerprint(self, option_type, option_val):\n    \"\"\"Returns a hash of the given option_val based on the option_type.\n\n    :API: public\n\n    Returns None if option_val is None.\n    \"\"\"\n    if option_val is None:\n      return None\n\n    # Wrapping all other values in a list here allows us to easily handle single-valued and\n    # list-valued options uniformly. For non-list-valued options, this will be a singleton list\n    # (with the exception of dict, which is not modified). This dict exception works because we do\n    # not currently have any \"list of dict\" type, so there is no ambiguity.\n    if not isinstance(option_val, (list, tuple, dict)):\n      option_val = [option_val]\n\n    if option_type == target_option:\n      return self._fingerprint_target_specs(option_val)\n    elif option_type == dir_option:\n      return self._fingerprint_dirs(option_val)\n    elif option_type == file_option:\n      return self._fingerprint_files(option_val)\n    elif option_type == dict_with_files_option:\n      return self._fingerprint_dict_with_files(option_val)\n    else:\n      return self._fingerprint_primitives(option_val)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _fingerprint_target_specs(self, specs):\n    assert self._build_graph is not None, (\n      'cannot fingerprint specs `{}` without a `BuildGraph`'.format(specs)\n    )\n    hasher = sha1()\n    for spec in sorted(specs):\n      for target in sorted(self._build_graph.resolve(spec)):\n        # Not all targets have hashes; in particular, `Dependencies` targets don't.\n        h = target.compute_invalidation_hash()\n        if h:\n          hasher.update(h.encode('utf-8'))\n    return hasher.hexdigest()", "response": "Returns a fingerprint of the targets resolved from given target specs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _assert_in_buildroot(self, filepath):\n    filepath = os.path.normpath(filepath)\n    root = get_buildroot()\n    if not os.path.abspath(filepath) == filepath:\n      # If not absolute, assume relative to the build root.\n      return os.path.join(root, filepath)\n    else:\n      if '..' in os.path.relpath(filepath, root).split(os.path.sep):\n        # The path wasn't in the buildroot. This is an error because it violates the pants being\n        # hermetic.\n        raise ValueError('Received a file_option that was not inside the build root:\\n'\n                         '  file_option: {filepath}\\n'\n                         '  build_root:  {buildroot}\\n'\n                         .format(filepath=filepath, buildroot=root))\n      return filepath", "response": "Raises an error if the given filepath isn t in the buildroot."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _fingerprint_dirs(self, dirpaths, topdown=True, onerror=None, followlinks=False):\n    # Note that we don't sort the dirpaths, as their order may have meaning.\n    filepaths = []\n    for dirpath in dirpaths:\n      dirs = os.walk(dirpath, topdown=topdown, onerror=onerror,\n                     followlinks=followlinks)\n      sorted_dirs = sorted(dirs, key=lambda d: d[0])\n      filepaths.extend([os.path.join(dirpath, filename)\n                   for dirpath, dirnames, filenames in sorted_dirs\n                   for filename in sorted(filenames)])\n    return self._fingerprint_files(filepaths)", "response": "Returns a fingerprint of the given file directories and all their sub contents."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _fingerprint_files(self, filepaths):\n    hasher = sha1()\n    # Note that we don't sort the filepaths, as their order may have meaning.\n    for filepath in filepaths:\n      filepath = self._assert_in_buildroot(filepath)\n      hasher.update(os.path.relpath(filepath, get_buildroot()).encode('utf-8'))\n      with open(filepath, 'rb') as f:\n        hasher.update(f.read())\n    return hasher.hexdigest()", "response": "Returns a fingerprint of the given filepaths and their contents."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _fingerprint_dict_with_files(self, option_val):\n    return stable_option_fingerprint({\n      k: self._expand_possible_file_value(v) for k, v in option_val.items()\n    })", "response": "Returns a fingerprint of the given dictionary containing file paths."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexpands possible file value.", "response": "def _expand_possible_file_value(self, value):\n    \"\"\"If the value is a file, returns its contents. Otherwise return the original value.\"\"\"\n    if value and os.path.isfile(str(value)):\n      with open(value, 'r') as f:\n        return f.read()\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bootstrap_c_source(scheduler_bindings_path, output_dir, module_name=NATIVE_ENGINE_MODULE):\n\n  safe_mkdir(output_dir)\n\n  with temporary_dir() as tempdir:\n    temp_output_prefix = os.path.join(tempdir, module_name)\n    real_output_prefix = os.path.join(output_dir, module_name)\n    temp_c_file = '{}.c'.format(temp_output_prefix)\n    if PY2:\n      temp_c_file = temp_c_file.encode('utf-8')\n    c_file = '{}.c'.format(real_output_prefix)\n    env_script = '{}.cflags'.format(real_output_prefix)\n\n    # Preprocessor directives won't parse in the .cdef calls, so we have to hide them for now.\n    scheduler_bindings_content = read_file(scheduler_bindings_path)\n    scheduler_bindings = _hackily_rewrite_scheduler_bindings(scheduler_bindings_content)\n\n    ffibuilder = cffi.FFI()\n    ffibuilder.cdef(scheduler_bindings)\n    ffibuilder.cdef(_FFISpecification.format_cffi_externs())\n    ffibuilder.set_source(module_name, scheduler_bindings)\n    ffibuilder.emit_c_code(temp_c_file)\n\n    # Work around https://github.com/rust-lang/rust/issues/36342 by renaming initnative_engine to\n    # wrapped_initnative_engine so that the rust code can define the symbol initnative_engine.\n    #\n    # If we dont do this, we end up at the mercy of the implementation details of rust's stripping\n    # and LTO. In the past we have found ways to trick it into not stripping symbols which was handy\n    # (it kept the binary working) but inconvenient (it was relying on unspecified behavior, it meant\n    # our binaries couldn't be stripped which inflated them by 2~3x, and it reduced the amount of LTO\n    # we could use, which led to unmeasured performance hits).\n    #\n    # We additionally remove the ifdefs that apply conditional `init` logic for Py2 vs Py3, in order\n    # to define a module that is loadable by either 2 or 3.\n    # TODO: Because PyPy uses the same `init` function name regardless of the python version, this\n    # trick does not work there: we leave its conditional in place.\n    file_content = read_file(temp_c_file)\n    if CFFI_C_PATCH_BEFORE not in file_content:\n      raise Exception('The patch for the CFFI generated code will not apply cleanly.')\n    file_content = file_content.replace(CFFI_C_PATCH_BEFORE, CFFI_C_PATCH_AFTER)\n\n    # Extract the preprocessor directives we had to hide to get the .cdef call to parse.\n    file_content = _hackily_recreate_includes_for_bindings(file_content)\n\n  _replace_file(c_file, file_content)\n\n  # Write a shell script to be sourced at build time that contains inherited CFLAGS.\n  _replace_file(env_script, get_build_cflags())", "response": "Bootstrap an external CFFI C source file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _replace_file(path, content):\n  if os.path.exists(path):\n    with open(path, 'r') as f:\n      if content == f.read():\n        print(\"Not overwriting {} because it is unchanged\".format(path), file=sys.stderr)\n        return\n\n  with open(path, 'w') as f:\n    f.write(content)", "response": "Writes a file if it doesn t already exist with the same content."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _extern_decl(return_type, arg_types):\n  def wrapper(func):\n    signature = _ExternSignature(\n      return_type=str(return_type),\n      method_name=str(func.__name__),\n      arg_types=tuple(arg_types))\n    func.extern_signature = signature\n    return func\n  return wrapper", "response": "A decorator for methods corresponding to extern functions. All types should be strings."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef format_cffi_externs(cls):\n    extern_decls = [\n      f.extern_signature.pretty_print()\n      for _, f in cls._extern_fields.items()\n    ]\n    return (\n      'extern \"Python\" {\\n'\n      + '\\n'.join(extern_decls)\n      + '\\n}\\n')", "response": "Generate stubs for the cffi bindings from the _extern_decl methods."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nregisters the _extern_decl methods with our ffi instance.", "response": "def register_cffi_externs(self):\n    \"\"\"Registers the @_extern_decl methods with our ffi instance.\"\"\"\n    for field_name, _ in self._extern_fields.items():\n      bound_method = getattr(self, field_name)\n      self._ffi.def_extern()(bound_method)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a representation of the object s type.", "response": "def extern_get_type_for(self, context_handle, val):\n    \"\"\"Return a representation of the object's type.\"\"\"\n    c = self._ffi.from_handle(context_handle)\n    obj = self._ffi.from_handle(val[0])\n    type_id = c.to_id(type(obj))\n    return TypeId(type_id)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a representation of the object s identity including a hash and TypeId.", "response": "def extern_identify(self, context_handle, val):\n    \"\"\"Return a representation of the object's identity, including a hash and TypeId.\n\n    `extern_get_type_for()` also returns a TypeId, but doesn't hash the object -- this allows that\n    method to be used on unhashable objects. `extern_identify()` returns a TypeId as well to avoid\n    having to make two separate Python calls when interning a Python object in interning.rs, which\n    requires both the hash and type.\n    \"\"\"\n    c = self._ffi.from_handle(context_handle)\n    obj = self._ffi.from_handle(val[0])\n    return c.identify(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extern_equals(self, context_handle, val1, val2):\n    return self._ffi.from_handle(val1[0]) == self._ffi.from_handle(val2[0])", "response": "Return True if the given Handles are equal."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extern_clone_val(self, context_handle, val):\n    c = self._ffi.from_handle(context_handle)\n    return c.to_value(self._ffi.from_handle(val[0]))", "response": "Clone the given Handle."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndrop the given Handles.", "response": "def extern_drop_handles(self, context_handle, handles_ptr, handles_len):\n    \"\"\"Drop the given Handles.\"\"\"\n    c = self._ffi.from_handle(context_handle)\n    handles = self._ffi.unpack(handles_ptr, handles_len)\n    c.drop_handles(handles)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extern_type_to_str(self, context_handle, type_id):\n    c = self._ffi.from_handle(context_handle)\n    return c.utf8_buf(text_type(c.from_id(type_id.tup_0).__name__))", "response": "Given a TypeId write type. __name__ and return it."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extern_val_to_str(self, context_handle, val):\n    c = self._ffi.from_handle(context_handle)\n    v = c.from_value(val[0])\n    # Consistently use the empty string to indicate None.\n    v_str = '' if v is None else text_type(v)\n    return c.utf8_buf(v_str)", "response": "Given a Handle for obj write str ( obj ) and return it."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive storage and an array of Handles return a new Handle to represent the list.", "response": "def extern_store_tuple(self, context_handle, vals_ptr, vals_len):\n    \"\"\"Given storage and an array of Handles, return a new Handle to represent the list.\"\"\"\n    c = self._ffi.from_handle(context_handle)\n    return c.to_value(tuple(c.from_value(val[0]) for val in self._ffi.unpack(vals_ptr, vals_len)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives storage and an array of Handles return a new Handle to represent the set.", "response": "def extern_store_set(self, context_handle, vals_ptr, vals_len):\n    \"\"\"Given storage and an array of Handles, return a new Handle to represent the set.\"\"\"\n    c = self._ffi.from_handle(context_handle)\n    return c.to_value(OrderedSet(c.from_value(val[0]) for val in self._ffi.unpack(vals_ptr, vals_len)))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving storage and an array of Handles return a new Handle to represent the dict.", "response": "def extern_store_dict(self, context_handle, vals_ptr, vals_len):\n    \"\"\"Given storage and an array of Handles, return a new Handle to represent the dict.\n\n    Array of handles alternates keys and values (i.e. key0, value0, key1, value1, ...).\n\n    It is assumed that an even number of values were passed.\n    \"\"\"\n    c = self._ffi.from_handle(context_handle)\n    tup = tuple(c.from_value(val[0]) for val in self._ffi.unpack(vals_ptr, vals_len))\n    d = dict()\n    for i in range(0, len(tup), 2):\n      d[tup[i]] = tup[i + 1]\n    return c.to_value(d)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extern_store_bytes(self, context_handle, bytes_ptr, bytes_len):\n    c = self._ffi.from_handle(context_handle)\n    return c.to_value(binary_type(self._ffi.buffer(bytes_ptr, bytes_len)))", "response": "Given a context and raw bytes return a new Handle to represent the content."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extern_store_utf8(self, context_handle, utf8_ptr, utf8_len):\n    c = self._ffi.from_handle(context_handle)\n    return c.to_value(self._ffi.string(utf8_ptr, utf8_len).decode('utf-8'))", "response": "Given a context and UTF8 bytes return a new Handle to represent the content."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extern_store_i64(self, context_handle, i64):\n    c = self._ffi.from_handle(context_handle)\n    return c.to_value(i64)", "response": "Given a context and int32_t return a new Handle to represent the int32_t."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a context and double return a new Handle to represent the double.", "response": "def extern_store_f64(self, context_handle, f64):\n    \"\"\"Given a context and double, return a new Handle to represent the double.\"\"\"\n    c = self._ffi.from_handle(context_handle)\n    return c.to_value(f64)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extern_store_bool(self, context_handle, b):\n    c = self._ffi.from_handle(context_handle)\n    return c.to_value(b)", "response": "Given a context and _Bool return a new Handle to represent the _Bool."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a Handle for obj and a field name project the field as a new Handle.", "response": "def extern_project_ignoring_type(self, context_handle, val, field_str_ptr, field_str_len):\n    \"\"\"Given a Handle for `obj`, and a field name, project the field as a new Handle.\"\"\"\n    c = self._ffi.from_handle(context_handle)\n    obj = c.from_value(val[0])\n    field_name = self.to_py_str(field_str_ptr, field_str_len)\n    projected = getattr(obj, field_name)\n\n    return c.to_value(projected)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a Key for obj and a field name project the field as a list of Keys.", "response": "def extern_project_multi(self, context_handle, val, field_str_ptr, field_str_len):\n    \"\"\"Given a Key for `obj`, and a field name, project the field as a list of Keys.\"\"\"\n    c = self._ffi.from_handle(context_handle)\n    obj = c.from_value(val[0])\n    field_name = self.to_py_str(field_str_ptr, field_str_len)\n\n    return c.vals_buf(tuple(c.to_value(p) for p in getattr(obj, field_name)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extern_create_exception(self, context_handle, msg_ptr, msg_len):\n    c = self._ffi.from_handle(context_handle)\n    msg = self.to_py_str(msg_ptr, msg_len)\n    return c.to_value(Exception(msg))", "response": "Given a utf8 message string create an Exception object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extern_generator_send(self, context_handle, func, arg):\n    c = self._ffi.from_handle(context_handle)\n    response = self._ffi.new('PyGeneratorResponse*')\n    try:\n      res = c.from_value(func[0]).send(c.from_value(arg[0]))\n      if isinstance(res, Get):\n        # Get.\n        response.tag = self._lib.Get\n        response.get = (\n            TypeId(c.to_id(res.product)),\n            c.to_value(res.subject),\n            c.identify(res.subject),\n          )\n      elif type(res) in (tuple, list):\n        # GetMulti.\n        response.tag = self._lib.GetMulti\n        response.get_multi = (\n            c.type_ids_buf([TypeId(c.to_id(g.product)) for g in res]),\n            c.vals_buf([c.to_value(g.subject) for g in res]),\n            c.identities_buf([c.identify(g.subject) for g in res]),\n          )\n      else:\n        # Break.\n        response.tag = self._lib.Broke\n        response.broke = (c.to_value(res),)\n    except Exception as e:\n      # Throw.\n      response.tag = self._lib.Throw\n      val = e\n      val._formatted_exc = traceback.format_exc()\n      response.throw = (c.to_value(val),)\n\n    return response[0]", "response": "Given a generator send it the given value and return a response."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives a callable call it.", "response": "def extern_call(self, context_handle, func, args_ptr, args_len):\n    \"\"\"Given a callable, call it.\"\"\"\n    c = self._ffi.from_handle(context_handle)\n    runnable = c.from_value(func[0])\n    args = tuple(c.from_value(arg[0]) for arg in self._ffi.unpack(args_ptr, args_len))\n    return self.call(c, runnable, args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extern_eval(self, context_handle, python_code_str_ptr, python_code_str_len):\n    c = self._ffi.from_handle(context_handle)\n    return self.call(c, eval, [self.to_py_str(python_code_str_ptr, python_code_str_len)])", "response": "Given an evalable string return a Handle for its result."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef raise_or_return(self, pyresult):\n    value = self.from_value(pyresult.handle)\n    self._handles.remove(pyresult.handle)\n    if pyresult.is_throw:\n      raise value\n    else:\n      return value", "response": "Consumes the given PyResult to raise or return the exception or value it represents."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an Ident - shaped tuple for the given object.", "response": "def identify(self, obj):\n    \"\"\"Return an Ident-shaped tuple for the given object.\"\"\"\n    hash_ = hash(obj)\n    type_id = self.to_id(type(obj))\n    return (hash_, TypeId(type_id))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef binary(self):\n    lib_name = '{}.so'.format(NATIVE_ENGINE_MODULE)\n    lib_path = os.path.join(safe_mkdtemp(), lib_name)\n    try:\n      with closing(pkg_resources.resource_stream(__name__, lib_name)) as input_fp:\n        # NB: The header stripping code here must be coordinated with header insertion code in\n        #     build-support/bin/native/bootstrap_code.sh\n        engine_version = input_fp.readline().decode('utf-8').strip()\n        repo_version = input_fp.readline().decode('utf-8').strip()\n        logger.debug('using {} built at {}'.format(engine_version, repo_version))\n        with open(lib_path, 'wb') as output_fp:\n          output_fp.write(input_fp.read())\n    except (IOError, OSError) as e:\n      raise self.BinaryLocationError(\n        \"Error unpacking the native engine binary to path {}: {}\".format(lib_path, e),\n        e)\n    return lib_path", "response": "Load and return the path to the native engine binary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lib(self):\n    lib = self.ffi.dlopen(self.binary)\n    _FFISpecification(self.ffi, lib).register_cffi_externs()\n    return lib", "response": "Load and return the native engine module."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads the native engine as a python module and register CFFI externs.", "response": "def _ffi_module(self):\n    \"\"\"Load the native engine as a python module and register CFFI externs.\"\"\"\n    native_bin_dir = os.path.dirname(self.binary)\n    logger.debug('loading native engine python module from: %s', native_bin_dir)\n    sys.path.insert(0, native_bin_dir)\n    return importlib.import_module(NATIVE_ENGINE_MODULE)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating and return an ExternContext and native Scheduler.", "response": "def new_scheduler(self,\n                    tasks,\n                    root_subject_types,\n                    build_root,\n                    work_dir,\n                    local_store_dir,\n                    ignore_patterns,\n                    execution_options,\n                    construct_directory_digest,\n                    construct_snapshot,\n                    construct_file_content,\n                    construct_files_content,\n                    construct_process_result,\n                    type_address,\n                    type_path_globs,\n                    type_directory_digest,\n                    type_snapshot,\n                    type_merge_snapshots_request,\n                    type_files_content,\n                    type_dir,\n                    type_file,\n                    type_link,\n                    type_process_request,\n                    type_process_result,\n                    type_generator,\n                    type_url_to_fetch):\n    \"\"\"Create and return an ExternContext and native Scheduler.\"\"\"\n\n    def func(fn):\n      return Function(self.context.to_key(fn))\n    def ti(type_obj):\n      return TypeId(self.context.to_id(type_obj))\n\n    scheduler = self.lib.scheduler_create(\n        tasks,\n        # Constructors/functions.\n        func(construct_directory_digest),\n        func(construct_snapshot),\n        func(construct_file_content),\n        func(construct_files_content),\n        func(construct_process_result),\n        # Types.\n        ti(type_address),\n        ti(type_path_globs),\n        ti(type_directory_digest),\n        ti(type_snapshot),\n        ti(type_merge_snapshots_request),\n        ti(type_files_content),\n        ti(type_dir),\n        ti(type_file),\n        ti(type_link),\n        ti(type_process_request),\n        ti(type_process_result),\n        ti(type_generator),\n        ti(type_url_to_fetch),\n        ti(text_type),\n        ti(binary_type),\n        # Project tree.\n        self.context.utf8_buf(build_root),\n        self.context.utf8_buf(work_dir),\n        self.context.utf8_buf(local_store_dir),\n        self.context.utf8_buf_buf(ignore_patterns),\n        self.to_ids_buf(root_subject_types),\n        # Remote execution config.\n        self.context.utf8_buf_buf(execution_options.remote_store_server),\n        # We can't currently pass Options to the rust side, so we pass empty strings for None.\n        self.context.utf8_buf(execution_options.remote_execution_server or \"\"),\n        self.context.utf8_buf(execution_options.remote_execution_process_cache_namespace or \"\"),\n        self.context.utf8_buf(execution_options.remote_instance_name or \"\"),\n        self.context.utf8_buf(execution_options.remote_ca_certs_path or \"\"),\n        self.context.utf8_buf(execution_options.remote_oauth_bearer_token_path or \"\"),\n        execution_options.remote_store_thread_count,\n        execution_options.remote_store_chunk_bytes,\n        execution_options.remote_store_chunk_upload_timeout_seconds,\n        execution_options.remote_store_rpc_retries,\n        execution_options.process_execution_parallelism,\n        execution_options.process_execution_cleanup_local_dirs,\n      )\n    return self.gc(scheduler, self.lib.scheduler_destroy)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a target address spec and returns the path from the root of the repo to this Target and Target name.", "response": "def parse_spec(spec, relative_to=None, subproject_roots=None):\n  \"\"\"Parses a target address spec and returns the path from the root of the repo to this Target\n  and Target name.\n\n  :API: public\n\n  :param string spec: Target address spec.\n  :param string relative_to: path to use for sibling specs, ie: ':another_in_same_build_family',\n    interprets the missing spec_path part as `relative_to`.\n  :param list subproject_roots: Paths that correspond with embedded build roots under\n    the current build root.\n\n  For Example::\n\n    some_target(name='mytarget',\n      dependencies=['path/to/buildfile:targetname']\n    )\n\n  Where ``path/to/buildfile:targetname`` is the dependent target address spec\n\n  In case the target name is empty it returns the last component of the path as target name, ie::\n\n    spec_path, target_name = parse_spec('path/to/buildfile/foo')\n\n  Will return spec_path as 'path/to/buildfile/foo' and target_name as 'foo'.\n\n  Optionally, specs can be prefixed with '//' to denote an absolute spec path.  This is normally\n  not significant except when a spec referring to a root level target is needed from deeper in\n  the tree.  For example, in ``path/to/buildfile/BUILD``::\n\n    some_target(name='mytarget',\n      dependencies=[':targetname']\n    )\n\n  The ``targetname`` spec refers to a target defined in ``path/to/buildfile/BUILD*``.  If instead\n  you want to reference ``targetname`` in a root level BUILD file, use the absolute form.\n  For example::\n\n    some_target(name='mytarget',\n      dependencies=['//:targetname']\n    )\n  \"\"\"\n  def normalize_absolute_refs(ref):\n    return strip_prefix(ref, '//')\n\n  subproject = longest_dir_prefix(relative_to, subproject_roots) if subproject_roots else None\n\n  def prefix_subproject(spec_path):\n    if not subproject:\n      return spec_path\n    elif spec_path:\n      return os.path.join(subproject, spec_path)\n    else:\n      return os.path.normpath(subproject)\n\n  spec_parts = spec.rsplit(':', 1)\n  if len(spec_parts) == 1:\n    default_target_spec = spec_parts[0]\n    spec_path = prefix_subproject(normalize_absolute_refs(default_target_spec))\n    target_name = os.path.basename(spec_path)\n  else:\n    spec_path, target_name = spec_parts\n    if not spec_path and relative_to:\n      spec_path = fast_relpath(relative_to, subproject) if subproject else relative_to\n    spec_path = prefix_subproject(normalize_absolute_refs(spec_path))\n\n  return spec_path, target_name"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing an address from its serialized form.", "response": "def parse(cls, spec, relative_to='', subproject_roots=None):\n    \"\"\"Parses an address from its serialized form.\n\n    :param string spec: An address in string form <path>:<name>.\n    :param string relative_to: For sibling specs, ie: ':another_in_same_build_family', interprets\n                               the missing spec_path part as `relative_to`.\n    :param list subproject_roots: Paths that correspond with embedded build roots\n                                  under the current build root.\n    :returns: A new address.\n    :rtype: :class:`pants.base.address.Address`\n    \"\"\"\n    spec_path, target_name = parse_spec(spec,\n                                        relative_to=relative_to,\n                                        subproject_roots=subproject_roots)\n    return cls(spec_path, target_name)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reference(self, referencing_path=None):\n    if referencing_path is not None and self._spec_path == referencing_path:\n      return self.relative_spec\n    elif os.path.basename(self._spec_path) != self._target_name:\n      return self.spec\n    else:\n      return self._spec_path", "response": "Return the path to the target file that this address points to."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npredicate which determines whether the given target in question is relevant to this codegen task.", "response": "def is_gentarget(self, target):\n    \"\"\"Predicate which determines whether the target in question is relevant to this codegen task.\n\n    E.g., the JaxbGen task considers JaxbLibrary targets to be relevant, and nothing else.\n\n    :API: public\n\n    :param Target target: The target to check.\n    :return: True if this class can generate code for the given target, False otherwise.\n    \"\"\"\n    if self.gentarget_type:\n      return isinstance(target, self.gentarget_type)\n    else:\n      raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _do_validate_sources_present(self, target):\n    if not self.validate_sources_present:\n      return True\n    sources = target.sources_relative_to_buildroot()\n    if not sources:\n      message = ('Target {} has no sources.'.format(target.address.spec))\n      if not self.get_options().allow_empty:\n        raise TaskError(message)\n      else:\n        logging.warn(message)\n        return False\n    return True", "response": "Checks whether sources is empty and raises a TaskError if not."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _inject_synthetic_target(self, vt, sources):\n    target = vt.target\n\n    # NB: For stability, the injected target exposes the stable-symlinked `vt.results_dir`,\n    # rather than the hash-named `vt.current_results_dir`.\n    synthetic_target_dir = self.synthetic_target_dir(target, vt.results_dir)\n    synthetic_target_type = self.synthetic_target_type(target)\n    synthetic_extra_dependencies = self.synthetic_target_extra_dependencies(target, synthetic_target_dir)\n\n    copied_attributes = {}\n    for attribute in self._copy_target_attributes:\n      copied_attributes[attribute] = getattr(target, attribute)\n\n    if self._supports_exports(synthetic_target_type):\n      extra_exports = self.synthetic_target_extra_exports(target, synthetic_target_dir)\n\n      extra_exports_not_in_extra_dependencies = set(extra_exports).difference(\n        set(synthetic_extra_dependencies))\n      if len(extra_exports_not_in_extra_dependencies) > 0:\n        raise self.MismatchedExtraExports(\n          'Extra synthetic exports included targets not in the extra dependencies: {}. Affected target: {}'\n            .format(extra_exports_not_in_extra_dependencies, target))\n\n      extra_export_specs = {e.address.spec for e in extra_exports}\n      original_export_specs = self._original_export_specs(target)\n      union = set(original_export_specs).union(extra_export_specs)\n\n      copied_attributes['exports'] = sorted(union)\n\n    synthetic_target = self.context.add_new_target(\n      address=self._get_synthetic_address(target, synthetic_target_dir),\n      target_type=synthetic_target_type,\n      dependencies=synthetic_extra_dependencies,\n      sources=sources,\n      derived_from=target,\n      **copied_attributes\n    )\n\n    build_graph = self.context.build_graph\n    # NB(pl): This bypasses the convenience function (Target.inject_dependency) in order\n    # to improve performance.  Note that we can walk the transitive dependee subgraph once\n    # for transitive invalidation rather than walking a smaller subgraph for every single\n    # dependency injected.\n    for dependent_address in build_graph.dependents_of(target.address):\n      build_graph.inject_dependency(\n        dependent=dependent_address,\n        dependency=synthetic_target.address,\n      )\n    # NB(pl): See the above comment.  The same note applies.\n    for concrete_dependency_address in build_graph.dependencies_of(target.address):\n      build_graph.inject_dependency(\n        dependent=synthetic_target.address,\n        dependency=concrete_dependency_address,\n      )\n\n    if target in self.context.target_roots:\n      self.context.target_roots.append(synthetic_target)\n\n    return synthetic_target", "response": "Create and inject a synthetic target for the given target and workdir."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef resolve_deps(self, unresolved_deps):\n    deps = OrderedSet()\n    for dep in unresolved_deps:\n      try:\n        deps.update(self.context.resolve(dep))\n      except AddressLookupError as e:\n        raise AddressLookupError('{message}\\n  on dependency {dep}'.format(message=e, dep=dep))\n    return deps", "response": "Resolves the unresolved_deps and returns a set of all the dependencies that are resolved."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling duplicate sources generated by the given gen target by either failure or deletion.", "response": "def _handle_duplicate_sources(self, vt, sources):\n    \"\"\"Handles duplicate sources generated by the given gen target by either failure or deletion.\n\n    This method should be called after all dependencies have been injected into the graph, but\n    before injecting the synthetic version of this target.\n\n    Returns a boolean indicating whether it modified the underlying filesystem.\n\n    NB(gm): Some code generators may re-generate code that their dependent libraries generate.\n    This results in targets claiming to generate sources that they really don't, so we try to\n    filter out sources that were actually generated by dependencies of the target. This causes\n    the code generated by the dependencies to 'win' over the code generated by dependees. By\n    default, this behavior is disabled, and duplication in generated sources will raise a\n    TaskError. This is controlled by the --allow-dups flag.\n    \"\"\"\n    target = vt.target\n    target_workdir = vt.results_dir\n\n    # Walk dependency gentargets and record any sources owned by those targets that are also\n    # owned by this target.\n    duplicates_by_target = OrderedDict()\n    def record_duplicates(dep):\n      if dep == target or not self.is_gentarget(dep.concrete_derived_from):\n        return False\n      duped_sources = [s for s in dep.sources_relative_to_source_root() if s in sources.files and\n                       not self.ignore_dup(target, dep, s)]\n      if duped_sources:\n        duplicates_by_target[dep] = duped_sources\n    target.walk(record_duplicates)\n\n    # If there were no dupes, we're done.\n    if not duplicates_by_target:\n      return False\n\n    # If there were duplicates warn or error.\n    messages = ['{target} generated sources that had already been generated by dependencies.'\n                .format(target=target.address.spec)]\n    for dep, duped_sources in duplicates_by_target.items():\n      messages.append('\\t{} also generated:'.format(dep.concrete_derived_from.address.spec))\n      messages.extend(['\\t\\t{}'.format(source) for source in duped_sources])\n    message = '\\n'.join(messages)\n    if self.get_options().allow_dups:\n      logger.warn(message)\n    else:\n      raise self.DuplicateSourceError(message)\n\n    did_modify = False\n\n    # Finally, remove duplicates from the workdir. This prevents us from having to worry\n    # about them during future incremental compiles.\n    for dep, duped_sources in duplicates_by_target.items():\n      for duped_source in duped_sources:\n        safe_delete(os.path.join(target_workdir, duped_source))\n        did_modify = True\n    if did_modify:\n      Digest.clear(vt.current_results_dir)\n    return did_modify"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_targets_by_declared_platform_with_placeholders(self, targets_by_platform):\n\n    if not targets_by_platform:\n      for platform in self._python_setup.platforms:\n        targets_by_platform[platform] = ['(No target) Platform inherited from either the '\n                                          '--platforms option or a pants.ini file.']\n    return targets_by_platform", "response": "Returns a dict that maps a platform string to a list of targets that have the specified platform."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks whether the current target closure has native sources and if so ensures that Pants is only targeting the current platform.", "response": "def check_build_for_current_platform_only(self, targets):\n    \"\"\"\n    Performs a check of whether the current target closure has native sources and if so, ensures\n    that Pants is only targeting the current platform.\n\n    :param tgts: a list of :class:`Target` objects.\n    :return: a boolean value indicating whether the current target closure has native sources.\n    :raises: :class:`pants.base.exceptions.IncompatiblePlatformsError`\n    \"\"\"\n    if not self._any_targets_have_native_sources(targets):\n      return False\n\n    targets_by_platform = pex_build_util.targets_by_platform(targets, self._python_setup)\n    platforms_with_sources = self._get_targets_by_declared_platform_with_placeholders(targets_by_platform)\n    platform_names = list(platforms_with_sources.keys())\n\n    if len(platform_names) < 1:\n      raise self.PythonNativeCodeError(\n        \"Error: there should be at least one platform in the target closure, because \"\n        \"we checked that there are native sources.\")\n\n    if platform_names == ['current']:\n      return True\n\n    raise IncompatiblePlatformsError(\n      'The target set contains one or more targets that depend on '\n      'native code. Please ensure that the platform arguments in all relevant targets and build '\n      'options are compatible with the current platform. Found targets for platforms: {}'\n      .format(str(platforms_with_sources)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find(self, compile_failure_log, target):\n    not_found_classnames = [err.classname for err in\n                            self.compile_error_extractor.extract(compile_failure_log)]\n    return self._select_target_candidates_for_class(not_found_classnames, target)", "response": "Find missing deps on a best -effort basis from target s transitive dependencies."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nselect a target that contains the given classname.", "response": "def _select_target_candidates_for_class(self, classnames, target):\n    \"\"\"Select a target that contains the given classname.\n\n    When multiple candidates are available, not uncommon in 3rdparty dependencies,\n    they are ranked according to their string similiarities with the classname because\n    the way 3rdparty targets are conventionally named often shares similar naming\n    structure.\n    \"\"\"\n    class2deps, no_dep_found = {}, set()\n    for classname in classnames:\n      if classname not in class2deps:\n        candidates_for_class = []\n        for tgt in self.dep_analyzer.targets_for_class(target, classname):\n          if tgt.is_synthetic and tgt.derived_from:\n            tgt = tgt.derived_from\n          candidates_for_class.append(tgt.address.spec)\n        if candidates_for_class:\n          candidates_for_class = StringSimilarityRanker(classname).sort(candidates_for_class)\n          class2deps[classname] = OrderedSet(candidates_for_class)\n        else:\n          no_dep_found.add(classname)\n    return class2deps, no_dep_found"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a. pex file for the specified binary target.", "response": "def _create_binary(self, binary_tgt, results_dir):\n    \"\"\"Create a .pex file for the specified binary target.\"\"\"\n    # Note that we rebuild a chroot from scratch, instead of using the REQUIREMENTS_PEX\n    # and PYTHON_SOURCES products, because those products are already-built pexes, and there's\n    # no easy way to merge them into a single pex file (for example, they each have a __main__.py,\n    # metadata, and so on, which the merging code would have to handle specially).\n    interpreter = self.context.products.get_data(PythonInterpreter)\n    with temporary_dir() as tmpdir:\n      # Create the pex_info for the binary.\n      run_info_dict = self.context.run_tracker.run_info.get_as_dict()\n      build_properties = PexInfo.make_build_properties()\n      build_properties.update(run_info_dict)\n      pex_info = binary_tgt.pexinfo.copy()\n      pex_info.build_properties = build_properties\n\n      pex_builder = PexBuilderWrapper.Factory.create(\n        builder=PEXBuilder(path=tmpdir, interpreter=interpreter, pex_info=pex_info, copy=True),\n        log=self.context.log)\n\n      if binary_tgt.shebang:\n        self.context.log.info('Found Python binary target {} with customized shebang, using it: {}'\n          .format(binary_tgt.name, binary_tgt.shebang))\n        pex_builder.set_shebang(binary_tgt.shebang)\n      else:\n        self.context.log.debug('No customized shebang found for {}'.format(binary_tgt.name))\n\n      # Find which targets provide sources and which specify requirements.\n      source_tgts = []\n      req_tgts = []\n      constraint_tgts = []\n      for tgt in binary_tgt.closure(exclude_scopes=Scopes.COMPILE):\n        if has_python_sources(tgt) or has_resources(tgt):\n          source_tgts.append(tgt)\n        elif has_python_requirements(tgt):\n          req_tgts.append(tgt)\n        if is_python_target(tgt):\n          constraint_tgts.append(tgt)\n\n      # Add interpreter compatibility constraints to pex info. This will first check the targets for any\n      # constraints, and if they do not have any will resort to the global constraints.\n      pex_builder.add_interpreter_constraints_from(constraint_tgts)\n\n      # Dump everything into the builder's chroot.\n      for tgt in source_tgts:\n        pex_builder.add_sources_from(tgt)\n\n      # We need to ensure that we are resolving for only the current platform if we are\n      # including local python dist targets that have native extensions.\n      self._python_native_code_settings.check_build_for_current_platform_only(self.context.targets())\n      pex_builder.add_requirement_libs_from(req_tgts, platforms=binary_tgt.platforms)\n\n      # Build the .pex file.\n      pex_path = os.path.join(results_dir, '{}.pex'.format(binary_tgt.name))\n      pex_builder.build(pex_path)\n      return pex_path"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking which of the named patterns matches the given content.", "response": "def check_content(self, content_pattern_names, content, encoding):\n    \"\"\"Check which of the named patterns matches the given content.\n\n    Returns a pair (matching, nonmatching), in which each element is a tuple of pattern names.\n\n    :param iterable content_pattern_names: names of content patterns to check.\n    :param bytes content: the content to check.\n    :param str encoding: the expected encoding of content.\n    \"\"\"\n    if not content_pattern_names or not encoding:\n      return (), ()\n\n    matching = []\n    nonmatching = []\n    for content_pattern_name in content_pattern_names:\n      if self._content_matchers[content_pattern_name].matches(content.decode(encoding)):\n        matching.append(content_pattern_name)\n      else:\n        nonmatching.append(content_pattern_name)\n    return tuple(matching), tuple(nonmatching)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_applicable_content_pattern_names(self, path):\n    encodings = set()\n    applicable_content_pattern_names = set()\n    for path_pattern_name, content_pattern_names in self._required_matches.items():\n      m = self._path_matchers[path_pattern_name]\n      if m.matches(path):\n        encodings.add(m.content_encoding)\n        applicable_content_pattern_names.update(content_pattern_names)\n    if len(encodings) > 1:\n      raise ValueError('Path matched patterns with multiple content encodings ({}): {}'.format(\n        ', '.join(sorted(encodings)), path\n      ))\n    content_encoding = next(iter(encodings)) if encodings else None\n    return applicable_content_pattern_names, content_encoding", "response": "Return the content patterns applicable to a given path."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef datatype(field_decls, superclass_name=None, **kwargs):\n  field_names = []\n  fields_with_constraints = OrderedDict()\n  for maybe_decl in field_decls:\n    # ('field_name', type)\n    if isinstance(maybe_decl, tuple):\n      field_name, type_spec = maybe_decl\n      if isinstance(type_spec, type):\n        type_constraint = Exactly(type_spec)\n      elif isinstance(type_spec, TypeConstraint):\n        type_constraint = type_spec\n      else:\n        raise TypeError(\n          \"type spec for field '{}' was not a type or TypeConstraint: was {!r} (type {!r}).\"\n          .format(field_name, type_spec, type(type_spec).__name__))\n      fields_with_constraints[field_name] = type_constraint\n    else:\n      # interpret it as a field name without a type to check\n      field_name = maybe_decl\n    # namedtuple() already checks field uniqueness\n    field_names.append(field_name)\n\n  if not superclass_name:\n    superclass_name = '_anonymous_namedtuple_subclass'\n\n  namedtuple_cls = namedtuple(superclass_name, field_names, **kwargs)\n\n  class DataType(namedtuple_cls, DatatypeMixin):\n    type_check_error_type = TypedDatatypeInstanceConstructionError\n\n    def __new__(cls, *args, **kwargs):\n      # TODO: Ideally we could execute this exactly once per `cls` but it should be a\n      # relatively cheap check.\n      if not hasattr(cls.__eq__, '_eq_override_canary'):\n        raise cls.make_type_error('Should not override __eq__.')\n\n      try:\n        this_object = super(DataType, cls).__new__(cls, *args, **kwargs)\n      except TypeError as e:\n        raise cls.make_type_error(\n          \"error in namedtuple() base constructor: {}\".format(e))\n\n      # TODO: Make this kind of exception pattern (filter for errors then display them all at once)\n      # more ergonomic.\n      type_failure_msgs = []\n      for field_name, field_constraint in fields_with_constraints.items():\n        field_value = getattr(this_object, field_name)\n        try:\n          field_constraint.validate_satisfied_by(field_value)\n        except TypeConstraintError as e:\n          type_failure_msgs.append(\n            \"field '{}' was invalid: {}\".format(field_name, e))\n      if type_failure_msgs:\n        raise cls.make_type_error(\n          'error(s) type checking constructor arguments:\\n{}'\n          .format('\\n'.join(type_failure_msgs)))\n\n      return this_object\n\n    def __eq__(self, other):\n      if self is other:\n        return True\n\n      # Compare types and fields.\n      if type(self) != type(other):\n        return False\n      # Explicitly return super.__eq__'s value in case super returns NotImplemented\n      return super(DataType, self).__eq__(other)\n    # We define an attribute on the `cls` level definition of `__eq__` that will allow us to detect\n    # that it has been overridden.\n    __eq__._eq_override_canary = None\n\n    def __ne__(self, other):\n      return not (self == other)\n\n    # NB: in Python 3, whenever __eq__ is overridden, __hash__() must also be\n    # explicitly implemented, otherwise Python will raise \"unhashable type\". See\n    # https://docs.python.org/3/reference/datamodel.html#object.__hash__.\n    def __hash__(self):\n      return super(DataType, self).__hash__()\n\n    # NB: As datatype is not iterable, we need to override both __iter__ and all of the\n    # namedtuple methods that expect self to be iterable.\n    def __iter__(self):\n      raise self.make_type_error(\"datatype object is not iterable\")\n\n    def _super_iter(self):\n      return super(DataType, self).__iter__()\n\n    def _asdict(self):\n      \"\"\"Return a new OrderedDict which maps field names to their values.\n\n      Overrides a namedtuple() method which calls __iter__.\n      \"\"\"\n      return OrderedDict(zip(self._fields, self._super_iter()))\n\n    def _replace(self, **kwargs):\n      \"\"\"Return a new datatype object replacing specified fields with new values.\n\n      Overrides a namedtuple() method which calls __iter__.\n      \"\"\"\n      field_dict = self._asdict()\n      field_dict.update(**kwargs)\n      return type(self)(**field_dict)\n\n    def copy(self, **kwargs):\n      return self._replace(**kwargs)\n\n    # NB: it is *not* recommended to rely on the ordering of the tuple returned by this method.\n    def __getnewargs__(self):\n      \"\"\"Return self as a plain tuple.  Used by copy and pickle.\"\"\"\n      return tuple(self._super_iter())\n\n    def __repr__(self):\n      args_formatted = []\n      for field_name in field_names:\n        field_value = getattr(self, field_name)\n        args_formatted.append(\"{}={!r}\".format(field_name, field_value))\n      return '{class_name}({args_joined})'.format(\n        class_name=type(self).__name__,\n        args_joined=', '.join(args_formatted))\n\n    def __str__(self):\n      elements_formatted = []\n      for field_name in field_names:\n        constraint_for_field = fields_with_constraints.get(field_name, None)\n        field_value = getattr(self, field_name)\n        if not constraint_for_field:\n          elements_formatted.append(\n            # TODO: consider using the repr of arguments in this method.\n            \"{field_name}={field_value}\"\n            .format(field_name=field_name,\n                    field_value=field_value))\n        else:\n          elements_formatted.append(\n            \"{field_name}<{type_constraint}>={field_value}\"\n            .format(field_name=field_name,\n                    type_constraint=constraint_for_field,\n                    field_value=field_value))\n      return '{class_name}({typed_tagged_elements})'.format(\n        class_name=type(self).__name__,\n        typed_tagged_elements=', '.join(elements_formatted))\n\n  # Return a new type with the given name, inheriting from the DataType class\n  # just defined, with an empty class body.\n  try:  # Python3\n    return type(superclass_name, (DataType,), {})\n  except TypeError:  # Python2\n    return type(superclass_name.encode('utf-8'), (DataType,), {})", "response": "A wrapper for namedtuple that accounts for the type of the object in equality."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_type_error(cls, msg, *args, **kwargs):\n    return cls.type_check_error_type(cls.__name__, msg, *args, **kwargs)", "response": "A helper method to generate an exception type for type checking errors."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn obj if the object satisfies this type constraint or raise.", "response": "def validate_satisfied_by(self, obj):\n    \"\"\"Return `obj` if the object satisfies this type constraint, or raise.\n\n    :raises: `TypeConstraintError` if `obj` does not satisfy the constraint.\n    \"\"\"\n\n    if self.satisfied_by(obj):\n      return obj\n\n    raise self.make_type_constraint_error(obj, self)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreplace from_str with to_str in the name and content of the given file.", "response": "def replace_in_file(workspace, src_file_path, from_str, to_str):\n  \"\"\"Replace from_str with to_str in the name and content of the given file.\n\n  If any edits were necessary, returns the new filename (which may be the same as the old filename).\n  \"\"\"\n  from_bytes = from_str.encode('ascii')\n  to_bytes = to_str.encode('ascii')\n  data = read_file(os.path.join(workspace, src_file_path), binary_mode=True)\n  if from_bytes not in data and from_str not in src_file_path:\n    return None\n\n  dst_file_path = src_file_path.replace(from_str, to_str)\n  safe_file_dump(os.path.join(workspace, dst_file_path),\n                 data.replace(from_bytes, to_bytes),\n                 mode='wb')\n  if src_file_path != dst_file_path:\n    os.unlink(os.path.join(workspace, src_file_path))\n  return dst_file_path"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfingerprinting a relative filename located in a workspace.", "response": "def fingerprint_file(workspace, filename):\n  \"\"\"Given a relative filename located in a workspace, fingerprint the file.\n\n  Returns a tuple of fingerprint string and size string.\n  \"\"\"\n  content = read_file(os.path.join(workspace, filename), binary_mode=True)\n  fingerprint = hashlib.sha256(content)\n  b64_encoded = base64.b64encode(fingerprint.digest())\n  return 'sha256={}'.format(b64_encoded.decode('utf-8')), str(len(content))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rewrite_record_file(workspace, src_record_file, mutated_file_tuples):\n  mutated_files = set()\n  dst_record_file = None\n  for src, dst in mutated_file_tuples:\n    if src == src_record_file:\n      dst_record_file = dst\n    else:\n      mutated_files.add(dst)\n  if not dst_record_file:\n    raise Exception('Malformed whl or bad globs: `{}` was not rewritten.'.format(src_record_file))\n\n  output_records = []\n  file_name = os.path.join(workspace, dst_record_file)\n  for line in read_file(file_name).splitlines():\n    filename, fingerprint_str, size_str = line.rsplit(',', 3)\n    if filename in mutated_files:\n      fingerprint_str, size_str = fingerprint_file(workspace, filename)\n      output_line = ','.join((filename, fingerprint_str, size_str))\n    else:\n      output_line = line\n    output_records.append(output_line)\n\n  safe_file_dump(file_name, '\\r\\n'.join(output_records) + '\\r\\n')", "response": "This function rewrites the RECORD file in place."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument('whl_file',\n                      help='The input whl file.')\n  parser.add_argument('dest_dir',\n                      help='The destination directory for the output whl.')\n  parser.add_argument('target_version',\n                      help='The target version of the output whl.')\n  parser.add_argument('--glob', action='append',\n                      default=[\n                        '*.dist-info/*',\n                        '*-nspkg.pth',\n                      ],\n                      help='Globs (fnmatch) to rewrite within the whl: may be specified multiple times.')\n  args = parser.parse_args()\n  reversion(args)", "response": "This function is used to create a copy of the whl with the given target version."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if a potentially ambiguous single address spec really exists.", "response": "def is_valid_single_address(self, single_address):\n    \"\"\"Check if a potentially ambiguous single address spec really exists.\n\n    :param single_address: A SingleAddress spec.\n    :return: True if given spec exists, False otherwise.\n    \"\"\"\n    if not isinstance(single_address, SingleAddress):\n      raise TypeError(\n        'Parameter \"{}\" is of type {}, expecting type {}.'.format(\n          single_address, type(single_address), SingleAddress))\n\n    try:\n      return bool(self.scan_specs([single_address]))\n    except AddressLookupError:\n      return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef end_workunit(self, workunit):\n\n    additional_data = {\n      'outcome': workunit.outcome_string(workunit.outcome()),\n      'end_time': workunit.end_time,\n      'unaccounted_time': workunit.unaccounted_time(),\n    }\n\n    root_id = str(workunit.root().id)\n\n    # We update the last workunit in the stack, which is always the\n    # youngest workunit currently open, and then pop it off. The\n    # last workunit serves as a pointer to the child workunit under\n    # it's parent (the parent of the youngest workunit currently\n    # open is always the one immediately above it in the stack).\n    self._root_id_to_workunit_stack[root_id][-1].update(additional_data)\n    self._root_id_to_workunit_stack[root_id].pop()", "response": "Implementation of Reporter callback."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_output(self, workunit, label, stream):\n\n    self._root_id_to_workunit_stack[str(workunit.root().id)][-1]['outputs'][label] += stream", "response": "Implementation of Reporter callback."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nresolve an artifact version conflict between the specified jar_dependencies and the specified jar_library target.", "response": "def resolve_version_conflict(self, managed_coord, direct_coord, force=False):\n    \"\"\"Resolves an artifact version conflict between directly specified and managed jars.\n\n    This uses the user-defined --conflict-strategy to pick the appropriate artifact version (or to\n    raise an error).\n\n    This assumes the two conflict coordinates differ only by their version.\n\n    :param M2Coordinate managed_coord: the artifact coordinate as defined by a\n      managed_jar_dependencies object.\n    :param M2Coordinate direct_coord: the artifact coordinate as defined by a jar_library target.\n    :param bool force: Whether the artifact defined by the jar_library() was marked with force=True.\n      This is checked only if one of the *_IF_FORCED conflict strategies is being used.\n    :return: the coordinate of the artifact that should be resolved.\n    :rtype: M2Coordinate\n    :raises: JarDependencyManagement.DirectManagedVersionConflict if the versions are different and\n      the --conflict-strategy is 'FAIL' (which is the default).\n    \"\"\"\n    if M2Coordinate.unversioned(managed_coord) != M2Coordinate.unversioned(direct_coord):\n      raise ValueError('Illegal arguments passed to resolve_version_conflict: managed_coord and '\n                       'direct_coord must only differ by their version!\\n'\n                       '  Managed: {}\\n  Direct:  {}\\n'.format(\n        M2Coordinate.unversioned(managed_coord),\n        M2Coordinate.unversioned(direct_coord),\n      ))\n\n    if direct_coord.rev is None or direct_coord.rev == managed_coord.rev:\n      return managed_coord\n\n    strategy = self.get_options().conflict_strategy\n    message = dedent(\"\"\"\n      An artifact directly specified by a jar_library target has a different version than what\n      is specified by managed_jar_dependencies.\n\n        Artifact: jar(org={org}, name={name}, classifier={classifier}, ext={ext})\n        Direct version:  {direct}\n        Managed version: {managed}\n    \"\"\").format(\n      org=direct_coord.org,\n      name=direct_coord.name,\n      classifier=direct_coord.classifier,\n      ext=direct_coord.ext,\n      direct=direct_coord.rev,\n      managed=managed_coord.rev,\n    )\n\n    if strategy == 'FAIL':\n      raise self.DirectManagedVersionConflict(\n        '{}\\nThis raises an error due to the current --jar-dependency-management-conflict-strategy.'\n        .format(message)\n      )\n\n    is_silent = self.get_options().suppress_conflict_warnings\n    log = logger.debug if is_silent else logger.warn\n\n    if strategy == 'USE_DIRECT':\n      log(message)\n      log('[{}] Using direct version: {}'.format(strategy, direct_coord))\n      return direct_coord\n\n    if strategy == 'USE_DIRECT_IF_FORCED':\n      log(message)\n      if force:\n        log('[{}] Using direct version, because force=True: {}'.format(strategy, direct_coord))\n        return direct_coord\n      else:\n        log('[{}] Using managed version, because force=False: {}'.format(strategy, managed_coord))\n        return managed_coord\n\n    if strategy == 'USE_MANAGED':\n      log(message)\n      log('[{}] Using managed version: {}'.format(strategy, managed_coord))\n      return managed_coord\n\n    if strategy == 'USE_NEWER':\n      newer = max([managed_coord, direct_coord],\n                  key=lambda coord: Revision.lenient(coord.rev))\n      log(message)\n      log('[{}] Using newer version: {}'.format(strategy, newer))\n      return newer\n\n    raise TaskError('Unknown value for --conflict-strategy: {}'.format(strategy))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef targets_by_artifact_set(self, targets):\n    sets_to_targets = defaultdict(list)\n    for target in targets:\n      sets_to_targets[self.for_target(target)].append(target)\n    return dict(sets_to_targets)", "response": "Partitions the input targets by the sets of pinned artifacts they are managed by."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef for_target(self, target):\n    if not isinstance(target, JarLibrary):\n      return None\n    found_target = target.managed_dependencies\n    if not found_target:\n      return self.default_artifact_set\n    return self._artifact_set_map[found_target.id]", "response": "Computes and returns the artifact set that should be used to manage the given target."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef id(self):\n    if not self._id:\n      # NB(gmalmquist): This id is not cheap to compute if there are a large number of artifacts.\n      # We cache it here, but invalidate the cached value if an artifact gets added or changed.\n      self._id = tuple(sorted(map(str, self)))\n    return self._id", "response": "A unique stable hashable id over the set of pinned artifacts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd the given coordinate to the set using its version to pin it.", "response": "def put(self, artifact):\n    \"\"\"Adds the given coordinate to the set, using its version to pin it.\n\n    If this set already contains an artifact with the same coordinates other than the version, it is\n    replaced by the new artifact.\n\n    :param M2Coordinate artifact: the artifact coordinate.\n    \"\"\"\n    artifact = M2Coordinate.create(artifact)\n    if artifact.rev is None:\n      raise self.MissingVersion('Cannot pin an artifact to version \"None\"! {}'.format(artifact))\n    key = self._key(artifact)\n    previous = self._artifacts_to_versions.get(key)\n    self._artifacts_to_versions[key] = artifact\n    if previous != artifact:\n      self._id = None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, artifact):\n    coord = self._key(artifact)\n    if coord in self._artifacts_to_versions:\n      return self._artifacts_to_versions[coord]\n    return artifact", "response": "Gets the coordinate with the correct version for the given artifact."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the set of pinned artifacts that are referenced by this target and any of its dependencies.", "response": "def _compute_artifact_set(self, management_target):\n    \"\"\"Computes the set of pinned artifacts specified by this target, and any of its dependencies.\n\n    An error is raised if a conflict exists between a pinned version between a\n    ManagedJarDependencies target and any of its dependencies, or if two versions of a jar exist in\n    the same ManagedJarDependencies target.\n\n    :param Target management_target: a target object which is (or at least depends on) a\n      ManagedJarDependencies target.\n    :return: the computed transitive artifact set (approximately the union of all pinned artifacts\n      in the transitive closure of the input target).\n    :rtype: PinnedJarArtifactSet\n    \"\"\"\n    artifact_set = PinnedJarArtifactSet()\n\n    # Keeps track of where pinned artifacts came from for logging purposes.\n    specs_by_coordinate = {}\n\n    def handle_managed_jar_dependencies(target):\n      subset = PinnedJarArtifactSet()\n      for jar in self._jar_iterator(target):\n        if jar.rev is None:\n          raise self.MissingVersion(target, jar)\n        if jar in subset and subset[jar].rev != jar.rev:\n          raise self.DuplicateCoordinateError(target, jar, artifact_set[jar].rev, jar.rev)\n        subset.put(jar)\n      return subset\n\n    def handle_conflict(artifact, target):\n      previous_coord = artifact_set[artifact]\n      previous_spec = specs_by_coordinate[previous_coord]\n      message = ('Artifact {previous_coord} (from {previous_target}) overridden by {new_coord} '\n                 '(in {new_target}).'.format(previous_coord=previous_coord,\n                                             previous_target=previous_spec,\n                                             new_coord=artifact,\n                                             new_target=target.address.spec))\n      raise self.IllegalVersionOverride(message)\n\n    def handle_target(target):\n      if not isinstance(target, ManagedJarDependencies):\n        return\n      for artifact in handle_managed_jar_dependencies(target):\n        if artifact.rev != artifact_set[artifact].rev:\n          handle_conflict(artifact, target)\n        specs_by_coordinate[M2Coordinate.create(artifact)] = target.address.spec\n        artifact_set.put(artifact)\n\n    self.context.build_graph.walk_transitive_dependency_graph(\n      addresses=[management_target.address],\n      work=handle_target,\n      postorder=True, # Process dependencies first.\n    )\n    return artifact_set"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nattempting to open the given files using the preferred desktop viewer or editor.", "response": "def ui_open(*files):\n  \"\"\"Attempts to open the given files using the preferred desktop viewer or editor.\n\n  :raises :class:`OpenError`: if there is a problem opening any of the files.\n  \"\"\"\n  if files:\n    osname = get_os_name()\n    opener = _OPENER_BY_OS.get(osname)\n    if opener:\n      opener(files)\n    else:\n      raise OpenError('Open currently not supported for ' + osname)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate an addressable factory for the given target type and alias.", "response": "def factory(cls, target_type, alias=None):\n    \"\"\"Creates an addressable factory for the given target type and alias.\n\n    :returns: A factory that can capture :class:`TargetAddressable` instances.\n    :rtype: :class:`Addressable.Factory`\n    \"\"\"\n    class Factory(Addressable.Factory):\n      @property\n      def target_types(self):\n        return (target_type,)\n\n      def capture(self, *args, **kwargs):\n        return TargetAddressable(alias, target_type, *args, **kwargs)\n    return Factory()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef format_args_for_target(self, target, target_workdir):\n\n    args = ['--java_out={0}'.format(target_workdir)]\n\n    # Add all params in payload to args\n\n    relative_sources, source_roots = self._compute_sources(target)\n\n    if target.payload.get_field_value('no_options'):\n      args.append('--no_options')\n\n    if target.payload.service_writer:\n      args.append('--service_writer={}'.format(target.payload.service_writer))\n      if target.payload.service_writer_options:\n        for opt in target.payload.service_writer_options:\n          args.append('--service_writer_opt')\n          args.append(opt)\n\n    registry_class = target.payload.registry_class\n    if registry_class:\n      args.append('--registry_class={0}'.format(registry_class))\n\n    if target.payload.roots:\n      args.append('--roots={0}'.format(','.join(target.payload.roots)))\n\n    if target.payload.enum_options:\n      args.append('--enum_options={0}'.format(','.join(target.payload.enum_options)))\n\n    for source_root in source_roots:\n      args.append('--proto_path={0}'.format(os.path.join(get_buildroot(), source_root)))\n\n    args.extend(relative_sources)\n    return args", "response": "Calculate the arguments to pass to the command line for a single target."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nspecifies a Main - Class entry for this jar s manifest.", "response": "def main(self, main):\n    \"\"\"Specifies a Main-Class entry for this jar's manifest.\n\n    :param string main: a fully qualified class name\n    \"\"\"\n    if not main or not isinstance(main, string_types):\n      raise ValueError('The main entry must be a non-empty string')\n    self._main = main"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nschedules a write of the file at src to dest in this jar.", "response": "def write(self, src, dest=None):\n    \"\"\"Schedules a write of the file at ``src`` to the ``dest`` path in this jar.\n\n    If the ``src`` is a file, then ``dest`` must be specified.\n\n    If the ``src`` is a directory then by default all descendant files will be added to the jar as\n    entries carrying their relative path.  If ``dest`` is specified it will be prefixed to each\n    descendant's relative path to form its jar entry path.\n\n    :param string src: the path to the pre-existing source file or directory\n    :param string dest: the path the source file or directory should have in this jar\n    \"\"\"\n    if not src or not isinstance(src, string_types):\n      raise ValueError('The src path must be a non-empty string, got {} of type {}.'.format(\n        src, type(src)))\n    if dest and not isinstance(dest, string_types):\n      raise ValueError('The dest entry path must be a non-empty string, got {} of type {}.'.format(\n        dest, type(dest)))\n    if not os.path.isdir(src) and not dest:\n      raise self.Error('Source file {} must have a jar destination specified'.format(src))\n\n    self._add_entry(self.FileSystemEntry(src, dest))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds all entries from the given jar to the jar save for the manifest.", "response": "def writejar(self, jar):\n    \"\"\"Schedules all entries from the given ``jar``'s to be added to this jar save for the manifest.\n\n    :param string jar: the path to the pre-existing jar to graft into this jar\n    \"\"\"\n    if not jar or not isinstance(jar, string_types):\n      raise ValueError('The jar path must be a non-empty string')\n\n    self._jars.append(jar)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _render_jar_tool_args(self, options):\n    args = []\n\n    with temporary_dir() as manifest_stage_dir:\n      # relativize urls in canonical classpath, this needs to be stable too therefore\n      # do not follow the symlinks because symlinks may vary from platform to platform.\n      classpath = relativize_classpath(self.classpath,\n                                       os.path.dirname(self._path),\n                                       followlinks=False)\n\n      def as_cli_entry(entry):\n        src = entry.materialize(manifest_stage_dir)\n        return '{}={}'.format(src, entry.dest) if entry.dest else src\n      files = [as_cli_entry(entry) for entry in self._entries] if self._entries else []\n\n      jars = self._jars or []\n\n      with safe_args(classpath, options, delimiter=',') as classpath_args:\n        with safe_args(files, options, delimiter=',') as files_args:\n          with safe_args(jars, options, delimiter=',') as jars_args:\n\n            # If you specify --manifest to jar-tool you cannot specify --main.\n            if self._manifest_entry:\n              manifest_file = self._manifest_entry.materialize(manifest_stage_dir)\n            else:\n              manifest_file = None\n\n            if self._main and manifest_file:\n              main_arg = None\n              with open(manifest_file, 'a') as f:\n                f.write(\"Main-Class: {}\\n\".format(self._main))\n            else:\n              main_arg = self._main\n\n            if main_arg:\n              args.append('-main={}'.format(self._main))\n\n            if classpath_args:\n              args.append('-classpath={}'.format(','.join(classpath_args)))\n\n            if manifest_file:\n              args.append('-manifest={}'.format(manifest_file))\n\n            if files_args:\n              args.append('-files={}'.format(','.join(files_args)))\n\n            if jars_args:\n              args.append('-jars={}'.format(','.join(jars_args)))\n\n            yield args", "response": "Format the arguments to jar - tool."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef open_jar(self, path, overwrite=False, compressed=True, jar_rules=None):\n    jar = Jar(path)\n    try:\n      yield jar\n    except jar.Error as e:\n      raise TaskError('Failed to write to jar at {}: {}'.format(path, e))\n\n    with jar._render_jar_tool_args(self.get_options()) as args:\n      if args:  # Don't build an empty jar\n        args.append('-update={}'.format(self._flag(not overwrite)))\n        args.append('-compress={}'.format(self._flag(compressed)))\n\n        jar_rules = jar_rules or JarRules.default()\n        args.append('-default_action={}'.format(self._action_name(jar_rules.default_dup_action)))\n\n        skip_patterns = []\n        duplicate_actions = []\n\n        for rule in jar_rules.rules:\n          if isinstance(rule, Skip):\n            skip_patterns.append(rule.apply_pattern)\n          elif isinstance(rule, Duplicate):\n            duplicate_actions.append('{}={}'.format(\n              rule.apply_pattern.pattern, self._action_name(rule.action)))\n          else:\n            raise ValueError('Unrecognized rule: {}'.format(rule))\n\n        if skip_patterns:\n          args.append('-skip={}'.format(','.join(p.pattern for p in skip_patterns)))\n\n        if duplicate_actions:\n          args.append('-policies={}'.format(','.join(duplicate_actions)))\n\n        args.append(path)\n\n        if JarTool.global_instance().run(context=self.context, runjava=self.runjava, args=args):\n          raise TaskError('jar-tool failed')", "response": "Yields a Jar that will be written to the given path."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new JarTask. JarBuilder ready for use.", "response": "def create_jar_builder(self, jar):\n    \"\"\"Creates a ``JarTask.JarBuilder`` ready for use.\n\n    This method should be called during in `execute` context and only after ensuring\n    `JarTask.JarBuilder.prepare` has already been called in `prepare` context.\n\n    :param jar: An opened ``pants.backend.jvm.tasks.jar_task.Jar`.\n    \"\"\"\n    builder = self.JarBuilder(self.context, jar)\n    yield builder\n    builder.commit_manifest(jar)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new watchman client using the BSER protocol over a UNIX socket.", "response": "def _make_client(self):\n    \"\"\"Create a new watchman client using the BSER protocol over a UNIX socket.\"\"\"\n    self._logger.debug('setting initial watchman timeout to %s', self._startup_timeout)\n    return StreamableWatchmanClient(sockpath=self.socket,\n                                    transport='local',\n                                    timeout=self._startup_timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef launch(self):\n    cmd = self._construct_cmd((self._watchman_path, 'get-pid'),\n                              state_file=self._state_file,\n                              sock_file=self._sock_file,\n                              pid_file=self._pid_file,\n                              log_file=self._log_file,\n                              log_level=str(self._log_level))\n    self._logger.debug('watchman cmd is: {}'.format(' '.join(cmd)))\n    self._maybe_init_metadata()\n\n    # Watchman is launched via its cli. By running the 'get-pid' command on the client we implicitly\n    # launch the Watchman daemon. This approach is somewhat error-prone - in some cases the client\n    # can successfully trigger the launch of the Watchman daemon, but fail to return successfully\n    # for the 'get-pid' result due to server <-> daemon socket issues - these can look like:\n    #\n    #   2016-04-01T17:31:23,820: [cli] unable to talk to your watchman\n    #                                  on .../watchman.sock! (Permission denied)\n    #\n    # This results in a subprocess execution failure and leaves us with no pid information to write\n    # to the metadata directory - while in reality a Watchman daemon is actually running but now\n    # untracked. To safeguard against this, we retry the (idempotent) 'get-pid' command a few times\n    # to give the server-side socket setup a few chances to quiesce before potentially orphaning it.\n\n    get_output = functools.partial(self.get_subprocess_output, cmd)\n    output = retry_on_exception(get_output, 3, (ProcessManager.ExecutionError,), lambda n: n * .5)\n\n    # Parse the watchman PID from the cli output.\n    pid = self._parse_pid_from_output(output)\n\n    # Write the process metadata to disk.\n    self.write_pid(pid)\n    self.write_socket(self._sock_file)", "response": "Launch and synchronously write metadata."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting a timeout on the inner watchman client s socket.", "response": "def _attempt_set_timeout(self, timeout):\n    \"\"\"Sets a timeout on the inner watchman client's socket.\"\"\"\n    try:\n      self.client.setTimeout(timeout)\n    except Exception:\n      self._logger.debug('failed to set post-startup watchman timeout to %s', self._timeout)\n    else:\n      self._logger.debug('set post-startup watchman timeout to %s', self._timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef watch_project(self, path):\n    # TODO(kwlzn): Add a client.query(timeout=X) param to the upstream pywatchman project.\n    try:\n      return self.client.query('watch-project', os.path.realpath(path))\n    finally:\n      self._attempt_set_timeout(self._timeout)", "response": "Issues the watch - project command to watchman to begin watching the buildroot."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbulk subscribe generator for StreamableWatchmanClient.", "response": "def subscribed(self, build_root, handlers):\n    \"\"\"Bulk subscribe generator for StreamableWatchmanClient.\n\n    :param str build_root: the build_root for all subscriptions.\n    :param iterable handlers: a sequence of Watchman.EventHandler namedtuple objects.\n    :yields: a stream of tuples in the form (subscription_name: str, subscription_event: dict).\n    \"\"\"\n    command_list = [['subscribe',\n                     build_root,\n                     handler.name,\n                     handler.metadata] for handler in handlers]\n\n    self._logger.debug('watchman command_list is: {}'.format(command_list))\n\n    try:\n      for event in self.client.stream_query(command_list):\n        if event is None:\n          yield None, None\n        elif 'subscribe' in event:\n          self._logger.info('confirmed watchman subscription: {}'.format(event))\n          yield None, None\n        elif 'subscription' in event:\n          yield event.get('subscription'), event\n        else:\n          self._logger.warning('encountered non-subscription event: {}'.format(event))\n    except self.client.WatchmanError as e:\n      raise self.WatchmanCrash(e)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a dependency to this target.", "response": "def add_dependency(self, address):\n    \"\"\"Add a dependency to this target.  This will deduplicate existing dependencies.\"\"\"\n    if address in self._dependencies_by_address:\n      if self._dependencies_by_address[address].has_comment():\n        logger.warn('BuildFileManipulator would have added {address} as a dependency of '\n                    '{target_address}, but that dependency was already forced with a comment.'\n                    .format(address=address.spec, target_address=self.target_address.spec))\n        return\n    spec = address.reference(referencing_path=self.build_file.spec_path)\n    self._dependencies_by_address[address] = DependencySpec(spec)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clear_unforced_dependencies(self):\n    self._dependencies_by_address = dict(\n      (address, dep) for address, dep in self._dependencies_by_address.items()\n      if dep.has_comment()\n    )", "response": "Removes all unforced dependencies from the set of dependencies."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlikes target_lines the entire BUILD file s lines after dependency manipulation.", "response": "def build_file_lines(self):\n    \"\"\"Like `target_lines`, the entire BUILD file's lines after dependency manipulation.\"\"\"\n    build_file_lines = self._build_file_source_lines[:]\n    target_begin, target_end = self._target_interval\n    build_file_lines[target_begin:target_end] = self.target_lines()\n    return build_file_lines"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef diff_lines(self):\n    start_lines = self._build_file_source_lines[:]\n    end_lines = self.build_file_lines()\n    diff_generator = unified_diff(start_lines,\n                                  end_lines,\n                                  fromfile=self.build_file.relpath,\n                                  tofile=self.build_file.relpath,\n                                  lineterm='')\n    return list(diff_generator)", "response": "A diff between the original BUILD file and the resulting BUILD file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write(self, dry_run=True):\n    start_lines = self._build_file_source_lines[:]\n    end_lines = self.build_file_lines()\n    diff_generator = unified_diff(start_lines,\n                                  end_lines,\n                                  fromfile=self.build_file.relpath,\n                                  tofile=self.build_file.relpath,\n                                  lineterm='')\n    if dry_run:\n      msg = 'DRY RUN, would have written this diff:'\n    else:\n      msg = 'REAL RUN, about to write the following diff:'\n    sys.stderr.write(msg + '\\n')\n    sys.stderr.write('*' * 40 + '\\n')\n    sys.stderr.write('target at: ')\n    sys.stderr.write(str(self.target_address) + '\\n')\n\n    for line in diff_generator:\n      sys.stderr.write(line + '\\n')\n\n    sys.stderr.write('*' * 40 + '\\n')\n    if not dry_run:\n      with open(self.build_file.full_path, 'w') as bf:\n        bf.write('\\n'.join(end_lines))\n      sys.stderr.write('WROTE to {full_path}\\n'.format(full_path=self.build_file.full_path))", "response": "Write out the changes made to the BUILD file and print the diff to stderr."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a file containing overrides for Conan remotes which get applied to registry. json.", "response": "def _remotes_txt_content(self):\n    \"\"\"Generate a file containing overrides for Conan remotes which get applied to registry.json.\"\"\"\n    return '{}\\n'.format('\\n'.join(\n      '{name} {url} {is_ssl}'.format(\n        name=name,\n        url=url,\n        is_ssl=re.match(r'^https://', url) is not None)\n        for name, url in self.get_options().conan_remotes.items()))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _conan_user_home(self, conan, in_workdir=False):\n    # This argument is exposed so tests don't leak out of the workdir.\n    if in_workdir:\n      base_cache_dir = self.workdir\n    else:\n      base_cache_dir = get_pants_cachedir()\n    user_home_base = os.path.join(base_cache_dir, 'conan-support', 'conan-user-home')\n    # Locate the subdirectory of the pants shared cachedir specific to this task's option values.\n    user_home = os.path.join(user_home_base, self.fingerprint)\n    conan_install_base = os.path.join(user_home, '.conan')\n    # Conan doesn't copy remotes.txt into the .conan subdir after the \"config install\" command, it\n    # simply edits registry.json. However, it is valid to have this file there, and Conan won't\n    # touch it, so we use its presence to detect whether we have appropriately initialized the\n    # Conan installation.\n    remotes_txt_sentinel = os.path.join(conan_install_base, 'remotes.txt')\n    if not os.path.isfile(remotes_txt_sentinel):\n      safe_mkdir(conan_install_base)\n      # Conan doesn't consume the remotes.txt file just by being in the conan directory -- we need\n      # to create another directory containing any selection of files detailed in\n      # https://docs.conan.io/en/latest/reference/commands/consumer/config.html#conan-config-install\n      # and \"install\" from there to our desired conan directory.\n      with temporary_dir() as remotes_install_dir:\n        # Create an artificial conan configuration dir containing just remotes.txt.\n        remotes_txt_for_install = os.path.join(remotes_install_dir, 'remotes.txt')\n        safe_file_dump(remotes_txt_for_install, self._remotes_txt_content)\n        # Configure the desired user home from this artificial config dir.\n        argv = ['config', 'install', remotes_install_dir]\n        workunit_factory = functools.partial(\n          self.context.new_workunit,\n          name='initial-conan-config',\n          labels=[WorkUnitLabel.TOOL])\n        env = {\n          'CONAN_USER_HOME': user_home,\n        }\n        cmdline, exit_code = conan.run(workunit_factory, argv, env=env)\n        if exit_code != 0:\n          raise self.ConanConfigError(\n            'Error configuring conan with argv {} and environment {}: exited non-zero ({}).'\n            .format(cmdline, env, exit_code),\n            exit_code=exit_code)\n      # Generate the sentinel file so that we know the remotes have been successfully configured for\n      # this particular task fingerprint in successive pants runs.\n      safe_file_dump(remotes_txt_sentinel, self._remotes_txt_content)\n\n    return user_home", "response": "Create the CONAN_USER_HOME for this task fingerprint and initialize the Conan remotes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef execute_codegen(self, target, target_workdir):\n    conan = self.context.products.get_data(ConanPrep.tool_instance_cls)\n\n    # TODO: we should really be able to download all of these in one go, and we should make an\n    # upstream PR to allow that against Conan if not.\n    for conan_requirement in target.packages:\n      # See https://docs.conan.io/en/latest/reference/commands/consumer/install.html for\n      # documentation on the 'install' command.\n      argv = [\n        'install',\n        conan_requirement.pkg_spec,\n        '--settings', 'os={}'.format(self._conan_os_name),\n      ]\n      for remote in self.get_options().conan_remotes:\n        argv.extend(['--remote', remote])\n\n      workunit_factory = functools.partial(\n        self.context.new_workunit,\n        name='install-conan-{}'.format(conan_requirement.pkg_spec),\n        labels=[WorkUnitLabel.TOOL])\n      # CONAN_USER_HOME is somewhat documented at\n      # https://docs.conan.io/en/latest/mastering/sharing_settings_and_config.html.\n      user_home = self._conan_user_home(conan)\n      env = {\n        'CONAN_USER_HOME': user_home,\n      }\n\n      with conan.run_with(workunit_factory, argv, env=env) as (cmdline, exit_code, workunit):\n        if exit_code != 0:\n          raise self.ConanFetchError(\n            'Error performing conan install with argv {} and environment {}: exited non-zero ({}).'\n            .format(cmdline, env, exit_code),\n            exit_code=exit_code)\n\n        # Read the stdout from the read-write buffer, from the beginning of the output, and convert\n        # to unicode.\n        conan_install_stdout = workunit.output('stdout').read_from(0).decode('utf-8')\n        pkg_sha = conan_requirement.parse_conan_stdout_for_pkg_sha(conan_install_stdout)\n\n      installed_data_dir = os.path.join(\n        user_home,\n        '.conan', 'data',\n        conan_requirement.directory_path,\n        'package',\n        pkg_sha)\n\n      # Copy over the contents of the installed package into the target output directory. These\n      # paths are currently hardcoded -- see `ExternalNativeLibrary`.\n      mergetree(os.path.join(installed_data_dir, conan_requirement.include_relpath),\n                os.path.join(target_workdir, 'include'))\n      mergetree(os.path.join(installed_data_dir, conan_requirement.lib_relpath),\n                os.path.join(target_workdir, 'lib'))", "response": "Invoke the conan pex to fetch conan packages specified by target. packages."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load(data):\n\n    if hasattr(data, 'read') and callable(data.read):\n      contents = data.read()\n    elif isinstance(data, six.string_types):\n      contents = data\n    else:\n      raise TypeError('Can only process data from a string or a readable object, given: %s' % data)\n\n    return Properties._parse(contents.splitlines())", "response": "Loads properties from an open stream or the contents of a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dump(props, output):\n    def escape(token):\n      return re.sub(r'([=:\\s])', r'\\\\\\1', token)\n\n    def write(out):\n      for k, v in props.items():\n        out.write('%s=%s\\n' % (escape(str(k)), escape(str(v))))\n\n    if hasattr(output, 'write') and callable(output.write):\n      write(output)\n    elif isinstance(output, six.string_types):\n      with open(output, 'w+') as out:\n        write(out)\n    else:\n      raise TypeError('Can only dump data to a path or a writable object, given: %s' % output)", "response": "Dumps a dict of properties to a file path or file path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing as a decorator on a method definition to make it a class-level attribute. This decorator can be applied to a method, a classmethod, or a staticmethod. This decorator will bind the first argument to the class object. Usage: >>> class Foo(object): ... @classproperty ... def name(cls): ... return cls.__name__ ... >>> Foo.name 'Foo' Setting or deleting the attribute of this name will overwrite this property. The docstring of the classproperty `x` for a class `C` can be obtained by `C.__dict__['x'].__doc__`.", "response": "def classproperty(func):\n  \"\"\"Use as a decorator on a method definition to make it a class-level attribute.\n\n  This decorator can be applied to a method, a classmethod, or a staticmethod. This decorator will\n  bind the first argument to the class object.\n\n  Usage:\n  >>> class Foo(object):\n  ...   @classproperty\n  ...   def name(cls):\n  ...     return cls.__name__\n  ...\n  >>> Foo.name\n  'Foo'\n\n  Setting or deleting the attribute of this name will overwrite this property.\n\n  The docstring of the classproperty `x` for a class `C` can be obtained by\n  `C.__dict__['x'].__doc__`.\n  \"\"\"\n  doc = func.__doc__\n\n  if not isinstance(func, (classmethod, staticmethod)):\n    func = classmethod(func)\n\n  return ClassPropertyDescriptor(func, doc)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nuse as a decorator on a method definition to make it a class-level attribute (without binding). This decorator can be applied to a method or a staticmethod. This decorator does not bind any arguments. Usage: >>> other_x = 'value' >>> class Foo(object): ... @staticproperty ... def x(): ... return other_x ... >>> Foo.x 'value' Setting or deleting the attribute of this name will overwrite this property. The docstring of the classproperty `x` for a class `C` can be obtained by `C.__dict__['x'].__doc__`.", "response": "def staticproperty(func):\n  \"\"\"Use as a decorator on a method definition to make it a class-level attribute (without binding).\n\n  This decorator can be applied to a method or a staticmethod. This decorator does not bind any\n  arguments.\n\n  Usage:\n  >>> other_x = 'value'\n  >>> class Foo(object):\n  ...   @staticproperty\n  ...   def x():\n  ...     return other_x\n  ...\n  >>> Foo.x\n  'value'\n\n  Setting or deleting the attribute of this name will overwrite this property.\n\n  The docstring of the classproperty `x` for a class `C` can be obtained by\n  `C.__dict__['x'].__doc__`.\n  \"\"\"\n  doc = func.__doc__\n\n  if not isinstance(func, staticmethod):\n    func = staticmethod(func)\n\n  return ClassPropertyDescriptor(func, doc)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _collect_internal_deps(self, targets):\n    roots = set()\n    for target in targets:\n      roots.update(target.dependencies)\n    return Target.closure_for_targets(roots)", "response": "Collect one level of dependencies from the given targets and transitively walk the tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nyield args if there are less than a limit otherwise writes args to a temporary file and yields an argument list with one argument formed from the path of the argfile.", "response": "def safe_args(args,\n              options,\n              max_args=None,\n              argfile=None,\n              delimiter='\\n',\n              quoter=None,\n              delete=True):\n  \"\"\"Yields args if there are less than a limit otherwise writes args to an argfile and yields an\n  argument list with one argument formed from the path of the argfile.\n\n  :param args: The args to work with.\n  :param OptionValueContainer options: scoped options object for this task\n  :param max_args: The maximum number of args to let though without writing an argfile.  If not\n    specified then the maximum will be loaded from the --max-subprocess-args option.\n  :param argfile: The file to write args to when there are too many; defaults to a temporary file.\n  :param delimiter: The delimiter to insert between args written to the argfile, defaults to '\\n'\n  :param quoter: A function that can take the argfile path and return a single argument value;\n    defaults to: <code>lambda f: '@' + f<code>\n  :param delete: If True deletes any arg files created upon exit from this context; defaults to\n    True.\n  \"\"\"\n  max_args = max_args or options.max_subprocess_args\n  if len(args) > max_args:\n    def create_argfile(f):\n      logger.debug('Creating argfile {} with contents {}'.format(f.name, ' '.join(args)))\n      f.write(delimiter.join(args))\n      f.close()\n      return [quoter(f.name) if quoter else '@{}'.format(f.name)]\n\n    if argfile:\n      try:\n        with safe_open(argfile, 'w') as fp:\n          yield create_argfile(fp)\n      finally:\n        if delete and os.path.exists(argfile):\n          os.unlink(argfile)\n    else:\n      with temporary_file(cleanup=delete, binary_mode=False) as fp:\n        yield create_argfile(fp)\n  else:\n    yield args"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the options to run this JVM with.", "response": "def get_jvm_options(self):\n    \"\"\"Return the options to run this JVM with.\n\n    These are options to the JVM itself, such as -Dfoo=bar, -Xmx=1g, -XX:-UseParallelGC and so on.\n\n    Thus named because get_options() already exists (and returns this object's Pants options).\n    \"\"\"\n    ret = []\n    for opt in self.get_options().options:\n      ret.extend(safe_shlex_split(opt))\n\n    if (self.get_options().debug or\n        self.get_options().is_flagged('debug_port') or\n        self.get_options().is_flagged('debug_args')):\n      debug_port = self.get_options().debug_port\n      ret.extend(arg.format(debug_port=debug_port) for arg in self.get_options().debug_args)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the program args to run this JVM with.", "response": "def get_program_args(self):\n    \"\"\"Get the program args to run this JVM with.\n\n    These are the arguments passed to main() and are program-specific.\n    \"\"\"\n    ret = []\n    for arg in self.get_options().program_args:\n      ret.extend(safe_shlex_split(arg))\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a VersionedTargetSet from a list of VersionedTarget objects.", "response": "def from_versioned_targets(versioned_targets):\n    \"\"\"\n    :API: public\n    \"\"\"\n    first_target = versioned_targets[0]\n    cache_manager = first_target._cache_manager\n\n    # Quick sanity check; all the versioned targets should have the same cache manager.\n    # TODO(ryan): the way VersionedTargets store their own links to a single CacheManager instance\n    # feels hacky; see if there's a cleaner way for callers to handle awareness of the CacheManager.\n    for versioned_target in versioned_targets:\n      if versioned_target._cache_manager != cache_manager:\n        raise ValueError(\"Attempting to combine versioned targets {} and {} with different\"\n                         \" CacheManager instances: {} and {}\".format(first_target, versioned_target,\n                                                                 cache_manager,\n                                                                 versioned_target._cache_manager))\n    return VersionedTargetSet(cache_manager, versioned_targets)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ensure_legal(self):\n    # Do our best to provide complete feedback, it's easy to imagine the frustration of flipping between error states.\n    if self._results_dir:\n      errors = ''\n      if not os.path.islink(self._results_dir):\n        errors += '\\nThe results_dir is no longer a symlink:\\n\\t* {}'.format(self._results_dir)\n      if not os.path.isdir(self._current_results_dir):\n        errors += '\\nThe current_results_dir directory was not found\\n\\t* {}'.format(self._current_results_dir)\n      if errors:\n        raise self.IllegalResultsDir(\n          '\\nThe results_dirs state should not be manually cleaned or recreated by tasks.\\n{}'.format(errors)\n        )\n    return True", "response": "Return True as long as the state does not break any internal contracts."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef live_dirs(self):\n    # The only caller of this function is the workdir cleaning pipeline. It is not clear that the previous_results_dir\n    # should be returned for that purpose. And, by the time this is called, the contents have already been copied.\n    if self.has_results_dir:\n      yield self.results_dir\n      yield self.current_results_dir\n      if self.has_previous_results_dir:\n        yield self.previous_results_dir", "response": "Yields directories that must exist for this VersionedTarget to function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nensure that the empty results directory and a stable symlink exist for these versioned targets.", "response": "def create_results_dir(self):\n    \"\"\"Ensure that the empty results directory and a stable symlink exist for these versioned targets.\"\"\"\n    self._current_results_dir = self._cache_manager._results_dir_path(self.cache_key, stable=False)\n    self._results_dir = self._cache_manager._results_dir_path(self.cache_key, stable=True)\n\n    if not self.valid:\n      # Clean the workspace for invalid vts.\n      safe_mkdir(self._current_results_dir, clean=True)\n\n    relative_symlink(self._current_results_dir, self._results_dir)\n    self.ensure_legal()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncopy the previous results_dir to the current results_dir.", "response": "def copy_previous_results(self):\n    \"\"\"Use the latest valid results_dir as the starting contents of the current results_dir.\n\n    Should be called after the cache is checked, since previous_results are not useful if there is\n    a cached artifact.\n    \"\"\"\n    # TODO(mateo): This should probably be managed by the task, which manages the rest of the\n    # incremental support.\n    if not self.previous_cache_key:\n      return None\n    previous_path = self._cache_manager._results_dir_path(self.previous_cache_key, stable=False)\n    if os.path.isdir(previous_path):\n      self.is_incremental = True\n      safe_rmtree(self._current_results_dir)\n      shutil.copytree(previous_path, self._current_results_dir)\n    safe_mkdir(self._current_results_dir)\n    relative_symlink(self._current_results_dir, self.results_dir)\n    # Set the self._previous last, so that it is only True after the copy completed.\n    self._previous_results_dir = previous_path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update(self, vts):\n    for vt in vts.versioned_targets:\n      vt.ensure_legal()\n      if not vt.valid:\n        self._invalidator.update(vt.cache_key)\n        vt.valid = True\n        self._artifact_write_callback(vt)\n    if not vts.valid:\n      vts.ensure_legal()\n      self._invalidator.update(vts.cache_key)\n      vts.valid = True\n      self._artifact_write_callback(vts)", "response": "Mark a changed or invalidated VersionedTargetSet as successfully processed."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nforces invalidation of a VersionedTargetSet.", "response": "def force_invalidate(self, vts):\n    \"\"\"Force invalidation of a VersionedTargetSet.\"\"\"\n    for vt in vts.versioned_targets:\n      self._invalidator.force_invalidate(vt.cache_key)\n      vt.valid = False\n    self._invalidator.force_invalidate(vts.cache_key)\n    vts.valid = False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking whether each of the targets has changed and invalidates it if so.", "response": "def check(self,\n            targets,\n            topological_order=False):\n    \"\"\"Checks whether each of the targets has changed and invalidates it if so.\n\n    Returns a list of VersionedTargetSet objects (either valid or invalid). The returned sets\n    'cover' the input targets, with one caveat: if the FingerprintStrategy\n    opted out of fingerprinting a target because it doesn't contribute to invalidation, then that\n    target will be excluded from all_vts and invalid_vts.\n\n    Callers can inspect these vts and rebuild the invalid ones, for example.\n    \"\"\"\n    all_vts = self.wrap_targets(targets, topological_order=topological_order)\n    invalid_vts = [vt for vt in all_vts if not vt.valid]\n    return InvalidationCheck(all_vts, invalid_vts)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a path to the results directory for the given key.", "response": "def _results_dir_path(self, key, stable):\n    \"\"\"Return a results directory path for the given key.\n\n    :param key: A CacheKey to generate an id for.\n    :param stable: True to use a stable subdirectory, false to use a portion of the cache key to\n      generate a path unique to the key.\n    \"\"\"\n    # TODO: Shorten cache_key hashes in general?\n    return os.path.join(\n      self._results_dir_prefix,\n      key.id,\n      self._STABLE_DIR_NAME if stable else sha1(key.hash.encode('utf-8')).hexdigest()[:12]\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wrap_targets(self, targets, topological_order=False):\n    def vt_iter():\n      if topological_order:\n        target_set = set(targets)\n        sorted_targets = [t for t in reversed(sort_targets(targets)) if t in target_set]\n      else:\n        sorted_targets = sorted(targets)\n      for target in sorted_targets:\n        target_key = self._key_for(target)\n        if target_key is not None:\n          yield VersionedTarget(self, target, target_key)\n    return list(vt_iter())", "response": "Wrap targets and their computed cache keys in VersionedTargets."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the environment to the supplied values for example.", "response": "def environment_as(**kwargs):\n  \"\"\"Update the environment to the supplied values, for example:\n\n  with environment_as(PYTHONPATH='foo:bar:baz',\n                      PYTHON='/usr/bin/python2.7'):\n    subprocess.Popen(foo).wait()\n  \"\"\"\n  new_environment = kwargs\n  old_environment = {}\n\n  def setenv(key, val):\n    if val is not None:\n      os.environ[key] = val if PY3 else _os_encode(val)\n    else:\n      if key in os.environ:\n        del os.environ[key]\n\n  for key, val in new_environment.items():\n    old_environment[key] = os.environ.get(key)\n    setenv(key, val)\n  try:\n    yield\n  finally:\n    for key, val in old_environment.items():\n      setenv(key, val)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hermetic_environment_as(**kwargs):\n  old_environment = os.environ.copy() if PY3 else _copy_and_decode_env(os.environ)\n  _purge_env()\n  try:\n    with environment_as(**kwargs):\n      yield\n  finally:\n    _purge_env()\n    _restore_env(old_environment)", "response": "Set the environment to the supplied values from an empty state."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreplace the given dst_fd and attribute on sys with an open handle to the given src_fd.", "response": "def _stdio_stream_as(src_fd, dst_fd, dst_sys_attribute, mode):\n  \"\"\"Replace the given dst_fd and attribute on `sys` with an open handle to the given src_fd.\"\"\"\n  if src_fd == -1:\n    src = open('/dev/null', mode)\n    src_fd = src.fileno()\n\n  # Capture the python and os level file handles.\n  old_dst = getattr(sys, dst_sys_attribute)\n  old_dst_fd = os.dup(dst_fd)\n  if src_fd != dst_fd:\n    os.dup2(src_fd, dst_fd)\n\n  # Open up a new file handle to temporarily replace the python-level io object, then yield.\n  new_dst = os.fdopen(dst_fd, mode)\n  setattr(sys, dst_sys_attribute, new_dst)\n  try:\n    yield\n  finally:\n    new_dst.close()\n\n    # Restore the python and os level file handles.\n    os.dup2(old_dst_fd, dst_fd)\n    setattr(sys, dst_sys_attribute, old_dst)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nredirects sys.{stdout, stderr, stdin} to alternate file descriptors. As a special case, if a given destination fd is `-1`, we will replace it with an open file handle to `/dev/null`. NB: If the filehandles for sys.{stdout, stderr, stdin} have previously been closed, it's possible that the OS has repurposed fds `0, 1, 2` to represent other files or sockets. It's impossible for this method to locate all python objects which refer to those fds, so it's up to the caller to guarantee that `0, 1, 2` are safe to replace. In Python3, the streams expect unicode. To write and read bytes, access their buffer, e.g. `stdin.buffer.read()`.", "response": "def stdio_as(stdout_fd, stderr_fd, stdin_fd):\n  \"\"\"Redirect sys.{stdout, stderr, stdin} to alternate file descriptors.\n\n  As a special case, if a given destination fd is `-1`, we will replace it with an open file handle\n  to `/dev/null`.\n\n  NB: If the filehandles for sys.{stdout, stderr, stdin} have previously been closed, it's\n  possible that the OS has repurposed fds `0, 1, 2` to represent other files or sockets. It's\n  impossible for this method to locate all python objects which refer to those fds, so it's up\n  to the caller to guarantee that `0, 1, 2` are safe to replace.\n\n  In Python3, the streams expect unicode. To write and read bytes, access their buffer, e.g. `stdin.buffer.read()`.\n  \"\"\"\n  with _stdio_stream_as(stdin_fd,  0, 'stdin',  'r'),\\\n       _stdio_stream_as(stdout_fd, 1, 'stdout', 'w'),\\\n       _stdio_stream_as(stderr_fd, 2, 'stderr', 'w'):\n    yield"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef temporary_dir(root_dir=None, cleanup=True, suffix='', permissions=None, prefix=tempfile.template):\n  path = tempfile.mkdtemp(dir=root_dir, suffix=suffix, prefix=prefix)\n\n  try:\n    if permissions is not None:\n      os.chmod(path, permissions)\n    yield path\n  finally:\n    if cleanup:\n      shutil.rmtree(path, ignore_errors=True)", "response": "A with - context that creates a temporary directory."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef safe_file(path, suffix=None, cleanup=True):\n  safe_path = '{0}.{1}'.format(path, suffix or uuid.uuid4())\n  if os.path.exists(path):\n    shutil.copy(path, safe_path)\n  try:\n    yield safe_path\n    if cleanup:\n      shutil.move(safe_path, path)\n    else:\n      shutil.copy(safe_path, path)\n  finally:\n    if cleanup:\n      safe_delete(safe_path)", "response": "A with - context that copies a file and copies it back to the original file on success."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pushd(directory):\n  cwd = os.getcwd()\n  os.chdir(directory)\n  try:\n    yield directory\n  finally:\n    os.chdir(cwd)", "response": "A with - context that encapsulates pushd and popd."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef open_zip(path_or_file, *args, **kwargs):\n  if not path_or_file:\n    raise InvalidZipPath('Invalid zip location: {}'.format(path_or_file))\n  allowZip64 = kwargs.pop('allowZip64', True)\n  try:\n    zf = zipfile.ZipFile(path_or_file, *args, allowZip64=allowZip64, **kwargs)\n  except zipfile.BadZipfile as bze:\n    # Use the realpath in order to follow symlinks back to the problem source file.\n    raise zipfile.BadZipfile(\"Bad Zipfile {0}: {1}\".format(os.path.realpath(path_or_file), bze))\n  try:\n    yield zf\n  finally:\n    zf.close()", "response": "A with - context for zip files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a tuple of the plugins registered with the python style checker.", "response": "def plugins():\n  \"\"\"Returns a tuple of the plugin classes registered with the python style checker.\n\n  :rtype: tuple of :class:`pants.contrib.python.checks.checker.common.CheckstylePlugin` subtypes\n  \"\"\"\n  return (\n    ClassFactoring,\n    ConstantLogic,\n    ExceptStatements,\n    FutureCompatibility,\n    ImportOrder,\n    Indentation,\n    MissingContextManager,\n    NewStyleClasses,\n    Newlines,\n    PrintStatements,\n    TrailingWhitespace,\n    PEP8VariableNames,\n    PyflakesChecker,\n    PyCodeStyleChecker,\n  )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\niterate over the instances style checker and yield Nits.", "response": "def _get_nits(self, filename):\n    \"\"\"Iterate over the instances style checker and yield Nits.\n\n    :param filename: str pointing to a file within the buildroot.\n    \"\"\"\n    try:\n      python_file = PythonFile.parse(filename, root=self._root_dir)\n    except CheckSyntaxError as e:\n      yield e.as_nit()\n      return\n\n    if noqa_file_filter(python_file):\n      return\n\n    if self._excluder:\n      # Filter out any suppressed plugins\n      check_plugins = [(plugin_name, plugin_factory)\n                       for plugin_name, plugin_factory in self._plugin_factories.items()\n                       if self._excluder.should_include(filename, plugin_name)]\n    else:\n      check_plugins = self._plugin_factories.items()\n\n    for plugin_name, plugin_factory in check_plugins:\n      for i, nit in enumerate(plugin_factory(python_file)):\n        if i == 0:\n          # NB: Add debug log header for nits from each plugin, but only if there are nits from it.\n          self.log.debug('Nits from plugin {} for {}'.format(plugin_name, filename))\n\n        if not nit.has_lines_to_display:\n          yield nit\n          continue\n\n        if all(not line_contains_noqa(line) for line in nit.lines):\n          yield nit"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _check_file(self, filename):\n    # If the user specifies an invalid severity use comment.\n    log_threshold = Nit.SEVERITY.get(self._severity, Nit.COMMENT)\n\n    failure_count = 0\n    fail_threshold = Nit.WARNING if self._strict else Nit.ERROR\n\n    for i, nit in enumerate(self._get_nits(filename)):\n      if i == 0:\n        print()  # Add an extra newline to clean up the output only if we have nits.\n      if nit.severity >= log_threshold:\n        print('{nit}\\n'.format(nit=nit))\n      if nit.severity >= fail_threshold:\n        failure_count += 1\n    return failure_count", "response": "Process a python file looking for problems."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\niterates over sources and run checker on each file.", "response": "def checkstyle(self, sources):\n    \"\"\"Iterate over sources and run checker on each file.\n\n    Files can be suppressed with a --suppress option which takes an xml file containing\n    file paths that have exceptions and the plugins they need to ignore.\n\n    :param sources: iterable containing source file names.\n    :return: (int) number of failures\n    \"\"\"\n    failure_count = 0\n    for filename in sources:\n      failure_count += self._check_file(filename)\n    return failure_count"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_urls(self, _version, host_platform):\n    binary_path = self._binary_request.get_download_path(host_platform)\n    return [posixpath.join(baseurl, binary_path) for baseurl in self._baseurls]", "response": "Generate the urls to download for this binary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _select_binary_stream(self, name, urls):\n    downloaded_successfully = False\n    accumulated_errors = []\n    for url in OrderedSet(urls):  # De-dup URLS: we only want to try each URL once.\n      logger.info('Attempting to fetch {name} binary from: {url} ...'.format(name=name, url=url))\n      try:\n        with temporary_file() as dest:\n          logger.debug(\"in BinaryToolFetcher: url={}, timeout_secs={}\"\n                       .format(url, self._timeout_secs))\n          self._fetcher.download(url,\n                                 listener=Fetcher.ProgressListener(),\n                                 path_or_fd=dest,\n                                 timeout_secs=self._timeout_secs)\n          logger.info('Fetched {name} binary from: {url} .'.format(name=name, url=url))\n          downloaded_successfully = True\n          dest.seek(0)\n          yield dest\n          break\n      except (IOError, Fetcher.Error, ValueError) as e:\n        accumulated_errors.append('Failed to fetch binary from {url}: {error}'\n                                  .format(url=url, error=e))\n    if not downloaded_successfully:\n      raise self.BinaryNotFound(name, accumulated_errors)", "response": "Download a file from a list of urls yielding a stream after downloading the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fetch_binary(self, fetch_request):\n    bootstrap_dir = os.path.realpath(os.path.expanduser(self._bootstrap_dir))\n    bootstrapped_binary_path = os.path.join(bootstrap_dir, fetch_request.download_path)\n    logger.debug(\"bootstrapped_binary_path: {}\".format(bootstrapped_binary_path))\n    file_name = fetch_request.file_name\n    urls = fetch_request.urls\n\n    if self._ignore_cached_download or not os.path.exists(bootstrapped_binary_path):\n      self._do_fetch(bootstrapped_binary_path, file_name, urls)\n\n    logger.debug('Selected {binary} binary bootstrapped to: {path}'\n                 .format(binary=file_name, path=bootstrapped_binary_path))\n    return bootstrapped_binary_path", "response": "Fulfill a binary fetch request."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfetches a file unpacking it if necessary.", "response": "def select(self, binary_request):\n    \"\"\"Fetches a file, unpacking it if necessary.\"\"\"\n\n    logger.debug(\"binary_request: {!r}\".format(binary_request))\n\n    try:\n      download_path = self._get_download_path(binary_request)\n    except self.MissingMachineInfo as e:\n      raise self.BinaryResolutionError(binary_request, e)\n\n    try:\n      url_generator = self._get_url_generator(binary_request)\n    except self.NoBaseUrlsError as e:\n      raise self.BinaryResolutionError(binary_request, e)\n\n    urls = self._get_urls(url_generator, binary_request)\n    if not isinstance(urls, list):\n      # TODO: add test for this error!\n      raise self.BinaryResolutionError(\n        binary_request,\n        TypeError(\"urls must be a list: was '{}'.\".format(urls)))\n    fetch_request = BinaryFetchRequest(\n      download_path=download_path,\n      urls=urls)\n\n    logger.debug(\"fetch_request: {!r}\".format(fetch_request))\n\n    try:\n      downloaded_file = self._binary_tool_fetcher.fetch_binary(fetch_request)\n    except BinaryToolFetcher.BinaryNotFound as e:\n      raise self.BinaryResolutionError(binary_request, e)\n\n    # NB: we mark the downloaded file executable if it is not an archive.\n    archiver = binary_request.archiver\n    if archiver is None:\n      chmod_plus_x(downloaded_file)\n      return downloaded_file\n\n    download_dir = os.path.dirname(downloaded_file)\n    # Use the 'name' given in the request as the directory name to extract to.\n    unpacked_dirname = os.path.join(download_dir, binary_request.name)\n    if not os.path.isdir(unpacked_dirname):\n      logger.info(\"Extracting {} to {} .\".format(downloaded_file, unpacked_dirname))\n      archiver.extract(downloaded_file, unpacked_dirname, concurrency_safe=True)\n    return unpacked_dirname"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsplit a string using shell - like syntax.", "response": "def safe_shlex_split(text_or_binary):\n  \"\"\"Split a string using shell-like syntax.\n\n  Safe even on python versions whose shlex.split() method doesn't accept unicode.\n  \"\"\"\n  value = ensure_text(text_or_binary) if PY3 else ensure_binary(text_or_binary)\n  return shlex.split(value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_path_env_var(new_entries, env=None, env_var='PATH', delimiter=':', prepend=False):\n  if env is None:\n    env = {}\n\n  prev_path = env.get(env_var, None)\n  if prev_path is None:\n    path_dirs = list()\n  else:\n    path_dirs = list(prev_path.split(delimiter))\n\n  new_entries_list = list(new_entries)\n\n  if prepend:\n    path_dirs = new_entries_list + path_dirs\n  else:\n    path_dirs += new_entries_list\n\n  return delimiter.join(path_dirs)", "response": "Join path entries combining with an environment variable if specified."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npluralize the item_type and returns the count and inflected item_type together.", "response": "def pluralize(count, item_type):\n  \"\"\"Pluralizes the item_type if the count does not equal one.\n\n  For example `pluralize(1, 'apple')` returns '1 apple',\n  while `pluralize(0, 'apple') returns '0 apples'.\n\n  :return The count and inflected item_type together as a string\n  :rtype string\n  \"\"\"\n  def pluralize_string(x):\n    if x.endswith('s'):\n      return x + 'es'\n    else:\n      return x + 's'\n\n  text = '{} {}'.format(count, item_type if count == 1 else pluralize_string(item_type))\n  return text"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef strip_prefix(string, prefix):\n  if string.startswith(prefix):\n    return string[len(prefix):]\n  else:\n    return string", "response": "Returns a copy of the string with the multi - character prefix stripped."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dictionary of keyword arguments to construct a TaskRule that will construct the target Optionable.", "response": "def signature(cls):\n    \"\"\"Returns kwargs to construct a `TaskRule` that will construct the target Optionable.\n\n    TODO: This indirection avoids a cycle between this module and the `rules` module.\n    \"\"\"\n    snake_scope = cls.options_scope.replace('-', '_')\n    partial_construct_optionable = functools.partial(_construct_optionable, cls)\n    partial_construct_optionable.__name__ = 'construct_scope_{}'.format(snake_scope)\n    return dict(\n        output_type=cls.optionable_cls,\n        input_selectors=tuple(),\n        func=partial_construct_optionable,\n        input_gets=(Get.create_statically_for_rule_graph(ScopedOptions, Scope),),\n        dependency_optionables=(cls.optionable_cls,),\n      )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a ScopeInfo instance representing this Optionable s options scope.", "response": "def get_scope_info(cls):\n    \"\"\"Returns a ScopeInfo instance representing this Optionable's options scope.\"\"\"\n    if cls.options_scope is None or cls.options_scope_category is None:\n      raise OptionsError(\n        '{} must set options_scope and options_scope_category.'.format(cls.__name__))\n    return ScopeInfo(cls.options_scope, cls.options_scope_category, cls)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_fileset_with_spec(cls, rel_path, *patterns, **kwargs):\n    for pattern in patterns:\n      if not isinstance(pattern, string_types):\n        raise ValueError(\"Expected string patterns for {}: got {}\".format(cls.__name__, patterns))\n\n    raw_exclude = kwargs.pop('exclude', [])\n    buildroot = get_buildroot()\n    root = os.path.normpath(os.path.join(buildroot, rel_path))\n\n    # making sure there are no unknown arguments.\n    unknown_args = set(kwargs.keys()) - cls.KNOWN_PARAMETERS\n\n    if unknown_args:\n      raise ValueError('Unexpected arguments while parsing globs: {}'.format(\n        ', '.join(unknown_args)))\n\n    for glob in patterns:\n      if cls._is_glob_dir_outside_root(glob, root):\n        raise ValueError('Invalid glob {}, points outside BUILD file root {}'.format(glob, root))\n\n    exclude = cls.process_raw_exclude(raw_exclude)\n\n    files_calculator = cls._file_calculator(root, patterns, kwargs, exclude)\n\n    rel_root = fast_relpath(root, buildroot)\n    if rel_root == '.':\n      rel_root = ''\n    filespec = cls.to_filespec(patterns, root=rel_root, exclude=exclude)\n\n    return LazyFilesetWithSpec(rel_root, filespec, files_calculator)", "response": "Create a new lazy fileset with a given set of patterns."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dict representation of this glob list relative to the buildroot.", "response": "def to_filespec(cls, args, root='', exclude=None):\n    \"\"\"Return a dict representation of this glob list, relative to the buildroot.\n\n    The format of the dict is {'globs': [ 'list', 'of' , 'strings' ]\n                    (optional) 'exclude' : [{'globs' : ... }, ...] }\n\n    The globs are in zglobs format.\n    \"\"\"\n    result = {'globs': [os.path.join(root, arg) for arg in args]}\n    if exclude:\n      result['exclude'] = []\n      for exclude in exclude:\n        if hasattr(exclude, 'filespec'):\n          result['exclude'].append(exclude.filespec)\n        else:\n          result['exclude'].append({'globs': [os.path.join(root, x) for x in exclude]})\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexecute a given run with a pants runner.", "response": "def _run_pants(self, sock, arguments, environment):\n    \"\"\"Execute a given run with a pants runner.\"\"\"\n    self.server.runner_factory(sock, arguments, environment).run()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle(self):\n    # Parse the Nailgun request portion.\n    _, _, arguments, environment = NailgunProtocol.parse_request(self.request)\n\n    # N.B. the first and second nailgun request arguments (working_dir and command) are currently\n    # ignored in favor of a get_buildroot() call within LocalPantsRunner.run() and an assumption\n    # that anyone connecting to this nailgun server always intends to run pants itself.\n\n    # Prepend the command to our arguments so it aligns with the expected sys.argv format of python\n    # (e.g. [list', '::'] -> ['./pants', 'list', '::']).\n    arguments.insert(0, './pants')\n\n    self.logger.info('handling pailgun request: `{}`'.format(' '.join(arguments)))\n    self.logger.debug('pailgun request environment: %s', environment)\n\n    # Execute the requested command with optional daemon-side profiling.\n    with maybe_profiled(environment.get('PANTSD_PROFILE')):\n      self._run_pants(self.request, arguments, environment)\n\n    # NB: This represents the end of pantsd's involvement in the request, but the request will\n    # continue to run post-fork.\n    self.logger.info('pailgun request completed: `{}`'.format(' '.join(arguments)))", "response": "Handle a single Pailgun request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle_error(self, exc=None):\n    if exc:\n      NailgunProtocol.send_stderr(self.request, traceback.format_exc())\n    failure_code = 1\n    NailgunProtocol.send_exit_with_code(self.request, failure_code)", "response": "Error handler for failed calls to handle()."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\noverrides of TCPServer. server_bind that tracks bind - time assigned random ports.", "response": "def server_bind(self):\n    \"\"\"Override of TCPServer.server_bind() that tracks bind-time assigned random ports.\"\"\"\n    TCPServer.server_bind(self)\n    _, self.server_port = self.socket.getsockname()[:2]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts a new thread to process the request.", "response": "def process_request(self, request, client_address):\n    \"\"\"Start a new thread to process the request.\n\n    This is lovingly copied and pasted from ThreadingMixIn, with the addition of setting the name\n    of the thread. It's a shame that ThreadingMixIn doesn't provide a customization hook.\n    \"\"\"\n    t = threading.Thread(\n      target=self.process_request_thread,\n      args=(request, client_address),\n      name=\"PailgunRequestThread\",\n    )\n    t.daemon = self.daemon_threads\n    t.start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\noverrides of TCPServer. handle_request that provides locking.", "response": "def handle_request(self):\n    \"\"\"Override of TCPServer.handle_request() that provides locking.\n\n    Calling this method has the effect of \"maybe\" (if the socket does not time out first)\n    accepting a request and (because we mixin in ThreadingMixIn) spawning it on a thread. It should\n    always return within `min(self.timeout, socket.gettimeout())`.\n\n    N.B. Most of this is copied verbatim from SocketServer.py in the stdlib.\n    \"\"\"\n    timeout = self.socket.gettimeout()\n    if timeout is None:\n      timeout = self.timeout\n    elif self.timeout is not None:\n      timeout = min(timeout, self.timeout)\n    fd_sets = safe_select([self], [], [], timeout)\n    if not fd_sets[0]:\n      self.handle_timeout()\n      return\n\n    # After select tells us we can safely accept, guard the accept and request\n    # handling with the lifecycle lock to avoid abrupt teardown mid-request.\n    with self.lifecycle_lock():\n      self._handle_request_noblock()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_request_thread(self, request, client_address):\n    # Instantiate the request handler.\n    handler = self.RequestHandlerClass(request, client_address, self)\n    try:\n      # Attempt to handle a request with the handler.\n      handler.handle_request()\n      self.request_complete_callback()\n\n    except Exception as e:\n      # If that fails, (synchronously) handle the error with the error handler sans-fork.\n      try:\n        handler.handle_error(e)\n      finally:\n        # Shutdown the socket since we don't expect a fork() in the exception context.\n        self.shutdown_request(request)\n    else:\n      # At this point, we expect a fork() has taken place - the parent side will return, and so we\n      # close the request here from the parent without explicitly shutting down the socket. The\n      # child half of this will perform an os._exit() before it gets to this point and is also\n      # responsible for shutdown and closing of the socket when its execution is complete.\n      self.close_request(request)", "response": "Override of ThreadingMixIn. process_request_thread that delegates to the request handler."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _generate_to_tempfile(self, generator):\n    with temporary_file(cleanup=False, binary_mode=False) as output:\n      generator.write(output)\n      return output.name", "response": "Applies the specified generator to a temp file and returns the path to that file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_include_paths(self, target):\n    paths = OrderedSet()\n    paths.add(os.path.join(get_buildroot(), target.target_base))\n\n    def collect_paths(dep):\n      if not dep.has_sources('.thrift'):\n        return\n      paths.add(os.path.join(get_buildroot(), dep.target_base))\n\n    collect_paths(target)\n    target.walk(collect_paths)\n    return paths", "response": "Computes the set of paths that thrifty uses to lookup imports."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _instance_from_process(self, process):\n    return ProcessManager(name=process.name(),\n                          pid=process.pid,\n                          process_name=process.name(),\n                          metadata_base_dir=self._metadata_base_dir)", "response": "Default converter from psutil. Process to process instance classes for subclassing."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iter_processes(self, proc_filter=None):\n    with swallow_psutil_exceptions():  # process_iter may raise\n      for proc in psutil.process_iter():\n        with swallow_psutil_exceptions():  # proc_filter may raise\n          if (proc_filter is None) or proc_filter(proc):\n            yield proc", "response": "Yields processes from psutil. process_iter with an optional filter and swallows psutil errors."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes a function or closure until a True condition or timeout is met.", "response": "def _deadline_until(cls, closure, action_msg, timeout=FAIL_WAIT_SEC,\n                      wait_interval=WAIT_INTERVAL_SEC, info_interval=INFO_INTERVAL_SEC):\n    \"\"\"Execute a function/closure repeatedly until a True condition or timeout is met.\n\n    :param func closure: the function/closure to execute (should not block for long periods of time\n                         and must return True on success).\n    :param str action_msg: a description of the action that is being executed, to be rendered as\n                           info while we wait, and as part of any rendered exception.\n    :param float timeout: the maximum amount of time to wait for a true result from the closure in\n                          seconds. N.B. this is timing based, so won't be exact if the runtime of\n                          the closure exceeds the timeout.\n    :param float wait_interval: the amount of time to sleep between closure invocations.\n    :param float info_interval: the amount of time to wait before and between reports via info\n                                logging that we're still waiting for the closure to succeed.\n    :raises: :class:`ProcessManager.Timeout` on execution timeout.\n    \"\"\"\n    now = time.time()\n    deadline = now + timeout\n    info_deadline = now + info_interval\n    while 1:\n      if closure():\n        return True\n\n      now = time.time()\n      if now > deadline:\n        raise cls.Timeout('exceeded timeout of {} seconds while waiting for {}'.format(timeout, action_msg))\n\n      if now > info_deadline:\n        logger.info('waiting for {}...'.format(action_msg))\n        info_deadline = info_deadline + info_interval\n      elif wait_interval:\n        time.sleep(wait_interval)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwaits up to timeout seconds for a file to appear with a non - zero size or raise Timeout", "response": "def _wait_for_file(cls, filename, timeout=FAIL_WAIT_SEC, want_content=True):\n    \"\"\"Wait up to timeout seconds for filename to appear with a non-zero size or raise Timeout().\"\"\"\n    def file_waiter():\n      return os.path.exists(filename) and (not want_content or os.path.getsize(filename))\n\n    action_msg = 'file {} to appear'.format(filename)\n    return cls._deadline_until(file_waiter, action_msg, timeout=timeout)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _maybe_init_metadata_dir_by_name(self, name):\n    safe_mkdir(self.__class__._get_metadata_dir_by_name(name, self._metadata_base_dir))", "response": "Initialize the metadata directory for a named identity if it doesn t exist."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading process metadata using a named identity.", "response": "def read_metadata_by_name(self, name, metadata_key, caster=None):\n    \"\"\"Read process metadata using a named identity.\n\n    :param string name: The ProcessMetadataManager identity/name (e.g. 'pantsd').\n    :param string metadata_key: The metadata key (e.g. 'pid').\n    :param func caster: A casting callable to apply to the read value (e.g. `int`).\n    \"\"\"\n    file_path = self._metadata_file_path(name, metadata_key)\n    try:\n      metadata = read_file(file_path).strip()\n      return self._maybe_cast(metadata, caster)\n    except (IOError, OSError):\n      return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_metadata_by_name(self, name, metadata_key, metadata_value):\n    self._maybe_init_metadata_dir_by_name(name)\n    file_path = self._metadata_file_path(name, metadata_key)\n    safe_file_dump(file_path, metadata_value)", "response": "Write process metadata using a named identity."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef await_metadata_by_name(self, name, metadata_key, timeout, caster=None):\n    file_path = self._metadata_file_path(name, metadata_key)\n    self._wait_for_file(file_path, timeout=timeout)\n    return self.read_metadata_by_name(name, metadata_key, caster)", "response": "Block up to a timeout for process metadata to arrive on disk."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npurges a processes metadata directory by name.", "response": "def purge_metadata_by_name(self, name):\n    \"\"\"Purge a processes metadata directory.\n\n    :raises: `ProcessManager.MetadataError` when OSError is encountered on metadata dir removal.\n    \"\"\"\n    meta_dir = self._get_metadata_dir_by_name(name, self._metadata_base_dir)\n    logger.debug('purging metadata directory: {}'.format(meta_dir))\n    try:\n      rm_rf(meta_dir)\n    except OSError as e:\n      raise ProcessMetadataManager.MetadataError('failed to purge metadata directory {}: {!r}'.format(meta_dir, e))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef socket(self):\n    return self._socket or self.read_metadata_by_name(self._name, 'socket', self._socket_type)", "response": "The running processes socket information or None if not set."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the output of a command in a sequence of strings.", "response": "def get_subprocess_output(cls, command, ignore_stderr=True, **kwargs):\n    \"\"\"Get the output of an executed command.\n\n    :param command: An iterable representing the command to execute (e.g. ['ls', '-al']).\n    :param ignore_stderr: Whether or not to ignore stderr output vs interleave it with stdout.\n    :raises: `ProcessManager.ExecutionError` on `OSError` or `CalledProcessError`.\n    :returns: The output of the command.\n    \"\"\"\n    if ignore_stderr is False:\n      kwargs.setdefault('stderr', subprocess.STDOUT)\n\n    try:\n      return subprocess.check_output(command, **kwargs).decode('utf-8').strip()\n    except (OSError, subprocess.CalledProcessError) as e:\n      subprocess_output = getattr(e, 'output', '').strip()\n      raise cls.ExecutionError(str(e), subprocess_output)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwaiting up to a given timeout for a process to write socket info.", "response": "def await_socket(self, timeout):\n    \"\"\"Wait up to a given timeout for a process to write socket info.\"\"\"\n    return self.await_metadata_by_name(self._name, 'socket', timeout, self._socket_type)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting the current processes PID to the pidfile location", "response": "def write_pid(self, pid=None):\n    \"\"\"Write the current processes PID to the pidfile location\"\"\"\n    pid = pid or os.getpid()\n    self.write_metadata_by_name(self._name, 'pid', str(pid))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_named_socket(self, socket_name, socket_info):\n    self.write_metadata_by_name(self._name, 'socket_{}'.format(socket_name), str(socket_info))", "response": "A multi - tenant named alternative to ProcessManager. write_socket."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a psutil. Process object wrapping our pid.", "response": "def _as_process(self):\n    \"\"\"Returns a psutil `Process` object wrapping our pid.\n\n    NB: Even with a process object in hand, subsequent method calls against it can always raise\n    `NoSuchProcess`.  Care is needed to document the raises in the public API or else trap them and\n    do something sensible for the API.\n\n    :returns: a psutil Process object or else None if we have no pid.\n    :rtype: :class:`psutil.Process`\n    :raises: :class:`psutil.NoSuchProcess` if the process identified by our pid has died.\n    \"\"\"\n    if self._process is None and self.pid:\n      self._process = psutil.Process(self.pid)\n    return self._process"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a boolean indicating whether the process is running or not.", "response": "def is_alive(self, extended_check=None):\n    \"\"\"Return a boolean indicating whether the process is running or not.\n\n    :param func extended_check: An additional callable that will be invoked to perform an extended\n                                liveness check. This callable should take a single argument of a\n                                `psutil.Process` instance representing the context-local process\n                                and return a boolean True/False to indicate alive vs not alive.\n    \"\"\"\n    try:\n      process = self._as_process()\n      return not (\n        # Can happen if we don't find our pid.\n        (not process) or\n        # Check for walkers.\n        (process.status() == psutil.STATUS_ZOMBIE) or\n        # Check for stale pids.\n        (self.process_name and self.process_name != process.name()) or\n        # Extended checking.\n        (extended_check and not extended_check(process))\n      )\n    except (psutil.NoSuchProcess, psutil.AccessDenied):\n      # On some platforms, accessing attributes of a zombie'd Process results in NoSuchProcess.\n      return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninstance - based version of ProcessMetadataManager. purge_metadata_by_name that checks for process liveness before purging metadata.", "response": "def purge_metadata(self, force=False):\n    \"\"\"Instance-based version of ProcessMetadataManager.purge_metadata_by_name() that checks\n    for process liveness before purging metadata.\n\n    :param bool force: If True, skip process liveness check before purging metadata.\n    :raises: `ProcessManager.MetadataError` when OSError is encountered on metadata dir removal.\n    \"\"\"\n    if not force and self.is_alive():\n      raise ProcessMetadataManager.MetadataError('cannot purge metadata for a running process!')\n\n    super(ProcessManager, self).purge_metadata_by_name(self._name)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a signal to the current process.", "response": "def _kill(self, kill_sig):\n    \"\"\"Send a signal to the current process.\"\"\"\n    if self.pid:\n      os.kill(self.pid, kill_sig)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef terminate(self, signal_chain=KILL_CHAIN, kill_wait=KILL_WAIT_SEC, purge=True):\n    alive = self.is_alive()\n    if alive:\n      logger.debug('terminating {}'.format(self._name))\n      for signal_type in signal_chain:\n        pid = self.pid\n        try:\n          logger.debug('sending signal {} to pid {}'.format(signal_type, pid))\n          self._kill(signal_type)\n        except OSError as e:\n          logger.warning('caught OSError({e!s}) during attempt to kill -{signal} {pid}!'\n                         .format(e=e, signal=signal_type, pid=pid))\n\n        # Wait up to kill_wait seconds to terminate or move onto the next signal.\n        try:\n          if self._deadline_until(self.is_dead, 'daemon to exit', timeout=kill_wait):\n            alive = False\n            logger.debug('successfully terminated pid {}'.format(pid))\n            break\n        except self.Timeout:\n          # Loop to the next kill signal on timeout.\n          pass\n\n    if alive:\n      raise ProcessManager.NonResponsiveProcess('failed to kill pid {pid} with signals {chain}'\n                                      .format(pid=self.pid, chain=signal_chain))\n\n    if purge:\n      self.purge_metadata(force=True)", "response": "Terminate a process by sending a chain of kill signals."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nperform a double-fork, execute callbacks and write the child pid file. The double-fork here is necessary to truly daemonize the subprocess such that it can never take control of a tty. The initial fork and setsid() creates a new, isolated process group and also makes the first child a session leader (which can still acquire a tty). By forking a second time, we ensure that the second child can never acquire a controlling terminal because it's no longer a session leader - but it now has its own separate process group. Additionally, a normal daemon implementation would typically perform an os.umask(0) to reset the processes file mode creation mask post-fork. We do not do this here (and in daemon_spawn below) due to the fact that the daemons that pants would run are typically personal user daemons. Having a disparate umask from pre-vs-post fork causes files written in each phase to differ in their permissions without good reason - in this case, we want to inherit the umask. :param fork_context: A function which accepts and calls a function that will call fork. This is not a contextmanager/generator because that would make interacting with native code more challenging. If no fork_context is passed, the fork function is called directly.", "response": "def daemonize(self, pre_fork_opts=None, post_fork_parent_opts=None, post_fork_child_opts=None,\n                fork_context=None, write_pid=True):\n    \"\"\"Perform a double-fork, execute callbacks and write the child pid file.\n\n    The double-fork here is necessary to truly daemonize the subprocess such that it can never\n    take control of a tty. The initial fork and setsid() creates a new, isolated process group\n    and also makes the first child a session leader (which can still acquire a tty). By forking a\n    second time, we ensure that the second child can never acquire a controlling terminal because\n    it's no longer a session leader - but it now has its own separate process group.\n\n    Additionally, a normal daemon implementation would typically perform an os.umask(0) to reset\n    the processes file mode creation mask post-fork. We do not do this here (and in daemon_spawn\n    below) due to the fact that the daemons that pants would run are typically personal user\n    daemons. Having a disparate umask from pre-vs-post fork causes files written in each phase to\n    differ in their permissions without good reason - in this case, we want to inherit the umask.\n\n    :param fork_context: A function which accepts and calls a function that will call fork. This\n      is not a contextmanager/generator because that would make interacting with native code more\n      challenging. If no fork_context is passed, the fork function is called directly.\n    \"\"\"\n\n    def double_fork():\n      logger.debug('forking %s', self)\n      pid = os.fork()\n      if pid == 0:\n        os.setsid()\n        second_pid = os.fork()\n        if second_pid == 0:\n          return False, True\n        else:\n          if write_pid: self.write_pid(second_pid)\n          return False, False\n      else:\n        # This prevents un-reaped, throw-away parent processes from lingering in the process table.\n        os.waitpid(pid, 0)\n        return True, False\n\n    fork_func = functools.partial(fork_context, double_fork) if fork_context else double_fork\n\n    # Perform the double fork (optionally under the fork_context). Three outcomes are possible after\n    # the double fork: we're either the original parent process, the middle double-fork process, or\n    # the child. We assert below that a process is not somehow both the parent and the child.\n    self.purge_metadata()\n    self.pre_fork(**pre_fork_opts or {})\n    is_parent, is_child = fork_func()\n\n    try:\n      if not is_parent and not is_child:\n        # Middle process.\n        os._exit(0)\n      elif is_parent:\n        assert not is_child\n        self.post_fork_parent(**post_fork_parent_opts or {})\n      else:\n        assert not is_parent\n        os.chdir(self._buildroot)\n        self.post_fork_child(**post_fork_child_opts or {})\n    except Exception:\n      logger.critical(traceback.format_exc())\n      os._exit(0)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform a single - fork to run a subprocess and write the child pid file.", "response": "def daemon_spawn(self, pre_fork_opts=None, post_fork_parent_opts=None, post_fork_child_opts=None):\n    \"\"\"Perform a single-fork to run a subprocess and write the child pid file.\n\n    Use this if your post_fork_child block invokes a subprocess via subprocess.Popen(). In this\n    case, a second fork such as used in daemonize() is extraneous given that Popen() also forks.\n    Using this daemonization method vs daemonize() leaves the responsibility of writing the pid\n    to the caller to allow for library-agnostic flexibility in subprocess execution.\n    \"\"\"\n    self.purge_metadata()\n    self.pre_fork(**pre_fork_opts or {})\n    pid = os.fork()\n    if pid == 0:\n      try:\n        os.setsid()\n        os.chdir(self._buildroot)\n        self.post_fork_child(**post_fork_child_opts or {})\n      except Exception:\n        logger.critical(traceback.format_exc())\n      finally:\n        os._exit(0)\n    else:\n      try:\n        self.post_fork_parent(**post_fork_parent_opts or {})\n      except Exception:\n        logger.critical(traceback.format_exc())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a psutil. Process. cmdline parse and return a fingerprint.", "response": "def parse_fingerprint(self, cmdline, key=None, sep=None):\n    \"\"\"Given a psutil.Process.cmdline, parse and return a fingerprint.\n\n    :param list cmdline: The psutil.Process.cmdline of the current process.\n    :param string key: The key for fingerprint discovery.\n    :param string sep: The key/value separator for fingerprint discovery.\n    :returns: The parsed fingerprint or `None`.\n    :rtype: string or `None`\n    \"\"\"\n    key = key or self.FINGERPRINT_CMD_KEY\n    if key:\n      sep = sep or self.FINGERPRINT_CMD_SEP\n      cmdline = cmdline or []\n      for cmd_part in cmdline:\n        if cmd_part.startswith('{}{}'.format(key, sep)):\n          return cmd_part.split(sep)[1]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an intermediate target.", "response": "def _create_intermediate_target(self, address, suffix):\n    \"\"\"\n    :param string address: A target address.\n    :param string suffix: A string used as a suffix of the intermediate target name.\n    :returns: The address of a synthetic intermediary target.\n    \"\"\"\n    if not isinstance(address, string_types):\n      raise self.ExpectedAddressError(\"Expected string address argument, got type {type}\"\n                                      .format(type=type(address)))\n\n    address = Address.parse(address, self._parse_context.rel_path)\n    # NB(gmalmquist): Ideally there should be a way to indicate that these targets are synthetic\n    # and shouldn't show up in `./pants list` etc, because we really don't want people to write\n    # handwritten dependencies on them. For now just give them names containing \"-unstable-\" as a\n    # hint.\n    hash_str = hash_target(str(address), suffix)\n    name = '{name}-unstable-{suffix}-{index}'.format(\n      name=address.target_name,\n      suffix=suffix.replace(' ', '.'),\n      index=hash_str,\n    )\n\n    self._parse_context.create_object_if_not_exists(\n      'target',\n      name=name,\n      dependencies=[address.spec],\n      **self.extra_target_arguments\n    )\n\n    return ':{}'.format(name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef register_options(cls, register):\n    super(RunPrepCommandBase, cls).register_options(register)\n    PrepCommand.add_allowed_goal(cls.goal)", "response": "Register options for this optionable."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlisting of specs to resolve to jar_libraries containing more jars.", "response": "def library_specs(self):\n    \"\"\"Lists of specs to resolve to jar_libraries containing more jars.\"\"\"\n    return [Address.parse(spec, relative_to=self.address.spec_path).spec\n            for spec in self.payload.library_specs]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an env dict that represents a proper Go environment mapping for this distribution.", "response": "def go_env(self, gopath=None):\n    \"\"\"Return an env dict that represents a proper Go environment mapping for this distribution.\"\"\"\n    # Forcibly nullify the GOPATH if the command does not need one - this can prevent bad user\n    # GOPATHs from erroring out commands; see: https://github.com/pantsbuild/pants/issues/2321.\n    # NB: As of go 1.8, when GOPATH is unset (set to ''), it defaults to ~/go (assuming HOME is\n    # set - and we can't unset that since it might legitimately be used by the subcommand); so we\n    # set the GOPATH here to a valid value that nonetheless will fail to work if GOPATH is\n    # actually used by the subcommand.\n    no_gopath = os.devnull\n    return OrderedDict(GOROOT=self.goroot, GOPATH=gopath or no_gopath)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a Go command that is optionally targeted to a Go workspace.", "response": "def create_go_cmd(self, cmd, gopath=None, args=None):\n    \"\"\"Creates a Go command that is optionally targeted to a Go workspace.\n\n    :param string cmd: Go command to execute, e.g. 'test' for `go test`\n    :param string gopath: An optional $GOPATH which points to a valid Go workspace from which to run\n                          the command.\n    :param list args: A list of arguments and flags to pass to the Go command.\n    :returns: A go command that can be executed later.\n    :rtype: :class:`GoDistribution.GoCommand`\n    \"\"\"\n    return self.GoCommand._create(self.goroot, cmd, go_env=self.go_env(gopath=gopath), args=args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns a Go command that is optionally targeted to a Go workspace.", "response": "def execute_go_cmd(self, cmd, gopath=None, args=None, env=None,\n                     workunit_factory=None, workunit_name=None, workunit_labels=None, **kwargs):\n    \"\"\"Runs a Go command that is optionally targeted to a Go workspace.\n\n    If a `workunit_factory` is supplied the command will run in a work unit context.\n\n    :param string cmd: Go command to execute, e.g. 'test' for `go test`\n    :param string gopath: An optional $GOPATH which points to a valid Go workspace from which to run\n                          the command.\n    :param list args: An optional list of arguments and flags to pass to the Go command.\n    :param dict env: A custom environment to launch the Go command in.  If `None` the current\n                     environment is used.\n    :param workunit_factory: An optional callable that can produce a `WorkUnit` context\n    :param string workunit_name: An optional name for the work unit; defaults to the `cmd`\n    :param list workunit_labels: An optional sequence of labels for the work unit.\n    :param kwargs: Keyword arguments to pass through to `subprocess.Popen`.\n    :returns: A tuple of the exit code and the go command that was run.\n    :rtype: (int, :class:`GoDistribution.GoCommand`)\n    \"\"\"\n    go_cmd = self.create_go_cmd(cmd, gopath=gopath, args=args)\n    if workunit_factory is None:\n      return go_cmd.spawn(**kwargs).wait()\n    else:\n      name = workunit_name or cmd\n      labels = [WorkUnitLabel.TOOL] + (workunit_labels or [])\n      with workunit_factory(name=name, labels=labels, cmd=str(go_cmd)) as workunit:\n        process = go_cmd.spawn(env=env,\n                               stdout=workunit.output('stdout'),\n                               stderr=workunit.output('stderr'),\n                               **kwargs)\n        returncode = process.wait()\n        workunit.set_outcome(WorkUnit.SUCCESS if returncode == 0 else WorkUnit.FAILURE)\n        return returncode, go_cmd"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn internal Jarable dependencies that were directly declared.", "response": "def target_internal_dependencies(target):\n  \"\"\"Returns internal Jarable dependencies that were \"directly\" declared.\n\n  Directly declared deps are those that are explicitly listed in the definition of a\n  target, rather than being depended on transitively. But in order to walk through\n  aggregator targets such as `target`, `dependencies`, or `jar_library`, this recursively\n  descends the dep graph and stops at Jarable instances.\"\"\"\n  for dep in target.dependencies:\n    if isinstance(dep, Jarable):\n      yield dep\n    else:\n      for childdep in target_internal_dependencies(dep):\n        yield childdep"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load(path):\n    with open(path, 'r') as props:\n      properties = Properties.load(props)\n      return PushDb(properties)", "response": "Loads a pushdb maintained in a properties file at the given path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_entry(self, target):\n    db_get, _ = self._accessors_for_target(target)\n\n    major = int(db_get('revision.major', '0'))\n    minor = int(db_get('revision.minor', '0'))\n    patch = int(db_get('revision.patch', '0'))\n    snapshot = str(db_get('revision.snapshot', 'false')).lower() == 'true'\n    named_version = db_get('revision.named_version', None)\n    named_is_latest = str(db_get('revision.named_is_latest', 'false')).lower() == 'true'\n    sha = db_get('revision.sha', None)\n    fingerprint = db_get('revision.fingerprint', None)\n    sem_ver = Semver(major, minor, patch, snapshot=snapshot)\n    named_ver = Namedver(named_version) if named_version else None\n    return self.Entry(sem_ver, named_ver, named_is_latest, sha, fingerprint)", "response": "Given an internal target return a PushDb. Entry which might contain defaults."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dump(self, path):\n    with open(path, 'w') as props:\n      Properties.dump(self._props, props)", "response": "Saves the pushdb as a properties file to the given path."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _as_versioned_jar(self, internal_target):\n    jar, _ = internal_target.get_artifact_info()\n    pushdb_entry = self._get_db(internal_target).get_entry(internal_target)\n    return jar.copy(rev=pushdb_entry.version().version())", "response": "Fetches the jar representation of the given target and applies the latest pushdb version."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nasks the user if a push should be done for a particular version of a particular coordinate. Returns True if the push should be done", "response": "def confirm_push(self, coord, version):\n    \"\"\"Ask the user if a push should be done for a particular version of a\n       particular coordinate.   Return True if the push should be done\"\"\"\n    if not self.get_options().prompt:\n      return True\n    try:\n      isatty = os.isatty(sys.stdin.fileno())\n    except ValueError:\n      # In tests, sys.stdin might not have a fileno\n      isatty = False\n    if not isatty:\n      return True\n    push = input('\\nPublish {} with revision {} ? [y|N] '.format(\n      coord, version\n    ))\n    print('\\n')\n    return push.strip().lower() == 'y'"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _copy_artifact(self, tgt, jar, version, typename, suffix='', extension='jar',\n                     artifact_ext='', override_name=None):\n    \"\"\"Copy the products for a target into the artifact path for the jar/version\"\"\"\n    genmap = self.context.products.get(typename)\n    product_mapping = genmap.get(tgt)\n    if product_mapping is None:\n      raise ValueError(\"No product mapping in {} for {}. \"\n                       \"You may need to run some other task first\".format(typename, tgt))\n    for basedir, jars in product_mapping.items():\n      for artifact in jars:\n        path = self.artifact_path(jar, version, name=override_name, suffix=suffix,\n                                  extension=extension, artifact_ext=artifact_ext)\n        safe_mkdir(os.path.dirname(path))\n        shutil.copy(os.path.join(basedir, artifact), path)", "response": "Copy the artifact into the artifact path for the jar version"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the JVM options for ivy authentication if needed.", "response": "def _ivy_jvm_options(self, repo):\n    \"\"\"Get the JVM options for ivy authentication, if needed.\"\"\"\n    # Get authentication for the publish repo if needed.\n    if not repo.get('auth'):\n      # No need to copy here, as this list isn't modified by the caller.\n      return self._jvm_options\n\n    # Create a copy of the options, so that the modification is appropriately transient.\n    jvm_options = copy(self._jvm_options)\n    user = repo.get('username')\n    password = repo.get('password')\n    if user and password:\n      jvm_options.append('-Dlogin={}'.format(user))\n      jvm_options.append('-Dpassword={}'.format(password))\n    else:\n      raise TaskError('Unable to publish to {}. {}'\n                      .format(repo.get('resolver'), repo.get('help', '')))\n    return jvm_options"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns ivy to publish a jar.", "response": "def publish(self, publications, jar, entry, repo, published):\n    \"\"\"Run ivy to publish a jar.  ivyxml_path is the path to the ivy file; published\n    is a list of jars published so far (including this one). entry is a pushdb entry.\"\"\"\n\n    try:\n      ivy = Bootstrapper.default_ivy()\n    except Bootstrapper.Error as e:\n      raise TaskError('Failed to push {0}! {1}'.format(pushdb_coordinate(jar, entry), e))\n\n    path = repo.get('path')\n    ivysettings = self.generate_ivysettings(ivy, published, publish_local=path)\n\n    version = entry.version().version()\n    ivyxml = self.generate_ivy(jar, version, publications)\n\n    resolver = repo['resolver']\n    args = [\n      '-settings', ivysettings,\n      '-ivy', ivyxml,\n\n      # Without this setting, the ivy.xml is delivered to the CWD, littering the workspace.  We\n      # don't need the ivy.xml, so just give it path under the workdir we won't use.\n      '-deliverto', ivyxml + '.unused',\n\n      '-publish', resolver,\n      '-publishpattern', '{}/[organisation]/[module]/'\n                         '[artifact]-[revision](-[classifier]).[ext]'.format(self.workdir),\n      '-revision', version,\n      '-m2compatible',\n    ]\n\n    # TODO(John Sirois): global logging options should be hidden behind some sort of log manager\n    # that we can:\n    # a.) obtain a handle to (dependency injection or manual plumbing)\n    # b.) query for log detail, ie: `if log_manager.is_verbose:`\n    if self.get_options().level == 'debug':\n      args.append('-verbose')\n\n    if self.local_snapshot:\n      args.append('-overwrite')\n\n    try:\n      jvm_options = self._ivy_jvm_options(repo)\n      ivy.execute(jvm_options=jvm_options, args=args,\n                  workunit_factory=self.context.new_workunit, workunit_name='ivy-publish')\n    except Ivy.Error as e:\n      raise TaskError('Failed to push {0}! {1}'.format(pushdb_coordinate(jar, entry), e))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a doc jar if either scala or java docs are available for the given target.", "response": "def create_doc_jar(self, target, open_jar, version):\n    \"\"\"Returns a doc jar if either scala or java docs are available for the given target.\"\"\"\n    javadoc = self._java_doc(target)\n    scaladoc = self._scala_doc(target)\n    if javadoc or scaladoc:\n      jar_path = self.artifact_path(open_jar, version, suffix='-javadoc')\n      with self.open_jar(jar_path, overwrite=True, compressed=True) as open_jar:\n        def add_docs(docs):\n          if docs:\n            for basedir, doc_files in docs.items():\n              for doc_file in doc_files:\n                open_jar.write(os.path.join(basedir, doc_file), doc_file)\n\n        add_docs(javadoc)\n        add_docs(scaladoc)\n      return jar_path\n    else:\n      return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef registered_aliases(self):\n    target_factories_by_alias = self._target_by_alias.copy()\n    target_factories_by_alias.update(self._target_macro_factory_by_alias)\n    return BuildFileAliases(\n        targets=target_factories_by_alias,\n        objects=self._exposed_object_by_alias.copy(),\n        context_aware_object_factories=self._exposed_context_aware_object_factory_by_alias.copy()\n    )", "response": "Return the registered aliases exposed in BUILD files."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef register_aliases(self, aliases):\n    if not isinstance(aliases, BuildFileAliases):\n      raise TypeError('The aliases must be a BuildFileAliases, given {}'.format(aliases))\n\n    for alias, target_type in aliases.target_types.items():\n      self._register_target_alias(alias, target_type)\n\n    for alias, target_macro_factory in aliases.target_macro_factories.items():\n      self._register_target_macro_factory_alias(alias, target_macro_factory)\n\n    for alias, obj in aliases.objects.items():\n      self._register_exposed_object(alias, obj)\n\n    for alias, context_aware_object_factory in aliases.context_aware_object_factories.items():\n      self._register_exposed_context_aware_object_factory(alias, context_aware_object_factory)", "response": "Registers the given aliases to be exposed in parsed BUILD files."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef register_optionables(self, optionables):\n    if not isinstance(optionables, Iterable):\n      raise TypeError('The optionables must be an iterable, given {}'.format(optionables))\n    optionables = tuple(optionables)\n    if not optionables:\n      return\n\n    invalid_optionables = [s\n                           for s in optionables\n                           if not isinstance(s, type) or not issubclass(s, Optionable)]\n    if invalid_optionables:\n      raise TypeError('The following items from the given optionables are not Optionable '\n                      'subclasses:\\n\\t{}'.format('\\n\\t'.join(str(i) for i in invalid_optionables)))\n\n    self._optionables.update(optionables)", "response": "Registers the given subsystem types."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nregistering the given rules.", "response": "def register_rules(self, rules):\n    \"\"\"Registers the given rules.\n\n    param rules: The rules to register.\n    :type rules: :class:`collections.Iterable` containing\n                 :class:`pants.engine.rules.Rule` instances.\n    \"\"\"\n    if not isinstance(rules, Iterable):\n      raise TypeError('The rules must be an iterable, given {!r}'.format(rules))\n\n    # \"Index\" the rules to normalize them and expand their dependencies.\n    normalized_rules = RuleIndex.create(rules).normalized_rules()\n    indexed_rules = normalized_rules.rules\n    union_rules = normalized_rules.union_rules\n\n    # Store the rules and record their dependency Optionables.\n    self._rules.update(indexed_rules)\n    self._union_rules.update(union_rules)\n    dependency_optionables = {do\n                              for rule in indexed_rules\n                              for do in rule.dependency_optionables\n                              if rule.dependency_optionables}\n    self.register_optionables(dependency_optionables)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitialize a new parse state for the given build file.", "response": "def initialize_parse_state(self, build_file):\n    \"\"\"Creates a fresh parse state for the given build file.\n\n    :param build_file: The BUILD file to set up a new ParseState for.\n    :type build_file: :class:`pants.base.build_file.BuildFile`\n    :returns: A fresh ParseState for parsing the given `build_file` with.\n    :rtype: :class:`BuildConfiguration.ParseState`\n    \"\"\"\n    # TODO(John Sirois): Introduce a factory method to seal the BuildConfiguration and add a check\n    # there that all anonymous types are covered by context aware object factories that are\n    # Macro instances.  Without this, we could have non-Macro context aware object factories being\n    # asked to be a BuildFileTargetFactory when they are not (in SourceRoot registration context).\n    # See: https://github.com/pantsbuild/pants/issues/2125\n    type_aliases = self._exposed_object_by_alias.copy()\n    parse_context = ParseContext(rel_path=build_file.spec_path, type_aliases=type_aliases)\n\n    def create_call_proxy(tgt_type, tgt_alias=None):\n      def registration_callback(address, addressable):\n        parse_context._storage.add(addressable, name=address.target_name)\n      addressable_factory = self._get_addressable_factory(tgt_type, tgt_alias)\n      return AddressableCallProxy(addressable_factory=addressable_factory,\n                                  build_file=build_file,\n                                  registration_callback=registration_callback)\n\n    # Expose all aliased Target types.\n    for alias, target_type in self._target_by_alias.items():\n      proxy = create_call_proxy(target_type, alias)\n      type_aliases[alias] = proxy\n\n    # Expose aliases for exposed objects and targets in the BUILD file.\n    parse_globals = type_aliases.copy()\n\n    # Now its safe to add mappings from both the directly exposed and macro-created target types to\n    # their call proxies for context awares and macros to use to manufacture targets by type\n    # instead of by alias.\n    for alias, target_type in self._target_by_alias.items():\n      proxy = type_aliases[alias]\n      type_aliases[target_type] = proxy\n\n    for target_macro_factory in self._target_macro_factory_by_alias.values():\n      for target_type in target_macro_factory.target_types:\n        proxy = create_call_proxy(target_type)\n        type_aliases[target_type] = proxy\n\n    for alias, object_factory in self._exposed_context_aware_object_factory_by_alias.items():\n      parse_globals[alias] = object_factory(parse_context)\n\n    for alias, target_macro_factory in self._target_macro_factory_by_alias.items():\n      parse_globals[alias] = target_macro_factory.target_macro(parse_context)\n\n    return self.ParseState(parse_context, parse_globals)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef end(self):\n    self.end_time = time.time()\n\n    return self.path(), self.duration(), self._self_time(), self.has_label(WorkUnitLabel.TOOL)", "response": "Returns a tuple with the path duration self_time and label."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the outcome of this work unit.", "response": "def set_outcome(self, outcome):\n    \"\"\"Set the outcome of this work unit.\n\n    We can set the outcome on a work unit directly, but that outcome will also be affected by\n    those of its subunits. The right thing happens: The outcome of a work unit is the\n    worst outcome of any of its subunits and any outcome set on it directly.\"\"\"\n    if outcome not in range(0, 5):\n      raise Exception('Invalid outcome: {}'.format(outcome))\n\n    if outcome < self._outcome:\n      self._outcome = outcome\n      if self.parent: self.parent.set_outcome(self._outcome)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef output(self, name):\n    m = WorkUnit._valid_name_re.match(name)\n    if not m or m.group(0) != name:\n      raise Exception('Invalid output name: {}'.format(name))\n    if name not in self._outputs:\n      workunit_name = re.sub(r'\\W', '_', self.name)\n      path = os.path.join(self.run_info_dir,\n                          'tool_outputs', '{workunit_name}-{id}.{output_name}'\n                          .format(workunit_name=workunit_name,\n                                  id=self.id,\n                                  output_name=name))\n      safe_mkdir_for(path)\n      self._outputs[name] = FileBackedRWBuf(path)\n      self._output_paths[name] = path\n    return self._outputs[name]", "response": "Returns the output buffer for the specified name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef start_delta_string(self):\n    delta = int(self.start_time) - int(self.root().start_time)\n    return '{:02}:{:02}'.format(int(delta / 60), delta % 60)", "response": "A convenient string representation of how long after the run started we started."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the root node of the hierarchy.", "response": "def root(self):\n    \"\"\"\n    :API: public\n    \"\"\"\n    ret = self\n    while ret.parent is not None:\n      ret = ret.parent\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list consisting of this workunit and those enclosing it up to the root.", "response": "def ancestors(self):\n    \"\"\"Returns a list consisting of this workunit and those enclosing it, up to the root.\n\n    :API: public\n    \"\"\"\n    ret = []\n    workunit = self\n    while workunit is not None:\n      ret.append(workunit)\n      workunit = workunit.parent\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the time spent in this workunit outside of any children.", "response": "def _self_time(self):\n    \"\"\"Returns the time spent in this workunit outside of any children.\"\"\"\n    return self.duration() - sum([child.duration() for child in self.children])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef longest_dir_prefix(path, prefixes):\n  longest_match, longest_prefix = 0, None\n  for prefix in prefixes:\n    if fast_relpath_optional(path, prefix) is not None and len(prefix) > longest_match:\n      longest_match, longest_prefix = len(prefix), prefix\n\n  return longest_prefix", "response": "Given a list of prefixes return the one that is the longest prefix to the given path."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fast_relpath(path, start):\n  relpath = fast_relpath_optional(path, start)\n  if relpath is None:\n    raise ValueError('{} is not a directory containing {}'.format(start, path))\n  return relpath", "response": "A prefix - based relpath with no normalization support for returning..."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef safe_mkdir_for(path, clean=False):\n  safe_mkdir(os.path.dirname(path), clean=clean)", "response": "Ensure that the parent directory for a file is present."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake directories which would contain all of the passed paths.", "response": "def safe_mkdir_for_all(paths):\n  \"\"\"Make directories which would contain all of the passed paths.\n\n  This avoids attempting to re-make the same directories, which may be noticeably expensive if many\n  paths mostly fall in the same set of directories.\n\n  :param list of str paths: The paths for which containing directories should be created.\n  \"\"\"\n  created_dirs = set()\n  for path in paths:\n    dir_to_make = os.path.dirname(path)\n    if dir_to_make not in created_dirs:\n      safe_mkdir(dir_to_make)\n      created_dirs.add(dir_to_make)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites a string to a file.", "response": "def safe_file_dump(filename, payload='', mode='w'):\n  \"\"\"Write a string to a file.\n\n  This method is \"safe\" to the extent that `safe_open` is \"safe\". See the explanation on the method\n  doc there.\n\n  When `payload` is an empty string (the default), this method can be used as a concise way to\n  create an empty file along with its containing directory (or truncate it if it already exists).\n\n  :param string filename: The filename of the file to write to.\n  :param string payload: The string to write to the file.\n  :param string mode: A mode argument for the python `open` builtin which should be a write mode\n                      variant. Defaults to 'w'.\n  \"\"\"\n  with safe_open(filename, mode=mode) as f:\n    f.write(payload)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_file(filename, binary_mode=False):\n  mode = 'rb' if binary_mode else 'r'\n  with open(filename, mode) as f:\n    return f.read()", "response": "Read and return the contents of a file in a single file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mergetree(src, dst, symlinks=False, ignore=None, file_filter=None):\n  safe_mkdir(dst)\n\n  if not file_filter:\n    file_filter = lambda _: True\n\n  for src_path, dirnames, filenames in safe_walk(src, topdown=True, followlinks=True):\n    ignorenames = ()\n    if ignore:\n      to_ignore = ignore(src_path, dirnames + filenames)\n      if to_ignore:\n        ignorenames = frozenset(to_ignore)\n\n    src_relpath = os.path.relpath(src_path, src)\n    dst_path = os.path.join(dst, src_relpath)\n\n    visit_dirs = []\n    for dirname in dirnames:\n      if dirname in ignorenames:\n        continue\n\n      src_dir = os.path.join(src_path, dirname)\n      dst_dir = os.path.join(dst_path, dirname)\n      if os.path.exists(dst_dir):\n        if not os.path.isdir(dst_dir):\n          raise ExistingFileError('While copying the tree at {} to {}, encountered directory {} in '\n                                  'the source tree that already exists in the destination as a '\n                                  'non-directory.'.format(src, dst, dst_dir))\n        visit_dirs.append(dirname)\n      elif symlinks and os.path.islink(src_dir):\n        link = os.readlink(src_dir)\n        os.symlink(link, dst_dir)\n        # We need to halt the walk at a symlink dir; so we do not place dirname in visit_dirs\n        # here.\n      else:\n        os.makedirs(dst_dir)\n        visit_dirs.append(dirname)\n\n    # In-place mutate dirnames to halt the walk when the dir is ignored by the caller.\n    dirnames[:] = visit_dirs\n\n    for filename in filenames:\n      if filename in ignorenames:\n        continue\n      src_file_relpath = os.path.join(src_relpath, filename)\n      if not file_filter(src_file_relpath):\n        continue\n\n      dst_filename = os.path.join(dst_path, filename)\n      if os.path.exists(dst_filename):\n        if not os.path.isfile(dst_filename):\n          raise ExistingDirError('While copying the tree at {} to {}, encountered file {} in the '\n                                 'source tree that already exists in the destination as a non-file.'\n                                 .format(src, dst, dst_filename))\n        else:\n          os.unlink(dst_filename)\n      src_filename = os.path.join(src_path, filename)\n      if symlinks and os.path.islink(src_filename):\n        link = os.readlink(src_filename)\n        os.symlink(link, dst_filename)\n      else:\n        shutil.copy2(src_filename, dst_filename)", "response": "A version of shutil. copytree that recursively walks src and merges the contents of the files into dst."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a temporary directory that is cleaned up on process exit.", "response": "def safe_mkdtemp(cleaner=_mkdtemp_atexit_cleaner, **kw):\n  \"\"\"Create a temporary directory that is cleaned up on process exit.\n\n  Arguments are as to tempfile.mkdtemp.\n\n  :API: public\n  \"\"\"\n  # Proper lock sanitation on fork [issue 6721] would be desirable here.\n  with _MKDTEMP_LOCK:\n    return register_rmtree(tempfile.mkdtemp(**kw), cleaner=cleaner)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef register_rmtree(directory, cleaner=_mkdtemp_atexit_cleaner):\n  with _MKDTEMP_LOCK:\n    _mkdtemp_register_cleaner(cleaner)\n    _MKDTEMP_DIRS[os.getpid()].add(directory)\n  return directory", "response": "Register an existing directory to be cleaned up at process exit."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef safe_rmtree(directory):\n  if os.path.islink(directory):\n    safe_delete(directory)\n  else:\n    shutil.rmtree(directory, ignore_errors=True)", "response": "Delete a directory if it s present."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nopen a file safely ensuring that its directory exists.", "response": "def safe_open(filename, *args, **kwargs):\n  \"\"\"Open a file safely, ensuring that its directory exists.\n\n  :API: public\n  \"\"\"\n  safe_mkdir_for(filename)\n  return open(filename, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrenaming src to dst ignoring errors due to dst already existing.", "response": "def safe_concurrent_rename(src, dst):\n  \"\"\"Rename src to dst, ignoring errors due to dst already existing.\n\n  Useful when concurrent processes may attempt to create dst, and it doesn't matter who wins.\n  \"\"\"\n  # Delete dst, in case it existed (with old content) even before any concurrent processes\n  # attempted this write. This ensures that at least one process writes the new content.\n  if os.path.isdir(src):  # Note that dst may not exist, so we test for the type of src.\n    safe_rmtree(dst)\n  else:\n    safe_delete(dst)\n  try:\n    shutil.move(src, dst)\n  except IOError as e:\n    if e.errno != errno.EEXIST:\n      raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove oldest num_of_items_to_keep newly modified items in root_dir.", "response": "def safe_rm_oldest_items_in_dir(root_dir, num_of_items_to_keep, excludes=frozenset()):\n  \"\"\"\n  Keep `num_of_items_to_keep` newly modified items besides `excludes` in `root_dir` then remove the rest.\n  :param root_dir: the folder to examine\n  :param num_of_items_to_keep: number of files/folders/symlinks to keep after the cleanup\n  :param excludes: absolute paths excluded from removal (must be prefixed with `root_dir`)\n  :return: none\n  \"\"\"\n  if os.path.isdir(root_dir):\n    found_files = []\n    for old_file in os.listdir(root_dir):\n      full_path = os.path.join(root_dir, old_file)\n      if full_path not in excludes:\n        found_files.append((full_path, os.path.getmtime(full_path)))\n    found_files = sorted(found_files, key=lambda x: x[1], reverse=True)\n    for cur_file, _ in found_files[num_of_items_to_keep:]:\n      rm_rf(cur_file)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef safe_concurrent_creation(target_path):\n  safe_mkdir_for(target_path)\n  tmp_path = '{}.tmp.{}'.format(target_path, uuid.uuid4().hex)\n  try:\n    yield tmp_path\n  except Exception:\n    rm_rf(tmp_path)\n    raise\n  else:\n    if os.path.exists(tmp_path):\n      safe_concurrent_rename(tmp_path, target_path)", "response": "A contextmanager that yields a temporary path and renames it to a final target path when the contextmanager exits."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef absolute_symlink(source_path, target_path):\n  if not os.path.isabs(source_path):\n    raise ValueError(\"Path for source : {} must be absolute\".format(source_path))\n  if not os.path.isabs(target_path):\n    raise ValueError(\"Path for link : {} must be absolute\".format(target_path))\n  if source_path == target_path:\n    raise ValueError(\"Path for link is identical to source : {}\".format(source_path))\n  try:\n    if os.path.lexists(target_path):\n      if os.path.islink(target_path) or os.path.isfile(target_path):\n        os.unlink(target_path)\n      else:\n        shutil.rmtree(target_path)\n    safe_mkdir_for(target_path)\n    os.symlink(source_path, target_path)\n  except OSError as e:\n    # Another run may beat us to deletion or creation.\n    if not (e.errno == errno.EEXIST or e.errno == errno.ENOENT):\n      raise", "response": "Create a symlink at target pointing to source using the absolute path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a symlink at link_path pointing to relative source_path.", "response": "def relative_symlink(source_path, link_path):\n  \"\"\"Create a symlink at link_path pointing to relative source\n\n  :param source_path: Absolute path to source file\n  :param link_path: Absolute path to intended symlink\n  :raises ValueError if source_path or link_path are not unique, absolute paths\n  :raises OSError on failure UNLESS file already exists or no such file/directory\n  \"\"\"\n  if not os.path.isabs(source_path):\n    raise ValueError(\"Path for source:{} must be absolute\".format(source_path))\n  if not os.path.isabs(link_path):\n    raise ValueError(\"Path for link:{} must be absolute\".format(link_path))\n  if source_path == link_path:\n    raise ValueError(\"Path for link is identical to source:{}\".format(source_path))\n  # The failure state below had a long life as an uncaught error. No behavior was changed here, it just adds a catch.\n  # Raising an exception does differ from absolute_symlink, which takes the liberty of deleting existing directories.\n  if os.path.isdir(link_path) and not os.path.islink(link_path):\n    raise ValueError(\"Path for link would overwrite an existing directory: {}\".format(link_path))\n  try:\n    if os.path.lexists(link_path):\n      os.unlink(link_path)\n    rel_path = os.path.relpath(source_path, os.path.dirname(link_path))\n    safe_mkdir_for(link_path)\n    os.symlink(rel_path, link_path)\n  except OSError as e:\n    # Another run may beat us to deletion or creation.\n    if not (e.errno == errno.EEXIST or e.errno == errno.ENOENT):\n      raise"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef touch(path, times=None):\n  if times:\n    if len(times) > 2:\n      raise ValueError('times must either be a tuple of (atime, mtime) or else a single time value '\n                       'to use for both.')\n\n    if len(times) == 1:\n      times = (times, times)\n\n  with safe_open(path, 'a'):\n    os.utime(path, times)", "response": "Equivalent of unix touch path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_basedir(path):\n  return path[:path.index(os.sep)] if os.sep in path else path", "response": "Returns the base directory of a path."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rm_rf(name):\n  if not os.path.exists(name):\n    return\n\n  try:\n    # Avoid using safe_rmtree so we can detect failures.\n    shutil.rmtree(name)\n  except OSError as e:\n    if e.errno == errno.ENOTDIR:\n      # 'Not a directory', but a file. Attempt to os.unlink the file, raising OSError on failure.\n      safe_delete(name)\n    elif e.errno != errno.ENOENT:\n      # Pass on 'No such file or directory', otherwise re-raise OSError to surface perm issues etc.\n      raise", "response": "Remove a file or a directory similarly to running rm - rf <name > in a UNIX shell."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn whether a path names an existing executable file.", "response": "def is_executable(path):\n  \"\"\"Returns whether a path names an existing executable file.\"\"\"\n  return os.path.isfile(path) and os.access(path, os.X_OK)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_no_overlapping_paths(paths):\n  for path in paths:\n    list_copy_without_path = list(paths)\n    list_copy_without_path.remove(path)\n    if path in list_copy_without_path:\n      raise ValueError('{} appeared more than once. All paths must be unique.'.format(path))\n    for p in list_copy_without_path:\n      if path in p:\n        raise ValueError('{} and {} have the same prefix. All paths must be unique and cannot overlap.'.format(path, p))", "response": "Given a list of paths ensure that all are unique and do not have the same prefix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn whether a path names an existing directory we can list and read files from.", "response": "def is_readable_dir(path):\n  \"\"\"Returns whether a path names an existing directory we can list and read files from.\"\"\"\n  return os.path.isdir(path) and os.access(path, os.R_OK) and os.access(path, os.X_OK)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrecording the given RuleIndex on self. _tasks.", "response": "def _register_rules(self, rule_index):\n    \"\"\"Record the given RuleIndex on `self._tasks`.\"\"\"\n    registered = set()\n    for output_type, rules in rule_index.rules.items():\n      for rule in rules:\n        key = (output_type, rule)\n        if key in registered:\n          continue\n        registered.add(key)\n\n        if type(rule) is TaskRule:\n          self._register_task(output_type, rule, rule_index.union_rules)\n        else:\n          raise ValueError('Unexpected Rule type: {}'.format(rule))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister the given TaskRule with the native scheduler.", "response": "def _register_task(self, output_type, rule, union_rules):\n    \"\"\"Register the given TaskRule with the native scheduler.\"\"\"\n    func = Function(self._to_key(rule.func))\n    self._native.lib.tasks_task_begin(self._tasks, func, self._to_type(output_type), rule.cacheable)\n    for selector in rule.input_selectors:\n      self._native.lib.tasks_add_select(self._tasks, self._to_type(selector))\n\n    def add_get_edge(product, subject):\n      self._native.lib.tasks_add_get(self._tasks, self._to_type(product), self._to_type(subject))\n\n    for the_get in rule.input_gets:\n      union_members = union_rules.get(the_get.subject_declared_type, None)\n      if union_members:\n        # If the registered subject type is a union, add Get edges to all registered union members.\n        for union_member in union_members:\n          add_get_edge(the_get.product, union_member)\n      else:\n        # Otherwise, the Get subject is a \"concrete\" type, so add a single Get edge.\n        add_get_edge(the_get.product, the_get.subject_declared_type)\n\n    self._native.lib.tasks_task_end(self._tasks)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef with_fork_context(self, func):\n    res = self._native.lib.scheduler_fork_context(self._scheduler, Function(self._to_key(func)))\n    return self._raise_or_return(res)", "response": "Returns a new object with the fork context of the current scheduler."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmerge any number of directories.", "response": "def merge_directories(self, directory_digests):\n    \"\"\"Merges any number of directories.\n\n    :param directory_digests: Tuple of DirectoryDigests.\n    :return: A Digest.\n    \"\"\"\n    result = self._native.lib.merge_directories(\n      self._scheduler,\n      self._to_value(_DirectoryDigests(directory_digests)),\n    )\n    return self._raise_or_return(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmaterializes the specified directories on the file system.", "response": "def materialize_directories(self, directories_paths_and_digests):\n    \"\"\"Creates the specified directories on the file system.\n\n    :param directories_paths_and_digests tuple<DirectoryToMaterialize>: Tuple of the path and\n           digest of the directories to materialize.\n    :returns: Nothing or an error.\n    \"\"\"\n    # Ensure there isn't more than one of the same directory paths and paths do not have the same prefix.\n    dir_list = [dpad.path for dpad in directories_paths_and_digests]\n    check_no_overlapping_paths(dir_list)\n\n    result = self._native.lib.materialize_directories(\n      self._scheduler,\n      self._to_value(_DirectoriesToMaterialize(directories_paths_and_digests)),\n    )\n    return self._raise_or_return(result)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new SchedulerSession for this Scheduler.", "response": "def new_session(self, zipkin_trace_v2, v2_ui=False):\n    \"\"\"Creates a new SchedulerSession for this Scheduler.\"\"\"\n    return SchedulerSession(self, self._native.new_session(\n      self._scheduler, zipkin_trace_v2, v2_ui, multiprocessing.cpu_count())\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nyield a stringified stacktrace starting from the scheduler s roots.", "response": "def trace(self, execution_request):\n    \"\"\"Yields a stringified 'stacktrace' starting from the scheduler's roots.\"\"\"\n    for line in self._scheduler.graph_trace(execution_request.native):\n      yield line"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates and return an ExecutionRequest object for the given products and subjects.", "response": "def execution_request(self, products, subjects):\n    \"\"\"Create and return an ExecutionRequest for the given products and subjects.\n\n    The resulting ExecutionRequest object will contain keys tied to this scheduler's product Graph,\n    and so it will not be directly usable with other scheduler instances without being re-created.\n\n    NB: This method does a \"cross product\", mapping all subjects to all products. To create a\n    request for just the given list of subject -> product tuples, use `execution_request_literal()`!\n\n    :param products: A list of product types to request for the roots.\n    :type products: list of types\n    :param subjects: A list of Spec and/or PathGlobs objects.\n    :type subject: list of :class:`pants.base.specs.Spec`, `pants.build_graph.Address`, and/or\n      :class:`pants.engine.fs.PathGlobs` objects.\n    :returns: An ExecutionRequest for the given products and subjects.\n    \"\"\"\n    roots = (tuple((s, p) for s in subjects for p in products))\n    return self.execution_request_literal(roots)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef invalidate_files(self, direct_filenames):\n    invalidated = self._scheduler.invalidate_files(direct_filenames)\n    self._maybe_visualize()\n    return invalidated", "response": "Invalidates the given filenames in an internal product Graph instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef execute(self, execution_request):\n    start_time = time.time()\n    roots = list(zip(execution_request.roots,\n                     self._scheduler._run_and_return_roots(self._session, execution_request.native)))\n\n    self._maybe_visualize()\n\n    logger.debug(\n      'computed %s nodes in %f seconds. there are %s total nodes.',\n      len(roots),\n      time.time() - start_time,\n      self._scheduler.graph_len()\n    )\n\n    returns = tuple((root, state) for root, state in roots if type(state) is Return)\n    throws = tuple((root, state) for root, state in roots if type(state) is Throw)\n    return returns, throws", "response": "Invoke the engine for the given ExecutionRequest returning Return and Throw states."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_console_rule(self, product, subject):\n    request = self.execution_request([product], [subject])\n    returns, throws = self.execute(request)\n\n    if throws:\n      _, state = throws[0]\n      exc = state.exc\n      self._trace_on_error([exc], request)\n      return PANTS_FAILED_EXIT_CODE\n    _, state = returns[0]\n    return state.value.exit_code", "response": "Executes a console rule on the given product and subject."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexecute a request for a single product for some subjects and returns the products.", "response": "def product_request(self, product, subjects):\n    \"\"\"Executes a request for a single product for some subjects, and returns the products.\n\n    :param class product: A product type for the request.\n    :param list subjects: A list of subjects or Params instances for the request.\n    :returns: A list of the requested products, with length match len(subjects).\n    \"\"\"\n    request = self.execution_request([product], subjects)\n    returns, throws = self.execute(request)\n\n    # Throw handling.\n    if throws:\n      unique_exceptions = tuple({t.exc for _, t in throws})\n      self._trace_on_error(unique_exceptions, request)\n\n    # Everything is a Return: we rely on the fact that roots are ordered to preserve subject\n    # order in output lists.\n    return [ret.value for _, ret in returns]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a file path without the file specifier", "response": "def parse_file_path(cls, file_path):\n    \"\"\"Parse a file address path without the file specifier\"\"\"\n    address = None\n    pattern = cls.file_regex.match(file_path)\n    if pattern:\n      address = pattern.group(1)\n    return address"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncopying sources from a target to a results directory.", "response": "def _copy_sources(self, target, results_dir):\n    \"\"\"Copy sources from a target to a results directory.\n\n    :param NodePackage target: A subclass of NodePackage\n    :param string results_dir: The results directory\n    \"\"\"\n    buildroot = get_buildroot()\n    source_relative_to = target.address.spec_path\n    for source in target.sources_relative_to_buildroot():\n      dest = os.path.join(results_dir, os.path.relpath(source, source_relative_to))\n      safe_mkdir(os.path.dirname(dest))\n      shutil.copyfile(os.path.join(buildroot, source), dest)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_target_from_package_name(self, target, package_name, file_path):\n    address_path = self.parse_file_path(file_path)\n    if not address_path:\n      return None\n\n    dep_spec_path = os.path.normpath(os.path.join(target.address.spec_path, address_path))\n    for dep in target.dependencies:\n      if dep.package_name == package_name and dep.address.spec_path == dep_spec_path:\n        return dep\n    return None", "response": "Get a dependent target given the package name and relative file path."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nverifies the given action is a valid duplicate jar rule action.", "response": "def validate_action(cls, action):\n    \"\"\"Verifies the given action is a valid duplicate jar rule action.\n\n    :returns: The action if it is valid.\n    :raises: ``ValueError`` if the action is invalid.\n    \"\"\"\n    if action not in cls._VALID_ACTIONS:\n      raise ValueError('The supplied action must be one of {valid}, given: {given}'\n                       .format(valid=cls._VALID_ACTIONS, given=action))\n    return action"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef skip_signatures_and_duplicates_concat_well_known_metadata(cls, default_dup_action=None,\n                                                                additional_rules=None):\n    \"\"\"Produces a rule set useful in many deploy jar creation contexts.\n\n    The rule set skips duplicate entries by default, retaining the 1st encountered.  In addition it\n    has the following special handling:\n\n    - jar signature metadata is dropped\n    - jar indexing files INDEX.LIST are dropped\n    - ``java.util.ServiceLoader`` provider-configuration files are concatenated in the order\n      encountered\n\n    :param default_dup_action: An optional default action to take for duplicates.  Defaults to\n      `Duplicate.SKIP` if not specified.\n    :param additional_rules: Optionally one or more jar rules to add to those described above.\n    :returns: JarRules\n    \"\"\"\n    default_dup_action = Duplicate.validate_action(default_dup_action or Duplicate.SKIP)\n    additional_rules = assert_list(additional_rules,\n                                   expected_type=(Duplicate, Skip))\n\n    rules = [Skip(r'^META-INF/[^/]+\\.SF$'),  # signature file\n             Skip(r'^META-INF/[^/]+\\.DSA$'),  # default signature alg. file\n             Skip(r'^META-INF/[^/]+\\.RSA$'),  # default signature alg. file\n             Skip(r'^META-INF/INDEX.LIST$'),  # interferes with Class-Path: see man jar for i option\n             Duplicate(r'^META-INF/services/', Duplicate.CONCAT_TEXT)]  # 1 svc fqcn per line\n\n    return JarRules(rules=rules + additional_rules, default_dup_action=default_dup_action)", "response": "Returns a rule set that skips duplicate entries by default and concatenates well - known metadata files."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef default(cls):\n    if cls._DEFAULT is None:\n      cls._DEFAULT = cls.skip_signatures_and_duplicates_concat_well_known_metadata()\n    return cls._DEFAULT", "response": "Returns the default set of jar rules."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_default(cls, rules):\n    if not isinstance(rules, JarRules):\n      raise ValueError('The default rules must be a JarRules instance.')\n    cls._DEFAULT = rules", "response": "Sets the default site - wide jar rules."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the build root for the current workspace.", "response": "def path(self):\n    \"\"\"Returns the build root for the current workspace.\"\"\"\n    if self._root_dir is None:\n      # This env variable is for testing purpose.\n      override_buildroot = os.environ.get('PANTS_BUILDROOT_OVERRIDE', None)\n      if override_buildroot:\n        self._root_dir = override_buildroot\n      else:\n        self._root_dir = os.path.realpath(self.find_buildroot())\n      if PY2:\n        self._root_dir = self._root_dir.decode('utf-8')\n    return self._root_dir"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef path(self, root_dir):\n    path = os.path.realpath(root_dir)\n    if not os.path.exists(path):\n      raise ValueError('Build root does not exist: {}'.format(root_dir))\n    self._root_dir = path", "response": "Manually establishes the build root for the current workspace."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef temporary(self, path):\n    if path is None:\n      raise ValueError('Can only temporarily establish a build root given a path.')\n    prior = self._root_dir\n    self._root_dir = path\n    try:\n      yield\n    finally:\n      self._root_dir = prior", "response": "Establishes a temporary build root restoring the prior build root on exit."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef print_help(self):\n    def print_hint():\n      print('Use `pants goals` to list goals.')\n      print('Use `pants help` to get help.')\n    if isinstance(self._help_request, VersionHelp):\n      print(pants_version())\n    elif isinstance(self._help_request, OptionsHelp):\n      self._print_options_help()\n    elif isinstance(self._help_request, GoalsHelp):\n      self._print_goals_help()\n    elif isinstance(self._help_request, UnknownGoalHelp):\n      print('Unknown goals: {}'.format(', '.join(self._help_request.unknown_goals)))\n      print_hint()\n      return 1\n    elif isinstance(self._help_request, NoGoalHelp):\n      print('No goals specified.')\n      print_hint()\n      return 1\n    return 0", "response": "Print help to the console."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _print_options_help(self):\n    show_all_help = self._help_request.all_scopes\n    if show_all_help:\n      help_scopes = list(self._options.known_scope_to_info.keys())\n    else:\n      # The scopes explicitly mentioned by the user on the cmd line.\n      help_scopes = set(self._options.scope_to_flags.keys()) - {GLOBAL_SCOPE}\n\n    scope_infos = list(ScopeInfoIterator(self._options.known_scope_to_info).iterate(help_scopes))\n    if scope_infos:\n      for scope_info in scope_infos:\n        help_str = self._format_help(scope_info)\n        if help_str:\n          print(help_str)\n      return\n    else:\n      print(pants_release())\n      print('\\nUsage:')\n      print('  ./pants [option ...] [goal ...] [target...]  Attempt the specified goals.')\n      print('  ./pants help                                 Get help.')\n      print('  ./pants help [goal]                          Get help for a goal.')\n      print('  ./pants help-advanced [goal]                 Get help for a goal\\'s advanced options.')\n      print('  ./pants help-all                             Get help for all goals.')\n      print('  ./pants goals                                List all installed goals.')\n      print('')\n      print('  [target] accepts two special forms:')\n      print('    dir:  to include all targets in the specified directory.')\n      print('    dir:: to include all targets found recursively under the directory.')\n      print('\\nFriendly docs:\\n  http://pantsbuild.org/')\n\n      print(self._format_help(ScopeInfo(GLOBAL_SCOPE, ScopeInfo.GLOBAL)))", "response": "Print a help screen for the specified options."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a help message for the options registered on this object.", "response": "def _format_help(self, scope_info):\n    \"\"\"Return a help message for the options registered on this object.\n\n    Assumes that self._help_request is an instance of OptionsHelp.\n\n    :param scope_info: Scope of the options.\n    \"\"\"\n    scope = scope_info.scope\n    description = scope_info.description\n    show_recursive = self._help_request.advanced\n    show_advanced = self._help_request.advanced\n    color = sys.stdout.isatty()\n    help_formatter = HelpFormatter(scope, show_recursive, show_advanced, color)\n    return '\\n'.join(help_formatter.format_options(scope, description,\n        self._options.get_parser(scope).option_registrations_iter()))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_travis_yml():\n  def get_mustache_file(file_name):\n    return pkg_resources.resource_string(__name__, file_name).decode('utf-8')\n\n  template = get_mustache_file('travis.yml.mustache')\n  before_install_linux = get_mustache_file('before_install_linux.mustache')\n  before_install_osx = get_mustache_file('before_install_osx.mustache')\n  env_osx_with_pyenv = get_mustache_file('env_osx_with_pyenv.mustache')\n  docker_build_image = get_mustache_file('docker_build_image.mustache')\n  docker_run_image = get_mustache_file('docker_run_image.mustache')\n\n  context = {\n    'header': HEADER,\n    'integration_shards': range(0, num_integration_shards),\n    'integration_shards_length': num_integration_shards,\n  }\n  renderer = pystache.Renderer(partials={\n    'before_install_linux': before_install_linux,\n    'before_install_osx': before_install_osx,\n    'env_osx_with_pyenv': env_osx_with_pyenv,\n    'docker_build_image': docker_build_image,\n    'docker_run_image': docker_run_image\n  })\n  print(renderer.render(template, context))", "response": "Generates a. travis. yml file from templates."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate_platform_dependencies(self):\n    conflicts = []\n\n    def is_conflicting(target, dependency):\n      return self.jvm_version(dependency) > self.jvm_version(target)\n\n    try:\n      sort_targets(self.jvm_targets)\n    except CycleException:\n      self.context.log.warn('Cannot validate dependencies when cycles exist in the build graph.')\n      return\n\n    try:\n      with self.invalidated(self.jvm_targets,\n                            fingerprint_strategy=self.PlatformFingerprintStrategy(),\n                            invalidate_dependents=True) as vts:\n        dependency_map = self.jvm_dependency_map\n        for vts_target in vts.invalid_vts:\n          for target in vts_target.targets:\n            if target in dependency_map:\n              deps = dependency_map[target]\n              invalid_dependencies = [dep for dep in deps if is_conflicting(target, dep)]\n              if invalid_dependencies:\n                conflicts.append((target, invalid_dependencies))\n        if conflicts:\n          # NB(gmalmquist): It's important to unconditionally raise an exception, then decide later\n          # whether to continue raising it or just print a warning, to make sure the targets aren't\n          # marked as valid if there are invalid platform dependencies.\n          error_message = self._create_full_error_message(conflicts)\n          raise self.IllegalJavaTargetLevelDependency(error_message)\n    except self.IllegalJavaTargetLevelDependency as e:\n      if self.check == 'fatal':\n        raise e\n      else:\n        assert self.check == 'warn'\n        self.context.log.warn(error_message)\n        return error_message", "response": "Validate the platform dependencies of the jvm targets."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef possible_version_evaluation(self):\n    only_broken = self.get_options().only_broken\n    ranges = self._ranges\n    yield 'Allowable JVM platform ranges (* = anything):'\n    for target in sorted(filter(self._is_relevant, self.jvm_targets)):\n      min_version = ranges.min_allowed_version.get(target)\n      max_version = ranges.max_allowed_version.get(target)\n      current_valid = True\n      if min_version and self.jvm_version(target) < min_version:\n        current_valid = False\n      if max_version and self.jvm_version(target) > max_version:\n        current_valid = False\n      current_text = str(self.jvm_version(target))\n      if not current_valid:\n        current_text = self._format_error(current_text)\n      elif only_broken:\n        continue\n\n      if min_version and max_version:\n        range_text = '{} to {}'.format(min_version, max_version)\n        if min_version > max_version:\n          range_text = self._format_error(range_text)\n      elif min_version:\n        range_text = '{}+'.format(min_version)\n      elif max_version:\n        range_text = '<={}'.format(max_version)\n      else:\n        range_text = '*'\n      yield '{address}: {range}  (is {current})'.format(address=target.address.spec,\n                                                        range=range_text,\n                                                        current=current_text,)\n      if self.get_options().detailed or not current_valid:\n        if min_version:\n          min_because = [t for t in ranges.target_dependencies[target]\n                         if self.jvm_version(t) == min_version]\n          yield '  min={} because of dependencies:'.format(min_version)\n          for dep in sorted(min_because):\n            yield '    {}'.format(dep.address.spec)\n        if max_version:\n          max_because = [t for t in ranges.target_dependees[target]\n                         if self.jvm_version(t) == max_version]\n          yield '  max={} because of dependees:'.format(max_version)\n          for dep in sorted(max_because):\n            yield '    {}'.format(dep.address.spec)\n        yield ''", "response": "Evaluate the possible range of versions for each target yielding the output analysis."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _nailgunnable_combined_classpath(self):\n    cp = []\n    cp.extend(self.tool_classpath('rsc'))\n    # Add zinc's classpath so that it can be invoked from the same nailgun instance.\n    cp.extend(super(RscCompile, self).get_zinc_compiler_classpath())\n    return cp", "response": "Returns the classpath that will be used to invoke the zinc compiler if nailgun is not using nailgun."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the compile workflow to use for this target.", "response": "def _classify_target_compile_workflow(self, target):\n    \"\"\"Return the compile workflow to use for this target.\"\"\"\n    if target.has_sources('.java') or target.has_sources('.scala'):\n      return self.get_scalar_mirrored_target_option('workflow', target)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _on_invalid_compile_dependency(self, dep, compile_target, contexts):\n    return contexts[compile_target][0].workflow.resolve_for_enum_variant({\n      'zinc-only': lambda : contexts[dep][0].workflow == self.JvmCompileWorkflowType.rsc_then_zinc,\n      'rsc-then-zinc': lambda : False\n    })()", "response": "Decide whether to continue searching for invalid targets to use in the execution graph."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef target_types_for_alias(self, alias):\n    registered_aliases = self.context.build_configuration.registered_aliases()\n    target_types = registered_aliases.target_types_by_alias.get(alias, None)\n    if not target_types:\n      raise self.InvalidTargetType('Not a target type: {}'.format(alias))\n    return target_types", "response": "Returns all the target types that can be produced by the given alias."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef submit_async_work(self, work, workunit_parent=None, on_success=None, on_failure=None):\n    if work is None or len(work.args_tuples) == 0:  # map_async hangs on 0-length iterables.\n      if on_success:\n        on_success([])\n    else:\n      def do_work(*args):\n        self._do_work(work.func, *args, workunit_name=work.workunit_name,\n                      workunit_parent=workunit_parent, on_failure=on_failure)\n      return self._pool.map_async(do_work, work.args_tuples, chunksize=1, callback=on_success)", "response": "Submit a work to the pool."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef submit_async_work_chain(self, work_chain, workunit_parent, done_hook=None):\n    def done():\n      if done_hook:\n        done_hook()\n      with self._pending_workchains_cond:\n        self._pending_workchains -= 1\n        self._pending_workchains_cond.notify()\n\n    def error(e):\n      done()\n      self._run_tracker.log(Report.ERROR, '{}'.format(e))\n\n    # We filter out Nones defensively. There shouldn't be any, but if a bug causes one,\n    # Pants might hang indefinitely without this filtering.\n    work_iter = (_f for _f in work_chain if _f)\n\n    def submit_next():\n      try:\n        self.submit_async_work(next(work_iter), workunit_parent=workunit_parent,\n                               on_success=lambda x: submit_next(), on_failure=error)\n      except StopIteration:\n        done()  # The success case.\n\n    with self._pending_workchains_cond:\n      self._pending_workchains += 1\n    try:\n      submit_next()\n    except Exception as e:  # Handles errors in the submission code.\n      done()\n      self._run_tracker.log(Report.ERROR, '{}'.format(e))\n      raise", "response": "Submit a list of Work instances to be executed in the background."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef submit_work_and_wait(self, work, workunit_parent=None):\n    if work is None or len(work.args_tuples) == 0:  # map hangs on 0-length iterables.\n      return []\n    else:\n      def do_work(*args):\n        return self._do_work(work.func, *args, workunit_name=work.workunit_name,\n                             workunit_parent=workunit_parent)\n      # We need to specify a timeout explicitly, because otherwise python ignores SIGINT when waiting\n      # on a condition variable, so we won't be able to ctrl-c out.\n      return self._pool.map_async(do_work, work.args_tuples, chunksize=1).get(timeout=1000000000)", "response": "Submit a work to the pool but wait for it to complete."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a Command object with required tools installed and paths set up.", "response": "def command_gen(tool_installations, tool_executable, args=None, node_paths=None):\n  \"\"\"Generate a Command object with required tools installed and paths set up.\n\n  :param list tool_installations: A list of functions to install required tools.  Those functions\n    should take no parameter and return an installation path to be included in the runtime path.\n  :param tool_executable: Name of the tool to be executed.\n  :param list args: A list of arguments to be passed to the executable\n  :param list node_paths: A list of path to node_modules.  node_modules/.bin will be appended\n    to the run time path.\n  :rtype: class: `Command`\n  \"\"\"\n  node_module_bin_dir = 'node_modules/.bin'\n  extra_paths = []\n  for t in tool_installations:\n    # Calling tool_installation[i]() triggers installation if tool is not installed\n    extra_paths.append(t())\n  if node_paths:\n    for node_path in node_paths:\n      if not node_path.endswith(node_module_bin_dir):\n        node_path = os.path.join(node_path, node_module_bin_dir)\n      extra_paths.append(node_path)\n  return Command(executable=tool_executable, args=args, extra_paths=extra_paths)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a modifed copy of kwargs [ env ] and kwargs with env removed.", "response": "def _prepare_env(self, kwargs):\n    \"\"\"Returns a modifed copy of kwargs['env'], and a copy of kwargs with 'env' removed.\n\n    If there is no 'env' field in the kwargs, os.environ.copy() is used.\n    env['PATH'] is set/modified to contain the Node distribution's bin directory at the front.\n\n    :param kwargs: The original kwargs.\n    :returns: An (env, kwargs) tuple containing the modified env and kwargs copies.\n    :rtype: (dict, dict)\n    \"\"\"\n    kwargs = kwargs.copy()\n    env = kwargs.pop('env', os.environ).copy()\n    env['PATH'] = create_path_env_var(self.extra_paths, env=env, prepend=True)\n    return env, kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun this command. :param kwargs: Any extra keyword arguments to pass along to `subprocess.Popen`. :returns: A handle to the running command. :rtype: :class:`subprocess.Popen`", "response": "def run(self, **kwargs):\n    \"\"\"Runs this command.\n\n    :param kwargs: Any extra keyword arguments to pass along to `subprocess.Popen`.\n    :returns: A handle to the running command.\n    :rtype: :class:`subprocess.Popen`\n    \"\"\"\n    env, kwargs = self._prepare_env(kwargs)\n    logger.debug('Running command {}'.format(self.cmd))\n    return subprocess.Popen(self.cmd, env=env, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun this command returning its captured stdout.", "response": "def check_output(self, **kwargs):\n    \"\"\"Runs this command returning its captured stdout.\n\n    :param kwargs: Any extra keyword arguments to pass along to `subprocess.Popen`.\n    :returns: The captured standard output stream of the command.\n    :rtype: string\n    :raises: :class:`subprocess.CalledProcessError` if the command fails.\n    \"\"\"\n    env, kwargs = self._prepare_env(kwargs)\n    return subprocess.check_output(self.cmd, env=env, **kwargs).decode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_main_manifest_entry(jar, binary):\n    main = binary.main\n    if main is not None:\n      jar.main(main)", "response": "Adds the main manifest entry to the given jar."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the external jar dependencies of the given jvm binary.", "response": "def list_external_jar_dependencies(self, binary):\n    \"\"\"Returns the external jar dependencies of the given binary.\n\n    :param binary: The jvm binary target to list transitive external dependencies for.\n    :type binary: :class:`pants.backend.jvm.targets.jvm_binary.JvmBinary`\n    :returns: A list of (jar path, coordinate) tuples.\n    :rtype: list of (string, :class:`pants.java.jar.M2Coordinate`)\n    \"\"\"\n    classpath_products = self.context.products.get_data('runtime_classpath')\n    classpath_entries = classpath_products.get_artifact_classpath_entries_for_targets(\n        binary.closure(bfs=True, include_scopes=Scopes.JVM_RUNTIME_SCOPES,\n                       respect_intransitive=True))\n    external_jars = OrderedSet(jar_entry for conf, jar_entry in classpath_entries\n                               if conf == 'default')\n    return [(entry.path, entry.coordinate) for entry in external_jars\n            if not entry.is_excluded_by(binary.deploy_excludes)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a monolithic jar for the given jvm_binary target.", "response": "def monolithic_jar(self, binary, path, manifest_classpath=None):\n    \"\"\"Creates a jar containing all the dependencies for a jvm_binary target.\n\n    Yields a handle to the open jarfile, so the caller can add to the jar if needed.\n    The yielded jar file either has all the class files for the jvm_binary target as\n    a fat jar, or includes those dependencies in the `Class-Path` field of its\n    Manifest.\n\n    :param binary: The jvm_binary target to operate on.\n    :param path: Write the output jar here, overwriting an existing file, if any.\n    :param iterable manifest_classpath: If set output jar will set as its manifest's\n      classpath, otherwise output jar will simply include class files.\n    \"\"\"\n    # TODO(benjy): There's actually nothing here that requires 'binary' to be a jvm_binary.\n    # It could be any target. And that might actually be useful.\n    with self.context.new_workunit(name='create-monolithic-jar'):\n      with self.open_jar(path,\n                         jar_rules=binary.deploy_jar_rules,\n                         overwrite=True,\n                         compressed=True) as monolithic_jar:\n        if manifest_classpath:\n          monolithic_jar.append_classpath(manifest_classpath)\n        else:\n          with self.context.new_workunit(name='add-internal-classes'):\n            with self.create_jar_builder(monolithic_jar) as jar_builder:\n              jar_builder.add_target(binary, recursive=True)\n\n          # NB(gmalmquist): Shading each jar dependency with its own prefix would be a nice feature,\n          # but is not currently possible with how things are set up. It may not be possible to do\n          # in general, at least efficiently.\n          with self.context.new_workunit(name='add-dependency-jars'):\n            dependencies = self.list_external_jar_dependencies(binary)\n            for jar, coordinate in dependencies:\n              self.context.log.debug('  dumping {} from {}'.format(coordinate, jar))\n              monolithic_jar.writejar(jar)\n\n        yield monolithic_jar\n\n      if binary.shading_rules:\n        with self.context.new_workunit('shade-monolithic-jar'):\n          self.shade_jar(binary.shading_rules, jar_path=path)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef shade_jar(self, shading_rules, jar_path):\n    self.context.log.debug('Shading {}.'.format(jar_path))\n    with temporary_dir() as tempdir:\n      output_jar = os.path.join(tempdir, os.path.basename(jar_path))\n      with self.shader.binary_shader_for_rules(output_jar, jar_path, shading_rules) as shade_runner:\n        result = execute_runner(shade_runner, workunit_factory=self.context.new_workunit,\n                                workunit_name='jarjar')\n        if result != 0:\n          raise TaskError('Shading tool failed to shade {0} (error code {1})'.format(jar_path,\n                                                                                     result))\n        if not os.path.exists(output_jar):\n          raise TaskError('Shading tool returned success for {0}, but '\n                          'the output jar was not found at {1}'.format(jar_path, output_jar))\n        atomic_copy(output_jar, jar_path)\n        return jar_path", "response": "Shades a jar using the given shading rules."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_address_family(address_mapper, directory):\n  patterns = tuple(join(directory.path, p) for p in address_mapper.build_patterns)\n  path_globs = PathGlobs(include=patterns,\n                         exclude=address_mapper.build_ignore_patterns)\n  snapshot = yield Get(Snapshot, PathGlobs, path_globs)\n  files_content = yield Get(FilesContent, Digest, snapshot.directory_digest)\n\n  if not files_content:\n    raise ResolveError('Directory \"{}\" does not contain any BUILD files.'.format(directory.path))\n  address_maps = []\n  for filecontent_product in files_content:\n    address_maps.append(AddressMap.parse(filecontent_product.path,\n                                         filecontent_product.content,\n                                         address_mapper.parser))\n  yield AddressFamily.create(directory.path, address_maps)", "response": "Given an AddressMapper and a directory return an AddressFamily."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives an AddressMapper and an Address return a Struct containing all of the inline dependencies of the struct.", "response": "def hydrate_struct(address_mapper, address):\n  \"\"\"Given an AddressMapper and an Address, resolve a Struct from a BUILD file.\n\n  Recursively collects any embedded addressables within the Struct, but will not walk into a\n  dependencies field, since those should be requested explicitly by rules.\n  \"\"\"\n\n  address_family = yield Get(AddressFamily, Dir(address.spec_path))\n\n  struct = address_family.addressables.get(address)\n  addresses = address_family.addressables\n  if not struct or address not in addresses:\n    _raise_did_you_mean(address_family, address.target_name)\n  # TODO: This is effectively: \"get the BuildFileAddress for this Address\".\n  #  see https://github.com/pantsbuild/pants/issues/6657\n  address = next(build_address for build_address in addresses if build_address == address)\n\n  inline_dependencies = []\n  def maybe_append(outer_key, value):\n    if isinstance(value, six.string_types):\n      if outer_key != 'dependencies':\n        inline_dependencies.append(Address.parse(value,\n                                          relative_to=address.spec_path,\n                                          subproject_roots=address_mapper.subproject_roots))\n    elif isinstance(value, Struct):\n      collect_inline_dependencies(value)\n\n  def collect_inline_dependencies(item):\n    for key, value in sorted(item._asdict().items(), key=_key_func):\n      if not AddressableDescriptor.is_addressable(item, key):\n        continue\n      if isinstance(value, MutableMapping):\n        for _, v in sorted(value.items(), key=_key_func):\n          maybe_append(key, v)\n      elif isinstance(value, MutableSequence):\n        for v in value:\n          maybe_append(key, v)\n      else:\n        maybe_append(key, value)\n\n  # Recursively collect inline dependencies from the fields of the struct into `inline_dependencies`.\n  collect_inline_dependencies(struct)\n\n  # And then hydrate the inline dependencies.\n  hydrated_inline_dependencies = yield [Get(HydratedStruct, Address, a) for a in inline_dependencies]\n  dependencies = [d.value for d in hydrated_inline_dependencies]\n\n  def maybe_consume(outer_key, value):\n    if isinstance(value, six.string_types):\n      if outer_key == 'dependencies':\n        # Don't recurse into the dependencies field of a Struct, since those will be explicitly\n        # requested by tasks. But do ensure that their addresses are absolute, since we're\n        # about to lose the context in which they were declared.\n        value = Address.parse(value,\n                              relative_to=address.spec_path,\n                              subproject_roots=address_mapper.subproject_roots)\n      else:\n        value = dependencies[maybe_consume.idx]\n        maybe_consume.idx += 1\n    elif isinstance(value, Struct):\n      value = consume_dependencies(value)\n    return value\n  # NB: Some pythons throw an UnboundLocalError for `idx` if it is a simple local variable.\n  maybe_consume.idx = 0\n\n  # 'zip' the previously-requested dependencies back together as struct fields.\n  def consume_dependencies(item, args=None):\n    hydrated_args = args or {}\n    for key, value in sorted(item._asdict().items(), key=_key_func):\n      if not AddressableDescriptor.is_addressable(item, key):\n        hydrated_args[key] = value\n        continue\n\n      if isinstance(value, MutableMapping):\n        container_type = type(value)\n        hydrated_args[key] = container_type((k, maybe_consume(key, v))\n                                            for k, v in sorted(value.items(), key=_key_func))\n      elif isinstance(value, MutableSequence):\n        container_type = type(value)\n        hydrated_args[key] = container_type(maybe_consume(key, v) for v in value)\n      else:\n        hydrated_args[key] = maybe_consume(key, value)\n    return _hydrate(type(item), address.spec_path, **hydrated_args)\n\n  yield HydratedStruct(consume_dependencies(struct, args={'address': address}))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef addresses_from_address_families(address_mapper, specs):\n  # Capture a Snapshot covering all paths for these Specs, then group by directory.\n  snapshot = yield Get(Snapshot, PathGlobs, _spec_to_globs(address_mapper, specs))\n  dirnames = {dirname(f) for f in snapshot.files}\n  address_families = yield [Get(AddressFamily, Dir(d)) for d in dirnames]\n  address_family_by_directory = {af.namespace: af for af in address_families}\n\n  matched_addresses = OrderedSet()\n  for spec in specs:\n    # NB: if a spec is provided which expands to some number of targets, but those targets match\n    # --exclude-target-regexp, we do NOT fail! This is why we wait to apply the tag and exclude\n    # patterns until we gather all the targets the spec would have matched without them.\n    try:\n      addr_families_for_spec = spec.matching_address_families(address_family_by_directory)\n    except Spec.AddressFamilyResolutionError as e:\n      raise raise_from(ResolveError(e), e)\n\n    try:\n      all_addr_tgt_pairs = spec.address_target_pairs_from_address_families(addr_families_for_spec)\n    except Spec.AddressResolutionError as e:\n      raise raise_from(AddressLookupError(e), e)\n    except SingleAddress._SingleAddressResolutionError as e:\n      _raise_did_you_mean(e.single_address_family, e.name, source=e)\n\n    matched_addresses.update(\n      addr for (addr, tgt) in all_addr_tgt_pairs\n      if specs.matcher.matches_target_address_pair(addr, tgt)\n    )\n\n  # NB: This may be empty, as the result of filtering by tag and exclude patterns!\n  yield BuildFileAddresses(tuple(matched_addresses))", "response": "Given an AddressMapper and list of Specs return a list of BuildFileAddresses."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a Specs object return a PathGlobs object for the build files that it matches.", "response": "def _spec_to_globs(address_mapper, specs):\n  \"\"\"Given a Specs object, return a PathGlobs object for the build files that it matches.\"\"\"\n  patterns = set()\n  for spec in specs:\n    patterns.update(spec.make_glob_patterns(address_mapper))\n  return PathGlobs(include=patterns, exclude=address_mapper.build_ignore_patterns)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate the list of tasks used to parse Structs from BUILD files.", "response": "def create_graph_rules(address_mapper):\n  \"\"\"Creates tasks used to parse Structs from BUILD files.\n\n  :param address_mapper_key: The subject key for an AddressMapper instance.\n  :param symbol_table: A SymbolTable instance to provide symbols for Address lookups.\n  \"\"\"\n\n  @rule(AddressMapper, [])\n  def address_mapper_singleton():\n    return address_mapper\n\n  return [\n    address_mapper_singleton,\n    # BUILD file parsing.\n    hydrate_struct,\n    parse_address_family,\n    # Spec handling: locate directories that contain build files, and request\n    # AddressFamilies for each of them.\n    addresses_from_address_families,\n    # Root rules representing parameters that might be provided via root subjects.\n    RootRule(Address),\n    RootRule(BuildFileAddress),\n    RootRule(BuildFileAddresses),\n    RootRule(Specs),\n  ]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding binaries tagged to this artifact.", "response": "def with_binaries(self, *args, **kw):\n    \"\"\"Add binaries tagged to this artifact.\n\n    For example: ::\n\n      provides = setup_py(\n        name = 'my_library',\n        zip_safe = True\n      ).with_binaries(\n        my_command = ':my_library_bin'\n      )\n\n    This adds a console_script entry_point for the python_binary target\n    pointed at by :my_library_bin.  Currently only supports\n    python_binaries that specify entry_point explicitly instead of source.\n\n    Also can take a dictionary, e.g.\n    with_binaries({'my-command': ':my_library_bin'})\n    \"\"\"\n    for arg in args:\n      if isinstance(arg, dict):\n        self._binaries.update(arg)\n    self._binaries.update(kw)\n    return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_artifact_info(self):\n    exported = bool(self.provides)\n\n    org = self.provides.org if exported else 'internal'\n    name = self.provides.name if exported else self.identifier\n\n    # TODO(John Sirois): This should return something less than a JarDependency encapsulating just\n    # the org and name.  Perhaps a JarFamily?\n    return JarDependency(org=org, name=name, rev=None), exported", "response": "Returns a tuple describing the jar for this target and a bool indicating if this target is exportable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprompting the user to log in to confluence and returns a connected Confluence object.", "response": "def login(confluence_url, user=None, api_entrypoint='confluence2'):\n    \"\"\"Prompts the user to log in to confluence, and returns a Confluence object.\n    :param confluence_url: Base url of wiki, e.g. https://confluence.atlassian.com/\n    :param user: Username\n    :param api_entrypoint: 'confluence1' or None results in Confluence 3.x. The default\n                           value is 'confluence2' which results in Confluence 4.x or 5.x\n    :rtype: returns a connected Confluence instance\n    raises ConfluenceError if login is unsuccessful.\n    \"\"\"\n    server = ServerProxy(confluence_url + '/rpc/xmlrpc')\n    user = user or getpass.getuser()\n    password = getpass.getpass('Please enter confluence password for %s: ' % user)\n\n    if api_entrypoint in (None, 'confluence1'):\n      # TODO(???) didn't handle this JSirois review comment:\n      #   Can you just switch on in create_html_page?\n      #   Alternatively store a lambda here in each branch.\n      api = server.confluence1\n      fmt = 'markdown'\n    elif api_entrypoint == 'confluence2':\n      api = server.confluence2\n      fmt = 'xhtml'\n    else:\n      raise ConfluenceError(\"Don't understand api_entrypoint %s\" % api_entrypoint)\n\n    try:\n      return Confluence(api, confluence_url, api.login(user, password), fmt)\n    except XMLRPCError as e:\n      raise ConfluenceError('Failed to log in to %s: %s' % (confluence_url, e))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch a page object.", "response": "def getpage(self, wiki_space, page_title):\n    \"\"\" Fetches a page object.\n    Returns None if the page does not exist or otherwise could not be fetched.\n    \"\"\"\n    try:\n      return self._api_entrypoint.getPage(self._session_token, wiki_space, page_title)\n    except XMLRPCError as e:\n      log.warning('Failed to fetch page %s: %s' % (page_title, e))\n      return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef storepage(self, page):\n    try:\n      return self._api_entrypoint.storePage(self._session_token, page)\n    except XMLRPCError as e:\n      log.error('Failed to store page %s: %s' % (page.get('title', '[unknown title]'), e))\n      return None", "response": "Stores a page object updating the page if it already exists. Returns the stored page or None if the page could not be stored."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes a page from confluence.", "response": "def removepage(self, page):\n    \"\"\"Deletes a page from confluence.\n    raises ConfluenceError if the page could not be removed.\n    \"\"\"\n    try:\n      self._api_entrypoint.removePage(self._session_token, page)\n    except XMLRPCError as e:\n      raise ConfluenceError('Failed to delete page: %s' % e)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new confluence page with the given title and content.", "response": "def create(self, space, title, content, parent_page=None, **pageoptions):\n    \"\"\" Create a new confluence page with the given title and content.  Additional page options\n    available in the xmlrpc api can be specified as kwargs.\n    returns the created page or None if the page could not be stored.\n    raises ConfluenceError if a parent page was specified but could not be found.\n    \"\"\"\n\n    pagedef = dict(\n      space = space,\n      title = title,\n      url = Confluence.get_url(self._server_url, space, title),\n      content = content,\n      contentStatus = 'current',\n      current = True\n    )\n    pagedef.update(**pageoptions)\n\n    if parent_page:\n      # Get the parent page id.\n      parent_page_obj = self.getpage(space, parent_page)\n      if parent_page_obj is None:\n        raise ConfluenceError('Failed to find parent page %s in space %s' % (parent_page, space))\n      pagedef['parentId'] = parent_page_obj['id']\n\n    # Now create the page\n    return self.storepage(pagedef)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef addattachment(self, page, filename):\n    mime_type = mimetypes.guess_type(filename, strict=False)[0]\n    if not mime_type:\n      raise ConfluenceError('Failed to detect MIME type of %s' % filename)\n\n    try:\n      with open(filename, 'rb') as f:\n        file_data = f.read()\n\n      attachment = dict(fileName=basename(filename), contentType=mime_type)\n      return self._api_entrypoint.addAttachment(self._session_token,\n                                                page['id'],\n                                                attachment,\n                                                Binary(file_data))\n    except (IOError, OSError) as e:\n      log.error('Failed to read data from file %s: %s' % (filename, str(e)))\n      return None\n    except XMLRPCError:\n      log.error('Failed to add file attachment %s to page: %s' %\n          (filename, page.get('title', '[unknown title]')))\n      return None", "response": "Add an attachment to an existing page."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef execute(self):\n    if JvmResolveSubsystem.global_instance().get_options().resolver != 'ivy':\n      return\n\n    compile_classpath = self.context.products.get_data('compile_classpath',\n        init_func=ClasspathProducts.init_func(self.get_options().pants_workdir))\n\n    targets = self.context.targets()\n    if all(not isinstance(target, JarLibrary) for target in targets):\n      if self._report:\n        self.context.log.info(\"Not generating a report. No resolution performed.\")\n      return\n\n    executor = self.create_java_executor()\n    results = self.resolve(executor=executor,\n                                      targets=targets,\n                                      classpath_products=compile_classpath,\n                                      confs=self.get_options().confs,\n                                      extra_args=self._args)\n    if self._report:\n      results_with_resolved_artifacts = [r for r in results if r.has_resolved_artifacts]\n\n      if not results_with_resolved_artifacts:\n        self.context.log.info(\"Not generating a report. No resolution performed.\")\n      else:\n        for result in results_with_resolved_artifacts:\n          self._generate_ivy_report(result)", "response": "Resolves the specified confs for the configured targets and returns an iterator over the results."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a Snapshot of the input globs", "response": "def _get_snapshot(self):\n    \"\"\"Returns a Snapshot of the input globs\"\"\"\n    return self._scheduler_session.product_request(\n      Snapshot, subjects=[PathGlobs(self._invalidation_globs)])[0]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _enqueue_fs_event(self, event):\n    self._logger.info('enqueuing {} changes for subscription {}'\n                      .format(len(event['files']), event['subscription']))\n    self._event_queue.put(event)", "response": "Enqueue a watchman filesystem event."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading pidfile and returns False if its PID is ours else a printable ( maybe falsey value.", "response": "def _check_pid_changed(self):\n    \"\"\"Reads pidfile and returns False if its PID is ours, else a printable (maybe falsey) value.\"\"\"\n    try:\n      with open(os.path.join(self._build_root, self._pantsd_pidfile), \"r\") as f:\n        pid_from_file = f.read()\n    except IOError:\n      return \"[no file could be read]\"\n    if int(pid_from_file) != os.getpid():\n      return pid_from_file\n    else:\n      return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses the file event queue.", "response": "def _process_event_queue(self):\n    \"\"\"File event notification queue processor. \"\"\"\n    try:\n      event = self._event_queue.get(timeout=0.05)\n    except queue.Empty:\n      return\n\n    try:\n      subscription, is_initial_event, files = (event['subscription'],\n                                               event['is_fresh_instance'],\n                                               event['files'] if PY3 else [f.decode('utf-8') for f in event['files']])\n    except (KeyError, UnicodeDecodeError) as e:\n      self._logger.warn('%r raised by invalid watchman event: %s', e, event)\n      return\n\n    self._logger.debug('processing {} files for subscription {} (first_event={})'\n                       .format(len(files), subscription, is_initial_event))\n\n    # The first watchman event is a listing of all files - ignore it.\n    if not is_initial_event:\n      if subscription == self._fs_event_service.PANTS_PID_SUBSCRIPTION_NAME:\n        self._maybe_invalidate_scheduler_pidfile()\n      else:\n        self._handle_batch_event(files)\n\n    if not self._watchman_is_running.is_set():\n      self._watchman_is_running.set()\n\n    self._event_queue.task_done()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef prefork(self, options, options_bootstrapper):\n    # If any nodes exist in the product graph, wait for the initial watchman event to avoid\n    # racing watchman startup vs invalidation events.\n    graph_len = self._scheduler.graph_len()\n    if graph_len > 0:\n      self._logger.debug('graph len was {}, waiting for initial watchman event'.format(graph_len))\n      self._watchman_is_running.wait()\n    v2_ui = options.for_global_scope().v2_ui\n    zipkin_trace_v2 = options.for_scope('reporting').zipkin_trace_v2\n    session = self._graph_helper.new_session(zipkin_trace_v2, v2_ui)\n\n    if options.for_global_scope().loop:\n      prefork_fn = self._prefork_loop\n    else:\n      prefork_fn = self._prefork_body\n\n    target_roots, exit_code = prefork_fn(session, options, options_bootstrapper)\n    return session, target_roots, exit_code", "response": "Runs all pre - fork logic in the process context of the daemon."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwaiting for the condition for at most the given timeout. Returns True if the condition triggered.", "response": "def wait(self, timeout):\n    \"\"\"Waits for the condition for at most the given timeout and returns True if the condition triggered.\n\n    Generally called in a loop until the condition triggers.\n    \"\"\"\n\n    with self._condition:\n      previous_iteration = self._iteration\n      self._condition.wait(timeout)\n      return previous_iteration != self._iteration"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the target root count in the run tracker s daemon stats object.", "response": "def _set_target_root_count_in_runtracker(self):\n    \"\"\"Sets the target root count in the run tracker's daemon stats object.\"\"\"\n    # N.B. `self._target_roots` is always an expanded list of `Target` objects as\n    # provided by `GoalRunner`.\n    target_count = len(self._target_roots)\n    self.run_tracker.pantsd_stats.set_target_root_size(target_count)\n    return target_count"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the realized target count in the run tracker s daemon stats object.", "response": "def _set_affected_target_count_in_runtracker(self):\n    \"\"\"Sets the realized target count in the run tracker's daemon stats object.\"\"\"\n    target_count = len(self.build_graph)\n    self.run_tracker.pantsd_stats.set_affected_targets_size(target_count)\n    return target_count"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsubmits a work_chain to the background worker pool.", "response": "def submit_background_work_chain(self, work_chain, parent_workunit_name=None):\n    \"\"\"\n    :API: public\n    \"\"\"\n    background_root_workunit = self.run_tracker.get_background_root_workunit()\n    if parent_workunit_name:\n      # We have to keep this workunit alive until all its child work is done, so\n      # we manipulate the context manually instead of using it as a contextmanager.\n      # This is slightly funky, but the with-context usage is so pervasive and\n      # useful elsewhere that it's worth the funkiness in this one place.\n      workunit_parent_ctx = self.run_tracker.new_workunit_under_parent(\n        name=parent_workunit_name, labels=[WorkUnitLabel.MULTITOOL], parent=background_root_workunit)\n      workunit_parent = workunit_parent_ctx.__enter__()\n      done_hook = lambda: workunit_parent_ctx.__exit__(None, None, None)\n    else:\n      workunit_parent = background_root_workunit  # Run directly under the root.\n      done_hook = None\n    self.run_tracker.background_worker_pool().submit_async_work_chain(\n      work_chain, workunit_parent=workunit_parent, done_hook=done_hook)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmaps function f over items in subprocesses and return the result.", "response": "def subproc_map(self, f, items):\n    \"\"\"Map function `f` over `items` in subprocesses and return the result.\n\n      :API: public\n\n      :param f: A multiproc-friendly (importable) work function.\n      :param items: A iterable of pickleable arguments to f.\n    \"\"\"\n    try:\n      # Pool.map (and async_map().get() w/o timeout) can miss SIGINT.\n      # See: http://stackoverflow.com/a/1408476, http://bugs.python.org/issue8844\n      # Instead, we map_async(...), wait *with a timeout* until ready, then .get()\n      # NB: in 2.x, wait() with timeout wakes up often to check, burning CPU. Oh well.\n      res = SubprocPool.foreground().map_async(f, items)\n      while not res.ready():\n        res.wait(60)  # Repeatedly wait for up to a minute.\n        if not res.ready():\n          self.log.debug('subproc_map result still not ready...')\n      return res.get()\n    except KeyboardInterrupt:\n      SubprocPool.shutdown(True)\n      raise"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new workunit under the calling thread s current workunit.", "response": "def new_workunit(self, name, labels=None, cmd='', log_config=None):\n    \"\"\"Create a new workunit under the calling thread's current workunit.\n\n    :API: public\n    \"\"\"\n    with self.run_tracker.new_workunit(name=name, labels=labels, cmd=cmd, log_config=log_config) as workunit:\n      yield workunit"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nacquire the global lock for the root directory associated with this context.", "response": "def acquire_lock(self):\n    \"\"\" Acquire the global lock for the root directory associated with this context. When\n    a goal requires serialization, it will call this to acquire the lock.\n\n    :API: public\n    \"\"\"\n    if self.options.for_global_scope().lock:\n      if not self._lock.acquired:\n        self._lock.acquire()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreleasing the global lock if it s held. Returns True if the lock was released before this call.", "response": "def release_lock(self):\n    \"\"\"Release the global lock if it's held.\n    Returns True if the lock was held before this call.\n\n    :API: public\n    \"\"\"\n    if not self._lock.acquired:\n      return False\n    else:\n      self._lock.release()\n      return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_new_target(self, address, target_type, target_base=None, dependencies=None,\n                     derived_from=None, **kwargs):\n    \"\"\"Creates a new target, adds it to the context and returns it.\n\n    This method ensures the target resolves files against the given target_base, creating the\n    directory if needed and registering a source root.\n\n    :API: public\n    \"\"\"\n    rel_target_base = target_base or address.spec_path\n    abs_target_base = os.path.join(get_buildroot(), rel_target_base)\n    if not os.path.exists(abs_target_base):\n      os.makedirs(abs_target_base)\n      # TODO: Adding source roots on the fly like this is yucky, but hopefully this\n      # method will go away entirely under the new engine. It's primarily used for injecting\n      # synthetic codegen targets, and that isn't how codegen will work in the future.\n    if not self.source_roots.find_by_path(rel_target_base):\n      # TODO: Set the lang and root category (source/test/thirdparty) based on the target type?\n      self.source_roots.add_source_root(rel_target_base)\n    if dependencies:\n      dependencies = [dep.address for dep in dependencies]\n\n    self.build_graph.inject_synthetic_target(address=address,\n                                             target_type=target_type,\n                                             dependencies=dependencies,\n                                             derived_from=derived_from,\n                                             **kwargs)\n    new_target = self.build_graph.get_target(address)\n\n    return new_target", "response": "Creates a new target and adds it to the context and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nselects targets in - play in this run from the target roots and their transitive dependencies.", "response": "def targets(self, predicate=None, **kwargs):\n    \"\"\"Selects targets in-play in this run from the target roots and their transitive dependencies.\n\n    Also includes any new synthetic targets created from the target roots or their transitive\n    dependencies during the course of the run.\n\n    See Target.closure_for_targets for remaining parameters.\n\n    :API: public\n\n    :param predicate: If specified, the predicate will be used to narrow the scope of targets\n                      returned.\n    :param bool postorder: `True` to gather transitive dependencies with a postorder traversal;\n                          `False` or preorder by default.\n    :returns: A list of matching targets.\n    \"\"\"\n    target_set = self._collect_targets(self.target_roots, **kwargs)\n\n    synthetics = OrderedSet()\n    for synthetic_address in self.build_graph.synthetic_addresses:\n      if self.build_graph.get_concrete_derived_from(synthetic_address) in target_set:\n        synthetics.add(self.build_graph.get_target(synthetic_address))\n    target_set.update(self._collect_targets(synthetics, **kwargs))\n\n    return list(filter(predicate, target_set))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dependents(self, on_predicate=None, from_predicate=None):\n    core = set(self.targets(on_predicate))\n    dependees = defaultdict(set)\n    for target in self.targets(from_predicate):\n      for dependency in target.dependencies:\n        if dependency in core:\n          dependees[target].add(dependency)\n    return dependees", "response": "Returns a map from targets that satisfy the from_predicate to targets that depend on that\n      satisfy the on_predicate."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nscanning and parses all BUILD files found under root.", "response": "def scan(self, root=None):\n    \"\"\"Scans and parses all BUILD files found under ``root``.\n\n    Only BUILD files found under ``root`` are parsed as roots in the graph, but any dependencies of\n    targets parsed in the root tree's BUILD files will be followed and this may lead to BUILD files\n    outside of ``root`` being parsed and included in the returned build graph.\n\n    :API: public\n\n    :param string root: The path to scan; by default, the build root.\n    :returns: A new build graph encapsulating the targets found.\n    \"\"\"\n    build_graph = self.build_graph.clone_new()\n    for address in self.address_mapper.scan_addresses(root):\n      build_graph.inject_address_closure(address)\n    return build_graph"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexecute a process asynchronously and returns the output of the process.", "response": "def execute_process_synchronously_without_raising(self, execute_process_request, name, labels=None):\n    \"\"\"Executes a process (possibly remotely), and returns information about its output.\n\n    :param execute_process_request: The ExecuteProcessRequest to run.\n    :param name: A descriptive name representing the process being executed.\n    :param labels: A tuple of WorkUnitLabels.\n    :return: An ExecuteProcessResult with information about the execution.\n\n    Note that this is an unstable, experimental API, which is subject to change with no notice.\n    \"\"\"\n    with self.new_workunit(\n      name=name,\n      labels=labels,\n      cmd=' '.join(execute_process_request.argv),\n    ) as workunit:\n      result = self._scheduler.product_request(FallibleExecuteProcessResult, [execute_process_request])[0]\n      workunit.output(\"stdout\").write(result.stdout)\n      workunit.output(\"stderr\").write(result.stderr)\n      workunit.set_outcome(WorkUnit.FAILURE if result.exit_code else WorkUnit.SUCCESS)\n      return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef execute_process_synchronously_or_raise(self, execute_process_request, name, labels=None):\n    fallible_result = self.execute_process_synchronously_without_raising(execute_process_request, name, labels)\n    return fallible_to_exec_result_or_raise(\n      fallible_result,\n      execute_process_request\n    )", "response": "Execute a process asynchronously and raise if the return code is not 0."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a VersionedTargetSet entry to the report.", "response": "def add_vts(self, task_name, targets, cache_key, valid, phase):\n    \"\"\" Add a single VersionedTargetSet entry to the report.\n    :param InvalidationCacheManager cache_manager:\n    :param CacheKey cache_key:\n    :param bool valid:\n    :param string phase:\n    \"\"\"\n    if task_name not in self._task_reports:\n      self.add_task(task_name)\n    self._task_reports[task_name].add(targets, cache_key, valid, phase)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef report(self, filename=None):\n    # TODO(zundel) set report to stream to the file\n    filename = filename or self._filename\n    if filename:\n      # Usually the directory exists from reporting initialization, but not if clean-all was a goal.\n      with safe_open(filename, 'w') as writer:\n        writer.write(\n          'invocation_id,task_name,targets_hash,target_id,cache_key_id,cache_key_hash,phase,valid'\n          + '\\n')\n        for task_report in self._task_reports.values():\n          task_report.report(writer)", "response": "Write out the report of each versioned target to a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a self - contained application bundle.", "response": "def bundle(self, app, results_dir):\n    \"\"\"Create a self-contained application bundle.\n\n    The bundle will contain the target classes, dependencies and resources.\n    \"\"\"\n    assert(isinstance(app, BundleCreate.App))\n\n    bundle_dir = self.get_bundle_dir(app.id, results_dir)\n    self.context.log.debug('creating {}'.format(os.path.relpath(bundle_dir, get_buildroot())))\n\n    safe_mkdir(bundle_dir, clean=True)\n\n    classpath = OrderedSet()\n\n    # Create symlinks for both internal and external dependencies under `lib_dir`. This is\n    # only needed when not creating a deployjar\n    lib_dir = os.path.join(bundle_dir, self.LIBS_DIR)\n    if not app.deployjar:\n      os.mkdir(lib_dir)\n      consolidated_classpath = self.context.products.get_data('consolidated_classpath')\n      classpath.update(ClasspathProducts.create_canonical_classpath(\n        consolidated_classpath,\n        app.target.closure(bfs=True, **self._target_closure_kwargs),\n        lib_dir,\n        internal_classpath_only=False,\n        excludes=app.binary.deploy_excludes,\n      ))\n\n    bundle_jar = os.path.join(bundle_dir, '{}.jar'.format(app.binary.basename))\n    with self.monolithic_jar(app.binary, bundle_jar,\n                             manifest_classpath=classpath) as jar:\n      self.add_main_manifest_entry(jar, app.binary)\n\n      # Make classpath complete by adding the monolithic jar.\n      classpath.update([jar.path])\n\n    if app.binary.shading_rules:\n      for jar_path in classpath:\n        # In case `jar_path` is a symlink, this is still safe, shaded jar will overwrite jar_path,\n        # original file `jar_path` linked to remains untouched.\n        # TODO run in parallel to speed up\n        self.shade_jar(shading_rules=app.binary.shading_rules, jar_path=jar_path)\n\n    self.symlink_bundles(app, bundle_dir)\n\n    return bundle_dir"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking that the basenames are unique.", "response": "def check_basename_conflicts(self, targets):\n    \"\"\"Apps' basenames are used as bundle directory names. Ensure they are all unique.\"\"\"\n\n    basename_seen = {}\n    for target in targets:\n      if target.basename in basename_seen:\n        raise self.BasenameConflictError('Basename must be unique, found two targets use '\n                                         \"the same basename: {}'\\n\\t{} and \\n\\t{}\"\n                                         .format(target.basename,\n                                                 basename_seen[target.basename].address.spec,\n                                                 target.address.spec))\n      basename_seen[target.basename] = target"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_repro(self):\n    path = self.get_options().capture\n    if path is None:\n      return None\n    buildroot = get_buildroot()\n    # Ignore a couple of common cases. Note: If we support SCMs other than git in the future,\n    # add their (top-level only) metadata dirs here if relevant.\n    ignore = ['.git', os.path.relpath(self.get_options().pants_distdir, buildroot)]\n    if self.get_options().ignore:\n      ignore.extend(self.get_options().ignore)\n    return Repro(path, buildroot, ignore)", "response": "Create a Repro instance for capturing a repro of the current workspace state."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a pair ( description details ) from the obj s docstring.", "response": "def get_description_from_docstring(cls, obj):\n    \"\"\"Returns a pair (description, details) from the obj's docstring.\n\n    description is a single line.\n    details is a list of subsequent lines, possibly empty.\n    \"\"\"\n    doc = obj.__doc__ or ''\n    p = doc.find('\\n')\n    if p == -1:\n      return doc, []\n    else:\n      description = doc[:p]\n      details = textwrap.dedent(doc[p+1:]).splitlines()\n      # Remove leading and trailing empty lines.\n      while details and not details[0].strip():\n        details = details[1:]\n      while details and not details[-1].strip():\n        details.pop()\n\n      recording = True\n      details_without_params = []\n      for detail_line in details:\n        if \":param\" in detail_line:\n          recording = False\n        if not detail_line.strip():\n          recording = True\n        if recording:\n          details_without_params.append(detail_line)\n      return description, details_without_params"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an ordered dict of arg name - > arg description found in the docstring.", "response": "def get_arg_descriptions_from_docstring(cls, obj):\n    \"\"\"Returns an ordered map of arg name -> arg description found in :param: stanzas.\"\"\"\n\n    ret = OrderedDict()\n    name = ''\n    doc = obj.__doc__ or ''\n    lines = [s.strip() for s in doc.split('\\n')]\n    stanza_first_line_re = cls._get_stanza_first_line_re()\n    for line in lines:\n      m = stanza_first_line_re.match(line)\n      if m and m.group(1) == 'param':\n        # If first line of a parameter description, set name and description.\n        name, description = m.group(3, 4)\n        ret[name] = description\n      elif m and m.group(1) != 'param':\n        # If first line of a description of an item other than a parameter, clear name.\n        name = ''\n      elif name and line:\n        # If subsequent line of a parameter description, add to existing description (if any) for\n        # that parameter.\n        ret[name] += (' ' + line) if ret[name] else line\n      # Ignore subsequent lines of descriptions of items other than parameters.\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of FunctionArgs for the specified target_type.", "response": "def get_target_args(self, alias):\n    \"\"\"Returns a list of FunctionArgs for the specified target_type.\"\"\"\n    target_types = list(self._buildfile_aliases.target_types_by_alias.get(alias))\n    if not target_types:\n      raise TaskError('No such target type: {}'.format(alias))\n    return self.get_args_for_target_type(target_types[0])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef initialize(self, all_options):\n    if self.run_info:\n      raise AssertionError('RunTracker.initialize must not be called multiple times.')\n\n    # Initialize the run.\n\n    # Select a globally unique ID for the run, that sorts by time.\n    millis = int((self._run_timestamp * 1000) % 1000)\n    # run_uuid is used as a part of run_id and also as a trace_id for Zipkin tracing\n    run_uuid = uuid.uuid4().hex\n    run_id = 'pants_run_{}_{}_{}'.format(\n      time.strftime('%Y_%m_%d_%H_%M_%S', time.localtime(self._run_timestamp)),\n      millis,\n      run_uuid\n    )\n\n    info_dir = os.path.join(self.get_options().pants_workdir, self.options_scope)\n    self.run_info_dir = os.path.join(info_dir, run_id)\n    self.run_info = RunInfo(os.path.join(self.run_info_dir, 'info'))\n    self.run_info.add_basic_info(run_id, self._run_timestamp)\n    self.run_info.add_info('cmd_line', self._cmd_line)\n\n    # Create a 'latest' symlink, after we add_infos, so we're guaranteed that the file exists.\n    link_to_latest = os.path.join(os.path.dirname(self.run_info_dir), 'latest')\n\n    relative_symlink(self.run_info_dir, link_to_latest)\n\n    # Time spent in a workunit, including its children.\n    self.cumulative_timings = AggregatedTimings(os.path.join(self.run_info_dir,\n                                                             'cumulative_timings'))\n\n    # Time spent in a workunit, not including its children.\n    self.self_timings = AggregatedTimings(os.path.join(self.run_info_dir, 'self_timings'))\n\n    # Hit/miss stats for the artifact cache.\n    self.artifact_cache_stats = ArtifactCacheStats(os.path.join(self.run_info_dir,\n                                                                'artifact_cache_stats'))\n\n    # Daemon stats.\n    self.pantsd_stats = PantsDaemonStats()\n\n    self._all_options = all_options\n\n    return (run_id, run_uuid)", "response": "Create run_info and relevant directories and return the run id."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting tracking this pants run using the given Report.", "response": "def start(self, report, run_start_time=None):\n    \"\"\"Start tracking this pants run using the given Report.\n\n    `RunTracker.initialize` must have been called first to create the run_info_dir and\n    run_info. TODO: This lifecycle represents a delicate dance with the `Reporting.initialize`\n    method, and portions of the `RunTracker` should likely move to `Reporting` instead.\n\n    report: an instance of pants.reporting.Report.\n    \"\"\"\n    if not self.run_info:\n      raise AssertionError('RunTracker.initialize must be called before RunTracker.start.')\n\n    self.report = report\n\n    # Set up the JsonReporter for V2 stats.\n    if self.get_options().stats_version == 2:\n      json_reporter_settings = JsonReporter.Settings(log_level=Report.INFO)\n      self.json_reporter = JsonReporter(self, json_reporter_settings)\n      report.add_reporter('json', self.json_reporter)\n\n    self.report.open()\n\n    # And create the workunit.\n    self._main_root_workunit = WorkUnit(run_info_dir=self.run_info_dir, parent=None,\n                                        name=RunTracker.DEFAULT_ROOT_NAME, cmd=None)\n    self.register_thread(self._main_root_workunit)\n    # Set the true start time in the case of e.g. the daemon.\n    self._main_root_workunit.start(run_start_time)\n    self.report.start_workunit(self._main_root_workunit)\n\n    # Log reporting details.\n    url = self.run_info.get_info('report_url')\n    if url:\n      self.log(Report.INFO, 'See a report at: {}'.format(url))\n    else:\n      self.log(Report.INFO, '(To run a reporting server: ./pants server)')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a (hierarchical) subunit of work for the purpose of timing and reporting. - name: A short name for this work. E.g., 'resolve', 'compile', 'scala', 'zinc'. - labels: An optional iterable of labels. The reporters can use this to decide how to display information about this work. - cmd: An optional longer string representing this work. E.g., the cmd line of a compiler invocation. - log_config: An optional tuple WorkUnit.LogConfig of task-level options affecting reporting. Use like this: with run_tracker.new_workunit(name='compile', labels=[WorkUnitLabel.TASK]) as workunit: <do scoped work here> <set the outcome on workunit if necessary> Note that the outcome will automatically be set to failure if an exception is raised in a workunit, and to success otherwise, so usually you only need to set the outcome explicitly if you want to set it to warning. :API: public", "response": "def new_workunit(self, name, labels=None, cmd='', log_config=None):\n    \"\"\"Creates a (hierarchical) subunit of work for the purpose of timing and reporting.\n\n    - name: A short name for this work. E.g., 'resolve', 'compile', 'scala', 'zinc'.\n    - labels: An optional iterable of labels. The reporters can use this to decide how to\n              display information about this work.\n    - cmd: An optional longer string representing this work.\n           E.g., the cmd line of a compiler invocation.\n    - log_config: An optional tuple WorkUnit.LogConfig of task-level options affecting reporting.\n\n    Use like this:\n\n    with run_tracker.new_workunit(name='compile', labels=[WorkUnitLabel.TASK]) as workunit:\n      <do scoped work here>\n      <set the outcome on workunit if necessary>\n\n    Note that the outcome will automatically be set to failure if an exception is raised\n    in a workunit, and to success otherwise, so usually you only need to set the\n    outcome explicitly if you want to set it to warning.\n\n    :API: public\n    \"\"\"\n    parent = self._threadlocal.current_workunit\n    with self.new_workunit_under_parent(name, parent=parent, labels=labels, cmd=cmd,\n                                        log_config=log_config) as workunit:\n      self._threadlocal.current_workunit = workunit\n      try:\n        yield workunit\n      finally:\n        self._threadlocal.current_workunit = parent"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef new_workunit_under_parent(self, name, parent, labels=None, cmd='', log_config=None):\n    workunit = WorkUnit(run_info_dir=self.run_info_dir, parent=parent, name=name, labels=labels,\n                        cmd=cmd, log_config=log_config)\n    workunit.start()\n\n    outcome = WorkUnit.FAILURE  # Default to failure we will override if we get success/abort.\n    try:\n      self.report.start_workunit(workunit)\n      yield workunit\n    except KeyboardInterrupt:\n      outcome = WorkUnit.ABORTED\n      self._aborted = True\n      raise\n    else:\n      outcome = WorkUnit.SUCCESS\n    finally:\n      workunit.set_outcome(outcome)\n      self.end_workunit(workunit)", "response": "Create a new workunit under a parent."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef log(self, level, *msg_elements):\n    self.report.log(self._threadlocal.current_workunit, level, *msg_elements)", "response": "Log a message against the current workunit."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npost stats to the given url.", "response": "def post_stats(cls, stats_url, stats, timeout=2, auth_provider=None):\n    \"\"\"POST stats to the given url.\n\n    :return: True if upload was successful, False otherwise.\n    \"\"\"\n    def error(msg):\n      # Report aleady closed, so just print error.\n      print('WARNING: Failed to upload stats to {}. due to {}'.format(stats_url, msg),\n            file=sys.stderr)\n      return False\n\n    # TODO(benjy): The upload protocol currently requires separate top-level params, with JSON\n    # values.  Probably better for there to be one top-level JSON value, namely json.dumps(stats).\n    # But this will first require changing the upload receiver at every shop that uses this.\n    params = {k: cls._json_dump_options(v) for (k, v) in stats.items()}\n    cookies = Cookies.global_instance()\n    auth_provider = auth_provider or '<provider>'\n\n    # We can't simply let requests handle redirects, as we only allow them for specific codes:\n    # 307 and 308 indicate that the redirected request must use the same method, POST in this case.\n    # So they indicate a true redirect of the POST itself, and we allow them.\n    # The other redirect codes either must, or in practice do, cause the user agent to switch the\n    # method to GET. So when they are encountered on a POST, it indicates an auth problem (a\n    # redirection to a login page).\n    def do_post(url, num_redirects_allowed):\n      if num_redirects_allowed < 0:\n        return error('too many redirects.')\n      r = requests.post(url, data=params, timeout=timeout,\n                        cookies=cookies.get_cookie_jar(), allow_redirects=False)\n      if r.status_code in {307, 308}:\n        return do_post(r.headers['location'], num_redirects_allowed - 1)\n      elif r.status_code != 200:\n        error('HTTP error code: {}. Reason: {}.'.format(r.status_code, r.reason))\n        if 300 <= r.status_code < 400 or r.status_code == 401:\n          print('Use `path/to/pants login --to={}` to authenticate against the stats '\n                'upload service.'.format(auth_provider), file=sys.stderr)\n        return False\n      return True\n\n    try:\n      return do_post(stats_url, num_redirects_allowed=6)\n    except Exception as e:  # Broad catch - we don't want to fail the build over upload errors.\n      return error('Error: {}'.format(e))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting stats to a local json file.", "response": "def write_stats_to_json(cls, file_name, stats):\n    \"\"\"Write stats to a local json file.\"\"\"\n    params = cls._json_dump_options(stats)\n    mode = 'w' if PY3 else 'wb'\n    try:\n      safe_file_dump(file_name, params, mode=mode)\n    except Exception as e: # Broad catch - we don't want to fail in stats related failure.\n      print('WARNING: Failed to write stats to {} due to Error: {}'.format(file_name, e),\n            file=sys.stderr)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstoring stats about this run in local and optionally remote stats dbs.", "response": "def store_stats(self):\n    \"\"\"Store stats about this run in local and optionally remote stats dbs.\"\"\"\n    stats = self._stats()\n\n    # Write stats to user-defined json file.\n    stats_json_file_name = self.get_options().stats_local_json_file\n    if stats_json_file_name:\n      self.write_stats_to_json(stats_json_file_name, stats)\n\n    # Upload to remote stats db.\n    stats_upload_urls = copy.copy(self.get_options().stats_upload_urls)\n    timeout = self.get_options().stats_upload_timeout\n    for stats_url, auth_provider in stats_upload_urls.items():\n      self.post_stats(stats_url, stats, timeout=timeout, auth_provider=auth_provider)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef end(self):\n    if self._end_memoized_result is not None:\n      return self._end_memoized_result\n    if self._background_worker_pool:\n      if self._aborted:\n        self.log(Report.INFO, \"Aborting background workers.\")\n        self._background_worker_pool.abort()\n      else:\n        self.log(Report.INFO, \"Waiting for background workers to finish.\")\n        self._background_worker_pool.shutdown()\n      self.end_workunit(self._background_root_workunit)\n\n    self.shutdown_worker_pool()\n\n    # Run a dummy work unit to write out one last timestamp.\n    with self.new_workunit(\"complete\"):\n      pass\n\n    self.end_workunit(self._main_root_workunit)\n\n    outcome = self._main_root_workunit.outcome()\n    if self._background_root_workunit:\n      outcome = min(outcome, self._background_root_workunit.outcome())\n    outcome_str = WorkUnit.outcome_string(outcome)\n    log_level = RunTracker._log_levels[outcome]\n    self.log(log_level, outcome_str)\n\n    if self.run_info.get_info('outcome') is None:\n      # If the goal is clean-all then the run info dir no longer exists, so ignore that error.\n      self.run_info.add_info('outcome', outcome_str, ignore_errors=True)\n\n    if self._target_to_data:\n      self.run_info.add_info('target_data', self._target_to_data)\n\n    self.report.close()\n    self.store_stats()\n\n    run_failed = outcome in [WorkUnit.FAILURE, WorkUnit.ABORTED]\n    result = PANTS_FAILED_EXIT_CODE if run_failed else PANTS_SUCCEEDED_EXIT_CODE\n    self._end_memoized_result = result\n    return self._end_memoized_result", "response": "Ends the pants run."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_critical_path_timings(self):\n    setup_workunit = WorkUnitLabel.SETUP.lower()\n    transitive_dependencies = dict()\n    for goal_info in self._sorted_goal_infos:\n      deps = transitive_dependencies.setdefault(goal_info.goal.name, set())\n      for dep in goal_info.goal_dependencies:\n        deps.add(dep.name)\n        deps.update(transitive_dependencies.get(dep.name))\n      # Add setup workunit as a dep manually, as its unaccounted for, otherwise.\n      deps.add(setup_workunit)\n    raw_timings = dict()\n    for entry in self.cumulative_timings.get_all():\n      raw_timings[entry[\"label\"]] = entry[\"timing\"]\n\n    critical_path_timings = AggregatedTimings()\n\n    def add_to_timings(goal, dep):\n      tracking_label = get_label(goal)\n      timing_label = get_label(dep)\n      critical_path_timings.add_timing(tracking_label, raw_timings.get(timing_label, 0.0))\n\n    def get_label(dep):\n      return \"{}:{}\".format(RunTracker.DEFAULT_ROOT_NAME, dep)\n\n    # Add setup workunit to critical_path_timings manually, as its unaccounted for, otherwise.\n    add_to_timings(setup_workunit, setup_workunit)\n\n    for goal, deps in transitive_dependencies.items():\n      add_to_timings(goal, goal)\n      for dep in deps:\n        add_to_timings(goal, dep)\n\n    return critical_path_timings", "response": "Get the cumulative timings of each goal and all of the goals it depended on."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlooking up an option scope and optionally option therein in the options parsed by Pants.", "response": "def _get_option_to_record(self, scope, option=None):\n    \"\"\"Looks up an option scope (and optionally option therein) in the options parsed by Pants.\n\n    Returns a dict of of all options in the scope, if option is None.\n    Returns the specific option if option is not None.\n    Raises ValueError if scope or option could not be found.\n    \"\"\"\n    scope_to_look_up = scope if scope != 'GLOBAL' else ''\n    try:\n      value = self._all_options.for_scope(scope_to_look_up, inherit_from_enclosing_scope=False).as_dict()\n      if option is None:\n        return value\n      else:\n        return value[option]\n    except (Config.ConfigValidationError, AttributeError) as e:\n      raise ValueError(\"Couldn't find option scope {}{} for recording ({})\".format(\n        scope,\n        \"\" if option is None else \" option {}\".format(option),\n        e\n      ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _create_dict_with_nested_keys_and_val(cls, keys, value):\n\n    if len(keys) > 1:\n      new_keys = keys[:-1]\n      new_val = {keys[-1]: value}\n      return cls._create_dict_with_nested_keys_and_val(new_keys, new_val)\n    elif len(keys) == 1:\n      return {keys[0]: value}\n    else:\n      raise ValueError('Keys must contain at least one key.')", "response": "Recursively creates a nested dictionary with the keys pointing to the value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _merge_list_of_keys_into_dict(cls, data, keys, value, index=0):\n    if len(keys) == 0 or index < 0 or index >= len(keys):\n      raise ValueError('Keys must contain at least one key and index must be'\n                       'an integer greater than 0 and less than the number of keys.')\n    if len(keys) < 2 or not data:\n      new_data_to_add = cls._create_dict_with_nested_keys_and_val(keys, value)\n      data.update(new_data_to_add)\n\n    this_keys_contents = data.get(keys[index])\n    if this_keys_contents:\n      if isinstance(this_keys_contents, dict):\n        cls._merge_list_of_keys_into_dict(this_keys_contents, keys, value, index + 1)\n      elif index < len(keys) - 1:\n        raise ValueError('Keys must point to a dictionary.')\n      else:\n        data[keys[index]] = value\n    else:\n      new_keys = keys[index:]\n      new_data_to_add = cls._create_dict_with_nested_keys_and_val(new_keys, value)\n      data.update(new_data_to_add)", "response": "Recursively merges a list of keys into a single dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding target information to run_info under target_data.", "response": "def report_target_info(self, scope, target, keys, val):\n    \"\"\"Add target information to run_info under target_data.\n\n    Will Recursively construct a nested dict with the keys provided.\n\n    Primitive values can be overwritten with other primitive values,\n    but a primitive value cannot be overwritten with a dictionary.\n\n    For example:\n    Where the dictionary being updated is {'a': {'b': 16}}, reporting the value\n    15 with the key list ['a', 'b'] will result in {'a': {'b':15}};\n    but reporting the value 20 with the key list ['a', 'b', 'c'] will throw\n    an error.\n\n    :param string scope: The scope for which we are reporting the information.\n    :param target: The target for which we want to store information.\n    :type target: :class:`pants.build_graph.target.Target`\n    :param list of string keys: The keys that will be recursively\n           nested and pointing to the information being stored.\n    :param primitive val: The value of the information being stored.\n\n    :API: public\n    \"\"\"\n    new_key_list = [target.address.spec, scope]\n    new_key_list += keys\n    self._merge_list_of_keys_into_dict(self._target_to_data, new_key_list, val, 0)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef runner(self, classpath, main, jvm_options=None, args=None, cwd=None):\n    return self._runner(*self._scrub_args(classpath, main, jvm_options, args, cwd=cwd))", "response": "Returns an Executor. Runner for the given java command."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlaunches the java program defined by the classpath and main.", "response": "def execute(self, classpath, main, jvm_options=None, args=None, stdout=None, stderr=None,\n      cwd=None):\n    \"\"\"Launches the java program defined by the classpath and main.\n\n    :param list classpath: the classpath for the java program\n    :param string main: the fully qualified class name of the java program's entry point\n    :param list jvm_options: an optional sequence of options for the underlying jvm\n    :param list args: an optional sequence of args to pass to the java program\n    :param string cwd: optionally set the working directory\n\n    Returns the exit code of the java program.\n    Raises Executor.Error if there was a problem launching java itself.\n    \"\"\"\n    runner = self.runner(classpath=classpath, main=main, jvm_options=jvm_options, args=args,\n                         cwd=cwd)\n    return runner.run(stdout=stdout, stderr=stderr)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nspawn the java program passing any extra subprocess kwargs on to subprocess. Popen.", "response": "def spawn(self, classpath, main, jvm_options=None, args=None, cwd=None, **subprocess_args):\n    \"\"\"Spawns the java program passing any extra subprocess kwargs on to subprocess.Popen.\n\n    Returns the Popen process object handle to the spawned java program subprocess.\n\n    :API: public\n\n    :raises: :class:`Executor.Error` if there is a problem spawning the subprocess.\n    \"\"\"\n    cwd = cwd or os.getcwd()\n    cmd = self._create_command(*self._scrub_args(classpath, main, jvm_options, args, cwd=cwd))\n    return self._spawn(cmd, cwd, **subprocess_args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the exit timeout for the remainder of the sequence.", "response": "def _set_exit_timeout(self, timeout, reason):\n    \"\"\"Set a timeout for the remainder of the session, along with an exception to raise.\n    which is implemented by NailgunProtocol.\n\n    This method may be called by a signal handler to set a timeout for the remainder of the\n    session. If the session completes before the timeout does, the exception in `reason` is\n    raised. Otherwise, `NailgunProtocol.ProcessStreamTimeout` is raised.\n\n    :param float timeout: The length of time to time out, in seconds.\n    :param Exception reason: The exception to raise if the session completes before the timeout\n                             occurs.\n    \"\"\"\n    self._exit_timeout_start_time = time.time()\n    self._exit_timeout = timeout\n    self._exit_reason = reason"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nimplement the NailgunProtocol. TimeoutProvider interface.", "response": "def maybe_timeout_options(self):\n    \"\"\"Implements the NailgunProtocol.TimeoutProvider interface.\"\"\"\n    if self._exit_timeout_start_time:\n      return NailgunProtocol.TimeoutOptions(self._exit_timeout_start_time, self._exit_timeout)\n    else:\n      return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite a payload to a given fd and flush the fd.", "response": "def _write_flush(self, fd, payload=None):\n    \"\"\"Write a payload to a given fd (if provided) and flush the fd.\"\"\"\n    try:\n      if payload:\n        fd.write(ensure_binary(payload))\n      fd.flush()\n    except (IOError, OSError) as e:\n      # If a `Broken Pipe` is encountered during a stdio fd write, we're headless - bail.\n      if e.errno == errno.EPIPE and self._exit_on_broken_pipe:\n        sys.exit()\n      # Otherwise, re-raise.\n      raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _process_session(self):\n    try:\n      for chunk_type, payload in self.iter_chunks(self._sock, return_bytes=True,\n                                                  timeout_object=self):\n        # TODO(#6579): assert that we have at this point received all the chunk types in\n        # ChunkType.REQUEST_TYPES, then require PID and PGRP (exactly once?), and then allow any of\n        # ChunkType.EXECUTION_TYPES.\n        if chunk_type == ChunkType.STDOUT:\n          self._write_flush(self._stdout, payload)\n        elif chunk_type == ChunkType.STDERR:\n          self._write_flush(self._stderr, payload)\n        elif chunk_type == ChunkType.EXIT:\n          self._write_flush(self._stdout)\n          self._write_flush(self._stderr)\n          return int(payload)\n        elif chunk_type == ChunkType.PID:\n          self.remote_pid = int(payload)\n          self.remote_process_cmdline = psutil.Process(self.remote_pid).cmdline()\n          if self._remote_pid_callback:\n            self._remote_pid_callback(self.remote_pid)\n        elif chunk_type == ChunkType.PGRP:\n          self.remote_pgrp = int(payload)\n          if self._remote_pgrp_callback:\n            self._remote_pgrp_callback(self.remote_pgrp)\n        elif chunk_type == ChunkType.START_READING_INPUT:\n          self._maybe_start_input_writer()\n        else:\n          raise self.ProtocolError('received unexpected chunk {} -> {}'.format(chunk_type, payload))\n    except NailgunProtocol.ProcessStreamTimeout as e:\n      assert(self.remote_pid is not None)\n      # NB: We overwrite the process title in the pantsd-runner process, which causes it to have an\n      # argv with lots of empty spaces for some reason. We filter those out and pretty-print the\n      # rest here.\n      filtered_remote_cmdline = safe_shlex_join(\n        arg for arg in self.remote_process_cmdline if arg != '')\n      logger.warning(\n        \"timed out when attempting to gracefully shut down the remote client executing \\\"{}\\\". \"\n        \"sending SIGKILL to the remote client at pid: {}. message: {}\"\n        .format(filtered_remote_cmdline, self.remote_pid, e))\n    finally:\n      # Bad chunk types received from the server can throw NailgunProtocol.ProtocolError in\n      # NailgunProtocol.iter_chunks(). This ensures the NailgunStreamWriter is always stopped.\n      self._maybe_stop_input_writer()\n      # If an asynchronous error was set at any point (such as in a signal handler), we want to make\n      # sure we clean up the remote process before exiting with error.\n      if self._exit_reason:\n        if self.remote_pgrp:\n          safe_kill(self.remote_pgrp, signal.SIGKILL)\n        if self.remote_pid:\n          safe_kill(self.remote_pid, signal.SIGKILL)\n        raise self._exit_reason", "response": "Process the outputs of the nailgun session."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef try_connect(self):\n    sock = RecvBufferedSocket(socket.socket(socket.AF_INET, socket.SOCK_STREAM))\n    try:\n      sock.connect(self._address)\n    except (socket.error, socket.gaierror) as e:\n      logger.debug('Encountered socket exception {!r} when attempting connect to nailgun'.format(e))\n      sock.close()\n      raise self.NailgunConnectionError(\n        address=self._address_string,\n        pid=self._maybe_last_pid(),\n        pgrp=self._maybe_last_pgrp(),\n        wrapped_exc=e,\n      )\n    else:\n      return sock", "response": "Creates a socket connects it to the nailgun and returns the connected socket."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends the signal signum send if the PID and PGRP chunks have been received.", "response": "def maybe_send_signal(self, signum, include_pgrp=True):\n    \"\"\"Send the signal `signum` send if the PID and/or PGRP chunks have been received.\n\n    No error is raised if the pid or pgrp are None or point to an already-dead process.\n    \"\"\"\n    remote_pid = self._maybe_last_pid()\n    if remote_pid is not None:\n      safe_kill(remote_pid, signum)\n    if include_pgrp:\n      remote_pgrp = self._maybe_last_pgrp()\n      if remote_pgrp:\n        safe_kill(remote_pgrp, signum)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes the given main_class with any supplied args in the given environment.", "response": "def execute(self, main_class, cwd=None, *args, **environment):\n    \"\"\"Executes the given main_class with any supplied args in the given environment.\n\n    :param string main_class: the fully qualified class name of the main entrypoint\n    :param string cwd: Set the working directory for this command\n    :param list args: any arguments to pass to the main entrypoint\n    :param dict environment: an env mapping made available to native nails via the nail context\n    :returns: the exit code of the main_class.\n    :raises: :class:`NailgunClient.NailgunError` if there was an error during execution.\n    \"\"\"\n    environment = dict(**environment)\n    environment.update(self.ENV_DEFAULTS)\n    cwd = cwd or self._workdir\n\n    sock = self.try_connect()\n\n    # TODO(#6579): NailgunClientSession currently requires callbacks because it can't depend on\n    # having received these chunks, so we need to avoid clobbering these fields until we initialize\n    # a new session.\n    self._current_remote_pid = None\n    self._current_remote_pgrp = None\n    self._session = NailgunClientSession(\n      sock=sock,\n      in_file=self._stdin,\n      out_file=self._stdout,\n      err_file=self._stderr,\n      exit_on_broken_pipe=self._exit_on_broken_pipe,\n      remote_pid_callback=self._receive_remote_pid,\n      remote_pgrp_callback=self._receive_remote_pgrp)\n    try:\n      return self._session.execute(cwd, main_class, *args, **environment)\n    except (socket.error, NailgunProtocol.ProtocolError) as e:\n      raise self.NailgunError(\n        address=self._address_string,\n        pid=self._maybe_last_pid(),\n        pgrp=self._maybe_last_pgrp(),\n        wrapped_exc=e,\n      )\n    finally:\n      sock.close()\n      self._session = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _fetch_pkg(self, gopath, pkg, rev):\n    fetcher = self._get_fetcher(pkg)\n    root = fetcher.root()\n    root_dir = os.path.join(self.workdir, 'fetches', root, rev)\n\n    # Only fetch each remote root once.\n    if not os.path.exists(root_dir):\n      with temporary_dir() as tmp_fetch_root:\n        with self.context.new_workunit('fetch {}'.format(pkg)):\n          fetcher.fetch(dest=tmp_fetch_root, rev=rev)\n          safe_mkdir(root_dir)\n          for path in os.listdir(tmp_fetch_root):\n            shutil.move(os.path.join(tmp_fetch_root, path), os.path.join(root_dir, path))\n\n    # TODO(John Sirois): Circle back and get get rid of this symlink tree.\n    # GoWorkspaceTask will further symlink a single package from the tree below into a\n    # target's workspace when it could just be linking from the fetch_dir.  The only thing\n    # standing in the way is a determination of what we want to artifact cache.  If we don't\n    # want to cache fetched zips, linking straight from the fetch_dir works simply.  Otherwise\n    # thought needs to be applied to using the artifact cache directly or synthesizing a\n    # canonical owner target for the fetched files that 'child' targets (subpackages) can\n    # depend on and share the fetch from.\n    dest_dir = os.path.join(gopath, 'src', root)\n    # We may have been `invalidate`d and not `clean-all`ed so we need a new empty symlink\n    # chroot to avoid collision; thus `clean=True`.\n    safe_mkdir(dest_dir, clean=True)\n    for path in os.listdir(root_dir):\n      os.symlink(os.path.join(root_dir, path), os.path.join(dest_dir, path))", "response": "Fetch the package and setup symlinks."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _resolve(self, dependent_remote_lib, address, pkg, rev, implicit_ok):\n    try:\n      self.context.build_graph.inject_address_closure(address)\n    except AddressLookupError:\n      if implicit_ok:\n        self.context.add_new_target(address=address,\n                                    target_base=dependent_remote_lib.target_base,\n                                    target_type=GoRemoteLibrary,\n                                    pkg=pkg,\n                                    rev=rev)\n      else:\n        raise self.UndeclaredRemoteLibError(address)\n    return self.context.build_graph.get_target(address)", "response": "Resolves the GoRemoteLibrary at address defining the given pkg."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_remote_import_paths(self, pkg, gopath=None):\n    import_listing = self.import_oracle.list_imports(pkg, gopath=gopath)\n    return [imp for imp in import_listing.imports\n            if (not self.import_oracle.is_go_internal_import(imp) and\n                # We assume relative imports are local to the package and skip attempts to\n                # recursively resolve them.\n                not self._is_relative(imp))]", "response": "Returns the remote import paths declared by the given remote Go package."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_import_root_map_file(path):\n    if os.path.exists(path):\n      with open(path, 'r') as fp:\n        return dict({import_path: root for import_path, root in\n                     (x.strip().split('\\t') for x in fp.readlines())})\n    else:\n      return {}", "response": "Reads a file mapping import paths to roots."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting a file mapping import paths to roots.", "response": "def _write_import_root_map_file(path, import_root_map):\n    \"\"\"Writes a file mapping import paths to roots.\"\"\"\n    with safe_concurrent_creation(path) as safe_path:\n      with open(safe_path, 'w') as fp:\n        for import_path, root in sorted(import_root_map.items()):\n          fp.write('{}\\t{}\\n'.format(import_path, root))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a filename walk its ast and determine if it declares a namespace package.", "response": "def declares_namespace_package(filename):\n  \"\"\"Given a filename, walk its ast and determine if it declares a namespace package.\"\"\"\n\n  import ast\n\n  with open(filename) as fp:\n    init_py = ast.parse(fp.read(), filename)\n  calls = [node for node in ast.walk(init_py) if isinstance(node, ast.Call)]\n  for call in calls:\n    if len(call.args) != 1:\n      continue\n    if isinstance(call.func, ast.Attribute) and call.func.attr != 'declare_namespace':\n      continue\n    if isinstance(call.func, ast.Name) and call.func.id != 'declare_namespace':\n      continue\n    if isinstance(call.args[0], ast.Name) and call.args[0].id == '__name__':\n      return True\n  return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating an iterator over the target s siblings and ancestor lineage.", "response": "def iter_target_siblings_and_ancestors(self, target):\n    \"\"\"Produces an iterator over a target's siblings and ancestor lineage.\n\n    :returns: A target iterator yielding the target and its siblings and then it ancestors from\n              nearest to furthest removed.\n    \"\"\"\n    def iter_targets_in_spec_path(spec_path):\n      try:\n        siblings = SiblingAddresses(spec_path)\n        for address in self._build_graph.inject_specs_closure([siblings]):\n          yield self._build_graph.get_target(address)\n      except AddressLookupError:\n        # A spec path may not have any addresses registered under it and that's ok.\n        # For example:\n        #  a:a\n        #  a/b/c:c\n        #\n        # Here a/b contains no addresses.\n        pass\n\n    def iter_siblings_and_ancestors(spec_path):\n      for sibling in iter_targets_in_spec_path(spec_path):\n        yield sibling\n      parent_spec_path = os.path.dirname(spec_path)\n      if parent_spec_path != spec_path:\n        for parent in iter_siblings_and_ancestors(parent_spec_path):\n          yield parent\n\n    for target in iter_siblings_and_ancestors(target.address.spec_path):\n      yield target"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwalk the dependency graph for the given target.", "response": "def _walk(self, target, visitor):\n    \"\"\"Walks the dependency graph for the given target.\n\n    :param target: The target to start the walk from.\n    :param visitor: A function that takes a target and returns `True` if its dependencies should\n                    also be visited.\n    \"\"\"\n    visited = set()\n\n    def walk(current):\n      if current not in visited:\n        visited.add(current)\n        keep_going = visitor(current)\n        if keep_going:\n          for dependency in self.dependencies(current):\n            walk(dependency)\n\n    walk(target)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _closure(self, target):\n    closure = set()\n\n    def collect(current):\n      closure.add(current)\n      return True\n    self._walk(target, collect)\n\n    return closure", "response": "Return the target closure as defined by this dependency calculator s definition of a walk."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the reduced transitive dependencies for an exported target.", "response": "def reduced_dependencies(self, exported_target):\n    \"\"\"Calculates the reduced transitive dependencies for an exported target.\n\n    The reduced set of dependencies will be just those transitive dependencies \"owned\" by\n    the `exported_target`.\n\n    A target is considered \"owned\" if:\n    1. It's \"3rdparty\" and \"directly reachable\" from `exported_target` by at least 1 path.\n    2. It's not \"3rdparty\" and not \"directly reachable\" by any of `exported_target`'s \"3rdparty\"\n       dependencies.\n\n    Here \"3rdparty\" refers to targets identified as either `is_third_party` or `is_exported`.\n\n    And in this context \"directly reachable\" means the target can be reached by following a series\n    of dependency links from the `exported_target`, never crossing another exported target and\n    staying within the `exported_target` address space.  It's the latter restriction that allows for\n    unambiguous ownership of exportable targets and mirrors the BUILD file convention of targets\n    only being able to own sources in their filesystem subtree.  The single ambiguous case that can\n    arise is when there is more than one exported target in the same BUILD file family that can\n    \"directly reach\" a target in its address space.\n\n    :raises: `UnExportedError` if the given `exported_target` is not, in-fact, exported.\n    :raises: `NoOwnerError` if a transitive dependency is found with no proper owning exported\n             target.\n    :raises: `AmbiguousOwnerError` if there is more than one viable exported owner target for a\n             given transitive dependency.\n    \"\"\"\n    # The strategy adopted requires 3 passes:\n    # 1.) Walk the exported target to collect provisional owned exportable targets, but _not_\n    #     3rdparty since these may be introduced by exported subgraphs we discover in later steps!\n    # 2.) Determine the owner of each target collected in 1 by walking the ancestor chain to find\n    #     the closest exported target.  The ancestor chain is just all targets whose spec path is\n    #     a prefix of the descendant.  In other words, all targets in descendant's BUILD file family\n    #     (its siblings), all targets in its parent directory BUILD file family, and so on.\n    # 3.) Finally walk the exported target once more, replacing each visited dependency with its\n    #     owner.\n\n    if not self.is_exported(exported_target):\n      raise self.UnExportedError('Cannot calculate reduced dependencies for a non-exported '\n                                 'target, given: {}'.format(exported_target))\n\n    owner_by_owned_python_target = OrderedDict()\n\n    # Only check ownership on the original target graph.\n    original_exported_target = exported_target.derived_from\n\n    def collect_potentially_owned_python_targets(current):\n      if current.is_original:\n        owner_by_owned_python_target[current] = None  # We can't know the owner in the 1st pass.\n      return (current == exported_target) or not self.is_exported(current)\n\n    self._walk(original_exported_target, collect_potentially_owned_python_targets)\n\n    for owned in owner_by_owned_python_target:\n      if self.requires_export(owned) and not self.is_exported(owned):\n        potential_owners = set()\n        for potential_owner in self._ancestor_iterator.iter_target_siblings_and_ancestors(owned):\n          if self.is_exported(potential_owner) and owned in self._closure(potential_owner):\n            potential_owners.add(potential_owner)\n        if not potential_owners:\n          raise self.NoOwnerError('No exported target owner found for {}'.format(owned))\n        owner = potential_owners.pop()\n        if potential_owners:\n          ambiguous_owners = [o for o in potential_owners\n                              if o.address.spec_path == owner.address.spec_path]\n          if ambiguous_owners:\n            raise self.AmbiguousOwnerError('Owners for {} are ambiguous.  Found {} and '\n                                           '{} others: {}'.format(owned,\n                                                                  owner,\n                                                                  len(ambiguous_owners),\n                                                                  ambiguous_owners))\n        owner_by_owned_python_target[owned] = owner\n\n    reduced_dependencies = OrderedSet()\n\n    def collect_reduced_dependencies(current):\n      if current == exported_target:\n        return True\n      else:\n        # The provider will be one of:\n        # 1. `None`, ie: a 3rdparty requirement we should collect.\n        # 2. `exported_target`, ie: a local exportable target owned by `exported_target` that we\n        #    should collect\n        # 3. Or else a local exportable target owned by some other exported target in which case\n        #    we should collect the exported owner.\n        owner = owner_by_owned_python_target.get(current)\n        if owner is None or owner == exported_target:\n          reduced_dependencies.add(current)\n        else:\n          reduced_dependencies.add(owner)\n        return owner == exported_target or not self.requires_export(current)\n\n    self._walk(exported_target, collect_reduced_dependencies)\n    return OrderedSet(d for d in reduced_dependencies if d.is_original)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef iter_entry_points(cls, target):\n    for name, binary_target in target.provided_binaries.items():\n      concrete_target = binary_target\n      if not isinstance(concrete_target, PythonBinary) or concrete_target.entry_point is None:\n        raise TargetDefinitionException(target,\n            'Cannot add a binary to a PythonArtifact if it does not contain an entry_point.')\n      yield name, concrete_target.entry_point", "response": "Yields the name entry_point pairs of binary targets in this PythonArtifact."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef nearest_subpackage(cls, package, all_packages):\n    def shared_prefix(candidate):\n      zipped = zip(package.split('.'), candidate.split('.'))\n      matching = itertools.takewhile(lambda pair: pair[0] == pair[1], zipped)\n      return [pair[0] for pair in matching]\n    shared_packages = [_f for _f in map(shared_prefix, all_packages) if _f]\n    return '.'.join(max(shared_packages, key=len)) if shared_packages else package", "response": "Given a package find its nearest parent in all_packages."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetect packages namespace packages and resources from an existing chroot.", "response": "def find_packages(self, root_target, chroot):\n    \"\"\"Detect packages, namespace packages and resources from an existing chroot.\n\n    :returns: a tuple of:\n                set(packages)\n                set(namespace_packages)\n                map(package => set(files))\n    \"\"\"\n    base = os.path.join(chroot.path(), self.SOURCE_ROOT)\n    packages, namespace_packages = set(), set()\n    resources = defaultdict(set)\n\n    def iter_files():\n      for root, _, files in safe_walk(base):\n        module = os.path.relpath(root, base).replace(os.path.sep, '.')\n        for filename in files:\n          yield module, filename, os.path.join(root, filename)\n\n    # establish packages, namespace packages in first pass\n    inits_to_check = {}\n    for module, filename, real_filename in iter_files():\n      if filename != '__init__.py':\n        continue\n      packages.add(module)\n      inits_to_check[real_filename] = module\n    namespace_packages = {inits_to_check[init]\n                          for init in self.filter_namespace_packages(root_target,\n                                                                     inits_to_check.keys())}\n\n    # second pass establishes non-source content (resources)\n    for module, filename, real_filename in iter_files():\n      if filename.endswith('.py'):\n        if module not in packages:\n          # TODO(wickman) Consider changing this to a full-on error as it could indicate bad BUILD\n          # hygiene.\n          # raise cls.UndefinedSource('{} is source but does not belong to a package!'\n          #                           .format(filename))\n          self.context.log.warn('{} is source but does not belong to a package.'\n                                .format(real_filename))\n        else:\n          continue\n      submodule = self.nearest_subpackage(module, packages)\n      if submodule == module:\n        resources[submodule].add(filename)\n      else:\n        assert module.startswith(submodule + '.')\n        relative_module = module[len(submodule) + 1:]\n        relative_filename = os.path.join(relative_module.replace('.', os.path.sep), filename)\n        resources[submodule].add(relative_filename)\n\n    return packages, namespace_packages, resources"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting the contents of the target.", "response": "def write_contents(self, root_target, reduced_dependencies, chroot):\n    \"\"\"Write contents of the target.\"\"\"\n    def write_target_source(target, src):\n      chroot.copy(os.path.join(get_buildroot(), target.target_base, src),\n                  os.path.join(self.SOURCE_ROOT, src))\n      # check parent __init__.pys to see if they also need to be copied.  this is to allow\n      # us to determine if they belong to regular packages or namespace packages.\n      while True:\n        src = os.path.dirname(src)\n        if not src:\n          # Do not allow the repository root to leak (i.e. '.' should not be a package in setup.py)\n          break\n        if os.path.exists(os.path.join(target.target_base, src, '__init__.py')):\n          chroot.copy(os.path.join(target.target_base, src, '__init__.py'),\n                      os.path.join(self.SOURCE_ROOT, src, '__init__.py'))\n\n    def write_target(target):\n      # We want to operate on the final sources target owns, so we potentially replace it with\n      # the target derived from it (by a codegen task).\n      subject = self.derived_by_original.get(target, target)\n      for rel_source in subject.sources_relative_to_buildroot():\n        abs_source_path = os.path.join(get_buildroot(), rel_source)\n        abs_source_root_path = os.path.join(get_buildroot(), subject.target_base)\n        source_root_relative_path = os.path.relpath(abs_source_path, abs_source_root_path)\n        write_target_source(subject, source_root_relative_path)\n\n    write_target(root_target)\n    for dependency in reduced_dependencies:\n      if self.is_python_target(dependency) and not dependency.provides:\n        write_target(dependency)\n      elif self.is_resources_target(dependency):\n        write_target(dependency)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites the setup. py of a target.", "response": "def write_setup(self, root_target, reduced_dependencies, chroot):\n    \"\"\"Write the setup.py of a target.\n\n    Must be run after writing the contents to the chroot.\n    \"\"\"\n    setup_keywords = root_target.provides.setup_py_keywords.copy()\n\n    package_dir = {'': self.SOURCE_ROOT}\n    packages, namespace_packages, resources = self.find_packages(root_target, chroot)\n\n    if namespace_packages:\n      setup_keywords['namespace_packages'] = list(sorted(namespace_packages))\n\n    if packages:\n      setup_keywords.update(\n          package_dir=package_dir,\n          packages=list(sorted(packages)),\n          package_data=dict((str(package), list(map(str, rs)))\n                            for (package, rs) in resources.items()))\n\n    setup_keywords['install_requires'] = list(self.install_requires(reduced_dependencies))\n\n    for binary_name, entry_point in self.iter_entry_points(root_target):\n      if 'entry_points' not in setup_keywords:\n        setup_keywords['entry_points'] = {}\n      if 'console_scripts' not in setup_keywords['entry_points']:\n        setup_keywords['entry_points']['console_scripts'] = []\n      setup_keywords['entry_points']['console_scripts'].append(\n          '{} = {}'.format(binary_name, entry_point))\n\n    setup_py = self._setup_boilerplate().format(setup_dict=distutils_repr(setup_keywords),\n                                                setup_target=root_target.address.reference())\n    chroot.write(ensure_binary(setup_py), 'setup.py')\n\n    # Make sure that `setup.py` is included.\n    chroot.write('include *.py', 'MANIFEST.in', mode='w')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_all(self):\n    ret = []\n    for cache_name, stat in self.stats_per_cache.items():\n      ret.append({\n        'cache_name': cache_name,\n        'num_hits': len(stat.hit_targets),\n        'num_misses': len(stat.miss_targets),\n        'hits': stat.hit_targets,\n        'misses': stat.miss_targets\n      })\n    return ret", "response": "Returns the cache stats as a list of dicts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the proper classpath based on products and scala version.", "response": "def _tool_classpath(self, tool, products, scheduler):\n    \"\"\"Return the proper classpath based on products and scala version.\"\"\"\n    classpath = self.tool_classpath_from_products(products,\n                                                  self.versioned_tool_name(tool, self.version),\n                                                  scope=self.options_scope)\n    classpath = tuple(fast_relpath(c, get_buildroot()) for c in classpath)\n\n    return self._memoized_scalac_classpath(classpath, scheduler)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef style_classpath(self, products, scheduler):\n    classpath_entries = self._tool_classpath('scalastyle', products, scheduler)\n    return [classpath_entry.path for classpath_entry in classpath_entries]", "response": "Returns classpath as paths for scalastyle."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef suffix_version(self, name):\n    if self.version == 'custom':\n      suffix = self.get_options().suffix_version\n      if suffix:\n        return '{0}_{1}'.format(name, suffix)\n      else:\n        raise RuntimeError('Suffix version must be specified if using a custom scala version. '\n                           'Suffix version is used for bootstrapping jars.  If a custom '\n                           'scala version is not specified, then the version specified in '\n                           '--scala-suffix-version is used.  For example for Scala '\n                           '2.10.7 you would use the suffix version \"2.10\".')\n\n    elif name.endswith(self.version):\n      raise ValueError('The name \"{0}\" should not be suffixed with the scala platform version '\n                      '({1}): it will be added automatically.'.format(name, self.version))\n    return '{0}_{1}'.format(name, self.version)", "response": "Appends the platform version to the given artifact name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_all_cmd_line_scopes():\n    all_scopes ={GLOBAL_SCOPE}\n    for goal in Goal.all():\n      for task in goal.task_types():\n        all_scopes.add(task.get_scope_info().scope)\n    return all_scopes", "response": "Return all scopes that may be explicitly specified on the cmd line."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning all cmd - line options that are available at the given scope.", "response": "def get_autocomplete_options_by_scope(self):\n    \"\"\"Return all cmd-line options.\n\n    These are of two types: scoped and unscoped.  Scoped options are explicitly scoped\n    (e.g., --goal-task-foo-bar) and may appear anywhere on the cmd line. Unscoped options\n    may only appear in the appropriate cmd line scope (e.g., ./pants goal.task --foo-bar).\n\n    Technically, any scoped option can appear anywhere, but in practice, having so many\n    autocomplete options is more confusing than useful. So, as a heuristic:\n     1. In global scope we only autocomplete globally-registered options.\n     2. In a goal scope we only autocomplete options registered by any task in that goal.\n     3. In a task scope we only autocomplete options registered by that task.\n\n    :return: A map of scope -> options to complete at that scope.\n    \"\"\"\n    autocomplete_options_by_scope = defaultdict(set)\n    def get_from_parser(parser):\n      oschi = HelpInfoExtracter.get_option_scope_help_info_from_parser(parser)\n      # We ignore advanced options, as they aren't intended to be used on the cmd line.\n      option_help_infos = oschi.basic + oschi.recursive\n      for ohi in option_help_infos:\n        autocomplete_options_by_scope[oschi.scope].update(ohi.unscoped_cmd_line_args)\n        autocomplete_options_by_scope[oschi.scope].update(ohi.scoped_cmd_line_args)\n        # Autocomplete to this option in the enclosing goal scope, but exclude options registered\n        # on us, but not by us, e.g., recursive options (which are registered by\n        # GlobalOptionsRegisterer).\n        # We exclude those because they are already registered on the goal scope anyway\n        # (via the recursion) and it would be confusing and superfluous to have autocompletion\n        # to both --goal-recursive-opt and --goal-task-recursive-opt in goal scope.\n        if issubclass(ohi.registering_class, TaskBase):\n          goal_scope = oschi.scope.partition('.')[0]\n          autocomplete_options_by_scope[goal_scope].update(ohi.scoped_cmd_line_args)\n    self.context.options.walk_parsers(get_from_parser)\n    return autocomplete_options_by_scope"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef register_tool(self, tool, name=None):\n    name = name or os.path.basename(tool)\n    if name in self._validated_tools:\n      return self._validated_tools[name]\n\n    def which(program):\n      def is_exe(fpath):\n        return os.path.isfile(fpath) and os.access(fpath, os.X_OK)\n\n      fpath, fname = os.path.split(program)\n      if fpath:\n        if is_exe(program):\n          return program\n      else:\n        for path in os.environ['PATH'].split(os.pathsep):\n          path = path.strip('\"')\n          exe_file = os.path.join(path, program)\n          if is_exe(exe_file):\n            return exe_file\n\n      return None\n\n    tool_path = which(tool)\n    if tool_path is None:\n      raise self.Error('Failed to locate {0}. Please install.'.format(tool))\n    self._validated_tools[name] = tool_path\n    return tool_path", "response": "Register a tool in the cpp toolchain."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef retry_on_exception(func, max_retries, exception_types, backoff_func=lambda n: 0):\n  for i in range(0, max_retries):\n    if i:\n      time.sleep(backoff_func(i))\n    try:\n      return func()\n    except exception_types as e:\n      logger.debug('encountered exception on retry #{}: {!r}'.format(i, e))\n      if i == max_retries - 1:\n        raise", "response": "Retry a function against a set of exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nschedules the tasks that produce product_type to be executed before the requesting task.", "response": "def require(self, product_type):\n    \"\"\"Schedules the tasks that produce product_type to be executed before the requesting task.\n\n    There must be at least one task that produces the required product type, or the\n    dependencies will not be satisfied.\n\n    :API: public\n    \"\"\"\n    self._dependencies.add(product_type)\n    self._context.products.require(product_type)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nscheduling tasks that produce product_type to be executed before the requesting task.", "response": "def optional_product(self, product_type):\n    \"\"\"Schedules tasks, if any, that produce product_type to be executed before the requesting task.\n\n    There need not be any tasks that produce the required product type.  All this method\n    guarantees is that if there are any then they will be executed before the requesting task.\n\n    :API: public\n    \"\"\"\n    self._optional_dependencies.add(product_type)\n    self.require(product_type)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef require_data(self, product_type):\n    self._dependencies.add(product_type)\n    self._context.products.require_data(product_type)", "response": "Schedules the tasks that produce product_type to be executed before the requesting task."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef optional_data(self, product_type):\n    self._optional_dependencies.add(product_type)\n    self.require_data(product_type)", "response": "Schedules tasks that produce product_type to be executed before the requesting task."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the set of data dependencies as producer infos corresponding to data requirements.", "response": "def get_dependencies(self):\n    \"\"\"Returns the set of data dependencies as producer infos corresponding to data requirements.\"\"\"\n    producer_infos = set()\n    for product_type in self._dependencies:\n      producer_infos.update(self._get_producer_infos_by_product_type(product_type))\n    return producer_infos"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create(cls, jar):\n    if isinstance(jar, cls):\n      return jar\n    return cls(org=jar.org, name=jar.name, rev=jar.rev, classifier=jar.classifier, ext=jar.ext)", "response": "Creates an actual M2Coordinate from the given M2Coordinate - like object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unversioned(cls, coord):\n    coord = cls.create(coord)\n    if coord.rev is None:\n      return coord\n    return M2Coordinate(org=coord.org, name=coord.name, classifier=coord.classifier, ext=coord.ext)", "response": "Returns the M2Coordinate without the version."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the canonical maven - style filename for an artifact pointed at by this coordinate.", "response": "def artifact_filename(self):\n    \"\"\"Returns the canonical maven-style filename for an artifact pointed at by this coordinate.\n\n    :API: public\n\n    :rtype: string\n    \"\"\"\n    def maybe_compenent(component):\n      return '-{}'.format(component) if component else ''\n\n    return '{org}-{name}{rev}{classifier}.{ext}'.format(org=self.org,\n                                                        name=self.name,\n                                                        rev=maybe_compenent(self.rev),\n                                                        classifier=maybe_compenent(self.classifier),\n                                                        ext=self.ext)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copy(self, **replacements):\n    cls = type(self)\n    kwargs = {'org': self.org, 'name': self.name, 'ext': self.ext, 'classifier': self.classifier, 'rev': self.rev}\n    for key, val in replacements.items():\n      kwargs[key] = val\n    return cls(**kwargs)", "response": "Returns a copy of this M2Coordinate with the given replacements kwargs overlaid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_log(self, workunit, level, *msg_elements):\n    if level <= self.level_for_workunit(workunit, self.settings.log_level):\n      self.do_handle_log(workunit, level, *msg_elements)", "response": "Handle a message logged by pants code."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nattempt to execute the goal s tasks in installed order.", "response": "def attempt(self, explain):\n    \"\"\"Attempts to execute the goal's tasks in installed order.\n\n    :param bool explain: If ``True`` then the goal plan will be explained instead of being\n                         executed.\n    \"\"\"\n    goal_workdir = os.path.join(self._context.options.for_global_scope().pants_workdir,\n                                self._goal.name)\n    with self._context.new_workunit(name=self._goal.name, labels=[WorkUnitLabel.GOAL]):\n      for name, task_type in reversed(list(self._tasktypes_by_name.items())):\n        task_workdir = os.path.join(goal_workdir, name)\n        task = task_type(self._context, task_workdir)\n        log_config = WorkUnit.LogConfig(level=task.get_options().level, colors=task.get_options().colors)\n        with self._context.new_workunit(name=name, labels=[WorkUnitLabel.TASK], log_config=log_config):\n          if explain:\n            self._context.log.debug('Skipping execution of {} in explain mode'.format(name))\n          elif task.skip_execution:\n            self._context.log.info('Skipping {}'.format(name))\n          else:\n            task.execute()\n\n      if explain:\n        reversed_tasktypes_by_name = reversed(list(self._tasktypes_by_name.items()))\n        goal_to_task = ', '.join(\n            '{}->{}'.format(name, task_type.__name__) for name, task_type in reversed_tasktypes_by_name)\n        print('{goal} [{goal_to_task}]'.format(goal=self._goal.name, goal_to_task=goal_to_task))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef attempt(self, context, goals):\n    sorted_goal_infos = self.sort_goals(context, goals)\n    self._propose_alternative_target_roots(context, sorted_goal_infos)\n    goal_executors = list(self._prepare(context, sorted_goal_infos))\n    execution_goals = ' -> '.join(e.goal.name for e in goal_executors)\n    context.log.info('Executing tasks in goals: {goals}'.format(goals=execution_goals))\n\n    explain = context.options.for_global_scope().explain\n    if explain:\n      print('Goal Execution Order:\\n\\n{}\\n'.format(execution_goals))\n      print('Goal [TaskRegistrar->Task] Order:\\n')\n\n    serialized_goals_executors = [ge for ge in goal_executors if ge.goal.serialize]\n    outer_lock_holder = serialized_goals_executors[-1] if serialized_goals_executors else None\n\n    if outer_lock_holder:\n      context.acquire_lock()\n    try:\n      for goal_executor in goal_executors:\n        goal_executor.attempt(explain)\n        if goal_executor is outer_lock_holder:\n          context.release_lock()\n          outer_lock_holder = None\n    finally:\n      if outer_lock_holder:\n        context.release_lock()", "response": "Attempt the given goals."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a set of ranges where we accept trailing slashes specifically within comments and strings.", "response": "def build_exception_map(cls, tokens):\n    \"\"\"Generates a set of ranges where we accept trailing slashes, specifically within comments\n       and strings.\n    \"\"\"\n    exception_ranges = defaultdict(list)\n    for token in tokens:\n      token_type, _, token_start, token_end = token[0:4]\n      if token_type in (tokenize.COMMENT, tokenize.STRING):\n        if token_start[0] == token_end[0]:\n          exception_ranges[token_start[0]].append((token_start[1], token_end[1]))\n        else:\n          exception_ranges[token_start[0]].append((token_start[1], sys.maxsize))\n          for line in range(token_start[0] + 1, token_end[0]):\n            exception_ranges[line].append((0, sys.maxsize))\n          exception_ranges[token_end[0]].append((0, token_end[1]))\n    return exception_ranges"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the GOPATH for the given target.", "response": "def get_gopath(self, target):\n    \"\"\"Returns the $GOPATH for the given target.\"\"\"\n    return os.path.join(self.workdir, target.id)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ensure_workspace(self, target):\n    gopath = self.get_gopath(target)\n    for d in ('bin', 'pkg', 'src'):\n      safe_mkdir(os.path.join(gopath, d))\n    required_links = set()\n    for dep in target.closure():\n      if not isinstance(dep, GoTarget):\n        continue\n      if self.is_remote_lib(dep):\n        self._symlink_remote_lib(gopath, dep, required_links)\n      else:\n        self._symlink_local_src(gopath, dep, required_links)\n    self.remove_unused_links(os.path.join(gopath, 'src'), required_links)", "response": "Ensures that an up - to - date Go workspace exists for the given target."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating symlinks from the given gopath to the source files of the given local package.", "response": "def _symlink_local_src(self, gopath, go_local_src, required_links):\n    \"\"\"Creates symlinks from the given gopath to the source files of the given local package.\n\n    Also duplicates directory structure leading to source files of package within\n    gopath, in order to provide isolation to the package.\n\n    Adds the symlinks to the source files to required_links.\n    \"\"\"\n    source_list = [os.path.join(get_buildroot(), src)\n                   for src in go_local_src.sources_relative_to_buildroot()]\n    rel_list = go_local_src.sources_relative_to_target_base()\n    source_iter = zip(source_list, rel_list)\n    return self._symlink_lib(gopath, go_local_src, source_iter, required_links)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating symlinks from the given gopath to the source files of the given remote lib.", "response": "def _symlink_remote_lib(self, gopath, go_remote_lib, required_links):\n    \"\"\"Creates symlinks from the given gopath to the source files of the given remote lib.\n\n    Also duplicates directory structure leading to source files of package within\n    gopath, in order to provide isolation to the package.\n\n    Adds the symlinks to the source files to required_links.\n    \"\"\"\n    def source_iter():\n      remote_lib_source_dir = self.context.products.get_data('go_remote_lib_src')[go_remote_lib]\n      for path in os.listdir(remote_lib_source_dir):\n        remote_src = os.path.join(remote_lib_source_dir, path)\n        # We grab any file since a go package might have .go, .c, .cc, etc files - all needed for\n        # installation.\n        if os.path.isfile(remote_src):\n          yield (remote_src, os.path.basename(path))\n    return self._symlink_lib(gopath, go_remote_lib, source_iter(), required_links)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iterate(self, scopes):\n    scope_infos = [self._scope_to_info[s] for s in self._expand_tasks(scopes)]\n    if scope_infos:\n      for scope_info in self._expand_subsystems(scope_infos):\n        yield scope_info", "response": "Yields ScopeInfo instances for the specified scopes plus relevant related scopes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds all tasks in any requested goals. Returns the expanded scopes plus the added tasks sorted by scope name.", "response": "def _expand_tasks(self, scopes):\n    \"\"\"Add all tasks in any requested goals.\n\n    Returns the requested scopes, plus the added tasks, sorted by scope name.\n    \"\"\"\n    expanded_scopes = set(scopes)\n    for scope, info in self._scope_to_info.items():\n      if info.category == ScopeInfo.TASK:\n        outer = enclosing_scope(scope)\n        while outer != GLOBAL_SCOPE:\n          if outer in expanded_scopes:\n            expanded_scopes.add(scope)\n            break\n          outer = enclosing_scope(outer)\n    return sorted(expanded_scopes)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _expand_subsystems(self, scope_infos):\n\n    # Get non-global subsystem dependencies of the specified subsystem client.\n    def subsys_deps(subsystem_client_cls):\n      for dep in subsystem_client_cls.subsystem_dependencies_iter():\n        if dep.scope != GLOBAL_SCOPE:\n          yield self._scope_to_info[dep.options_scope]\n          for x in subsys_deps(dep.subsystem_cls):\n            yield x\n\n    for scope_info in scope_infos:\n      yield scope_info\n      if scope_info.optionable_cls is not None:\n        # We don't currently subclass GlobalOptionsRegistrar, and I can't think of any reason why\n        # we would, but might as well be robust.\n        if issubclass(scope_info.optionable_cls, GlobalOptionsRegistrar):\n          # We were asked for global help, so also yield for all global subsystems.\n          for scope, info in self._scope_to_info.items():\n            if info.category == ScopeInfo.SUBSYSTEM and enclosing_scope(scope) == GLOBAL_SCOPE:\n              yield info\n              for subsys_dep in subsys_deps(info.optionable_cls):\n                yield subsys_dep\n        elif issubclass(scope_info.optionable_cls, SubsystemClientMixin):\n          for subsys_dep in subsys_deps(scope_info.optionable_cls):\n            yield subsys_dep", "response": "Yields all subsystems tied to a scope."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_doc(self, language_predicate, create_jvmdoc_command):\n    catalog = self.context.products.isrequired(self.jvmdoc().product_type)\n    if catalog and self.combined:\n      raise TaskError(\n          'Cannot provide {} target mappings for combined output'.format(self.jvmdoc().product_type))\n\n    def docable(target):\n      if not language_predicate(target):\n        self.context.log.debug('Skipping [{}] because it is does not pass the language predicate'.format(target.address.spec))\n        return False\n      if not self._include_codegen and target.is_synthetic:\n        self.context.log.debug('Skipping [{}] because it is a synthetic target'.format(target.address.spec))\n        return False\n      for pattern in self._exclude_patterns:\n        if pattern.search(target.address.spec):\n          self.context.log.debug(\n            \"Skipping [{}] because it matches exclude pattern '{}'\".format(target.address.spec, pattern.pattern))\n          return False\n      return True\n\n    targets = self.get_targets(predicate=docable)\n    if not targets:\n      return\n\n    with self.invalidated(targets, invalidate_dependents=self.combined) as invalidation_check:\n      def find_invalid_targets():\n        invalid_targets = set()\n        for vt in invalidation_check.invalid_vts:\n          invalid_targets.update(vt.targets)\n        return invalid_targets\n\n      invalid_targets = list(find_invalid_targets())\n      if invalid_targets:\n        if self.combined:\n          self._generate_combined(targets, create_jvmdoc_command)\n        else:\n          self._generate_individual(invalid_targets, create_jvmdoc_command)\n\n    if self.open and self.combined:\n      try:\n        desktop.ui_open(os.path.join(self.workdir, 'combined', 'index.html'))\n      except desktop.OpenError as e:\n        raise TaskError(e)\n\n    if catalog:\n      for target in targets:\n        gendir = self._gendir(target)\n        jvmdocs = []\n        for root, dirs, files in safe_walk(gendir):\n          jvmdocs.extend(os.path.relpath(os.path.join(root, f), gendir) for f in files)\n        self.context.products.get(self.jvmdoc().product_type).add(target, gendir, jvmdocs)", "response": "Generate a documentation for the targets in the current context."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef classpath(self, targets, classpath_prefix=None, classpath_product=None, exclude_scopes=None,\n                include_scopes=None):\n    \"\"\"Builds a transitive classpath for the given targets.\n\n    Optionally includes a classpath prefix or building from a non-default classpath product.\n\n    :param targets: the targets for which to build the transitive classpath.\n    :param classpath_prefix: optional additional entries to prepend to the classpath.\n    :param classpath_product: an optional ClasspathProduct from which to build the classpath. if not\n    specified, the runtime_classpath will be used.\n    :param :class:`pants.build_graph.target_scopes.Scope` exclude_scopes: Exclude targets which\n      have at least one of these scopes on the classpath.\n    :param :class:`pants.build_graph.target_scopes.Scope` include_scopes: Only include targets which\n      have at least one of these scopes on the classpath. Defaults to Scopes.JVM_RUNTIME_SCOPES.\n    :return: a list of classpath strings.\n    \"\"\"\n    include_scopes = Scopes.JVM_RUNTIME_SCOPES if include_scopes is None else include_scopes\n    classpath_product = classpath_product or self.context.products.get_data('runtime_classpath')\n    closure = BuildGraph.closure(targets, bfs=True, include_scopes=include_scopes,\n                                 exclude_scopes=exclude_scopes, respect_intransitive=True)\n\n    classpath_for_targets = ClasspathUtil.classpath(closure, classpath_product, self.confs)\n    classpath = list(classpath_prefix or ())\n    classpath.extend(classpath_for_targets)\n    return classpath", "response": "Builds a transitive classpath for the given targets."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef atomic_copy(src, dst):\n  with temporary_file(root_dir=os.path.dirname(dst)) as tmp_dst:\n    shutil.copyfile(src, tmp_dst.name)\n    os.chmod(tmp_dst.name, os.stat(src).st_mode)\n    os.rename(tmp_dst.name, dst)", "response": "Copy the file src to dst overwriting dst atomically."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a dictionary of name to a function that returns an estimated size for a given target.", "response": "def create_size_estimators():\n  \"\"\"Create a dict of name to a function that returns an estimated size for a given target.\n\n  The estimated size is used to build the largest targets first (subject to dependency constraints).\n  Choose 'random' to choose random sizes for each target, which may be useful for distributed\n  builds.\n  :returns: Dict of a name to a function that returns an estimated size.\n  \"\"\"\n  def line_count(filename):\n    with open(filename, 'rb') as fh:\n      return sum(1 for line in fh)\n  return {\n    'linecount': lambda srcs: sum(line_count(src) for src in srcs),\n    'filecount': lambda srcs: len(srcs),\n    'filesize': lambda srcs: sum(os.path.getsize(src) for src in srcs),\n    'nosize': lambda srcs: 0,\n    'random': lambda srcs: random.randint(0, 10000),\n  }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fallible_to_exec_result_or_raise(fallible_result, request):\n\n  if fallible_result.exit_code == 0:\n    return ExecuteProcessResult(\n      fallible_result.stdout,\n      fallible_result.stderr,\n      fallible_result.output_directory_digest\n    )\n  else:\n    raise ProcessExecutionFailure(\n      fallible_result.exit_code,\n      fallible_result.stdout,\n      fallible_result.stderr,\n      request.description\n    )", "response": "Converts a FallibleExecuteProcessResult to a ExecuteProcessResult or raises an error."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves the coding header from the file text.", "response": "def _remove_coding_header(cls, blob):\n    \"\"\"\n    There is a bug in ast.parse that cause it to throw a syntax error if\n    you have a header similar to...\n    # coding=utf-8,\n\n    we replace this line with something else to bypass the bug.\n    :param blob: file text contents\n    :return: adjusted blob\n    \"\"\"\n    # Remove the # coding=utf-8 to avoid AST erroneous parse errors\n    #   https://bugs.python.org/issue22221\n    lines = blob.decode('utf-8').split('\\n')\n    if lines and 'coding=utf-8' in lines[0]:\n      lines[0] = '#remove coding'\n    return '\\n'.join(lines).encode('ascii', errors='replace')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(cls, filename, root=None):\n    if root is not None:\n      if os.path.isabs(filename):\n        raise ValueError(\"filename must be a relative path if root is specified\")\n      full_filename = os.path.join(root, filename)\n    else:\n      full_filename = filename\n\n    with io.open(full_filename, 'rb') as fp:\n      blob = fp.read()\n\n    tree = cls._parse(blob, filename)\n    return cls(blob=blob, tree=tree, root=root, filename=filename)", "response": "Parses the file at filename and returns a PythonFile object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_statement(cls, statement, filename='<expr>'):\n    lines = textwrap.dedent(statement).split('\\n')\n    if lines and not lines[0]:  # Remove the initial empty line, which is an artifact of dedent.\n      lines = lines[1:]\n\n    blob = '\\n'.join(lines).encode('utf-8')\n    tree = cls._parse(blob, filename)\n    return cls(blob=blob, tree=tree, root=None, filename=filename)", "response": "A helper to construct a PythonFile from a triple - quoted string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef iter_tokens(cls, blob):\n    readline_func = io.StringIO(blob.decode('utf-8')).readline\n    return tokenize.generate_tokens(readline_func)", "response": "Iterate over tokens found in blob contents\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef translate_logical_line(start, end, contents, indent_stack, endmarker=False):\n    # Remove leading blank lines.\n    while contents[0] == '\\n':\n      start += 1\n      contents.pop(0)\n    # Remove trailing blank lines.\n    while contents[-1] == '\\n':\n      end -= 1\n      contents.pop()\n\n    indent = len(indent_stack[-1]) if indent_stack else 0\n    if endmarker:\n      indent = len(contents[0])\n    return start, end + 1, indent", "response": "Translate raw contents to logical lines."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an iterator of start_line stop_line indent for logical lines", "response": "def iter_logical_lines(self, blob):\n    \"\"\"Returns an iterator of (start_line, stop_line, indent) for logical lines \"\"\"\n    indent_stack = []\n    contents = []\n    line_number_start = None\n    for token in self.iter_tokens(blob):\n      token_type, token_text, token_start = token[0:3]\n      if token_type == tokenize.INDENT:\n        indent_stack.append(token_text)\n      if token_type == tokenize.DEDENT:\n        indent_stack.pop()\n      if token_type in self.SKIP_TOKENS:\n        continue\n      contents.append(token_text)\n      if line_number_start is None:\n        line_number_start = token_start[0]\n      elif token_type in (tokenize.NEWLINE, tokenize.ENDMARKER):\n        yield self.translate_logical_line(\n            line_number_start,\n            token_start[0] + (1 if token_type is tokenize.NEWLINE else -1),\n            [_f for _f in contents if _f],\n            indent_stack,\n            endmarker=token_type == tokenize.ENDMARKER)\n        contents = []\n        line_number_start = None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a slice for the given line number", "response": "def line_range(self, line_number):\n    \"\"\"Return a slice for the given line number\"\"\"\n    if line_number <= 0 or line_number > len(self.lines):\n      raise IndexError('NOTE: Python file line numbers are offset by 1.')\n\n    if line_number not in self.logical_lines:\n      return slice(line_number, line_number + 1)\n    else:\n      start, stop, _ = self.logical_lines[line_number]\n      return slice(start, stop)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the dependencies of target filtered by predicate accounting for strict_deps.", "response": "def strict_deps_for_target(self, target, predicate=None):\n    \"\"\"Get the dependencies of `target` filtered by `predicate`, accounting for 'strict_deps'.\n\n    If 'strict_deps' is on, instead of using the transitive closure of dependencies, targets will\n    only be able to see their immediate dependencies declared in the BUILD file. The 'strict_deps'\n    setting is obtained from the result of `get_compile_settings()`.\n\n    NB: This includes the current target in the result.\n    \"\"\"\n    if self._native_build_settings.get_strict_deps_value_for_target(target):\n      strict_deps = target.strict_dependencies(DependencyContext())\n      if predicate:\n        filtered_deps = list(filter(predicate, strict_deps))\n      else:\n        filtered_deps = strict_deps\n      deps = [target] + filtered_deps\n    else:\n      deps = self.context.build_graph.transitive_subgraph_of_addresses(\n        [target.address], predicate=predicate)\n\n    # Filter out the beginning target depending on whether it matches the predicate.\n    # TODO: There should be a cleaner way to do this.\n    deps = filter(predicate, deps)\n\n    return deps"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _rearrange_output_for_package(self, target_workdir, java_package):\n    package_dir_rel = java_package.replace('.', os.path.sep)\n    package_dir = os.path.join(target_workdir, package_dir_rel)\n    safe_mkdir(package_dir)\n    for root, dirs, files in safe_walk(target_workdir):\n      if root == package_dir_rel:\n        # This path is already in the correct location\n        continue\n      for f in files:\n        os.rename(\n          os.path.join(root, f),\n          os.path.join(package_dir, f)\n        )\n\n    # Remove any empty directories that were left behind\n    for root, dirs, files in safe_walk(target_workdir, topdown = False):\n      for d in dirs:\n        full_dir = os.path.join(root, d)\n        if not os.listdir(full_dir):\n          os.rmdir(full_dir)", "response": "Rearrange the output files to match a standard Java structure."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _scrub_generated_timestamps(self, target_workdir):\n    for root, _, filenames in safe_walk(target_workdir):\n      for filename in filenames:\n        source = os.path.join(root, filename)\n\n        with open(source, 'r') as f:\n          lines = f.readlines()\n        if len(lines) < 1:\n          return\n        with open(source, 'w') as f:\n          if not self._COMMENT_WITH_TIMESTAMP_RE.match(lines[0]):\n            f.write(lines[0])\n          for line in lines[1:]:\n            f.write(line)", "response": "Remove the first line of comment from each file if it contains a timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef for_compiler(self, compiler, platform):\n    return (self.linker\n            # TODO(#6143): describe why the compiler needs to be first on the PATH!\n            .sequence(compiler, exclude_list_fields=['extra_args', 'path_entries'])\n            .prepend_field('path_entries', compiler.path_entries)\n            .copy(exe_filename=compiler.exe_filename))", "response": "Return a Linker object which is intended to be compatible with the given compiler."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_module_on_std_lib_path(cls, module):\n    module_file_real_path = os.path.realpath(module.__file__)\n    if module_file_real_path.startswith(cls.STANDARD_LIB_PATH):\n      return True\n    elif os.path.splitext(module_file_real_path)[1] == '.pyc':\n      py_file_real_path = os.path.realpath(os.path.splitext(module_file_real_path)[0] + '.py')\n      return py_file_real_path.startswith(cls.STANDARD_LIB_PATH)\n    return False", "response": "Checks if a module is on the standard lib path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef classify_imports(self, chunk):\n    errors = []\n    all_module_types = set()\n    for node in chunk:\n      errors.extend(self.import_errors(node))\n      module_types = self.classify_import_node(node)\n      if len(module_types) > 1:\n        errors.append(self.error(\n          'T403',\n          'Import statement imports from multiple module types: {types}.'.format(\n            types=ImportType.order_names(module_types)),\n          node))\n      if ImportType.UNKNOWN in module_types:\n        errors.append(self.warning('T404', 'Unclassifiable import.', node))\n      all_module_types.update(module_types)\n    if len(chunk) > 0 and len(all_module_types) > 1:\n      errors.append(\n          self.error(\n            'T405',\n            'Import block starting here contains imports '\n            'from multiple module types: {types}.'.format(\n              types=ImportType.order_names(all_module_types)),\n            chunk[0].lineno))\n    return all_module_types, errors", "response": "Classifies the imports in a chunk."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\niterate over space - separated import chunks in a file.", "response": "def iter_import_chunks(self):\n    \"\"\"Iterate over space-separated import chunks in a file.\"\"\"\n    chunk = []\n    last_line = None\n    for leaf in self.python_file.tree.body:\n      if isinstance(leaf, (ast.Import, ast.ImportFrom)):\n        # we've seen previous imports but this import is not in the same chunk\n        if last_line and leaf.lineno != last_line[1]:\n          yield chunk\n          chunk = [leaf]\n        # we've either not seen previous imports or this is part of the same chunk\n        elif not last_line or last_line and leaf.lineno == last_line[1]:\n          chunk.append(leaf)\n        last_line = self.python_file.logical_lines[leaf.lineno]\n    if chunk:\n      yield chunk"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of all files that have been touched in the repository.", "response": "def touched_files(self, parent):\n    \"\"\"\n    :API: public\n    \"\"\"\n    try:\n      return self._scm.changed_files(from_commit=parent,\n                                     include_untracked=True,\n                                     relative_to=get_buildroot())\n    except Scm.ScmException as e:\n      raise self.WorkspaceError(\"Problem detecting changed files.\", e)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef changes_in(self, rev_or_range):\n    try:\n      return self._scm.changes_in(rev_or_range, relative_to=get_buildroot())\n    except Scm.ScmException as e:\n      raise self.WorkspaceError(\"Problem detecting changes in {}.\".format(rev_or_range), e)", "response": "Returns a list of changes in the given revision range."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compiler(self, target):\n    self._check_target(target)\n    return target.compiler or self._default_compiler", "response": "Returns the thrift compiler to use for the given target."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the target language to generate the Thrift stubs for.", "response": "def language(self, target):\n    \"\"\"Returns the target language to generate thrift stubs for.\n\n    :param target: The target to extract the target language from.\n    :type target: :class:`pants.backend.codegen.thrift.java.java_thrift_library.JavaThriftLibrary`\n    :returns: The target language to generate stubs for.\n    :rtype: string\n    \"\"\"\n    self._check_target(target)\n    return target.language or self._default_language"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef namespace_map(self, target):\n    self._check_target(target)\n    return target.namespace_map or self._default_namespace_map", "response": "Returns the namespace_map used for Thrift generation."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the compiler_args used for Thrift generation.", "response": "def compiler_args(self, target):\n    \"\"\"Returns the compiler_args used for Thrift generation.\n\n    :param target: The target to extract the compiler args from.\n    :type target: :class:`pants.backend.codegen.targets.java_thrift_library.JavaThriftLibrary`\n    :returns: Extra arguments for the thrift compiler\n    :rtype: list\n    \"\"\"\n    self._check_target(target)\n    return target.compiler_args or self._default_compiler_args"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef default_java_namespace(self, target):\n    self._check_target(target)\n    return target.default_java_namespace or self._default_default_java_namespace", "response": "Returns the default Java namespace used for Thrift generation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _file_filter(cls, filename, include_patterns, exclude_patterns):\n    logger.debug('filename: {}'.format(filename))\n    for exclude_pattern in exclude_patterns:\n      if exclude_pattern.match(filename):\n        return False\n    if include_patterns:\n      found = False\n      for include_pattern in include_patterns:\n        if include_pattern.match(filename):\n          found = True\n          break\n      if not found:\n        return False\n    return True", "response": "Returns True if the file should be allowed through the filter."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntake regex patterns and return a filter function.", "response": "def _calculate_unpack_filter(cls, includes=None, excludes=None, spec=None):\n    \"\"\"Take regex patterns and return a filter function.\n\n    :param list includes: List of include patterns to pass to _file_filter.\n    :param list excludes: List of exclude patterns to pass to _file_filter.\n    \"\"\"\n    include_patterns = cls.compile_patterns(includes or [],\n                                            field_name='include_patterns',\n                                            spec=spec)\n    logger.debug('include_patterns: {}'\n                 .format(list(p.pattern for p in include_patterns)))\n    exclude_patterns = cls.compile_patterns(excludes or [],\n                                            field_name='exclude_patterns',\n                                            spec=spec)\n    logger.debug('exclude_patterns: {}'\n                 .format(list(p.pattern for p in exclude_patterns)))\n    return lambda f: cls._file_filter(f, include_patterns, exclude_patterns)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating a filter function from the include and exclude patterns of a Target.", "response": "def get_unpack_filter(cls, unpackable_target):\n    \"\"\"Calculate a filter function from the include/exclude patterns of a Target.\n\n    :param ImportRemoteSourcesMixin unpackable_target: A target with include_patterns and\n                                                       exclude_patterns attributes.\n    \"\"\"\n    # TODO: we may be able to make use of glob matching in the engine to avoid doing this filtering.\n    return cls._calculate_unpack_filter(includes=unpackable_target.payload.include_patterns,\n                                        excludes=unpackable_target.payload.exclude_patterns,\n                                        spec=unpackable_target.address.spec)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a map of gen - key = > target specs resolves the target specs into references.", "response": "def _resolve_deps(self, depmap):\n    \"\"\"Given a map of gen-key=>target specs, resolves the target specs into references.\"\"\"\n    deps = defaultdict(lambda: OrderedSet())\n    for category, depspecs in depmap.items():\n      dependencies = deps[category]\n      for depspec in depspecs:\n        dep_address = Address.parse(depspec)\n        try:\n          self.context.build_graph.maybe_inject_address_closure(dep_address)\n          dependencies.add(self.context.build_graph.get_target(dep_address))\n        except AddressLookupError as e:\n          raise AddressLookupError('{}\\n  referenced from {} scope'.format(e, self.options_scope))\n    return deps"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_specs(cls, target_specs, build_root=None, exclude_patterns=None, tags=None):\n    build_root = build_root or get_buildroot()\n    spec_parser = CmdLineSpecParser(build_root)\n\n    dependencies = tuple(OrderedSet(spec_parser.parse_spec(spec_str) for spec_str in target_specs))\n    return Specs(\n      dependencies=dependencies,\n      exclude_patterns=exclude_patterns if exclude_patterns else tuple(),\n      tags=tags)", "response": "Parse a list of string specs into unique Specs objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining the files changed according to SCM and options.", "response": "def changed_files(cls, scm, changes_since=None, diffspec=None):\n    \"\"\"Determines the files changed according to SCM/workspace and options.\"\"\"\n    workspace = ScmWorkspace(scm)\n    if diffspec:\n      return workspace.changes_in(diffspec)\n\n    changes_since = changes_since or scm.current_rev_identifier()\n    return workspace.touched_files(changes_since)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new node tree from the given options.", "response": "def create(cls, options, session, build_root=None, exclude_patterns=None, tags=None):\n    \"\"\"\n    :param Options options: An `Options` instance to use.\n    :param session: The Scheduler session\n    :param string build_root: The build root.\n    \"\"\"\n    # Determine the literal target roots.\n    spec_roots = cls.parse_specs(\n      target_specs=options.target_specs,\n      build_root=build_root,\n      exclude_patterns=exclude_patterns,\n      tags=tags)\n\n    # Determine `Changed` arguments directly from options to support pre-`Subsystem`\n    # initialization paths.\n    changed_options = options.for_scope('changed')\n    changed_request = ChangedRequest.from_options(changed_options)\n\n    # Determine the `--owner-of=` arguments provided from the global options\n    owned_files = options.for_global_scope().owner_of\n\n    logger.debug('spec_roots are: %s', spec_roots)\n    logger.debug('changed_request is: %s', changed_request)\n    logger.debug('owned_files are: %s', owned_files)\n    targets_specified = sum(1 for item\n                         in (changed_request.is_actionable(), owned_files, spec_roots.dependencies)\n                         if item)\n\n    if targets_specified > 1:\n      # We've been provided more than one of: a change request, an owner request, or spec roots.\n      raise InvalidSpecConstraint(\n        'Multiple target selection methods provided. Please use only one of '\n        '--changed-*, --owner-of, or target specs'\n      )\n\n    if changed_request.is_actionable():\n      scm = get_scm()\n      if not scm:\n        raise InvalidSpecConstraint(\n          'The --changed-* options are not available without a recognized SCM (usually git).'\n        )\n      changed_files = cls.changed_files(\n          scm,\n          changes_since=changed_request.changes_since,\n          diffspec=changed_request.diffspec)\n      # We've been provided no spec roots (e.g. `./pants list`) AND a changed request. Compute\n      # alternate target roots.\n      request = OwnersRequest(sources=tuple(changed_files),\n                              include_dependees=str(changed_request.include_dependees))\n      changed_addresses, = session.product_request(BuildFileAddresses, [request])\n      logger.debug('changed addresses: %s', changed_addresses)\n      dependencies = tuple(SingleAddress(a.spec_path, a.target_name) for a in changed_addresses)\n      return TargetRoots(Specs(dependencies=dependencies, exclude_patterns=exclude_patterns, tags=tags))\n\n    if owned_files:\n      # We've been provided no spec roots (e.g. `./pants list`) AND a owner request. Compute\n      # alternate target roots.\n      request = OwnersRequest(sources=tuple(owned_files), include_dependees=str('none'))\n      owner_addresses, = session.product_request(BuildFileAddresses, [request])\n      logger.debug('owner addresses: %s', owner_addresses)\n      dependencies = tuple(SingleAddress(a.spec_path, a.target_name) for a in owner_addresses)\n      return TargetRoots(Specs(dependencies=dependencies, exclude_patterns=exclude_patterns, tags=tags))\n\n    return TargetRoots(spec_roots)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resolve_requirements(self, interpreter, req_libs):\n    with self.invalidated(req_libs) as invalidation_check:\n      # If there are no relevant targets, we still go through the motions of resolving\n      # an empty set of requirements, to prevent downstream tasks from having to check\n      # for this special case.\n      if invalidation_check.all_vts:\n        target_set_id = VersionedTargetSet.from_versioned_targets(\n            invalidation_check.all_vts).cache_key.hash\n      else:\n        target_set_id = 'no_targets'\n\n      # We need to ensure that we are resolving for only the current platform if we are\n      # including local python dist targets that have native extensions.\n      targets_by_platform = pex_build_util.targets_by_platform(self.context.targets(), self._python_setup)\n      if self._python_native_code_settings.check_build_for_current_platform_only(targets_by_platform):\n        platforms = ['current']\n      else:\n        platforms = list(sorted(targets_by_platform.keys()))\n\n      path = os.path.realpath(os.path.join(self.workdir, str(interpreter.identity), target_set_id))\n      # Note that we check for the existence of the directory, instead of for invalid_vts,\n      # to cover the empty case.\n      if not os.path.isdir(path):\n        with safe_concurrent_creation(path) as safe_path:\n          pex_builder = PexBuilderWrapper.Factory.create(\n            builder=PEXBuilder(path=safe_path, interpreter=interpreter, copy=True),\n            log=self.context.log)\n          pex_builder.add_requirement_libs_from(req_libs, platforms=platforms)\n          pex_builder.freeze()\n    return PEX(path, interpreter=interpreter)", "response": "Resolve requirements for a given Python interpreter."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nresolves a list of pip - style requirement strings.", "response": "def resolve_requirement_strings(self, interpreter, requirement_strings):\n    \"\"\"Resolve a list of pip-style requirement strings.\"\"\"\n    requirement_strings = sorted(requirement_strings)\n    if len(requirement_strings) == 0:\n      req_strings_id = 'no_requirements'\n    elif len(requirement_strings) == 1:\n      req_strings_id = requirement_strings[0]\n    else:\n      req_strings_id = hash_all(requirement_strings)\n\n    path = os.path.realpath(os.path.join(self.workdir, str(interpreter.identity), req_strings_id))\n    if not os.path.isdir(path):\n      reqs = [PythonRequirement(req_str) for req_str in requirement_strings]\n      with safe_concurrent_creation(path) as safe_path:\n        pex_builder = PexBuilderWrapper.Factory.create(\n          builder=PEXBuilder(path=safe_path, interpreter=interpreter, copy=True),\n          log=self.context.log)\n        pex_builder.add_resolved_requirements(reqs)\n        pex_builder.freeze()\n    return PEX(path, interpreter=interpreter)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nyields a pex. pex_builder. PEXBuilder at path with the given pexes already merged.", "response": "def merged_pex(cls, path, pex_info, interpreter, pexes, interpeter_constraints=None):\n    \"\"\"Yields a pex builder at path with the given pexes already merged.\n\n    :rtype: :class:`pex.pex_builder.PEXBuilder`\n    \"\"\"\n    pex_paths = [pex.path() for pex in pexes if pex]\n    if pex_paths:\n      pex_info = pex_info.copy()\n      pex_info.merge_pex_path(':'.join(pex_paths))\n\n    with safe_concurrent_creation(path) as safe_path:\n      builder = PEXBuilder(safe_path, interpreter, pex_info=pex_info)\n      if interpeter_constraints:\n        for constraint in interpeter_constraints:\n          builder.add_interpreter_constraint(constraint)\n      yield builder"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a merged pex at path.", "response": "def merge_pexes(cls, path, pex_info, interpreter, pexes, interpeter_constraints=None):\n    \"\"\"Generates a merged pex at path.\"\"\"\n    with cls.merged_pex(path, pex_info, interpreter, pexes, interpeter_constraints) as builder:\n      builder.freeze()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract the variants from the given string.", "response": "def _extract_variants(address, variants_str):\n  \"\"\"Return the variants (if any) represented by the given variants_str.\n\n  :returns: The variants or else `None` if there are none.\n  :rtype: tuple of tuples (key, value) strings\n  \"\"\"\n  def entries():\n    for entry in variants_str.split(','):\n      key, _, value = entry.partition('=')\n      if not key or not value:\n        raise ValueError('Invalid variants after the @ in: {}'.format(address))\n      yield (key, value)\n  return tuple(entries())"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the path to the artifact cache directory.", "response": "def artifact_cache_dir(self):\n    \"\"\"Note that this is unrelated to the general pants artifact cache.\"\"\"\n    return (self.get_options().artifact_cache_dir or\n            os.path.join(self.scratch_dir, 'artifacts'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns either the compatibility of the given target or the interpreter constraints.", "response": "def compatibility_or_constraints(self, target):\n    \"\"\"\n    Return either the compatibility of the given target, or the interpreter constraints.\n    If interpreter constraints are supplied by the CLI flag, return those only.\n    \"\"\"\n    if self.get_options().is_flagged('interpreter_constraints'):\n      return tuple(self.interpreter_constraints)\n    return tuple(target.compatibility or self.interpreter_constraints)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of paths to Python interpreters as defined in a pexrc file.", "response": "def get_pex_python_paths():\n    \"\"\"Returns a list of paths to Python interpreters as defined in a pexrc file.\n\n    These are provided by a PEX_PYTHON_PATH in either of '/etc/pexrc', '~/.pexrc'.\n    PEX_PYTHON_PATH defines a colon-separated list of paths to interpreters\n    that a pex can be built and run against.\n    \"\"\"\n    ppp = Variables.from_rc().get('PEX_PYTHON_PATH')\n    if ppp:\n      return ppp.split(os.pathsep)\n    else:\n      return []"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_pyenv_paths(pyenv_root_func=None):\n    pyenv_root_func = pyenv_root_func or get_pyenv_root\n    pyenv_root = pyenv_root_func()\n    if pyenv_root is None:\n      return []\n    versions_dir = os.path.join(pyenv_root, 'versions')\n    paths = []\n    for version in sorted(os.listdir(versions_dir)):\n      path = os.path.join(versions_dir, version, 'bin')\n      if os.path.isdir(path):\n        paths.append(path)\n    return paths", "response": "Returns a list of paths to Python interpreters managed by pyenv."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nauthenticating against the specified provider.", "response": "def authenticate(self, provider, creds=None, cookies=None):\n    \"\"\"Authenticate against the specified provider.\n\n    :param str provider: Authorize against this provider.\n    :param pants.auth.basic_auth.BasicAuthCreds creds: The creds to use.\n      If unspecified, assumes that creds are set in the netrc file.\n    :param pants.auth.cookies.Cookies cookies: Store the auth cookies in this instance.\n      If unspecified, uses the global instance.\n    :raises pants.auth.basic_auth.BasicAuthException: If auth fails due to misconfiguration or\n      rejection by the server.\n    \"\"\"\n    cookies = cookies or Cookies.global_instance()\n\n    if not provider:\n      raise BasicAuthException('No basic auth provider specified.')\n\n    provider_config = self.get_options().providers.get(provider)\n    if not provider_config:\n      raise BasicAuthException('No config found for provider {}.'.format(provider))\n\n    url = provider_config.get('url')\n    if not url:\n      raise BasicAuthException('No url found in config for provider {}.'.format(provider))\n    if not self.get_options().allow_insecure_urls and not url.startswith('https://'):\n      raise BasicAuthException('Auth url for provider {} is not secure: {}.'.format(provider, url))\n\n    if creds:\n      auth = requests.auth.HTTPBasicAuth(creds.username, creds.password)\n    else:\n      auth = None  # requests will use the netrc creds.\n    response = requests.get(url, auth=auth)\n\n    if response.status_code != requests.codes.ok:\n      if response.status_code == requests.codes.unauthorized:\n        parsed = www_authenticate.parse(response.headers.get('WWW-Authenticate', ''))\n        if 'Basic' in parsed:\n          raise Challenged(url, response.status_code, response.reason, parsed['Basic']['realm'])\n      raise BasicAuthException(url, response.status_code, response.reason)\n\n    cookies.update(response.cookies)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef glob1(self, dir_relpath, glob):\n    if self.isignored(dir_relpath, directory=True):\n      return []\n\n    matched_files = self._glob1_raw(dir_relpath, glob)\n    prefix = self._relpath_no_dot(dir_relpath)\n    return self._filter_ignored(matched_files, selector=lambda p: os.path.join(prefix, p))", "response": "Returns a list of paths in path that match glob and are not ignored."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef scandir(self, relpath):\n    if self.isignored(relpath, directory=True):\n      self._raise_access_ignored(relpath)\n\n    return self._filter_ignored(self._scandir_raw(relpath), selector=lambda e: e.path)", "response": "Return a list of paths relative to the root which are in the given directory and not ignored."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if path is a directory and is not ignored.", "response": "def isdir(self, relpath):\n    \"\"\"Returns True if path is a directory and is not ignored.\"\"\"\n    if self._isdir_raw(relpath):\n      if not self.isignored(relpath, directory=True):\n        return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if path is a file.", "response": "def isfile(self, relpath):\n    \"\"\"Returns True if path is a file and is not ignored.\"\"\"\n    if self.isignored(relpath):\n      return False\n    return self._isfile_raw(relpath)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if path exists and is not ignored.", "response": "def exists(self, relpath):\n    \"\"\"Returns True if path exists and is not ignored.\"\"\"\n    if self.isignored(self._append_slash_if_dir_path(relpath)):\n      return False\n    return self._exists_raw(relpath)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the content of the file at the given path. Raises exception if path is ignored.", "response": "def content(self, file_relpath):\n    \"\"\"Returns the content for file at path. Raises exception if path is ignored.\n\n    Raises exception if path is ignored.\n    \"\"\"\n    if self.isignored(file_relpath):\n      self._raise_access_ignored(file_relpath)\n\n    return self._content_raw(file_relpath)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes a readlink on the given path which may result in a relative path.", "response": "def relative_readlink(self, relpath):\n    \"\"\"Execute `readlink` for the given path, which may result in a relative path.\n\n    Raises exception if path is ignored.\n    \"\"\"\n    if self.isignored(self._append_slash_if_dir_path(relpath)):\n      self._raise_access_ignored(relpath)\n    return self._relative_readlink_raw(relpath)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef walk(self, relpath, topdown=True):\n    for root, dirs, files in self._walk_raw(relpath, topdown):\n      matched_dirs = self.ignore.match_files([os.path.join(root, \"{}/\".format(d)) for d in dirs])\n      matched_files = self.ignore.match_files([os.path.join(root, f) for f in files])\n      for matched_dir in matched_dirs:\n        dirs.remove(fast_relpath(matched_dir, root).rstrip('/'))\n\n      for matched_file in matched_files:\n        files.remove(fast_relpath(matched_file, root))\n\n      yield root, dirs, files", "response": "Walk the file tree rooted at path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef isignored(self, relpath, directory=False):\n    relpath = self._relpath_no_dot(relpath)\n    if directory:\n      relpath = self._append_trailing_slash(relpath)\n    return self.ignore.match_file(relpath)", "response": "Returns True if path matches pants ignore pattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives an opaque entry list filter any entries that are not in the ignore list.", "response": "def _filter_ignored(self, entries, selector=None):\n    \"\"\"Given an opaque entry list, filter any ignored entries.\n\n    :param entries: A list or generator that produces entries to filter.\n    :param selector: A function that computes a path for an entry relative to the root of the\n      ProjectTree, or None to use identity.\n    \"\"\"\n    selector = selector or (lambda x: x)\n    prefixed_entries = [(self._append_slash_if_dir_path(selector(entry)), entry)\n                          for entry in entries]\n    ignored_paths = set(self.ignore.match_files(path for path, _ in prefixed_entries))\n    return [entry for path, entry in prefixed_entries if path not in ignored_paths]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _append_slash_if_dir_path(self, relpath):\n    if self._isdir_raw(relpath):\n      return self._append_trailing_slash(relpath)\n\n    return relpath", "response": "For a dir path return a path that has a trailing slash."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of target specs that are imported from the target.", "response": "def imported_target_specs(cls, kwargs=None, payload=None):\n    \"\"\"\n    :param kwargs: A kwargs dict representing Target.__init__(**kwargs) (Optional).\n    :param payload: A Payload object representing the Target.__init__(payload=...) param.  (Optional).\n    :returns: list of target specs to be imported.\n    :rtype: list of str\n    \"\"\"\n    if kwargs is not None:\n      assert payload is None, 'may not provide both kwargs and payload'\n      field = cls.imported_target_kwargs_field\n      target_representation = kwargs\n    else:\n      assert payload is not None, 'must provide either kwargs or payload'\n      field = cls.imported_target_payload_field\n      target_representation = payload.as_dict()\n\n    specs = []\n    for item in target_representation.get(field, ()):\n      if not isinstance(item, string_types):\n        raise cls.ExpectedAddressError(\n          'expected imports to contain string addresses, got {obj} (type: {found_class}) instead.'\n          .format(obj=item, found_class=type(item).__name__)\n        )\n      specs.append(item)\n\n    return specs"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of target instances for each target in the list of imported_target_specs.", "response": "def imported_targets(self):\n    \"\"\"\n    :returns: target instances for specs referenced by imported_target_specs.\n    :rtype: list of Target\n    \"\"\"\n    libs = []\n    for spec in self.imported_target_specs(payload=self.payload):\n      resolved_target = self._build_graph.get_target_from_spec(spec,\n                                                               relative_to=self.address.spec_path)\n      if not resolved_target:\n        raise self.UnresolvedImportError(\n          'Could not find target {spec} referenced from {relative_to}'\n          .format(spec=spec, relative_to=self.address.spec))\n      try:\n        libs.append(self.expected_target_constraint.validate_satisfied_by(resolved_target))\n      except TypeConstraintError as e:\n        raise self.WrongTargetTypeError(\n          'Wrong target type {spec} referenced from remote sources target {relative_to}: {err}'\n          .format(spec=spec, relative_to=self.address.spec, err=str(e)),\n          e)\n    return libs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute_dependency_specs(cls, kwargs=None, payload=None):\n    for spec in super(ImportRemoteSourcesMixin, cls).compute_dependency_specs(kwargs, payload):\n      yield spec\n\n    imported_target_specs = cls.imported_target_specs(kwargs=kwargs, payload=payload)\n    for spec in imported_target_specs:\n      yield spec", "response": "Tack imported_target_specs onto the traversable_specs generator for this target."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nallocate a temporary file on same device as cache with a suffix chosen to prevent collisions", "response": "def _tmpfile(self, cache_key, use):\n    \"\"\"Allocate tempfile on same device as cache with a suffix chosen to prevent collisions\"\"\"\n    with temporary_file(suffix=cache_key.id + use, root_dir=self._cache_root,\n                        permissions=self._permissions) as tmpfile:\n      yield tmpfile"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngather paths into artifact store it and yield the path to stored artifact tarball.", "response": "def insert_paths(self, cache_key, paths):\n    \"\"\"Gather paths into artifact, store it, and yield the path to stored artifact tarball.\"\"\"\n    with self._tmpfile(cache_key, 'write') as tmp:\n      self._artifact(tmp.name).collect(paths)\n      yield self._store_tarball(cache_key, tmp.name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstore and then extract the artifact from the given iterator for the given cache_key.", "response": "def store_and_use_artifact(self, cache_key, src, results_dir=None):\n    \"\"\"Store and then extract the artifact from the given `src` iterator for the given cache_key.\n\n    :param cache_key: Cache key for the artifact.\n    :param src: Iterator over binary data to store for the artifact.\n    :param str results_dir: The path to the expected destination of the artifact extraction: will\n      be cleared both before extraction, and after a failure to extract.\n    \"\"\"\n    with self._tmpfile(cache_key, 'read') as tmp:\n      for chunk in src:\n        tmp.write(chunk)\n      tmp.close()\n      tarball = self._store_tarball(cache_key, tmp.name)\n      artifact = self._artifact(tarball)\n\n      # NOTE(mateo): The two clean=True args passed in this method are likely safe, since the cache will by\n      # definition be dealing with unique results_dir, as opposed to the stable vt.results_dir (aka 'current').\n      # But if by chance it's passed the stable results_dir, safe_makedir(clean=True) will silently convert it\n      # from a symlink to a real dir and cause mysterious 'Operation not permitted' errors until the workdir is cleaned.\n      if results_dir is not None:\n        safe_mkdir(results_dir, clean=True)\n\n      try:\n        artifact.extract()\n      except Exception:\n        # Do our best to clean up after a failed artifact extraction. If a results_dir has been\n        # specified, it is \"expected\" to represent the output destination of the extracted\n        # artifact, and so removing it should clear any partially extracted state.\n        if results_dir is not None:\n          safe_mkdir(results_dir, clean=True)\n        safe_delete(tarball)\n        raise\n\n      return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef prune(self, root):\n\n    max_entries_per_target = self._max_entries_per_target\n    if os.path.isdir(root) and max_entries_per_target:\n      safe_rm_oldest_items_in_dir(root, max_entries_per_target)", "response": "Prune stale cache files based on the option - cache - target - max - entry."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hash_all(strs, digest=None):\n  digest = digest or hashlib.sha1()\n  for s in strs:\n    s = ensure_binary(s)\n    digest.update(s)\n  return digest.hexdigest() if PY3 else digest.hexdigest().decode('utf-8')", "response": "Returns a hash of the concatenation of all the strings in strs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hash_file(path, digest=None):\n  digest = digest or hashlib.sha1()\n  with open(path, 'rb') as fd:\n    s = fd.read(8192)\n    while s:\n      digest.update(s)\n      s = fd.read(8192)\n  return digest.hexdigest() if PY3 else digest.hexdigest().decode('utf-8')", "response": "Hashes the contents of the file at the given path and returns the hash digest in hex form."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhashing the given object using the given json. JSONEncoder.", "response": "def json_hash(obj, digest=None, encoder=None):\n  \"\"\"Hashes `obj` by dumping to JSON.\n\n  :param obj: An object that can be rendered to json using the given `encoder`.\n  :param digest: An optional `hashlib` compatible message digest. Defaults to `hashlib.sha1`.\n  :param encoder: An optional custom json encoder.\n  :type encoder: :class:`json.JSONEncoder`\n  :returns: A hash of the given `obj` according to the given `encoder`.\n  :rtype: str\n\n  :API: public\n  \"\"\"\n  json_str = json.dumps(obj, ensure_ascii=True, allow_nan=False, sort_keys=True, cls=encoder)\n  return hash_all(json_str, digest=digest)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_in_shard(self, s):\n    return self.compute_shard(s, self._nshards) == self._shard", "response": "Returns True iff the string s is in this shard."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_classpath_entries(cls, targets, classpath_products, extra_classpath_tuples, confs):\n    classpath_iter = cls._classpath_iter(\n      classpath_products.get_classpath_entries_for_targets(targets),\n      confs=confs,\n    )\n    total_classpath = OrderedSet(classpath_iter)\n\n    filtered_extra_classpath_iter = cls._filtered_classpath_by_confs_iter(\n      extra_classpath_tuples,\n      confs,\n    )\n    extra_classpath_iter = cls._entries_iter(filtered_extra_classpath_iter)\n    total_classpath.update(extra_classpath_iter)\n    return list(total_classpath)", "response": "Computes the classpath entries for the passed targets."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the classpath entries for a given set of targets.", "response": "def compute_classpath(cls, targets, classpath_products, extra_classpath_tuples, confs):\n    \"\"\"Return the list of classpath entries for a classpath covering the passed targets.\n\n    Filters and adds paths from extra_classpath_tuples to the end of the resulting list.\n\n    As compute_classpath_entries but expects and returns strings, not ClasspathEntries.\n    \"\"\"\n    return list(\n      entry.path for entry in\n      cls.compute_classpath_entries(\n        targets,\n        classpath_products,\n        ((scope, ClasspathEntry(path)) for scope, path in extra_classpath_tuples),\n        confs\n      )\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef classpath(cls, targets, classpath_products, confs=('default',)):\n    classpath_iter = cls._classpath_iter(classpath_products.get_for_targets(targets), confs=confs)\n    return list(classpath_iter)", "response": "Returns the classpath for the passed targets."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef internal_classpath(cls, targets, classpath_products, confs=('default',)):\n    classpath_tuples = classpath_products.get_internal_classpath_entries_for_targets(targets)\n    filtered_tuples_iter = cls._filtered_classpath_by_confs_iter(classpath_tuples, confs)\n    return [entry.path for entry in cls._entries_iter(filtered_tuples_iter)]", "response": "Returns the list of internal classpath entries for a list of targets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning classpath entries grouped by their targets.", "response": "def classpath_by_targets(cls, targets, classpath_products, confs=('default',)):\n    \"\"\"Return classpath entries grouped by their targets for the given `targets`.\n\n    :param targets: The targets to lookup classpath products for.\n    :param ClasspathProducts classpath_products: Product containing classpath elements.\n    :param confs: The list of confs for use by this classpath.\n    :returns: The ordered (target, classpath) mappings.\n    :rtype: OrderedDict\n    \"\"\"\n    classpath_target_tuples = classpath_products.get_product_target_mappings_for_targets(targets)\n    filtered_items_iter = filter(cls._accept_conf_filter(confs, lambda x: x[0][0]),\n                                            classpath_target_tuples)\n\n    # group (classpath_entry, target) tuples by targets\n    target_to_classpath = OrderedDict()\n    for classpath_entry, target in filtered_items_iter:\n      _, entry = classpath_entry\n      if not target in target_to_classpath:\n        target_to_classpath[target] = []\n      target_to_classpath[target].append(entry)\n    return target_to_classpath"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a generator over the contents of the classpath for the given targets.", "response": "def classpath_contents(cls, targets, classpath_products, confs=('default',)):\n    \"\"\"Provide a generator over the contents (classes/resources) of a classpath.\n\n    :param targets: Targets to iterate the contents classpath for.\n    :param ClasspathProducts classpath_products: Product containing classpath elements.\n    :param confs: The list of confs for use by this classpath.\n    :returns: An iterator over all classpath contents, one directory, class or resource relative\n              path per iteration step.\n    :rtype: :class:`collections.Iterator` of string\n    \"\"\"\n    classpath_iter = cls._classpath_iter(\n      classpath_products.get_for_targets(targets),\n      confs=confs,\n    )\n    for f in cls.classpath_entries_contents(classpath_iter):\n      yield f"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef classpath_entries_contents(cls, classpath_entries):\n    for entry in classpath_entries:\n      if cls.is_jar(entry):\n        # Walk the jar namelist.\n        with open_zip(entry, mode='r') as jar:\n          for name in jar.namelist():\n            yield ensure_text(name)\n      elif os.path.isdir(entry):\n        # Walk the directory, including subdirs.\n        def rel_walk_name(abs_sub_dir, name):\n          return fast_relpath(os.path.join(abs_sub_dir, name), entry)\n        for abs_sub_dir, dirnames, filenames in safe_walk(entry):\n          for name in dirnames:\n            yield '{}/'.format(rel_walk_name(abs_sub_dir, name))\n          for name in filenames:\n            yield rel_walk_name(abs_sub_dir, name)\n      else:\n        # non-jar and non-directory classpath entries should be ignored\n        pass", "response": "Generates a generator over the contents of a classpath_entries."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating an object of the given type with the given alias.", "response": "def create_object(self, alias, *args, **kwargs):\n    \"\"\"Constructs the type with the given alias using the given args and kwargs.\n\n    NB: aliases may be the alias' object type itself if that type is known.\n\n    :API: public\n\n    :param alias: Either the type alias or the type itself.\n    :type alias: string|type\n    :param *args: These pass through to the underlying callable object.\n    :param **kwargs: These pass through to the underlying callable object.\n    :returns: The created object.\n    \"\"\"\n    object_type = self._type_aliases.get(alias)\n    if object_type is None:\n      raise KeyError('There is no type registered for alias {0}'.format(alias))\n    return object_type(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an object of the given type with the given alias if it does not already exist.", "response": "def create_object_if_not_exists(self, alias, name=None, *args, **kwargs):\n    \"\"\"Constructs the type with the given alias using the given args and kwargs.\n\n    NB: aliases may be the alias' object type itself if that type is known.\n\n    :API: public\n\n    :param alias: Either the type alias or the type itself.\n    :type alias: string|type\n    :param *args: These pass through to the underlying callable object.\n    :param **kwargs: These pass through to the underlying callable object.\n    :returns: The created object, or an existing object with the same `name`.\n    \"\"\"\n    if name is None:\n      raise ValueError(\"Method requires an object `name`.\")\n    obj_creator = functools.partial(self.create_object,\n                                    alias,\n                                    name=name,\n                                    *args,\n                                    **kwargs)\n    return self._storage.add_if_not_exists(name, obj_creator)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _setup_pailgun(self):\n    # Constructs and returns a runnable PantsRunner.\n    def runner_factory(sock, arguments, environment):\n      return self._runner_class.create(\n        sock,\n        arguments,\n        environment,\n        self.services,\n        self._scheduler_service,\n      )\n\n    # Plumb the daemon's lifecycle lock to the `PailgunServer` to safeguard teardown.\n    # This indirection exists to allow the server to be created before PantsService.setup\n    # has been called to actually initialize the `services` field.\n    @contextmanager\n    def lifecycle_lock():\n      with self.services.lifecycle_lock:\n        yield\n\n    return PailgunServer(self._bind_addr, runner_factory, lifecycle_lock, self._request_complete_callback)", "response": "Creates a PailgunServer instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef terminate(self):\n    # Tear down the Pailgun TCPServer.\n    if self.pailgun:\n      self.pailgun.server_close()\n\n    super(PailgunService, self).terminate()", "response": "Override of PantsService. terminate that cleans up when the Pailgun server is terminated."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef open(self):\n    safe_mkdir(os.path.dirname(self._html_dir))\n    self._report_file = open(self.report_path(), 'w')", "response": "Implementation of Reporter callback."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef close(self):\n    self._report_file.close()\n    # Make sure everything's closed.\n    for files in self._output_files.values():\n      for f in files.values():\n        f.close()", "response": "Implementation of Reporter callback."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start_workunit(self, workunit):\n    # We use these properties of the workunit to decide how to render information about it.\n    is_bootstrap = workunit.has_label(WorkUnitLabel.BOOTSTRAP)\n    is_tool = workunit.has_label(WorkUnitLabel.TOOL)\n    is_multitool = workunit.has_label(WorkUnitLabel.MULTITOOL)\n    is_test = workunit.has_label(WorkUnitLabel.TEST)\n\n    initially_open = is_test or not (is_bootstrap or is_tool or is_multitool)\n\n    # Render the workunit's div.\n    s = self._start_workunit_fmt_string.format(\n      indent=len(workunit.ancestors()) * 10,\n      id=workunit.id,\n      parent_id=workunit.parent.id if workunit.parent else '',\n      workunit=workunit,\n      icon_caret='down' if initially_open else 'right',\n      display_class='' if initially_open else 'nodisplay',\n      icon='cog' if is_tool else 'cogs' if is_multitool else 'none'\n    )\n    self._emit(s)\n\n    if is_tool:\n      tool_invocation_details = '\\n'.join([\n        self._render_tool_detail(workunit=workunit, title='cmd', class_prefix='cmd'),\n        # Have test framework stdout open by default, but not that of other tools.\n        # This is an arbitrary choice, but one that turns out to be useful to users in practice.\n        self._render_tool_detail(workunit=workunit, title='stdout', initially_open=is_test),\n        self._render_tool_detail(workunit=workunit, title='stderr'),\n      ])\n\n      cmd = workunit.cmd or ''\n      linkified_cmd = linkify(self._buildroot, cmd.replace('$', '\\\\\\\\$'), self._linkify_memo)\n\n      s = self._start_tool_invocation_fmt_string.format(\n        tool_invocation_details=tool_invocation_details,\n        html_path_base=self._html_path_base,\n        id=workunit.id,\n        cmd=linkified_cmd\n      )\n\n      self._emit(s)", "response": "Implementation of Reporter callback."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef end_workunit(self, workunit):\n    duration = workunit.duration()\n    timing = '{:.3f}'.format(duration)\n    unaccounted_time = ''\n    # Background work may be idle a lot, no point in reporting that as unaccounted.\n    if self.is_under_main_root(workunit):\n      unaccounted_time_secs = workunit.unaccounted_time()\n      if unaccounted_time_secs >= 1 and unaccounted_time_secs > 0.05 * duration:\n        unaccounted_time = '{:.3f}'.format(unaccounted_time_secs)\n\n    status = HtmlReporter._outcome_css_classes[workunit.outcome()]\n\n    if workunit.has_label(WorkUnitLabel.TOOL):\n      self._emit(self._end_tool_invocation_fmt_string.format(\n        id=workunit.id,\n        status=status\n      ))\n\n    self._emit(self._end_workunit_fmt_string.format(\n      id=workunit.id,\n      status=status,\n      timing=timing,\n      unaccounted_time=unaccounted_time,\n      aborted='true' if workunit.outcome() == WorkUnit.ABORTED else 'false'\n    ))\n\n    # If we're a root workunit, force an overwrite, as we may be the last ever write in this run.\n    force_overwrite = workunit.parent is None\n\n    # Update the timings.\n    def render_timings(timings):\n      timings_dict = timings.get_all()\n      for item in timings_dict:\n        item['timing_string'] = '{:.3f}'.format(item['timing'])\n\n      res = ['<table>']\n      for item in timings_dict:\n        res.append(\"\"\"<tr><td class=\"timing-string\">{timing:.3f}</td>\n                          <td class=\"timing-label\">{label}\"\"\".format(\n          timing=item['timing'],\n          label=item['label']\n        ))\n        if item['is_tool']:\n          res.append(\"\"\"<i class=\"icon-cog\"></i>\"\"\")\n        res.append(\"\"\"</td></tr>\"\"\")\n      res.append('<table>')\n\n      return ''.join(res)\n\n    self._overwrite('cumulative_timings',\n                    lambda: render_timings(self.run_tracker.cumulative_timings),\n                    force=force_overwrite)\n    self._overwrite('self_timings',\n                    lambda: render_timings(self.run_tracker.self_timings),\n                    force=force_overwrite)\n\n    # Update the artifact cache stats.\n    def render_cache_stats(artifact_cache_stats):\n      def fix_detail_id(e, _id):\n        return e if isinstance(e, string_types) else e + (_id, )\n\n      msg_elements = []\n      for cache_name, stat in artifact_cache_stats.stats_per_cache.items():\n        # TODO consider display causes for hit/miss targets\n        hit_targets = [tgt for tgt, cause in stat.hit_targets]\n        miss_targets = [tgt for tgt, cause in stat.miss_targets]\n        msg_elements.extend([\n          cache_name + ' artifact cache: ',\n          # Explicitly set the detail ids, so their displayed/hidden state survives a refresh.\n          fix_detail_id(items_to_report_element(hit_targets, 'hit'), 'cache-hit-details'),\n          ', ',\n          fix_detail_id(items_to_report_element(miss_targets, 'miss'), 'cache-miss-details'),\n          '.'\n        ])\n      if not msg_elements:\n        msg_elements = ['No artifact cache use.']\n      return self._render_message(*msg_elements)\n\n    self._overwrite('artifact_cache_stats',\n                    lambda: render_cache_stats(self.run_tracker.artifact_cache_stats),\n                    force=force_overwrite)\n\n    for f in self._output_files[workunit.id].values():\n      f.close()", "response": "Implementation of Reporter callback."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_output(self, workunit, label, s):\n    if os.path.exists(self._html_dir):  # Make sure we're not immediately after a clean-all.\n      path = os.path.join(self._html_dir, '{}.{}'.format(workunit.id, label))\n      output_files = self._output_files[workunit.id]\n      if path not in output_files:\n        f = open(path, 'w')\n        output_files[path] = f\n      else:\n        f = output_files[path]\n      f.write(self._htmlify_text(s))\n      # We must flush in the same thread as the write.\n      f.flush()", "response": "Implementation of Reporter callback."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_handle_log(self, workunit, level, *msg_elements):\n    message = self._render_message(*msg_elements)\n    s = self._log_fmt_string.format(content_id=uuid.uuid4(),\n                                    workunit_id=workunit.id,\n                                    css_class=HtmlReporter._log_level_css_map[level],\n                                    message=message)\n\n    # Emit that javascript to the main report body.\n    self._emit(s)", "response": "Implementation of Reporter callback."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _emit(self, s):\n    if os.path.exists(self._html_dir):  # Make sure we're not immediately after a clean-all.\n      self._report_file.write(s)\n      self._report_file.flush()", "response": "Append content to the main report file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nover-writes a file with the specified contents.", "response": "def _overwrite(self, filename, func, force=False):\n    \"\"\"Overwrite a file with the specified contents.\n\n    Write times are tracked, too-frequent overwrites are skipped, for performance reasons.\n\n    :param filename: The path under the html dir to write to.\n    :param func: A no-arg function that returns the contents to write.\n    :param force: Whether to force a write now, regardless of the last overwrite time.\n    \"\"\"\n    now = int(time.time() * 1000)\n    last_overwrite_time = self._last_overwrite_time.get(filename) or now\n    # Overwrite only once per second.\n    if (now - last_overwrite_time >= 1000) or force:\n      if os.path.exists(self._html_dir):  # Make sure we're not immediately after a clean-all.\n        with open(os.path.join(self._html_dir, filename), 'w') as f:\n          f.write(func())\n      self._last_overwrite_time[filename] = now"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _htmlify_text(self, s):\n    colored = self._handle_ansi_color_codes(html.escape(s))\n    return linkify(self._buildroot, colored, self._linkify_memo).replace('\\n', '</br>')", "response": "Make text HTML - friendly."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreplace ANSI escape sequences with spans of appropriately named css classes.", "response": "def _handle_ansi_color_codes(self, s):\n    \"\"\"Replace ansi escape sequences with spans of appropriately named css classes.\"\"\"\n    parts = HtmlReporter._ANSI_COLOR_CODE_RE.split(s)\n    ret = []\n    span_depth = 0\n    # Note that len(parts) is always odd: text, code, text, code, ..., text.\n    for i in range(0, len(parts), 2):\n      ret.append(parts[i])\n      if i + 1 < len(parts):\n        for code in parts[i + 1].split(';'):\n          if code == 0:  # Reset.\n            while span_depth > 0:\n              ret.append('</span>')\n              span_depth -= 1\n          else:\n            ret.append('<span class=\"ansi-{}\">'.format(code))\n            span_depth += 1\n    while span_depth > 0:\n      ret.append('</span>')\n      span_depth -= 1\n\n    return ''.join(ret)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a set of source files from the given targets.", "response": "def _calculate_isortable_python_sources(self, targets):\n    \"\"\"Generate a set of source files from the given targets.\"\"\"\n    sources = set()\n    for target in targets:\n      sources.update(\n        source for source in target.sources_relative_to_buildroot()\n        if os.path.splitext(source)[1] == self._PYTHON_SOURCE_EXTENSION\n      )\n    return list(sources)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new instance of LocalPantsRunner by parsing options and creating a new run.", "response": "def create(cls, exiter, args, env, target_roots=None, daemon_graph_session=None,\n             options_bootstrapper=None):\n    \"\"\"Creates a new LocalPantsRunner instance by parsing options.\n\n    :param Exiter exiter: The Exiter instance to use for this run.\n    :param list args: The arguments (e.g. sys.argv) for this run.\n    :param dict env: The environment (e.g. os.environ) for this run.\n    :param TargetRoots target_roots: The target roots for this run.\n    :param LegacyGraphSession daemon_graph_session: The graph helper for this session.\n    :param OptionsBootstrapper options_bootstrapper: The OptionsBootstrapper instance to reuse.\n    \"\"\"\n    build_root = get_buildroot()\n\n    options, build_config, options_bootstrapper = cls.parse_options(\n      args,\n      env,\n      setup_logging=True,\n      options_bootstrapper=options_bootstrapper,\n    )\n    global_options = options.for_global_scope()\n\n    # Option values are usually computed lazily on demand,\n    # but command line options are eagerly computed for validation.\n    for scope in options.scope_to_flags.keys():\n      options.for_scope(scope)\n\n    # Verify configs.\n    if global_options.verify_config:\n      options_bootstrapper.verify_configs_against_options(options)\n\n    # If we're running with the daemon, we'll be handed a session from the\n    # resident graph helper - otherwise initialize a new one here.\n    graph_session = cls._maybe_init_graph_session(\n      daemon_graph_session,\n      options_bootstrapper,\n      build_config,\n      options\n    )\n\n    target_roots = cls._maybe_init_target_roots(\n      target_roots,\n      graph_session,\n      options,\n      build_root\n    )\n\n    profile_path = env.get('PANTS_PROFILE')\n\n    return cls(\n      build_root,\n      exiter,\n      options,\n      options_bootstrapper,\n      build_config,\n      target_roots,\n      graph_session,\n      daemon_graph_session is not None,\n      profile_path\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle requests for help information.", "response": "def _maybe_handle_help(self):\n    \"\"\"Handle requests for `help` information.\"\"\"\n    if self._options.help_request:\n      help_printer = HelpPrinter(self._options)\n      result = help_printer.print_help()\n      self._exiter(result)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compute_final_exit_code(*codes):\n    max_code = None\n    for code in codes:\n      if max_code is None or abs(max_code) < abs(code):\n        max_code = code\n    return max_code", "response": "Compute the final exit code for the given list of codes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef import_oracle(self):\n    return ImportOracle(go_dist=self.go_dist, workunit_factory=self.context.new_workunit)", "response": "Return an import oracle that can help look up and categorize imports."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the set of all Go standard library import paths.", "response": "def go_stdlib(self):\n    \"\"\"Return the set of all Go standard library import paths.\n\n    :rtype: frozenset of string\n    \"\"\"\n    out = self._go_dist.create_go_cmd('list', args=['std']).check_output()\n    return frozenset(out.decode('utf-8').strip().split())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a listing of all imports of the given package.", "response": "def list_imports(self, pkg, gopath=None):\n    \"\"\"Return a listing of the dependencies of the given package.\n\n    :param string pkg: The package whose files to list all dependencies of.\n    :param string gopath: An optional $GOPATH which points to a Go workspace containing `pkg`.\n    :returns: The import listing for `pkg` that represents all its dependencies.\n    :rtype: :class:`ImportOracle.ImportListing`\n    :raises: :class:`ImportOracle.ListDepsError` if there was a problem listing the dependencies\n             of `pkg`.\n    \"\"\"\n    go_cmd = self._go_dist.create_go_cmd('list', args=['-json', pkg], gopath=gopath)\n    with self._workunit_factory('list {}'.format(pkg), cmd=str(go_cmd),\n                                labels=[WorkUnitLabel.TOOL]) as workunit:\n      # TODO(John Sirois): It would be nice to be able to tee the stdout to the workunit to we have\n      # a capture of the json available for inspection in the server console.\n      process = go_cmd.spawn(stdout=subprocess.PIPE, stderr=workunit.output('stderr'))\n      out, _ = process.communicate()\n      returncode = process.returncode\n      workunit.set_outcome(WorkUnit.SUCCESS if returncode == 0 else WorkUnit.FAILURE)\n      if returncode != 0:\n        raise self.ListDepsError('Problem listing imports for {}: {} failed with exit code {}'\n                                 .format(pkg, go_cmd, returncode))\n      data = json.loads(out.decode('utf-8'))\n\n      # XTestImports are for black box tests.  These test files live inside the package dir but\n      # declare a different package and thus can only access the public members of the package's\n      # production code.  This style of test necessarily means the test file will import the main\n      # package.  For pants, this would lead to a cyclic self-dependency, so we omit the main\n      # package as implicitly included as its own dependency.\n      x_test_imports = [i for i in data.get('XTestImports', []) if i != pkg]\n\n      return self.ImportListing(pkg_name=data.get('Name'),\n                                imports=data.get('Imports', []),\n                                test_imports=data.get('TestImports', []),\n                                x_test_imports=x_test_imports)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\naugmenting text by heuristically finding URL and file references and turning them into links.", "response": "def linkify(buildroot, s, memoized_urls):\n  \"\"\"Augment text by heuristically finding URL and file references and turning them into links.\n\n  :param string buildroot: The base directory of the project.\n  :param string s: The text to insert links into.\n  :param dict memoized_urls: A cache of text to links so repeated substitutions don't require\n                             additional file stats calls.\n  \"\"\"\n  def memoized_to_url(m):\n    # to_url uses None to signal not to replace the text,\n    # so we use a different sentinel value.\n    value = memoized_urls.get(m.group(0), _NO_URL)\n    if value is _NO_URL:\n      value = to_url(m)\n      memoized_urls[m.group(0)] = value\n    return value\n\n  def to_url(m):\n    if m.group(1):\n      return m.group(0)  # It's an http(s) url.\n    path = m.group(0)\n\n    if path.startswith('/'):\n      path = os.path.relpath(path, buildroot)\n    elif path.startswith('..'):\n      # The path is not located inside the buildroot, so it's definitely not a BUILD file.\n      return None\n    else:\n      # The path is located in the buildroot: see if it's a reference to a target in a BUILD file.\n      parts = path.split(':')\n      if len(parts) == 2:\n        putative_dir = parts[0]\n      else:\n        putative_dir = path\n      if os.path.isdir(os.path.join(buildroot, putative_dir)):\n        build_files = list(BuildFile.get_build_files_family(\n          FileSystemProjectTree(buildroot),\n          putative_dir))\n        if build_files:\n          path = build_files[0].relpath\n        else:\n          return None\n\n    if os.path.exists(os.path.join(buildroot, path)):\n      # The reporting server serves file content at /browse/<path_from_buildroot>.\n      return '/browse/{}'.format(path)\n    else:\n      return None\n\n  def maybe_add_link(url, text):\n    return '<a target=\"_blank\" href=\"{}\">{}</a>'.format(url, text) if url else text\n\n  return _PATH_RE.sub(lambda m: maybe_add_link(memoized_to_url(m), m.group(0)), s)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_timing(self, label, secs, is_tool=False):\n    self._timings_by_path[label] += secs\n    if is_tool:\n      self._tool_labels.add(label)\n    # Check existence in case we're a clean-all. We don't want to write anything in that case.\n    if self._path and os.path.exists(os.path.dirname(self._path)):\n      with open(self._path, 'w') as f:\n        for x in self.get_all():\n          f.write('{label}: {timing}\\n'.format(**x))", "response": "Add a timing to the list of timings."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns all the timings sorted in decreasing order.", "response": "def get_all(self):\n    \"\"\"Returns all the timings, sorted in decreasing order.\n\n    Each value is a dict: { path: <path>, timing: <timing in seconds> }\n    \"\"\"\n    return [{'label': x[0], 'timing': x[1], 'is_tool': x[0] in self._tool_labels}\n            for x in sorted(self._timings_by_path.items(), key=lambda x: x[1], reverse=True)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nyielding a sequence of absolute path of source class or jar files provided by the target.", "response": "def files_for_target(self, target):\n    \"\"\"Yields a sequence of abs path of source, class or jar files provided by the target.\n\n    The runtime classpath for a target must already have been finalized for a target in order\n    to compute its provided files.\n    \"\"\"\n    def gen():\n      # Compute src -> target.\n      if isinstance(target, JvmTarget):\n        for src in target.sources_relative_to_buildroot():\n          yield os.path.join(self.buildroot, src)\n      # TODO(Tejal Desai): pantsbuild/pants/65: Remove java_sources attribute for ScalaLibrary\n      if isinstance(target, ScalaLibrary):\n        for java_source in target.java_sources:\n          for src in java_source.sources_relative_to_buildroot():\n            yield os.path.join(self.buildroot, src)\n\n      # Compute classfile -> target and jar -> target.\n      files = ClasspathUtil.classpath_contents((target,), self.runtime_classpath)\n      # And jars; for binary deps, zinc doesn't emit precise deps (yet).\n      cp_entries = ClasspathUtil.classpath((target,), self.runtime_classpath)\n      jars = [cpe for cpe in cp_entries if ClasspathUtil.is_jar(cpe)]\n      for coll in [files, jars]:\n        for f in coll:\n          yield f\n    return set(gen())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef targets_by_file(self, targets):\n    targets_by_file = defaultdict(OrderedSet)\n\n    for target in targets:\n      for f in self.files_for_target(target):\n        targets_by_file[f].add(target)\n\n    return targets_by_file", "response": "Returns a map from absolute path of source class or jar file to an OrderedSet of targets."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsearches which targets from target s transitive dependencies contain classname.", "response": "def targets_for_class(self, target, classname):\n    \"\"\"Search which targets from `target`'s transitive dependencies contain `classname`.\"\"\"\n    targets_with_class = set()\n    for target in target.closure():\n      for one_class in self._target_classes(target):\n        if classname in one_class:\n          targets_with_class.add(target)\n          break\n\n    return targets_with_class"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a set of target s provided classes.", "response": "def _target_classes(self, target):\n    \"\"\"Set of target's provided classes.\n\n    Call at the target level is to memoize efficiently.\n    \"\"\"\n    target_classes = set()\n    contents = ClasspathUtil.classpath_contents((target,), self.runtime_classpath)\n    for f in contents:\n      classname = ClasspathUtil.classname_for_rel_classfile(f)\n      if classname:\n        target_classes.add(classname)\n    return target_classes"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an iterator over the classfiles inside jar_file.", "response": "def _jar_classfiles(self, jar_file):\n    \"\"\"Returns an iterator over the classfiles inside jar_file.\"\"\"\n    for cls in ClasspathUtil.classpath_entries_contents([jar_file]):\n      if cls.endswith('.class'):\n        yield cls"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a set of classfiles from the JVM bootstrap jars.", "response": "def bootstrap_jar_classfiles(self):\n    \"\"\"Returns a set of classfiles from the JVM bootstrap jars.\"\"\"\n    bootstrap_jar_classfiles = set()\n    for jar_file in self._find_all_bootstrap_jars():\n      for cls in self._jar_classfiles(jar_file):\n        bootstrap_jar_classfiles.add(cls)\n    return bootstrap_jar_classfiles"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_transitive_deps_by_target(self, targets):\n    # Sort from least to most dependent.\n    sorted_targets = reversed(sort_targets(targets))\n    transitive_deps_by_target = defaultdict(set)\n    # Iterate in dep order, to accumulate the transitive deps for each target.\n    for target in sorted_targets:\n      transitive_deps = set()\n      for dep in target.dependencies:\n        transitive_deps.update(transitive_deps_by_target.get(dep, []))\n        transitive_deps.add(dep)\n\n      # Need to handle the case where a java_sources target has dependencies.\n      # In particular if it depends back on the original target.\n      if hasattr(target, 'java_sources'):\n        for java_source_target in target.java_sources:\n          for transitive_dep in java_source_target.dependencies:\n            transitive_deps_by_target[java_source_target].add(transitive_dep)\n\n      transitive_deps_by_target[target] = transitive_deps\n    return transitive_deps_by_target", "response": "Compute the transitive dependencies of the given targets."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resolve_aliases(self, target, scope=None):\n    for declared in target.dependencies:\n      if scope is not None and declared.scope != scope:\n        # Only `DEFAULT` scoped deps are eligible for the unused dep check.\n        continue\n      elif type(declared) in (AliasTarget, Target):\n        # Is an alias. Recurse to expand.\n        for r, _ in self.resolve_aliases(declared, scope=scope):\n          yield r, declared\n      else:\n        yield declared, None", "response": "Resolves aliases in the direct dependencies of the target."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef resolve(self, executor, targets, classpath_products, confs=None, extra_args=None,\n              invalidate_dependents=False):\n    \"\"\"Resolves external classpath products (typically jars) for the given targets.\n\n    :API: public\n\n    :param executor: A java executor to run ivy with.\n    :type executor: :class:`pants.java.executor.Executor`\n    :param targets: The targets to resolve jvm dependencies for.\n    :type targets: :class:`collections.Iterable` of :class:`pants.build_graph.target.Target`\n    :param classpath_products: The classpath products to populate with the results of the resolve.\n    :type classpath_products: :class:`pants.backend.jvm.tasks.classpath_products.ClasspathProducts`\n    :param confs: The ivy configurations to resolve; ('default',) by default.\n    :type confs: :class:`collections.Iterable` of string\n    :param extra_args: Any extra command line arguments to pass to ivy.\n    :type extra_args: list of string\n    :param bool invalidate_dependents: `True` to invalidate dependents of targets that needed to be\n                                        resolved.\n    :returns: The results of each of the resolves run by this call.\n    :rtype: list of IvyResolveResult\n    \"\"\"\n    confs = confs or ('default',)\n    targets_by_sets = JarDependencyManagement.global_instance().targets_by_artifact_set(targets)\n    results = []\n    for artifact_set, target_subset in targets_by_sets.items():\n      results.append(self._resolve_subset(executor,\n                                                     target_subset,\n                                                     classpath_products,\n                                                     confs=confs,\n                                                     extra_args=extra_args,\n                                                     invalidate_dependents=invalidate_dependents,\n                                                     pinned_artifacts=artifact_set))\n    return results", "response": "Resolves external classpath products for the given targets."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ivy_classpath(self, targets, silent=True, workunit_name=None):\n    result = self._ivy_resolve(targets, silent=silent, workunit_name=workunit_name)\n    return result.resolved_artifact_paths", "response": "Create the classpath for the passed targets."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ivy_resolve(self,\n                   targets,\n                   executor=None,\n                   silent=False,\n                   workunit_name=None,\n                   confs=None,\n                   extra_args=None,\n                   invalidate_dependents=False,\n                   pinned_artifacts=None):\n    \"\"\"Resolves external dependencies for the given targets.\n\n    If there are no targets suitable for jvm transitive dependency resolution, an empty result is\n    returned.\n\n    :param targets: The targets to resolve jvm dependencies for.\n    :type targets: :class:`collections.Iterable` of :class:`pants.build_graph.target.Target`\n    :param executor: A java executor to run ivy with.\n    :type executor: :class:`pants.java.executor.Executor`\n\n    :param confs: The ivy configurations to resolve; ('default',) by default.\n    :type confs: :class:`collections.Iterable` of string\n    :param extra_args: Any extra command line arguments to pass to ivy.\n    :type extra_args: list of string\n    :param bool invalidate_dependents: `True` to invalidate dependents of targets that needed to be\n                                        resolved.\n    :returns: The result of the resolve.\n    :rtype: IvyResolveResult\n    \"\"\"\n    # If there are no targets, we don't need to do a resolve.\n    if not targets:\n      return NO_RESOLVE_RUN_RESULT\n\n    confs = confs or ('default',)\n\n    fingerprint_strategy = IvyResolveFingerprintStrategy(confs)\n\n    with self.invalidated(targets,\n                          invalidate_dependents=invalidate_dependents,\n                          silent=silent,\n                          fingerprint_strategy=fingerprint_strategy) as invalidation_check:\n      # In case all the targets were filtered out because they didn't participate in fingerprinting.\n      if not invalidation_check.all_vts:\n        return NO_RESOLVE_RUN_RESULT\n\n      resolve_vts = VersionedTargetSet.from_versioned_targets(invalidation_check.all_vts)\n\n      resolve_hash_name = resolve_vts.cache_key.hash\n      # NB: This used to be a global directory, but is now specific to each task that includes\n      # this mixin.\n      ivy_workdir = os.path.join(self.versioned_workdir, 'ivy')\n      targets = resolve_vts.targets\n\n      fetch = IvyFetchStep(confs,\n                           resolve_hash_name,\n                           pinned_artifacts,\n                           self.get_options().soft_excludes,\n                           self.ivy_resolution_cache_dir,\n                           self.ivy_repository_cache_dir,\n                           ivy_workdir)\n      resolve = IvyResolveStep(confs,\n                               resolve_hash_name,\n                               pinned_artifacts,\n                               self.get_options().soft_excludes,\n                               self.ivy_resolution_cache_dir,\n                               self.ivy_repository_cache_dir,\n                               ivy_workdir)\n\n      return self._perform_resolution(fetch, resolve, executor, extra_args, invalidation_check,\n                                      resolve_vts, targets, workunit_name)", "response": "Resolves external dependencies for the given targets."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new pex that merges the other pexes via PEX_PATH.", "response": "def create_pex(self, pex_info=None):\n    \"\"\"Returns a wrapped pex that \"merges\" the other pexes via PEX_PATH.\"\"\"\n    relevant_targets = self.context.targets(\n      lambda tgt: isinstance(tgt, (\n        PythonDistribution, PythonRequirementLibrary, PythonTarget, Files)))\n    with self.invalidated(relevant_targets) as invalidation_check:\n\n      # If there are no relevant targets, we still go through the motions of resolving\n      # an empty set of requirements, to prevent downstream tasks from having to check\n      # for this special case.\n      if invalidation_check.all_vts:\n        target_set_id = VersionedTargetSet.from_versioned_targets(\n          invalidation_check.all_vts).cache_key.hash\n      else:\n        target_set_id = 'no_targets'\n\n      interpreter = self.context.products.get_data(PythonInterpreter)\n      path = os.path.realpath(os.path.join(self.workdir, str(interpreter.identity), target_set_id))\n\n      # Note that we check for the existence of the directory, instead of for invalid_vts,\n      # to cover the empty case.\n      if not os.path.isdir(path):\n        pexes = [\n          self.context.products.get_data(ResolveRequirements.REQUIREMENTS_PEX),\n          self.context.products.get_data(GatherSources.PYTHON_SOURCES)\n        ]\n\n        if self.extra_requirements():\n          extra_requirements_pex = self.resolve_requirement_strings(\n            interpreter, self.extra_requirements())\n          # Add the extra requirements first, so they take precedence over any colliding version\n          # in the target set's dependency closure.\n          pexes = [extra_requirements_pex] + pexes\n        constraints = {constraint for rt in relevant_targets if is_python_target(rt)\n                       for constraint in PythonSetup.global_instance().compatibility_or_constraints(rt)}\n\n        with self.merged_pex(path, pex_info, interpreter, pexes, constraints) as builder:\n          for extra_file in self.extra_files():\n            extra_file.add_to(builder)\n          builder.freeze()\n\n    return PEX(path, interpreter)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the pants global cache directory.", "response": "def get_pants_cachedir():\n  \"\"\"Return the pants global cache directory.\"\"\"\n  # Follow the unix XDB base spec: http://standards.freedesktop.org/basedir-spec/latest/index.html.\n  cache_home = os.environ.get('XDG_CACHE_HOME')\n  if not cache_home:\n    cache_home = '~/.cache'\n  return os.path.expanduser(os.path.join(cache_home, 'pants'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_pants_configdir():\n  # Follow the unix XDB base spec: http://standards.freedesktop.org/basedir-spec/latest/index.html.\n  config_home = os.environ.get('XDG_CONFIG_HOME')\n  if not config_home:\n    config_home = '~/.config'\n  return os.path.expanduser(os.path.join(config_home, 'pants'))", "response": "Return the pants global config directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the pants Scm if any.", "response": "def get_scm():\n  \"\"\"Returns the pants Scm if any.\n\n  :API: public\n  \"\"\"\n  # TODO(John Sirois): Extract a module/class to carry the bootstrap logic.\n  global _SCM\n  if not _SCM:\n    from pants.scm.git import Git\n    # We know about git, so attempt an auto-configure\n    worktree = Git.detect_worktree()\n    if worktree and os.path.isdir(worktree):\n      git = Git(worktree=worktree)\n      try:\n        logger.debug('Detected git repository at {} on branch {}'.format(worktree, git.branch_name))\n        set_scm(git)\n      except git.LocalException as e:\n        logger.info('Failed to load git repository at {}: {}'.format(worktree, e))\n  return _SCM"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_scm(scm):\n  if scm is not None:\n    if not isinstance(scm, Scm):\n      raise ValueError('The scm must be an instance of Scm, given {}'.format(scm))\n    global _SCM\n    _SCM = scm", "response": "Sets the pants Scm."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of file arguments to provide to the compiler to compile the target.", "response": "def get_sources_headers_for_target(self, target):\n    \"\"\"Return a list of file arguments to provide to the compiler.\n\n    NB: result list will contain both header and source files!\n\n    :raises: :class:`NativeCompile.NativeCompileError` if there is an error processing the sources.\n    \"\"\"\n    # Get source paths relative to the target base so the exception message with the target and\n    # paths makes sense.\n    target_relative_sources = target.sources_relative_to_target_base()\n    rel_root = target_relative_sources.rel_root\n\n    # Unique file names are required because we just dump object files into a single directory, and\n    # the compiler will silently just produce a single object file if provided non-unique filenames.\n    # TODO: add some shading to file names so we can remove this check.\n    # NB: It shouldn't matter if header files have the same name, but this will raise an error in\n    # that case as well. We won't need to do any shading of header file names.\n    seen_filenames = defaultdict(list)\n    for src in target_relative_sources:\n      seen_filenames[os.path.basename(src)].append(src)\n    duplicate_filename_err_msgs = []\n    for fname, source_paths in seen_filenames.items():\n      if len(source_paths) > 1:\n        duplicate_filename_err_msgs.append(\"filename: {}, paths: {}\".format(fname, source_paths))\n    if duplicate_filename_err_msgs:\n      raise self.NativeCompileError(\n        \"Error in target '{}': source files must have a unique filename within a '{}' target. \"\n        \"Conflicting filenames:\\n{}\"\n        .format(target.address.spec, target.alias(), '\\n'.join(duplicate_filename_err_msgs)))\n\n    return [os.path.join(get_buildroot(), rel_root, src) for src in target_relative_sources]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of arguments to use to compile sources. Subclasses can override and append.", "response": "def _make_compile_argv(self, compile_request):\n    \"\"\"Return a list of arguments to use to compile sources. Subclasses can override and append.\"\"\"\n\n    sources_minus_headers = list(self._iter_sources_minus_headers(compile_request))\n    if len(sources_minus_headers) == 0:\n      raise self._HeaderOnlyLibrary()\n\n    compiler = compile_request.compiler\n    compiler_options = compile_request.compiler_options\n    # We are going to execute in the target output, so get absolute paths for everything.\n    buildroot = get_buildroot()\n    # TODO: add -v to every compiler and linker invocation!\n    argv = (\n      [compiler.exe_filename] +\n      compiler.extra_args +\n      # TODO: If we need to produce static libs, don't add -fPIC! (could use Variants -- see #5788).\n      ['-c', '-fPIC'] +\n      compiler_options +\n      [\n        '-I{}'.format(os.path.join(buildroot, inc_dir))\n        for inc_dir in compile_request.include_dirs\n      ] +\n      [os.path.join(buildroot, src) for src in sources_minus_headers])\n\n    self.context.log.info(\"selected compiler exe name: '{}'\".format(compiler.exe_filename))\n    self.context.log.debug(\"compile argv: {}\".format(argv))\n\n    return argv"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming the process of compilation writing object files to the request s output_dir.", "response": "def _compile(self, compile_request):\n    \"\"\"Perform the process of compilation, writing object files to the request's 'output_dir'.\n\n    NB: This method must arrange the output files so that `collect_cached_objects()` can collect all\n    of the results (or vice versa)!\n    \"\"\"\n\n    try:\n      argv = self._make_compile_argv(compile_request)\n    except self._HeaderOnlyLibrary:\n      self.context.log.debug('{} is a header-only library'.format(compile_request))\n      return\n\n    compiler = compile_request.compiler\n    output_dir = compile_request.output_dir\n    env = compiler.invocation_environment_dict\n\n    with self.context.new_workunit(\n        name=self.workunit_label, labels=[WorkUnitLabel.COMPILER]) as workunit:\n      try:\n        process = subprocess.Popen(\n          argv,\n          cwd=output_dir,\n          stdout=workunit.output('stdout'),\n          stderr=workunit.output('stderr'),\n          env=env)\n      except OSError as e:\n        workunit.set_outcome(WorkUnit.FAILURE)\n        raise self.NativeCompileError(\n          \"Error invoking '{exe}' with command {cmd} and environment {env} for request {req}: {err}\"\n          .format(exe=compiler.exe_filename, cmd=argv, env=env, req=compile_request, err=e))\n\n      rc = process.wait()\n      if rc != 0:\n        workunit.set_outcome(WorkUnit.FAILURE)\n        raise self.NativeCompileError(\n          \"Error in '{section_name}' with command {cmd} and environment {env} for request {req}. \"\n          \"Exit code was: {rc}.\"\n          .format(section_name=self.workunit_label, cmd=argv, env=env, req=compile_request, rc=rc))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef collect_cached_objects(self, versioned_target):\n    return ObjectFiles(versioned_target.results_dir, os.listdir(versioned_target.results_dir))", "response": "Scan versioned_target s results directory and return the output files from that directory."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef all_enclosing_scopes(scope, allow_global=True):\n\n  _validate_full_scope(scope)\n\n  # TODO: validate scopes here and/or in `enclosing_scope()` instead of assuming correctness.\n  def scope_within_range(tentative_scope):\n    if tentative_scope is None:\n      return False\n    if not allow_global and tentative_scope == GLOBAL_SCOPE:\n      return False\n    return True\n\n  while scope_within_range(scope):\n    yield scope\n    scope = (None if scope == GLOBAL_SCOPE else enclosing_scope(scope))", "response": "Utility function to return all scopes up to the global scope enclosing a\n  given scope."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _jar_id(jar):\n    if jar.rev:\n      return '{0}:{1}:{2}'.format(jar.org, jar.name, jar.rev)\n    else:\n      return '{0}:{1}'.format(jar.org, jar.name)", "response": "Create a string identifier for the IvyModuleRef key."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a string identifier for the Exclude key.", "response": "def _exclude_id(jar):\n    \"\"\"Create a string identifier for the Exclude key.\n    :param Exclude jar: key for an excluded jar\n    :returns: String representing the key as a maven coordinate\n    \"\"\"\n    return '{0}:{1}'.format(jar.org, jar.name) if jar.name else jar.org"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a dictionary containing all pertinent information about the given targets.", "response": "def generate_targets_map(self, targets, classpath_products=None):\n    \"\"\"Generates a dictionary containing all pertinent information about the target graph.\n\n    The return dictionary is suitable for serialization by json.dumps.\n    :param targets: The list of targets to generate the map for.\n    :param classpath_products: Optional classpath_products. If not provided when the --libraries\n      option is `True`, this task will perform its own jar resolution.\n    \"\"\"\n    targets_map = {}\n    resource_target_map = {}\n    python_interpreter_targets_mapping = defaultdict(list)\n\n    if self.get_options().libraries:\n      # NB(gmalmquist): This supports mocking the classpath_products in tests.\n      if classpath_products is None:\n        classpath_products = self.resolve_jars(targets)\n    else:\n      classpath_products = None\n\n    target_roots_set = set(self.context.target_roots)\n\n    def process_target(current_target):\n      \"\"\"\n      :type current_target:pants.build_graph.target.Target\n      \"\"\"\n      def get_target_type(tgt):\n        def is_test(t):\n          return isinstance(t, JUnitTests) or isinstance(t, PythonTests)\n        if is_test(tgt):\n          return ExportTask.SourceRootTypes.TEST\n        else:\n          if (isinstance(tgt, Resources) and\n              tgt in resource_target_map and\n                is_test(resource_target_map[tgt])):\n            return ExportTask.SourceRootTypes.TEST_RESOURCE\n          elif isinstance(tgt, Resources):\n            return ExportTask.SourceRootTypes.RESOURCE\n          else:\n            return ExportTask.SourceRootTypes.SOURCE\n\n      info = {\n        'targets': [],\n        'libraries': [],\n        'roots': [],\n        'id': current_target.id,\n        'target_type': get_target_type(current_target),\n        # NB: is_code_gen should be removed when export format advances to 1.1.0 or higher\n        'is_code_gen': current_target.is_synthetic,\n        'is_synthetic': current_target.is_synthetic,\n        'pants_target_type': self._get_pants_target_alias(type(current_target)),\n      }\n\n      if not current_target.is_synthetic:\n        info['globs'] = current_target.globs_relative_to_buildroot()\n        if self.get_options().sources:\n          info['sources'] = list(current_target.sources_relative_to_buildroot())\n\n      info['transitive'] = current_target.transitive\n      info['scope'] = str(current_target.scope)\n      info['is_target_root'] = current_target in target_roots_set\n\n      if isinstance(current_target, PythonRequirementLibrary):\n        reqs = current_target.payload.get_field_value('requirements', set())\n        \"\"\":type : set[pants.backend.python.python_requirement.PythonRequirement]\"\"\"\n        info['requirements'] = [req.key for req in reqs]\n\n      if isinstance(current_target, PythonTarget):\n        interpreter_for_target = self._interpreter_cache.select_interpreter_for_targets(\n          [current_target])\n        if interpreter_for_target is None:\n          raise TaskError('Unable to find suitable interpreter for {}'\n                          .format(current_target.address))\n        python_interpreter_targets_mapping[interpreter_for_target].append(current_target)\n        info['python_interpreter'] = str(interpreter_for_target.identity)\n\n      def iter_transitive_jars(jar_lib):\n        \"\"\"\n        :type jar_lib: :class:`pants.backend.jvm.targets.jar_library.JarLibrary`\n        :rtype: :class:`collections.Iterator` of\n                :class:`pants.java.jar.M2Coordinate`\n        \"\"\"\n        if classpath_products:\n          jar_products = classpath_products.get_artifact_classpath_entries_for_targets((jar_lib,))\n          for _, jar_entry in jar_products:\n            coordinate = jar_entry.coordinate\n            # We drop classifier and type_ since those fields are represented in the global\n            # libraries dict and here we just want the key into that dict (see `_jar_id`).\n            yield M2Coordinate(org=coordinate.org, name=coordinate.name, rev=coordinate.rev)\n\n      target_libraries = OrderedSet()\n      if isinstance(current_target, JarLibrary):\n        target_libraries = OrderedSet(iter_transitive_jars(current_target))\n      for dep in current_target.dependencies:\n        info['targets'].append(dep.address.spec)\n        if isinstance(dep, JarLibrary):\n          for jar in dep.jar_dependencies:\n            target_libraries.add(M2Coordinate(jar.org, jar.name, jar.rev))\n          # Add all the jars pulled in by this jar_library\n          target_libraries.update(iter_transitive_jars(dep))\n        if isinstance(dep, Resources):\n          resource_target_map[dep] = current_target\n\n      if isinstance(current_target, ScalaLibrary):\n        for dep in current_target.java_sources:\n          info['targets'].append(dep.address.spec)\n          process_target(dep)\n\n      if isinstance(current_target, JvmTarget):\n        info['excludes'] = [self._exclude_id(exclude) for exclude in current_target.excludes]\n        info['platform'] = current_target.platform.name\n        if hasattr(current_target, 'test_platform'):\n          info['test_platform'] = current_target.test_platform.name\n\n      info['roots'] = [{\n        'source_root': source_root_package_prefix[0],\n        'package_prefix': source_root_package_prefix[1]\n      } for source_root_package_prefix in self._source_roots_for_target(current_target)]\n\n      if classpath_products:\n        info['libraries'] = [self._jar_id(lib) for lib in target_libraries]\n      targets_map[current_target.address.spec] = info\n\n    for target in targets:\n      process_target(target)\n\n    jvm_platforms_map = {\n      'default_platform' : JvmPlatform.global_instance().default_platform.name,\n      'platforms': {\n        str(platform_name): {\n          'target_level' : str(platform.target_level),\n          'source_level' : str(platform.source_level),\n          'args' : platform.args,\n        } for platform_name, platform in JvmPlatform.global_instance().platforms_by_name.items() },\n    }\n\n    graph_info = {\n      'version': self.DEFAULT_EXPORT_VERSION,\n      'targets': targets_map,\n      'jvm_platforms': jvm_platforms_map,\n      # `jvm_distributions` are static distribution settings from config,\n      # `preferred_jvm_distributions` are distributions that pants actually uses for the\n      # given platform setting.\n      'preferred_jvm_distributions': {}\n    }\n\n    for platform_name, platform in JvmPlatform.global_instance().platforms_by_name.items():\n      preferred_distributions = {}\n      for strict, strict_key in [(True, 'strict'), (False, 'non_strict')]:\n        try:\n          dist = JvmPlatform.preferred_jvm_distribution([platform], strict=strict)\n          preferred_distributions[strict_key] = dist.home\n        except DistributionLocator.Error:\n          pass\n\n      if preferred_distributions:\n        graph_info['preferred_jvm_distributions'][platform_name] = preferred_distributions\n\n    if classpath_products:\n      graph_info['libraries'] = self._resolve_jars_info(targets, classpath_products)\n\n    if python_interpreter_targets_mapping:\n      # NB: We've selected a python interpreter compatible with each python target individually into\n      # the `python_interpreter_targets_mapping`. These python targets may not be compatible, ie: we\n      # could have a python target requiring 'CPython>=2.7<3' (ie: CPython-2.7.x) and another\n      # requiring 'CPython>=3.6'. To pick a default interpreter then from among these two choices\n      # is arbitrary and not to be relied on to work as a default interpreter if ever needed by the\n      # export consumer.\n      #\n      # TODO(John Sirois): consider either eliminating the 'default_interpreter' field and pressing\n      # export consumers to make their own choice of a default (if needed) or else use\n      # `select.select_interpreter_for_targets` and fail fast if there is no interpreter compatible\n      # across all the python targets in-play.\n      #\n      # For now, make our arbitrary historical choice of a default interpreter explicit and use the\n      # lowest version.\n      default_interpreter = min(python_interpreter_targets_mapping.keys())\n\n      interpreters_info = {}\n      for interpreter, targets in six.iteritems(python_interpreter_targets_mapping):\n        req_libs = [target for target in Target.closure_for_targets(targets)\n                    if has_python_requirements(target)]\n        chroot = self.resolve_requirements(interpreter, req_libs)\n        interpreters_info[str(interpreter.identity)] = {\n          'binary': interpreter.binary,\n          'chroot': chroot.path()\n        }\n\n      graph_info['python_setup'] = {\n        'default_interpreter': str(default_interpreter.identity),\n        'interpreters': interpreters_info\n      }\n\n    return graph_info"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nresolving the external libraries for the given targets.", "response": "def _resolve_jars_info(self, targets, classpath_products):\n    \"\"\"Consults ivy_jar_products to export the external libraries.\n\n    :return: mapping of jar_id -> { 'default'     : <jar_file>,\n                                    'sources'     : <jar_file>,\n                                    'javadoc'     : <jar_file>,\n                                    <other_confs> : <jar_file>,\n                                  }\n    \"\"\"\n    mapping = defaultdict(dict)\n    jar_products = classpath_products.get_artifact_classpath_entries_for_targets(\n      targets, respect_excludes=False)\n    for conf, jar_entry in jar_products:\n      conf = jar_entry.coordinate.classifier or 'default'\n      mapping[self._jar_id(jar_entry.coordinate)][conf] = jar_entry.cache_path\n    return mapping"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_pants_target_alias(self, pants_target_type):\n    if pants_target_type in self.target_aliases_map:\n      return self.target_aliases_map.get(pants_target_type)\n    else:\n      return \"{}.{}\".format(pants_target_type.__module__, pants_target_type.__name__)", "response": "Returns the pants target alias for the given target type"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _source_roots_for_target(target):\n    def root_package_prefix(source_file):\n      source = os.path.dirname(source_file)\n      return os.path.join(get_buildroot(), target.target_base, source), source.replace(os.sep, '.')\n    return {root_package_prefix(source) for source in target.sources_relative_to_source_root()}", "response": "Returns a list of source roots that are used for the target."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an iterator over all transitive dependencies of the target.", "response": "def all_dependencies(self, target):\n    \"\"\"All transitive dependencies of the context's target.\"\"\"\n    for dep in target.closure(bfs=True, **self.target_closure_kwargs):\n      yield dep"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes a language property setting for the given target and option_name.", "response": "def defaulted_property(self, target, option_name):\n    \"\"\"Computes a language property setting for the given JvmTarget.\n\n    :param selector A function that takes a target or platform and returns the boolean value of the\n                    property for that target or platform, or None if that target or platform does\n                    not directly define the property.\n\n    If the target does not override the language property, returns true iff the property\n    is true for any of the matched languages for the target.\n    \"\"\"\n    if target.has_sources('.java'):\n      matching_subsystem = Java.global_instance()\n    elif target.has_sources('.scala'):\n      matching_subsystem = ScalaPlatform.global_instance()\n    else:\n      return getattr(target, option_name)\n\n    return matching_subsystem.get_scalar_mirrored_target_option(option_name, target)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dist(self):\n    underlying_dist = self.underlying_dist\n    if self._execution_strategy != NailgunTaskBase.HERMETIC:\n      # symlink .pants.d/.jdk -> /some/java/home/\n      jdk_home_symlink = os.path.relpath(\n        os.path.join(self._zinc_factory.get_options().pants_workdir, '.jdk'),\n        get_buildroot())\n\n      # Since this code can be run in multi-threading mode due to multiple\n      # zinc workers, we need to make sure the file operations below is atomic.\n      with self._lock:\n        # Create the symlink if it does not exist\n        if not os.path.exists(jdk_home_symlink):\n          os.symlink(underlying_dist.home, jdk_home_symlink)\n        # Recreate if the symlink exists but does not match `underlying_dist.home`.\n        elif os.readlink(jdk_home_symlink) != underlying_dist.home:\n          os.remove(jdk_home_symlink)\n          os.symlink(underlying_dist.home, jdk_home_symlink)\n\n      return Distribution(home_path=jdk_home_symlink)\n    else:\n      return underlying_dist", "response": "Returns the distribution selected for Zinc based on execution strategy."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _compiler_bridge_cache_dir(self):\n    hasher = sha1()\n    for cp_entry in [self.zinc, self.compiler_interface, self.compiler_bridge]:\n      hasher.update(os.path.relpath(cp_entry, self._workdir()).encode('utf-8'))\n    key = hasher.hexdigest()[:12]\n\n    return os.path.join(self._workdir(), 'zinc', 'compiler-bridge', key)", "response": "A directory where we can store compiled copies of the compiler - bridge."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncompiles the compiler bridge jar to be used by zinc using our scala bootstrapper.", "response": "def compile_compiler_bridge(self, context):\n    \"\"\"Compile the compiler bridge to be used by zinc, using our scala bootstrapper.\n    It will compile and cache the jar, and materialize it if not already there.\n\n    :param context: The context of the task trying to compile the bridge.\n                    This is mostly needed to use its scheduler to create digests of the relevant jars.\n    :return: The absolute path to the compiled scala-compiler-bridge jar.\n    \"\"\"\n    bridge_jar_name = 'scala-compiler-bridge.jar'\n    bridge_jar = os.path.join(self._compiler_bridge_cache_dir, bridge_jar_name)\n    global_bridge_cache_dir = os.path.join(self._zinc_factory.get_options().pants_bootstrapdir, fast_relpath(self._compiler_bridge_cache_dir,  self._workdir()))\n    globally_cached_bridge_jar = os.path.join(global_bridge_cache_dir, bridge_jar_name)\n\n    # Workaround to avoid recompiling the bridge for every integration test\n    # We check the bootstrapdir (.cache) for the bridge.\n    # If it exists, we make a copy to the buildroot.\n    #\n    # TODO Remove when action caches are implemented.\n    if os.path.exists(globally_cached_bridge_jar):\n      # Cache the bridge jar under buildroot, to allow snapshotting\n      safe_mkdir(self._relative_to_buildroot(self._compiler_bridge_cache_dir))\n      safe_hardlink_or_copy(globally_cached_bridge_jar, bridge_jar)\n\n    if not os.path.exists(bridge_jar):\n      res = self._run_bootstrapper(bridge_jar, context)\n      context._scheduler.materialize_directories((\n        DirectoryToMaterialize(get_buildroot(), res.output_directory_digest),\n      ))\n      # For the workaround above to work, we need to store a copy of the bridge in\n      # the bootstrapdir cache (.cache).\n      safe_mkdir(global_bridge_cache_dir)\n      safe_hardlink_or_copy(bridge_jar, globally_cached_bridge_jar)\n\n      return ClasspathEntry(bridge_jar, res.output_directory_digest)\n    else:\n      bridge_jar_snapshot = context._scheduler.capture_snapshots((PathGlobsAndRoot(\n        PathGlobs((self._relative_to_buildroot(bridge_jar),)),\n        text_type(get_buildroot())\n      ),))[0]\n      bridge_jar_digest = bridge_jar_snapshot.directory_digest\n      return ClasspathEntry(bridge_jar, bridge_jar_digest)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compiler_plugins_cp_entries(self):\n    java_options_src = Java.global_instance()\n    scala_options_src = ScalaPlatform.global_instance()\n\n    def cp(instance, toolname):\n      scope = instance.options_scope\n      return instance.tool_classpath_from_products(self._products, toolname, scope=scope)\n    classpaths = (cp(java_options_src, 'javac-plugin-dep') +\n                  cp(scala_options_src, 'scalac-plugin-dep'))\n    return [(conf, ClasspathEntry(jar)) for conf in self.DEFAULT_CONFS for jar in classpaths]", "response": "Any additional global compiletime classpath entries for compiler plugins."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compile_classpath(self, classpath_product_key, target, extra_cp_entries=None):\n\n    classpath_entries = list(\n      entry.path\n        for entry in self.compile_classpath_entries(classpath_product_key, target, extra_cp_entries)\n    )\n\n    # Verify that all classpath entries are under the build root.\n    for entry in classpath_entries:\n      assert entry.startswith(get_buildroot()), \\\n             \"Classpath entry does not start with buildroot: {}\".format(entry)\n\n    return classpath_entries", "response": "Compute the compile classpath for the given target."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a sequence of all the Subsystems used outside of any task.", "response": "def get(cls):\n    \"\"\"Subsystems used outside of any task.\"\"\"\n    return {\n      SourceRootConfig,\n      Reporting,\n      Reproducer,\n      RunTracker,\n      Changed,\n      BinaryUtil.Factory,\n      Subprocess.Factory\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the named template embedded in our package.", "response": "def _get_template_text_from_package(self, template_name):\n    \"\"\"Load the named template embedded in our package.\"\"\"\n    if self._package_name is None:\n      raise self.MustacheError('No package specified for template loading.')\n\n    path = os.path.join('templates', template_name + '.mustache')\n    template_text = pkgutil.get_data(self._package_name, path)\n    if template_text is None:\n      raise self.MustacheError(\n        'could not find template {} in package {}'.format(path, self._package_name))\n    return template_text.decode('utf8')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_starting_indent(source):\n  if source.startswith(\" \"):\n    return sum(1 for _ in itertools.takewhile(lambda c: c in {' ', b' '}, source))\n  return 0", "response": "Remove leading indentation from source so ast. parse doesn t raise an exception."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _make_rule(output_type, input_selectors, cacheable=True):\n\n  is_goal_cls = isinstance(output_type, type) and issubclass(output_type, Goal)\n  if is_goal_cls == cacheable:\n    raise TypeError('An `@rule` that produces a `Goal` must be declared with @console_rule in order '\n                    'to signal that it is not cacheable.')\n\n  def wrapper(func):\n    if not inspect.isfunction(func):\n      raise ValueError('The @rule decorator must be applied innermost of all decorators.')\n\n    owning_module = sys.modules[func.__module__]\n    source = inspect.getsource(func)\n    beginning_indent = _get_starting_indent(source)\n    if beginning_indent:\n      source = \"\\n\".join(line[beginning_indent:] for line in source.split(\"\\n\"))\n    module_ast = ast.parse(source)\n\n    def resolve_type(name):\n      resolved = getattr(owning_module, name, None) or owning_module.__builtins__.get(name, None)\n      if resolved is None:\n        raise ValueError('Could not resolve type `{}` in top level of module {}'\n                         .format(name, owning_module.__name__))\n      elif not isinstance(resolved, type):\n        raise ValueError('Expected a `type` constructor for `{}`, but got: {} (type `{}`)'\n                         .format(name, resolved, type(resolved).__name__))\n      return resolved\n\n    gets = OrderedSet()\n    rule_func_node = assert_single_element(\n      node for node in ast.iter_child_nodes(module_ast)\n      if isinstance(node, ast.FunctionDef) and node.name == func.__name__)\n\n    parents_table = {}\n    for parent in ast.walk(rule_func_node):\n      for child in ast.iter_child_nodes(parent):\n        parents_table[child] = parent\n\n    rule_visitor = _RuleVisitor(\n      func=func,\n      func_node=rule_func_node,\n      func_source=source,\n      orig_indent=beginning_indent,\n      parents_table=parents_table,\n    )\n    rule_visitor.visit(rule_func_node)\n    gets.update(\n      Get.create_statically_for_rule_graph(resolve_type(p), resolve_type(s))\n      for p, s in rule_visitor.gets)\n\n    # Register dependencies for @console_rule/Goal.\n    if is_goal_cls:\n      dependency_rules = (optionable_rule(output_type.Options),)\n    else:\n      dependency_rules = None\n\n    func.rule = TaskRule(\n        output_type,\n        tuple(input_selectors),\n        func,\n        input_gets=tuple(gets),\n        dependency_rules=dependency_rules,\n        cacheable=cacheable,\n      )\n\n    return func\n  return wrapper", "response": "A decorator that creates a TaskRule from a function."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetermines if a statement is at the end of a list of statements.", "response": "def _stmt_is_at_end_of_parent_list(self, stmt):\n    \"\"\"Determine if `stmt` is at the end of a list of statements (i.e. can be an implicit `return`).\n\n    If there are any statements following `stmt` at the same level of nesting, this method returns\n    False, such as the following (if `stmt` is the Expr for `yield 'good'`):\n\n    if 2 + 2 == 5:\n      yield 'good'\n      a = 3\n\n    Note that this returns False even if the statement following `stmt` is a `return`.\n\n    However, if `stmt` is at the end of a list of statements, it can be made more clear that `stmt`\n    is intended to represent a `return`. Another way to view this method is as a dead code\n    elimination check, for a `stmt` which is intended to represent control flow moving out of the\n    current @rule. For example, this method would return True for both of the yield Expr statements\n    in the below snippet.\n\n    if True:\n      yield 3\n    else:\n      a = 3\n      yield a\n\n    This checking is performed by getting the parent of `stmt` with a pre-generated table passed\n    into the constructor.\n\n    See https://docs.python.org/2/library/ast.html#abstract-grammar for the grammar specification.\n    'body', 'orelse', and 'finalbody' are the only attributes on any AST nodes which can contain\n    lists of stmts.  'body' is also an attribute in the Exec statement for some reason, but as a\n    single expr, so we simply check if it is iterable in `_maybe_end_of_stmt_list()`.\n    \"\"\"\n    parent_stmt = self._parents_table[stmt]\n    last_body_stmt = self._maybe_end_of_stmt_list(getattr(parent_stmt, 'body', None))\n    if stmt == last_body_stmt:\n      return True\n    last_orelse_stmt = self._maybe_end_of_stmt_list(getattr(parent_stmt, 'orelse', None))\n    if stmt == last_orelse_stmt:\n      return True\n    last_finally_stmt = self._maybe_end_of_stmt_list(getattr(parent_stmt, 'finalbody', None))\n    if stmt == last_finally_stmt:\n      return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create(cls, rule_entries, union_rules=None):\n    serializable_rules = OrderedDict()\n    serializable_roots = OrderedSet()\n    union_rules = OrderedDict(union_rules or ())\n\n    def add_task(product_type, rule):\n      # TODO(#7311): make a defaultdict-like wrapper for OrderedDict if more widely used.\n      if product_type not in serializable_rules:\n        serializable_rules[product_type] = OrderedSet()\n      serializable_rules[product_type].add(rule)\n\n    def add_root_rule(root_rule):\n      serializable_roots.add(root_rule)\n\n    def add_rule(rule):\n      if isinstance(rule, RootRule):\n        add_root_rule(rule)\n      else:\n        add_task(rule.output_type, rule)\n      for dep_rule in rule.dependency_rules:\n        add_rule(dep_rule)\n\n    def add_type_transition_rule(union_rule):\n      # NB: This does not require that union bases be supplied to `def rules():`, as the union type\n      # is never instantiated!\n      union_base = union_rule.union_base\n      assert union_base._is_union\n      union_member = union_rule.union_member\n      if union_base not in union_rules:\n        union_rules[union_base] = OrderedSet()\n      union_rules[union_base].add(union_member)\n\n    for entry in rule_entries:\n      if isinstance(entry, Rule):\n        add_rule(entry)\n      elif isinstance(entry, UnionRule):\n        add_type_transition_rule(entry)\n      elif hasattr(entry, '__call__'):\n        rule = getattr(entry, 'rule', None)\n        if rule is None:\n          raise TypeError(\"Expected callable {} to be decorated with @rule.\".format(entry))\n        add_rule(rule)\n      else:\n        raise TypeError(\"\"\"\\\nRule entry {} had an unexpected type: {}. Rules either extend Rule or UnionRule, or are static \\\nfunctions decorated with @rule.\"\"\".format(entry, type(entry)))\n\n    return cls(serializable_rules, serializable_roots, union_rules)", "response": "Creates a RuleIndex with tasks indexed by their output type."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nassert that the path components which are joined into a glob and that exactly one path is found.", "response": "def assert_single_path_by_glob(self, components):\n    \"\"\"Assert that the path components (which are joined into a glob) match exactly one path.\n\n    The matched path may be a file or a directory. This method is used to avoid having to guess\n    platform-specific intermediate directory names, e.g. 'x86_64-linux-gnu' or\n    'x86_64-apple-darwin17.5.0'.\n    \"\"\"\n    glob_path_string = os.path.join(*components)\n    expanded_glob = glob.glob(glob_path_string)\n\n    try:\n      return assert_single_element(expanded_glob)\n    except StopIteration as e:\n      raise self.ArchiveFileMappingError(\n        \"No elements for glob '{}' -- expected exactly one.\"\n        .format(glob_path_string),\n        e)\n    except ValueError as e:\n      raise self.ArchiveFileMappingError(\n        \"Should have exactly one path matching expansion of glob '{}'.\"\n        .format(glob_path_string),\n        e)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply assert_single_path_by_glob to all elements of all_components_list and return a list of matched paths.", "response": "def map_files(self, base_dir, all_components_list):\n    \"\"\"Apply `assert_single_path_by_glob()` to all elements of `all_components_list`.\n\n    Each element of `all_components_list` should be a tuple of path components, including\n    wildcards. The elements of each tuple are joined, and interpreted as a glob expression relative\n    to `base_dir`. The resulting glob should match exactly one path.\n\n    :return: List of matched paths, one per element of `all_components_list`.\n    :raises: :class:`ArchiveFileMapper.ArchiveFileMappingError` if more or less than one path was\n             matched by one of the glob expressions interpreted from `all_components_list`.\n    \"\"\"\n    mapped_paths = []\n    for components_tupled in all_components_list:\n      with_base = [base_dir] + list(components_tupled)\n      # Results are known to exist, since they match a glob.\n      mapped_paths.append(self.assert_single_path_by_glob(with_base))\n\n    return mapped_paths"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_java_executor(self, dist=None):\n    dist = dist or self.dist\n    if self.execution_strategy == self.NAILGUN:\n      classpath = os.pathsep.join(self.tool_classpath('nailgun-server'))\n      return NailgunExecutor(self._identity,\n                             self._executor_workdir,\n                             classpath,\n                             dist,\n                             startup_timeout=self.get_options().nailgun_subprocess_startup_timeout,\n                             connect_timeout=self.get_options().nailgun_timeout_seconds,\n                             connect_attempts=self.get_options().nailgun_connect_attempts)\n    else:\n      return SubprocessExecutor(dist)", "response": "Create java executor that uses this task s ng daemon if allowed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef runjava(self, classpath, main, jvm_options=None, args=None, workunit_name=None,\n              workunit_labels=None, workunit_log_config=None, dist=None):\n    \"\"\"Runs the java main using the given classpath and args.\n\n    If --execution-strategy=subprocess is specified then the java main is run in a freshly spawned\n    subprocess, otherwise a persistent nailgun server dedicated to this Task subclass is used to\n    speed up amortized run times.\n\n    :API: public\n    \"\"\"\n    executor = self.create_java_executor(dist=dist)\n\n    # Creating synthetic jar to work around system arg length limit is not necessary\n    # when `NailgunExecutor` is used because args are passed through socket, therefore turning off\n    # creating synthetic jar if nailgun is used.\n    create_synthetic_jar = self.execution_strategy != self.NAILGUN\n    try:\n      return util.execute_java(classpath=classpath,\n                               main=main,\n                               jvm_options=jvm_options,\n                               args=args,\n                               executor=executor,\n                               workunit_factory=self.context.new_workunit,\n                               workunit_name=workunit_name,\n                               workunit_labels=workunit_labels,\n                               workunit_log_config=workunit_log_config,\n                               create_synthetic_jar=create_synthetic_jar,\n                               synthetic_jar_dir=self._executor_workdir)\n    except executor.Error as e:\n      raise TaskError(e)", "response": "Runs the java main using the given classpath and args."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a filename from a name and extension ensuring that the final length is within the maximum length constraint.", "response": "def safe_filename(name, extension=None, digest=None, max_length=_MAX_FILENAME_LENGTH):\n  \"\"\"Creates filename from name and extension ensuring that the final length is within the\n  max_length constraint.\n\n  By default the length is capped to work on most filesystems and the fallback to achieve\n  shortening is a sha1 hash of the proposed name.\n\n  Raises ValueError if the proposed name is not a simple filename but a file path.\n  Also raises ValueError when the name is simple but cannot be satisfactorily shortened with the\n  given digest.\n\n  :API: public\n\n  name:       the proposed filename without extension\n  extension:  an optional extension to append to the filename\n  digest:     the digest to fall back on for too-long name, extension concatenations - should\n              support the hashlib digest api of update(string) and hexdigest\n  max_length: the maximum desired file name length\n  \"\"\"\n  if os.path.basename(name) != name:\n    raise ValueError('Name must be a filename, handed a path: {}'.format(name))\n\n  ext = extension or ''\n  filename = name + ext\n  if len(filename) <= max_length:\n    return filename\n  else:\n    digest = digest or hashlib.sha1()\n    digest.update(filename.encode('utf-8'))\n    hexdigest = digest.hexdigest()[:16]\n\n    # Prefix and suffix length: max length less 2 periods, the extension length, and the digest length.\n    ps_len = max(0, (max_length - (2 + len(ext) + len(hexdigest))) // 2)\n    sep = '.' if ps_len > 0 else ''\n    prefix = name[:ps_len]\n    suffix = name[-ps_len:] if ps_len > 0 else ''\n\n    safe_name = '{}{}{}{}{}{}'.format(prefix, sep, hexdigest, sep, suffix, ext)\n    if len(safe_name) > max_length:\n      raise ValueError('Digest {} failed to produce a filename <= {} '\n                       'characters for {} - got {}'.format(digest, max_length, filename, safe_name))\n    return safe_name"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns path as an absolute path with ~user and env var expansion applied.", "response": "def expand_path(path):\n  \"\"\"Returns ``path`` as an absolute path with ~user and env var expansion applied.\n\n  :API: public\n  \"\"\"\n  return os.path.abspath(os.path.expandvars(os.path.expanduser(path)))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _trapped_signals(self, client):\n    signal_handler = PailgunClientSignalHandler(\n      client,\n      timeout=self._bootstrap_options.for_global_scope().pantsd_pailgun_quit_timeout)\n    with ExceptionSink.trapped_signals(signal_handler):\n      yield", "response": "A contextmanager that handles SIGINT and SIGQUIT remotely."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _run_pants_with_retry(self, pantsd_handle, retries=3):\n    attempt = 1\n    while 1:\n      logger.debug(\n        'connecting to pantsd on port {} (attempt {}/{})'\n        .format(pantsd_handle.port, attempt, retries)\n      )\n      try:\n        return self._connect_and_execute(pantsd_handle)\n      except self.RECOVERABLE_EXCEPTIONS as e:\n        if attempt > retries:\n          raise self.Fallback(e)\n\n        self._backoff(attempt)\n        logger.warn(\n          'pantsd was unresponsive on port {}, retrying ({}/{})'\n          .format(pantsd_handle.port, attempt, retries)\n        )\n\n        # One possible cause of the daemon being non-responsive during an attempt might be if a\n        # another lifecycle operation is happening concurrently (incl teardown). To account for\n        # this, we won't begin attempting restarts until at least 1 second has passed (1 attempt).\n        if attempt > 1:\n          pantsd_handle = self._restart_pantsd()\n        attempt += 1\n      except NailgunClient.NailgunError as e:\n        # Ensure a newline.\n        logger.fatal('')\n        logger.fatal('lost active connection to pantsd!')\n        raise_with_traceback(self._extract_remote_exception(pantsd_handle.pid, e))", "response": "Runs pants remotely with retry and recovery for nascent executions."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _extract_remote_exception(self, pantsd_pid, nailgun_error):\n    sources = [pantsd_pid]\n    if nailgun_error.pid is not None:\n      sources = [abs(nailgun_error.pid)] + sources\n\n    exception_text = None\n    for source in sources:\n      log_path = ExceptionSink.exceptions_log_path(for_pid=source)\n      exception_text = maybe_read_file(log_path)\n      if exception_text:\n        break\n\n    exception_suffix = '\\nRemote exception:\\n{}'.format(exception_text) if exception_text else ''\n    return self.Terminated('abruptly lost active connection to pantsd runner: {!r}{}'.format(\n      nailgun_error, exception_suffix))", "response": "Given a NailgunError returns a Terminated exception with additional info ( where possible."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting ivy to use an http proxy", "response": "def http_proxy(self):\n    \"\"\"Set ivy to use an http proxy.\n\n    Expects a string of the form http://<host>:<port>\n    \"\"\"\n    if os.getenv('HTTP_PROXY'):\n      return os.getenv('HTTP_PROXY')\n    if os.getenv('http_proxy'):\n      return os.getenv('http_proxy')\n    return self.get_options().http_proxy"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef https_proxy(self):\n    if os.getenv('HTTPS_PROXY'):\n      return os.getenv('HTTPS_PROXY')\n    if os.getenv('https_proxy'):\n      return os.getenv('https_proxy')\n    return self.get_options().https_proxy", "response": "Set ivy to use an https proxy."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_imported_repo(self, import_path):\n    try:\n      session = requests.session()\n      # TODO: Support https with (optional) fallback to http, as Go does.\n      # See https://github.com/pantsbuild/pants/issues/3503.\n      session.mount(\"http://\",\n                    requests.adapters.HTTPAdapter(max_retries=self.get_options().retries))\n      page_data = session.get('http://{import_path}?go-get=1'.format(import_path=import_path))\n    except requests.ConnectionError:\n      return None\n\n    if not page_data:\n      return None\n\n    # Return the first match, rather than doing some kind of longest prefix search.\n    # Hopefully no one returns multiple valid go-import meta tags.\n    for (root, vcs, url) in self.find_meta_tags(page_data.text):\n      if root and vcs and url:\n        # Check to make sure returned root is an exact match to the provided import path. If it is\n        # not then run a recursive check on the returned and return the values provided by that call.\n        if root == import_path:\n          return ImportedRepo(root, vcs, url)\n        elif import_path.startswith(root):\n          return self.get_imported_repo(root)\n\n    return None", "response": "Returns an ImportedRepo instance with the information in the meta tag for the provided import_path."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _target_addressable_to_target(self, address, addressable):\n    try:\n      # TODO(John Sirois): Today - in practice, Addressable is unusable.  BuildGraph assumes\n      # addressables are in fact TargetAddressables with dependencies (see:\n      # `inject_address_closure` for example), ie: leaf nameable things with - by definition - no\n      # deps cannot actually be used.  Clean up BuildGraph to handle addressables as they are\n      # abstracted today which does not necessarily mean them having dependencies and thus forming\n      # graphs.  They may only be multiply-referred to leaf objects.\n      target = addressable.instantiate(build_graph=self, address=address)\n      return target\n    except Exception:\n      traceback.print_exc()\n      logger.exception('Failed to instantiate Target with type {target_type} with name \"{name}\"'\n                       ' at address {address}'\n                       .format(target_type=addressable.addressed_type,\n                               name=addressable.addressed_name,\n                               address=address))\n      raise", "response": "Realizes a TargetAddressable into a Target at address."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a tuple of dependency_id is_internal_dep.", "response": "def _dep_id(self, dependency):\n    \"\"\"Returns a tuple of dependency_id, is_internal_dep.\"\"\"\n    params = dict(sep=self.separator)\n\n    if isinstance(dependency, JarDependency):\n      # TODO(kwilson): handle 'classifier' and 'type'.\n      params.update(org=dependency.org, name=dependency.name, rev=dependency.rev)\n      is_internal_dep = False\n    else:\n      params.update(org='internal', name=dependency.id)\n      is_internal_dep = True\n\n    return ('{org}{sep}{name}{sep}{rev}' if params.get('rev') else\n            '{org}{sep}{name}').format(**params), is_internal_dep"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _output_dependency_tree(self, target):\n\n    def make_line(dep, indent, is_dupe=False):\n      indent_join, indent_chars = ('--', '  |') if self.should_tree else ('', '  ')\n      dupe_char = '*' if is_dupe else ''\n      return ''.join((indent * indent_chars, indent_join, dupe_char, dep))\n\n    def output_deps(dep, indent, outputted, stack):\n      dep_id, internal = self._dep_id(dep)\n\n      if self.is_minimal and dep_id in outputted:\n        return\n\n      if self.output_candidate(internal):\n        yield make_line(dep_id,\n                        0 if self.is_external_only else indent,\n                        is_dupe=dep_id in outputted)\n        outputted.add(dep_id)\n\n      for sub_dep in self._enumerate_visible_deps(dep, self.output_candidate):\n        for item in output_deps(sub_dep, indent + 1, outputted, stack + [(dep_id, indent)]):\n          yield item\n\n    for item in output_deps(target, 0, set(), []):\n      yield item", "response": "Plain-text depmap output handler."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\noutputting a graphviz format depmap output handler.", "response": "def _output_digraph(self, target):\n    \"\"\"Graphviz format depmap output handler.\"\"\"\n    color_by_type = {}\n\n    def maybe_add_type(dep, dep_id):\n      \"\"\"Add a class type to a dependency id if --show-types is passed.\"\"\"\n      return dep_id if not self.show_types else '\\\\n'.join((dep_id, dep.__class__.__name__))\n\n    def make_node(dep, dep_id, internal):\n      line_fmt = '  \"{id}\" [style=filled, fillcolor={color}{internal}];'\n      int_shape = ', shape=ellipse' if not internal else ''\n\n      dep_class = dep.__class__.__name__\n      if dep_class not in color_by_type:\n        color_by_type[dep_class] = len(color_by_type.keys()) + 1\n\n      return line_fmt.format(id=dep_id, internal=int_shape, color=color_by_type[dep_class])\n\n    def make_edge(from_dep_id, to_dep_id, internal):\n      style = ' [style=dashed]' if not internal else ''\n      return '  \"{}\" -> \"{}\"{};'.format(from_dep_id, to_dep_id, style)\n\n    def output_deps(dep, parent, parent_id, outputted):\n      dep_id, internal = self._dep_id(dep)\n\n      if dep_id not in outputted:\n        yield make_node(dep, maybe_add_type(dep, dep_id), internal)\n        outputted.add(dep_id)\n\n        for sub_dep in self._enumerate_visible_deps(dep, self.output_candidate):\n          for item in output_deps(sub_dep, dep, dep_id, outputted):\n            yield item\n\n      if parent:\n        edge_id = (parent_id, dep_id)\n        if edge_id not in outputted:\n          yield make_edge(maybe_add_type(parent, parent_id), maybe_add_type(dep, dep_id), internal)\n          outputted.add(edge_id)\n\n    yield 'digraph \"{}\" {{'.format(target.id)\n    yield '  node [shape=rectangle, colorscheme=set312;];'\n    yield '  rankdir=LR;'\n    for line in output_deps(target, parent=None, parent_id=None, outputted=set()):\n      yield line\n    yield '}'"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _algebraic_data(metaclass):\n  def wrapper(cls):\n    cls.__bases__ += (metaclass,)\n    cls._list_fields = metaclass._list_fields\n    return cls\n  return wrapper", "response": "A class decorator to pull out _list_fields from a mixin class for use with a datatype."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prepend_field(self, field_name, list_value):\n    return self._single_list_field_operation(field_name, list_value, prepend=True)", "response": "Returns a copy of this object with list_value prepended to the field named field_name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a copy of this object with list_value appended to the field named field_name.", "response": "def append_field(self, field_name, list_value):\n    \"\"\"Return a copy of this object with `list_value` appended to the field named `field_name`.\"\"\"\n    return self._single_list_field_operation(field_name, list_value, prepend=False)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a copy of this object which combines all the fields common to both self and other.", "response": "def sequence(self, other, exclude_list_fields=None):\n    \"\"\"Return a copy of this object which combines all the fields common to both `self` and `other`.\n\n    List fields will be concatenated.\n\n    The return type of this method is the type of `self` (or whatever `.copy()` returns), but the\n    `other` argument can be any `_ExtensibleAlgebraic` instance.\n    \"\"\"\n    exclude_list_fields = frozenset(exclude_list_fields or [])\n    overwrite_kwargs = {}\n\n    nonexistent_excluded_fields = exclude_list_fields - self._list_fields\n    if nonexistent_excluded_fields:\n      raise self.AlgebraicDataError(\n        \"Fields {} to exclude from a sequence() were not found in this object's list fields: {}. \"\n        \"This object is {}, the other object is {}.\"\n        .format(nonexistent_excluded_fields, self._list_fields, self, other))\n\n    shared_list_fields = (self._list_fields\n                          & other._list_fields\n                          - exclude_list_fields)\n    if not shared_list_fields:\n      raise self.AlgebraicDataError(\n        \"Objects to sequence have no shared fields after excluding {}. \"\n        \"This object is {}, with list fields: {}. \"\n        \"The other object is {}, with list fields: {}.\"\n        .format(exclude_list_fields, self, self._list_fields, other, other._list_fields))\n\n    for list_field_name in shared_list_fields:\n      lhs_value = getattr(self, list_field_name)\n      rhs_value = getattr(other, list_field_name)\n      overwrite_kwargs[list_field_name] = lhs_value + rhs_value\n\n    return self.copy(**overwrite_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_host_libc_from_host_compiler(self):\n    compiler_exe = self.get_options().host_compiler\n\n    # Implicitly, we are passing in the environment of the executing pants process to\n    # `get_compiler_library_dirs()`.\n    # These directories are checked to exist!\n    library_dirs = self._parse_search_dirs.get_compiler_library_dirs(compiler_exe)\n\n    libc_crti_object_file = None\n    for libc_dir_candidate in library_dirs:\n      maybe_libc_crti = os.path.join(libc_dir_candidate, self._LIBC_INIT_OBJECT_FILE)\n      if os.path.isfile(maybe_libc_crti):\n        libc_crti_object_file = maybe_libc_crti\n        break\n\n    if not libc_crti_object_file:\n      raise self.HostLibcDevResolutionError(\n        \"Could not locate {fname} in library search dirs {dirs} from compiler: {compiler!r}. \"\n        \"You may need to install a libc dev package for the current system. \"\n        \"For many operating systems, this package is named 'libc-dev' or 'libc6-dev'.\"\n        .format(fname=self._LIBC_INIT_OBJECT_FILE, dirs=library_dirs, compiler=compiler_exe))\n\n    return HostLibcDev(crti_object=libc_crti_object_file,\n                       fingerprint=hash_file(libc_crti_object_file))", "response": "Locate the host s libc - dev installation using a specified host compiler s search dirs."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuses the libc - dir option if provided otherwise invoke a host compiler to find libc dev.", "response": "def _host_libc(self):\n    \"\"\"Use the --libc-dir option if provided, otherwise invoke a host compiler to find libc dev.\"\"\"\n    libc_dir_option = self.get_options().libc_dir\n    if libc_dir_option:\n      maybe_libc_crti = os.path.join(libc_dir_option, self._LIBC_INIT_OBJECT_FILE)\n      if os.path.isfile(maybe_libc_crti):\n        return HostLibcDev(crti_object=maybe_libc_crti,\n                           fingerprint=hash_file(maybe_libc_crti))\n      raise self.HostLibcDevResolutionError(\n        \"Could not locate {} in directory {} provided by the --libc-dir option.\"\n        .format(self._LIBC_INIT_OBJECT_FILE, libc_dir_option))\n\n    return self._get_host_libc_from_host_compiler()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create(cls, bootstrap_options):\n    binary_tool_fetcher = BinaryToolFetcher(\n      bootstrap_dir=bootstrap_options.pants_bootstrapdir,\n      timeout_secs=bootstrap_options.binaries_fetch_timeout_secs)\n    binary_util = BinaryUtil(\n      baseurls=bootstrap_options.binaries_baseurls,\n      binary_tool_fetcher=binary_tool_fetcher,\n      path_by_id=bootstrap_options.binaries_path_by_id,\n      allow_external_binary_tool_downloads=bootstrap_options.allow_external_binary_tool_downloads)\n\n    return WatchmanLauncher(\n      binary_util,\n      bootstrap_options.level,\n      bootstrap_options.watchman_version,\n      bootstrap_options.watchman_supportdir,\n      bootstrap_options.watchman_startup_timeout,\n      bootstrap_options.watchman_socket_timeout,\n      bootstrap_options.watchman_socket_path,\n      bootstrap_options.pants_subprocessdir\n    )", "response": "Creates a new watchman launcher."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun the tool on all source files that are located.", "response": "def execute(self):\n    \"\"\"Runs the tool on all source files that are located.\"\"\"\n    relevant_targets = self._get_non_synthetic_targets(self.get_targets())\n\n    if self.sideeffecting:\n      # Always execute sideeffecting tasks without invalidation.\n      self._execute_for(relevant_targets)\n    else:\n      # If the task is not sideeffecting we can use invalidation.\n      with self.invalidated(relevant_targets) as invalidation_check:\n        self._execute_for([vt.target for vt in invalidation_check.invalid_vts])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _install_node(self):\n    node_package_path = self.select()\n    # Todo: https://github.com/pantsbuild/pants/issues/4431\n    # This line depends on repacked node distribution.\n    # Should change it from 'node/bin' to 'dist/bin'\n    node_bin_path = os.path.join(node_package_path, 'node', 'bin')\n    if not is_readable_dir(node_bin_path):\n      # The binary was pulled from nodejs and not our S3, in which\n      # case it's installed under a different directory.\n      return os.path.join(node_package_path, os.listdir(node_package_path)[0], 'bin')\n    return node_bin_path", "response": "Install the Node distribution from pants support binaries."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _install_yarnpkg(self):\n    yarnpkg_package_path = YarnpkgDistribution.scoped_instance(self).select()\n    yarnpkg_bin_path = os.path.join(yarnpkg_package_path, 'dist', 'bin')\n    return yarnpkg_bin_path", "response": "Install the Yarnpkg distribution from pants support binaries."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef node_command(self, args=None, node_paths=None):\n    # NB: We explicitly allow no args for the `node` command unlike the `npm` command since running\n    # `node` with no arguments is useful, it launches a REPL.\n    return command_gen([self._install_node], 'node', args=args, node_paths=node_paths)", "response": "Creates a command that can run node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef eslint_supportdir(self, task_workdir):\n    bootstrapped_support_path = os.path.join(task_workdir, 'eslint')\n\n    # TODO(nsaechao): Should only have to check if the \"eslint\" dir exists in the task_workdir\n    # assuming fingerprinting works as intended.\n\n    # If the eslint_setupdir is not provided or missing required files, then\n    # clean up the directory so that Pants can install a pre-defined eslint version later on.\n    # Otherwise, if there is no configurations changes, rely on the cache.\n    # If there is a config change detected, use the new configuration.\n    if self.eslint_setupdir:\n      configured = all(os.path.exists(os.path.join(self.eslint_setupdir, f))\n                       for f in self._eslint_required_files)\n    else:\n      configured = False\n    if not configured:\n      safe_mkdir(bootstrapped_support_path, clean=True)\n    else:\n      try:\n        installed = all(filecmp.cmp(\n          os.path.join(self.eslint_setupdir, f), os.path.join(bootstrapped_support_path, f))\n        for f in self._eslint_required_files)\n      except OSError:\n        installed = False\n\n      if not installed:\n        self._configure_eslinter(bootstrapped_support_path)\n    return bootstrapped_support_path, configured", "response": "Returns the path where the ESLint is bootstrapped and whether or not it is configured."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new a Req object from a list of packages.", "response": "def from_packages(cls, parse_context, rev='', packages=None, **kwargs):\n    \"\"\"\n    :param list packages: The package import paths within the remote library; by default just the\n                          root package will be available (equivalent to passing `packages=['']`).\n    :param string rev: Identifies which version of the remote library to download. This could be a\n                       commit SHA (git), node id (hg), etc.  If left unspecified the version will\n                       default to the latest available.  It's highly recommended to not accept the\n                       default and instead pin the rev explicitly for repeatable builds.\n    \"\"\"\n    for pkg in packages or ('',):\n      cls.from_package(parse_context, pkg=pkg, rev=rev, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_package(cls, parse_context, pkg='', rev='', **kwargs):\n    if 'name' in kwargs:\n      raise TargetDefinitionException(Address(parse_context.rel_path, kwargs['name']),\n                                      'A go_remote_library does not accept a name; instead, '\n                                      'the name is taken from the `pkg` or the BUILD file location '\n                                      'if no package is specified.')\n    name = pkg or os.path.basename(parse_context.rel_path)\n\n    # We trap for dependencies in the macro since dependencies are special and are popped from\n    # **kwargs by the time the constructor below is called by the BUILD file parsing infra.\n    if 'dependencies' in kwargs:\n      raise TargetDefinitionException(Address(parse_context.rel_path, name),\n                                      'A go_remote_library does not accept dependencies; instead, '\n                                      'they are discovered and when they are on foreign remote '\n                                      'libraries the versions are taken from other '\n                                      'go_remote_library targets you\\'ve defined in the same '\n                                      'source root.')\n    parse_context.create_object(cls, type_alias='go_remote_library', name=name, pkg=pkg, rev=rev,\n                                **kwargs)", "response": "Create a new object from a package."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remote_root(self):\n    return os.path.relpath(self.address.spec_path, self.target_base)", "response": "The remote package root prefix portion of the full import_path"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_path(self):\n    return os.path.join(self.remote_root, self.pkg) if self.pkg else self.remote_root", "response": "The full path to the import file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _dep_type(self, target, dep, declared_deps, eligible_unused_deps, is_used):\n    if target == dep:\n      return True, True\n    return (dep in declared_deps), (is_used or dep not in eligible_unused_deps)", "response": "Compute the type of a dependency."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a dependency usage graph of the given targets.", "response": "def create_dep_usage_graph(self, targets):\n    \"\"\"Creates a graph of concrete targets, with their sum of products and dependencies.\n\n    Synthetic targets contribute products and dependencies to their concrete target.\n    \"\"\"\n    with self.invalidated(targets,\n                          invalidate_dependents=True) as invalidation_check:\n      target_to_vts = {}\n      for vts in invalidation_check.all_vts:\n        target_to_vts[vts.target] = vts\n\n      if not self.get_options().use_cached:\n        node_creator = self.calculating_node_creator(\n          self.context.products.get_data('classes_by_source'),\n          self.context.products.get_data('runtime_classpath'),\n          self.context.products.get_data('product_deps_by_src'),\n          target_to_vts)\n      else:\n        node_creator = self.cached_node_creator(target_to_vts)\n\n      return DependencyUsageGraph(self.create_dep_usage_nodes(targets, node_creator),\n                                  self.size_estimators[self.get_options().size_estimator])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a function that creates a dependency graph node based on the given classes_by_source runtime_classpath product_deps_by_src and target_to_vts.", "response": "def calculating_node_creator(self, classes_by_source, runtime_classpath, product_deps_by_src,\n                               target_to_vts):\n    \"\"\"Strategy directly computes dependency graph node based on\n    `classes_by_source`, `runtime_classpath`, `product_deps_by_src` parameters and\n    stores the result to the build cache.\n    \"\"\"\n    analyzer = JvmDependencyAnalyzer(get_buildroot(), runtime_classpath)\n    targets = self.context.targets()\n    targets_by_file = analyzer.targets_by_file(targets)\n    transitive_deps_by_target = analyzer.compute_transitive_deps_by_target(targets)\n    def creator(target):\n      transitive_deps = set(transitive_deps_by_target.get(target))\n      node = self.create_dep_usage_node(target,\n                                        analyzer,\n                                        product_deps_by_src,\n                                        classes_by_source,\n                                        targets_by_file,\n                                        transitive_deps)\n      vt = target_to_vts[target]\n      mode = 'w' if PY3 else 'wb'\n      with open(self.nodes_json(vt.results_dir), mode=mode) as fp:\n        json.dump(node.to_cacheable_dict(), fp, indent=2, sort_keys=True)\n      vt.update()\n      return node\n\n    return creator"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cached_node_creator(self, target_to_vts):\n    def creator(target):\n      vt = target_to_vts[target]\n      if vt.valid and os.path.exists(self.nodes_json(vt.results_dir)):\n        try:\n          with open(self.nodes_json(vt.results_dir), 'r') as fp:\n            return Node.from_cacheable_dict(json.load(fp),\n                                            lambda spec: next(self.context.resolve(spec).__iter__()))\n        except Exception:\n          self.context.log.warn(\"Can't deserialize json for target {}\".format(target))\n          return Node(target.concrete_derived_from)\n      else:\n        self.context.log.warn(\"No cache entry for {}\".format(target))\n        return Node(target.concrete_derived_from)\n\n    return creator", "response": "Strategy creates a node from the build cache."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\noutputting summarized dependencies ordered by max usage and cost.", "response": "def to_summary(self):\n    \"\"\"Outputs summarized dependencies ordered by a combination of max usage and cost.\"\"\"\n\n    # Aggregate inbound edges by their maximum product usage ratio.\n    max_target_usage = defaultdict(lambda: 0.0)\n    for target, node in self._nodes.items():\n      for dep_target, edge in node.dep_edges.items():\n        if target == dep_target:\n          continue\n        used_ratio = self._used_ratio(dep_target, edge)\n        max_target_usage[dep_target] = max(max_target_usage[dep_target], used_ratio)\n\n    # Calculate a score for each.\n    Score = namedtuple('Score', ('badness', 'max_usage', 'cost_transitive', 'target'))\n    scores = []\n    for target, max_usage in max_target_usage.items():\n      cost_transitive = self._trans_cost(target)\n      score = int(max(cost_transitive, 1) / (max_usage if max_usage > 0.0 else 1.0))\n      scores.append(Score(score, max_usage, cost_transitive, target.address.spec))\n\n    # Output in order by score.\n    yield '[\\n'\n    first = True\n    for score in sorted(scores, key=lambda s: s.badness):\n      yield '{}  {}'.format('' if first else ',\\n', json.dumps(score._asdict()))\n      first = False\n    yield '\\n]\\n'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\noutputting the entire graph as a JSON string.", "response": "def to_json(self):\n    \"\"\"Outputs the entire graph.\"\"\"\n    res_dict = {}\n\n    def gen_dep_edge(node, edge, dep_tgt, aliases):\n      return {\n        'target': dep_tgt.address.spec,\n        'dependency_type': self._edge_type(node.concrete_target, edge, dep_tgt),\n        'products_used': len(edge.products_used),\n        'products_used_ratio': self._used_ratio(dep_tgt, edge),\n        'aliases': [alias.address.spec for alias in aliases],\n      }\n\n    for node in self._nodes.values():\n      res_dict[node.concrete_target.address.spec] = {\n        'cost': self._cost(node.concrete_target),\n        'cost_transitive': self._trans_cost(node.concrete_target),\n        'products_total': node.products_total,\n        'dependencies': [gen_dep_edge(node, edge, dep_tgt, node.dep_aliases.get(dep_tgt, {}))\n                         for dep_tgt, edge in node.dep_edges.items()],\n      }\n    yield str(json.dumps(res_dict, indent=2, sort_keys=True))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef choose_include_text(s, params, source_path):\n  lines = s.splitlines()\n  start_after = None\n  start_at = None\n  end_before = None\n  end_at = None\n\n  for term in params.split(\"&\"):\n    if '=' in term:\n      param, value = [p.strip() for p in term.split('=', 1)]\n    else:\n      param, value = term.strip(), ''\n    if not param: continue\n    if param == \"start-after\":\n      start_after = value\n    elif param == \"start-at\":\n      start_at = value\n    elif param == \"end-before\":\n      end_before = value\n    elif param == \"end-at\":\n      end_at = value\n    else:\n      raise TaskError('Invalid include directive \"{0}\"'\n                      ' in {1}'.format(params, source_path))\n\n  chosen_lines = []\n  # two loops, one waits to \"start recording\", one \"records\"\n  for line_ix in range(0, len(lines)):\n    line = lines[line_ix]\n    if (not start_at) and (not start_after):\n      # if we didn't set a start-* param, don't wait to start\n      break\n    if start_at is not None and start_at in line:\n      break\n    if start_after is not None and start_after in line:\n      line_ix += 1\n      break\n  else:\n    # never started recording:\n    return ''\n  for line_ix in range(line_ix, len(lines)):\n    line = lines[line_ix]\n    if end_before is not None and end_before in line:\n      break\n    chosen_lines.append(line)\n    if end_at is not None and end_at in line:\n      break\n  else:\n    if (end_before or end_at):\n      # we had an end- filter, but never encountered it.\n      return ''\n  return '\\n'.join(chosen_lines)", "response": "Given the contents of a file and a list of parameters return the lines that match the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrendering an HTML from an RST fragment.", "response": "def rst_to_html(in_rst, stderr):\n  \"\"\"Renders HTML from an RST fragment.\n\n  :param string in_rst: An rst formatted string.\n  :param stderr: An open stream to use for docutils stderr output.\n  :returns: A tuple of (html rendered rst, return code)\n  \"\"\"\n  if not in_rst:\n    return '', 0\n\n  # Unfortunately, docutils is really setup for command line use.\n  # We're forced to patch the bits of sys its hardcoded to use so that we can call it in-process\n  # and still reliably determine errors.\n  # TODO(John Sirois): Move to a subprocess execution model utilizing a docutil chroot/pex.\n  orig_sys_exit = sys.exit\n  orig_sys_stderr = sys.stderr\n  returncodes = []\n  try:\n    sys.exit = returncodes.append\n    sys.stderr = stderr\n    pp = publish_parts(in_rst,\n                       writer_name='html',\n                       # Report and exit at level 2 (warnings) or higher.\n                       settings_overrides=dict(exit_status_level=2, report_level=2),\n                       enable_exit_status=True)\n  finally:\n    sys.exit = orig_sys_exit\n    sys.stderr = orig_sys_stderr\n\n  return_value = ''\n  if 'title' in pp and pp['title']:\n    return_value += '<title>{0}</title>\\n<p style=\"font: 200% bold\">{0}</p>\\n'.format(pp['title'])\n  return_value += pp['body'].strip()\n  return return_value, returncodes.pop() if returncodes else 0"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef options_scope(self):\n    if self.is_global():\n      return self.subsystem_cls.options_scope\n    else:\n      return self.subsystem_cls.subscope(self.scope)", "response": "The scope that the options are read from when initializing the instance\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef subsystem_dependencies_iter(cls):\n    for dep in cls.subsystem_dependencies():\n      if isinstance(dep, SubsystemDependency):\n        yield dep\n      else:\n        yield SubsystemDependency(dep, GLOBAL_SCOPE, removal_version=None, removal_hint=None)", "response": "Iterate over the direct subsystem dependencies of this Optionable."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\niterates over the transitive closure of subsystem dependencies of this Optionable.", "response": "def subsystem_closure_iter(cls):\n    \"\"\"Iterate over the transitive closure of subsystem dependencies of this Optionable.\n\n    :rtype: :class:`collections.Iterator` of :class:`SubsystemDependency`\n    :raises: :class:`pants.subsystem.subsystem_client_mixin.SubsystemClientMixin.CycleException`\n             if a dependency cycle is detected.\n    \"\"\"\n    seen = set()\n    dep_path = OrderedSet()\n\n    def iter_subsystem_closure(subsystem_cls):\n      if subsystem_cls in dep_path:\n        raise cls.CycleException(list(dep_path) + [subsystem_cls])\n      dep_path.add(subsystem_cls)\n\n      for dep in subsystem_cls.subsystem_dependencies_iter():\n        if dep not in seen:\n          seen.add(dep)\n          yield dep\n          for d in iter_subsystem_closure(dep.subsystem_cls):\n            yield d\n\n      dep_path.remove(subsystem_cls)\n\n    for dep in iter_subsystem_closure(cls):\n      yield dep"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nyield ScopeInfo objects for all known scopes for this optionable.", "response": "def known_scope_infos(cls):\n    \"\"\"Yield ScopeInfo for all known scopes for this optionable, in no particular order.\n\n    :rtype: set of :class:`pants.option.scope.ScopeInfo`\n    :raises: :class:`pants.subsystem.subsystem_client_mixin.SubsystemClientMixin.CycleException`\n             if a dependency cycle is detected.\n    \"\"\"\n    known_scope_infos = set()\n    optionables_path = OrderedSet()  #  To check for cycles at the Optionable level, ignoring scope.\n\n    def collect_scope_infos(optionable_cls, scoped_to, removal_version=None, removal_hint=None):\n      if optionable_cls in optionables_path:\n        raise cls.CycleException(list(optionables_path) + [optionable_cls])\n      optionables_path.add(optionable_cls)\n\n      scope = (optionable_cls.options_scope if scoped_to == GLOBAL_SCOPE\n               else optionable_cls.subscope(scoped_to))\n      scope_info = ScopeInfo(\n          scope,\n          optionable_cls.options_scope_category,\n          optionable_cls,\n          removal_version=removal_version,\n          removal_hint=removal_hint\n        )\n\n      if scope_info not in known_scope_infos:\n        known_scope_infos.add(scope_info)\n        for dep in scope_info.optionable_cls.subsystem_dependencies_iter():\n          # A subsystem always exists at its global scope (for the purpose of options\n          # registration and specification), even if in practice we only use it scoped to\n          # some other scope.\n          #\n          # NB: We do not apply deprecations to this implicit global copy of the scope, because if\n          # the intention was to deprecate the entire scope, that could be accomplished by\n          # deprecating all options in the scope.\n          collect_scope_infos(dep.subsystem_cls, GLOBAL_SCOPE)\n          if not dep.is_global():\n            collect_scope_infos(dep.subsystem_cls,\n                                scope,\n                                removal_version=dep.removal_version,\n                                removal_hint=dep.removal_hint)\n\n      optionables_path.remove(scope_info.optionable_cls)\n\n    collect_scope_infos(cls, GLOBAL_SCOPE)\n    return known_scope_infos"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_requirement_libs_from(self, req_libs, platforms=None):\n    reqs = [req for req_lib in req_libs for req in req_lib.requirements]\n    self.add_resolved_requirements(reqs, platforms=platforms)", "response": "Multi - platform dependency resolution for PEX files."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nresolve a specific distribution from a set of requirements matching the current platform.", "response": "def extract_single_dist_for_current_platform(self, reqs, dist_key):\n    \"\"\"Resolve a specific distribution from a set of requirements matching the current platform.\n\n    :param list reqs: A list of :class:`PythonRequirement` to resolve.\n    :param str dist_key: The value of `distribution.key` to match for a `distribution` from the\n                         resolved requirements.\n    :return: The single :class:`pkg_resources.Distribution` matching `dist_key`.\n    :raises: :class:`self.SingleDistExtractionError` if no dists or multiple dists matched the given\n             `dist_key`.\n    \"\"\"\n    distributions = self._resolve_distributions_by_platform(reqs, platforms=['current'])\n    try:\n      matched_dist = assert_single_element(list(\n        dist\n        for _, dists in distributions.items()\n        for dist in dists\n        if dist.key == dist_key\n      ))\n    except (StopIteration, ValueError) as e:\n      raise self.SingleDistExtractionError(\n        \"Exactly one dist was expected to match name {} in requirements {}: {}\"\n        .format(dist_key, reqs, e))\n    return matched_dist"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_resolved_requirements(self, reqs, platforms=None):\n    distributions = self._resolve_distributions_by_platform(reqs, platforms=platforms)\n    locations = set()\n    for platform, dists in distributions.items():\n      for dist in dists:\n        if dist.location not in locations:\n          self._log.debug('  Dumping distribution: .../{}'.format(os.path.basename(dist.location)))\n          self.add_distribution(dist)\n        locations.add(dist.location)", "response": "Add the requirements to the PEX file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _resolve_multi(self, interpreter, requirements, platforms, find_links):\n    python_setup = self._python_setup_subsystem\n    python_repos = self._python_repos_subsystem\n    platforms = platforms or python_setup.platforms\n    find_links = find_links or []\n    distributions = {}\n    fetchers = python_repos.get_fetchers()\n    fetchers.extend(Fetcher([path]) for path in find_links)\n\n    for platform in platforms:\n      requirements_cache_dir = os.path.join(python_setup.resolver_cache_dir,\n        str(interpreter.identity))\n      resolved_dists = resolve(\n        requirements=[str(req.requirement) for req in requirements],\n        interpreter=interpreter,\n        fetchers=fetchers,\n        platform=platform,\n        context=python_repos.get_network_context(),\n        cache=requirements_cache_dir,\n        cache_ttl=python_setup.resolver_cache_ttl,\n        allow_prereleases=python_setup.resolver_allow_prereleases,\n        use_manylinux=python_setup.use_manylinux)\n      distributions[platform] = [resolved_dist.distribution for resolved_dist in resolved_dists]\n\n    return distributions", "response": "Multi - platform dependency resolution for PEX files."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stream_query(self, commands):\n    # The CLI transport does not support pipelining.\n    if self.transport is pywatchman.CLIProcessTransport:\n      raise NotImplementedError()\n\n    cmd_buf = deque(command for command in reversed(commands))\n    self._connect()\n\n    while 1:\n      # Interleave sends and receives to avoid bi-directional communication issues.\n      if cmd_buf:\n        item = cmd_buf.pop()\n        try:\n          self.sendConn.send(item)\n        except pywatchman.SocketTimeout:\n          cmd_buf.append(item)\n          yield\n\n      try:\n        result = self.recvConn.receive()\n      except pywatchman.SocketTimeout:\n        # Socket timeout - yield runtime context.\n        yield\n      else:\n        if 'error' in result:\n          raise pywatchman.WatchmanError('error from watchman: {}'.format(result['error']))\n        elif self.isUnilateralResponse(result) or 'subscribe' in result:\n          yield result\n        else:\n          yield result\n          break", "response": "A generator that yields unilateral events and subscribe events."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfetches data from the given URL notifying listener of all lifecycle events.", "response": "def fetch(self, url, listener, chunk_size_bytes=None, timeout_secs=None):\n    \"\"\"Fetches data from the given URL notifying listener of all lifecycle events.\n\n    :param string url: the url to GET data from\n    :param listener: the listener to notify of all download lifecycle events\n    :param chunk_size_bytes: the chunk size to use for buffering data, 10 KB by default\n    :param timeout_secs: the maximum time to wait for data to be available, 1 second by default\n    :raises: Fetcher.Error if there was a problem fetching all data from the given url\n    \"\"\"\n    if not isinstance(listener, self.Listener):\n      raise ValueError('listener must be a Listener instance, given {}'.format(listener))\n\n    chunk_size_bytes = chunk_size_bytes or 10 * 1024\n    timeout_secs = timeout_secs or 1.0\n\n    with closing(self._fetch(url, timeout_secs=timeout_secs)) as resp:\n      if resp.status_code != requests.codes.ok:\n        listener.status(resp.status_code)\n        raise self.PermanentError('Fetch of {} failed with status code {}'\n                                  .format(url, resp.status_code),\n                                  response_code=resp.status_code)\n      listener.status(resp.status_code, content_length=resp.size)\n\n      read_bytes = 0\n      for data in resp.iter_content(chunk_size_bytes=chunk_size_bytes):\n        listener.recv_chunk(data)\n        read_bytes += len(data)\n      if resp.size and read_bytes != resp.size:\n        raise self.Error('Expected {} bytes, read {}'.format(resp.size, read_bytes))\n      listener.finished()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef download(self, url, listener=None, path_or_fd=None, chunk_size_bytes=None, timeout_secs=None):\n    @contextmanager\n    def download_fp(_path_or_fd):\n      if _path_or_fd and not isinstance(_path_or_fd, six.string_types):\n        yield _path_or_fd, _path_or_fd.name\n      else:\n        if not _path_or_fd:\n          fd, _path_or_fd = tempfile.mkstemp()\n          os.close(fd)\n        with safe_open(_path_or_fd, 'wb') as fp:\n          yield fp, _path_or_fd\n\n    with download_fp(path_or_fd) as (fp, path):\n      listener = self.DownloadListener(fp).wrap(listener)\n      self.fetch(url, listener, chunk_size_bytes=chunk_size_bytes, timeout_secs=timeout_secs)\n      return path", "response": "Downloads data from the given URL."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef killall(self, everywhere=False):\n    with self._NAILGUN_KILL_LOCK:\n      for proc in self._iter_nailgun_instances(everywhere):\n        logger.info('killing nailgun server pid={pid}'.format(pid=proc.pid))\n        proc.terminate()", "response": "Kills all nailgun servers started by this machine."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes a fingerprint for this invocation of a Java task.", "response": "def _fingerprint(jvm_options, classpath, java_version):\n    \"\"\"Compute a fingerprint for this invocation of a Java task.\n\n       :param list jvm_options: JVM options passed to the java invocation\n       :param list classpath: The -cp arguments passed to the java invocation\n       :param Revision java_version: return value from Distribution.version()\n       :return: a hexstring representing a fingerprint of the java invocation\n    \"\"\"\n    digest = hashlib.sha1()\n    # TODO(John Sirois): hash classpath contents?\n    encoded_jvm_options = [option.encode('utf-8') for option in sorted(jvm_options)]\n    encoded_classpath = [cp.encode('utf-8') for cp in sorted(classpath)]\n    encoded_java_version = repr(java_version).encode('utf-8')\n    for item in (encoded_jvm_options, encoded_classpath, encoded_java_version):\n      digest.update(str(item).encode('utf-8'))\n    return digest.hexdigest() if PY3 else digest.hexdigest().decode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _runner(self, classpath, main, jvm_options, args, cwd=None):\n    command = self._create_command(classpath, main, jvm_options, args)\n\n    class Runner(self.Runner):\n      @property\n      def executor(this):\n        return self\n\n      @property\n      def command(self):\n        return list(command)\n\n      def run(this, stdout=None, stderr=None, stdin=None, cwd=None):\n        nailgun = None\n        try:\n          nailgun = self._get_nailgun_client(jvm_options, classpath, stdout, stderr, stdin)\n          logger.debug('Executing via {ng_desc}: {cmd}'.format(ng_desc=nailgun, cmd=this.cmd))\n          return nailgun.execute(main, cwd, *args)\n        except (NailgunClient.NailgunError, self.InitialNailgunConnectTimedOut) as e:\n          self.terminate()\n          raise self.Error('Problem launching via {ng_desc} command {main} {args}: {msg}'\n                           .format(ng_desc=nailgun or '<no nailgun connection>',\n                                   main=main,\n                                   args=' '.join(args),\n                                   msg=e))\n\n    return Runner()", "response": "Factory for executing a command via Nailgun."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nblocks for the nailgun subprocess to bind and emit a listening port in the nailgun stdout.", "response": "def _await_socket(self, timeout):\n    \"\"\"Blocks for the nailgun subprocess to bind and emit a listening port in the nailgun stdout.\"\"\"\n    with safe_open(self._ng_stdout, 'r') as ng_stdout:\n      start_time = time.time()\n      accumulated_stdout = ''\n      while 1:\n        # TODO: share the decreasing timeout logic here with NailgunProtocol.iter_chunks() by adding\n        # a method to pants.util.contextutil!\n        remaining_time = time.time() - (start_time + timeout)\n        if remaining_time > 0:\n          stderr = read_file(self._ng_stderr, binary_mode=True)\n          raise self.InitialNailgunConnectTimedOut(\n            timeout=timeout,\n            stdout=accumulated_stdout,\n            stderr=stderr,\n          )\n\n        readable, _, _ = select.select([ng_stdout], [], [], (-1 * remaining_time))\n        if readable:\n          line = ng_stdout.readline()                          # TODO: address deadlock risk here.\n          try:\n            return self._NG_PORT_REGEX.match(line).group(1)\n          except AttributeError:\n            pass\n          accumulated_stdout += line"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nensuring that a nailgun client is connectable or raises NailgunError.", "response": "def ensure_connectable(self, nailgun):\n    \"\"\"Ensures that a nailgun client is connectable or raises NailgunError.\"\"\"\n    attempt_count = 1\n    while 1:\n      try:\n        with closing(nailgun.try_connect()) as sock:\n          logger.debug('Verified new ng server is connectable at {}'.format(sock.getpeername()))\n          return\n      except nailgun.NailgunConnectionError:\n        if attempt_count >= self._connect_attempts:\n          logger.debug('Failed to connect to ng after {} attempts'.format(self._connect_attempts))\n          raise     # Re-raise the NailgunConnectionError which provides more context to the user.\n\n      attempt_count += 1\n      time.sleep(self.WAIT_INTERVAL_SEC)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npost - fork child callback for ProcessManager. daemon_spawn.", "response": "def post_fork_child(self, fingerprint, jvm_options, classpath, stdout, stderr):\n    \"\"\"Post-fork() child callback for ProcessManager.daemon_spawn().\"\"\"\n    java = SubprocessExecutor(self._distribution)\n\n    subproc = java.spawn(classpath=classpath,\n                         main='com.martiansoftware.nailgun.NGServer',\n                         jvm_options=jvm_options,\n                         args=[':0'],\n                         stdin=safe_open('/dev/null', 'r'),\n                         stdout=safe_open(self._ng_stdout, 'w'),\n                         stderr=safe_open(self._ng_stderr, 'w'),\n                         close_fds=True)\n\n    self.write_pid(subproc.pid)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse xml file and return parsed text as a DOM Document.", "response": "def _parse(cls, xml_path):\n    \"\"\"Parse .xml file and return parsed text as a DOM Document.\n\n    :param string xml_path: File path of xml file to be parsed.\n    :returns xml.dom.minidom.Document parsed_xml: Document instance containing parsed xml.\n    \"\"\"\n    try:\n      parsed_xml = parse(xml_path)\n    # Minidom is a frontend for various parsers, only Exception covers ill-formed .xml for them all.\n    except Exception as e:\n      raise cls.XmlError('Error parsing xml file at {0}: {1}'.format(xml_path, e))\n    return parsed_xml"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_file(cls, xml_path):\n    try:\n      parsed_xml = cls._parse(xml_path)\n    except OSError as e:\n      raise XmlParser.XmlError(\"Problem reading xml file at {}: {}\".format(xml_path, e))\n    return cls(xml_path, parsed_xml)", "response": "Parse. xml file and create a XmlParser object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_attribute(self, element, attribute):\n    parsed_element = self.parsed.getElementsByTagName(element)\n    if not parsed_element:\n      raise self.XmlError(\"There is no '{0}' element in \"\n                          \"xml file at: {1}\".format(element, self.xml_path))\n    parsed_attribute = parsed_element[0].getAttribute(attribute)\n    if not parsed_attribute:\n      raise self.XmlError(\"There is no '{0}' attribute in \"\n                          \"xml at: {1}\".format(attribute, self.xml_path))\n    return parsed_attribute", "response": "Retrieve the value of an attribute that is contained by the xml element."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nattempt to retrieve an optional attribute from the xml and return None on failure.", "response": "def get_optional_attribute(self, element, attribute):\n    \"\"\"Attempt to retrieve an optional attribute from the xml and return None on failure.\"\"\"\n    try:\n      return self.get_attribute(element, attribute)\n    except self.XmlError:\n      return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_rank_value(cls, name):\n    if name in cls._RANK_NAMES.values():\n      return getattr(cls, name, None)\n    return None", "response": "Returns the integer constant value for the given rank name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef prioritized_iter(cls, flag_val, env_val, config_val, config_default_val,\n                       hardcoded_val, default):\n    \"\"\"Yield the non-None values from highest-ranked to lowest, wrapped in RankedValue instances.\"\"\"\n    if flag_val is not None:\n      yield RankedValue(cls.FLAG, flag_val)\n    if env_val is not None:\n      yield RankedValue(cls.ENVIRONMENT, env_val)\n    if config_val is not None:\n      yield RankedValue(cls.CONFIG, config_val)\n    if config_default_val is not None:\n      yield RankedValue(cls.CONFIG_DEFAULT, config_default_val)\n    if hardcoded_val is not None:\n      yield RankedValue(cls.HARDCODED, hardcoded_val)\n    yield RankedValue(cls.NONE, default)", "response": "Yields the non - None values from highest - ranked to lowest wrapped in RankedValue instances."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nimports a method from ArtifactCache. use_cached_files on a cached artifact.", "response": "def call_use_cached_files(tup):\n  \"\"\"Importable helper for multi-proc calling of ArtifactCache.use_cached_files on a cache instance.\n\n  Multiprocessing map/apply/etc require functions which can be imported, not bound methods.\n  To call a bound method, instead call a helper like this and pass tuple of the instance and args.\n  The helper can then call the original method on the deserialized instance.\n\n  :param tup: A tuple of an ArtifactCache and args (eg CacheKey) for ArtifactCache.use_cached_files.\n  \"\"\"\n\n  try:\n    cache, key, results_dir = tup\n    res = cache.use_cached_files(key, results_dir)\n    if res:\n      sys.stderr.write('.')\n    else:\n      sys.stderr.write(' ')\n    sys.stderr.flush()\n    return res\n  except NonfatalArtifactCacheError as e:\n    logger.warn('Error calling use_cached_files in artifact cache: {0}'.format(e))\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef call_insert(tup):\n  try:\n    cache, key, files, overwrite = tup\n    return cache.insert(key, files, overwrite)\n  except NonfatalArtifactCacheError as e:\n    logger.warn('Error while inserting into artifact cache: {0}'.format(e))\n    return False", "response": "Importable helper for multi - proc calling of ArtifactCache. insert on an ArtifactCache instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninserts a new artifact into the artifact cache.", "response": "def insert(self, cache_key, paths, overwrite=False):\n    \"\"\"Cache the output of a build.\n\n    By default, checks cache.has(key) first, only proceeding to create and insert an artifact\n    if it is not already in the cache (though `overwrite` can be used to skip the check and\n    unconditionally insert).\n\n    :param CacheKey cache_key: A CacheKey object.\n    :param list<str> paths: List of absolute paths to generated dirs/files.\n                            These must be under the artifact_root.\n    :param bool overwrite: Skip check for existing, insert even if already in cache.\n    \"\"\"\n    missing_files = [f for f in paths if not os.path.exists(f)]\n    if missing_files:\n      raise ArtifactCacheError('Tried to cache nonexistent files {0}'.format(missing_files))\n\n    if not overwrite:\n      if self.has(cache_key):\n        logger.debug('Skipping insert of existing artifact: {0}'.format(cache_key))\n        return False\n\n    try:\n      self.try_insert(cache_key, paths)\n      return True\n    except NonfatalArtifactCacheError as e:\n      logger.error('Error while writing to artifact cache: {0}'.format(e))\n      return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nyields functions for writing to stdout and stderr respectively.", "response": "def line_oriented(cls, line_oriented_options, console):\n    \"\"\"Given Goal.Options and a Console, yields functions for writing to stdout and stderr, respectively.\n\n    The passed options instance will generally be the `Goal.Options` of a `LineOriented` `Goal`.\n    \"\"\"\n    if type(line_oriented_options) != cls.Options:\n      raise AssertionError(\n          'Expected Options for `{}`, got: {}'.format(cls.__name__, line_oriented_options))\n\n    output_file = line_oriented_options.values.output_file\n    sep = line_oriented_options.values.sep.encode('utf-8').decode('unicode_escape')\n\n    stdout, stderr = console.stdout, console.stderr\n    if output_file:\n      stdout = open(output_file, 'w')\n\n    try:\n      print_stdout = lambda msg: print(msg, file=stdout, end=sep)\n      print_stderr = lambda msg: print(msg, file=stderr)\n      yield print_stdout, print_stderr\n    finally:\n      if output_file:\n        stdout.close()\n      else:\n        stdout.flush()\n      stderr.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a synthetic subclass of the given superclass and options scope.", "response": "def _create_stable_task_type(superclass, options_scope):\n  \"\"\"Creates a singleton (via `memoized`) subclass instance for the given superclass and scope.\n\n  Currently we need to support registering the same task type multiple times in different\n  scopes. However we still want to have each task class know the options scope it was\n  registered in. So we create a synthetic subclass here.\n\n  TODO(benjy): Revisit this when we revisit the task lifecycle. We probably want to have\n  a task *instance* know its scope, but this means converting option registration from\n  a class method to an instance method, and instantiating the task much sooner in the\n  lifecycle.\n  \"\"\"\n  subclass_name = '{0}_{1}'.format(superclass.__name__,\n                                  options_scope.replace('.', '_').replace('-', '_'))\n  if PY2:\n    subclass_name = subclass_name.encode('utf-8')\n  return type(subclass_name, (superclass,), {\n    '__doc__': superclass.__doc__,\n    '__module__': superclass.__module__,\n    'options_scope': options_scope,\n    '_stable_name': superclass.stable_name()\n  })"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nregisters a new goal description.", "response": "def register(cls, name, description, options_registrar_cls=None):\n    \"\"\"Register a goal description.\n\n    Otherwise the description must be set when registering some task on the goal,\n    which is clunky, and dependent on things like registration order of tasks in the goal.\n\n    A goal that isn't explicitly registered with a description will fall back to the description\n    of the task in that goal with the same name (if any).  So singleton goals (e.g., 'clean-all')\n    need not be registered explicitly.  This method is primarily useful for setting a\n    description on a generic goal like 'compile' or 'test', that multiple backends will\n    register tasks on.\n\n    :API: public\n\n    :param string name: The name of the goal; ie: the way to specify it on the command line.\n    :param string description: A description of the tasks in the goal do.\n    :param :class:pants.option.Optionable options_registrar_cls: A class for registering options\n           at the goal scope. Useful for registering recursive options on all tasks in a goal.\n    :return: The freshly registered goal.\n    :rtype: :class:`_Goal`\n    \"\"\"\n    goal = cls.by_name(name)\n    goal._description = description\n    goal._options_registrar_cls = (options_registrar_cls.registrar_for_scope(name)\n                                   if options_registrar_cls else None)\n    return goal"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the unique object representing the goal of the specified name.", "response": "def by_name(cls, name):\n    \"\"\"Returns the unique object representing the goal of the specified name.\n\n    :API: public\n    \"\"\"\n    if name not in cls._goal_by_name:\n      cls._goal_by_name[name] = _Goal(name)\n    return cls._goal_by_name[name]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef all():\n    return [goal for _, goal in sorted(Goal._goal_by_name.items()) if goal.active]", "response": "Returns all active registered goals sorted alphabetically by name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef subsystems(cls):\n    ret = set()\n    for goal in cls.all():\n      ret.update(goal.subsystems())\n    return ret", "response": "Returns all subsystem types used by all tasks in no particular order."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef install(self, task_registrar, first=False, replace=False, before=None, after=None):\n    if [bool(place) for place in [first, replace, before, after]].count(True) > 1:\n      raise GoalError('Can only specify one of first, replace, before or after')\n\n    otn = self._ordered_task_names\n    if replace:\n      for tt in self.task_types():\n        tt.options_scope = None\n      del otn[:]\n      self._task_type_by_name = {}\n\n    task_name = task_registrar.name\n    if task_name in self._task_type_by_name:\n      raise GoalError(\n        'Can only specify a task name once per goal, saw multiple values for {} in goal {}'.format(\n          task_name,\n          self.name))\n    Optionable.validate_scope_name_component(task_name)\n    options_scope = Goal.scope(self.name, task_name)\n\n    task_type = _create_stable_task_type(task_registrar.task_type, options_scope)\n\n    if first:\n      otn.insert(0, task_name)\n    elif before in otn:\n      otn.insert(otn.index(before), task_name)\n    elif after in otn:\n      otn.insert(otn.index(after) + 1, task_name)\n    else:\n      otn.append(task_name)\n\n    self._task_type_by_name[task_name] = task_type\n\n    if task_registrar.serialize:\n      self.serialize = True\n\n    return self", "response": "Installs the given task in this goal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove the named task from this goal.", "response": "def uninstall_task(self, name):\n    \"\"\"Removes the named task from this goal.\n\n    Allows external plugins to modify the execution plan. Use with caution.\n\n    Note: Does not relax a serialization requirement that originated\n    from the uninstalled task's install() call.\n\n    :API: public\n    \"\"\"\n    if name in self._task_type_by_name:\n      self._task_type_by_name[name].options_scope = None\n      del self._task_type_by_name[name]\n      self._ordered_task_names = [x for x in self._ordered_task_names if x != name]\n    else:\n      raise GoalError('Cannot uninstall unknown task: {0}'.format(name))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef subsystems(self):\n    ret = set()\n    for task_type in self.task_types():\n      ret.update([dep.subsystem_cls for dep in task_type.subsystem_dependencies_iter()])\n    return ret", "response": "Returns all subsystem types used by tasks in this goal in no particular order."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if this goal has a task of the given type.", "response": "def has_task_of_type(self, typ):\n    \"\"\"Returns True if this goal has a task of the given type (or a subtype of it).\"\"\"\n    for task_type in self.task_types():\n      if issubclass(task_type, typ):\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef complete_scopes(cls, scope_infos):\n    ret = {GlobalOptionsRegistrar.get_scope_info()}\n    original_scopes = dict()\n    for si in scope_infos:\n      ret.add(si)\n      if si.scope in original_scopes:\n        raise cls.DuplicateScopeError('Scope `{}` claimed by {}, was also claimed by {}.'.format(\n            si.scope, si, original_scopes[si.scope]\n          ))\n      original_scopes[si.scope] = si\n      if si.deprecated_scope:\n        ret.add(ScopeInfo(si.deprecated_scope, si.category, si.optionable_cls))\n        original_scopes[si.deprecated_scope] = si\n\n    # TODO: Once scope name validation is enforced (so there can be no dots in scope name\n    # components) we can replace this line with `for si in scope_infos:`, because it will\n    # not be possible for a deprecated_scope to introduce any new intermediate scopes.\n    for si in copy.copy(ret):\n      for scope in all_enclosing_scopes(si.scope, allow_global=False):\n        if scope not in original_scopes:\n          ret.add(ScopeInfo(scope, ScopeInfo.INTERMEDIATE))\n    return ret", "response": "Expand a set of scopes to include all enclosing scopes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create(cls, env, config, known_scope_infos, args=None, bootstrap_option_values=None):\n    # We need parsers for all the intermediate scopes, so inherited option values\n    # can propagate through them.\n    complete_known_scope_infos = cls.complete_scopes(known_scope_infos)\n    splitter = ArgSplitter(complete_known_scope_infos)\n    args = sys.argv if args is None else args\n    goals, scope_to_flags, target_specs, passthru, passthru_owner, unknown_scopes = splitter.split_args(args)\n\n    option_tracker = OptionTracker()\n\n    if bootstrap_option_values:\n      target_spec_files = bootstrap_option_values.target_spec_files\n      if target_spec_files:\n        for spec in target_spec_files:\n          with open(spec, 'r') as f:\n            target_specs.extend([line for line in [line.strip() for line in f] if line])\n\n    help_request = splitter.help_request\n\n    parser_hierarchy = ParserHierarchy(env, config, complete_known_scope_infos, option_tracker)\n    bootstrap_option_values = bootstrap_option_values\n    known_scope_to_info = {s.scope: s for s in complete_known_scope_infos}\n    return cls(goals, scope_to_flags, target_specs, passthru, passthru_owner, help_request,\n               parser_hierarchy, bootstrap_option_values, known_scope_to_info,\n               option_tracker, unknown_scopes)", "response": "Create an Options instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of GOALs organized into three tuples by whether they are v1 ambiguous or v2 goals.", "response": "def goals_by_version(self):\n    \"\"\"Goals organized into three tuples by whether they are v1, ambiguous, or v2 goals (respectively).\n\n    It's possible for a goal to be implemented with both v1 and v2, in which case a consumer\n    should use the `--v1` and `--v2` global flags to disambiguate.\n    \"\"\"\n    v1, ambiguous, v2 = [], [], []\n    for goal in self._goals:\n      goal_dot = '{}.'.format(goal)\n      scope_categories = {s.category\n                          for s in self.known_scope_to_info.values()\n                          if s.scope == goal or s.scope.startswith(goal_dot)}\n      is_v1 = ScopeInfo.TASK in scope_categories\n      is_v2 = ScopeInfo.GOAL in scope_categories\n      if is_v1 and is_v2:\n        ambiguous.append(goal)\n      elif is_v1:\n        v1.append(goal)\n      else:\n        v2.append(goal)\n    return tuple(v1), tuple(ambiguous), tuple(v2)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef drop_flag_values(self):\n    # An empty scope_to_flags to force all values to come via the config -> env hierarchy alone\n    # and empty values in case we already cached some from flags.\n    no_flags = {}\n    return Options(self._goals,\n                   no_flags,\n                   self._target_specs,\n                   self._passthru,\n                   self._passthru_owner,\n                   self._help_request,\n                   self._parser_hierarchy,\n                   self._bootstrap_option_values,\n                   self._known_scope_to_info,\n                   self._option_tracker,\n                   self._unknown_scopes)", "response": "Returns a copy of this Options that ignores values specified via flags."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef register(self, scope, *args, **kwargs):\n    self._assert_not_frozen()\n    self.get_parser(scope).register(*args, **kwargs)\n    deprecated_scope = self.known_scope_to_info[scope].deprecated_scope\n    if deprecated_scope:\n      self.get_parser(deprecated_scope).register(*args, **kwargs)", "response": "Register an option in the given scope."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef registration_function_for_optionable(self, optionable_class):\n    self._assert_not_frozen()\n    # TODO(benjy): Make this an instance of a class that implements __call__, so we can\n    # docstring it, and so it's less weird than attatching properties to a function.\n    def register(*args, **kwargs):\n      kwargs['registering_class'] = optionable_class\n      self.register(optionable_class.options_scope, *args, **kwargs)\n    # Clients can access the bootstrap option values as register.bootstrap.\n    register.bootstrap = self.bootstrap_option_values()\n    # Clients can access the scope as register.scope.\n    register.scope = optionable_class.options_scope\n    return register", "response": "Returns a function that registers options on the given scope."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_and_apply_deprecations(self, scope, values):\n    si = self.known_scope_to_info[scope]\n\n    # If this Scope is itself deprecated, report that.\n    if si.removal_version:\n      explicit_keys = self.for_scope(scope, inherit_from_enclosing_scope=False).get_explicit_keys()\n      if explicit_keys:\n        warn_or_error(\n            removal_version=si.removal_version,\n            deprecated_entity_description='scope {}'.format(scope),\n            hint=si.removal_hint,\n          )\n\n    # Check if we're the new name of a deprecated scope, and clone values from that scope.\n    # Note that deprecated_scope and scope share the same Optionable class, so deprecated_scope's\n    # Optionable has a deprecated_options_scope equal to deprecated_scope. Therefore we must\n    # check that scope != deprecated_scope to prevent infinite recursion.\n    deprecated_scope = si.deprecated_scope\n    if deprecated_scope is not None and scope != deprecated_scope:\n      # Do the deprecation check only on keys that were explicitly set on the deprecated scope\n      # (and not on its enclosing scopes).\n      explicit_keys = self.for_scope(deprecated_scope,\n                                     inherit_from_enclosing_scope=False).get_explicit_keys()\n      if explicit_keys:\n        # Update our values with those of the deprecated scope (now including values inherited\n        # from its enclosing scope).\n        # Note that a deprecated val will take precedence over a val of equal rank.\n        # This makes the code a bit neater.\n        values.update(self.for_scope(deprecated_scope))\n\n        warn_or_error(\n            removal_version=self.known_scope_to_info[scope].deprecated_scope_removal_version,\n            deprecated_entity_description='scope {}'.format(deprecated_scope),\n            hint='Use scope {} instead (options: {})'.format(scope, ', '.join(explicit_keys))\n          )", "response": "Checks whether a ScopeInfo has options specified in a deprecated scope and applies deprecated options to the given values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef for_scope(self, scope, inherit_from_enclosing_scope=True):\n\n    # First get enclosing scope's option values, if any.\n    if scope == GLOBAL_SCOPE or not inherit_from_enclosing_scope:\n      values = OptionValueContainer()\n    else:\n      values = copy.copy(self.for_scope(enclosing_scope(scope)))\n\n    # Now add our values.\n    flags_in_scope = self._scope_to_flags.get(scope, [])\n    self._parser_hierarchy.get_parser_by_scope(scope).parse_args(flags_in_scope, values)\n\n    # Check for any deprecation conditions, which are evaluated using `self._flag_matchers`.\n    if inherit_from_enclosing_scope:\n      self._check_and_apply_deprecations(scope, values)\n\n    return values", "response": "Return the option values for the given scope."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of all options that are fingerprintable for the given scope.", "response": "def get_fingerprintable_for_scope(self, bottom_scope, include_passthru=False,\n                                    fingerprint_key=None, invert=False):\n    \"\"\"Returns a list of fingerprintable (option type, option value) pairs for the given scope.\n\n    Fingerprintable options are options registered via a \"fingerprint=True\" kwarg. This flag\n    can be parameterized with `fingerprint_key` for special cases.\n\n    This method also searches enclosing options scopes of `bottom_scope` to determine the set of\n    fingerprintable pairs.\n\n    :param str bottom_scope: The scope to gather fingerprintable options for.\n    :param bool include_passthru: Whether to include passthru args captured by `bottom_scope` in the\n                                  fingerprintable options.\n    :param string fingerprint_key: The option kwarg to match against (defaults to 'fingerprint').\n    :param bool invert: Whether or not to invert the boolean check for the fingerprint_key value.\n\n    :API: public\n    \"\"\"\n    fingerprint_key = fingerprint_key or 'fingerprint'\n    fingerprint_default = bool(invert)\n    pairs = []\n\n    if include_passthru:\n      # Passthru args can only be sent to outermost scopes so we gather them once here up-front.\n      passthru_args = self.passthru_args_for_scope(bottom_scope)\n      # NB: We can't sort passthru args, the underlying consumer may be order-sensitive.\n      pairs.extend((str, pass_arg) for pass_arg in passthru_args)\n\n    # Note that we iterate over options registered at `bottom_scope` and at all\n    # enclosing scopes, since option-using code can read those values indirectly\n    # via its own OptionValueContainer, so they can affect that code's output.\n    for registration_scope in all_enclosing_scopes(bottom_scope):\n      parser = self._parser_hierarchy.get_parser_by_scope(registration_scope)\n      # Sort the arguments, so that the fingerprint is consistent.\n      for (_, kwargs) in sorted(parser.option_registrations_iter()):\n        if kwargs.get('recursive', False) and not kwargs.get('recursive_root', False):\n          continue  # We only need to fprint recursive options once.\n        if kwargs.get(fingerprint_key, fingerprint_default) is not True:\n          continue\n        # Note that we read the value from scope, even if the registration was on an enclosing\n        # scope, to get the right value for recursive options (and because this mirrors what\n        # option-using code does).\n        val = self.for_scope(bottom_scope)[kwargs['dest']]\n        # If we have a list then we delegate to the fingerprinting implementation of the members.\n        if is_list_option(kwargs):\n          val_type = kwargs.get('member_type', str)\n        else:\n          val_type = kwargs.get('type', str)\n        pairs.append((val_type, val))\n    return pairs"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrecords that the given option was set to the given value.", "response": "def record_option(self, scope, option, value, rank, deprecation_version=None, details=None):\n    \"\"\"Records that the given option was set to the given value.\n\n    :param string scope: scope of the option.\n    :param string option: name of the option.\n    :param string value: value the option was set to.\n    :param int rank: the rank of the option (Eg, RankedValue.HARDCODED), to keep track of where the\n      option came from.\n    :param deprecation_version: Deprecation version for this option.\n    :param string details: optional additional details about how the option was set (eg, the name\n           of a particular config file, if the rank is RankedValue.CONFIG).\n    \"\"\"\n    scoped_options = self.option_history_by_scope[scope]\n    if option not in scoped_options:\n      scoped_options[option] = self.OptionHistory()\n    scoped_options[option].record_value(value, rank, deprecation_version, details)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fingerprint_target(self, target):\n    fingerprint = self.compute_fingerprint(target)\n    if fingerprint:\n      return '{fingerprint}-{name}'.format(fingerprint=fingerprint, name=type(self).__name__)\n    else:\n      return None", "response": "This method returns a fingerprint labeled with the name of the target object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compute_fingerprint(self, target):\n    if isinstance(target, UnpackedWheels):\n      hasher = sha1()\n      for cache_key in sorted(req.cache_key() for req in target.all_imported_requirements):\n        hasher.update(cache_key.encode('utf-8'))\n      hasher.update(target.payload.fingerprint().encode('utf-8'))\n      return hasher.hexdigest() if PY3 else hasher.hexdigest().decode('utf-8')\n    return None", "response": "Compute the fingerprint of the target."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_matching_wheel(self, pex_path, interpreter, requirements, module_name):\n    with self.context.new_workunit('extract-native-wheels'):\n      with safe_concurrent_creation(pex_path) as chroot:\n        pex_builder = PexBuilderWrapper.Factory.create(\n          builder=PEXBuilder(path=chroot, interpreter=interpreter),\n          log=self.context.log)\n        return pex_builder.extract_single_dist_for_current_platform(requirements, module_name)", "response": "Use PexBuilderWrapper to resolve a single wheel from the requirement specs using pex."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef exit(self, result=PANTS_SUCCEEDED_EXIT_CODE, msg=None, out=None):\n    if msg:\n      out = out or sys.stderr\n      if PY3 and hasattr(out, 'buffer'):\n        out = out.buffer\n\n      msg = ensure_binary(msg)\n      try:\n        out.write(msg)\n        out.write(b'\\n')\n        # TODO: Determine whether this call is a no-op because the stream gets flushed on exit, or\n        # if we could lose what we just printed, e.g. if we get interrupted by a signal while\n        # exiting and the stream is buffered like stdout.\n        out.flush()\n      except Exception as e:\n        # If the file is already closed, or any other error occurs, just log it and continue to\n        # exit.\n        if msg:\n          logger.warning(\"Encountered error when trying to log this message: {}\".format(msg))\n          # In pantsd, this won't go anywhere, because there's really nowhere for us to log if we\n          # can't log :(\n          # Not in pantsd, this will end up in sys.stderr.\n          traceback.print_stack()\n        logger.exception(e)\n    self._exit(result)", "response": "Exits the runtime.\n\n    :param result: The exit status. Typically either PANTS_SUCCEEDED_EXIT_CODE or\n                   PANTS_FAILED_EXIT_CODE, but can be a string as well. (Optional)\n    :param msg: A string message to print to stderr or another custom file desciptor before exiting.\n                (Optional)\n    :param out: The file descriptor to emit `msg` to. (Optional)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef exit_and_fail(self, msg=None, out=None):\n    self.exit(result=PANTS_FAILED_EXIT_CODE, msg=msg, out=out)", "response": "Exits the runtime with a nonzero exit code indicating failure."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(self, content):\n    if self.format == 'json_map':\n      try:\n        return assert_list(json.loads(content.decode(self.encoding))[self.index])\n      except (KeyError, UnicodeDecodeError, ValueError) as e:\n        raise self.ResponseParserError(\"Error while parsing response content: {0}\".format(str(e)))\n\n    # Should never get here.\n    raise ValueError('Unknown content format: \"{}\"'.format(self.format))", "response": "Parse the raw response content for a list of remote artifact cache URLs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resolve(self, resolve_from):\n    session = requests.Session()\n    session.mount(resolve_from, requests.adapters.HTTPAdapter(max_retries=self._tries))\n    content = self._safe_get_content(session, resolve_from)\n    try:\n      parsed_urls = self._response_parser.parse(content)\n      if len(parsed_urls) > 0:\n        return parsed_urls\n      raise self.ResolverError('Empty result received from {0}'.format(resolve_from))\n    except ResponseParser.ResponseParserError as e:\n      raise self.ResolverError('Error parsing response: {0}'.format(str(e)))", "response": "Resolve the URL from a resolve_from URL."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of tuples only including entries marked as unfinished.", "response": "def unfinished_items(self):\n    \"\"\"Returns a list of (name, status) tuples, only including entries marked as unfinished.\"\"\"\n    return [(key, stat) for key, stat in self._statuses.items() if stat not in self.DONE_STATES]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwalks the dependency graph breadth - first and computes the job priority as the sum of the jobs sizes along the critical path along the critical path.", "response": "def _compute_job_priorities(self, job_list):\n    \"\"\"Walks the dependency graph breadth-first, starting from the most dependent tasks,\n     and computes the job priority as the sum of the jobs sizes along the critical path.\"\"\"\n\n    job_size = {job.key: job.size for job in job_list}\n    job_priority = defaultdict(int)\n\n    bfs_queue = deque()\n    for job in job_list:\n      if len(self._dependees[job.key]) == 0:\n        job_priority[job.key] = job_size[job.key]\n        bfs_queue.append(job.key)\n\n    satisfied_dependees_count = defaultdict(int)\n    while len(bfs_queue) > 0:\n      job_key = bfs_queue.popleft()\n      for dependency_key in self._dependencies[job_key]:\n        job_priority[dependency_key] = \\\n          max(job_priority[dependency_key],\n              job_size[dependency_key] + job_priority[job_key])\n        satisfied_dependees_count[dependency_key] += 1\n        if satisfied_dependees_count[dependency_key] == len(self._dependees[dependency_key]):\n          bfs_queue.append(dependency_key)\n\n    return job_priority"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexecute the scheduled work on the given worker pool.", "response": "def execute(self, pool, log):\n    \"\"\"Runs scheduled work, ensuring all dependencies for each element are done before execution.\n\n    :param pool: A WorkerPool to run jobs on\n    :param log: logger for logging debug information and progress\n\n    submits all the work without any dependencies to the worker pool\n    when a unit of work finishes,\n      if it is successful\n        calls success callback\n        checks for dependees whose dependencies are all successful, and submits them\n      if it fails\n        calls failure callback\n        marks dependees as failed and queues them directly into the finished work queue\n    when all work is either successful or failed,\n      cleans up the work pool\n    if there's an exception on the main thread,\n      calls failure callback for unfinished work\n      aborts work pool\n      re-raises\n    \"\"\"\n    log.debug(self.format_dependee_graph())\n\n    status_table = StatusTable(self._job_keys_as_scheduled,\n                               {key: len(self._jobs[key].dependencies) for key in self._job_keys_as_scheduled})\n    finished_queue = queue.Queue()\n\n    heap = []\n    jobs_in_flight = ThreadSafeCounter()\n\n    def put_jobs_into_heap(job_keys):\n      for job_key in job_keys:\n        status_table.mark_queued(job_key)\n        # minus because jobs with larger priority should go first\n        heappush(heap, (-self._job_priority[job_key], job_key))\n\n    def try_to_submit_jobs_from_heap():\n      def worker(worker_key, work):\n        status_table.mark_as(RUNNING, worker_key)\n        try:\n          work()\n          result = (worker_key, SUCCESSFUL, None)\n        except BaseException:\n          _, exc_value, exc_traceback = sys.exc_info()\n          result = (worker_key, FAILED, (exc_value, traceback.format_tb(exc_traceback)))\n        finished_queue.put(result)\n        jobs_in_flight.decrement()\n\n      while len(heap) > 0 and jobs_in_flight.get() < pool.num_workers:\n        priority, job_key = heappop(heap)\n        jobs_in_flight.increment()\n        pool.submit_async_work(Work(worker, [(job_key, (self._jobs[job_key]))]))\n\n    def submit_jobs(job_keys):\n      put_jobs_into_heap(job_keys)\n      try_to_submit_jobs_from_heap()\n\n    try:\n      submit_jobs(self._job_keys_with_no_dependencies)\n\n      while not status_table.are_all_done():\n        try:\n          finished_key, result_status, value = finished_queue.get(timeout=10)\n        except queue.Empty:\n          self.log_progress(log, status_table)\n\n          try_to_submit_jobs_from_heap()\n          continue\n\n        finished_job = self._jobs[finished_key]\n        direct_dependees = self._dependees[finished_key]\n        status_table.mark_as(result_status, finished_key)\n\n        # Queue downstream tasks.\n        if result_status is SUCCESSFUL:\n          try:\n            finished_job.run_success_callback()\n          except Exception as e:\n            log.debug(traceback.format_exc())\n            raise ExecutionFailure(\"Error in on_success for {}\".format(finished_key), e)\n\n          ready_dependees = []\n          for dependee in direct_dependees:\n            status_table.mark_one_successful_dependency(dependee)\n            if status_table.is_ready_to_submit(dependee):\n              ready_dependees.append(dependee)\n\n          submit_jobs(ready_dependees)\n        else:  # Failed or canceled.\n          try:\n            finished_job.run_failure_callback()\n          except Exception as e:\n            log.debug(traceback.format_exc())\n            raise ExecutionFailure(\"Error in on_failure for {}\".format(finished_key), e)\n\n          # Propagate failures downstream.\n          for dependee in direct_dependees:\n            if status_table.is_unstarted(dependee):\n              status_table.mark_queued(dependee)\n              finished_queue.put((dependee, CANCELED, None))\n\n        # Log success or failure for this job.\n        if result_status is FAILED:\n          exception, tb = value\n          log.error(\"{} failed: {}\".format(finished_key, exception))\n          if self._print_stack_trace:\n            log.error('Traceback:\\n{}'.format('\\n'.join(tb)))\n        else:\n          log.debug(\"{} finished with status {}\".format(finished_key, result_status))\n    except ExecutionFailure:\n      raise\n    except Exception as e:\n      # Call failure callbacks for jobs that are unfinished.\n      for key, state in status_table.unfinished_items():\n        self._jobs[key].run_failure_callback()\n      log.debug(traceback.format_exc())\n      raise ExecutionFailure(\"Error running job\", e)\n\n    if status_table.has_failures():\n      raise ExecutionFailure(\"Failed jobs: {}\".format(', '.join(status_table.failed_keys())))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef preferred_jvm_distribution(cls, platforms, strict=False):\n    if not platforms:\n      return DistributionLocator.cached()\n    min_version = max(platform.target_level for platform in platforms)\n    max_version = Revision(*(min_version.components + [9999])) if strict else None\n    return DistributionLocator.cached(minimum_version=min_version, maximum_version=max_version)", "response": "Returns a jvm Distribution with a version that should work for all the requested platforms."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_platform_by_name(self, name, for_target=None):\n    if not name:\n      return self.default_platform\n    if name not in self.platforms_by_name:\n      raise self.UndefinedJvmPlatform(for_target, name, self.platforms_by_name)\n    return self.platforms_by_name[name]", "response": "Finds the jvm platform object with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_platform_for_target(self, target):\n    if not target.payload.platform and target.is_synthetic:\n      derived_from = target.derived_from\n      platform = derived_from and getattr(derived_from, 'platform', None)\n      if platform:\n        return platform\n    return self.get_platform_by_name(target.payload.platform, target)", "response": "Returns the jvm platform object associated with this target."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the java version into a Revision object.", "response": "def parse_java_version(cls, version):\n    \"\"\"Parses the java version (given a string or Revision object).\n\n    Handles java version-isms, converting things like '7' -> '1.7' appropriately.\n\n    Truncates input versions down to just the major and minor numbers (eg, 1.6), ignoring extra\n    versioning information after the second number.\n\n    :param version: the input version, given as a string or Revision object.\n    :return: the parsed and cleaned version, suitable as a javac -source or -target argument.\n    :rtype: Revision\n    \"\"\"\n    conversion = {str(i): '1.{}'.format(i) for i in cls.SUPPORTED_CONVERSION_VERSIONS}\n    if str(version) in conversion:\n      return Revision.lenient(conversion[str(version)])\n\n    if not hasattr(version, 'components'):\n      version = Revision.lenient(version)\n    if len(version.components) <= 2:\n      return version\n    return Revision(*version.components[:2])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create(cls, serializable_instance):\n    if not Serializable.is_serializable(serializable_instance):\n      raise ValueError('Can only create pickles for Serializable objects, given {} of type {}'\n                       .format(serializable_instance, type(serializable_instance).__name__))\n    return cls(unpickle_func=_unpickle_serializable,\n               args=(type(serializable_instance), serializable_instance._asdict()))", "response": "Return a tuple that implements the __reduce__ pickle protocol for serializable_instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_serializable(obj):\n    if inspect.isclass(obj):\n      return Serializable.is_serializable_type(obj)\n    return isinstance(obj, Serializable) or hasattr(obj, '_asdict')", "response": "Return True if the given object conforms to the Serializable protocol."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_serializable_type(type_):\n    if not inspect.isclass(type_):\n      return Serializable.is_serializable(type_)\n    return issubclass(type_, Serializable) or hasattr(type_, '_asdict')", "response": "Return True if the given type s instances conform to the Serializable protocol."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a help message for the specified options.", "response": "def format_options(self, scope, description, option_registrations_iter):\n    \"\"\"Return a help message for the specified options.\n\n    :param option_registrations_iter: An iterator over (args, kwargs) pairs, as passed in to\n                                      options registration.\n    \"\"\"\n    oshi = HelpInfoExtracter(self._scope).get_option_scope_help_info(option_registrations_iter)\n    lines = []\n    def add_option(category, ohis):\n      if ohis:\n        lines.append('')\n        display_scope = scope or 'Global'\n        if category:\n          lines.append(self._maybe_blue('{} {} options:'.format(display_scope, category)))\n        else:\n          lines.append(self._maybe_blue('{} options:'.format(display_scope)))\n          if description:\n            lines.append(description)\n        lines.append(' ')\n        for ohi in ohis:\n          lines.extend(self.format_option(ohi))\n    add_option('', oshi.basic)\n    if self._show_recursive:\n      add_option('recursive', oshi.recursive)\n    if self._show_advanced:\n      add_option('advanced', oshi.advanced)\n    return lines"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nformats the help output for a single option.", "response": "def format_option(self, ohi):\n    \"\"\"Format the help output for a single option.\n\n    :param OptionHelpInfo ohi: Extracted information for option to print\n    :return: Formatted help text for this option\n    :rtype: list of string\n    \"\"\"\n    lines = []\n    choices = 'one of: [{}] '.format(ohi.choices) if ohi.choices else ''\n    arg_line = ('{args} {dflt}'\n                .format(args=self._maybe_cyan(', '.join(ohi.display_args)),\n                        dflt=self._maybe_green('({}default: {})'.format(choices, ohi.default))))\n    lines.append(arg_line)\n\n    indent = '    '\n    lines.extend(['{}{}'.format(indent, s) for s in wrap(ohi.help, 76)])\n    if ohi.deprecated_message:\n      lines.append(self._maybe_red('{}{}.'.format(indent, ohi.deprecated_message)))\n      if ohi.removal_hint:\n        lines.append(self._maybe_red('{}{}'.format(indent, ohi.removal_hint)))\n    return lines"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pants_setup_py(name, description, additional_classifiers=None, **kwargs):\n  if not name.startswith('pantsbuild.pants'):\n    raise ValueError(\"Pants distribution package names must start with 'pantsbuild.pants', \"\n                     \"given {}\".format(name))\n\n  standard_classifiers = [\n      'Intended Audience :: Developers',\n      'License :: OSI Approved :: Apache Software License',\n      # We know for a fact these OSs work but, for example, know Windows\n      # does not work yet.  Take the conservative approach and only list OSs\n      # we know pants works with for now.\n      'Operating System :: MacOS :: MacOS X',\n      'Operating System :: POSIX :: Linux',\n      'Programming Language :: Python',\n      'Topic :: Software Development :: Build Tools']\n  classifiers = OrderedSet(standard_classifiers + (additional_classifiers or []))\n\n  notes = PantsReleases.global_instance().notes_for_version(PANTS_SEMVER)\n\n  return PythonArtifact(\n      name=name,\n      version=VERSION,\n      description=description,\n      long_description=(_read_contents('src/python/pants/ABOUT.rst') + notes),\n      url='https://github.com/pantsbuild/pants',\n      license='Apache License, Version 2.0',\n      zip_safe=True,\n      classifiers=list(classifiers),\n      **kwargs)", "response": "Creates a setup_py for a pants artifact."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef contrib_setup_py(name, description, additional_classifiers=None, **kwargs):\n  if not name.startswith('pantsbuild.pants.contrib.'):\n    raise ValueError(\"Contrib plugin package names must start with 'pantsbuild.pants.contrib.', \"\n                     \"given {}\".format(name))\n\n  return pants_setup_py(name,\n                        description,\n                        additional_classifiers=additional_classifiers,\n                        namespace_packages=['pants', 'pants.contrib'],\n                        **kwargs)", "response": "Creates a setup_py for a pants contrib plugin artifact."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _branch_name(cls, version):\n    suffix = version.public[len(version.base_version):]\n    components = version.base_version.split('.') + [suffix]\n    if suffix == '' or suffix.startswith('rc'):\n      # An un-suffixed, or suffixed-with-rc version is a release from a stable branch.\n      return '{}.{}.x'.format(*components[:2])\n    elif suffix.startswith('.dev'):\n      # Suffixed `dev` release version in master.\n      return 'master'\n    else:\n      raise ValueError('Unparseable pants version number: {}'.format(version))", "response": "Defines a mapping between versions and branches."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef notes_for_version(self, version):\n    branch_name = self._branch_name(version)\n    branch_notes_file = self._branch_notes.get(branch_name, None)\n    if branch_notes_file is None:\n      raise ValueError(\n          'Version {} lives in branch {}, which is not configured in {}.'.format(\n            version, branch_name, self._branch_notes))\n    return _read_contents(branch_notes_file)", "response": "Given the parsed Version of pants return its release notes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setup(self):\n    return self._load_plugins(\n      self._working_set,\n      self._bootstrap_options.pythonpath,\n      self._bootstrap_options.plugins,\n      self._bootstrap_options.backend_packages\n    )", "response": "Load backends and plugins and return a BuildConfiguration object constructed during backend loading."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _construct_options(options_bootstrapper, build_configuration):\n    # Now that plugins and backends are loaded, we can gather the known scopes.\n\n    # Gather the optionables that are not scoped to any other.  All known scopes are reachable\n    # via these optionables' known_scope_infos() methods.\n    top_level_optionables = (\n      {GlobalOptionsRegistrar} |\n      GlobalSubsystems.get() |\n      build_configuration.optionables() |\n      set(Goal.get_optionables())\n    )\n\n    # Now that we have the known scopes we can get the full options. `get_full_options` will\n    # sort and de-duplicate these for us.\n    known_scope_infos = [si\n                         for optionable in top_level_optionables\n                         for si in optionable.known_scope_infos()]\n    return options_bootstrapper.get_full_options(known_scope_infos)", "response": "Parse and register options.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_filters(predicate_params, predicate_factory):\n  filters = []\n  for predicate_param in predicate_params:\n    filters.append(create_filter(predicate_param, predicate_factory))\n  return filters", "response": "Create filter functions from a list of string parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a filter function from a string parameter.", "response": "def create_filter(predicate_param, predicate_factory):\n  \"\"\"Create a filter function from a string parameter.\n\n  :param predicate_param: Create a filter for this param string. Each string is a\n                          comma-separated list of arguments to the predicate_factory.\n                          If the entire comma-separated list is prefixed by a '-' then the\n                          sense of the resulting filter is inverted.\n  :param predicate_factory: A function that takes a parameter and returns a predicate, i.e., a\n                            function that takes a single parameter (of whatever type the filter\n                            operates on) and returns a boolean.\n  :return: A filter function of one argument that is the logical OR of the predicates for each of\n           the comma-separated arguments. If the comma-separated list was prefixed by a '-',\n           the sense of the filter is inverted.\n  \"\"\"\n  # NOTE: Do not inline this into create_filters above. A separate function is necessary\n  # in order to capture the different closure on each invocation.\n  modifier, param = _extract_modifier(predicate_param)\n  predicates = [predicate_factory(p) for p in param.split(',')]\n  def filt(x):\n    return modifier(any(pred(x) for pred in predicates))\n  return filt"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wrap_filters(filters):\n  def combined_filter(x):\n    for filt in filters:\n      if not filt(x):\n        return False\n    return True\n  return combined_filter", "response": "Returns a single filter that short - circuit ANDs the specified filters."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef binary(self):\n    dependencies = self.dependencies\n    if len(dependencies) != 1:\n      raise TargetDefinitionException(self, 'An app must define exactly one binary '\n                                            'dependency, have: {}'.format(dependencies))\n    binary = dependencies[0]\n    if not isinstance(binary, self.binary_target_type()):\n      raise TargetDefinitionException(self, 'Expected binary dependency to be a {} '\n                                            'target, found {}'.format(self.binary_target_type(),\n                                                                      binary))\n    return binary", "response": "Returns the binary this target references."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a shallow copy of this UnionProducts.", "response": "def copy(self):\n    \"\"\"Returns a copy of this UnionProducts.\n\n    Edits to the copy's mappings will not affect the product mappings in the original.\n    The copy is shallow though, so edits to the copy's product values will mutate the original's\n    product values.\n\n    :API: public\n\n    :rtype: :class:`UnionProducts`\n    \"\"\"\n    products_by_target = defaultdict(OrderedSet)\n    for key, value in self._products_by_target.items():\n      products_by_target[key] = OrderedSet(value)\n    return UnionProducts(products_by_target=products_by_target)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_for_targets(self, targets, products):\n    # TODO: This is a temporary helper for use until the classpath has been split.\n    for target in targets:\n      self.add_for_target(target, products)", "response": "Updates the products for the given targets."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_for_target(self, target, products):\n    for product in products:\n      self._products_by_target[target].discard(product)", "response": "Updates the products for a particular target removing the given existing entries."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the union of the products for the given targets preserving the input order.", "response": "def get_for_targets(self, targets):\n    \"\"\"Gets the union of the products for the given targets, preserving the input order.\n\n    :API: public\n    \"\"\"\n    products = OrderedSet()\n    for target in targets:\n      products.update(self._products_by_target[target])\n    return products"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the product - target associations for the given targets preserving the input order.", "response": "def get_product_target_mappings_for_targets(self, targets):\n    \"\"\"Gets the product-target associations for the given targets, preserving the input order.\n\n    :API: public\n\n    :param targets: The targets to lookup products for.\n    :returns: The ordered (product, target) tuples.\n    \"\"\"\n    product_target_mappings = []\n    for target in targets:\n      for product in self._products_by_target[target]:\n        product_target_mappings.append((product, target))\n\n    return product_target_mappings"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlook up the target key for a product.", "response": "def target_for_product(self, product):\n    \"\"\"Looks up the target key for a product.\n\n    :API: public\n\n    :param product: The product to search for\n    :return: None if there is no target for the product\n    \"\"\"\n    for target, products in self._products_by_target.items():\n      if product in products:\n        return target\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_abs_paths(self, abs_paths):\n    for abs_path in abs_paths:\n      self._rel_paths.add(fast_relpath(abs_path, self._root))", "response": "Adds the absolute paths to the internal list of relative paths."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef abs_paths(self):\n    for relpath in self._rel_paths:\n      yield os.path.join(self._root, relpath)", "response": "Yields absolute paths of all relative paths in the base directory."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nyielding a generator of all relative paths of all rooted products.", "response": "def rel_paths(self):\n    \"\"\"\n    :API: public\n    \"\"\"\n    for root, products in self._rooted_products_by_root.items():\n      yield root, products.rel_paths()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nyielding a generator of absolute paths for each rooted product.", "response": "def abs_paths(self):\n    \"\"\"\n    :API: public\n    \"\"\"\n    for root, products in self._rooted_products_by_root.items():\n      yield root, products.abs_paths()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef register_data(self, typename, value):\n    if typename in self.data_products:\n      raise ProductError('Already have a product registered for {}, cannot over-write with {}'\n                         .format(typename, value))\n    return self.safe_create_data(typename, lambda: value)", "response": "Registers a data product with the given value."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a data product.", "response": "def get_data(self, typename, init_func=None):\n    \"\"\"Returns a data product.\n\n    :API: public\n\n    If the product isn't found, returns None, unless init_func is set, in which case the product's\n    value is set to the return value of init_func(), and returned.\n    \"\"\"\n    if typename not in self.data_products:\n      if not init_func:\n        return None\n      self.data_products[typename] = init_func()\n    return self.data_products.get(typename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_only(self, product_type, target):\n    product_mapping = self.get(product_type).get(target)\n    if len(product_mapping) != 1:\n      raise ProductError('{} directories in product mapping: requires exactly 1.'\n                         .format(len(product_mapping)))\n\n    for _, files in product_mapping.items():\n      if len(files) != 1:\n        raise ProductError('{} files in target directory: requires exactly 1.'\n                           .format(len(files)))\n\n      return files[0]", "response": "Returns the full filepath of a single product for the given product type and target."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the binary that builds the pex for this lambda.", "response": "def binary(self):\n    \"\"\"Returns the binary that builds the pex for this lambda.\"\"\"\n    dependencies = self.dependencies\n    if len(dependencies) != 1:\n      raise TargetDefinitionException(self, 'An app must define exactly one binary '\n                                            'dependency, have: {}'.format(dependencies))\n    binary = dependencies[0]\n    if not isinstance(binary, PythonBinary):\n      raise TargetDefinitionException(self, 'Expected binary dependency to be a python_binary '\n                                            'target, found {}'.format(binary))\n    return binary"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads named plugins and source backends ethernet.", "response": "def load_backends_and_plugins(plugins, working_set, backends, build_configuration=None):\n  \"\"\"Load named plugins and source backends\n\n  :param list<str> plugins: Plugins to load (see `load_plugins`).  Plugins are loaded after\n    backends.\n  :param WorkingSet working_set: A pkg_resources.WorkingSet to load plugins from.\n  :param list<str> backends: Source backends to load (see `load_build_configuration_from_source`).\n  \"\"\"\n  build_configuration = build_configuration or BuildConfiguration()\n  load_build_configuration_from_source(build_configuration, backends)\n  load_plugins(build_configuration, plugins or [], working_set)\n  return build_configuration"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_build_configuration_from_source(build_configuration, backends=None):\n  # pants.build_graph and pants.core_task must always be loaded, and before any other backends.\n  # TODO: Consider replacing the \"backend\" nomenclature here. pants.build_graph and\n  # pants.core_tasks aren't really backends.\n  backend_packages = OrderedSet(['pants.build_graph', 'pants.core_tasks'] + (backends or []))\n  for backend_package in backend_packages:\n    load_backend(build_configuration, backend_package)", "response": "Installs pants backend packages to provide BUILD file symbols and cli goals."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads the given backend package into the given build configuration.", "response": "def load_backend(build_configuration, backend_package):\n  \"\"\"Installs the given backend package into the build configuration.\n\n  :param build_configuration the :class:``pants.build_graph.build_configuration.BuildConfiguration`` to\n    install the backend plugin into.\n  :param string backend_package: the package name containing the backend plugin register module that\n    provides the plugin entrypoints.\n  :raises: :class:``pants.base.exceptions.BuildConfigurationError`` if there is a problem loading\n    the build configuration.\"\"\"\n  backend_module = backend_package + '.register'\n  try:\n    module = importlib.import_module(backend_module)\n  except ImportError as e:\n    traceback.print_exc()\n    raise BackendConfigurationError('Failed to load the {backend} backend: {error}'\n                                    .format(backend=backend_module, error=e))\n\n  def invoke_entrypoint(name):\n    entrypoint = getattr(module, name, lambda: None)\n    try:\n      return entrypoint()\n    except TypeError as e:\n      traceback.print_exc()\n      raise BackendConfigurationError(\n          'Entrypoint {entrypoint} in {backend} must be a zero-arg callable: {error}'\n          .format(entrypoint=name, backend=backend_module, error=e))\n\n  build_file_aliases = invoke_entrypoint('build_file_aliases')\n  if build_file_aliases:\n    build_configuration.register_aliases(build_file_aliases)\n\n  subsystems = invoke_entrypoint('global_subsystems')\n  if subsystems:\n    build_configuration.register_optionables(subsystems)\n\n  rules = invoke_entrypoint('rules')\n  if rules:\n    build_configuration.register_rules(rules)\n\n  invoke_entrypoint('register_goals')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the sources attribute of the target if exists or the default sources if defined.", "response": "def get_sources(self):\n    \"\"\"Returns target's non-deferred sources if exists or the default sources if defined.\n\n    :rtype: :class:`GlobsWithConjunction`\n\n    NB: once ivy is implemented in the engine, we can fetch sources natively here, and/or\n    refactor how deferred sources are implemented.\n      see: https://github.com/pantsbuild/pants/issues/2997\n    \"\"\"\n    source = getattr(self, 'source', None)\n    sources = getattr(self, 'sources', None)\n\n    if source is not None and sources is not None:\n      raise Target.IllegalArgument(\n        self.address.spec,\n        'Cannot specify both source and sources attribute.'\n      )\n\n    if source is not None:\n      if not isinstance(source, string_types):\n        raise Target.IllegalArgument(\n          self.address.spec,\n          'source must be a string containing a path relative to the target, but got {} of type {}'\n            .format(source, type(source))\n        )\n      sources = [source]\n\n    # N.B. Here we check specifically for `sources is None`, as it's possible for sources\n    # to be e.g. an explicit empty list (sources=[]).\n    if sources is None:\n      if self.default_sources_globs is not None:\n        globs = Globs(*self.default_sources_globs,\n                      spec_path=self.address.spec_path,\n                      exclude=self.default_sources_exclude_globs or [])\n        conjunction_globs = GlobsWithConjunction(globs, GlobExpansionConjunction.any_match)\n      else:\n        globs = None\n        conjunction_globs = None\n    else:\n      globs = BaseGlobs.from_sources_field(sources, self.address.spec_path)\n      conjunction_globs = GlobsWithConjunction(globs, GlobExpansionConjunction.all_match)\n\n    return conjunction_globs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef field_adaptors(self):\n    with exception_logging(logger, 'Exception in `field_adaptors` property'):\n      conjunction_globs = self.get_sources()\n\n      if conjunction_globs is None:\n        return tuple()\n\n      sources = conjunction_globs.non_path_globs\n      conjunction = conjunction_globs.conjunction\n\n      if not sources:\n        return tuple()\n      base_globs = BaseGlobs.from_sources_field(sources, self.address.spec_path)\n      path_globs = base_globs.to_path_globs(self.address.spec_path, conjunction)\n\n      return (SourcesField(\n        self.address,\n        'sources',\n        base_globs.filespecs,\n        base_globs,\n        path_globs,\n        self.validate_sources,\n      ),)", "response": "Returns a tuple of Fields for captured fields which need additional treatment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a BaseGlobs instance for the given sources field.", "response": "def from_sources_field(sources, spec_path):\n    \"\"\"Return a BaseGlobs for the given sources field.\n\n    `sources` may be None, a list/tuple/set, a string or a BaseGlobs instance.\n    \"\"\"\n    if sources is None:\n      return Files(spec_path=spec_path)\n    elif isinstance(sources, BaseGlobs):\n      return sources\n    elif isinstance(sources, string_types):\n      return Files(sources, spec_path=spec_path)\n    elif isinstance(sources, (MutableSet, MutableSequence, tuple)) and \\\n         all(isinstance(s, string_types) for s in sources):\n      return Files(*sources, spec_path=spec_path)\n    else:\n      raise ValueError('Expected either a glob or list of literal sources: got: {}'.format(sources))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a filespecs dict representing both globs and excludes.", "response": "def filespecs(self):\n    \"\"\"Return a filespecs dict representing both globs and excludes.\"\"\"\n    filespecs = {'globs': self._file_globs}\n    exclude_filespecs = self._exclude_filespecs\n    if exclude_filespecs:\n      filespecs['exclude'] = exclude_filespecs\n    return filespecs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a PathGlobs representing the included and excluded Files for these patterns.", "response": "def to_path_globs(self, relpath, conjunction):\n    \"\"\"Return a PathGlobs representing the included and excluded Files for these patterns.\"\"\"\n    return PathGlobs(\n      include=tuple(os.path.join(relpath, glob) for glob in self._file_globs),\n      exclude=tuple(os.path.join(relpath, exclude) for exclude in self._excluded_file_globs),\n      conjunction=conjunction)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _consolidate_classpath(self, targets, classpath_products):\n    # TODO: find a way to not process classpath entries for valid VTs.\n\n    # NB: It is very expensive to call to get entries for each target one at a time.\n    # For performance reasons we look them all up at once.\n    entries_map = defaultdict(list)\n    for (cp, target) in classpath_products.get_product_target_mappings_for_targets(targets, True):\n      entries_map[target].append(cp)\n\n    with self.invalidated(targets=targets, invalidate_dependents=True) as invalidation:\n      for vt in invalidation.all_vts:\n        entries = entries_map.get(vt.target, [])\n        for index, (conf, entry) in enumerate(entries):\n          if ClasspathUtil.is_dir(entry.path):\n            jarpath = os.path.join(vt.results_dir, 'output-{}.jar'.format(index))\n\n            # Regenerate artifact for invalid vts.\n            if not vt.valid:\n              with self.open_jar(jarpath, overwrite=True, compressed=False) as jar:\n                jar.write(entry.path)\n\n            # Replace directory classpath entry with its jarpath.\n            classpath_products.remove_for_target(vt.target, [(conf, entry.path)])\n            classpath_products.add_for_target(vt.target, [(conf, jarpath)])", "response": "Consolidate all classpath entries for the given targets."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _convert(val, acceptable_types):\n  if isinstance(val, acceptable_types):\n    return val\n  return parse_expression(val, acceptable_types, raise_type=ParseError)", "response": "Ensure that val is one of the acceptable types."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmerge components into a single component applying their actions appropriately.", "response": "def merge(cls, components):\n    \"\"\"Merges components into a single component, applying their actions appropriately.\n\n    This operation is associative:  M(M(a, b), c) == M(a, M(b, c)) == M(a, b, c).\n\n    :param list components: an iterable of instances of ListValueComponent.\n    :return: An instance representing the result of merging the components.\n    :rtype: `ListValueComponent`\n    \"\"\"\n    # Note that action of the merged component is MODIFY until the first REPLACE is encountered.\n    # This guarantees associativity.\n    action = cls.MODIFY\n    appends = []\n    filters = []\n    for component in components:\n      if component._action is cls.REPLACE:\n        appends = component._appends\n        filters = component._filters\n        action = cls.REPLACE\n      elif component._action is cls.MODIFY:\n        appends.extend(component._appends)\n        filters.extend(component._filters)\n      else:\n        raise ParseError('Unknown action for list value: {}'.format(component.action))\n    return cls(action, appends, filters)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninterpret value as either a list or something to extend another list with.", "response": "def create(cls, value):\n    \"\"\"Interpret value as either a list or something to extend another list with.\n\n    Note that we accept tuple literals, but the internal value is always a list.\n\n    :param value: The value to convert.  Can be an instance of ListValueComponent, a list, a tuple,\n                  a string representation of a list or tuple (possibly prefixed by + or -\n                  indicating modification instead of replacement), or any allowed member_type.\n                  May also be a comma-separated sequence of modifications.\n    :rtype: `ListValueComponent`\n    \"\"\"\n    if isinstance(value, bytes):\n      value = value.decode('utf-8')\n\n    if isinstance(value, str):\n      comma_separated_exprs = cls._split_modifier_expr(value)\n      if len(comma_separated_exprs) > 1:\n        return cls.merge([cls.create(x) for x in comma_separated_exprs])\n\n    action = cls.MODIFY\n    appends = []\n    filters = []\n    if isinstance(value, cls):  # Ensure idempotency.\n      action = value._action\n      appends = value._appends\n      filters = value._filters\n    elif isinstance(value, (list, tuple)):  # Ensure we can handle list-typed default values.\n      action = cls.REPLACE\n      appends = value\n    elif value.startswith('[') or value.startswith('('):\n      action = cls.REPLACE\n      appends = _convert(value, (list, tuple))\n    elif value.startswith('+[') or value.startswith('+('):\n      appends = _convert(value[1:], (list, tuple))\n    elif value.startswith('-[') or value.startswith('-('):\n      filters = _convert(value[1:], (list, tuple))\n    elif isinstance(value, str):\n      appends = [value]\n    else:\n      appends = _convert('[{}]'.format(value), list)\n    return cls(action, list(appends), list(filters))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmerge components into a single component applying their actions appropriately.", "response": "def merge(cls, components):\n    \"\"\"Merges components into a single component, applying their actions appropriately.\n\n    This operation is associative:  M(M(a, b), c) == M(a, M(b, c)) == M(a, b, c).\n\n    :param list components: an iterable of instances of DictValueComponent.\n    :return: An instance representing the result of merging the components.\n    :rtype: `DictValueComponent`\n    \"\"\"\n    # Note that action of the merged component is EXTEND until the first REPLACE is encountered.\n    # This guarantees associativity.\n    action = cls.EXTEND\n    val = {}\n    for component in components:\n      if component.action is cls.REPLACE:\n        val = component.val\n        action = cls.REPLACE\n      elif component.action is cls.EXTEND:\n        val.update(component.val)\n      else:\n        raise ParseError('Unknown action for dict value: {}'.format(component.action))\n    return cls(action, val)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create(cls, value):\n    if isinstance(value, bytes):\n      value = value.decode('utf-8')\n    if isinstance(value, cls):  # Ensure idempotency.\n      action = value.action\n      val = value.val\n    elif isinstance(value, dict):  # Ensure we can handle dict-typed default values.\n      action = cls.REPLACE\n      val = value\n    elif value.startswith('{'):\n      action = cls.REPLACE\n      val = _convert(value, dict)\n    elif value.startswith('+{'):\n      action = cls.EXTEND\n      val = _convert(value[1:], dict)\n    else:\n      raise ParseError('Invalid dict value: {}'.format(value))\n    return cls(action, dict(val))", "response": "Interpret value as either a dict or a string representation of a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extend(self, **kwargs):\n\n    props = self.copy()\n    props.update(kwargs)\n    return TemplateData(**props)", "response": "Returns a new instance with this instance s data overlayed by the key - value args."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef github(cls, user, repo):\n    # For the url format, see: http://maven.apache.org/scm/git.html\n    params = dict(user=user, repo=repo)\n    connection = 'scm:git:git@github.com:{user}/{repo}.git'.format(**params)\n    url = 'https://github.com/{user}/{repo}'.format(**params)\n    return cls(connection=connection, developer_connection=connection, url=url)", "response": "Creates a Scm for a github repo."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new Scm identical to this Scm but with the given tag.", "response": "def tagged(self, tag):\n    \"\"\"Creates a new `Scm` identical to this `Scm` but with the given `tag`.\"\"\"\n    return Scm(self.connection, self.developer_connection, self.url, tag=tag)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd specified cookies to our cookie jar and persists it.", "response": "def update(self, cookies):\n    \"\"\"Add specified cookies to our cookie jar, and persists it.\n\n    :param cookies: Any iterable that yields http.cookiejar.Cookie instances, such as a CookieJar.\n    \"\"\"\n    cookie_jar = self.get_cookie_jar()\n    for cookie in cookies:\n      cookie_jar.set_cookie(cookie)\n    with self._lock:\n      cookie_jar.save()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_cookie_jar(self):\n    cookie_file = self._get_cookie_file()\n    cookie_jar = LWPCookieJar(cookie_file)\n    if os.path.exists(cookie_file):\n      cookie_jar.load()\n    else:\n      safe_mkdir_for(cookie_file)\n      # Save an empty cookie jar so we can change the file perms on it before writing data to it.\n      with self._lock:\n        cookie_jar.save()\n      os.chmod(cookie_file, 0o600)\n    return cookie_jar", "response": "Returns our cookie jar."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate(self, local_go_targets):\n    visited = {l.import_path: l.address for l in local_go_targets}\n    with temporary_dir() as gopath:\n      for local_go_target in local_go_targets:\n        deps = self._list_deps(gopath, local_go_target.address)\n        self._generate_missing(gopath, local_go_target.address, deps, visited)\n    return list(visited.items())", "response": "Automatically generates a Go target graph for the given local go targets."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate Go targets in memory to form a complete target graph.", "response": "def generate_targets(self, local_go_targets=None):\n    \"\"\"Generate Go targets in memory to form a complete Go graph.\n\n    :param local_go_targets: The local Go targets to fill in a complete target graph for.  If\n                             `None`, then all local Go targets under the Go source root are used.\n    :type local_go_targets: :class:`collections.Iterable` of\n                            :class:`pants.contrib.go.targets.go_local_source import GoLocalSource`\n    :returns: A generation result if targets were generated, else `None`.\n    :rtype: :class:`GoBuildgen.GenerationResult`\n    \"\"\"\n    # TODO(John Sirois): support multiple source roots like GOPATH does?\n    # The GOPATH's 1st element is read-write, the rest are read-only; ie: their sources build to\n    # the 1st element's pkg/ and bin/ dirs.\n\n    go_roots_by_category = defaultdict(list)\n    # TODO: Add \"find source roots for lang\" functionality to SourceRoots and use that instead.\n    for sr in self.context.source_roots.all_roots():\n      if 'go' in sr.langs:\n        go_roots_by_category[sr.category].append(sr.path)\n\n    if go_roots_by_category[SourceRootCategories.TEST]:\n      raise self.InvalidLocalRootsError('Go buildgen does not support test source roots.')\n    if go_roots_by_category[SourceRootCategories.UNKNOWN]:\n      raise self.InvalidLocalRootsError('Go buildgen does not support source roots of '\n                                        'unknown category.')\n\n    local_roots = go_roots_by_category[SourceRootCategories.SOURCE]\n    if not local_roots:\n      raise self.NoLocalRootsError('Can only BUILD gen if a Go local sources source root is '\n                                   'defined.')\n    if len(local_roots) > 1:\n      raise self.InvalidLocalRootsError('Can only BUILD gen for a single Go local sources source '\n                                        'root, found:\\n\\t{}'\n                                        .format('\\n\\t'.join(sorted(local_roots))))\n    local_root = local_roots.pop()\n\n    if local_go_targets:\n      unrooted_locals = {t for t in local_go_targets if t.target_base != local_root}\n      if unrooted_locals:\n        raise self.UnrootedLocalSourceError('Cannot BUILD gen until the following targets are '\n                                            'relocated to the source root at {}:\\n\\t{}'\n                                            .format(local_root,\n                                                    '\\n\\t'.join(sorted(t.address.reference()\n                                                                       for t in unrooted_locals))))\n    else:\n      root = os.path.join(get_buildroot(), local_root)\n      local_go_targets = self.context.scan(root=root).targets(self.is_local_src)\n      if not local_go_targets:\n        return None\n\n    remote_roots = go_roots_by_category[SourceRootCategories.THIRDPARTY]\n    if len(remote_roots) > 1:\n      raise self.InvalidRemoteRootsError('Can only BUILD gen for a single Go remote library source '\n                                         'root, found:\\n\\t{}'\n                                         .format('\\n\\t'.join(sorted(remote_roots))))\n    remote_root = remote_roots.pop() if remote_roots else None\n\n    generator = GoTargetGenerator(self.import_oracle,\n                                  self.context.build_graph,\n                                  local_root,\n                                  self.get_fetcher_factory(),\n                                  generate_remotes=self.get_options().remote,\n                                  remote_root=remote_root)\n    with self.context.new_workunit('go.buildgen', labels=[WorkUnitLabel.MULTITOOL]):\n      try:\n        generated = generator.generate(local_go_targets)\n        return self.GenerationResult(generated=generated,\n                                     local_root=local_root,\n                                     remote_root=remote_root)\n      except generator.GenerationError as e:\n        raise self.GenerationError(e)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the pair for the single target matching the single AddressFamily.", "response": "def address_target_pairs_from_address_families(self, address_families):\n    \"\"\"Return the pair for the single target matching the single AddressFamily, or error.\n\n    :raises: :class:`SingleAddress._SingleAddressResolutionError` if no targets could be found for a\n             :class:`SingleAddress` instance.\n    :return: list of (Address, Target) pairs with exactly one element.\n    \"\"\"\n    single_af = assert_single_element(address_families)\n    addr_tgt_pairs = [\n      (addr, tgt) for addr, tgt in single_af.addressables.items()\n      if addr.target_name == self.name\n    ]\n    if len(addr_tgt_pairs) == 0:\n      raise self._SingleAddressResolutionError(single_af, self.name)\n    # There will be at most one target with a given name in a single AddressFamily.\n    assert(len(addr_tgt_pairs) == 1)\n    return addr_tgt_pairs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if the given Address and HydratedTarget are included by this matcher.", "response": "def matches_target_address_pair(self, address, target):\n    \"\"\"\n    :param Address address: An Address to match\n    :param HydratedTarget target: The Target for the address.\n\n    :return: True if the given Address/HydratedTarget are included by this matcher.\n    \"\"\"\n    return self._target_tag_matches(target) and not self._excluded_by_pattern(address)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an ivy instance bootstrapped by this bootstrapper.", "response": "def ivy(self, bootstrap_workunit_factory=None):\n    \"\"\"Returns an ivy instance bootstrapped by this bootstrapper.\n\n    :param bootstrap_workunit_factory: the optional workunit to bootstrap under.\n    :raises: Bootstrapper.Error if ivy could not be bootstrapped\n    \"\"\"\n    return Ivy(self._get_classpath(bootstrap_workunit_factory),\n               ivy_settings=self._ivy_subsystem.get_options().ivy_settings,\n               ivy_resolution_cache_dir=self._ivy_subsystem.resolution_cache_dir(),\n               extra_jvm_options=self._ivy_subsystem.extra_jvm_options())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_classpath(self, workunit_factory):\n    if not self._classpath:\n      self._classpath = self._bootstrap_ivy_classpath(workunit_factory)\n    return self._classpath", "response": "Returns the bootstrapped ivy classpath as a list of jar paths."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the base path for this target.", "response": "def target_base(self):\n    \"\"\"\n    :API: public\n\n    :returns: the source root path for this target.\n    \"\"\"\n    source_root = self._sources_field.source_root\n    if not source_root:\n      raise TargetDefinitionException(self, 'Not under any configured source root.')\n    return source_root.path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a combined id for a set of ids.", "response": "def maybe_readable_combine_ids(cls, ids):\n    \"\"\"Generates combined id for a set of ids, but if the set is a single id, just use that.\n\n    :API: public\n    \"\"\"\n    ids = list(ids)  # We can't len a generator.\n    return ids[0] if len(ids) == 1 else cls.combine_ids(ids)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef closure_for_targets(cls, target_roots, exclude_scopes=None, include_scopes=None,\n                          bfs=None, postorder=None, respect_intransitive=False):\n    \"\"\"Computes the closure of the given targets respecting the given input scopes.\n\n    :API: public\n\n    :param list target_roots: The list of Targets to start from. These targets will always be\n      included in the closure, regardless of scope settings.\n    :param Scope exclude_scopes: If present and non-empty, only dependencies which have none of the\n      scope names in this Scope will be traversed.\n    :param Scope include_scopes: If present and non-empty, only dependencies which have at least one\n      of the scope names in this Scope will be traversed.\n    :param bool bfs: Whether to traverse in breadth-first or depth-first order. (Defaults to True).\n    :param bool respect_intransitive: If True, any dependencies which have the 'intransitive' scope\n      will not be included unless they are direct dependencies of one of the root targets. (Defaults\n      to False).\n    \"\"\"\n    target_roots = list(target_roots) # Sometimes generators are passed into this function.\n    if not target_roots:\n      return OrderedSet()\n\n    build_graph = target_roots[0]._build_graph\n    addresses = [target.address for target in target_roots]\n    dep_predicate = cls._closure_dep_predicate(target_roots,\n                                               include_scopes=include_scopes,\n                                               exclude_scopes=exclude_scopes,\n                                               respect_intransitive=respect_intransitive)\n    closure = OrderedSet()\n\n    if not bfs:\n      build_graph.walk_transitive_dependency_graph(\n        addresses=addresses,\n        work=closure.add,\n        postorder=postorder,\n        dep_predicate=dep_predicate,\n      )\n    else:\n      closure.update(build_graph.transitive_subgraph_of_addresses_bfs(\n        addresses=addresses,\n        dep_predicate=dep_predicate,\n      ))\n\n    # Make sure all the roots made it into the closure.\n    closure.update(target_roots)\n    return closure", "response": "Computes the closure of the given targets."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef assert_list(self, putative_list, expected_type=string_types, key_arg=None):\n    return assert_list(putative_list, expected_type, key_arg=key_arg,\n                       raise_type=lambda msg: TargetDefinitionException(self, msg))", "response": "Assert that the input list is of the expected type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef invalidation_hash(self, fingerprint_strategy=None):\n    fingerprint_strategy = fingerprint_strategy or DefaultFingerprintStrategy()\n    if fingerprint_strategy not in self._cached_fingerprint_map:\n      self._cached_fingerprint_map[fingerprint_strategy] = self.compute_invalidation_hash(fingerprint_strategy)\n    return self._cached_fingerprint_map[fingerprint_strategy]", "response": "Returns the invalidation hash for the given fingerprint strategy."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmarks the invalidation hash dirty for this target.", "response": "def mark_invalidation_hash_dirty(self):\n    \"\"\"Invalidates memoized fingerprints for this target, including those in payloads.\n\n    Exposed for testing.\n\n    :API: public\n    \"\"\"\n    self._cached_fingerprint_map = {}\n    self._cached_all_transitive_fingerprint_map = {}\n    self._cached_direct_transitive_fingerprint_map = {}\n    self._cached_strict_dependencies_map = {}\n    self._cached_exports_addresses = None\n    self.mark_extra_invalidation_hash_dirty()\n    self.payload.mark_dirty()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef transitive_invalidation_hash(self, fingerprint_strategy=None, depth=0):\n    if depth > self._MAX_RECURSION_DEPTH:\n      # NB(zundel) without this catch, we'll eventually hit the python stack limit\n      # RuntimeError: maximum recursion depth exceeded while calling a Python object\n      raise self.RecursiveDepthError(\"Max depth of {} exceeded.\".format(self._MAX_RECURSION_DEPTH))\n\n    fingerprint_strategy = fingerprint_strategy or DefaultFingerprintStrategy()\n\n    direct = (depth == 0 and fingerprint_strategy.direct(self))\n    if direct:\n      fingerprint_map = self._cached_direct_transitive_fingerprint_map\n    else:\n      fingerprint_map = self._cached_all_transitive_fingerprint_map\n\n    if fingerprint_strategy not in fingerprint_map:\n      hasher = sha1()\n\n      def dep_hash_iter():\n        dep_list = fingerprint_strategy.dependencies(self) if direct else self.dependencies\n        for dep in dep_list:\n          try:\n            if direct:\n              dep_hash = dep.invalidation_hash(fingerprint_strategy)\n            else:\n              dep_hash = dep.transitive_invalidation_hash(fingerprint_strategy, depth=depth+1)\n            if dep_hash is not None:\n              yield dep_hash\n          except self.RecursiveDepthError as e:\n            raise self.RecursiveDepthError(\"{message}\\n  referenced from {spec}\"\n                                           .format(message=e, spec=dep.address.spec))\n\n      dep_hashes = sorted(list(dep_hash_iter()))\n      for dep_hash in dep_hashes:\n        hasher.update(dep_hash.encode('utf-8'))\n      target_hash = self.invalidation_hash(fingerprint_strategy)\n      if target_hash is None and not dep_hashes:\n        return None\n      dependencies_hash = hasher.hexdigest()[:12]\n      combined_hash = '{target_hash}.{deps_hash}'.format(target_hash=target_hash,\n                                                         deps_hash=dependencies_hash)\n      fingerprint_map[fingerprint_strategy] = combined_hash\n    return fingerprint_map[fingerprint_strategy]", "response": "Returns the transitive invalidation hash of this target and all of its dependencies."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef inject_dependency(self, dependency_address):\n    self._build_graph.inject_dependency(dependent=self.address, dependency=dependency_address)\n\n    def invalidate_dependee(dependee):\n      dependee.mark_transitive_invalidation_hash_dirty()\n    self._build_graph.walk_transitive_dependee_graph([self.address], work=invalidate_dependee)", "response": "Injects a dependency into the transitive graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef has_sources(self, extension=None):\n    source_paths = self._sources_field.source_paths\n    if not source_paths:\n      return False\n    if not extension:\n      return True\n    return any(source.endswith(extension) for source in source_paths)", "response": "Return True if this target owns sources ; optionally of the given extension."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sources_relative_to_source_root(self):\n    if self.has_sources():\n      abs_source_root = os.path.join(get_buildroot(), self.target_base)\n      for source in self.sources_relative_to_buildroot():\n        abs_source = os.path.join(get_buildroot(), source)\n        yield os.path.relpath(abs_source, abs_source_root)", "response": "Yields the absolute paths of all sources relative to the source root of the target."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef derived_from_chain(self):\n    cur = self\n    while cur.derived_from is not cur:\n      cur = cur.derived_from\n      yield cur", "response": "Returns all targets that this target was derived from another."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dependencies(self):\n    return [self._build_graph.get_target(dep_address)\n            for dep_address in self._build_graph.dependencies_of(self.address)]", "response": "API method that returns the list of targets that this target depends on"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef strict_dependencies(self, dep_context):\n    strict_deps = self._cached_strict_dependencies_map.get(dep_context, None)\n    if strict_deps is None:\n      default_predicate = self._closure_dep_predicate({self}, **dep_context.target_closure_kwargs)\n      # TODO(#5977): this branch needs testing!\n      if not default_predicate:\n        def default_predicate(*args, **kwargs):\n          return True\n\n      def dep_predicate(source, dependency):\n        if not default_predicate(source, dependency):\n          return False\n\n        # Always expand aliases.\n        if type(source) in dep_context.alias_types:\n          return True\n\n        # Traverse other dependencies if they are exported.\n        if source._dep_is_exported(dependency):\n          return True\n        return False\n\n      dep_addresses = [d.address for d in self.dependencies\n                        if default_predicate(self, d)\n                      ]\n      result = self._build_graph.transitive_subgraph_of_addresses_bfs(\n        addresses=dep_addresses,\n        dep_predicate=dep_predicate\n      )\n\n      strict_deps = OrderedSet()\n      for declared in result:\n        if type(declared) in dep_context.alias_types:\n          continue\n        if isinstance(declared, dep_context.types_with_closure):\n          strict_deps.update(declared.closure(\n            bfs=True,\n            **dep_context.target_closure_kwargs))\n        strict_deps.add(declared)\n\n      strict_deps = list(strict_deps)\n      self._cached_strict_dependencies_map[dep_context] = strict_deps\n    return strict_deps", "response": "Returns a list of targets that this target is strictly depends on."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef walk(self, work, predicate=None):\n    if not callable(work):\n      raise ValueError('work must be callable but was {}'.format(work))\n    if predicate and not callable(predicate):\n      raise ValueError('predicate must be callable but was {}'.format(predicate))\n    self._build_graph.walk_transitive_dependency_graph([self.address], work, predicate)", "response": "Walks the dependency graph of this target and returns a candidate set of targets that satisfy the given work."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a destination directory returns a list of tuples of src dst symlink pairs.", "response": "def _tools_classpath_pairs(self, dest_dir):\n    \"\"\"Given a destination directory, returns a list of tuples of (src, dst) symlink pairs.\"\"\"\n    tools_classpath = self._tools_classpath\n    return [(entry, os.path.join(dest_dir, '{}-{}'.format(idx, os.path.basename(entry))))\n            for idx, entry in enumerate(tools_classpath)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a classpath representing the equivalent of the tools. jar.", "response": "def _tools_classpath(self):\n    \"\"\"Returns a classpath representing the (equivalent of the) `tools.jar`.\n\n    If `javac` has been set explicitly, it is used. Otherwise, searches the current distribution.\n    \"\"\"\n\n    javac_classpath = Java.global_javac_classpath(self.context.products)\n    if javac_classpath:\n      return javac_classpath\n\n    self.set_distribution(jdk=True)\n    jars = self.dist.find_libs(['tools.jar'])\n    if len(jars) != 1:\n      raise TaskError('Expected a single `tools.jar` entry for {}; got: {}'.format(\n        self.dist, jars))\n    return jars"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create(self, relpath, langs, category):\n    return SourceRoot(relpath, tuple(self._canonicalize_langs(langs)), category)", "response": "Create a new source root at the given relpath for the given langs and category."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a fixed source root to the trie.", "response": "def add_source_root(self, path, langs=tuple(), category=SourceRootCategories.UNKNOWN):\n    \"\"\"Add the specified fixed source root, which must be relative to the buildroot.\n\n    Useful in a limited set of circumstances, e.g., when unpacking sources from a jar with\n    unknown structure.  Tests should prefer to use dirs that match our source root patterns\n    instead of explicitly setting source roots here.\n    \"\"\"\n    self._trie.add_fixed(path, langs, category)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the source root for the given path.", "response": "def find_by_path(self, path):\n    \"\"\"Find the source root for the given path, or None.\n\n    :param path: Find the source root for this path, relative to the buildroot.\n    :return: A SourceRoot instance, or None if the path is not located under a source root\n             and `unmatched==fail`.\n    \"\"\"\n    matched = self._trie.find(path)\n    if matched:\n      return matched\n    elif self._options.unmatched == 'fail':\n      return None\n    elif self._options.unmatched == 'create':\n      # If no source root is found, use the path directly.\n      # TODO: Remove this logic. It should be an error to have no matching source root.\n      return SourceRoot(path, [], SourceRootCategories.UNKNOWN)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef all_roots(self):\n    project_tree = get_project_tree(self._options)\n\n    fixed_roots = set()\n    for root, langs, category in self._trie.fixed():\n      if project_tree.exists(root):\n        yield self._source_root_factory.create(root, langs, category)\n      fixed_roots.add(root)\n\n    for relpath, dirnames, _ in project_tree.walk('', topdown=True):\n      match = self._trie.find(relpath)\n      if match:\n        if not any(fixed_root.startswith(relpath) for fixed_root in fixed_roots):\n          yield match  # Found a source root not a prefix of any fixed roots.\n        del dirnames[:]", "response": "Return a generator over all source roots."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a trie of source root patterns from options.", "response": "def create_trie(self):\n    \"\"\"Create a trie of source root patterns from options.\n\n    :returns: :class:`SourceRootTrie`\n    \"\"\"\n    trie = SourceRootTrie(self.source_root_factory)\n    options = self.get_options()\n\n    for category in SourceRootCategories.ALL:\n      # Add patterns.\n      for pattern in options.get('{}_root_patterns'.format(category), []):\n        trie.add_pattern(pattern, category)\n      # Add fixed source roots.\n      for path, langs in options.get('{}_roots'.format(category), {}).items():\n        trie.add_fixed(path, langs, category)\n\n    return trie"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_pattern(self, pattern, category=SourceRootCategories.UNKNOWN):\n    self._do_add_pattern(pattern, tuple(), category)", "response": "Add a pattern to the trie."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_fixed(self, path, langs, category=SourceRootCategories.UNKNOWN):\n    if '*' in path:\n      raise self.InvalidPath(path, 'fixed path cannot contain the * character')\n    fixed_path = os.path.join('^', path) if path else '^'\n    self._do_add_pattern(fixed_path, tuple(langs), category)", "response": "Add a fixed source root to the trie."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of just the fixed source roots in the trie.", "response": "def fixed(self):\n    \"\"\"Returns a list of just the fixed source roots in the trie.\"\"\"\n    for key, child in self._root.children.items():\n      if key == '^':\n        return list(child.subpatterns())\n    return []"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind the source root for the given path.", "response": "def find(self, path):\n    \"\"\"Find the source root for the given path.\"\"\"\n    keys = ['^'] + path.split(os.path.sep)\n    for i in range(len(keys)):\n      # See if we have a match at position i.  We have such a match if following the path\n      # segments into the trie, from the root, leads us to a terminal.\n      node = self._root\n      langs = set()\n      j = i\n      while j < len(keys):\n        child = node.get_child(keys[j], langs)\n        if child is None:\n          break\n        else:\n          node = child\n          j += 1\n      if node.is_terminal:\n        if j == 1:  # The match was on the root itself.\n          path = ''\n        else:\n          path = os.path.join(*keys[1:j])\n        return self._source_root_factory.create(path, langs, node.category)\n      # Otherwise, try the next value of i.\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the version of the specified binary tool.", "response": "def version(self, context=None):\n    \"\"\"Returns the version of the specified binary tool.\n\n    If replaces_scope and replaces_name are defined, then the caller must pass in\n    a context, otherwise no context should be passed.\n\n    # TODO: Once we're migrated, get rid of the context arg.\n\n    :API: public\n    \"\"\"\n    if self.replaces_scope and self.replaces_name:\n      if context:\n        # If the old option is provided explicitly, let it take precedence.\n        old_opts = context.options.for_scope(self.replaces_scope)\n        if old_opts.get(self.replaces_name) and not old_opts.is_default(self.replaces_name):\n          return old_opts.get(self.replaces_name)\n      else:\n        logger.warn('Cannot resolve version of {} from deprecated option {} in scope {} without a '\n                    'context!'.format(self._get_name(), self.replaces_name, self.replaces_scope))\n    return self.get_options().version"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef initialize(self, run_tracker, all_options, start_time=None):\n\n    run_id, run_uuid = run_tracker.initialize(all_options)\n    run_dir = os.path.join(self.get_options().reports_dir, run_id)\n\n    html_dir = os.path.join(run_dir, 'html')\n    safe_mkdir(html_dir)\n    relative_symlink(run_dir, os.path.join(self.get_options().reports_dir, 'latest'))\n\n    report = Report()\n\n    # Capture initial console reporting into a buffer. We'll do something with it once\n    # we know what the cmd-line flag settings are.\n    outfile = BytesIO()\n    errfile = BytesIO()\n    capturing_reporter_settings = PlainTextReporter.Settings(\n      outfile=outfile, errfile=errfile, log_level=Report.INFO,\n      color=False, indent=True, timing=False,\n      cache_stats=False,\n      label_format=self.get_options().console_label_format,\n      tool_output_format=self.get_options().console_tool_output_format)\n    capturing_reporter = PlainTextReporter(run_tracker, capturing_reporter_settings)\n    report.add_reporter('capturing', capturing_reporter)\n\n    # Set up HTML reporting. We always want that.\n    html_reporter_settings = HtmlReporter.Settings(log_level=Report.INFO,\n                                                   html_dir=html_dir,\n                                                   template_dir=self.get_options().template_dir)\n    html_reporter = HtmlReporter(run_tracker, html_reporter_settings)\n    report.add_reporter('html', html_reporter)\n\n    # Set up Zipkin reporting.\n    zipkin_endpoint = self.get_options().zipkin_endpoint\n    trace_id = self.get_options().zipkin_trace_id\n    parent_id = self.get_options().zipkin_parent_id\n    sample_rate = self.get_options().zipkin_sample_rate\n\n    if zipkin_endpoint is None and trace_id is not None and parent_id is not None:\n      raise ValueError(\n        \"The zipkin-endpoint flag must be set if zipkin-trace-id and zipkin-parent-id flags are given.\"\n      )\n    if (trace_id is None) != (parent_id is None):\n      raise ValueError(\n        \"Flags zipkin-trace-id and zipkin-parent-id must both either be set or not set.\"\n      )\n\n    # If trace_id isn't set by a flag, use UUID from run_id\n    if trace_id is None:\n      trace_id = run_uuid\n\n    if trace_id and (len(trace_id) != 16 and len(trace_id) != 32 or not is_hex_string(trace_id)):\n      raise ValueError(\n        \"Value of the flag zipkin-trace-id must be a 16-character or 32-character hex string. \"\n        + \"Got {}.\".format(trace_id)\n      )\n    if parent_id and (len(parent_id) != 16 or not is_hex_string(parent_id)):\n      raise ValueError(\n        \"Value of the flag zipkin-parent-id must be a 16-character hex string. \"\n        + \"Got {}.\".format(parent_id)\n      )\n\n    if zipkin_endpoint is not None:\n      zipkin_reporter_settings = ZipkinReporter.Settings(log_level=Report.INFO)\n      zipkin_reporter = ZipkinReporter(\n        run_tracker, zipkin_reporter_settings, zipkin_endpoint, trace_id, parent_id, sample_rate\n      )\n      report.add_reporter('zipkin', zipkin_reporter)\n\n    # Add some useful RunInfo.\n    run_tracker.run_info.add_info('default_report', html_reporter.report_path())\n    port = ReportingServerManager().socket\n    if port:\n      run_tracker.run_info.add_info('report_url', 'http://localhost:{}/run/{}'.format(port, run_id))\n\n    # And start tracking the run.\n    run_tracker.start(report, start_time)", "response": "Initialize with the given RunTracker."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_reporting(self, global_options, is_quiet, run_tracker):\n\n    # Get any output silently buffered in the old console reporter, and remove it.\n    removed_reporter = run_tracker.report.remove_reporter('capturing')\n    buffered_out = self._consume_stringio(removed_reporter.settings.outfile)\n    buffered_err = self._consume_stringio(removed_reporter.settings.errfile)\n\n    log_level = Report.log_level_from_string(global_options.level or 'info')\n    # Ideally, we'd use terminfo or somesuch to discover whether a\n    # terminal truly supports color, but most that don't set TERM=dumb.\n    color = global_options.colors and (os.getenv('TERM') != 'dumb')\n    timing = global_options.time\n    cache_stats = global_options.time  # TODO: Separate flag for this?\n\n    if is_quiet:\n      console_reporter = QuietReporter(run_tracker,\n                                       QuietReporter.Settings(log_level=log_level, color=color,\n                                                              timing=timing, cache_stats=cache_stats))\n    else:\n      # Set up the new console reporter.\n      stdout = sys.stdout.buffer if PY3 else sys.stdout\n      stderr = sys.stderr.buffer if PY3 else sys.stderr\n      settings = PlainTextReporter.Settings(log_level=log_level, outfile=stdout, errfile=stderr,\n                                            color=color, indent=True, timing=timing, cache_stats=cache_stats,\n                                            label_format=self.get_options().console_label_format,\n                                            tool_output_format=self.get_options().console_tool_output_format)\n      console_reporter = PlainTextReporter(run_tracker, settings)\n      console_reporter.emit(buffered_out, dest=ReporterDestination.OUT)\n      console_reporter.emit(buffered_err, dest=ReporterDestination.ERR)\n      console_reporter.flush()\n    run_tracker.report.add_reporter('console', console_reporter)\n\n    if global_options.logdir:\n      # Also write plaintext logs to a file. This is completely separate from the html reports.\n      safe_mkdir(global_options.logdir)\n      run_id = run_tracker.run_info.get_info('id')\n      outfile = open(os.path.join(global_options.logdir, '{}.log'.format(run_id)), 'wb')\n      errfile = open(os.path.join(global_options.logdir, '{}.err.log'.format(run_id)), 'wb')\n      settings = PlainTextReporter.Settings(log_level=log_level, outfile=outfile, errfile=errfile,\n                                            color=False, indent=True, timing=True, cache_stats=True,\n                                            label_format=self.get_options().console_label_format,\n                                            tool_output_format=self.get_options().console_tool_output_format)\n      logfile_reporter = PlainTextReporter(run_tracker, settings)\n      logfile_reporter.emit(buffered_out, dest=ReporterDestination.OUT)\n      logfile_reporter.emit(buffered_err, dest=ReporterDestination.ERR)\n      logfile_reporter.flush()\n      run_tracker.report.add_reporter('logfile', logfile_reporter)\n\n    invalidation_report = self._get_invalidation_report()\n    if invalidation_report:\n      run_id = run_tracker.run_info.get_info('id')\n      outfile = os.path.join(self.get_options().reports_dir, run_id, 'invalidation-report.csv')\n      invalidation_report.set_filename(outfile)\n\n    return invalidation_report", "response": "Updates the reporting config once we ve parsed cmd - line flags."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntimes a single roundtrip to a given url.", "response": "def ping(self, url):\n    \"\"\"Time a single roundtrip to the url.\n\n    :param url to ping.\n    :returns: the fastest ping time for a given netloc and number of tries.\n    or Pinger.UNREACHABLE if ping times out.\n    :rtype: float\n\n    Note that we don't use actual ICMP pings, because cmd-line ping is\n    inflexible and platform-dependent, so shelling out to it is annoying,\n    and the ICMP python lib can only be called by the superuser.\n    \"\"\"\n    return self._get_ping_time(url, self._timeout, self._tries)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef select_best_url(self):\n\n    best_url = self.parsed_urls[0]\n    try:\n      yield best_url\n    except Exception:\n      self.unsuccessful_calls[best_url] += 1\n\n      # Not thread-safe but pool used by cache is based on subprocesses, therefore no race.\n      if self.unsuccessful_calls[best_url] > self.max_failures:\n        self.parsed_urls.rotate(-1)\n        self.unsuccessful_calls[best_url] = 0\n      raise\n    else:\n      self.unsuccessful_calls[best_url] = 0", "response": "Select the best url from the list."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reset_log_location(cls, new_log_location):\n    # We could no-op here if the log locations are the same, but there's no reason not to have the\n    # additional safety of re-acquiring file descriptors each time (and erroring out early if the\n    # location is no longer writable).\n\n    # Create the directory if possible, or raise if not writable.\n    cls._check_or_create_new_destination(new_log_location)\n\n    pid_specific_error_stream, shared_error_stream = cls._recapture_fatal_error_log_streams(\n      new_log_location)\n\n    # NB: mutate process-global state!\n    if faulthandler.is_enabled():\n      logger.debug('re-enabling faulthandler')\n      # Call Py_CLEAR() on the previous error stream:\n      # https://github.com/vstinner/faulthandler/blob/master/faulthandler.c\n      faulthandler.disable()\n    # Send a stacktrace to this file if interrupted by a fatal error.\n    faulthandler.enable(file=pid_specific_error_stream, all_threads=True)\n\n    # NB: mutate the class variables!\n    cls._log_dir = new_log_location\n    cls._pid_specific_error_fileobj = pid_specific_error_stream\n    cls._shared_error_fileobj = shared_error_stream", "response": "Re - acquire file handles to error logs based on the new location."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreset the exception handler to the given Exiter.", "response": "def reset_exiter(cls, exiter):\n    \"\"\"\n    Class state:\n    - Overwrites `cls._exiter`.\n    Python state:\n    - Overwrites sys.excepthook.\n    \"\"\"\n    assert(isinstance(exiter, Exiter))\n    # NB: mutate the class variables! This is done before mutating the exception hook, because the\n    # uncaught exception handler uses cls._exiter to exit.\n    cls._exiter = exiter\n    # NB: mutate process-global state!\n    sys.excepthook = cls._log_unhandled_exception_and_exit"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reset_interactive_output_stream(\n    cls,\n    interactive_output_stream,\n    override_faulthandler_destination=True\n  ):\n    \"\"\"\n    Class state:\n    - Overwrites `cls._interactive_output_stream`.\n    OS state:\n    - Overwrites the SIGUSR2 handler.\n\n    This method registers a SIGUSR2 handler, which permits a non-fatal `kill -31 <pants pid>` for\n    stacktrace retrieval. This is also where the the error message on fatal exit will be printed to.\n    \"\"\"\n    try:\n      # NB: mutate process-global state!\n      # This permits a non-fatal `kill -31 <pants pid>` for stacktrace retrieval.\n      if override_faulthandler_destination:\n        faulthandler.register(signal.SIGUSR2, interactive_output_stream,\n                              all_threads=True, chain=False)\n      # NB: mutate the class variables!\n      cls._interactive_output_stream = interactive_output_stream\n    except ValueError:\n      # Warn about \"ValueError: IO on closed file\" when the stream is closed.\n      cls.log_exception(\n        \"Cannot reset interactive_output_stream -- stream (probably stderr) is closed\")", "response": "Reset the interactive output stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef exceptions_log_path(cls, for_pid=None, in_dir=None):\n    if for_pid is None:\n      intermediate_filename_component = ''\n    else:\n      assert(isinstance(for_pid, IntegerForPid))\n      intermediate_filename_component = '.{}'.format(for_pid)\n    in_dir = in_dir or cls._log_dir\n    return os.path.join(\n      in_dir,\n      'logs',\n      'exceptions{}.log'.format(intermediate_filename_component))", "response": "Get the path to either the shared or pid - specific fatal errors log file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntries to log an exception message to this process s error log and the shared error log.", "response": "def log_exception(cls, msg):\n    \"\"\"Try to log an error message to this process's error log and the shared error log.\n\n    NB: Doesn't raise (logs an error instead).\n    \"\"\"\n    pid = os.getpid()\n    fatal_error_log_entry = cls._format_exception_message(msg, pid)\n\n    # We care more about this log than the shared log, so write to it first.\n    try:\n      cls._try_write_with_flush(cls._pid_specific_error_fileobj, fatal_error_log_entry)\n    except Exception as e:\n      logger.error(\n        \"Error logging the message '{}' to the pid-specific file handle for {} at pid {}:\\n{}\"\n        .format(msg, cls._log_dir, pid, e))\n\n    # Write to the shared log.\n    try:\n      # TODO: we should probably guard this against concurrent modification by other pants\n      # subprocesses somehow.\n      cls._try_write_with_flush(cls._shared_error_fileobj, fatal_error_log_entry)\n    except Exception as e:\n      logger.error(\n        \"Error logging the message '{}' to the shared file handle for {} at pid {}:\\n{}\"\n        .format(msg, cls._log_dir, pid, e))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreset the signal handler for SIGINT SIGQUIT and SIGTERM.", "response": "def reset_signal_handler(cls, signal_handler):\n    \"\"\"\n    Class state:\n    - Overwrites `cls._signal_handler`.\n    OS state:\n    - Overwrites signal handlers for SIGINT, SIGQUIT, and SIGTERM.\n\n    :returns: The :class:`SignalHandler` that was previously registered, or None if this is\n              the first time this method was called.\n    \"\"\"\n    assert(isinstance(signal_handler, SignalHandler))\n    # NB: Modify process-global state!\n    for signum, handler in signal_handler.signal_handler_mapping.items():\n      signal.signal(signum, handler)\n      # Retry any system calls interrupted by any of the signals we just installed handlers for\n      # (instead of having them raise EINTR). siginterrupt(3) says this is the default behavior on\n      # Linux and OSX.\n      signal.siginterrupt(signum, False)\n\n    previous_signal_handler = cls._signal_handler\n    # NB: Mutate the class variables!\n    cls._signal_handler = signal_handler\n    return previous_signal_handler"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef trapped_signals(cls, new_signal_handler):\n    try:\n      previous_signal_handler = cls.reset_signal_handler(new_signal_handler)\n      yield\n    finally:\n      cls.reset_signal_handler(previous_signal_handler)", "response": "A contextmanager which temporarily overrides signal handling."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _log_unhandled_exception_and_exit(cls, exc_class=None, exc=None, tb=None, add_newline=False):\n    exc_class = exc_class or sys.exc_info()[0]\n    exc = exc or sys.exc_info()[1]\n    tb = tb or sys.exc_info()[2]\n\n    # This exception was raised by a signal handler with the intent to exit the program.\n    if exc_class == SignalHandler.SignalHandledNonLocalExit:\n      return cls._handle_signal_gracefully(exc.signum, exc.signame, exc.traceback_lines)\n\n    extra_err_msg = None\n    try:\n      # Always output the unhandled exception details into a log file, including the traceback.\n      exception_log_entry = cls._format_unhandled_exception_log(exc, tb, add_newline,\n                                                                should_print_backtrace=True)\n      cls.log_exception(exception_log_entry)\n    except Exception as e:\n      extra_err_msg = 'Additional error logging unhandled exception {}: {}'.format(exc, e)\n      logger.error(extra_err_msg)\n\n    # Generate an unhandled exception report fit to be printed to the terminal (respecting the\n    # Exiter's should_print_backtrace field).\n    stderr_printed_error = cls._format_unhandled_exception_log(\n      exc, tb, add_newline,\n      should_print_backtrace=cls._should_print_backtrace_to_terminal)\n    if extra_err_msg:\n      stderr_printed_error = '{}\\n{}'.format(stderr_printed_error, extra_err_msg)\n    cls._exit_with_failure(stderr_printed_error)", "response": "A sys. excepthook implementation which logs the error and exits with failure."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsignaling handler for non - fatal signals which raises or logs an error and exits with failure.", "response": "def _handle_signal_gracefully(cls, signum, signame, traceback_lines):\n    \"\"\"Signal handler for non-fatal signals which raises or logs an error and exits with failure.\"\"\"\n    # Extract the stack, and format an entry to be written to the exception log.\n    formatted_traceback = cls._format_traceback(traceback_lines=traceback_lines,\n                                                should_print_backtrace=True)\n    signal_error_log_entry = cls._CATCHABLE_SIGNAL_ERROR_LOG_FORMAT.format(\n      signum=signum,\n      signame=signame,\n      formatted_traceback=formatted_traceback)\n    # TODO: determine the appropriate signal-safe behavior here (to avoid writing to our file\n    # descriptors re-entrantly, which raises an IOError).\n    # This method catches any exceptions raised within it.\n    cls.log_exception(signal_error_log_entry)\n\n    # Create a potentially-abbreviated traceback for the terminal or other interactive stream.\n    formatted_traceback_for_terminal = cls._format_traceback(\n      traceback_lines=traceback_lines,\n      should_print_backtrace=cls._should_print_backtrace_to_terminal)\n    terminal_log_entry = cls._CATCHABLE_SIGNAL_ERROR_LOG_FORMAT.format(\n      signum=signum,\n      signame=signame,\n      formatted_traceback=formatted_traceback_for_terminal)\n    # Exit, printing the output to the terminal.\n    cls._exit_with_failure(terminal_log_entry)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles stdio redirection in the case of all stdio descriptors being the same tty.", "response": "def _tty_stdio(cls, env):\n    \"\"\"Handles stdio redirection in the case of all stdio descriptors being the same tty.\"\"\"\n    # If all stdio is a tty, there's only one logical I/O device (the tty device). This happens to\n    # be addressable as a file in OSX and Linux, so we take advantage of that and directly open the\n    # character device for output redirection - eliminating the need to directly marshall any\n    # interactive stdio back/forth across the socket and permitting full, correct tty control with\n    # no middle-man.\n    stdin_ttyname, stdout_ttyname, stderr_ttyname = NailgunProtocol.ttynames_from_env(env)\n    assert stdin_ttyname == stdout_ttyname == stderr_ttyname, (\n      'expected all stdio ttys to be the same, but instead got: {}\\n'\n      'please file a bug at http://github.com/pantsbuild/pants'\n      .format([stdin_ttyname, stdout_ttyname, stderr_ttyname])\n    )\n    with open(stdin_ttyname, 'rb+', 0) as tty:\n      tty_fileno = tty.fileno()\n      with stdio_as(stdin_fd=tty_fileno, stdout_fd=tty_fileno, stderr_fd=tty_fileno):\n        def finalizer():\n          termios.tcdrain(tty_fileno)\n        yield finalizer"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle stdio redirection in the case of pipes and/or mixed pipes and ttys.", "response": "def _pipe_stdio(cls, sock, stdin_isatty, stdout_isatty, stderr_isatty, handle_stdin):\n    \"\"\"Handles stdio redirection in the case of pipes and/or mixed pipes and ttys.\"\"\"\n    stdio_writers = (\n      (ChunkType.STDOUT, stdout_isatty),\n      (ChunkType.STDERR, stderr_isatty)\n    )\n    types, ttys = zip(*(stdio_writers))\n\n    @contextmanager\n    def maybe_handle_stdin(want):\n      if want:\n        # TODO: Launching this thread pre-fork to handle @rule input currently results\n        # in an unhandled SIGILL in `src/python/pants/engine/scheduler.py, line 313 in pre_fork`.\n        # More work to be done here in https://github.com/pantsbuild/pants/issues/6005\n        with NailgunStreamStdinReader.open(sock, stdin_isatty) as fd:\n          yield fd\n      else:\n        with open('/dev/null', 'rb') as fh:\n          yield fh.fileno()\n\n    with maybe_handle_stdin(handle_stdin) as stdin_fd,\\\n         NailgunStreamWriter.open_multi(sock, types, ttys) as ((stdout_fd, stderr_fd), writer),\\\n         stdio_as(stdout_fd=stdout_fd, stderr_fd=stderr_fd, stdin_fd=stdin_fd):\n      # N.B. This will be passed to and called by the `DaemonExiter` prior to sending an\n      # exit chunk, to avoid any socket shutdown vs write races.\n      stdout, stderr = sys.stdout, sys.stderr\n      def finalizer():\n        try:\n          stdout.flush()\n          stderr.flush()\n        finally:\n          time.sleep(.001)  # HACK: Sleep 1ms in the main thread to free the GIL.\n          writer.stop()\n          writer.join()\n          stdout.close()\n          stderr.close()\n      yield finalizer"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef nailgunned_stdio(cls, sock, env, handle_stdin=True):\n    # Determine output tty capabilities from the environment.\n    stdin_isatty, stdout_isatty, stderr_isatty = NailgunProtocol.isatty_from_env(env)\n    is_tty_capable = all((stdin_isatty, stdout_isatty, stderr_isatty))\n\n    if is_tty_capable:\n      with cls._tty_stdio(env) as finalizer:\n        yield finalizer\n    else:\n      with cls._pipe_stdio(\n        sock,\n        stdin_isatty,\n        stdout_isatty,\n        stderr_isatty,\n        handle_stdin\n      ) as finalizer:\n        yield finalizer", "response": "Redirects stdio to the connected socket speaking the nailgun protocol."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nraises deferred exceptions from the daemon s synchronous path in the post - fork client.", "response": "def _raise_deferred_exc(self):\n    \"\"\"Raises deferred exceptions from the daemon's synchronous path in the post-fork client.\"\"\"\n    if self._deferred_exception:\n      try:\n        exc_type, exc_value, exc_traceback = self._deferred_exception\n        raise_with_traceback(exc_value, exc_traceback)\n      except TypeError:\n        # If `_deferred_exception` isn't a 3-item tuple (raising a TypeError on the above\n        # destructuring), treat it like a bare exception.\n        raise self._deferred_exception"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self):\n    fork_context = self._graph_helper.scheduler_session.with_fork_context if self._graph_helper else None\n    self.daemonize(write_pid=False, fork_context=fork_context)", "response": "Forks the child process and daemonizes it."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef post_fork_child(self):\n    # Set the Exiter exception hook post-fork so as not to affect the pantsd processes exception\n    # hook with socket-specific behavior. Note that this intentionally points the faulthandler\n    # trace stream to sys.stderr, which at this point is still a _LoggerStream object writing to\n    # the `pantsd.log`. This ensures that in the event of e.g. a hung but detached pantsd-runner\n    # process that the stacktrace output lands deterministically in a known place vs to a stray\n    # terminal window.\n    # TODO: test the above!\n    ExceptionSink.reset_exiter(self._exiter)\n\n    ExceptionSink.reset_interactive_output_stream(sys.stderr.buffer if PY3 else sys.stderr)\n    ExceptionSink.reset_signal_handler(DaemonSignalHandler())\n\n    # Ensure anything referencing sys.argv inherits the Pailgun'd args.\n    sys.argv = self._args\n\n    # Set context in the process title.\n    set_process_title('pantsd-runner [{}]'.format(' '.join(self._args)))\n\n    # Broadcast our process group ID (in PID form - i.e. negated) to the remote client so\n    # they can send signals (e.g. SIGINT) to all processes in the runners process group.\n    NailgunProtocol.send_pid(self._socket, os.getpid())\n    NailgunProtocol.send_pgrp(self._socket, os.getpgrp() * -1)\n\n    # Stop the services that were paused pre-fork.\n    for service in self._services.services:\n      service.terminate()\n\n    # Invoke a Pants run with stdio redirected and a proxied environment.\n    with self.nailgunned_stdio(self._socket, self._env) as finalizer,\\\n         hermetic_environment_as(**self._env):\n      try:\n        # Setup the Exiter's finalizer.\n        self._exiter.set_finalizer(finalizer)\n\n        # Clean global state.\n        clean_global_runtime_state(reset_subsystem=True)\n\n        # Otherwise, conduct a normal run.\n        runner = LocalPantsRunner.create(\n          self._exiter,\n          self._args,\n          self._env,\n          self._target_roots,\n          self._graph_helper,\n          self._options_bootstrapper\n        )\n        runner.set_start_time(self._maybe_get_client_start_time_from_env(self._env))\n\n        # Re-raise any deferred exceptions, if present.\n        self._raise_deferred_exc()\n\n        runner.run()\n      except KeyboardInterrupt:\n        self._exiter.exit_and_fail('Interrupted by user.\\n')\n      except _GracefulTerminationException as e:\n        ExceptionSink.log_exception(\n          'Encountered graceful termination exception {}; exiting'.format(e))\n        self._exiter.exit(e.exit_code)\n      except Exception:\n        # TODO: We override sys.excepthook above when we call ExceptionSink.set_exiter(). That\n        # excepthook catches `SignalHandledNonLocalExit`s from signal handlers, which isn't\n        # happening here, so something is probably overriding the excepthook. By catching Exception\n        # and calling this method, we emulate the normal, expected sys.excepthook override.\n        ExceptionSink._log_unhandled_exception_and_exit()\n      else:\n        self._exiter.exit(PANTS_SUCCEEDED_EXIT_CODE)", "response": "Post - fork child process callback executed via daemonize."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef resolve_target(self, node_task, target, results_dir, node_paths, resolve_locally=False,\n                     install_optional=None, production_only=None, force=None, frozen_lockfile=None,\n                     **kwargs):\n    \"\"\"Installs the node_package target in the results directory copying sources if necessary.\n\n    :param Task node_task: The task executing this method.\n    :param Target target: The target being resolve.\n    :param String results_dir: The output location where this target will be resolved.\n    :param NodePaths node_paths: A mapping of targets and their resolved location, if resolved.\n    :param Boolean resolve_locally: If true, the sources do not have to be copied.\n    :param Boolean install_optional: If true, install optional dependencies.\n    :param Boolean force: If true, rebuild dependencies even if already built.\n    :param Boolean frozen_lockfile: Preserve lock file and fails if a change is detected.\n    \"\"\"\n    if self.get_options().force_option_override:\n      install_optional = self.get_options().install_optional\n      production_only = self.get_options().install_production\n      force = self.get_options().force\n      frozen_lockfile = self.get_options().frozen_lockfile\n    else:\n      install_optional = install_optional if install_optional is not None else self.get_options().install_optional\n      production_only = production_only if production_only is not None else self.get_options().install_production\n      force = force if force is not None else self.get_options().force\n      frozen_lockfile = frozen_lockfile if frozen_lockfile is not None else self.get_options().frozen_lockfile\n\n    if not resolve_locally:\n      self._copy_sources(target, results_dir)\n    with pushd(results_dir):\n      if not os.path.exists('package.json'):\n        raise TaskError(\n          'Cannot find package.json. Did you forget to put it in target sources?')\n      # TODO: remove/remodel the following section when node_module dependency is fleshed out.\n      package_manager = node_task.get_package_manager(target=target).name\n      if package_manager == PACKAGE_MANAGER_NPM:\n        if resolve_locally:\n          raise TaskError('Resolving node package modules locally is not supported for NPM.')\n        if os.path.exists('npm-shrinkwrap.json'):\n          node_task.context.log.info('Found npm-shrinkwrap.json, will not inject package.json')\n        else:\n          node_task.context.log.warn(\n            'Cannot find npm-shrinkwrap.json. Did you forget to put it in target sources? '\n            'This package will fall back to inject package.json with pants BUILD dependencies '\n            'including node_remote_module and other node dependencies. However, this is '\n            'not fully supported.')\n          self._emit_package_descriptor(node_task, target, results_dir, node_paths)\n      elif package_manager == PACKAGE_MANAGER_YARNPKG:\n        if not os.path.exists('yarn.lock') and frozen_lockfile:\n          raise TaskError(\n            'Cannot find yarn.lock. Did you forget to put it in target sources?')\n      # Install all dependencies except for `file:` dependencies\n      # `file:` dependencies are special dependencies that point to a local path to a pants node target\n      # `file:` dependencies are already in the build graph and should be already be installed by this point\n\n      # Copy the package.json and then remove the file: dependencies from package.json\n      # Run the install and symlink the file: dependencies using their node_paths\n      # Afterwards, restore the original package.json to not cause diff changes when resolve_locally=True\n      # The file mutation is occuring in place and the package.json may potentially not be restored here\n      # if the process is closed.\n      with safe_temp_edit('package.json') as package_json:\n        with open(package_json, 'r') as package_json_file:\n          json_data = json.load(package_json_file)\n          source_deps = { k : v for k,v in json_data.get('dependencies', {}).items() if self.parse_file_path(v)}\n          third_party_deps = { k : v for k,v in json_data.get('dependencies', {}).items() if not self.parse_file_path(v)}\n          json_data['dependencies'] = third_party_deps\n\n        # TODO(6489): Currently the file: dependencies need to be duplicated in BUILD.\n        # After this issue is closed, only dependencies need to be specified in package.json\n        for package_name, file_path in source_deps.items():\n          if self._get_target_from_package_name(target, package_name, file_path) is None:\n            raise TaskError('Local dependency in package.json not found in the build graph. '\n                            'Check your BUILD file for missing dependencies. [\"{}\": {}]'.format(package_name, file_path))\n        mode = 'w' if PY3 else 'wb'\n        with open(package_json, mode) as package_json_file:\n          json.dump(json_data, package_json_file, indent=2, separators=(',', ': '))\n        result, command = node_task.install_module(\n          target=target, install_optional=install_optional,\n          production_only=production_only, force=force, frozen_lockfile=frozen_lockfile,\n          workunit_name=target.address.reference(),\n          workunit_labels=[WorkUnitLabel.COMPILER])\n        if result != 0:\n          raise TaskError('Failed to resolve dependencies for {}:\\n\\t{} failed with exit code {}'\n                          .format(target.address.reference(), command, result))\n        if source_deps:\n          self._link_source_dependencies(node_task, target, results_dir, node_paths, source_deps)", "response": "Resolve the target in the results directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _scoped_package_name(node_task, package_name, node_scope):\n\n    if not node_scope:\n      return package_name\n\n    scoped_package_name = package_name\n    chunk = package_name.split('/', 1)\n    if len(chunk) > 1 and chunk[0].startswith('@'):\n      scoped_package_name = os.path.join('@{}'.format(node_scope), chunk[1:])\n    else:\n      scoped_package_name = os.path.join('@{}'.format(node_scope), package_name)\n\n    node_task.context.log.debug(\n      'Node package \"{}\" will be resolved with scope \"{}\".'.format(package_name, scoped_package_name))\n    return scoped_package_name", "response": "Apply a node_scope to the package name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compile(self, ctx, args, dependency_classpath, upstream_analysis,\n              settings, compiler_option_sets, zinc_file_manager,\n              javac_plugin_map, scalac_plugin_map):\n    \"\"\"Invoke the compiler.\n\n    Subclasses must implement. Must raise TaskError on compile failure.\n\n    :param CompileContext ctx: A CompileContext for the target to compile.\n    :param list args: Arguments to the compiler (such as javac or zinc).\n    :param list dependency_classpath: List of classpath entries of type ClasspathEntry for\n      dependencies.\n    :param upstream_analysis: A map from classpath entry to analysis file for dependencies.\n    :param JvmPlatformSettings settings: platform settings determining the -source, -target, etc for\n      javac to use.\n    :param list compiler_option_sets: The compiler_option_sets flags for the target.\n    :param zinc_file_manager: whether to use zinc provided file manager.\n    :param javac_plugin_map: Map of names of javac plugins to use to their arguments.\n    :param scalac_plugin_map: Map of names of scalac plugins to use to their arguments.\n    \"\"\"\n    raise NotImplementedError()", "response": "Invoke the compiler.\n\n    Subclasses must implement. Must raise TaskError on compile failure.\n\n    :param CompileContext ctx: A CompileContext for the target to compile.\n    :param list args: Arguments to the compiler (such as javac or zinc).\n    :param list dependency_classpath: List of classpath entries of type ClasspathEntry for\n      dependencies.\n    :param upstream_analysis: A map from classpath entry to analysis file for dependencies.\n    :param JvmPlatformSettings settings: platform settings determining the -source, -target, etc for\n      javac to use.\n    :param list compiler_option_sets: The compiler_option_sets flags for the target.\n    :param zinc_file_manager: whether to use zinc provided file manager.\n    :param javac_plugin_map: Map of names of javac plugins to use to their arguments.\n    :param scalac_plugin_map: Map of names of scalac plugins to use to their arguments."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef do_compile(self, invalidation_check, compile_contexts, classpath_product):\n\n    invalid_targets = [vt.target for vt in invalidation_check.invalid_vts]\n    valid_targets = [vt.target for vt in invalidation_check.all_vts if vt.valid]\n\n    if self.execution_strategy == self.HERMETIC:\n      self._set_directory_digests_for_valid_target_classpath_directories(valid_targets, compile_contexts)\n\n    for valid_target in valid_targets:\n      cc = self.select_runtime_context(compile_contexts[valid_target])\n\n      classpath_product.add_for_target(\n        valid_target,\n        [(conf, self._classpath_for_context(cc)) for conf in self._confs],\n      )\n    self.register_extra_products_from_contexts(valid_targets, compile_contexts)\n\n    if not invalid_targets:\n      return\n\n    # This ensures the workunit for the worker pool is set before attempting to compile.\n    with self.context.new_workunit('isolation-{}-pool-bootstrap'.format(self.name())) \\\n            as workunit:\n      # This uses workunit.parent as the WorkerPool's parent so that child workunits\n      # of different pools will show up in order in the html output. This way the current running\n      # workunit is on the bottom of the page rather than possibly in the middle.\n      worker_pool = WorkerPool(workunit.parent,\n                               self.context.run_tracker,\n                               self._worker_count)\n\n    # Prepare the output directory for each invalid target, and confirm that analysis is valid.\n    for target in invalid_targets:\n      cc = self.select_runtime_context(compile_contexts[target])\n      safe_mkdir(cc.classes_dir.path)\n\n    # Now create compile jobs for each invalid target one by one, using the classpath\n    # generated by upstream JVM tasks and our own prepare_compile().\n    jobs = self._create_compile_jobs(compile_contexts,\n                                     invalid_targets,\n                                     invalidation_check.invalid_vts,\n                                     classpath_product)\n\n    exec_graph = ExecutionGraph(jobs, self.get_options().print_exception_stacktrace)\n    try:\n      exec_graph.execute(worker_pool, self.context.log)\n    except ExecutionFailure as e:\n      raise TaskError(\"Compilation failure: {}\".format(e))", "response": "Executes compilations for the invalid targets contained in a single chunk."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompile the given vts into the given output dir.", "response": "def _compile_vts(self, vts, ctx, upstream_analysis, dependency_classpath, progress_message, settings,\n                   compiler_option_sets, zinc_file_manager, counter):\n    \"\"\"Compiles sources for the given vts into the given output dir.\n\n    :param vts: VersionedTargetSet with one entry for the target.\n    :param ctx: - A CompileContext instance for the target.\n    :param dependency_classpath: A list of classpath entries of type ClasspathEntry for dependencies\n\n    May be invoked concurrently on independent target sets.\n\n    Postcondition: The individual targets in vts are up-to-date, as if each were\n                   compiled individually.\n    \"\"\"\n    if not ctx.sources:\n      self.context.log.warn('Skipping {} compile for targets with no sources:\\n  {}'\n                            .format(self.name(), vts.targets))\n    else:\n      counter_val = str(counter()).rjust(counter.format_length(), ' ')\n      counter_str = '[{}/{}] '.format(counter_val, counter.size)\n      # Do some reporting.\n      self.context.log.info(\n        counter_str,\n        'Compiling ',\n        items_to_report_element(ctx.sources, '{} source'.format(self.name())),\n        ' in ',\n        items_to_report_element([t.address.reference() for t in vts.targets], 'target'),\n        ' (',\n        progress_message,\n        ').')\n      with self.context.new_workunit('compile', labels=[WorkUnitLabel.COMPILER]) as compile_workunit:\n        try:\n          directory_digest = self.compile(\n            ctx,\n            self._args,\n            dependency_classpath,\n            upstream_analysis,\n            settings,\n            compiler_option_sets,\n            zinc_file_manager,\n            self._get_plugin_map('javac', Java.global_instance(), ctx.target),\n            self._get_plugin_map('scalac', ScalaPlatform.global_instance(), ctx.target),\n          )\n          self._capture_logs(compile_workunit, ctx.log_dir)\n          return directory_digest\n        except TaskError:\n          if self.get_options().suggest_missing_deps:\n            logs = [path\n                    for _, name, _, path in self._find_logs(compile_workunit)\n                    if name == self.name()]\n            if logs:\n              self._find_missing_deps(logs, ctx.target)\n          raise"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a map of plugin to args for the given compiler.", "response": "def _get_plugin_map(self, compiler, options_src, target):\n    \"\"\"Returns a map of plugin to args, for the given compiler.\n\n    Only plugins that must actually be activated will be present as keys in the map.\n    Plugins with no arguments will have an empty list as a value.\n\n    Active plugins and their args will be gathered from (in order of precedence):\n    - The <compiler>_plugins and <compiler>_plugin_args fields of the target, if it has them.\n    - The <compiler>_plugins and <compiler>_plugin_args options of this task, if it has them.\n    - The <compiler>_plugins and <compiler>_plugin_args fields of this task, if it has them.\n\n    Note that in-repo plugins will not be returned, even if requested, when building\n    themselves.  Use published versions of those plugins for that.\n\n    See:\n    - examples/src/java/org/pantsbuild/example/javac/plugin/README.md.\n    - examples/src/scala/org/pantsbuild/example/scalac/plugin/README.md\n\n    :param compiler: one of 'javac', 'scalac'.\n    :param options_src: A JvmToolMixin instance providing plugin options.\n    :param target: The target whose plugins we compute.\n    \"\"\"\n    # Note that we get() options and getattr() target fields and task methods,\n    # so we're robust when those don't exist (or are None).\n    plugins_key = '{}_plugins'.format(compiler)\n    requested_plugins = (\n      tuple(getattr(self, plugins_key, []) or []) +\n      tuple(options_src.get_options().get(plugins_key, []) or []) +\n      tuple((getattr(target, plugins_key, []) or []))\n    )\n    # Allow multiple flags and also comma-separated values in a single flag.\n    requested_plugins = {p for val in requested_plugins for p in val.split(',')}\n\n    plugin_args_key = '{}_plugin_args'.format(compiler)\n    available_plugin_args = {}\n    available_plugin_args.update(getattr(self, plugin_args_key, {}) or {})\n    available_plugin_args.update(options_src.get_options().get(plugin_args_key, {}) or {})\n    available_plugin_args.update(getattr(target, plugin_args_key, {}) or {})\n\n    # From all available args, pluck just the ones for the selected plugins.\n    plugin_map = {}\n    for plugin in requested_plugins:\n      # Don't attempt to use a plugin while building that plugin.\n      # This avoids a bootstrapping problem.  Note that you can still\n      # use published plugins on themselves, just not in-repo plugins.\n      if target not in self._plugin_targets(compiler).get(plugin, {}):\n        plugin_map[plugin] = available_plugin_args.get(plugin, [])\n    return plugin_map"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _find_logs(self, compile_workunit):\n    for idx, workunit in enumerate(compile_workunit.children):\n      for output_name, outpath in workunit.output_paths().items():\n        if output_name in ('stdout', 'stderr'):\n          yield idx, workunit.name, output_name, outpath", "response": "Finds all logs under the given workunit."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nyield tuples of classes_dir - > analysis_file for the closure of the target.", "response": "def _upstream_analysis(self, compile_contexts, classpath_entries):\n    \"\"\"Returns tuples of classes_dir->analysis_file for the closure of the target.\"\"\"\n    # Reorganize the compile_contexts by class directory.\n    compile_contexts_by_directory = {}\n    for compile_context in compile_contexts.values():\n      compile_context = self.select_runtime_context(compile_context)\n      compile_contexts_by_directory[compile_context.classes_dir.path] = compile_context\n    # If we have a compile context for the target, include it.\n    for entry in classpath_entries:\n      path = entry.path\n      if not path.endswith('.jar'):\n        compile_context = compile_contexts_by_directory.get(path)\n        if not compile_context:\n          self.context.log.debug('Missing upstream analysis for {}'.format(path))\n        else:\n          yield compile_context.classes_dir.path, compile_context.analysis_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_cache(self, vts, counter):\n    if not self.artifact_cache_reads_enabled():\n      return False\n    cached_vts, _, _ = self.check_artifact_cache([vts])\n    if not cached_vts:\n      self.context.log.debug('Missed cache during double check for {}'\n        .format(vts.target.address.spec))\n      return False\n    assert cached_vts == [vts], (\n      'Cache returned unexpected target: {} vs {}'.format(cached_vts, [vts])\n    )\n    self.context.log.info('Hit cache during double check for {}'.format(vts.target.address.spec))\n    counter()\n    return True", "response": "Manually checks the artifact cache. Returns True if the artifact cache was hit successfully."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking to see if the compile should be re - used.", "response": "def should_compile_incrementally(self, vts, ctx):\n    \"\"\"Check to see if the compile should try to re-use the existing analysis.\n\n    Returns true if we should try to compile the target incrementally.\n    \"\"\"\n    if not vts.is_incremental:\n      return False\n    if not self._clear_invalid_analysis:\n      return True\n    return os.path.exists(ctx.analysis_file)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a jar up the compile_context to its output jar location.", "response": "def _create_context_jar(self, compile_context):\n    \"\"\"Jar up the compile_context to its output jar location.\n\n    TODO(stuhood): In the medium term, we hope to add compiler support for this step, which would\n    allow the jars to be used as compile _inputs_ as well. Currently using jar'd compile outputs as\n    compile inputs would make the compiler's analysis useless.\n      see https://github.com/twitter-forks/sbt/tree/stuhood/output-jars\n    \"\"\"\n    root = compile_context.classes_dir.path\n    with compile_context.open_jar(mode='w') as jar:\n      for abs_sub_dir, dirnames, filenames in safe_walk(root):\n        for name in dirnames + filenames:\n          abs_filename = os.path.join(abs_sub_dir, name)\n          arcname = fast_relpath(abs_filename, root)\n          jar.write(abs_filename, arcname)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _compute_sources_for_target(self, target):\n    def resolve_target_sources(target_sources):\n      resolved_sources = []\n      for tgt in target_sources:\n        if tgt.has_sources():\n          resolved_sources.extend(tgt.sources_relative_to_buildroot())\n      return resolved_sources\n\n    sources = [s for s in target.sources_relative_to_buildroot() if self._sources_predicate(s)]\n    # TODO: Make this less hacky. Ideally target.java_sources will point to sources, not targets.\n    if hasattr(target, 'java_sources') and target.java_sources:\n      sources.extend(resolve_target_sources(target.java_sources))\n    return sources", "response": "Computes and returns the sources for the given target."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _extra_compile_time_classpath(self):\n    def extra_compile_classpath_iter():\n      for conf in self._confs:\n        for jar in self.extra_compile_time_classpath_elements():\n          yield (conf, jar)\n\n    return list(extra_compile_classpath_iter())", "response": "Compute any extra compile - time - only classpath elements."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a map from plugin name to the targets that build that plugin.", "response": "def _plugin_targets(self, compiler):\n    \"\"\"Returns a map from plugin name to the targets that build that plugin.\"\"\"\n    if compiler == 'javac':\n      plugin_cls = JavacPlugin\n    elif compiler == 'scalac':\n      plugin_cls = ScalacPlugin\n    else:\n      raise TaskError('Unknown JVM compiler: {}'.format(compiler))\n    plugin_tgts = self.context.targets(predicate=lambda t: isinstance(t, plugin_cls))\n    return {t.plugin: t.closure() for t in plugin_tgts}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef execute(self):\n\n    def is_cc(source):\n      _, ext = os.path.splitext(source)\n      return ext in self.get_options().cc_extensions\n\n    targets = self.context.targets(self.is_cpp)\n\n    # Compile source files to objects.\n    with self.invalidated(targets, invalidate_dependents=True) as invalidation_check:\n      obj_mapping = self.context.products.get('objs')\n      for vt in invalidation_check.all_vts:\n        for source in vt.target.sources_relative_to_buildroot():\n          if is_cc(source):\n            if not vt.valid:\n              with self.context.new_workunit(name='cpp-compile', labels=[WorkUnitLabel.MULTITOOL]):\n                # TODO: Parallelise the compilation.\n                # TODO: Only recompile source files that have changed since the\n                #       object file was last written. Also use the output from\n                #       gcc -M to track dependencies on headers.\n                self._compile(vt.target, vt.results_dir, source)\n            objpath = self._objpath(vt.target, vt.results_dir, source)\n            obj_mapping.add(vt.target, vt.results_dir).append(objpath)", "response": "Compile all sources in a given target to object files."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compile(self, target, results_dir, source):\n    obj = self._objpath(target, results_dir, source)\n    safe_mkdir_for(obj)\n\n    abs_source = os.path.join(get_buildroot(), source)\n\n    # TODO: include dir should include dependent work dir when headers are copied there.\n    include_dirs = []\n    for dep in target.dependencies:\n      if self.is_library(dep):\n        include_dirs.extend([os.path.join(get_buildroot(), dep.target_base)])\n\n    cmd = [self.cpp_toolchain.compiler]\n    cmd.extend(['-c'])\n    cmd.extend(('-I{0}'.format(i) for i in include_dirs))\n    cmd.extend(['-o' + obj, abs_source])\n    cmd.extend(self.get_options().cc_options)\n\n    # TODO: submit_async_work with self.run_command, [(cmd)] as a Work object.\n    with self.context.new_workunit(name='cpp-compile', labels=[WorkUnitLabel.COMPILER]) as workunit:\n      self.run_command(cmd, workunit)\n\n    self.context.log.info('Built c++ object: {0}'.format(obj))", "response": "Compile given source to an object file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_GET(self):\n    if not self._client_allowed():\n      return\n\n    try:\n      (_, _, path, query, _) = urlsplit(self.path)\n      params = parse_qs(query)\n      # Give each handler a chance to respond.\n      for prefix, handler in self._GET_handlers:\n        if self._maybe_handle(prefix, handler, path, params):\n          return\n      # If no path specified, default to showing the list of all runs.\n      if path == '/':\n        self._handle_runs('', {})\n        return\n\n      content = 'Invalid GET request {}'.format(self.path).encode('utf-8'),\n      self._send_content(content, 'text/html', code=400)\n    except (IOError, ValueError):\n      pass", "response": "Method implementation for BaseHTTPRequestHandler. do_GET"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _handle_runs(self, relpath, params):\n    runs_by_day = self._partition_runs_by_day()\n    args = self._default_template_args('run_list.html')\n    args['runs_by_day'] = runs_by_day\n    content = self._renderer.render_name('base.html', args).encode(\"utf-8\")\n    self._send_content(content, 'text/html')", "response": "Show a listing of all pants runs since the last clean - all."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nshows the report for a single pants run.", "response": "def _handle_run(self, relpath, params):\n    \"\"\"Show the report for a single pants run.\"\"\"\n    args = self._default_template_args('run.html')\n    run_id = relpath\n    run_info = self._get_run_info_dict(run_id)\n    if run_info is None:\n      args['no_such_run'] = relpath\n      if run_id == 'latest':\n        args['is_latest'] = 'none'\n    else:\n      report_abspath = run_info['default_report']\n      report_relpath = os.path.relpath(report_abspath, self._root)\n      report_dir = os.path.dirname(report_relpath)\n      self_timings_path = os.path.join(report_dir, 'self_timings')\n      cumulative_timings_path = os.path.join(report_dir, 'cumulative_timings')\n      artifact_cache_stats_path = os.path.join(report_dir, 'artifact_cache_stats')\n      run_info['timestamp_text'] = \\\n        datetime.fromtimestamp(float(run_info['timestamp'])).strftime('%H:%M:%S on %A, %B %d %Y')\n\n      timings_and_stats = '\\n'.join([\n        self._collapsible_fmt_string.format(id='cumulative-timings-collapsible',\n                                            title='Cumulative timings', class_prefix='aggregated-timings'),\n        self._collapsible_fmt_string.format(id='self-timings-collapsible',\n                                            title='Self timings', class_prefix='aggregated-timings'),\n        self._collapsible_fmt_string.format(id='artifact-cache-stats-collapsible',\n                                            title='Artifact cache stats', class_prefix='artifact-cache-stats')\n      ])\n\n      args.update({'run_info': run_info,\n                   'report_path': report_relpath,\n                   'self_timings_path': self_timings_path,\n                   'cumulative_timings_path': cumulative_timings_path,\n                   'artifact_cache_stats_path': artifact_cache_stats_path,\n                   'timings_and_stats': timings_and_stats})\n      if run_id == 'latest':\n        args['is_latest'] = run_info['id']\n\n    content = self._renderer.render_name('base.html', args).encode(\"utf-8\")\n    self._send_content(content, 'text/html')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling requests to browse the filesystem under the build root.", "response": "def _handle_browse(self, relpath, params):\n    \"\"\"Handle requests to browse the filesystem under the build root.\"\"\"\n    abspath = os.path.normpath(os.path.join(self._root, relpath))\n    if not abspath.startswith(self._root):\n      raise ValueError  # Prevent using .. to get files from anywhere other than root.\n    if os.path.isdir(abspath):\n      self._serve_dir(abspath, params)\n    elif os.path.isfile(abspath):\n      self._serve_file(abspath, params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle the content of a file.", "response": "def _handle_content(self, relpath, params):\n    \"\"\"Render file content for pretty display.\"\"\"\n    abspath = os.path.normpath(os.path.join(self._root, relpath))\n    if os.path.isfile(abspath):\n      with open(abspath, 'rb') as infile:\n        content = infile.read()\n    else:\n      content = 'No file found at {}'.format(abspath).encode('utf-8')\n    content_type = mimetypes.guess_type(abspath)[0] or 'text/plain'\n    if not content_type.startswith('text/') and not content_type == 'application/xml':\n      # Binary file. Display it as hex, split into lines.\n      n = 120  # Display lines of this max size.\n      content = repr(content)[1:-1]  # Will escape non-printables etc, dropping surrounding quotes.\n      content = '\\n'.join([content[i:i + n] for i in range(0, len(content), n)])\n      prettify = False\n      prettify_extra_langs = []\n    else:\n      prettify = True\n      if self._settings.assets_dir:\n        prettify_extra_dir = os.path.join(self._settings.assets_dir, 'js', 'prettify_extra_langs')\n        prettify_extra_langs = [{'name': x} for x in os.listdir(prettify_extra_dir)]\n      else:\n        # TODO: Find these from our package, somehow.\n        prettify_extra_langs = []\n    linenums = True\n    args = {'prettify_extra_langs': prettify_extra_langs, 'content': content,\n            'prettify': prettify, 'linenums': linenums}\n    content = self._renderer.render_name('file_content.html', args).encode(\"utf-8\")\n    self._send_content(content, 'text/html')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _handle_poll(self, relpath, params):\n    request = json.loads(params.get('q')[0])\n    ret = {}\n    # request is a polling request for multiple files. For each file:\n    #  - id is some identifier assigned by the client, used to differentiate the results.\n    #  - path is the file to poll.\n    #  - pos is the last byte position in that file seen by the client.\n    for poll in request:\n      _id = poll.get('id', None)\n      path = poll.get('path', None)\n      pos = poll.get('pos', 0)\n      if path:\n        abspath = os.path.normpath(os.path.join(self._root, path))\n        if os.path.isfile(abspath):\n          with open(abspath, 'rb') as infile:\n            if pos:\n              infile.seek(pos)\n            content = infile.read()\n            ret[_id] = content.decode(\"utf-8\")\n    content = json.dumps(ret).encode(\"utf-8\")\n    self._send_content(content, 'application/json')", "response": "Handle poll requests for raw file contents."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsplits the runs by day so we can display them grouped that way.", "response": "def _partition_runs_by_day(self):\n    \"\"\"Split the runs by day, so we can display them grouped that way.\"\"\"\n    run_infos = self._get_all_run_infos()\n    for x in run_infos:\n      ts = float(x['timestamp'])\n      x['time_of_day_text'] = datetime.fromtimestamp(ts).strftime('%H:%M:%S')\n\n    def date_text(dt):\n      delta_days = (date.today() - dt).days\n      if delta_days == 0:\n        return 'Today'\n      elif delta_days == 1:\n        return 'Yesterday'\n      elif delta_days < 7:\n        return dt.strftime('%A')  # Weekday name.\n      else:\n        d = dt.day % 10\n        suffix = 'st' if d == 1 else 'nd' if d == 2 else 'rd' if d == 3 else 'th'\n        return dt.strftime('%B %d') + suffix  # E.g., October 30th.\n\n    keyfunc = lambda x: datetime.fromtimestamp(float(x['timestamp']))\n    sorted_run_infos = sorted(run_infos, key=keyfunc, reverse=True)\n    return [{'date_text': date_text(dt), 'run_infos': [x for x in infos]}\n            for dt, infos in itertools.groupby(sorted_run_infos, lambda x: keyfunc(x).date())]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the RunInfo for a run as a dict.", "response": "def _get_run_info_dict(self, run_id):\n    \"\"\"Get the RunInfo for a run, as a dict.\"\"\"\n    run_info_path = os.path.join(self._settings.info_dir, run_id, 'info')\n    if os.path.exists(run_info_path):\n      # We copy the RunInfo as a dict, so we can add stuff to it to pass to the template.\n      return RunInfo(run_info_path).get_as_dict()\n    else:\n      return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_all_run_infos(self):\n    info_dir = self._settings.info_dir\n    if not os.path.isdir(info_dir):\n      return []\n    paths = [os.path.join(info_dir, x) for x in os.listdir(info_dir)]\n\n    # We copy the RunInfo as a dict, so we can add stuff to it to pass to the template.\n    # We filter only those that have a timestamp, to avoid a race condition with writing\n    # that field.\n    return [d for d in\n            [RunInfo(os.path.join(p, 'info')).get_as_dict() for p in paths\n             if os.path.isdir(p) and not os.path.islink(p)]\n            if 'timestamp' in d]", "response": "Find the RunInfos for all runs since the last clean - all."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nshow a directory listing.", "response": "def _serve_dir(self, abspath, params):\n    \"\"\"Show a directory listing.\"\"\"\n    relpath = os.path.relpath(abspath, self._root)\n    breadcrumbs = self._create_breadcrumbs(relpath)\n    entries = [{'link_path': os.path.join(relpath, e), 'name': e} for e in os.listdir(abspath)]\n    args = self._default_template_args('dir.html')\n    args.update({'root_parent': os.path.dirname(self._root),\n                 'breadcrumbs': breadcrumbs,\n                 'entries': entries,\n                 'params': params})\n    content = self._renderer.render_name('base.html', args).encode(\"utf-8\")\n    self._send_content(content, 'text/html')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _serve_file(self, abspath, params):\n    relpath = os.path.relpath(abspath, self._root)\n    breadcrumbs = self._create_breadcrumbs(relpath)\n    link_path = urlunparse(['', '', relpath, '', urlencode(params), ''])\n    args = self._default_template_args('file.html')\n    args.update({'root_parent': os.path.dirname(self._root),\n                 'breadcrumbs': breadcrumbs,\n                 'link_path': link_path})\n    content = self._renderer.render_name('base.html', args).encode(\"utf-8\")\n    self._send_content(content, 'text/html')", "response": "Show a file.\n\n    The actual content of the file is rendered by _handle_content."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending content to client.", "response": "def _send_content(self, content, content_type, code=200):\n    \"\"\"Send content to client.\"\"\"\n    assert isinstance(content, bytes)\n    self.send_response(code)\n    self.send_header('Content-Type', content_type)\n    self.send_header('Content-Length', str(len(content)))\n    self.end_headers()\n    self.wfile.write(content)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _client_allowed(self):\n    client_ip = self._client_address[0]\n    if not client_ip in self._settings.allowed_clients and \\\n       not 'ALL' in self._settings.allowed_clients:\n      content = 'Access from host {} forbidden.'.format(client_ip).encode('utf-8')\n      self._send_content(content, 'text/html')\n      return False\n    return True", "response": "Check if client is allowed to connect to this server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _maybe_handle(self, prefix, handler, path, params, data=None):\n    if path.startswith(prefix):\n      relpath = path[len(prefix):]\n      if data:\n        handler(relpath, params, data)\n      else:\n        handler(relpath, params)\n      return True\n    else:\n      return False", "response": "Apply the handler if the prefix matches."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_breadcrumbs(self, relpath):\n    if relpath == '.':\n      breadcrumbs = []\n    else:\n      path_parts = [os.path.basename(self._root)] + relpath.split(os.path.sep)\n      path_links = ['/'.join(path_parts[1:i + 1]) for i, name in enumerate(path_parts)]\n      breadcrumbs = [{'link_path': link_path, 'name': name}\n                     for link_path, name in zip(path_links, path_parts)]\n    return breadcrumbs", "response": "Create filesystem browsing breadcrumb navigation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef post_fork_child(self):\n    # The server finds run-specific info dirs by looking at the subdirectories of info_dir,\n    # which is conveniently and obviously the parent dir of the current run's info dir.\n    info_dir = os.path.dirname(self.context.run_tracker.run_info_dir)\n\n    settings = ReportingServer.Settings(info_dir=info_dir,\n                                        root=get_buildroot(),\n                                        template_dir=self.options.template_dir,\n                                        assets_dir=self.options.assets_dir,\n                                        allowed_clients=self.options.allowed_clients)\n\n    server = ReportingServer(self.options.port, settings)\n\n    self.write_socket(server.server_port())\n\n    # Block forever.\n    server.start()", "response": "Post - fork child callback for ProcessManager. daemonize."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef split_args(self, args=None):\n    goals = OrderedSet()\n    scope_to_flags = {}\n\n    def add_scope(s):\n      # Force the scope to appear, even if empty.\n      if s not in scope_to_flags:\n        scope_to_flags[s] = []\n\n    targets = []\n    passthru = []\n    passthru_owner = None\n\n    self._unconsumed_args = list(reversed(sys.argv if args is None else args))\n    # In regular use the first token is the binary name, so skip it. However tests may\n    # pass just a list of flags, so don't skip it in that case.\n    if not self._at_flag() and self._unconsumed_args:\n      self._unconsumed_args.pop()\n    if self._unconsumed_args and self._unconsumed_args[-1] == 'goal':\n      # TODO: Temporary warning. Eventually specifying 'goal' will be an error.\n      print(\"WARNING: Specifying 'goal' explicitly is no longer necessary, and deprecated.\",\n            file=sys.stderr)\n      self._unconsumed_args.pop()\n\n    def assign_flag_to_scope(flag, default_scope):\n      flag_scope, descoped_flag = self._descope_flag(flag, default_scope=default_scope)\n      if flag_scope not in scope_to_flags:\n        scope_to_flags[flag_scope] = []\n      scope_to_flags[flag_scope].append(descoped_flag)\n\n    global_flags = self._consume_flags()\n\n    add_scope(GLOBAL_SCOPE)\n    for flag in global_flags:\n      assign_flag_to_scope(flag, GLOBAL_SCOPE)\n    scope, flags = self._consume_scope()\n    while scope:\n      if not self._check_for_help_request(scope.lower()):\n        add_scope(scope)\n        goals.add(scope.partition('.')[0])\n        passthru_owner = scope\n        for flag in flags:\n          assign_flag_to_scope(flag, scope)\n      scope, flags = self._consume_scope()\n\n    while self._unconsumed_args and not self._at_double_dash():\n      arg = self._unconsumed_args.pop()\n      if arg.startswith('-'):\n        # We assume any args here are in global scope.\n        if not self._check_for_help_request(arg):\n          assign_flag_to_scope(arg, GLOBAL_SCOPE)\n      elif os.path.sep in arg or ':' in arg or os.path.isdir(arg):\n        targets.append(arg)\n      elif arg not in self._known_scopes:\n        self._unknown_scopes.append(arg)\n\n    if self._at_double_dash():\n      self._unconsumed_args.pop()\n      passthru = list(reversed(self._unconsumed_args))\n\n    if self._unknown_scopes:\n      self._help_request = UnknownGoalHelp(self._unknown_scopes)\n\n    if not goals and not self._help_request:\n      self._help_request = NoGoalHelp()\n\n    return SplitArgs(goals, scope_to_flags, targets, passthru, passthru_owner if passthru else None, self._unknown_scopes)", "response": "Splits the specified arg list into a list of Goals and Target objects."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _consume_scope(self):\n    if not self._at_scope():\n      return None, []\n    scope = self._unconsumed_args.pop()\n    flags = self._consume_flags()\n    return scope, flags", "response": "Consume a scope and flags."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _consume_flags(self):\n    flags = []\n    while self._at_flag():\n      flag = self._unconsumed_args.pop()\n      if not self._check_for_help_request(flag):\n        flags.append(flag)\n    return flags", "response": "Read flags until we encounter the first token that isn t a flag."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _descope_flag(self, flag, default_scope):\n    for scope_prefix, scope_info in self._known_scoping_prefixes:\n      for flag_prefix in ['--', '--no-']:\n        prefix = flag_prefix + scope_prefix\n        if flag.startswith(prefix):\n          scope = scope_info.scope\n          if scope_info.category == ScopeInfo.SUBSYSTEM and default_scope != GLOBAL_SCOPE:\n            # We allow goal.task --subsystem-foo to refer to the task-level subsystem instance,\n            # i.e., as if qualified by --subsystem-goal-task-foo.\n            # Note that this means that we can't set a task option on the cmd-line if its\n            # name happens to start with a subsystem scope.\n            # TODO: Either fix this or at least detect such options and warn.\n            task_subsystem_scope = '{}.{}'.format(scope_info.scope, default_scope)\n            if task_subsystem_scope in self._known_scopes:  # Such a task subsystem actually exists.\n              scope = task_subsystem_scope\n          return scope, flag_prefix + flag[len(prefix):]\n    return default_scope, flag", "response": "Given a flag and a default scope extract the scope and flag."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetect the git working tree above cwd and return it ; else return None.", "response": "def detect_worktree(cls, binary='git', subdir=None):\n    \"\"\"Detect the git working tree above cwd and return it; else, return None.\n\n    :param string binary: The path to the git binary to use, 'git' by default.\n    :param string subdir: The path to start searching for a git repo.\n    :returns: path to the directory where the git working tree is rooted.\n    :rtype: string\n    \"\"\"\n    # TODO(John Sirois): This is only used as a factory for a Git instance in\n    # pants.base.build_environment.get_scm, encapsulate in a true factory method.\n    cmd = [binary, 'rev-parse', '--show-toplevel']\n    try:\n      if subdir:\n        with pushd(subdir):\n          process, out = cls._invoke(cmd)\n      else:\n        process, out = cls._invoke(cmd)\n      cls._check_result(cmd, process.returncode, raise_type=Scm.ScmException)\n    except Scm.ScmException:\n      return None\n    return cls._cleanse(out)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclone the repo at repo_url into dest.", "response": "def clone(cls, repo_url, dest, binary='git'):\n    \"\"\"Clone the repo at repo_url into dest.\n\n    :param string binary: The path to the git binary to use, 'git' by default.\n    :returns: an instance of this class representing the cloned repo.\n    :rtype: Git\n    \"\"\"\n    cmd = [binary, 'clone', repo_url, dest]\n    process, out = cls._invoke(cmd)\n    cls._check_result(cmd, process.returncode)\n    return cls(binary=binary, worktree=dest)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _invoke(cls, cmd):\n    try:\n      process = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n    except OSError as e:\n      # Binary DNE or is not executable\n      raise cls.LocalException('Failed to execute command {}: {}'.format(' '.join(cmd), e))\n    out, _ = process.communicate()\n    return process, out", "response": "Invoke the given command and return a tuple of process and raw binary output."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the merge - base of master and HEAD in bash", "response": "def merge_base(self, left='master', right='HEAD'):\n    \"\"\"Returns the merge-base of master and HEAD in bash: `git merge-base left right`\"\"\"\n    return self._check_output(['merge-base', left, right], raise_type=Scm.LocalException)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef refresh(self, leave_clean=False):\n    remote, merge = self._get_upstream()\n    self._check_call(['fetch', '--tags', remote, merge], raise_type=Scm.RemoteException)\n    try:\n      self._check_call(['rebase', 'FETCH_HEAD'], raise_type=Scm.LocalException)\n    except Scm.LocalException as e:\n      if leave_clean:\n        logger.debug('Cleaning up after failed rebase')\n        try:\n          self._check_call(['rebase', '--abort'], raise_type=Scm.LocalException)\n        except Scm.LocalException as abort_exc:\n          logger.debug('Failed to up after failed rebase')\n          logger.debug(traceback.format_exc(abort_exc))\n          # But let the original exception propagate, since that's the more interesting one\n      raise e", "response": "Attempt to pull - with - rebase from upstream."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the remote and remote merge branch for the current branch", "response": "def _get_upstream(self):\n    \"\"\"Return the remote and remote merge branch for the current branch\"\"\"\n    if not self._remote or not self._branch:\n      branch = self.branch_name\n      if not branch:\n        raise Scm.LocalException('Failed to determine local branch')\n\n      def get_local_config(key):\n        value = self._check_output(['config', '--local', '--get', key],\n                                   raise_type=Scm.LocalException)\n        return value.strip()\n\n      self._remote = self._remote or get_local_config('branch.{}.remote'.format(branch))\n      self._branch = self._branch or get_local_config('branch.{}.merge'.format(branch))\n    return self._remote, self._branch"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef listdir(self, relpath):\n\n    path = self._realpath(relpath)\n    if not path.endswith('/'):\n      raise self.NotADirException(self.rev, relpath)\n\n    if path[0] == '/' or path.startswith('../'):\n      return os.listdir(path)\n\n    tree = self._read_tree(path[:-1])\n    return list(tree.keys())", "response": "Like os. listdir but reads from the git repository."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef open(self, relpath):\n\n    path = self._realpath(relpath)\n    if path.endswith('/'):\n      raise self.IsDirException(self.rev, relpath)\n\n    if path.startswith('../') or path[0] == '/':\n      yield open(path, 'rb')\n      return\n\n    object_type, data = self._read_object_from_repo(rev=self.rev, relpath=path)\n    if object_type == b'tree':\n      raise self.IsDirException(self.rev, relpath)\n    assert object_type == b'blob'\n    yield io.BytesIO(data)", "response": "Read a file out of the repository at a certain revision."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _realpath(self, relpath):\n    obj, path_so_far = self._read_object(relpath, MAX_SYMLINKS_IN_REALPATH)\n    if isinstance(obj, self.Symlink):\n      raise self.SymlinkLoopException(self.rev, relpath)\n    return path_so_far", "response": "Follow symlinks to find the real path to a file or directory in the repo."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read_tree(self, path):\n\n    path = self._fixup_dot_relative(path)\n\n    tree = self._trees.get(path)\n    if tree:\n      return tree\n    tree = {}\n    object_type, tree_data = self._read_object_from_repo(rev=self.rev, relpath=path)\n    assert object_type == b'tree'\n    # The tree data here is (mode ' ' filename \\0 20-byte-sha)*\n    # It's transformed to a list of byte chars to allow iteration.\n    # See http://python-future.org/compatible_idioms.html#byte-string-literals.\n    tree_data = [bytes([b]) for b in tree_data]\n    i = 0\n    while i < len(tree_data):\n      start = i\n      while tree_data[i] != b' ':\n        i += 1\n      mode = b''.join(tree_data[start:i])\n      i += 1  # skip space\n      start = i\n      while tree_data[i] != NUL:\n        i += 1\n      name = b''.join(tree_data[start:i])\n      sha = b''.join(tree_data[i + 1:i + 1 + GIT_HASH_LENGTH])\n      sha_hex = binascii.hexlify(sha)\n      i += 1 + GIT_HASH_LENGTH\n      if mode == b'120000':\n        tree[name] = self.Symlink(name, sha_hex)\n      elif mode == b'40000':\n        tree[name] = self.Dir(name, sha_hex)\n      else:\n        tree[name] = self.File(name, sha_hex)\n    self._trees[path] = tree\n    return tree", "response": "Given a revision and path parse the tree data out of git cat - file output."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads an object from the git repo.", "response": "def _read_object_from_repo(self, rev=None, relpath=None, sha=None):\n    \"\"\"Read an object from the git repo.\n    This is implemented via a pipe to git cat-file --batch\n    \"\"\"\n    if sha:\n      spec = sha + b'\\n'\n    else:\n      assert rev is not None\n      assert relpath is not None\n      rev = ensure_text(rev)\n      relpath = ensure_text(relpath)\n      relpath = self._fixup_dot_relative(relpath)\n      spec = '{}:{}\\n'.format(rev, relpath).encode('utf-8')\n\n    self._maybe_start_cat_file_process()\n    self._cat_file_process.stdin.write(spec)\n    self._cat_file_process.stdin.flush()\n    header = None\n    while not header:\n      header = self._cat_file_process.stdout.readline()\n      if self._cat_file_process.poll() is not None:\n        raise self.GitDiedException(\"Git cat-file died while trying to read '{}'.\".format(spec))\n\n    header = header.rstrip()\n    parts = header.rsplit(SPACE, 2)\n    if len(parts) == 2:\n      assert parts[1] == b'missing'\n      raise self.MissingFileException(rev, relpath)\n\n    _, object_type, object_len = parts\n\n    # Read the object data\n    blob = bytes(self._cat_file_process.stdout.read(int(object_len)))\n\n    # Read the trailing newline\n    assert self._cat_file_process.stdout.read(1) == b'\\n'\n    assert len(blob) == int(object_len)\n    return object_type, blob"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef register_options(cls, register):\n    super(BundleMixin, cls).register_options(register)\n    register('--archive', choices=list(archive.TYPE_NAMES),\n             fingerprint=True,\n             help='Create an archive of this type from the bundle. '\n                  'This option is also defined in app target. '\n                  'Precedence is CLI option > target option > pants.ini option.')\n    # `target.id` ensures global uniqueness, this flag is provided primarily for\n    # backward compatibility.\n    register('--use-basename-prefix', advanced=True, type=bool,\n             help='Use target basename to prefix bundle folder or archive; otherwise a unique '\n                  'identifier derived from target will be used.')", "response": "Register options common to all bundle tasks."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resolved_option(options, target, key):\n    option_value = options.get(key)\n    if not isinstance(target, AppBase) or options.is_flagged(key):\n      return option_value\n    v = target.payload.get_field_value(key, None)\n    return option_value if v is None else v", "response": "Get value for option key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef publish_results(self, dist_dir, use_basename_prefix, vt, bundle_dir, archivepath, id, archive_ext):\n    # TODO (from mateor) move distdir management somewhere more general purpose.\n    name = vt.target.basename if use_basename_prefix else id\n    bundle_copy = os.path.join(dist_dir, '{}-bundle'.format(name))\n    absolute_symlink(bundle_dir, bundle_copy)\n    self.context.log.info(\n      'created bundle copy {}'.format(os.path.relpath(bundle_copy, get_buildroot())))\n\n    if archivepath:\n      ext = archive.archive_extensions.get(archive_ext, archive_ext)\n      archive_copy = os.path.join(dist_dir,'{}.{}'.format(name, ext))\n      safe_mkdir_for(archive_copy)  # Ensure parent dir exists\n      atomic_copy(archivepath, archive_copy)\n      self.context.log.info(\n        'created archive copy {}'.format(os.path.relpath(archive_copy, get_buildroot())))", "response": "Publish a copy of the bundle and archive from the results dir in dist."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nallow each resolver to declare additional product requirements.", "response": "def prepare(cls, options, round_manager):\n    \"\"\"Allow each resolver to declare additional product requirements.\"\"\"\n    super(NodeResolve, cls).prepare(options, round_manager)\n    for resolver in cls._resolver_by_type.values():\n      resolver.prepare(options, round_manager)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nregister a : class : JarImportProducts. JarImport for the given target.", "response": "def imported(self, target, coordinate, jar):\n    \"\"\"Registers a :class`JarImportProducts.JarImport` for the given target.\n\n    :param target: The :class:`pants.backend.jvm.targets.import_jars_mixin.ImportJarsMixin` target\n                   whose `all_imported_jar_deps` were resolved.\n    :param coordinate: The maven coordinate of the import jar.\n    :type coordinate: :class:`pants.java.jar_utls.M2Coordinate`\n    :param string jar: The path of the resolved import jar.\n    \"\"\"\n    if not isinstance(target, ImportJarsMixin):\n      raise ValueError('The given target is not an `ImportJarsMixin`: {}'.format(target))\n    self._imports[target].append(self.JarImport(coordinate, jar))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nnormalize the operating system name.", "response": "def normalize_os_name(os_name):\n  \"\"\"\n  :API: public\n  \"\"\"\n  if os_name not in OS_ALIASES:\n    for proper_name, aliases in OS_ALIASES.items():\n      if os_name in aliases:\n        return proper_name\n    logger.warning('Unknown operating system name: {bad}, known names are: {known}'\n                   .format(bad=os_name, known=', '.join(sorted(known_os_names()))))\n  return os_name"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef safe_kill(pid, signum):\n  assert(isinstance(pid, IntegerForPid))\n  assert(isinstance(signum, int))\n  try:\n    os.kill(pid, signum)\n  except (IOError, OSError) as e:\n    if e.errno in [errno.ESRCH, errno.EPERM]:\n      pass\n    elif e.errno == errno.EINVAL:\n      raise ValueError(\"Invalid signal number {}: {}\"\n                       .format(signum, e),\n                       e)\n    else:\n      raise", "response": "Kill a process with the specified signal catching nonfatal errors."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_closest_mac_host_platform_pair(darwin_version_upper_bound,\n                                       platform_name_map=SUPPORTED_PLATFORM_NORMALIZED_NAMES):\n  \"\"\"Return the (host, platform) pair for the highest known darwin version less than the bound.\"\"\"\n  darwin_versions = [int(x[1]) for x in platform_name_map if x[0] == 'darwin']\n  bounded_darwin_versions = [v for v in darwin_versions if v <= int(darwin_version_upper_bound)]\n  if not bounded_darwin_versions:\n    return None, None\n  max_darwin_version = str(max(bounded_darwin_versions))\n  return platform_name_map[('darwin', max_darwin_version)]", "response": "Return the host and platform pair closest to the highest known darwin version less than the bound."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if all of the artifact paths for this resolve point to existing files.", "response": "def all_linked_artifacts_exist(self):\n    \"\"\"All of the artifact paths for this resolve point to existing files.\"\"\"\n    if not self.has_resolved_artifacts:\n      return False\n    for path in self.resolved_artifact_paths:\n      if not os.path.isfile(path):\n        return False\n    else:\n      return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resolved_jars_for_each_target(self, conf, targets):\n    ivy_info = self._ivy_info_for(conf)\n\n    if not ivy_info:\n      return\n\n    jar_library_targets = [t for t in targets if isinstance(t, JarLibrary)]\n    ivy_jar_memo = {}\n    for target in jar_library_targets:\n      # Add the artifacts from each dependency module.\n      resolved_jars = self._resolved_jars_with_hardlinks(conf, ivy_info, ivy_jar_memo,\n                                                        self._jar_dependencies_for_target(conf,\n                                                                                          target),\n                                                        target)\n      yield target, resolved_jars", "response": "Yields the resolved jars for each target in targets."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef traverse_dependency_graph(self, ref, collector, memo=None):\n\n    resolved_ref = self.refs_by_unversioned_refs.get(ref.unversioned)\n    if resolved_ref:\n      ref = resolved_ref\n    if memo is None:\n      memo = dict()\n    visited = set()\n    return self._do_traverse_dependency_graph(ref, collector, memo, visited)", "response": "Traverses the dependency graph starting with ref collecting values for each ref into the set containing all the dependencies for that ref."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of artifacts for all jars for the given coordinates.", "response": "def get_resolved_jars_for_coordinates(self, coordinates, memo=None):\n    \"\"\"Collects jars for the passed coordinates.\n\n    Because artifacts are only fetched for the \"winning\" version of a module, the artifacts\n    will not always represent the version originally declared by the library.\n\n    This method is transitive within the passed coordinates dependencies.\n\n    :param coordinates collections.Iterable: Collection of coordinates to collect transitive\n                                             resolved jars for.\n    :param memo: See `traverse_dependency_graph`.\n    :returns: All the artifacts for all of the jars for the provided coordinates,\n              including transitive dependencies.\n    :rtype: list of :class:`pants.java.jar.ResolvedJar`\n    \"\"\"\n    def to_resolved_jar(jar_ref, jar_path):\n      return ResolvedJar(coordinate=M2Coordinate(org=jar_ref.org,\n                                                 name=jar_ref.name,\n                                                 rev=jar_ref.rev,\n                                                 classifier=jar_ref.classifier,\n                                                 ext=jar_ref.ext),\n                         cache_path=jar_path)\n    resolved_jars = OrderedSet()\n    def create_collection(dep):\n      return OrderedSet([dep])\n    for jar in coordinates:\n      classifier = jar.classifier if self._conf == 'default' else self._conf\n      jar_module_ref = IvyModuleRef(jar.org, jar.name, jar.rev, classifier, jar.ext)\n      for module_ref in self.traverse_dependency_graph(jar_module_ref, create_collection, memo):\n        for artifact_path in self._artifacts_by_ref[module_ref.unversioned]:\n          resolved_jars.add(to_resolved_jar(module_ref, artifact_path))\n    return resolved_jars"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting Ivy with the given ivy. xml and copies all relevant files into the workdir.", "response": "def do_resolve(cls, executor, extra_args, ivyxml, jvm_options, workdir_report_paths_by_conf,\n                 confs, ivy_resolution_cache_dir, ivy_cache_classpath_filename, resolve_hash_name,\n                 workunit_factory, workunit_name):\n    \"\"\"Execute Ivy with the given ivy.xml and copies all relevant files into the workdir.\n\n    This method does an Ivy resolve, which may be either a Pants resolve or a Pants fetch depending\n    on whether there is an existing frozen resolution.\n\n    After it is run, the Ivy reports are copied into the workdir at the paths specified by\n    workdir_report_paths_by_conf along with a file containing a list of all the requested artifacts\n    and their transitive dependencies.\n\n    :param executor: A JVM executor to use to invoke ivy.\n    :param extra_args: Extra arguments to pass to ivy.\n    :param ivyxml: The input ivy.xml containing the dependencies to resolve.\n    :param jvm_options: A list of jvm option strings to use for the ivy invoke, or None.\n    :param workdir_report_paths_by_conf: A dict mapping confs to report paths in the workdir.\n    :param confs: The confs used in the resolve.\n    :param resolve_hash_name: The hash to use as the module name for finding the ivy report file.\n    :param workunit_factory: A workunit factory for the ivy invoke, or None.\n    :param workunit_name: A workunit name for the ivy invoke, or None.\n    \"\"\"\n    ivy = Bootstrapper.default_ivy(bootstrap_workunit_factory=workunit_factory)\n\n    with safe_concurrent_creation(ivy_cache_classpath_filename) as raw_target_classpath_file_tmp:\n      extra_args = extra_args or []\n      args = ['-cachepath', raw_target_classpath_file_tmp] + extra_args\n\n      with cls._ivy_lock:\n        cls._exec_ivy(ivy, confs, ivyxml, args,\n                      jvm_options=jvm_options,\n                      executor=executor,\n                      workunit_name=workunit_name,\n                      workunit_factory=workunit_factory)\n\n      if not os.path.exists(raw_target_classpath_file_tmp):\n        raise cls.IvyError('Ivy failed to create classpath file at {}'\n                           .format(raw_target_classpath_file_tmp))\n\n      cls._copy_ivy_reports(workdir_report_paths_by_conf, confs, ivy_resolution_cache_dir, resolve_hash_name)\n\n    logger.debug('Moved ivy classfile file to {dest}'\n                 .format(dest=ivy_cache_classpath_filename))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef xml_report_path(cls, resolution_cache_dir, resolve_hash_name, conf):\n    return os.path.join(resolution_cache_dir, '{}-{}-{}.xml'.format(IvyUtils.INTERNAL_ORG_NAME,\n                                                                    resolve_hash_name, conf))", "response": "Returns the path to the xml report."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_xml_report(cls, conf, path):\n    if not os.path.exists(path):\n      raise cls.IvyResolveReportError('Missing expected ivy output file {}'.format(path))\n\n    logger.debug(\"Parsing ivy report {}\".format(path))\n    ret = IvyInfo(conf)\n    etree = ET.parse(path)\n    doc = etree.getroot()\n    for module in doc.findall('dependencies/module'):\n      org = module.get('organisation')\n      name = module.get('name')\n      for revision in module.findall('revision'):\n        rev = revision.get('name')\n        callers = []\n        for caller in revision.findall('caller'):\n          callers.append(IvyModuleRef(caller.get('organisation'),\n                                      caller.get('name'),\n                                      caller.get('callerrev')))\n\n        for artifact in revision.findall('artifacts/artifact'):\n          classifier = artifact.get('extra-classifier')\n          ext = artifact.get('ext')\n          ivy_module_ref = IvyModuleRef(org=org, name=name, rev=rev,\n                                        classifier=classifier, ext=ext)\n\n          artifact_cache_path = artifact.get('location')\n          ivy_module = IvyModule(ivy_module_ref, artifact_cache_path, tuple(callers))\n\n          ret.add_module(ivy_module)\n    return ret", "response": "Parse the ivy xml report corresponding to the name passed to ivy."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating an ivy xml with all jars marked as intransitive using the all conflict manager.", "response": "def generate_fetch_ivy(cls, jars, ivyxml, confs, resolve_hash_name):\n    \"\"\"Generates an ivy xml with all jars marked as intransitive using the all conflict manager.\"\"\"\n    org = IvyUtils.INTERNAL_ORG_NAME\n    name = resolve_hash_name\n\n    extra_configurations = [conf for conf in confs if conf and conf != 'default']\n\n    # Use org name _and_ rev so that we can have dependencies with different versions. This will\n    # allow for batching fetching if we want to do that.\n    jars_by_key = OrderedDict()\n    for jar in jars:\n      jars_by_key.setdefault((jar.org, jar.name, jar.rev), []).append(jar)\n\n\n    dependencies = [cls._generate_fetch_jar_template(_jars) for _jars in jars_by_key.values()]\n\n    template_data = TemplateData(org=org,\n                                 module=name,\n                                 extra_configurations=extra_configurations,\n                                 dependencies=dependencies)\n\n    template_relpath = os.path.join('templates', 'ivy_utils', 'ivy_fetch.xml.mustache')\n    cls._write_ivy_xml_file(ivyxml, template_data, template_relpath)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calculate_classpath(cls, targets):\n    jars = OrderedDict()\n    global_excludes = set()\n    provide_excludes = set()\n    targets_processed = set()\n\n    # Support the ivy force concept when we sanely can for internal dep conflicts.\n    # TODO(John Sirois): Consider supporting / implementing the configured ivy revision picking\n    # strategy generally.\n    def add_jar(jar):\n      # TODO(John Sirois): Maven allows for depending on an artifact at one rev and one of its\n      # attachments (classified artifacts) at another.  Ivy does not, allow this, the dependency\n      # can carry only 1 rev and that hosts multiple artifacts for that rev.  This conflict\n      # resolution happens at the classifier level, allowing skew in a\n      # multi-artifact/multi-classifier dependency.  We only find out about the skew later in\n      # `_generate_jar_template` below which will blow up with a conflict.  Move this logic closer\n      # together to get a more clear validate, then emit ivy.xml then resolve flow instead of the\n      # spread-out validations happening here.\n      # See: https://github.com/pantsbuild/pants/issues/2239\n      coordinate = (jar.org, jar.name, jar.classifier)\n      existing = jars.get(coordinate)\n      jars[coordinate] = jar if not existing else cls._resolve_conflict(existing=existing,\n                                                                        proposed=jar)\n\n    def collect_jars(target):\n      if isinstance(target, JarLibrary):\n        for jar in target.jar_dependencies:\n          add_jar(jar)\n\n    def collect_excludes(target):\n      target_excludes = target.payload.get_field_value('excludes')\n      if target_excludes:\n        global_excludes.update(target_excludes)\n\n    def collect_provide_excludes(target):\n      if not (isinstance(target, ExportableJvmLibrary) and target.provides):\n        return\n      logger.debug('Automatically excluding jar {}.{}, which is provided by {}'.format(\n        target.provides.org, target.provides.name, target))\n      provide_excludes.add(Exclude(org=target.provides.org, name=target.provides.name))\n\n    def collect_elements(target):\n      targets_processed.add(target)\n      collect_jars(target)\n      collect_excludes(target)\n      collect_provide_excludes(target)\n\n    for target in targets:\n      target.walk(collect_elements, predicate=lambda target: target not in targets_processed)\n\n    # If a source dep is exported (ie, has a provides clause), it should always override\n    # remote/binary versions of itself, ie \"round trip\" dependencies.\n    # TODO: Move back to applying provides excludes as target-level excludes when they are no\n    # longer global.\n    if provide_excludes:\n      additional_excludes = tuple(provide_excludes)\n      new_jars = OrderedDict()\n      for coordinate, jar in jars.items():\n        new_jars[coordinate] = jar.copy(excludes=jar.excludes + additional_excludes)\n      jars = new_jars\n\n    return list(jars.values()), global_excludes", "response": "Creates a consistent classpath and list of excludes for the passed targets."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the attribute field_name from target if set else from this subsystem s options.", "response": "def get_scalar_mirrored_target_option(self, option_name, target):\n    \"\"\"Get the attribute `field_name` from `target` if set, else from this subsystem's options.\"\"\"\n    mirrored_option_declaration = self._mirrored_option_declarations[option_name]\n    return mirrored_option_declaration.get_mirrored_scalar_option_value(target)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef scoped(cls, optionable, removal_version=None, removal_hint=None):\n    return SubsystemDependency(cls, optionable.options_scope, removal_version, removal_hint)", "response": "Returns a dependency on this subsystem scoped to optionable."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scoped_instance(cls, optionable):\n    if not isinstance(optionable, Optionable) and not issubclass(optionable, Optionable):\n      raise TypeError('Can only scope an instance against an Optionable, given {} of type {}.'\n                      .format(optionable, type(optionable)))\n    return cls._instance_for_scope(cls.subscope(optionable.options_scope))", "response": "Returns an instance of this subsystem for exclusive use by the given optionable."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nblocks until the service is in the Paused state then returns True.", "response": "def await_paused(self, timeout=None):\n    \"\"\"Blocks until the service is in the Paused state, then returns True.\n\n    If a timeout is specified, the method may return False to indicate a timeout: with no timeout\n    it will always (eventually) return True.\n\n    Raises if the service is not currently in the Pausing state.\n    \"\"\"\n    deadline = time.time() + timeout if timeout else None\n    with self._lock:\n      # Wait until the service transitions out of Pausing.\n      while self._state != self._PAUSED:\n        if self._state != self._PAUSING:\n          raise AssertionError('Cannot wait for {} to reach `{}` while it is in `{}`.'.format(self, self._PAUSED, self._state))\n        timeout = deadline - time.time() if deadline else None\n        if timeout and timeout <= 0:\n          return False\n        self._condition.wait(timeout=timeout)\n      return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncall by the service to indicate that the resource is paused.", "response": "def maybe_pause(self, timeout=None):\n    \"\"\"Called by the service to indicate that it is pausable.\n\n    If the service calls this method while the state is `Pausing`, the state will transition\n    to `Paused`, and the service will block here until it is marked `Running` or `Terminating`.\n\n    If the state is not currently `Pausing`, and a timeout is not passed, this method returns\n    immediately. If a timeout is passed, this method blocks up to that number of seconds to wait\n    to transition to `Pausing`.\n    \"\"\"\n    deadline = time.time() + timeout if timeout else None\n    with self._lock:\n      while self._state != self._PAUSING:\n        # If we've been terminated, or the deadline has passed, return.\n        timeout = deadline - time.time() if deadline else None\n        if self._state == self._TERMINATING or not timeout or timeout <= 0:\n          return\n        # Otherwise, wait for the state to change.\n        self._condition.wait(timeout=timeout)\n\n      # Set Paused, and then wait until we are no longer Paused.\n      self._set_state(self._PAUSED, self._PAUSING)\n      while self._state == self._PAUSED:\n        self._condition.wait()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrequesting that the service is in the Paused state without waiting for it to do so.", "response": "def mark_pausing(self):\n    \"\"\"Requests that the service move to the Paused state, without waiting for it to do so.\n\n    Raises if the service is not currently in the Running state.\n    \"\"\"\n    with self._lock:\n      self._set_state(self._PAUSING, self._RUNNING)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmove the service to the Paused state.", "response": "def mark_running(self):\n    \"\"\"Moves the service to the Running state.\n\n    Raises if the service is not currently in the Paused state.\n    \"\"\"\n    with self._lock:\n      self._set_state(self._RUNNING, self._PAUSED)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nnormalizing keys of kwargs to be ascii.", "response": "def _normalize_utf8_keys(kwargs):\n  \"\"\"When kwargs are passed literally in a source file, their keys are ascii: normalize.\"\"\"\n  if any(type(key) is binary_type for key in kwargs.keys()):\n    # This is to preserve the original dict type for kwargs.\n    dict_type = type(kwargs)\n    return dict_type([(text_type(k), v) for k, v in kwargs.items()])\n  return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dict of the kwargs for this Struct which were not interpreted by the baseclass.", "response": "def kwargs(self):\n    \"\"\"Returns a dict of the kwargs for this Struct which were not interpreted by the baseclass.\n\n    This excludes fields like `extends`, `merges`, and `abstract`, which are consumed by\n    SerializableFactory.create and Validatable.validate.\n    \"\"\"\n    return {k: v for k, v in self._kwargs.items() if k not in self._INTERNAL_FIELDS}"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the type alias this target was constructed via.", "response": "def type_alias(self):\n    \"\"\"Return the type alias this target was constructed via.\n\n    For a target read from a BUILD file, this will be target alias, like 'java_library'.\n    For a target constructed in memory, this will be the simple class name, like 'JavaLibrary'.\n\n    The end result is that the type alias should be the most natural way to refer to this target's\n    type to the author of the target instance.\n\n    :rtype: string\n    \"\"\"\n    type_alias = self._kwargs.get(self._TYPE_ALIAS_FIELD, None)\n    return type_alias if type_alias is not None else type(self).__name__"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncompiles the source code into a module object.", "response": "def compile_file(source, globals_=None):\n  \"\"\"Compile by saving to file and importing that.\n\n  Compiling the AST/source code this way ensures that the source code is\n  readable by e.g. `pdb` or `inspect`.\n\n  Args:\n    source: The code to compile, either as a string or as an AST.\n    globals_: A dictionary of variables that should be available as globals in\n        the compiled module. They will be monkey patched after importing the\n        module.\n\n  Returns:\n    A module object containing the compiled source code.\n  \"\"\"\n  if isinstance(source, gast.AST):\n    source = quoting.to_source(source)\n\n  # Write source to temporary file\n  tempdir = tempfile.mkdtemp()\n  uuid = str(uuid4().hex[:4])\n  tmpname = os.path.join(tempdir, 'tangent_%s.py' % uuid)\n  with open(tmpname, 'w') as f:\n    f.write(source)\n\n  # Load the temporary file as a module\n  module_name = 'tangent_%s' % uuid\n  if six.PY3:\n    spec = util.spec_from_file_location(module_name, tmpname)\n    m = util.module_from_spec(spec)\n    spec.loader.exec_module(m)\n  else:\n    m = imp.load_source(module_name, tmpname)\n\n  # Update the modules namespace\n  if globals_:\n    m.__dict__.update(globals_)\n  return m"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compile_function(node, globals_=None):\n  if not isinstance(node, gast.AST):\n    if not isinstance(node, six.string_types):\n      raise TypeError\n    node = gast.parse(node)\n  if isinstance(node, gast.Module):\n    for succ in node.body:\n      if isinstance(succ, gast.FunctionDef):\n        name = succ.name\n        break\n    else:\n      raise ValueError('no function found')\n  elif isinstance(node, gast.FunctionDef):\n    name = node.name\n  else:\n    raise TypeError\n  module = compile_file(node, globals_)\n  return getattr(module, name)", "response": "Convert an AST or string into a function with inspectable source."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef autodiff_ast(func, wrt, motion, mode, preserve_result, check_dims, verbose):\n  node = annotate.resolve_calls(func)\n  node = desugar.explicit_loop_indexes(node)\n  fence.validate(node, inspect.getsource(func))\n  node = anf_.anf(node)\n  if verbose >= 2:\n    print('ANF')\n    print(quoting.to_source(node))\n  if mode == 'reverse':\n    node, required, stack = reverse_ad.reverse_ad(node.body[0], wrt,\n                                                  preserve_result, check_dims)\n    if verbose >= 2:\n      print('RAW')\n      print(quoting.to_source(node))\n    if motion == 'split':\n      node = reverse_ad.split(node, stack)\n    else:\n      node = reverse_ad.joint(node)\n    if verbose >= 2:\n      print('MOTION')\n      print(quoting.to_source(node))\n  elif mode == 'forward':\n    node, required = forward_ad.forward_ad(node.body[0], wrt, preserve_result,\n                                           check_dims)\n  return node, required", "response": "Perform AD on a single function and return the AST."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef autodiff_tree(func, wrt, motion, mode, preserve_result, check_dims,\n                  verbose):\n  \"\"\"Perform AD on all functions in a call tree.\n\n  This function walks the call tree and differentiates each function in it. It\n  also ensures that the global namespaces that each function in the call tree\n  was in are merged.\n\n  The `tangent` and `numpy` packages are added to the namespace here, so that\n  the gradient templates can assume that they are present.\n\n  Args:\n    See `grad`.\n\n  Returns:\n    final: A single module which contains the primals and adjoints of all the\n        functions in the call tree.\n    namespace: A merged dictionary with all the variables in the global\n        namespaces of each function. The primals and adjoints need access to\n        these in order to execute.\n  \"\"\"\n  # Imported here to avoid circular imports\n  import tangent\n  namespace = {'tangent': tangent, 'numpy': numpy}\n\n  done = set()\n  final = gast.Module(body=[])\n  namespace.update(six.get_function_globals(func))\n\n  node, required = autodiff_ast(func, wrt, motion, mode, preserve_result,\n                                check_dims, verbose)\n  final.body.extend(node.body)\n\n  to_do = set(required)\n  if motion == 'split' and mode == 'reverse':\n    done.add((func, wrt))\n    to_do -= done\n\n  while to_do:\n    func, wrt = to_do.pop()\n    namespace.update(six.get_function_globals(func))\n\n    node, required = autodiff_ast(\n        func=func,\n        wrt=wrt,\n        motion='split',\n        mode=mode,\n        preserve_result=True,\n        check_dims=False,\n        verbose=verbose)\n\n    final.body.extend(node.body)\n    done.add((func, wrt))\n    to_do.update(required)\n    to_do -= done\n\n  return final, namespace", "response": "Perform AD on all functions in a call tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef vjp(func,\n        wrt=(0,),\n        optimized=True,\n        check_dims=True,\n        preserve_result=False,\n        verbose=0):\n  \"\"\"Convenience function to produce vector-Jacobian products.\n\n  See `autodiff` for function arguments.\n  Uses reverse-mode joint-motion autodiff to produce the VJP.\n  \"\"\"\n  return autodiff(\n      func,\n      wrt=wrt,\n      motion='joint',\n      mode='reverse',\n      optimized=optimized,\n      preserve_result=preserve_result,\n      input_derivative=INPUT_DERIVATIVE.Required,\n      check_dims=check_dims,\n      verbose=verbose)", "response": "Convenience function to produce vector - Jacobian products."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild the vector-Jacobian or Jacobian-vector product of a function `func`. For a vector-Jacobian product (reverse-mode autodiff): This function proceeds by finding the primals and adjoints of all the functions in the call tree. For a Jacobian-vector product (forward-mode autodiff): We first find the primals and tangents of all functions in the call tree. It then wraps the top level function (i.e. the one passed as `func`) in a slightly more user-friendly interface. It then compiles the function and attaches to it the global namespace it needs to run. Args: func: The function to take the gradient of. wrt: A tuple of argument indices to differentiate with respect to. By default the derivative is taken with respect to the first argument. optimized: Whether to optimize the gradient function (`True` by default). motion: Either 'split' (separate functions for forward and backward pass) or 'joint' motion (a single combined function). Joint mode is the default. mode: Either 'forward' or 'reverse' mode. Forward mode is more efficient when the input dimensionality is lower than the output dimensionality, whereas it is the opposite for reverse mode. input_derivative: An enum indicating whether the user must supply an input derivative, and if not, what the default value is. See the possible values of INPUT_DERIVATIVE in this file. preserve_result: A boolean indicating whether or not the generated gradient function should also return the output of the original function. If False, the return signature of the input and output functions will be > val = func(*args) > df = grad(func,preserve_result=False) > gradval = df(*args) If True, > val = func(*args) > df = grad(func,preserve_result=True) > gradval, val = df(*args) Note that if taking gradients with respect to multiple arguments, the primal value will be appended to the return signature. Ex: > val = func(x,y) > df = grad(func,wrt=(0,1),preserve_result=True) > dx,dy,val = df(x,y) verbose: If 1 the source code of the generated functions will be output to stdout at various stages of the process for debugging purposes. If > 1, all intermediate code generation steps will print. Returns: df: A function that calculates a derivative (see file-level documentation above for the kinds of derivatives available) with respect to arguments specified in `wrt`, using forward or reverse mode according to `mode`. If using reverse mode, the gradient is calculated in either split or joint motion according to the value passed in `motion`. If `preserve_result` is True, the function will also return the original result of `func`.", "response": "def autodiff(func,\n             wrt=(0,),\n             optimized=True,\n             motion='joint',\n             mode='reverse',\n             preserve_result=False,\n             check_dims=True,\n             input_derivative=INPUT_DERIVATIVE.Required,\n             verbose=0):\n  \"\"\"Build the vector-Jacobian or Jacobian-vector product of a function `func`.\n\n  For a vector-Jacobian product (reverse-mode autodiff):\n  This function proceeds by finding the primals and adjoints of all the\n  functions in the call tree.\n  For a Jacobian-vector product (forward-mode autodiff):\n  We first find the primals and tangents of all functions in the call tree.\n\n  It then wraps the top level function (i.e. the\n  one passed as `func`) in a slightly more user-friendly interface. It then\n  compiles the function and attaches to it the global namespace it needs to\n  run.\n\n  Args:\n    func: The function to take the gradient of.\n    wrt: A tuple of argument indices to differentiate with respect to. By\n        default the derivative is taken with respect to the first argument.\n    optimized: Whether to optimize the gradient function (`True` by default).\n    motion: Either 'split' (separate functions for forward and backward pass)\n        or 'joint' motion (a single combined function). Joint mode is the\n        default.\n    mode: Either 'forward' or 'reverse' mode. Forward mode is more efficient\n        when the input dimensionality is lower than the output dimensionality,\n        whereas it is the opposite for reverse mode.\n    input_derivative: An enum indicating whether the user must supply an input\n        derivative, and if not, what the default value is. See the\n        possible values of INPUT_DERIVATIVE in this file.\n\n    preserve_result: A boolean indicating whether or not the generated gradient\n        function should also return the output of the original function.\n        If False, the return signature of the input and output functions will be\n        > val = func(*args)\n        > df = grad(func,preserve_result=False)\n        > gradval = df(*args)\n        If True,\n        > val = func(*args)\n        > df = grad(func,preserve_result=True)\n        > gradval, val = df(*args)\n        Note that if taking gradients with respect to multiple arguments,\n        the primal value will be appended to the return signature. Ex:\n        > val = func(x,y)\n        > df = grad(func,wrt=(0,1),preserve_result=True)\n        > dx,dy,val = df(x,y)\n\n    verbose: If 1 the source code of the generated functions will be\n        output to stdout at various stages of the process for debugging\n        purposes. If > 1, all intermediate code generation steps will print.\n\n  Returns:\n    df: A function that calculates a derivative (see file-level documentation\n    above\n        for the kinds of derivatives available) with respect to arguments\n        specified in `wrt`, using forward or reverse mode according to `mode`.\n        If using reverse mode, the gradient is calculated in either split\n        or joint motion according to the value passed in `motion`. If\n        `preserve_result` is True, the function will also return the original\n        result of `func`.\n  \"\"\"\n  # If the function had the with insert_grad_of statements removed, retrieve them\n  func = getattr(func, 'tangent', func)\n\n  # Generate the derivative\n  node, namespace = autodiff_tree(func, wrt, motion, mode, preserve_result,\n                                  check_dims, verbose)\n\n  if mode == 'reverse' and motion == 'joint':\n    # Pull the stack definition and initial gradient into the function body\n    # TODO: Use first FunctionDef instead of first element\n    node.body[0] = _create_joint(node.body[0], func, wrt, input_derivative)\n    if verbose >= 2:\n      print('INLINED')\n      print(quoting.to_source(node))\n  if mode == 'forward':\n    node = _create_forward(node)\n  if optimized:\n    # Optimize the resulting functions\n    node = optimization.optimize(node)\n  node = comments.remove_repeated_comments(node)\n  if verbose >= 1:\n    print(quoting.to_source(node))\n\n  # Compile and return\n  module = compile_.compile_file(node, namespace)\n  if mode == 'forward' or motion == 'joint':\n    return getattr(module, node.body[0].name)\n  else:\n    # Compiling the top-level function in split mode makes no sense, but we use\n    # it for testing; hence we don't care about the source being readable\n    forward = getattr(module, node.body[0].name)\n    backward = getattr(module, node.body[1].name)\n\n    # Imported here to avoid circular imports\n    import tangent\n\n    def df(*args, **kwargs):\n      _stack = tangent.Stack()\n      init_grad = kwargs.pop('init_grad', 1.0)\n      forward(_stack, *args, **kwargs)\n      dx = backward(_stack, init_grad, *args, **kwargs)\n      if len(dx) == 1:\n        dx, = dx\n      return dx\n\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef grad(func,\n         wrt=(0,),\n         optimized=True,\n         preserve_result=False,\n         check_dims=True,\n         verbose=0):\n  \"\"\"Return the gradient of a function `func`.\n  Args:\n    func: The function to take the gradient of.\n    wrt: A tuple of argument indices to differentiate with respect to. By\n        default the derivative is taken with respect to the first argument.\n    optimized: Whether to optimize the gradient function (`True` by default).\n    preserve_result: A boolean indicating whether or not the generated gradient\n        function should also return the output of the original function.\n        If False, the return signature of the input and output functions will be\n        > val = func(*args)\n        > df = grad(func,preserve_result=False)\n        > gradval = df(*args)\n        If True,\n        > val = func(*args)\n        > df = grad(func,preserve_result=True)\n        > gradval, val = df(*args)\n        Note that if taking gradients with respect to multiple arguments,\n        the primal value will be appended to the return signature. Ex:\n        > val = func(x,y)\n        > df = grad(func,wrt=(0,1),preserve_result=True)\n        > dx,dy,val = df(x,y)\n    check_dims: A boolean (`True` by default) indicating whether to check\n        that the result of the original function `func` is a scalar, raising\n        an error if it is not.\n        Gradients are only valid for scalar-valued outputs, so we check\n        this by defualt.\n    verbose: If 1 the source code of the generated functions will be\n        output to stdout at various stages of the process for debugging\n        purposes. If > 1, all intermediate code generation steps will print.\n\n  Returns:\n    df: A function that calculates the gradient with respect to arguments\n        specified in `wrt`, using forward or reverse mode according to `mode`.\n        If using reverse mode, the gradient is calculated in either split\n        or joint motion according to the value passed in `motion`. If\n        `preserve_result` is True, the function will also return the original\n        result of `func`.\n  \"\"\"\n  return autodiff(\n      func,\n      wrt=wrt,\n      motion='joint',\n      mode='reverse',\n      optimized=optimized,\n      preserve_result=preserve_result,\n      check_dims=check_dims,\n      input_derivative=INPUT_DERIVATIVE.DefaultOne,\n      verbose=verbose)", "response": "Returns the gradient of a function func."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a user - friendly gradient function.", "response": "def _create_joint(fwdbwd, func, wrt, input_derivative):\n  \"\"\"Create a user-friendly gradient function.\n\n  By default, gradient functions expect the stack to be passed to them\n  explicitly. This function modifies the function so that the stack doesn't\n  need to be passed and gets initialized in the function body instead.\n\n  For consistency, gradient functions always return a tuple, even if the\n  gradient of only one input was required. We unpack the tuple if it is of\n  length one.\n\n  Args:\n    fwdbwd: An AST. The function definition of the joint primal and adjoint.\n    func: A function handle. The original function that was differentiated.\n    wrt: A tuple of integers. The arguments with respect to which we differentiated.\n\n  Returns:\n    The function definition of the new function.\n  \"\"\"\n  # Correct return to be a non-tuple if there's only one element\n  retval = fwdbwd.body[-1]\n  if len(retval.value.elts) == 1:\n    retval.value = retval.value.elts[0]\n\n  # Make a stack init statement\n  init_stack = quoting.quote('%s = tangent.Stack()' % fwdbwd.args.args[0].id)\n  init_stack = comments.add_comment(init_stack, 'Initialize the tape')\n\n  # Prepend the stack init to the top of the function\n  fwdbwd.body = [init_stack] + fwdbwd.body\n\n  # Replace the function arguments with the original ones\n  grad_name = fwdbwd.args.args[1].id\n  fwdbwd.args = quoting.parse_function(func).body[0].args\n\n  # Give the function a nice name\n  fwdbwd.name = naming.joint_name(func, wrt)\n\n  # Allow the initial gradient to be passed as a keyword argument\n  fwdbwd = ast_.append_args(fwdbwd, [grad_name])\n  if input_derivative == INPUT_DERIVATIVE.DefaultOne:\n    fwdbwd.args.defaults.append(quoting.quote('1.0'))\n  return fwdbwd"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _create_forward(out_node):\n  retval = out_node.body[0].body[-1]\n  if len(retval.value.elts) == 1:\n    retval.value = retval.value.elts[0]\n  return out_node", "response": "Create a user - friendly forward function."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tangent(f):\n  node = annotate.resolve_calls(f)\n  RemoveWith().visit(node)\n  wrapped = functools.wraps(f)(compile_.compile_function(node))\n  wrapped.tangent = f\n  return wrapped", "response": "A decorator which removes the insert_grad_of statement."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform a given analysis on all functions within an AST.", "response": "def forward(node, analysis):\n  \"\"\"Perform a given analysis on all functions within an AST.\"\"\"\n  if not isinstance(analysis, Forward):\n    raise TypeError('not a valid forward analysis object')\n  for succ in gast.walk(node):\n    if isinstance(succ, gast.FunctionDef):\n      cfg_obj = CFG.build_cfg(succ)\n      analysis.visit(cfg_obj.entry)\n  return node"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef backlink(node):\n    seen = set()\n    to_see = [node]\n    while to_see:\n      node = to_see.pop()\n      seen.add(node)\n      for succ in node.next:\n        succ.prev.add(node)\n        if succ not in seen:\n          to_see.append(succ)", "response": "Given a CFG with outgoing links create incoming links."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_head(self, node):\n    for head in self.head:\n      head.next.add(node)\n    self.head[:] = []\n    self.head.append(node)", "response": "Link this node to the current leaves."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild a CFG object for a function.", "response": "def build_cfg(cls, node):\n    \"\"\"Build a CFG for a function.\n\n    Args:\n      node: A function definition the body of which to analyze.\n\n    Returns:\n      A CFG object.\n\n    Raises:\n      TypeError: If the input is not a function definition.\n    \"\"\"\n    if not isinstance(node, gast.FunctionDef):\n      raise TypeError('input must be a function definition')\n    cfg = cls()\n    cfg.entry = Node(node.args)\n    cfg.head = [cfg.entry]\n    cfg.visit_statements(node.body)\n    cfg.exit = Node(None)\n    cfg.set_head(cfg.exit)\n    cfg.backlink(cfg.entry)\n    return cfg"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming a series of optimization passes. This function performs a series of optimizations (dead code elimination, constant folding, variable folding) on the given AST. It optimizes the code repeatedly until reaching a fixed point. The fixed point is determine roughly by checking whether the number of lines of generated source code changed after the latest pass. Args: node: The AST to optimize. Returns: The optimized AST.", "response": "def optimize(node):\n  \"\"\"Perform a series of optimization passes.\n\n  This function performs a series of optimizations (dead code elimination,\n  constant folding, variable folding) on the given AST.\n  It optimizes the code repeatedly until reaching a fixed point. The fixed\n  point is determine roughly by checking whether the number of lines of\n  generated source code changed after the latest pass.\n\n  Args:\n    node: The AST to optimize.\n  Returns:\n    The optimized AST.\n  \"\"\"\n  node = dead_code_elimination(node)\n  node = constant_folding(node)\n  node = assignment_propagation(node)\n  return node"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform a simple form of dead code elimination on a Python AST. This method performs reaching definitions analysis on all function definitions. It then looks for the definition of variables that are not used elsewhere and removes those definitions. This function takes into consideration push and pop statements; if a pop statement is removed, it will also try to remove the accompanying push statement. Note that this *requires dead code elimination to be performed on the primal and adjoint simultaneously*. Args: node: The AST to optimize. Returns: The optimized AST.", "response": "def dead_code_elimination(node):\n  \"\"\"Perform a simple form of dead code elimination on a Python AST.\n\n  This method performs reaching definitions analysis on all function\n  definitions. It then looks for the definition of variables that are not used\n  elsewhere and removes those definitions.\n\n  This function takes into consideration push and pop statements; if a pop\n  statement is removed, it will also try to remove the accompanying push\n  statement. Note that this *requires dead code elimination to be performed on\n  the primal and adjoint simultaneously*.\n\n  Args:\n    node: The AST to optimize.\n\n  Returns:\n    The optimized AST.\n  \"\"\"\n  to_remove = set(def_[1] for def_ in annotate.unused(node)\n                  if not isinstance(def_[1], (gast.arguments, gast.For)))\n  for n in list(to_remove):\n    for succ in gast.walk(n):\n      if anno.getanno(succ, 'push', False):\n        to_remove.add(anno.getanno(succ, 'push'))\n  transformers.Remove(to_remove).visit(node)\n  anno.clearanno(node)\n  return node"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_counts(node):\n  cfg.forward(node, cfg.ReachingDefinitions())\n\n  rc = ReadCounts()\n  rc.visit(node)\n  return rc.n_read", "response": "Check how many times a variable definition was used."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform assignment propagation. Assignment propagation is not a compiler optimization as much as a readability optimization. If a variable name is used only once, it gets renamed when possible e.g. `y = x; z = y` will become `z = x`. Args: node: The AST to optimize. Returns: The optimized AST.", "response": "def assignment_propagation(node):\n  \"\"\"Perform assignment propagation.\n\n  Assignment propagation is not a compiler optimization as much as a\n  readability optimization. If a variable name is used only once, it gets\n  renamed when possible e.g. `y = x; z = y` will become `z = x`.\n\n  Args:\n    node: The AST to optimize.\n\n  Returns:\n    The optimized AST.\n  \"\"\"\n  n_reads = read_counts(node)\n\n  to_remove = []\n  for succ in gast.walk(node):\n    # We found an assignment of the form a = b\n    # - Left-hand side is a Name, right-hand side is a Name.\n    if (isinstance(succ, gast.Assign) and isinstance(succ.value, gast.Name) and\n        len(succ.targets) == 1 and isinstance(succ.targets[0], gast.Name)):\n      rhs_name = succ.value.id\n      # We now find all the places that b was defined\n      rhs_defs = [def_[1] for def_ in anno.getanno(succ, 'definitions_in')\n                  if def_[0] == rhs_name]\n      # If b was defined in only one place (not an argument), and wasn't used\n      # anywhere else but in a == b, and was defined as b = x, then we can fold\n      # the statements\n      if (len(rhs_defs) == 1 and isinstance(rhs_defs[0], gast.Assign) and\n          n_reads[rhs_defs[0]] == 1 and\n          isinstance(rhs_defs[0].value, gast.Name) and\n          isinstance(rhs_defs[0].targets[0], gast.Name)):\n        # Mark rhs_def for deletion\n        to_remove.append(rhs_defs[0])\n        # Propagate the definition\n        succ.value = rhs_defs[0].value\n\n  # Remove the definitions we folded\n  transformers.Remove(to_remove).visit(node)\n  anno.clearanno(node)\n  return node"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unbroadcast_tfe_to(tensor, shape):\n  axis = utils.create_unbroadcast_axis(shape, shape_as_list(tensor))\n  return tf.reshape(tf.reduce_sum(tensor, axis=axis), shape)", "response": "Reverse the broadcasting operation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unreduce_tensor(tensor, shape, axis, keepdims):\n  if not keepdims:\n    if axis is None:\n      axis = range(len(shape))\n    elif isinstance(axis, int):\n      axis = axis,\n    for ax in sorted(axis):\n      tensor = tf.expand_dims(tensor, ax)\n  tile_shape = np.array(shape) / np.array(shape_as_list(tensor))\n  return tf.tile(tensor, tile_shape)", "response": "Reverse summing over a dimension."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef matmul_adjoint_x(dz, x, y, transpose_a, transpose_b):\n  if not transpose_a and not transpose_b:\n    return tf.matmul(dz, y, transpose_b=True)\n  elif not transpose_a and transpose_b:\n    return tf.matmul(dz, y)\n  elif transpose_a and not transpose_b:\n    return tf.matmul(y, dz, transpose_b=True)\n  else:  # transpose_a and transpose_b\n    return tf.matmul(y, dz, transpose_a=True, transpose_b=True)", "response": "Implementation of dtfmatmul wrt x separate for readability."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nname for the primal of a function.", "response": "def primal_name(func, wrt):\n  \"\"\"Name for the primal of a function.\"\"\"\n  if not isinstance(func, types.FunctionType):\n    raise TypeError(func)\n  varnames = six.get_function_code(func).co_varnames\n  return PRIMAL_NAME.format(func.__name__, ''.join(varnames[i] for i in wrt))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake sure that a method returns a unique name.", "response": "def uniqify(func):\n  \"\"\"Make sure that a method returns a unique name.\"\"\"\n  @six.wraps(func)\n  def unique(self, *args, **kwargs):\n    return self.unique(func(self, *args, **kwargs))\n  return unique"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef uniqify_once(func):\n  @six.wraps(func)\n  def unique_once(self, *args, **kwargs):\n    return self.unique_once(func(self, *args, **kwargs))\n  return unique_once", "response": "Make sure that a method returns a unique name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconstructing a namer object for a given function scope.", "response": "def build(cls, node):\n    \"\"\"Construct a namer object for a given function scope.\"\"\"\n    if not isinstance(node, gast.FunctionDef):\n      raise ValueError\n    namer = cls()\n    namer.names.update(get_names(node))\n    return namer"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef valid(self, name):\n    name = re.sub('[^0-9a-zA-Z_]', '', name)\n    if re.match('[0-9]', name):\n      name = '_' + name\n    return name", "response": "Ensure a variable name is valid."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unique(self, name):\n    # Make sure the name is valid\n    name = self.valid(name)\n    # Make sure it's not too long\n    name = self.trim(name)\n    # Now make sure it's unique\n    unique_name = name\n    i = 2\n    while unique_name in self.names:\n      unique_name = name + str(i)\n      i += 1\n    self.names.add(unique_name)\n    return unique_name", "response": "Make a variable name unique by appending a number if needed."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef array_size(x, axis):\n  axis_shape = x.shape if axis is None else tuple(x.shape[a] for a in axis)\n  return max(numpy.prod(axis_shape), 1)", "response": "Calculate the size of x along axis dimensions only."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreverse the broadcasting operation.", "response": "def unbroadcast(array, like):\n  \"\"\"Reverse the broadcasting operation.\n\n  Args:\n    array: An array.\n    like: An array that could have been broadcasted to the shape of array.\n\n  Returns:\n    Tensor with certain dimensions summed to match the shape of `like`.\n  \"\"\"\n  unbroadcaster = unbroadcasters[type(array)]\n  return unbroadcaster(array, like)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_unbroadcast_axis(shape, broadcast_shape):\n  return tuple(\n      -(1 + i)\n      for i in range(len(broadcast_shape))\n      if i >= len(shape) or broadcast_shape[-(1 + i)] > shape[-(1 + i)])", "response": "Creates the reduction axis for unbroadcasting."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreversing the broadcasting operation.", "response": "def unbroadcast_numpy_to(array, shape):\n  \"\"\"Reverse the broadcasting operation.\n\n  Args:\n    array: An array.\n    shape: A shape that could have been broadcasted to the shape of array.\n\n  Returns:\n    Array with dimensions summed to match `shape`.\n  \"\"\"\n  axis = create_unbroadcast_axis(shape, numpy.shape(array))\n  return numpy.reshape(numpy.sum(array, axis=axis), shape)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreverse summing over a dimension.", "response": "def unreduce(array, shape, axis, keepdims):\n  \"\"\"Reverse summing over a dimension.\n\n  Args:\n    array: The array that was reduced.\n    shape: The original shape of the array before reduction.\n    axis: The axis or axes that were summed.\n    keepdims: Whether these axes were kept as singleton axes.\n\n  Returns:\n    An array with axes broadcast to match the shape of the original array.\n  \"\"\"\n  unreducer = unreducers[type(array)]\n  return unreducer(array, shape, axis, keepdims)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreverse summing over a dimension.", "response": "def unreduce_like(array, original_array, axis, keepdims):\n  \"\"\"Reverse summing over a dimension.\n\n  Args:\n    array: The array that was reduced.\n    original_array: An array whose shape to unreduce to.\n    axis: The axis or axes that were summed.\n    keepdims: Whether these axes were kept as singleton axes.\n\n  Returns:\n    An array with axes broadcast to match the shape of the original array.\n  \"\"\"\n  atype = type(array)\n  unreducer = unreducers[atype]\n  shape = shape_functions[atype]\n  return unreducer(array, shape(original_array), axis, keepdims)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreversing summing over a dimension NumPy implementation.", "response": "def unreduce_array(array, shape, axis, keepdims):\n  \"\"\"Reverse summing over a dimension, NumPy implementation.\n\n  Args:\n    array: The array that was reduced.\n    shape: The original shape of the array before reduction.\n    axis: The axis or axes that were summed.\n    keepdims: Whether these axes were kept as singleton axes.\n\n  Returns:\n    An array with axes broadcast to match the shape of the original array.\n  \"\"\"\n  # NumPy uses a special default value for keepdims, which is equivalent to\n  # False.\n  if axis is not None and (not keepdims or keepdims is numpy._NoValue):  # pylint: disable=protected-access\n    if isinstance(axis, int):\n      axis = axis,\n    for ax in sorted(axis):\n      array = numpy.expand_dims(array, ax)\n  return numpy.broadcast_to(array, shape)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes the gradient for an object.", "response": "def init_grad(obj, allow_lazy_initializer=False):\n  \"\"\"Initialize the gradient for an object.\n\n  Args:\n    obj: The object to initialize the gradient for, can be either a number,\n      array, tuple, list, or dictionary.\n    allow_lazy_initializer: Whether to allow using the ZeroGradient wrapper,\n      for efficiency.\n\n  Returns:\n    An object of the same type, shape, etc. but with all numeric values set to\n    zero. If the type is unknown, a zero is returned.\n  \"\"\"\n  if obj is None:\n    # TODO: fixes.py appears to pass None value and expect 0.0 back. Bug?\n    return 0.0\n\n  initializer, supports_lazy_initializer = grad_initializers[type(obj)]\n  if supports_lazy_initializer:\n    if isinstance(obj, ZeroGradient):\n      if allow_lazy_initializer:\n        return ZeroGradient(obj.like)\n      else:\n        # TODO: Not sure this should normally be hit. In forward-over-reverse?\n        return obj.instantiate()\n    else:\n      if allow_lazy_initializer:\n        return ZeroGradient(obj)\n  else:\n    assert not isinstance(obj, ZeroGradient)\n  return initializer(obj)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nregister a gradient adder supporting the given types.", "response": "def register_add_grad(left_type, right_type, add_grad_function):\n  \"\"\"Register a new gradient adder supporting the given types.\n\n  Gradient adders are used to add (in the sense of arithmetic addition)\n  intermediate adjoint and tangent variables.\n  TODO: Link to the document explaining the overall terminology and mechanics.\n\n  Args:\n    left_type: A Python type object. The data type of the left operand\n      supported by the adder.\n    right_type: A Python type object. The data type of the right operand\n      supported by the adder.\n    add_grad_function: A binary function that takes two arguments, left and\n      right, of the types left_type and right_type respectively, and returns\n      their sum. For example, the gradient adder for Numpy objects is np.add.\n\n  Raises:\n    ValueError: If the given type pair was already registered.\n  \"\"\"\n  key = (left_type, right_type)\n  if key in grad_adders:\n    raise ValueError('Types %s already mapped to %s' % (key, grad_adders[key]))\n  grad_adders[key] = add_grad_function"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nregistering a gradient adder for all combinations of given types.", "response": "def register_all_add_grad(\n    add_grad_function, arg_types, exclude=(), ignore_existing=False):\n  \"\"\"Register a gradient adder for all combinations of given types.\n\n  This is a convenience shorthand for calling register_add_grad when registering\n  gradient adders for multiple types that can be interchanged for the purpose\n  of addition.\n\n  Args:\n    add_grad_function: A gradient adder, see register_add_grad.\n    arg_types: List of Python type objects. The gradient adder will be\n      registered for all pairs of these types.\n    exclude: Optional list of type tuples to exclude.\n    ignore_existing: Boolean. Whether to silently skip argument pairs that were\n      already registered.\n  \"\"\"\n  for t1 in arg_types:\n    for t2 in arg_types:\n      if (t1, t2) in exclude:\n        continue\n      if ignore_existing and (t1, t2) in grad_adders:\n        continue\n      register_add_grad(t1, t2, add_grad_function)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_grad(left, right):\n  # We assume that initial gradients are always identity WRT add_grad.\n  # We also assume that only init_grad could have created None values.\n  assert left is not None and right is not None\n  left_type = type(left)\n  right_type = type(right)\n  if left_type is ZeroGradient:\n    return right\n  if right_type is ZeroGradient:\n    return left\n  return grad_adders[(left_type, right_type)](left, right)", "response": "Recursively add the gradient of two objects."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef register_shape_checker(left_type, right_type, shape_checker_function):\n  key = (left_type, right_type)\n  if key in shape_checkers:\n    raise ValueError('Types %s already mapped to %s' % (key,\n                                                        shape_checkers[key]))\n  shape_checkers[key] = shape_checker_function", "response": "Registers a shape checker function supporting given types."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef register_all_shape_checker(shape_checker_function,\n                               arg_types,\n                               exclude=(),\n                               ignore_existing=False):\n  \"\"\"Register a gradient adder for all combinations of given types.\n\n  This is a convenience shorthand for calling register_add_grad when registering\n  gradient adders for multiple types that can be interchanged for the purpose\n  of addition.\n\n  Args:\n    shape_checker_function: A shape checker, see register_shape_checker.\n    arg_types: List of Python type objects. The shape checker will be\n      registered for all pairs of these types.\n    exclude: Optional list of type tuples to exclude.\n    ignore_existing: Boolean. Whether to silently skip argument pairs that were\n      already registered.\n  \"\"\"\n  for t1 in arg_types:\n    for t2 in arg_types:\n      if (t1, t2) in exclude:\n        continue\n      if ignore_existing and (t1, t2) in shape_checkers:\n        continue\n      register_shape_checker(t1, t2, shape_checker_function)", "response": "Register a shape checker function for all combinations of given types."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef push(stack, x, op_id):\n  if isinstance(x, numpy.ndarray):\n    x = x.copy()\n  elif isinstance(x, list):\n    x = x[:]\n  if __debug__:\n    stack.append((x, op_id))\n  else:\n    stack.append(x)", "response": "Push a value onto the stack."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pop(stack, op_id):\n  if __debug__:\n    pushed_value, pushed_op_id = stack.pop()\n    assert pushed_op_id == op_id, 'Wanted %s, got %s' % (op_id, pushed_op_id)\n  else:\n    pushed_value = stack.pop()\n  return pushed_value", "response": "Pop a value from the stack."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef grad_dot(dy, x1, x2):\n  if len(numpy.shape(x1)) == 1:\n    dy = numpy.atleast_2d(dy)\n  elif len(numpy.shape(x2)) == 1:\n    dy = numpy.transpose(numpy.atleast_2d(dy))\n    x2 = numpy.transpose(numpy.atleast_2d(x2))\n  x2_t = numpy.transpose(numpy.atleast_2d(\n      numpy.sum(x2, axis=tuple(numpy.arange(numpy.ndim(x2) - 2)))))\n  dy_x2 = numpy.sum(dy, axis=tuple(-numpy.arange(numpy.ndim(x2) - 2) - 2))\n  return numpy.reshape(numpy.dot(dy_x2, x2_t), numpy.shape(x1))", "response": "Gradient of NumPy dot product w. r. t. to the left hand side."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntrace a function and return a VJP and the function s output.", "response": "def trace_grad(fn, args):\n  \"\"\"Trace a function, and return a VJP and the function's output.\"\"\"\n  from tensorflow.python.eager.backprop import make_vjp\n  result, vjp = make_vjp(fn)(*args)\n  return result, vjp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_module_functions(modules):\n  module_fns = set()\n  for module in modules:\n    for key in dir(module):\n      attr = getattr(module, key)\n      if isinstance(\n          attr, (types.BuiltinFunctionType, types.FunctionType, numpy.ufunc)):\n        module_fns.add(attr)\n  return module_fns", "response": "Finds functions that do not have implemented derivatives."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef adet(z, x):\n  adjugate = numpy.linalg.det(x) * numpy.linalg.pinv(x)\n  d[x] = d[z] * numpy.transpose(adjugate)", "response": "Adet the matrix A at the given z to the given x."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall this function to validate an AST.", "response": "def validate(node, source):\n  \"\"\"Call this function to validate an AST.\"\"\"\n  # TODO: leaving strict checking off to support insert_grad_of\n  lf = LanguageFence(source, strict=False)\n  lf.visit(node)\n  return node"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the name of a variable.", "response": "def get_name(node):\n  \"\"\"Get the name of a variable.\n\n  Args:\n    node: A `Name`, `Subscript` or `Attribute` node.\n\n  Returns:\n    The name of the variable e.g. `'x'` for `x`, `x.i` and `x[i]`.\n  \"\"\"\n  if isinstance(node, gast.Name):\n    return node.id\n  elif isinstance(node, (gast.Subscript, gast.Attribute)):\n    return get_name(node.value)\n  else:\n    raise TypeError"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the variable names created or mutated by this statement.", "response": "def get_updated(node):\n  \"\"\"Return the variable names created or mutated by this statement.\n\n  This function considers assign statements, augmented assign statements, and\n  the targets of for loops, as well as function arguments.\n\n  For example, `x[0] = 2` will return `x`, `x, y = 3, 4` will return `x` and\n  `y`, `for i in range(x)` will return `i`, etc.\n\n  Args:\n    node: An AST node\n\n  Returns:\n    A set of variable names (strings) of all the variables created or mutated.\n  \"\"\"\n  if isinstance(node, gast.Assign):\n    return set.union(*(_get_target(target)\n                       for target in node.targets))\n  elif isinstance(node, (gast.For, gast.AugAssign)):\n    return _get_target(node.target)\n  elif isinstance(node, gast.arguments):\n    targets = set(arg.id for arg in node.args + node.kwonlyargs)\n    if node.vararg:\n      targets.add(node.vararg.id)\n    if node.kwarg:\n      targets.add(node.kwarg.id)\n    return targets\n  else:\n    return set()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copy_node(node):\n  if not isinstance(node, gast.AST):\n    return [copy_node(n) for n in node]\n  new_node = copy.deepcopy(node)\n  setattr(new_node, anno.ANNOTATION_FIELD,\n          getattr(node, anno.ANNOTATION_FIELD, {}).copy())\n  return new_node", "response": "Copy a node but keep its annotations intact."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_insert_grad_of_statement(node):\n  tangent_calls = [anno.getanno(item.context_expr, 'func', None)\n                   is utils.insert_grad_of for item in node.items]\n  if all(tangent_calls):\n    return True\n  elif any(tangent_calls):\n    raise ValueError\n  else:\n    return False", "response": "Checks whether a context manager calls insert_grad_of."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_comment(node, text, location='above'):\n  anno.setanno(node, 'comment', dict(location=location, text=text), safe=False)\n  return node", "response": "Adds a comment to the given node."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_repeated_comments(node):\n  last_comment = {'text': None}\n  for _node in gast.walk(node):\n    if anno.hasanno(_node, 'comment'):\n      comment = anno.getanno(_node, 'comment')\n      if comment['text'] == last_comment['text']:\n        anno.delanno(_node, 'comment')\n      last_comment = comment\n  return node", "response": "Removes comments that repeat themselves."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_grad(node, namer, tangent=False):\n  if not isinstance(node, (gast.Subscript, gast.Name, gast.Str)):\n    raise TypeError\n\n  if anno.hasanno(node, 'temp_var'):\n    return create_grad(anno.getanno(node, 'temp_var'), namer, tangent)\n\n  def _name_grad(node):\n    if not isinstance(node, gast.Name):\n      raise TypeError\n    varname = node.id\n    name = namer.grad(varname, tangent)\n    grad_node = gast.Name(\n        id=name, ctx=None, annotation=None)\n    anno.setanno(grad_node, 'adjoint_var', node)\n    return grad_node\n  if isinstance(node, gast.Subscript):\n    grad_node = create_grad(node.value, namer, tangent=tangent)\n    grad_node.ctx = gast.Load()\n    return gast.Subscript(value=grad_node, slice=node.slice, ctx=None)\n  elif isinstance(node, gast.Str):\n    grad_node = create_grad(\n        gast.Name(id=node.s, ctx=None, annotation=None), namer, tangent=tangent)\n    return gast.Str(grad_node.id)\n  else:\n    return _name_grad(node)", "response": "Given a variable create a gradient for the variable."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_temp_grad(node, namer, tangent=False):\n  if not isinstance(node, (gast.Subscript, gast.Name)):\n    raise TypeError\n\n  def _name_temp_grad(node):\n    name = namer.temp_grad(node.id, tangent)\n    temp_node = gast.Name(id=name, annotation=None, ctx=None)\n    return temp_node\n  if isinstance(node, gast.Subscript):\n    temp_node = _name_temp_grad(node.value)\n  else:\n    temp_node = _name_temp_grad(node)\n  anno.setanno(temp_node, 'temp_adjoint_var', node)\n  return temp_node", "response": "Create a variable to store partial gradients."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a temporary variable.", "response": "def create_temp(node, namer):\n  \"\"\"Create a temporary variable.\n\n  Args:\n    node: Create a temporary variable to store this variable in.\n    namer: A naming object that guarantees the names are unique.\n\n  Returns:\n    node: See `create_grad`. Returns a temporary variable, which is always a\n        simple variable annotated with `temp_var`.\n  \"\"\"\n  if isinstance(node, gast.Name):\n    name = node.id\n  elif isinstance(node, (gast.Attribute, gast.Subscript)):\n    name = node.value.id\n  else:\n    raise TypeError\n  temp_node = gast.Name(id=namer.temp(name), annotation=None, ctx=None)\n  anno.setanno(temp_node, 'temp_var', node)\n  return temp_node"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef forward_ad(node, wrt, preserve_result=False, check_dims=True):\n  if not isinstance(node, gast.FunctionDef):\n    raise TypeError\n\n  # Activity analysis\n  cfg_obj = cfg.CFG.build_cfg(node)\n  cfg.Active(range(len(node.args.args))).visit(cfg_obj.entry)\n\n  # Build forward mode function\n  fad = ForwardAD(wrt, preserve_result, check_dims)\n  node = fad.visit(node)\n\n  # Annotate stacks\n  node = annotate.find_stacks(node)\n\n  # Clean up naive forward-mode fcode\n  node = gast.Module([node])\n  anno.clearanno(node)\n\n  return node, fad.required", "response": "Perform forward - mode AD on an AST node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef visit_Assign(self, node):\n\n    # Generate tangent name of the assignment\n    self.target = node.targets[0]\n    self.value = node.value\n\n    # Get the tangent node\n    tangent_node = self.visit(self.value)\n\n    # If no forward-mode statement was created, then no extra work is needed.\n    if tangent_node == self.value:\n      self.target = None\n      return node\n\n    if self.value:\n      new_node = template.replace(\n          tangents.tangents[gast.Assign],\n          replace_grad=template.Replace.TANGENT,\n          namer=self.namer,\n          temp=self.tmp_node,\n          tangent=tangent_node,\n          target=self.target,\n          value=self.value)\n      # Ensure that we use a unique tmp node per primal/tangent pair\n      self.reset_tmp_node()\n    else:\n      # We're already in ANF form right now,\n      # so we can assume LHS is a single Name \"z\"\n      def template_(z, f):\n        tmp = f\n        d[z] = tmp[0]\n        z = tmp[1]\n\n      new_node = template.replace(\n          template_,\n          replace_grad=template.Replace.TANGENT,\n          namer=self.namer,\n          z=self.target,\n          f=tangent_node[0])\n\n    # Add it after the original statement\n    self.target = None\n\n    # Add in some cool comments\n    for i in range(len(new_node)):\n      new_node[i] = comments.add_comment(new_node[i], 'Primal and tangent of: '\n                                         '%s' % quoting.unquote(node))\n    return new_node", "response": "Visit an assignment statement."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef visit_NameConstant(self, node):\n    constant_val = {\n        True: 'True',\n        False: 'False',\n        None: 'None',\n    }[node.value]\n    new_node = gast.Name(id=constant_val,ctx=gast.Load(),annotation=None)\n    return self.visit_Name(new_node)", "response": "Visit a NameConstant node."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a source code of a given AST.", "response": "def to_source(node, indentation=' ' * 4):\n  \"\"\"Return source code of a given AST.\"\"\"\n  if isinstance(node, gast.AST):\n    node = gast.gast_to_ast(node)\n  generator = SourceWithCommentGenerator(indentation, False,\n                                         astor.string_repr.pretty_string)\n  generator.visit(node)\n  generator.result.append('\\n')\n  return astor.source_repr.pretty_source(generator.result).lstrip()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_function(fn):\n  try:\n    return parse_string(inspect.getsource(fn))\n  except (IOError, OSError) as e:\n    raise ValueError(\n        'Cannot differentiate function: %s. Tangent must be able to access the '\n        'source code of the function. Functions defined in a Python '\n        'interpreter and functions backed by C extension modules do not '\n        'have accessible source code.' % e)", "response": "Get the source of a function and return its AST."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngo from source code to AST nodes.", "response": "def quote(src_string, return_expr=False):\n  \"\"\"Go from source code to AST nodes.\n\n  This function returns a tree without enclosing `Module` or `Expr` nodes.\n\n  Args:\n    src_string: The source code to parse.\n    return_expr: Whether or not to return a containing expression. This can be\n        set to `True` if the result is to be part of a series of statements.\n\n  Returns:\n    An AST of the given source code.\n\n  \"\"\"\n  node = parse_string(src_string)\n  body = node.body\n  if len(body) == 1:\n    if isinstance(body[0], gast.Expr) and not return_expr:\n      out = body[0].value\n    else:\n      out = body[0]\n  else:\n    out = node\n  return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating pop and pop nodes that are linked.", "response": "def get_push_pop():\n  \"\"\"Create pop and push nodes that are linked.\n\n  Returns:\n    A push and pop node which have `push_func` and `pop_func` annotations\n        respectively, identifying them as such. They also have a `pop` and\n        `push` annotation respectively, which links the push node to the pop\n        node and vice versa.\n  \"\"\"\n  push = copy.deepcopy(PUSH)\n  pop = copy.deepcopy(POP)\n  anno.setanno(push, 'pop', pop)\n  anno.setanno(push, 'gen_push', True)\n  anno.setanno(pop, 'push', push)\n  op_id = _generate_op_id()\n  return push, pop, op_id"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates pop and push nodes for substacks that are linked.", "response": "def get_push_pop_stack():\n  \"\"\"Create pop and push nodes for substacks that are linked.\n\n  Returns:\n    A push and pop node which have `push_func` and `pop_func` annotations\n        respectively, identifying them as such. They also have a `pop` and\n        `push` annotation respectively, which links the push node to the pop\n        node and vice versa.\n  \"\"\"\n  push = copy.deepcopy(PUSH_STACK)\n  pop = copy.deepcopy(POP_STACK)\n  anno.setanno(push, 'pop', pop)\n  anno.setanno(push, 'gen_push', True)\n  anno.setanno(pop, 'push', push)\n  op_id = _generate_op_id()\n  return push, pop, op_id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform reverse - mode AD on an AST node.", "response": "def reverse_ad(node, wrt, preserve_result, check_dims):\n  \"\"\"Perform reverse-mode AD on an AST.\n\n  This function analyses the AST to determine which variables are active and\n  proceeds by taking the naive derivative. Before returning the primal and\n  adjoint it annotates push and pop statements as such.\n\n  Args:\n    node: A `FunctionDef` AST node.\n    wrt: A tuple of argument indices with respect to which we take the\n        derivative.\n    preserve_result: A boolean indicating whether the generated\n        derivative function should also return the original return value.\n    check_dims: A boolean indicating whether the seed derivatives should have\n        their dimensions checked to match their primal counterpart.\n\n\n  Returns:\n    mod: A `Module` node containing the naive primal and adjoint of the\n        function which can be fed to the `split` and `joint` functions.\n    required: A list of tuples of functions and argument indices. These\n        functions were called by the function but did not have an adjoint.\n  \"\"\"\n  if not isinstance(node, gast.FunctionDef):\n    raise TypeError\n  # Activity analysis\n  cfg.forward(node, cfg.Active(wrt))\n\n  ad = ReverseAD(wrt, preserve_result, check_dims)\n  pri, adj = ad.visit(node)\n  mod = gast.Module(body=[pri, adj])\n  mod = annotate.find_stacks(mod)\n  return mod, ad.required, ad.stack"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstore the state of the primal and adjoint function at the end of the function.", "response": "def store_state(node, reaching, defined, stack):\n  \"\"\"Push the final state of the primal onto the stack for the adjoint.\n\n  Python's scoping rules make it possible for variables to not be defined in\n  certain blocks based on the control flow path taken at runtime. In order to\n  make sure we don't try to push non-existing variables onto the stack, we\n  defined these variables explicitly (by assigning `None` to them) at the\n  beginning of the function.\n\n  All the variables that reach the return statement are pushed onto the\n  stack, and in the adjoint they are popped off in reverse order.\n\n  Args:\n    node: A module with the primal and adjoint function definitions as returned\n        by `reverse_ad`.\n    reaching: The variable definitions that reach the end of the primal.\n    defined: The variables defined at the end of the primal.\n    stack: The stack node to use for storing and restoring state.\n\n  Returns:\n    node: A node with the requisite pushes and pops added to make sure that\n        state is transferred between primal and adjoint split motion calls.\n  \"\"\"\n  defs = [def_ for def_ in reaching if not isinstance(def_[1], gast.arguments)]\n  if not len(defs):\n    return node\n  reaching, original_defs = zip(*defs)\n\n  # Explicitly define variables that might or might not be in scope at the end\n  assignments = []\n  for id_ in set(reaching) - defined:\n    assignments.append(quoting.quote('{} = None'.format(id_)))\n\n  # Store variables at the end of the function and restore them\n  store = []\n  load = []\n  for id_, def_ in zip(reaching, original_defs):\n    # If the original definition of a value that we need to store\n    # was an initialization as a stack, then we should be using `push_stack`\n    # to store its state, and `pop_stack` to restore it. This allows\n    # us to avoid doing any `add_grad` calls on the stack, which result\n    # in type errors in unoptimized mode (they are usually elided\n    # after calling `dead_code_elimination`).\n    if isinstance(\n        def_, gast.Assign) and 'tangent.Stack()' in quoting.unquote(def_.value):\n      push, pop, op_id = get_push_pop_stack()\n    else:\n      push, pop, op_id = get_push_pop()\n    store.append(\n        template.replace(\n            'push(_stack, val, op_id)',\n            push=push,\n            val=id_,\n            _stack=stack,\n            op_id=op_id))\n    load.append(\n        template.replace(\n            'val = pop(_stack, op_id)',\n            pop=pop,\n            val=id_,\n            _stack=stack,\n            op_id=op_id))\n\n  body, return_ = node.body[0].body[:-1], node.body[0].body[-1]\n  node.body[0].body = assignments + body + store + [return_]\n  node.body[1].body = load[::-1] + node.body[1].body\n\n  return node"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef split(node, stack):\n  node, defined, reaching = _fix(node)\n\n  # Store and restore the state\n  node = store_state(node, reaching, defined, stack)\n\n  # Clean up\n  anno.clearanno(node)\n  return node", "response": "Carry over the state from the primal to the adjoint."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef joint(node):\n  node, _, _ = _fix(node)\n  body = node.body[0].body[:-1] + node.body[1].body\n  func = gast.Module(body=[gast.FunctionDef(\n      name=node.body[0].name, args=node.body[1].args, body=body,\n      decorator_list=[], returns=None)])\n  # Clean up\n  anno.clearanno(func)\n  return func", "response": "Merge the bodies of primal and adjoint into a single function."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _fix(node):\n  # Do reaching definitions analysis on primal and adjoint\n  pri_cfg = cfg.CFG.build_cfg(node.body[0])\n  defined = cfg.Defined()\n  defined.visit(pri_cfg.entry)\n  reaching = cfg.ReachingDefinitions()\n  reaching.visit(pri_cfg.entry)\n\n  cfg.forward(node.body[1], cfg.Defined())\n  cfg.forward(node.body[1], cfg.ReachingDefinitions())\n\n  # Remove pushes of variables that were never defined\n  fixes.CleanStack().visit(node)\n  fixes.FixStack().visit(node.body[0])\n\n  # Change accumulation into definition if possible\n  fixes.CleanGrad().visit(node.body[1])\n  # Define gradients that might or might not be defined\n  fixes.FixGrad().visit(node.body[1])\n  return node, defined.exit, reaching.exit", "response": "Fix the naive construction of the adjont."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvisit a node. This method is largely modelled after the ast.NodeTransformer class. Args: node: The node to visit. Returns: A tuple of the primal and adjoint, each of which is a node or a list of nodes.", "response": "def visit(self, node):\n    \"\"\"Visit a node.\n\n    This method is largely modelled after the ast.NodeTransformer class.\n\n    Args:\n      node: The node to visit.\n\n    Returns:\n      A tuple of the primal and adjoint, each of which is a node or a list of\n      nodes.\n    \"\"\"\n    method = 'visit_' + node.__class__.__name__\n    if not hasattr(self, method):\n      raise ValueError('Unknown node type: %s' % node.__class__.__name__)\n    visitor = getattr(self, method)\n\n    # If this node is a statement, inform all child nodes what the active\n    # variables in this statement are\n    if anno.hasanno(node, 'active_in'):\n      self.active_variables = anno.getanno(node, 'active_in')\n    pri, adj = visitor(node)\n\n    # Annotate primal and adjoint statements\n    if isinstance(pri, gast.AST):\n      anno.setdefaultanno(pri, 'adj', adj)\n    else:\n      for node in pri:\n        anno.setdefaultanno(node, 'adj', adj)\n    if isinstance(adj, gast.AST):\n      anno.setdefaultanno(adj, 'pri', pri)\n    else:\n      for node in adj:\n        anno.setdefaultanno(node, 'pri', pri)\n\n    return pri, adj"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_active(self, node):\n    # Special case: If the right hand side is a pop statement, we want to\n    # process it\n    if (isinstance(node.value, gast.Call) and\n        anno.getanno(node.value, 'func', False) == utils.pop):\n      return True\n    for succ in gast.walk(node.value):\n      if (isinstance(succ, gast.Name) and isinstance(succ.ctx, gast.Load) and\n          succ.id in self.active_variables):\n        return True\n    return False", "response": "Checks whether a statement is active."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating the adjoint of a series of statements.", "response": "def visit_statements(self, nodes):\n    \"\"\"Generate the adjoint of a series of statements.\"\"\"\n    primals, adjoints = [], collections.deque()\n    for node in nodes:\n      primal, adjoint = self.visit(node)\n      if not isinstance(primal, list):\n        primal = [primal]\n      if not isinstance(adjoint, list):\n        adjoint = [adjoint]\n      # Methods will return `None` if the node is to be removed, so remove them\n      primals.extend(filter(None, primal))\n      # We reverse the order of the adjoints, but not the statements in\n      # the adjoint itself\n      adjoints.extendleft(filter(None, adjoint[::-1]))\n    return primals, list(adjoints)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndealing with the special with insert_grad_of ( x ) statement.", "response": "def visit_With(self, node):\n    \"\"\"Deal with the special with insert_grad_of(x) statement.\"\"\"\n    if ast_.is_insert_grad_of_statement(node):\n      primal = []\n      adjoint = node.body\n      if isinstance(adjoint[0], gast.With):\n        _, adjoint = self.visit(adjoint[0])\n      node.body[0] = comments.add_comment(node.body[0], 'Inserted code')\n      # Rename the gradients\n      replacements = {}\n      for item in node.items:\n        if (not isinstance(item.context_expr.args[0], gast.Name) or\n            not isinstance(item.optional_vars, gast.Name)):\n          raise ValueError\n        replacements[item.optional_vars.id] = create.create_grad(\n            item.context_expr.args[0], self.namer)\n      template.ReplaceTransformer(replacements).visit(node)\n      return primal, adjoint\n    else:\n      return node, []"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvisit an assignment statement.", "response": "def visit_Assign(self, node):\n    \"\"\"Visit assignment statement.\"\"\"\n    if len(node.targets) != 1:\n      raise ValueError('no support for chained assignment')\n\n    # Before the node gets modified, get a source code representation\n    # to add as a comment later on\n    if anno.hasanno(node, 'pre_anf'):\n      orig_src = anno.getanno(node, 'pre_anf')\n    else:\n      orig_src = quoting.unquote(node)\n\n    # Set target for the RHS visitor to access\n    self.orig_target = ast_.copy_node(node.targets[0])\n\n    # If we know we're going to be putting another stack on the stack,\n    # we should try to make that explicit\n    if isinstance(node.value, gast.Call) and \\\n        anno.hasanno(node.value, 'func') and \\\n        anno.getanno(node.value, 'func') in (utils.Stack, utils.pop_stack):\n      push, pop, op_id = get_push_pop_stack()\n    else:\n      push, pop, op_id = get_push_pop()\n    push_stack, pop_stack, op_id_stack = get_push_pop_stack()\n\n    # Every assignment statement requires us to store the pre-value, and in the\n    # adjoint restore the value, and reset the gradient\n    store = template.replace(\n        'push(_stack, y, op_id)',\n        push=push,\n        y=self.orig_target,\n        _stack=self.stack,\n        op_id=op_id)\n    create_substack = template.replace(\n        'substack = tangent.Stack()', substack=self.substack)\n    store_substack = template.replace(\n        'push(stack, substack, op_id)',\n        push=push_stack,\n        stack=self.stack,\n        substack=self.substack,\n        op_id=op_id_stack)\n    restore = template.replace(\n        'y = pop(_stack, op_id)',\n        _stack=self.stack,\n        pop=pop,\n        y=ast_.copy_node(self.orig_target),\n        op_id=op_id)\n    restore_substack = template.replace(\n        'substack = pop(stack, op_id)',\n        pop=pop_stack,\n        stack=self.stack,\n        substack=self.substack,\n        op_id=op_id_stack)\n    reset = template.replace(\n        'd[y] = init_grad(y, allow_lazy_initializer=True)',\n        y=self.orig_target,\n        init_grad=utils.INIT_GRAD,\n        namer=self.namer,\n        replace_grad=template.Replace.FULL)\n\n    # If there are no active nodes, we don't need to find an adjoint\n    # We simply store and restore the state, and reset the gradient\n    if not self.is_active(node):\n      return [store, node], [restore, reset]\n\n    # We create a temporary variable for the target that the RHS can use\n    self.target = create.create_temp(self.orig_target, self.namer)\n    create_tmp = template.replace(\n        'tmp = y', tmp=self.target, y=self.orig_target)\n\n    # Get the primal and adjoint of the RHS expression\n    try:\n      fx, adjoint_rhs = self.visit(node.value)\n    except ValueError as e:\n      context = [t.id if hasattr(t, 'id') else t for t in node.targets]\n      raise ValueError(\n          'Failed to process assignment to: %s. Error: %s' % (context, e))\n    if not isinstance(adjoint_rhs, list):\n      adjoint_rhs = [adjoint_rhs]\n\n    # Walk the RHS adjoint AST to find temporary adjoint variables to sum\n    accumulations = []\n    for n in adjoint_rhs:\n      for succ in gast.walk(n):\n        if anno.hasanno(succ, 'temp_adjoint_var'):\n          xi = anno.getanno(succ, 'temp_adjoint_var')\n          dxi_partial = ast_.copy_node(succ)\n          accumulations.append(template.replace(\n              'd[xi] = add_grad(d[xi], dxi_partial)',\n              namer=self.namer, replace_grad=template.Replace.FULL,\n              xi=xi, dxi_partial=dxi_partial, add_grad=utils.ADD_GRAD))\n\n    # The primal consists of storing the state and then performing the\n    # assignment with the new primal.\n    # The primal `fx` may be optionally (but rarely) redefined when the\n    # adjoint is generated, in `fx, adjoint_rhs = self.visit(node.value)`.\n    # If we see that the primal value is an Assign node, or a list of nodes\n    # (with at least one being an Assign) node, we allow the primal to change.\n    # Otherwise, we'll build our own Assign node.\n    if isinstance(fx, gast.Assign):\n      assign = [fx]\n    elif (isinstance(fx, list) and\n          any([isinstance(ifx, gast.Assign) for ifx in fx])):\n      assign = fx\n    else:\n      assign = template.replace(\n          'y = fx', y=ast_.copy_node(self.orig_target), fx=fx)\n      assign = [assign]\n    primal = [store, create_substack, store_substack] + assign\n\n    # The adjoint involves creating the temporary, restoring the store,\n    # calculating the adjoint, resetting the gradient, and finally accumulating\n    # the partials\n    adjoint = [create_tmp, restore_substack, restore\n              ] + adjoint_rhs + [reset] + accumulations\n\n    # If the LHS is a subscript assignment with variable index, we need to\n    # store and restore that as well\n    if (isinstance(self.orig_target, gast.Subscript) and\n        isinstance(self.orig_target.slice.value, gast.Name)):\n      push, pop, op_id = get_push_pop()\n      i = self.orig_target.slice.value\n      push_index = template.replace(\n          'push(_stack, i, op_id)',\n          push=push,\n          i=i,\n          _stack=self.stack,\n          op_id=op_id)\n      pop_index = template.replace(\n          'i = pop(_stack, op_id)',\n          pop=pop,\n          i=i,\n          _stack_=self.stack,\n          op_id=op_id)\n\n      primal.insert(len(primal), push_index)\n      adjoint.insert(0, pop_index)\n\n    # Add a comment in the backwards pass, indicating which\n    # lines in the forward pass generated the adjoint\n    for i, adj in enumerate(adjoint):\n      adjoint[i] = comments.add_comment(adj, 'Grad of: %s' % orig_src)\n\n    return primal, adjoint"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef primal_and_adjoint_for_tracing(self, node):\n    primal_template = grads.primals[tracing.Traceable]\n    adjoint_template = grads.adjoints[tracing.Traceable]\n\n    # Prep\n    to_pack = node.args\n    target = ast_.copy_node(self.orig_target)\n    vjp = quoting.quote(self.namer.unique('%s_grad' % node.func.id))\n    tmp = create.create_temp(quoting.quote('tmp'), self.namer)\n    assert len(node.keywords) == 0\n\n    # Full replacement of primal\n    # TODO: do we need to set 'pri_call' on this?\n    primal = template.replace(\n        primal_template,\n        namer=self.namer,\n        result=target,\n        fn=node.func,\n        tmp=tmp,\n        vjp=vjp,\n        args=gast.Tuple(elts=to_pack, ctx=gast.Load()))\n\n    # Building adjoint using the vjp generated with the primal\n    dto_pack = gast.Tuple(\n        elts=[create.create_temp_grad(arg, self.namer) for arg in to_pack],\n        ctx=gast.Store())\n\n    adjoint = template.replace(\n        adjoint_template,\n        namer=self.namer,\n        result=target,\n        vjp=vjp,\n        dargs=dto_pack)\n\n    return primal, adjoint", "response": "Build the primal and adjoint of a traceable function."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef visit_Call(self, node):\n    # Find the function we are differentiating\n    func = anno.getanno(node, 'func')\n\n    if func in non_differentiable.NON_DIFFERENTIABLE:\n      return node, []\n\n    if func == tracing.Traceable:\n      return self.primal_and_adjoint_for_tracing(node)\n\n    if func in grads.UNIMPLEMENTED_ADJOINTS:\n      raise errors.ReverseNotImplementedError(func)\n\n\n    # If we don't have an adjoint, we will have to step into the called\n    # function and differentiate it\n    if func not in grads.adjoints:\n      active_args = tuple(i for i, arg in enumerate(node.args)\n                          if arg.id in self.active_variables)\n\n      already_counted = False\n      for f, a in self.required:\n        if f.__name__ == func.__name__ and set(a) == set(active_args):\n          already_counted = True\n          break\n      if not already_counted:\n        self.required.append((func, active_args))\n\n      pri_name = naming.primal_name(func, active_args)\n      pri_call = gast.Call(\n          func=gast.Name(id=pri_name, ctx=gast.Load(), annotation=None),\n          args=[self.substack] + node.args,\n          keywords=node.keywords)\n      anno.setanno(pri_call, 'pri_call', True)\n\n      dy = create.create_grad(self.target, self.namer)\n      dy.ctx = gast.Load()\n      dx = create.create_grad(node.args[0], self.namer)\n      dx.ctx = gast.Store()\n      adj_name = naming.adjoint_name(func, active_args)\n      adj_call = gast.Call(\n          func=gast.Name(id=adj_name, ctx=gast.Load(), annotation=None),\n          args=[self.substack, dy] + node.args,\n          keywords=node.keywords)\n      anno.setanno(adj_call, 'adj_call', True)\n      adjoint = [template.replace('dxs = dfx', namer=self.namer, dfx=adj_call)]\n      for j, i in enumerate(active_args):\n        adjoint.append(template.replace('d[x] = dxs[i]', namer=self.namer,\n                                        x=node.args[i].id, i=gast.Num(n=j)))\n      return pri_call, adjoint\n\n    # We have a template for the gradient that we need to fill in\n    template_ = grads.adjoints[func]\n\n    # Match the function call to the template\n    sig = funcsigs.signature(template_)\n    sig = sig.replace(parameters=list(sig.parameters.values())[1:])\n    kwargs = dict((keyword.arg, keyword.value) for keyword in node.keywords)\n    bound_args = sig.bind(*node.args, **kwargs)\n\n    # Fill in any missing kwargs with the defaults from the template\n    args = quoting.parse_function(template_).body[0].args\n    kwargs = dict(zip(*map(reversed, [args.args, args.defaults])))\n    kwargs.update(dict(zip(args.kwonlyargs, args.kw_defaults)))\n    for arg, val in kwargs.items():\n      if arg.id not in bound_args.arguments:\n        bound_args.arguments[arg.id] = val\n\n    # Let's fill in the template. The first argument is the output, which\n    # was stored in a temporary variable\n    output_name = six.get_function_code(template_).co_varnames[0]\n    arg_replacements = {output_name: ast_.copy_node(self.target)}\n    arg_replacements.update(bound_args.arguments)\n\n    # If the template uses *args, then we pack the corresponding inputs\n    packing = []\n    flags = six.get_function_code(template_).co_flags\n\n    if flags & inspect.CO_VARARGS:\n      to_pack = node.args[six.get_function_code(template_).co_argcount - 1:]\n      vararg_name = six.get_function_code(template_).co_varnames[-1]\n      target = gast.Name(annotation=None, id=vararg_name, ctx=gast.Store())\n      value = gast.Tuple(elts=to_pack, ctx=gast.Load())\n      packing = [gast.Assign(targets=[target], value=value)]\n\n      # And we fill in the packed tuple into the template\n      arg_replacements[six.get_function_code(\n          template_).co_varnames[-1]] = target\n    adjoint = template.replace(template_, namer=self.namer, **arg_replacements)\n    unpacking = []\n    if flags & inspect.CO_VARARGS:\n      # If the template packs arguments, then we have to unpack the\n      # derivatives afterwards\n      # We also have to update the replacements tuple then\n      dto_pack = [create.create_temp_grad(arg, self.namer)\n                  for arg in to_pack]\n      value = create.create_grad(target, self.namer)\n      target = gast.Tuple(elts=dto_pack, ctx=gast.Store())\n      unpacking = [gast.Assign(targets=[target], value=value)]\n\n    return node, packing + adjoint + unpacking", "response": "Create adjoint for a call."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef prepend(self, node):\n    if not isinstance(node, grammar.STATEMENTS):\n      raise ValueError\n    self.to_prepend[-1].appendleft(node)", "response": "Prepend a statement to the current statement."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef append(self, node):\n    if not isinstance(node, grammar.STATEMENTS):\n      raise ValueError\n    self.to_append[-1].append(node)", "response": "Append a statement to the current statement."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninserting statements at the top of the function body.", "response": "def insert_top(self, node):\n    \"\"\"Insert statements at the top of the function body.\n\n    Note that multiple calls to `insert_top` will result in the statements\n    being prepended in that order; this is different behavior from `prepend`.\n\n    Args:\n      node: The statement to prepend.\n\n    Raises:\n      ValueError: If the given node is not a statement.\n\n    \"\"\"\n    if not isinstance(node, grammar.STATEMENTS):\n      raise ValueError\n    self.to_insert_top.append(node)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef append_block(self, node, reverse=False):\n    if not isinstance(node, grammar.STATEMENTS):\n      raise ValueError\n    if reverse:\n      self.to_append_block[-1].appendleft(node)\n    else:\n      self.to_append_block[-1].append(node)", "response": "Appends a statement to the current block."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvisit a series of nodes in a node body.", "response": "def visit_statements(self, nodes):\n    \"\"\"Visit a series of nodes in a node body.\n\n    This function is factored out so that it can be called recursively on\n    statements that are appended or prepended. This allows e.g. a nested\n    expression to prepend a statement, and that statement can prepend a\n    statement again, etc.\n\n    Args:\n      nodes: A list of statements.\n\n    Returns:\n      A list of transformed statements.\n    \"\"\"\n    for node in nodes:\n      if isinstance(node, gast.AST):\n        self.to_prepend.append(deque())\n        self.to_append.append(deque())\n        node = self.visit(node)\n        self.visit_statements(self.to_prepend.pop())\n        if isinstance(node, gast.AST):\n          self.to_insert[-1].append(node)\n        elif node:\n          self.to_insert[-1].extend(node)\n        self.visit_statements(self.to_append.pop())\n      else:\n        self.to_insert[-1].append(node)\n    return self.to_insert[-1]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef replace(template, replace_grad=Replace.PARTIAL,\n            namer=None, **replacements):\n  \"\"\"Replace placeholders in a Python template (quote).\n\n  Args:\n    template: A function, AST node or string to be used as a template. Note\n        that if a function is passed, any placeholder is expected to also be a\n        function argument. If a string is passed, it must represent valid\n        Python code, and any variable it references is a placeholder.\n    replace_grad: If Replace.NONE, statements of the form `d[x]` are ignored.\n        For the other possible values, see `ReplaceGradTransformer`.\n    namer: See `ReplaceGradTransformer`.\n    **replacements: A mapping from placeholder names to (lists of) AST nodes\n        that these placeholders will be replaced by. If a string is passed,\n        `quote` will be called on it to turn it into a node.\n\n  Returns:\n    body: An AST node or list of AST nodes with the replacements made. If the\n        template was a function, a list will be returned. If the template was a\n        node, the same node will be returned. If the template was a string, an\n        AST node will be returned (a `Module` node in the case of a multi-line\n        string, an `Expr` node otherwise).\n\n  Raises:\n    ValueError: If a function is used as a template and an incorrect set of\n        replacements was passed.\n  \"\"\"\n  # Handle the 3 different types of templates: funcs, nodes, and strings\n  is_function = isinstance(template, types.FunctionType)\n  if is_function:\n    tree = quoting.parse_function(template).body[0]\n    placeholders = set(arg.id for arg in tree.args.args)\n    tree.args.args = []\n    if tree.args.vararg:\n      placeholders.add(tree.args.vararg)\n      tree.args.vararg = None\n    if set(replacements.keys()) != placeholders:\n      raise ValueError('too many or few replacements')\n  elif isinstance(template, gast.AST):\n    tree = template\n  else:\n    tree = quoting.quote(template, return_expr=True)\n  # If the replacements are strings, turn them into nodes\n  for k, v in replacements.items():\n    if isinstance(v, six.string_types):\n      replacements[k] = quoting.quote(v)\n  # Perform the replacement\n  ReplaceTransformer(replacements).visit(tree)\n  # Handle the d[x] operator\n  if replace_grad is not Replace.NONE:\n    rgt = ReplaceGradTransformer(\n        replace_grad=replace_grad,\n        namer=namer,\n        tangent=replace_grad is Replace.TANGENT)\n    rgt.visit(tree)\n  # Return the AST node with replacements made\n  if is_function:\n    return tree.body\n  else:\n    return tree", "response": "Replaces placeholders in a Python template."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef resolve_calls(func):\n  node = quoting.parse_function(func)\n  ResolveCalls(func).visit(node)\n  return node", "response": "Parse a function into an AST with function calls resolved."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_stacks(node, strict=False):\n  # First, find all stack operation IDs.\n  fso = FindStackOps()\n  fso.visit(node)\n  # Using those IDs, make annotations onto the push and pop nodes.\n  AnnotateStacks(fso.push_pop_pairs, strict).visit(node)\n  return node", "response": "Find pushes and pops to the stack and annotate them as such."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds all variable definitions that can be removed later on.", "response": "def unused(node):\n  \"\"\"Find unused definitions that can be remove.\n\n  This runs reaching definitions analysis followed by a walk over the AST to\n  find all variable definitions that are not used later on.\n\n  Args:\n    node: The AST of e.g. a function body to find unused variable definitions.\n\n  Returns:\n    unused: After visiting all the nodes, this attribute contanis a set of\n        definitions in the form of `(variable_name, node)` pairs which are\n        unused in this AST.\n  \"\"\"\n  cfg.forward(node, cfg.ReachingDefinitions())\n  unused_obj = Unused()\n  unused_obj.visit(node)\n  return unused_obj.unused"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating which AST nodes are unused.", "response": "def unused(self):\n    \"\"\"Calculate which AST nodes are unused.\n\n    Note that we have to take special care in the case of\n    x,y = f(z) where x is used later, but y is not.\"\"\"\n    unused = self.definitions - self.used\n    # Filter (variable_name,node) pairs that should be removed, because\n    # node is used elsewhere\n    used_nodes = set([u[1] for u in self.used])\n    unused = set([u for u in unused if u[1] not in used_nodes])\n    return unused"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef package_config():\n    p = subprocess.Popen(['pkg-config', '--exists', '--atleast-version={}'.format(_TESSERACT_MIN_VERSION),\n                          '--print-errors', 'tesseract'],\n                         stderr=subprocess.PIPE)\n    _, error = p.communicate()\n    if p.returncode != 0:\n        raise Exception(error)\n    p = subprocess.Popen(['pkg-config', '--libs', '--cflags', 'tesseract'], stdout=subprocess.PIPE)\n    output, _ = p.communicate()\n    flags = _read_string(output).strip().split()\n    p = subprocess.Popen(['pkg-config', '--libs', '--cflags', 'lept'], stdout=subprocess.PIPE)\n    output, _ = p.communicate()\n    flags2 = _read_string(output).strip().split()\n    options = {'-L': 'library_dirs',\n               '-I': 'include_dirs',\n               '-l': 'libraries'}\n    config = {}\n    import itertools\n    for f in itertools.chain(flags, flags2):\n        try:\n            opt = options[f[:2]]\n        except KeyError:\n            continue\n        val = f[2:]\n        if opt == 'include_dirs' and psplit(val)[1].strip(os.sep) in ('leptonica', 'tesseract'):\n            val = dirname(val)\n        config.setdefault(opt, set()).add(val)\n    config = {k: list(v) for k, v in config.items()}\n    p = subprocess.Popen(['pkg-config', '--modversion', 'tesseract'], stdout=subprocess.PIPE)\n    version, _ = p.communicate()\n    version = _read_string(version).strip()\n    _LOGGER.info(\"Supporting tesseract v{}\".format(version))\n    config['cython_compile_time_env'] = {'TESSERACT_VERSION': version_to_int(version)}\n    _LOGGER.info(\"Configs from pkg-config: {}\".format(config))\n    return config", "response": "Use pkg - config to get library build parameters and tesseract version."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_tesseract_version():\n    config = {'libraries': ['tesseract', 'lept']}\n    try:\n        p = subprocess.Popen(['tesseract', '-v'], stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n        stdout_version, version = p.communicate()\n        version = _read_string(version).strip()\n        if version == '':\n            version = _read_string(stdout_version).strip()\n        version_match = re.search(r'^tesseract ((?:\\d+\\.)+\\d+).*', version, re.M)\n        if version_match:\n            version = version_match.group(1)\n        else:\n            _LOGGER.warn('Failed to extract tesseract version number from: {}'.format(version))\n            version = _TESSERACT_MIN_VERSION\n    except OSError as e:\n        _LOGGER.warn('Failed to extract tesseract version from executable: {}'.format(e))\n        version = _TESSERACT_MIN_VERSION\n    _LOGGER.info(\"Supporting tesseract v{}\".format(version))\n    version = version_to_int(version)\n    config['cython_compile_time_env'] = {'TESSERACT_VERSION': version}\n    _LOGGER.info(\"Building with configs: {}\".format(config))\n    return config", "response": "Try to extract version from tesseract otherwise default min version."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn proper build parameters.", "response": "def get_build_args():\n    \"\"\"Return proper build parameters.\"\"\"\n    try:\n        build_args = package_config()\n    except Exception as e:\n        if isinstance(e, OSError):\n            if e.errno != errno.ENOENT:\n                _LOGGER.warn('Failed to run pkg-config: {}'.format(e))\n        else:\n            _LOGGER.warn('pkg-config failed to find tesseract/lept libraries: {}'.format(e))\n        build_args = get_tesseract_version()\n\n    if build_args['cython_compile_time_env']['TESSERACT_VERSION'] >= 0x3050200:\n        _LOGGER.debug('tesseract >= 03.05.02 requires c++11 compiler support')\n        build_args['extra_compile_args'] = ['-std=c++11', '-DUSE_STD_NAMESPACE']\n\n    _LOGGER.debug('build parameters: {}'.format(build_args))\n    return build_args"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_star_report (self, raw_data):\n\n        regexes = {\n            'total_reads':                  r\"Number of input reads \\|\\s+(\\d+)\",\n            'avg_input_read_length':        r\"Average input read length \\|\\s+([\\d\\.]+)\",\n            'uniquely_mapped':              r\"Uniquely mapped reads number \\|\\s+(\\d+)\",\n            'uniquely_mapped_percent':      r\"Uniquely mapped reads % \\|\\s+([\\d\\.]+)\",\n            'avg_mapped_read_length':       r\"Average mapped length \\|\\s+([\\d\\.]+)\",\n            'num_splices':                  r\"Number of splices: Total \\|\\s+(\\d+)\",\n            'num_annotated_splices':        r\"Number of splices: Annotated \\(sjdb\\) \\|\\s+(\\d+)\",\n            'num_GTAG_splices':             r\"Number of splices: GT/AG \\|\\s+(\\d+)\",\n            'num_GCAG_splices':             r\"Number of splices: GC/AG \\|\\s+(\\d+)\",\n            'num_ATAC_splices':             r\"Number of splices: AT/AC \\|\\s+(\\d+)\",\n            'num_noncanonical_splices':     r\"Number of splices: Non-canonical \\|\\s+(\\d+)\",\n            'mismatch_rate':                r\"Mismatch rate per base, % \\|\\s+([\\d\\.]+)\",\n            'deletion_rate':                r\"Deletion rate per base \\|\\s+([\\d\\.]+)\",\n            'deletion_length':              r\"Deletion average length \\|\\s+([\\d\\.]+)\",\n            'insertion_rate':               r\"Insertion rate per base \\|\\s+([\\d\\.]+)\",\n            'insertion_length':             r\"Insertion average length \\|\\s+([\\d\\.]+)\",\n            'multimapped':                  r\"Number of reads mapped to multiple loci \\|\\s+(\\d+)\",\n            'multimapped_percent':          r\"% of reads mapped to multiple loci \\|\\s+([\\d\\.]+)\",\n            'multimapped_toomany':          r\"Number of reads mapped to too many loci \\|\\s+(\\d+)\",\n            'multimapped_toomany_percent':  r\"% of reads mapped to too many loci \\|\\s+([\\d\\.]+)\",\n            'unmapped_mismatches_percent':  r\"% of reads unmapped: too many mismatches \\|\\s+([\\d\\.]+)\",\n            'unmapped_tooshort_percent':    r\"% of reads unmapped: too short \\|\\s+([\\d\\.]+)\",\n            'unmapped_other_percent':       r\"% of reads unmapped: other \\|\\s+([\\d\\.]+)\",\n        }\n        parsed_data = {}\n        for k, r in regexes.items():\n            r_search = re.search(r, raw_data, re.MULTILINE)\n            if r_search:\n                parsed_data[k] = float(r_search.group(1))\n        # Figure out the numbers for unmapped as for some reason only the percentages are given\n        try:\n            total_mapped = parsed_data['uniquely_mapped'] + parsed_data['multimapped'] + parsed_data['multimapped_toomany']\n            unmapped_count = parsed_data['total_reads'] - total_mapped\n            total_unmapped_percent = parsed_data['unmapped_mismatches_percent'] + parsed_data['unmapped_tooshort_percent'] + parsed_data['unmapped_other_percent']\n            try:\n                parsed_data['unmapped_mismatches'] = int(round(unmapped_count * (parsed_data['unmapped_mismatches_percent'] / total_unmapped_percent), 0))\n                parsed_data['unmapped_tooshort'] = int(round(unmapped_count * (parsed_data['unmapped_tooshort_percent'] / total_unmapped_percent), 0))\n                parsed_data['unmapped_other'] = int(round(unmapped_count * (parsed_data['unmapped_other_percent'] / total_unmapped_percent), 0))\n            except ZeroDivisionError:\n                parsed_data['unmapped_mismatches'] = 0\n                parsed_data['unmapped_tooshort'] = 0\n                parsed_data['unmapped_other'] = 0\n        except KeyError:\n            pass\n\n        if len(parsed_data) == 0: return None\n        return parsed_data", "response": "Parse the STAR log file and return a dict of the attributes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_star_genecount_report(self, f):\n        # Three numeric columns: unstranded, stranded/first-strand, stranded/second-strand\n        keys = [ 'N_unmapped', 'N_multimapping', 'N_noFeature', 'N_ambiguous' ]\n        unstranded = { 'N_genes': 0 }\n        first_strand = { 'N_genes': 0 }\n        second_strand = { 'N_genes': 0 }\n        num_errors = 0\n        num_genes = 0\n        for l in f['f']:\n            s = l.split(\"\\t\")\n            try:\n                for i in [1,2,3]:\n                    s[i] = float(s[i])\n                if s[0] in keys:\n                    unstranded[s[0]] = s[1]\n                    first_strand[s[0]] = s[2]\n                    second_strand[s[0]] = s[3]\n                else:\n                    unstranded['N_genes'] += s[1]\n                    first_strand['N_genes'] += s[2]\n                    second_strand['N_genes'] += s[3]\n                    num_genes += 1\n            except IndexError:\n                # Tolerate a few errors in case there is something random added at the top of the file\n                num_errors += 1\n                if num_errors > 10 and num_genes == 0:\n                    log.warning(\"Error parsing {}\".format(f['fn']))\n                    return None\n        if num_genes > 0:\n            return { 'unstranded': unstranded, 'first_strand': first_strand, 'second_strand': second_strand }\n        else:\n            return None", "response": "Parse a STAR gene counts output file and return a dictionary of the values."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntake the parsed stats from the STAR report and add them to the basic stats table at the top of the report", "response": "def star_stats_table(self):\n        \"\"\" Take the parsed stats from the STAR report and add them to the\n        basic stats table at the top of the report \"\"\"\n\n        headers = OrderedDict()\n        headers['uniquely_mapped_percent'] = {\n            'title': '% Aligned',\n            'description': '% Uniquely mapped reads',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'YlGn'\n        }\n        headers['uniquely_mapped'] = {\n            'title': '{} Aligned'.format(config.read_count_prefix),\n            'description': 'Uniquely mapped reads ({})'.format(config.read_count_desc),\n            'min': 0,\n            'scale': 'PuRd',\n            'modify': lambda x: x * config.read_count_multiplier,\n            'shared_key': 'read_count'\n        }\n        self.general_stats_addcols(self.star_data, headers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef star_alignment_chart (self):\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['uniquely_mapped'] =      { 'color': '#437bb1', 'name': 'Uniquely mapped' }\n        keys['multimapped'] =          { 'color': '#7cb5ec', 'name': 'Mapped to multiple loci' }\n        keys['multimapped_toomany'] =  { 'color': '#f7a35c', 'name': 'Mapped to too many loci' }\n        keys['unmapped_mismatches'] =  { 'color': '#e63491', 'name': 'Unmapped: too many mismatches' }\n        keys['unmapped_tooshort'] =    { 'color': '#b1084c', 'name': 'Unmapped: too short' }\n        keys['unmapped_other'] =       { 'color': '#7f0000', 'name': 'Unmapped: other' }\n\n        # Config for the plot\n        pconfig = {\n            'id': 'star_alignment_plot',\n            'title': 'STAR: Alignment Scores',\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads'\n        }\n\n        return bargraph.plot(self.star_data, keys, pconfig)", "response": "Make the alignment rates plot for the current species"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef star_genecount_chart (self):\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['N_genes'] =        { 'color': '#2f7ed8', 'name': 'Overlapping Genes' }\n        keys['N_noFeature'] =    { 'color': '#0d233a', 'name': 'No Feature' }\n        keys['N_ambiguous'] =    { 'color': '#492970', 'name': 'Ambiguous Features' }\n        keys['N_multimapping'] = { 'color': '#f28f43', 'name': 'Multimapping' }\n        keys['N_unmapped'] =     { 'color': '#7f0000', 'name': 'Unmapped' }\n\n        # Config for the plot\n        pconfig = {\n            'id': 'star_gene_counts',\n            'title': 'STAR: Gene Counts',\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads',\n            'data_labels': ['Unstranded','Same Stranded','Reverse Stranded']\n        }\n        datasets = [\n            self.star_genecounts_unstranded,\n            self.star_genecounts_first_strand,\n            self.star_genecounts_second_strand\n        ]\n        return bargraph.plot(datasets, [keys,keys,keys,keys], pconfig)", "response": "Make a plot of the ReadsPerGene output"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate the miRTrace QC plot", "response": "def mirtrace_qc_plot(self):\n        \"\"\" Generate the miRTrace QC Plot\"\"\"\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['adapter_removed_length_ok']   =   { 'color': '#006837', 'name': 'Reads \u2265 18 nt after adapter removal' }\n        keys['adapter_not_detected']        =   { 'color': '#66bd63', 'name': 'Reads without adapter' }\n        keys['length_shorter_than_18']      =   { 'color': '#fdae61', 'name': 'Reads < 18 nt after adapter removal' }\n        keys['low_complexity']              =   { 'color': '#d73027', 'name': 'Reads with low complexity' }\n        keys['low_phred']                   =   { 'color': '#a50026', 'name': 'Reads with low PHRED score' }\n\n        # Config for the plot\n        config = {\n            'id': 'mirtrace_qc_plot',\n            'title': 'miRTrace: QC Plot',\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads'\n        }\n\n        return bargraph.plot(self.summary_data, keys, config)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating the miRTrace Read Length Distribution", "response": "def mirtrace_length_plot(self):\n        \"\"\" Generate the miRTrace Read Length Distribution\"\"\"\n\n        data = dict()\n        for s_name in self.length_data:\n            try:\n                data[s_name] = {int(d): int(self.length_data[s_name][d]) for d in self.length_data[s_name]}\n            except KeyError:\n                pass\n        if len(data) == 0:\n            log.debug('No valid data for read length distribution')\n            return None\n\n        config = {\n            'id': 'mirtrace_length_plot',\n            'title': 'miRTrace: Read Length Distribution',\n            'ylab': 'Read Count',\n            'xlab': 'Read Lenth (bp)',\n            'ymin': 0,\n            'xmin': 0,\n            'xDecimals': False,\n            'tt_label': '<b>Read Length (bp) {point.x}</b>: {point.y} Read Count',\n            'xPlotBands': [\n                {'from': 40, 'to': 50, 'color': '#ffebd1'},\n                {'from': 26, 'to': 40, 'color': '#e2f5ff'},\n                {'from': 18, 'to': 26, 'color': '#e5fce0'},\n                {'from': 0, 'to': 18, 'color': '#ffffe2'},\n            ]\n        }\n\n        return linegraph.plot(data, config)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mirtrace_rna_categories(self):\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['reads_mirna']      =   { 'color': '#33a02c', 'name': 'miRNA' }\n        keys['reads_rrna']       =   { 'color': '#ff7f00', 'name': 'rRNA' }\n        keys['reads_trna']       =   { 'color': '#1f78b4', 'name': 'tRNA' }\n        keys['reads_artifact']   =   { 'color': '#fb9a99', 'name': 'Artifact' }\n        keys['reads_unknown']    =   { 'color': '#d9d9d9', 'name': 'Unknown' }\n\n        # Config for the plot\n        config = {\n            'id': 'mirtrace_rna_categories_plot',\n            'title': 'miRTrace: RNA Categories',\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads'\n        }\n\n        return bargraph.plot(self.summary_data, keys, config)", "response": "Generate the miRTrace RNA Categories"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate the miRTrace Contamination Check plot", "response": "def mirtrace_contamination_check(self):\n        \"\"\" Generate the miRTrace Contamination Check\"\"\"\n\n        # A library of 24 colors. Should be enough for this plot\n        color_lib = ['rgb(166,206,227)', 'rgb(31,120,180)', 'rgb(178,223,138)', 'rgb(51,160,44)', 'rgb(251,154,153)', 'rgb(227,26,28)', 'rgb(253,191,111)', 'rgb(255,127,0)', 'rgb(202,178,214)', 'rgb(106,61,154)', 'rgb(255,255,153)', 'rgb(177,89,40)', 'rgb(141,211,199)', 'rgb(255,255,179)', 'rgb(190,186,218)', 'rgb(251,128,114)', 'rgb(128,177,211)', 'rgb(253,180,98)', 'rgb(179,222,105)', 'rgb(252,205,229)', 'rgb(217,217,217)', 'rgb(188,128,189)', 'rgb(204,235,197)', 'rgb(255,237,111)']\n\n        idx = 0\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        for clade in self.contamination_data[list(self.contamination_data.keys())[0]]:\n            keys[clade] = { 'color': color_lib[idx], 'name': clade }\n            if idx < 23:\n                idx += 1\n            else:\n                idx = 0\n\n        # Config for the plot\n        config = {\n            'cpswitch_c_active': False,\n            'id': 'mirtrace_contamination_check_plot',\n            'title': 'miRTrace: Contamination Check',\n            'ylab': '# miRNA detected',\n            'cpswitch_counts_label': 'Number of detected miRNA'\n        }\n\n        return bargraph.plot(self.contamination_data, keys, config)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mirtrace_complexity_plot(self):\n\n        data = dict()\n        for s_name in self.complexity_data:\n            try:\n                data[s_name] = {int(self.complexity_data[s_name][d]) : int(d) for d in self.complexity_data[s_name]}\n            except KeyError:\n                pass\n        if len(data) == 0:\n            log.debug('No valid data for miRNA complexity')\n            return None\n\n        config = {\n            'id': 'mirtrace_complexity_plot',\n            'title': 'miRTrace: miRNA Complexity Plot',\n            'ylab': 'Distinct miRNA Count',\n            'xlab': 'Number of Sequencing Reads',\n            'ymin': 0,\n            'xmin': 1,\n            'xDecimals': False,\n            'tt_label': '<b>Number of Sequencing Reads {point.x}</b>: {point.y} Distinct miRNA Count',\n        }\n\n        return linegraph.plot(data, config)", "response": "Generate the miRTrace miRNA Complexity Plot"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding RSeQC junction_annotation reports and parse their data", "response": "def parse_reports(self):\n    \"\"\" Find RSeQC junction_annotation reports and parse their data \"\"\"\n\n    # Set up vars\n    self.junction_annotation_data = dict()\n    regexes = {\n        'total_splicing_events': r\"^Total splicing  Events:\\s*(\\d+)$\",\n        'known_splicing_events': r\"^Known Splicing Events:\\s*(\\d+)$\",\n        'partial_novel_splicing_events': r\"^Partial Novel Splicing Events:\\s*(\\d+)$\",\n        'novel_splicing_events': r\"^Novel Splicing Events:\\s*(\\d+)$\",\n        'total_splicing_junctions': r\"^Total splicing  Junctions:\\s*(\\d+)$\",\n        'known_splicing_junctions': r\"^Known Splicing Junctions:\\s*(\\d+)$\",\n        'partial_novel_splicing_junctions': r\"^Partial Novel Splicing Junctions:\\s*(\\d+)$\",\n        'novel_splicing_junctions': r\"^Novel Splicing Junctions:\\s*(\\d+)$\",\n    }\n\n    # Go through files and parse data using regexes\n    for f in self.find_log_files('rseqc/junction_annotation'):\n        d = dict()\n        for k, r in regexes.items():\n            r_search = re.search(r, f['f'], re.MULTILINE)\n            if r_search:\n                d[k] = int(r_search.group(1))\n\n        # Calculate some percentages\n        if 'total_splicing_events' in d:\n            t = float(d['total_splicing_events'])\n            if 'known_splicing_events' in d:\n                d['known_splicing_events_pct'] = (float(d['known_splicing_events']) / t)*100.0\n            if 'partial_novel_splicing_events' in d:\n                d['partial_novel_splicing_events_pct'] = (float(d['partial_novel_splicing_events']) / t)*100.0\n            if 'novel_splicing_events' in d:\n                d['novel_splicing_events_pct'] = (float(d['novel_splicing_events']) / t)*100.0\n        if 'total_splicing_junctions' in d:\n            t = float(d['total_splicing_junctions'])\n            if 'known_splicing_junctions' in d:\n                d['known_splicing_junctions_pct'] = (float(d['known_splicing_junctions']) / t)*100.0\n            if 'partial_novel_splicing_junctions' in d:\n                d['partial_novel_splicing_junctions_pct'] = (float(d['partial_novel_splicing_junctions']) / t)*100.0\n            if 'novel_splicing_junctions' in d:\n                d['novel_splicing_junctions_pct'] = (float(d['novel_splicing_junctions']) / t)*100.0\n\n        if len(d) > 0:\n            if f['s_name'] in self.junction_annotation_data:\n                log.debug(\"Duplicate sample name found! Overwriting: {}\".format(f['s_name']))\n            self.add_data_source(f, section='junction_annotation')\n            self.junction_annotation_data[f['s_name']] = d\n\n    # Filter to strip out ignored sample names\n    self.junction_annotation_data = self.ignore_samples(self.junction_annotation_data)\n\n    if len(self.junction_annotation_data) > 0:\n\n        # Write to file\n        self.write_data_file(self.junction_annotation_data, 'multiqc_rseqc_junction_annotation')\n\n        # Plot junction annotations\n        keys = [OrderedDict(), OrderedDict()]\n        keys[0]['known_splicing_junctions'] = { 'name': 'Known Splicing Junctions' }\n        keys[0]['partial_novel_splicing_junctions'] = { 'name': 'Partial Novel Splicing Junctions' }\n        keys[0]['novel_splicing_junctions'] = { 'name': 'Novel Splicing Junctions' }\n        keys[1]['known_splicing_events'] = { 'name': 'Known Splicing Events' }\n        keys[1]['partial_novel_splicing_events'] = { 'name': 'Partial Novel Splicing Events' }\n        keys[1]['novel_splicing_events'] = { 'name': 'Novel Splicing Events' }\n\n        pconfig = {\n            'id': 'rseqc_junction_annotation_junctions_plot',\n            'title': 'RSeQC: Splicing Junctions',\n            'ylab': '% Junctions',\n            'cpswitch_c_active': False,\n            'data_labels': [ 'Junctions', 'Events' ]\n        }\n        self.add_section (\n            name = 'Junction Annotation',\n            anchor = 'rseqc_junction_annotation',\n            description = '<a href=\"http://rseqc.sourceforge.net/#junction-annotation-py\" target=\"_blank\">Junction annotation</a>' \\\n                \" compares detected splice junctions to\" \\\n                \" a reference gene model. An RNA read can be spliced 2\" \\\n                \" or more times, each time is called a splicing event.\",\n            plot = bargraph.plot([self.junction_annotation_data, self.junction_annotation_data], keys, pconfig)\n        )\n\n    # Return number of samples found\n    return len(self.junction_annotation_data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind bcftools stats logs and parse their data ArcGIS stats data", "response": "def parse_bcftools_stats(self):\n        \"\"\"\n        Find bcftools stats logs and parse their data\n          Bcftools stats reports contain 'sets' of data, which can\n          have multiple vcf files each (but usually don't). Here,\n          we treat each 'set' as a MultiQC sample, taking the first\n          input filename for each set as the name.\n        \"\"\"\n        collapse_complementary = getattr(config, 'bcftools', {}).get('collapse_complementary_changes', False)\n        if collapse_complementary:\n            types = ['A>C', 'A>G', 'A>T', 'C>A', 'C>G', 'C>T']\n        else:\n            types = ['A>C', 'A>G', 'A>T', 'C>A', 'C>G', 'C>T',\n                     'G>A', 'G>C', 'G>T', 'T>A', 'T>C', 'T>G']\n\n        self.bcftools_stats = dict()\n        self.bcftools_stats_indels = dict()\n        self.bcftools_stats_vqc_snp = dict()\n        self.bcftools_stats_vqc_transi = dict()\n        self.bcftools_stats_vqc_transv = dict()\n        self.bcftools_stats_vqc_indels = dict()\n        depth_data = dict()\n        for f in self.find_log_files('bcftools/stats'):\n            s_names = list()\n            for line in f['f'].splitlines():\n                s = line.split(\"\\t\")\n                # Get the sample names - one per 'set'\n                if s[0] == \"ID\":\n                    s_name = self.clean_s_name(s[2], f['root'])\n                    s_names.append(s_name)\n                    if s_name in self.bcftools_stats:\n                        log.debug(\"Duplicate sample name found! Overwriting: {}\".format(s_name))\n                    self.add_data_source(f, s_name, section='stats')\n                    self.bcftools_stats[s_name] = dict()\n                    self.bcftools_stats_indels[s_name] = dict()\n                    self.bcftools_stats_vqc_snp[s_name] = dict()\n                    self.bcftools_stats_vqc_transi[s_name] = dict()\n                    self.bcftools_stats_vqc_transv[s_name] = dict()\n                    self.bcftools_stats_vqc_indels[s_name] = dict()\n                    depth_data[s_name] = OrderedDict()\n                    self.bcftools_stats_indels[s_name][0] = None # Avoid joining line across missing 0\n\n                # Parse key stats\n                if s[0] == \"SN\" and len(s_names) > 0:\n                    s_name = s_names[int(s[1])]\n                    field = s[2].strip()[:-1]\n                    field = field.replace(' ', '_')\n                    value = float(s[3].strip())\n                    self.bcftools_stats[s_name][field] = value\n\n                # Parse transitions/transversions stats\n                if s[0] == \"TSTV\" and len(s_names) > 0:\n                    s_name = s_names[int(s[1])]\n                    fields = ['ts', 'tv', 'tstv', 'ts_1st_ALT', 'tv_1st_ALT', 'tstv_1st_ALT']\n                    for i, f in enumerate(fields):\n                        value = float(s[i+2].strip())\n\n                        self.bcftools_stats[s_name][f] = value\n\n                # Parse substitution types\n                if s[0] == \"ST\" and len(s_names) > 0:\n                    s_name = s_names[int(s[1])]\n\n                    rc = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A'}\n                    change = s[2].strip()\n                    if change not in types:\n                        change = '>'.join(rc[n] for n in change.split('>'))\n\n                    field = 'substitution_type_{}'.format(change)\n                    value = float(s[3].strip())\n                    if field not in self.bcftools_stats[s_name]:\n                        self.bcftools_stats[s_name][field] = 0\n                    self.bcftools_stats[s_name][field] += value\n\n                # Indel length distributions\n                if s[0] == \"IDD\" and len(s_names) > 0:\n                    s_name = s_names[int(s[1])]\n                    length = float(s[2].strip())\n                    count = float(s[3].strip())\n                    self.bcftools_stats_indels[s_name][length] = count\n\n                # Per-sample counts\n                if s[0] == \"PSC\" and len(s_names) > 0:\n                    s_name = s_names[int(s[1])]\n                    fields = ['variations_hom', 'variations_het']\n                    for i, f in enumerate(fields):\n                        self.bcftools_stats[s_name][f] = int(s[i + 4].strip())\n\n                # Depth plots\n                if s[0] == \"DP\" and len(s_names) > 0:\n                    s_name = s_names[int(s[1])]\n                    bin_name = s[2].strip()\n                    percent_sites = float(s[-1].strip())\n                    depth_data[s_name][bin_name] = percent_sites\n\n                # Variant Qualities\n                if s[0] == \"QUAL\" and len(s_names) > 0:\n                    s_name = s_names[int(s[1])]\n                    quality = float(s[2].strip())\n                    self.bcftools_stats_vqc_snp[s_name][quality] = float(s[3].strip())\n                    self.bcftools_stats_vqc_transi[s_name][quality] = float(s[4].strip())\n                    self.bcftools_stats_vqc_transv[s_name][quality] = float(s[5].strip())\n                    self.bcftools_stats_vqc_indels[s_name][quality] = float(s[6].strip())\n\n        # Filter to strip out ignored sample names\n        self.bcftools_stats = self.ignore_samples(self.bcftools_stats)\n\n        if len(self.bcftools_stats) > 0:\n\n            # Write parsed report data to a file\n            self.write_data_file(self.bcftools_stats, 'multiqc_bcftools_stats')\n\n            # Stats Table\n            stats_headers = self.bcftools_stats_genstats_headers()\n            if getattr(config, 'bcftools', {}).get('write_general_stats', True):\n                self.general_stats_addcols(self.bcftools_stats, stats_headers, 'Bcftools Stats')\n            if getattr(config, 'bcftools', {}).get('write_separate_table', False):\n                self.add_section(\n                    name='Bcftools Stats',\n                    anchor='bcftools-stats',\n                    plot=table.plot(self.bcftools_stats, stats_headers))\n\n            # Make bargraph plot of substitution types\n            keys = OrderedDict()\n            for t in types:\n                keys['substitution_type_{}'.format(t)] = {'name': t}\n            pconfig = {\n                'id': 'bcftools-stats-subtypes',\n                'title': 'Bcftools Stats: Substitutions',\n                'ylab': '# Substitutions',\n                'cpswitch_counts_label': 'Number of Substitutions'\n            }\n            self.add_section (\n                name = 'Variant Substitution Types',\n                anchor = 'bcftools-stats',\n                plot = bargraph.plot(self.bcftools_stats, keys, pconfig)\n            )\n\n            # Make histograms of variant quality\n            if len(self.bcftools_stats_vqc_snp) > 0:\n                pconfig = {\n                    'id': 'bcftools_stats_vqc',\n                    'title': 'Bcftools Stats: Variant Quality Count',\n                    'ylab': 'Count',\n                    'xlab': 'Quality',\n                    'xDecimals': False,\n                    'ymin': 0,\n                    'smooth_points': 600,\n                    # 'tt_label': '<b>{point.x} bp trimmed</b>: {point.y:.0f}',\n                    'data_labels': [\n                        {'name': 'Count SNP', 'ylab': 'Quality'},\n                        {'name': 'Count Transitions', 'ylab': 'Quality'},\n                        {'name': 'Count Transversions', 'ylab': 'Quality'},\n                        {'name': 'Count Indels', 'ylab': 'Quality'}\n                    ]\n                }\n                self.add_section (\n                    name = 'Variant Quality',\n                    anchor = 'bcftools-stats_variant_quality_plot',\n                    plot = linegraph.plot (\n                        [self.bcftools_stats_vqc_snp,\n                        self.bcftools_stats_vqc_transi,\n                        self.bcftools_stats_vqc_transv,\n                        self.bcftools_stats_vqc_indels], pconfig)\n                )\n\n            # Make line graph of indel lengths\n            if len(self.bcftools_stats_indels) > 0:\n                pconfig = {\n                    'id': 'bcftools_stats_indel-lengths',\n                    'title': 'Bcftools Stats: Indel Distribution',\n                    'ylab': 'Count',\n                    'xlab': 'InDel Length (bp)',\n                    'xDecimals': False,\n                    'ymin': 0,\n                }\n                self.add_section (\n                    name = 'Indel Distribution',\n                    anchor = 'bcftools-stats_indel_plot',\n                    plot = linegraph.plot(self.bcftools_stats_indels, pconfig)\n                )\n            # Make line graph of variants per depth\n            if len(depth_data) > 0:\n                pconfig = {\n                    'id': 'bcftools_stats_depth',\n                    'title': 'Bcftools Stats: Variant depths',\n                    'ylab': 'Fraction of sites (%)',\n                    'xlab': 'Variant depth',\n                    'ymin': 0,\n                    'ymax': 100,\n                    'categories': True\n                }\n                self.add_section (\n                    name = 'Variant depths',\n                    anchor = 'bcftools-stats_depth_plot',\n                    description = 'Read depth support distribution for called variants',\n                    plot = linegraph.plot(depth_data, pconfig)\n                )\n\n        # Return the number of logs that were found\n        return len(self.bcftools_stats)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bcftools_stats_genstats_headers(self):\n        stats_headers = OrderedDict()\n        stats_headers['number_of_records'] = {\n            'title': 'Vars',\n            'description': 'Variations total',\n            'min': 0, 'format': '{:,.0f}',\n        }\n        stats_headers['variations_hom'] = {\n            'title': 'Hom',\n            'description': 'Variations homozygous',\n            'min': 0, 'format': '{:,.0f}',\n        }\n        stats_headers['variations_het'] = {\n            'title': 'Het',\n            'description': 'Variations heterozygous',\n            'min': 0, 'format': '{:,.0f}',\n        }\n        stats_headers['number_of_SNPs'] = {\n            'title': 'SNP',\n            'description': 'Variation SNPs',\n            'min': 0, 'format': '{:,.0f}',\n        }\n        stats_headers['number_of_indels'] = {\n            'title': 'Indel',\n            'description': 'Variation Insertions/Deletions',\n            'min': 0, 'format': '{:,.0f}',\n        }\n        stats_headers['tstv'] = {\n            'title': 'Ts/Tv',\n            'description': 'Variant SNP transition / transversion ratio',\n            'min': 0, 'format': '{:,.2f}',\n        }\n        stats_headers['number_of_MNPs'] = {\n            'title': 'MNP',\n            'description': 'Variation multinucleotide polymorphisms',\n            'min': 0, 'format': '{:,.0f}', \"hidden\": True,\n        }\n        return stats_headers", "response": "Add the General Stats headers to the General Stats table"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a HiCUP summary report", "response": "def parse_hicup_logs(self, f):\n        \"\"\" Parse a HiCUP summary report \"\"\"\n        if not f['fn'].endswith('.txt'):\n            return None\n        header = []\n        lines = f['f'].splitlines()\n        for l in lines:\n            s = l.split(\"\\t\")\n            if len(header) == 0:\n                if s[0] != 'File':\n                    return None\n                header = s[1:]\n            else:\n                s_name = self.clean_s_name(s[0], f['root']).lstrip('HiCUP_output/')\n                parsed_data = {}\n                for idx, num in enumerate(s[1:]):\n                    try:\n                        parsed_data[header[idx]] = float(num)\n                    except:\n                        parsed_data[header[idx]] = num\n                parsed_data['Duplicate_Read_Pairs'] = parsed_data['Valid_Pairs'] - parsed_data['Deduplication_Read_Pairs_Uniques']\n                if s_name in self.hicup_data:\n                    log.debug(\"Duplicate sample name found! Overwriting: {}\".format(s_name))\n                self.add_data_source(f, s_name)\n                self.hicup_data[s_name] = parsed_data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hicup_stats_table(self):\n        headers = OrderedDict()\n        headers['Percentage_Ditags_Passed_Through_HiCUP'] = {\n            'title': '% Passed',\n            'description': 'Percentage Di-Tags Passed Through HiCUP',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'YlGn'\n        }\n        headers['Deduplication_Read_Pairs_Uniques'] = {\n            'title': '{} Unique'.format(config.read_count_prefix),\n            'description': 'Unique Di-Tags ({})'.format(config.read_count_desc),\n            'min': 0,\n            'scale': 'PuRd',\n            'modify': lambda x: x * config.read_count_multiplier,\n            'shared_key': 'read_count'\n        }\n        headers['Percentage_Uniques'] = {\n            'title': '% Duplicates',\n            'description': 'Percent Duplicate Di-Tags',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'YlGn-rev',\n            'modify': lambda x: 100 - x\n        }\n        headers['Valid_Pairs'] = {\n            'title': '{} Valid'.format(config.read_count_prefix),\n            'description': 'Valid Pairs ({})'.format(config.read_count_desc),\n            'min': 0,\n            'scale': 'PuRd',\n            'modify': lambda x: x * config.read_count_multiplier,\n            'shared_key': 'read_count'\n        }\n        headers['Percentage_Valid'] = {\n            'title': '% Valid',\n            'description': 'Percent Valid Pairs',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'YlGn'\n        }\n        headers['Paired_Read_1'] = {\n            'title': '{} Pairs Aligned'.format(config.read_count_prefix),\n            'description': 'Paired Alignments ({})'.format(config.read_count_desc),\n            'min': 0,\n            'scale': 'PuRd',\n            'modify': lambda x: x * config.read_count_multiplier,\n            'shared_key': 'read_count'\n        }\n        headers['Percentage_Mapped'] = {\n            'title': '% Aligned',\n            'description': 'Percentage of Paired Alignments',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'YlGn'\n        }\n        self.general_stats_addcols(self.hicup_data, headers, 'HiCUP')", "response": "Add core HiCUP stats to the general stats table"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates the HiCUP Truncated Reads plot", "response": "def hicup_truncating_chart (self):\n        \"\"\" Generate the HiCUP Truncated reads plot \"\"\"\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['Not_Truncated_Reads'] = { 'color': '#2f7ed8', 'name': 'Not Truncated' }\n        keys['Truncated_Read']      = { 'color': '#0d233a', 'name': 'Truncated' }\n\n        # Construct a data structure for the plot - duplicate the samples for read 1 and read 2\n        data = {}\n        for s_name in self.hicup_data:\n            data['{} Read 1'.format(s_name)] = {}\n            data['{} Read 2'.format(s_name)] = {}\n            data['{} Read 1'.format(s_name)]['Not_Truncated_Reads'] = self.hicup_data[s_name]['Not_Truncated_Reads_1']\n            data['{} Read 2'.format(s_name)]['Not_Truncated_Reads'] = self.hicup_data[s_name]['Not_Truncated_Reads_2']\n            data['{} Read 1'.format(s_name)]['Truncated_Read'] = self.hicup_data[s_name]['Truncated_Read_1']\n            data['{} Read 2'.format(s_name)]['Truncated_Read'] = self.hicup_data[s_name]['Truncated_Read_2']\n\n        # Config for the plot\n        config = {\n            'id': 'hicup_truncated_reads_plot',\n            'title': 'HiCUP: Truncated Reads',\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads'\n        }\n\n        return bargraph.plot(data, keys, config)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hicup_alignment_chart (self):\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['Unique_Alignments_Read']   = { 'color': '#2f7ed8', 'name': 'Unique Alignments' }\n        keys['Multiple_Alignments_Read'] = { 'color': '#492970', 'name': 'Multiple Alignments' }\n        keys['Failed_To_Align_Read']     = { 'color': '#0d233a', 'name': 'Failed To Align' }\n        keys['Too_Short_To_Map_Read']    = { 'color': '#f28f43', 'name': 'Too short to map' }\n\n        # Construct a data structure for the plot - duplicate the samples for read 1 and read 2\n        data = {}\n        for s_name in self.hicup_data:\n            data['{} Read 1'.format(s_name)] = {}\n            data['{} Read 2'.format(s_name)] = {}\n            data['{} Read 1'.format(s_name)]['Unique_Alignments_Read'] = self.hicup_data[s_name]['Unique_Alignments_Read_1']\n            data['{} Read 2'.format(s_name)]['Unique_Alignments_Read'] = self.hicup_data[s_name]['Unique_Alignments_Read_2']\n            data['{} Read 1'.format(s_name)]['Multiple_Alignments_Read'] = self.hicup_data[s_name]['Multiple_Alignments_Read_1']\n            data['{} Read 2'.format(s_name)]['Multiple_Alignments_Read'] = self.hicup_data[s_name]['Multiple_Alignments_Read_2']\n            data['{} Read 1'.format(s_name)]['Failed_To_Align_Read'] = self.hicup_data[s_name]['Failed_To_Align_Read_1']\n            data['{} Read 2'.format(s_name)]['Failed_To_Align_Read'] = self.hicup_data[s_name]['Failed_To_Align_Read_2']\n            data['{} Read 1'.format(s_name)]['Too_Short_To_Map_Read'] = self.hicup_data[s_name]['Too_Short_To_Map_Read_1']\n            data['{} Read 2'.format(s_name)]['Too_Short_To_Map_Read'] = self.hicup_data[s_name]['Too_Short_To_Map_Read_2']\n\n        # Config for the plot\n        config = {\n            'id': 'hicup_mapping_stats_plot',\n            'title': 'HiCUP: Mapping Statistics',\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads'\n        }\n\n        return bargraph.plot(data, keys, config)", "response": "Generate the HiCUP Aligned reads plot"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hicup_filtering_chart(self):\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['Valid_Pairs'] =            { 'color': '#2f7ed8', 'name': 'Valid Pairs' }\n        keys['Same_Fragment_Internal'] = { 'color': '#0d233a', 'name': 'Same Fragment - Internal' }\n        keys['Same_Circularised'] =      { 'color': '#910000', 'name': 'Same Fragment - Circularised' }\n        keys['Same_Dangling_Ends'] =     { 'color': '#8bbc21', 'name': 'Same Fragment - Dangling Ends' }\n        keys['Re_Ligation'] =            { 'color': '#1aadce', 'name': 'Re-ligation' }\n        keys['Contiguous_Sequence'] =    { 'color': '#f28f43', 'name': 'Contiguous Sequence' }\n        keys['Wrong_Size'] =             { 'color': '#492970', 'name': 'Wrong Size' }\n\n        # Config for the plot\n        config = {\n            'id': 'hicup_filtering_plot',\n            'title': 'HiCUP: Filtering Statistics',\n            'ylab': '# Read Pairs',\n            'cpswitch_counts_label': 'Number of Read Pairs',\n            'cpswitch_c_active': False\n        }\n\n        return bargraph.plot(self.hicup_data, keys, config)", "response": "Generate the HiCUP filtering plot"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_reports(self):\n\n    # General stats - genome_results.txt\n    self.qualimap_bamqc_genome_results = dict()\n    for f in self.find_log_files('qualimap/bamqc/genome_results'):\n        parse_genome_results(self, f)\n    self.qualimap_bamqc_genome_results = self.ignore_samples(self.qualimap_bamqc_genome_results)\n\n    # Coverage - coverage_histogram.txt\n    self.qualimap_bamqc_coverage_hist = dict()\n    for f in self.find_log_files('qualimap/bamqc/coverage', filehandles=True):\n        parse_coverage(self, f)\n    self.qualimap_bamqc_coverage_hist = self.ignore_samples(self.qualimap_bamqc_coverage_hist)\n\n    # Insert size - insert_size_histogram.txt\n    self.qualimap_bamqc_insert_size_hist = dict()\n    for f in self.find_log_files('qualimap/bamqc/insert_size', filehandles=True):\n        parse_insert_size(self, f)\n    self.qualimap_bamqc_insert_size_hist = self.ignore_samples(self.qualimap_bamqc_insert_size_hist)\n\n    # GC distribution - mapped_reads_gc-content_distribution.txt\n    self.qualimap_bamqc_gc_content_dist = dict()\n    self.qualimap_bamqc_gc_by_species = dict()  # {'HUMAN': data_dict, 'MOUSE': data_dict}\n    for f in self.find_log_files('qualimap/bamqc/gc_dist', filehandles=True):\n        parse_gc_dist(self, f)\n    self.qualimap_bamqc_gc_by_species = self.ignore_samples(self.qualimap_bamqc_gc_by_species)\n\n    num_parsed = max(\n        len(self.qualimap_bamqc_genome_results),\n        len(self.qualimap_bamqc_coverage_hist),\n        len(self.qualimap_bamqc_insert_size_hist),\n        len(self.qualimap_bamqc_gc_content_dist)\n    )\n    # Go no further if nothing found\n    if num_parsed == 0:\n        return 0\n\n    try:\n        covs = config.qualimap_config['general_stats_coverage']\n        assert type(covs) == list\n        assert len(covs) > 0\n        covs = [str(i) for i in covs]\n        log.debug(\"Custom Qualimap thresholds: {}\".format(\", \".join([i for i in covs])))\n    except (AttributeError, TypeError, AssertionError):\n        covs = [1, 5, 10, 30, 50]\n        covs = [str(i) for i in covs]\n        log.debug(\"Using default Qualimap thresholds: {}\".format(\", \".join([i for i in covs])))\n    self.covs = covs\n\n    # Make the plots for the report\n    report_sections(self)\n\n    # Set up the general stats table\n    general_stats_headers(self)\n\n    # Return the number of reports we found\n    return num_parsed", "response": "Find Qualimap BamQC reports and parse their data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the contents of the Qualimap BamQC genome_results. txt file and return a dictionary of the data structures that can be used to store the stats for this sample.", "response": "def parse_genome_results(self, f):\n    \"\"\" Parse the contents of the Qualimap BamQC genome_results.txt file \"\"\"\n    regexes = {\n        'bam_file': r\"bam file = (.+)\",\n        'total_reads': r\"number of reads = ([\\d,]+)\",\n        'mapped_reads': r\"number of mapped reads = ([\\d,]+)\",\n        'mapped_bases': r\"number of mapped bases = ([\\d,]+)\",\n        'sequenced_bases': r\"number of sequenced bases = ([\\d,]+)\",\n        'mean_insert_size': r\"mean insert size = ([\\d,\\.]+)\",\n        'median_insert_size': r\"median insert size = ([\\d,\\.]+)\",\n        'mean_mapping_quality': r\"mean mapping quality = ([\\d,\\.]+)\",\n        'general_error_rate': r\"general error rate = ([\\d,\\.]+)\",\n    }\n    d = dict()\n    for k, r in regexes.items():\n        r_search = re.search(r, f['f'], re.MULTILINE)\n        if r_search:\n            try:\n                d[k] = float(r_search.group(1).replace(',',''))\n            except ValueError:\n                d[k] = r_search.group(1)\n    # Check we have an input filename\n    if 'bam_file' not in d:\n        log.debug(\"Couldn't find an input filename in genome_results file {}\".format(f['fn']))\n        return None\n\n    # Get a nice sample name\n    s_name = self.clean_s_name(d['bam_file'], f['root'])\n\n    # Add to general stats table & calculate a nice % aligned\n    try:\n        self.general_stats_data[s_name]['total_reads'] = d['total_reads']\n        self.general_stats_data[s_name]['mapped_reads'] = d['mapped_reads']\n        d['percentage_aligned'] = (d['mapped_reads'] / d['total_reads'])*100\n        self.general_stats_data[s_name]['percentage_aligned'] = d['percentage_aligned']\n        self.general_stats_data[s_name]['general_error_rate'] = d['general_error_rate']*100\n    except KeyError:\n        pass\n\n    # Save results\n    if s_name in self.qualimap_bamqc_genome_results:\n        log.debug(\"Duplicate genome results sample name found! Overwriting: {}\".format(s_name))\n    self.qualimap_bamqc_genome_results[s_name] = d\n    self.add_data_source(f, s_name=s_name, section='genome_results')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the contents of the Qualimap BamQC Coverage Histogram file and store the result in self. qualimap_bamqc_coverage_hist.", "response": "def parse_coverage(self, f):\n    \"\"\" Parse the contents of the Qualimap BamQC Coverage Histogram file \"\"\"\n    # Get the sample name from the parent parent directory\n    # Typical path: <sample name>/raw_data_qualimapReport/coverage_histogram.txt\n    s_name = self.get_s_name(f)\n\n    d = dict()\n    for l in f['f']:\n        if l.startswith('#'):\n            continue\n        coverage, count = l.split(None, 1)\n        coverage = int(round(float(coverage)))\n        count = float(count)\n        d[coverage] = count\n\n    if len(d) == 0:\n        log.debug(\"Couldn't parse contents of coverage histogram file {}\".format(f['fn']))\n        return None\n\n    # Find median without importing anything to do it for us\n    num_counts = sum(d.values())\n    cum_counts = 0\n    median_coverage = None\n    for thiscov, thiscount in d.items():\n        cum_counts += thiscount\n        if cum_counts >= num_counts/2:\n            median_coverage = thiscov\n            break\n    self.general_stats_data[s_name]['median_coverage'] = median_coverage\n\n    # Save results\n    if s_name in self.qualimap_bamqc_coverage_hist:\n        log.debug(\"Duplicate coverage histogram sample name found! Overwriting: {}\".format(s_name))\n    self.qualimap_bamqc_coverage_hist[s_name] = d\n    self.add_data_source(f, s_name=s_name, section='coverage_histogram')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the Qualimap BamQC Insert Size Histogram file and store the insert size in the self. qualimap_bamqc_insert_size_hist dictionary.", "response": "def parse_insert_size(self, f):\n    \"\"\" Parse the contents of the Qualimap BamQC Insert Size Histogram file \"\"\"\n    # Get the sample name from the parent parent directory\n    # Typical path: <sample name>/raw_data_qualimapReport/insert_size_histogram.txt\n    s_name = self.get_s_name(f)\n\n    d = dict()\n    zero_insertsize = 0\n    for l in f['f']:\n        if l.startswith('#'):\n            continue\n        insertsize, count = l.split(None, 1)\n        insertsize = int(round(float(insertsize)))\n        count = float(count) / 1000000\n        if(insertsize == 0):\n            zero_insertsize = count\n        else:\n            d[insertsize] = count\n\n    # Find median without importing anything to do it for us\n    num_counts = sum(d.values())\n    cum_counts = 0\n    median_insert_size = None\n    for thisins, thiscount in d.items():\n        cum_counts += thiscount\n        if cum_counts >= num_counts/2:\n            median_insert_size = thisins\n            break\n    # Add the median insert size to the general stats table\n    self.general_stats_data[s_name]['median_insert_size'] = median_insert_size\n\n    # Save results\n    if s_name in self.qualimap_bamqc_insert_size_hist:\n        log.debug(\"Duplicate insert size histogram sample name found! Overwriting: {}\".format(s_name))\n    self.qualimap_bamqc_insert_size_hist[s_name] = d\n    self.add_data_source(f, s_name=s_name, section='insert_size_histogram')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the contents of the Qualimap BamQC Mapped Reads GC content distribution file and store the results in self. qualimap_bamqc_gc_content_dist self. qualimap_bamqc_gc_by_species self. qualimap_bamqc_gc_by_species self. general_stats_data.", "response": "def parse_gc_dist(self, f):\n    \"\"\" Parse the contents of the Qualimap BamQC Mapped Reads GC content distribution file \"\"\"\n    # Get the sample name from the parent parent directory\n    # Typical path: <sample name>/raw_data_qualimapReport/mapped_reads_gc-content_distribution.txt\n    s_name = self.get_s_name(f)\n\n    d = dict()\n    reference_species = None\n    reference_d = dict()\n    avg_gc = 0\n    for l in f['f']:\n        if l.startswith('#'):\n            sections = l.strip(\"\\n\").split(\"\\t\", 3)\n            if len(sections) > 2:\n                reference_species = sections[2]\n            continue\n        sections = l.strip(\"\\n\").split(\"\\t\", 3)\n        gc = int(round(float(sections[0])))\n        content = float(sections[1])\n        avg_gc += gc * content\n        d[gc] = content\n        if len(sections) > 2:\n            reference_content = float(sections[2])\n            reference_d[gc] = reference_content\n\n    # Add average GC to the general stats table\n    self.general_stats_data[s_name]['avg_gc'] = avg_gc\n\n    # Save results\n    if s_name in self.qualimap_bamqc_gc_content_dist:\n        log.debug(\"Duplicate Mapped Reads GC content distribution sample name found! Overwriting: {}\".format(s_name))\n    self.qualimap_bamqc_gc_content_dist[s_name] = d\n    if reference_species and reference_species not in self.qualimap_bamqc_gc_by_species:\n        self.qualimap_bamqc_gc_by_species[reference_species] = reference_d\n    self.add_data_source(f, s_name=s_name, section='mapped_gc_distribution')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef report_sections(self):\n    # Append to self.sections list\n\n    if len(self.qualimap_bamqc_coverage_hist) > 0:\n        # Chew back on histogram to prevent long flat tail\n        # (find a sensible max x - lose 1% of longest tail)\n        max_x = 0\n        total_bases_by_sample = dict()\n        for s_name, d in self.qualimap_bamqc_coverage_hist.items():\n            total_bases_by_sample[s_name] = sum(d.values())\n            cumulative = 0\n            for count in sorted(d.keys(), reverse=True):\n                cumulative += d[count]\n                if cumulative / total_bases_by_sample[s_name] > 0.01:\n                    max_x = max(max_x, count)\n                    break\n\n        rates_within_threshs = dict()\n        for s_name, hist in self.qualimap_bamqc_coverage_hist.items():\n            total = total_bases_by_sample[s_name]\n            # Make a range of depths that isn't stupidly huge for high coverage expts\n            depth_range = list(range(0, max_x + 1, math.ceil(float(max_x)/400.0) if max_x > 0 else 1))\n            # Check that we have our specified coverages in the list\n            for c in self.covs:\n                if int(c) not in depth_range:\n                    depth_range.append(int(c))\n            # Calculate the coverage rates for this range of coverages\n            rates_within_threshs[s_name] = _calculate_bases_within_thresholds(hist, total, depth_range)\n            # Add requested coverage levels to the General Statistics table\n            for c in self.covs:\n                if int(c) in rates_within_threshs[s_name]:\n                    self.general_stats_data[s_name]['{}_x_pc'.format(c)] = rates_within_threshs[s_name][int(c)]\n                else:\n                    self.general_stats_data[s_name]['{}_x_pc'.format(c)] = 0\n\n        # Section 1 - BamQC Coverage Histogram\n        coverage_histogram_helptext = '''\n        For a set of DNA or RNA reads mapped to a reference sequence, such as a genome\n        or transcriptome, the depth of coverage at a given base position is the number\n        of high-quality reads that map to the reference at that position\n        (<a href=\"https://doi.org/10.1038/nrg3642\" target=\"_blank\">Sims et al. 2014</a>).\n\n        QualiMap groups the bases of a reference sequence by their depth of coverage\n        (*0&#215;, 1&#215;, &#8230;, N&#215;*), then plots the number of bases of the\n        reference (y-axis) at each level of coverage depth (x-axis). This plot shows\n        the frequency of coverage depths relative to the reference sequence for each\n        read dataset, which provides an indirect measure of the level and variation of\n        coverage depth in the corresponding sequenced sample.\n\n        If reads are randomly distributed across the reference sequence, this plot\n        should resemble a Poisson distribution (<a href=\"https://doi.org/10.1016/0888-7543(88)90007-9\"\n        target=\"_blank\">Lander & Waterman 1988</a>), with a peak indicating approximate\n        depth of coverage, and more uniform coverage depth being reflected in a narrower\n        spread. The optimal level of coverage depth depends on the aims of the\n        experiment, though it should at minimum be sufficiently high to adequately\n        address the biological question; greater uniformity of coverage is generally\n        desirable, because it increases breadth of coverage for a given depth of\n        coverage, allowing equivalent results to be achieved at a lower sequencing depth\n        (<a href=\"https://doi.org/10.1002/gepi.20575\" target=\"_blank\">Sampson\n        et al. 2011</a>; <a href=\"https://doi.org/10.1038/nrg3642\" target=\"_blank\">Sims\n        et al. 2014</a>). However, it is difficult to achieve uniform coverage\n        depth in practice, due to biases introduced during sample preparation\n        (<a href=\"https://doi.org/10.1016/j.yexcr.2014.01.008\" target=\"_blank\">van\n        Dijk et al. 2014</a>), sequencing (<a href=\"https://doi.org/10.1186/gb-2013-14-5-r51\"\n        target=\"_blank\">Ross et al. 2013</a>) and read mapping\n        (<a href=\"https://doi.org/10.1038/nrg3642\" target=\"_blank\">Sims et al. 2014</a>).\n\n        This plot may include a small peak for regions of the reference sequence with\n        zero depth of coverage. Such regions may be absent from the given sample (due\n        to a deletion or structural rearrangement), present in the sample but not\n        successfully sequenced (due to bias in sequencing or preparation), or sequenced\n        but not successfully mapped to the reference (due to the choice of mapping\n        algorithm, the presence of repeat sequences, or mismatches caused by variants\n        or sequencing errors). Related factors cause most datasets to contain some\n        unmapped reads (<a href=\"https://doi.org/10.1038/nrg3642\" target=\"_blank\">Sims\n        et al. 2014</a>).'''\n        self.add_section (\n            name = 'Coverage histogram',\n            anchor = 'qualimap-coverage-histogram',\n            description = 'Distribution of the number of locations in the reference genome with a given depth of coverage.',\n            helptext = coverage_histogram_helptext,\n            plot = linegraph.plot(self.qualimap_bamqc_coverage_hist, {\n                'id': 'qualimap_coverage_histogram',\n                'title': 'Qualimap BamQC: Coverage histogram',\n                'ylab': 'Genome bin counts',\n                'xlab': 'Coverage (X)',\n                'ymin': 0,\n                'xmin': 0,\n                'xmax': max_x,\n                'xDecimals': False,\n                'tt_label': '<b>{point.x}X</b>: {point.y}',\n            })\n        )\n        # Section 2 - BamQC cumulative coverage genome fraction\n        genome_fraction_helptext = '''\n        For a set of DNA or RNA reads mapped to a reference sequence, such as a genome\n        or transcriptome, the depth of coverage at a given base position is the number\n        of high-quality reads that map to the reference at that position, while the\n        breadth of coverage is the fraction of the reference sequence to which reads\n        have been mapped with at least a given depth of coverage\n        (<a href=\"https://doi.org/10.1038/nrg3642\" target=\"_blank\">Sims et al. 2014</a>).\n\n        Defining coverage breadth in terms of coverage depth is useful, because\n        sequencing experiments typically require a specific minimum depth of coverage\n        over the region of interest (<a href=\"https://doi.org/10.1038/nrg3642\"\n        target=\"_blank\">Sims et al. 2014</a>), so the extent of the reference sequence\n        that is amenable to analysis is constrained to lie within regions that have\n        sufficient depth. With inadequate sequencing breadth, it can be difficult to\n        distinguish the absence of a biological feature (such as a gene) from a lack\n        of data (<a href=\"https://doi.org/10.1101/gr.7050807\" target=\"_blank\">Green 2007</a>).\n\n        For increasing coverage depths (*1&#215;, 2&#215;, &#8230;, N&#215;*),\n        QualiMap calculates coverage breadth as the percentage of the reference\n        sequence that is covered by at least that number of reads, then plots\n        coverage breadth (y-axis) against coverage depth (x-axis). This plot\n        shows the relationship between sequencing depth and breadth for each read\n        dataset, which can be used to gauge, for example, the likely effect of a\n        minimum depth filter on the fraction of a genome available for analysis.'''\n        self.add_section (\n            name = 'Cumulative genome coverage',\n            anchor = 'qualimap-cumulative-genome-fraction-coverage',\n            description = 'Percentage of the reference genome with at least the given depth of coverage.',\n            helptext = genome_fraction_helptext,\n            plot = linegraph.plot(rates_within_threshs, {\n                'id': 'qualimap_genome_fraction',\n                'title': 'Qualimap BamQC: Genome fraction covered by at least X reads',\n                'ylab': 'Fraction of reference (%)',\n                'xlab': 'Coverage (X)',\n                'ymax': 100,\n                'ymin': 0,\n                'xmin': 0,\n                'xmax': max_x,\n                'xDecimals': False,\n                'tt_label': '<b>{point.x}X</b>: {point.y:.2f}%',\n            })\n        )\n\n    # Section 3 - Insert size histogram\n    if len(self.qualimap_bamqc_insert_size_hist) > 0:\n        insert_size_helptext = '''\n        To overcome limitations in the length of DNA or RNA sequencing reads,\n        many sequencing instruments can produce two or more shorter reads from\n        one longer fragment in which the relative position of reads is\n        approximately known, such as paired-end or mate-pair reads\n        (<a href=\"https://doi.org/10.1146/annurev-anchem-062012-092628\"\n        target=\"_blank\">Mardis 2013</a>). Such techniques can extend the reach\n        of sequencing technology, allowing for more accurate placement of reads\n        (<a href=\"https://doi.org/10.1146/annurev-genom-090413-025358\"\n        target=\"_blank\">Reinert et al. 2015</a>) and better resolution of repeat\n        regions (<a href=\"https://doi.org/10.1146/annurev-genom-090413-025358\"\n        target=\"_blank\">Reinert et al. 2015</a>), as well as detection of\n        structural variation (<a href=\"https://doi.org/10.1038/nrg2958\"\n        target=\"_blank\">Alkan et al. 2011</a>) and chimeric transcripts\n        (<a href=\"https://doi.org/10.1073/pnas.0904720106\"\n        target=\"_blank\">Maher et al. 2009</a>).\n\n        All these methods assume that the approximate size of an insert is known.\n        (Insert size can be defined as the length in bases of a sequenced DNA or\n        RNA fragment, excluding technical sequences such as adapters, which are\n        typically removed before alignment.) This plot allows for that assumption\n        to be assessed. With the set of mapped fragments for a given sample, QualiMap\n        groups the fragments by insert size, then plots the frequency of mapped\n        fragments (y-axis) over a range of insert sizes (x-axis). In an ideal case,\n        the distribution of fragment sizes for a sequencing library would culminate\n        in a single peak indicating average insert size, with a narrow spread\n        indicating highly consistent fragment lengths.\n\n        QualiMap calculates insert sizes as follows: for each fragment in which\n        every read mapped successfully to the same reference sequence, it\n        extracts the insert size from the `TLEN` field of the leftmost read\n        (see the <a href=\"http://qualimap.bioinfo.cipf.es/doc_html/index.html\"\n        target=\"_blank\">Qualimap 2 documentation</a>), where the `TLEN` (or\n        'observed Template LENgth') field contains 'the number of bases from the\n        leftmost mapped base to the rightmost mapped base'\n        (<a href=\"https://samtools.github.io/hts-specs/\" target=\"_blank\">SAM\n        format specification</a>). Note that because it is defined in terms of\n        alignment to a reference sequence, the value of the `TLEN` field may\n        differ from the insert size due to factors such as alignment clipping,\n        alignment errors, or structural variation or splicing in a gap between\n        reads from the same fragment.'''\n        self.add_section (\n            name = 'Insert size histogram',\n            anchor = 'qualimap-insert-size-histogram',\n            description = 'Distribution of estimated insert sizes of mapped reads.',\n            helptext = insert_size_helptext,\n            plot = linegraph.plot(self.qualimap_bamqc_insert_size_hist, {\n                'id': 'qualimap_insert_size',\n                'title': 'Qualimap BamQC: Insert size histogram',\n                'ylab': 'Fraction of reads',\n                'xlab': 'Insert Size (bp)',\n                'ymin': 0,\n                'xmin': 0,\n                'tt_label': '<b>{point.x} bp</b>: {point.y}',\n            })\n        )\n\n    # Section 4 - GC-content distribution\n    if len(self.qualimap_bamqc_gc_content_dist) > 0:\n        gc_content_helptext = '''\n        GC bias is the difference between the guanine-cytosine content\n        (GC-content) of a set of sequencing reads and the GC-content of the DNA\n        or RNA in the original sample. It is a well-known issue with sequencing\n        systems, and may be introduced by PCR amplification, among other factors\n        (<a href=\"https://doi.org/10.1093/nar/gks001\" target=\"_blank\">Benjamini\n        & Speed 2012</a>; <a href=\"https://doi.org/10.1186/gb-2013-14-5-r51\"\n        target=\"_blank\">Ross et al. 2013</a>).\n\n        QualiMap calculates the GC-content of individual mapped reads, then\n        groups those reads by their GC-content (*1%, 2%, &#8230;, 100%*), and\n        plots the frequency of mapped reads (y-axis) at each level of GC-content\n        (x-axis). This plot shows the GC-content distribution of mapped reads\n        for each read dataset, which should ideally resemble that of the\n        original sample. It can be useful to display the GC-content distribution\n        of an appropriate reference sequence for comparison, and QualiMap has an\n        option to do this (see the <a href=\"http://qualimap.bioinfo.cipf.es/doc_html/index.html\"\n        target=\"_blank\">Qualimap 2 documentation</a>).'''\n        extra_series = []\n        for i, (species_name, species_data) in enumerate(sorted(self.qualimap_bamqc_gc_by_species.items())):\n            extra_series.append({\n                'name': species_name,\n                'data': list(species_data.items()),\n                'dashStyle': 'Dash',\n                'lineWidth': 1,\n                'color': ['#000000', '#E89191'][i % 2],\n            })\n        if len(self.qualimap_bamqc_gc_content_dist) == 1:\n            desc = 'The solid line represents the distribution of GC content of mapped reads for the sample.'\n        else:\n            desc = 'Each solid line represents the distribution of GC content of mapped reads for a given sample.'\n        lg_config = {\n            'id': 'qualimap_gc_content',\n            'title': 'Qualimap BamQC: GC content distribution',\n            'ylab': 'Fraction of reads',\n            'xlab': 'GC content (%)',\n            'ymin': 0,\n            'xmin': 0,\n            'xmax': 100,\n            'tt_label': '<b>{point.x}%</b>: {point.y:.3f}'\n        }\n        if len(extra_series) == 1:\n            desc += ' The dotted line represents a pre-calculated GC distribution for the reference genome.'\n            lg_config['extra_series'] = extra_series\n        elif len(extra_series) > 1:\n            desc += ' Each dotted line represents a pre-calculated GC distribution for a specific reference genome.'\n            lg_config['extra_series'] = extra_series\n\n        self.add_section (\n            name = 'GC content distribution',\n            anchor = 'qualimap-gc-distribution',\n            description = desc,\n            helptext = gc_content_helptext,\n            plot = linegraph.plot(self.qualimap_bamqc_gc_content_dist, lg_config)\n        )", "response": "Add results from Qualimap BamQC parsing to the report"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmake the HighCharts HTML to plot the flexbar rates for the given locale", "response": "def flexbar_barplot (self):\n        \"\"\" Make the HighCharts HTML to plot the flexbar rates \"\"\"\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['remaining_reads'] =                { 'color': '#437bb1', 'name': 'Remaining reads' }\n        keys['skipped_due_to_uncalled_bases'] =  { 'color': '#e63491', 'name': 'Skipped due to uncalled bases' }\n        keys['short_prior_to_adapter_removal'] = { 'color': '#b1084c', 'name': 'Short prior to adapter removal' }\n        keys['finally_skipped_short_reads'] =    { 'color': '#7f0000', 'name': 'Finally skipped short reads' }\n\n        # Config for the plot\n        pconfig = {\n            'id': 'flexbar_plot',\n            'title': 'Flexbar: Processed Reads',\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads',\n            'hide_zero_cats': False\n        }\n\n        self.add_section( plot = bargraph.plot(self.flexbar_data, keys, pconfig) )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind RSeQC read_GC reports and parse their data", "response": "def parse_reports(self):\n    \"\"\" Find RSeQC read_GC reports and parse their data \"\"\"\n\n    # Set up vars\n    self.read_gc = dict()\n    self.read_gc_pct = dict()\n\n    # Go through files and parse data\n    for f in self.find_log_files('rseqc/read_gc'):\n\n        if f['f'].startswith('GC%\tread_count'):\n            gc = list()\n            counts = list()\n            for l in f['f'].splitlines():\n                s = l.split()\n                try:\n                    gc.append(float(s[0]))\n                    counts.append(float(s[1]))\n                except:\n                    pass\n            if len(gc) > 0:\n                sorted_gc_keys = sorted(range(len(gc)), key=lambda k: gc[k])\n                total = sum(counts)\n                if f['s_name'] in self.read_gc:\n                    log.debug(\"Duplicate sample name found! Overwriting: {}\".format(f['s_name']))\n                self.add_data_source(f, section='read_GC')\n                self.read_gc[f['s_name']] = OrderedDict()\n                self.read_gc_pct[f['s_name']] = OrderedDict()\n                for i in sorted_gc_keys:\n                    self.read_gc[f['s_name']][gc[i]] = counts[i]\n                    self.read_gc_pct[f['s_name']][gc[i]] = (counts[i]/total)*100\n\n    # Filter to strip out ignored sample names\n    self.read_gc = self.ignore_samples(self.read_gc)\n\n    if len(self.read_gc) > 0:\n\n        # Add line graph to section\n        pconfig = {\n            'id': 'rseqc_read_gc_plot',\n            'title': 'RSeQC: Read GC Content',\n            'ylab': 'Number of Reads',\n            'xlab': \"GC content (%)\",\n            'xmin': 0,\n            'xmax': 100,\n            'tt_label': \"<strong>{point.x}% GC</strong>: {point.y:.2f}\",\n            'data_labels': [\n                {'name': 'Counts', 'ylab': 'Number of Reads'},\n                {'name': 'Percentages', 'ylab': 'Percentage of Reads'}\n            ]\n        }\n        self.add_section (\n            name = 'Read GC Content',\n            anchor = 'rseqc-read_gc',\n            description = '<a href=\"http://rseqc.sourceforge.net/#read-gc-py\" target=\"_blank\">read_GC</a>' \\\n                \" calculates a histogram of read GC content.</p>\",\n            plot = linegraph.plot([self.read_gc, self.read_gc_pct], pconfig)\n        )\n\n    # Return number of samples found\n    return len(self.read_gc)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds columns to General Statistics table", "response": "def phantompeakqualtools_general_stats(self):\n        \"\"\" Add columns to General Statistics table \"\"\"\n        headers = OrderedDict()\n        headers['Estimated_Fragment_Length_bp'] = {\n            'title': 'Frag Length',\n            'description': 'Estimated fragment length (bp)',\n            'min': 0,\n            'format': '{:,.0f}'\n        }\n        headers['NSC'] = {\n            'title': 'NSC',\n            'description': 'Normalized strand cross-correlation',\n            'max': 10,\n            'min': 0,\n            'format': '{:,.2f}',\n            'scale': 'RdYlGn-rev'\n        }\n        headers['RSC'] = {\n            'title': 'RSC',\n            'description': 'Relative strand cross-correlation',\n            'max': 10,\n            'min': 0,\n            'format': '{:,.2f}',\n            'scale': 'RdYlBu-rev'\n        }\n        self.general_stats_addcols(self.phantompeakqualtools_data, headers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_reports(parent_module):\n\n    # get data\n    data = collect_data(parent_module)\n\n    # Filter to strip out ignored sample names\n    data = parent_module.ignore_samples(data)\n\n    # Reference data in parent module\n    parent_module.picard_variantCalling_data = data\n\n    if len(data) > 0:\n        derive_data(data)\n\n        # Write parsed data to a file\n        parent_module.write_data_file(data, 'multiqc_picard_varientCalling')\n\n        parent_module.general_stats_headers['DBSNP_TITV'] = {\n            'title': 'TiTV ratio (known)',\n            'description': \"The Transition/Transversion ratio of the passing bi-allelic SNP calls made at SNP-database sites.\",\n            'min': 0,\n            'scale': 'Blues',\n            'shared_key': 'titv_ratio'\n        }\n\n        parent_module.general_stats_headers['NOVEL_TITV'] = {\n            'title': 'TiTV ratio (novel)',\n            'description': \"The Transition/Transversion ratio of the passing bi-allelic SNP calls made at non-SNP-database sites.\",\n            'min': 0,\n            'scale': 'Blues',\n            'shared_key': 'titv_ratio'\n        }\n\n        parent_module.general_stats_headers['DBSNP_INS_DEL_RATIO'] = {\n            'title': 'InDel ratio (known)',\n            'description': \"The Insertion / Deletion ratio of the passing bi-allelic SNP calls made at SNP-database sites.\",\n            'min': 0,\n            'scale': 'Greens',\n            'shared_key': 'indel_ratio',\n            'hidden': True,\n        }\n\n        parent_module.general_stats_headers['NOVEL_INS_DEL_RATIO'] = {\n            'title': 'InDel ratio (novel)',\n            'description': \"The Insertion / Deletion ratio of the passing bi-allelic SNP calls made at non-SNP-database sites.\",\n            'min': 0,\n            'scale': 'Greens',\n            'shared_key': 'indel_ratio',\n            'hidden': True,\n        }\n\n\n        parent_module.general_stats_headers['total_called_variants_known'] = {\n            'title': 'Called Variants (known)',\n            'description': \"Total counts of variants in SNP-database sites.\",\n            'shared_key': 'variant_count',\n            'min': 0,\n            'format': '{0:,.0f}',\n            'hidden': True,\n        }\n\n        parent_module.general_stats_headers['total_called_variants_novel'] = {\n            'title': 'Called Variants (novel)',\n            'description': \"Total counts of variants in non-SNP-database sites.\",\n            'shared_key': 'variant_count',\n            'min': 0,\n            'format': '{0:,.0f}',\n            'hidden': True,\n        }\n\n        for s_name in data:\n            if s_name not in parent_module.general_stats_data:\n                parent_module.general_stats_data[s_name] = dict()\n            parent_module.general_stats_data[s_name].update(data[s_name])\n\n\n        # Variant Counts Bargraph\n        parent_module.add_section(\n            name='Variant Types',\n            anchor='picard-variants-types',\n            description='Variants that have been called, looking at variant types. Optionally filtered on label.',\n            helptext=\"\"\"\n            Only passing variants are shown (i.e. non-filtered).\\n\n            SNPs are bi-allelic.\\n\n            Complex InDels are both an insertion and a deletion.\n            \"\"\",\n            plot=compare_variant_type_plot(data)\n        )\n\n        # Variant Counts Table\n        parent_module.add_section(\n            name='Variant Labels',\n            anchor='picard-variants-labels',\n            description='Variants that have been called, comparing with known variant sites.',\n            helptext=\"\"\"\n            Only passing variants are shown (i.e. non-filtered).\\n\n            Variants contain bi-allelic SNPs, multi-allelic SNPs, simple and complex inserts and deletions.\n            \"\"\",\n            plot=compare_variants_label_plot(data)\n        )\n\n    return len(data)", "response": "Find Picard VariantCallingMetrics reports and process their data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind Picard VariantCallingMetrics reports and parse their data", "response": "def collect_data(parent_module):\n    \"\"\" Find Picard VariantCallingMetrics reports and parse their data \"\"\"\n\n    data = dict()\n    for file_meta in parent_module.find_log_files('picard/variant_calling_metrics', filehandles=True):\n        s_name = None\n        for header, value in table_in(file_meta['f'], pre_header_string='## METRICS CLASS'):\n            if header == 'SAMPLE_ALIAS':\n                s_name = value\n                if s_name in data:\n                    log.debug(\"Duplicate sample name found in {}! Overwriting: {}\".format(file_meta['fn'], s_name))\n                data[s_name] = OrderedDict()\n            else:\n                data[s_name][header] = value\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef derive_data(data):\n\n    for s_name, values in data.items():\n        # setup holding variable\n\n        # Sum all variants that have been called\n        total_called_variants = 0\n        for value_name in ['TOTAL_SNPS', 'TOTAL_COMPLEX_INDELS', 'TOTAL_MULTIALLELIC_SNPS', 'TOTAL_INDELS']:\n            total_called_variants = total_called_variants + int(values[value_name])\n        values['total_called_variants'] = total_called_variants\n\n        # Sum all variants that have been called and are known\n        total_called_variants_known = 0\n        for value_name in ['NUM_IN_DB_SNP', 'NUM_IN_DB_SNP_COMPLEX_INDELS', 'NUM_IN_DB_SNP_MULTIALLELIC']:\n            total_called_variants_known = total_called_variants_known + int(values[value_name])\n        total_called_variants_known = total_called_variants_known + int(values['TOTAL_INDELS']) - int(values['NOVEL_INDELS'])\n        values['total_called_variants_known'] = total_called_variants_known\n\n        # Extrapolate the total novel variants\n        values['total_called_variants_novel'] = total_called_variants - total_called_variants_known", "response": "Based on the data derive additional data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning HTML for the Variant Counts barplot", "response": "def compare_variant_type_plot(data):\n    \"\"\" Return HTML for the Variant Counts barplot \"\"\"\n    keys = OrderedDict()\n    keys['snps'] = {'name': 'SNPs', 'color': '#7cb5ec'}\n    keys['indels'] = {'name': 'InDels', 'color': '#90ed7d'}\n    keys['multiallelic_snps'] = {'name': 'multi-allelic SNP', 'color': 'orange'}\n    keys['complex_indels'] = {'name': 'Complex InDels', 'color': '#8085e9'}\n\n    total_variants = dict()\n    known_variants = dict()\n    novel_variants = dict()\n    for s_name, values in data.items():\n        total_variants[s_name] = {\n            'snps': values['TOTAL_SNPS'],\n            'indels': values['TOTAL_INDELS'],\n            'multiallelic_snps': values['TOTAL_MULTIALLELIC_SNPS'],\n            'complex_indels': values['TOTAL_COMPLEX_INDELS'],\n        }\n\n        known_variants[s_name] = {\n            'snps': values['NUM_IN_DB_SNP'],\n            'indels': int(values['TOTAL_INDELS'])-int(values['NOVEL_INDELS']),\n            'multiallelic_snps': values['NUM_IN_DB_SNP_MULTIALLELIC'],\n            'complex_indels': values['NUM_IN_DB_SNP_COMPLEX_INDELS'],\n        }\n\n        novel_variants[s_name] = {\n            'snps': values['NOVEL_SNPS'],\n            'indels': values['NOVEL_INDELS'],\n            'multiallelic_snps': int(values['TOTAL_MULTIALLELIC_SNPS'])-int(values['NUM_IN_DB_SNP_MULTIALLELIC']),\n            'complex_indels': int(values['TOTAL_COMPLEX_INDELS'])-int(values['NUM_IN_DB_SNP_COMPLEX_INDELS']),\n        }\n\n    plot_conf = {\n        'id': 'picard_variantCallingMetrics_variant_type',\n        'title': 'Picard: Variants Called',\n        'ylab': 'Counts of Variants',\n        'hide_zero_cats': False,\n        'data_labels': [\n            {'name': 'Total'},\n            {'name': 'Known'},\n            {'name': 'Novel'}\n        ],\n    }\n    return bargraph.plot(data=[total_variants, known_variants, novel_variants],\n                         cats=[keys, keys, keys],\n                         pconfig=plot_conf)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compare_variants_label_plot(data):\n    keys = OrderedDict()\n\n    keys['total_called_variants_known'] = {'name': 'Known Variants'}\n    keys['total_called_variants_novel'] = {'name': 'Novel Variants'}\n\n    pconfig = {\n        'id': 'picard_variantCallingMetrics_variant_label',\n        'title': 'Picard: Variants Called',\n        'ylab': 'Counts of Variants',\n     }\n\n    return bargraph.plot(data, cats=keys, pconfig=pconfig)", "response": "Return HTML for the Compare variants plot"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntakes the parsed stats from the QUAST report and add some to the General Statistics table at the top of the report", "response": "def quast_general_stats_table(self):\n        \"\"\" Take the parsed stats from the QUAST report and add some to the\n        General Statistics table at the top of the report \"\"\"\n\n        headers = OrderedDict()\n        headers['N50'] = {\n            'title': 'N50 ({})'.format(self.contig_length_suffix),\n            'description': 'N50 is the contig length such that using longer or equal length contigs produces half (50%) of the bases of the assembly (kilo base pairs)',\n            'min': 0,\n            'suffix': self.contig_length_suffix,\n            'scale': 'RdYlGn',\n            'modify': lambda x: x * self.contig_length_multiplier\n        }\n        headers['Total length'] = {\n            'title': 'Length ({})'.format(self.total_length_suffix),\n            'description': 'The total number of bases in the assembly (mega base pairs).',\n            'min': 0,\n            'suffix': self.total_length_suffix,\n            'scale': 'YlGn',\n            'modify': lambda x: x * self.total_length_multiplier\n        }\n        self.general_stats_addcols(self.quast_data, headers)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting some more statistics about the assemblies in a table.", "response": "def quast_table(self):\n        \"\"\" Write some more statistics about the assemblies in a table. \"\"\"\n        headers = OrderedDict()\n\n        headers['N50'] = {\n            'title': 'N50 ({})'.format(self.contig_length_suffix),\n            'description': 'N50 is the contig length such that using longer or equal length contigs produces half (50%) of the bases of the assembly.',\n            'min': 0,\n            'suffix': self.contig_length_suffix,\n            'scale': 'RdYlGn',\n            'modify': lambda x: x * self.contig_length_multiplier\n        }\n\n        headers['N75'] = {\n            'title': 'N75 ({})'.format(self.contig_length_suffix),\n            'description': 'N75 is the contig length such that using longer or equal length contigs produces 75% of the bases of the assembly',\n            'min': 0,\n            'suffix': self.contig_length_suffix,\n            'scale': 'RdYlGn',\n            'modify': lambda x: x * self.contig_length_multiplier\n        }\n\n        headers['L50'] = {\n            'title': 'L50 ({})'.format(self.total_number_contigs_suffix) if self.total_number_contigs_suffix else 'L50',\n            'description': 'L50 is the number of contigs larger than N50, i.e. the minimum number of contigs comprising 50% of the total assembly length.',\n            'min': 0,\n            'suffix': self.total_number_contigs_suffix,\n            'scale': 'GnYlRd',\n            'modify': lambda x: x * self.total_number_contigs_multiplier\n        }\n\n        headers['L75'] = {\n            'title': 'L75 ({})'.format(self.total_number_contigs_suffix) if self.total_number_contigs_suffix else 'L75',\n            'description': 'L75 is the number of contigs larger than N75, i.e. the minimum number of contigs comprising 75% of the total assembly length.',\n            'min': 0,\n            'suffix': self.total_number_contigs_suffix,\n            'scale': 'GnYlRd',\n            'mofidy': lambda x: x * self.total_number_contigs_multiplier\n        }\n        headers['Largest contig'] = {\n            'title': 'Largest contig ({})'.format(self.contig_length_suffix),\n            'description': 'The size of the largest contig of the assembly',\n            'min': 0,\n            'suffix': self.contig_length_suffix,\n            'scale': 'YlGn',\n            'modify': lambda x: x * self.contig_length_multiplier\n        }\n\n        headers['Total length'] = {\n            'title': 'Length ({})'.format(self.total_length_suffix),\n            'description': 'The total number of bases in the assembly.',\n            'min': 0,\n            'suffix': self.total_length_suffix,\n            'scale': 'YlGn',\n            'modify': lambda x: x * self.total_length_multiplier\n        }\n\n        headers['# misassemblies'] = {\n            'title': 'Misassemblies',\n            'description': 'The number of positions in the assembled contigs where the left flanking sequence aligns over 1 kbp away from the right flanking sequence on the reference (relocation) or they overlap on more than 1 kbp (relocation) or flanking sequences align on different strands (inversion) or different chromosomes (translocation).',\n            'scale': 'RdYlGn-rev',\n            'format': '{,:.0f}'\n        }\n        headers['# mismatches per 100 kbp'] = {\n            'title': 'Mismatches/100kbp',\n            'description': 'The number of mismatches per 100 kbp',\n            'scale': 'YlOrRd',\n            'format': '{:,.2f}',\n        }\n        headers['# indels per 100 kbp'] = {\n            'title': 'Indels/100kbp',\n            'description': 'The number of indels per 100 kbp',\n            'scale': 'YlOrRd',\n            'format': '{:,.2f}',\n        }\n        headers['# genes'] = {\n            'title': 'Genes',\n            'description': '# Genes',\n            'scale': 'YlGnBu',\n            'format': '{:,.0f}',\n            'shared_key': 'gene_count'\n        }\n        headers['# genes_partial'] = {\n            'title': 'Genes (Partial)',\n            'description': '# Genes (Partial)',\n            'scale': 'YlGnBu',\n            'format': '{:,.0f}',\n            'shared_key': 'gene_count'\n        }\n        headers['# predicted genes (unique)'] = {\n            'title': 'Genes',\n            'description': '# Predicted Genes (Unique)',\n            'scale': 'YlGnBu',\n            'format': '{:,.0f}',\n            'shared_key': 'gene_count'\n        }\n        headers['Genome fraction (%)'] = {\n            'title': 'Genome Fraction',\n            'description': 'The total number of aligned bases in the reference, divided by the genome size.',\n            'max': 100,\n            'suffix': '%',\n            'scale': 'YlGn'\n        }\n        config = {\n            'id': 'quast_table',\n            'namespace': 'QUAST',\n            'min': 0,\n        }\n        return table.plot(self.quast_data, headers, config)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef quast_contigs_barplot(self):\n\n        # Prep the data\n        data = dict()\n        categories = []\n        for s_name, d in self.quast_data.items():\n            nums_by_t = dict()\n            for k, v in d.items():\n                m = re.match('# contigs \\(>= (\\d+) bp\\)', k)\n                if m and v != '-':\n                    nums_by_t[int(m.groups()[0])] = int(v)\n\n            tresholds = sorted(nums_by_t.keys(), reverse=True)\n            p = dict()\n            cats = []\n            for i, t in enumerate(tresholds):\n                if i == 0:\n                    c = '>= ' + str(t) + ' bp'\n                    cats.append(c)\n                    p[c] = nums_by_t[t]\n                else:\n                    c = str(t) + '-' + str(tresholds[i - 1]) + ' bp'\n                    cats.append(c)\n                    p[c] = nums_by_t[t] - nums_by_t[tresholds[i - 1]]\n            if not categories:\n                categories = cats\n            data[s_name] = p\n\n        pconfig = {\n            'id': 'quast_num_contigs',\n            'title': 'QUAST: Number of Contigs',\n            'ylab': '# Contigs',\n            'yDecimals': False\n        }\n\n        return bargraph.plot(data, categories, pconfig)", "response": "Make a bar plot showing the number and length of contigs for each assembly"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking a bar plot showing the number and length of predicted genes for each assembly", "response": "def quast_predicted_genes_barplot(self):\n        \"\"\"\n        Make a bar plot showing the number and length of predicted genes\n        for each assembly\n        \"\"\"\n\n        # Prep the data\n        # extract the ranges given to quast with \"--gene-thresholds\"\n        prefix = '# predicted genes (>= '\n        suffix = ' bp)'\n        all_thresholds = sorted(list(set([\n            int(key[len(prefix):-len(suffix)])\n            for _, d in self.quast_data.items()\n            for key in d.keys()\n            if key.startswith(prefix)\n            ])))\n\n        data = {}\n        ourpat = '>= {}{} bp'\n        theirpat = prefix+\"{}\"+suffix\n        for s_name, d in self.quast_data.items():\n            thresholds = sorted(list(set([\n                int(key[len(prefix):-len(suffix)])\n                for _, x in self.quast_data.items()\n                for key in x.keys()\n                if key.startswith(prefix)\n            ])))\n            if len(thresholds)<2: continue\n\n            p = dict()\n            try:\n                p = { ourpat.format(thresholds[-1],\"\"): d[theirpat.format(thresholds[-1])] }\n                for low,high in zip(thresholds[:-1], thresholds[1:]):\n                    p[ourpat.format(low,-high)] = d[theirpat.format(low)] - d[theirpat.format(high)]\n\n                assert sum(p.values()) == d[theirpat.format(0)]\n            except AssertionError:\n                log.warning(\"Predicted gene counts didn't add up properly for \\\"{}\\\"\".format(s_name))\n            except KeyError:\n                log.warning(\"Not all predicted gene thresholds available for \\\"{}\\\"\".format(s_name))\n            data[s_name] = p\n\n        cats = [ ourpat.format(low,-high if high else \"\")\n                 for low,high in zip(all_thresholds, all_thresholds[1:]+[None]) ]\n\n        if len(cats) > 0:\n            return bargraph.plot(data, cats, {'id': 'quast_predicted_genes', 'title': 'QUAST: Number of predicted genes', 'ylab': 'Number of predicted genes'})\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding RSeQC read_distribution reports and parse their data", "response": "def parse_reports(self):\n    \"\"\" Find RSeQC read_distribution reports and parse their data \"\"\"\n\n    # Set up vars\n    self.read_dist = dict()\n    first_regexes = {\n        'total_reads': r\"Total Reads\\s+(\\d+)\\s*\",\n        'total_tags': r\"Total Tags\\s+(\\d+)\\s*\",\n        'total_assigned_tags': r\"Total Assigned Tags\\s+(\\d+)\\s*\",\n    }\n    second_regexes = {\n        'cds_exons': r\"CDS_Exons\\s+(\\d+)\\s+(\\d+)\\s+([\\d\\.]+)\\s*\",\n        '5_utr_exons': r\"5'UTR_Exons\\s+(\\d+)\\s+(\\d+)\\s+([\\d\\.]+)\\s*\",\n        '3_utr_exons': r\"3'UTR_Exons\\s+(\\d+)\\s+(\\d+)\\s+([\\d\\.]+)\\s*\",\n        'introns': r\"Introns\\s+(\\d+)\\s+(\\d+)\\s+([\\d\\.]+)\\s*\",\n        'tss_up_1kb': r\"TSS_up_1kb\\s+(\\d+)\\s+(\\d+)\\s+([\\d\\.]+)\\s*\",\n        'tss_up_5kb': r\"TSS_up_5kb\\s+(\\d+)\\s+(\\d+)\\s+([\\d\\.]+)\\s*\",\n        'tss_up_10kb': r\"TSS_up_10kb\\s+(\\d+)\\s+(\\d+)\\s+([\\d\\.]+)\\s*\",\n        'tes_down_1kb': r\"TES_down_1kb\\s+(\\d+)\\s+(\\d+)\\s+([\\d\\.]+)\\s*\",\n        'tes_down_5kb': r\"TES_down_5kb\\s+(\\d+)\\s+(\\d+)\\s+([\\d\\.]+)\\s*\",\n        'tes_down_10kb': r\"TES_down_10kb\\s+(\\d+)\\s+(\\d+)\\s+([\\d\\.]+)\\s*\",\n    }\n\n    # Go through files and parse data using regexes\n    for f in self.find_log_files('rseqc/read_distribution'):\n        d = dict()\n        for k, r in first_regexes.items():\n            r_search = re.search(r, f['f'], re.MULTILINE)\n            if r_search:\n                d[k] = int(r_search.group(1))\n        for k, r in second_regexes.items():\n            r_search = re.search(r, f['f'], re.MULTILINE)\n            if r_search:\n                d['{}_total_bases'.format(k)] = int(r_search.group(1))\n                d['{}_tag_count'.format(k)] = int(r_search.group(2))\n                d['{}_tags_kb'.format(k)] = float(r_search.group(2))\n\n        d['other_intergenic_tag_count'] = d['total_tags']-d['total_assigned_tags']\n\n        # Calculate some percentages for parsed file\n        if 'total_tags' in d:\n            t = float(d['total_tags'])\n            pcts = dict()\n            for k in d:\n                if k.endswith('_tag_count'):\n                    pk = '{}_tag_pct'.format(k[:-10])\n                    pcts[pk] = (float(d[k]) / t)*100.0\n            d.update(pcts)\n\n        if len(d) > 0:\n            if f['s_name'] in self.read_dist:\n                log.debug(\"Duplicate sample name found! Overwriting: {}\".format(f['s_name']))\n            self.add_data_source(f, section='read_distribution')\n            self.read_dist[f['s_name']] = d\n\n    # Filter to strip out ignored sample names\n    self.read_dist = self.ignore_samples(self.read_dist)\n\n    if len(self.read_dist) > 0:\n\n        # Write to file\n        self.write_data_file(self.read_dist, 'multiqc_rseqc_read_distribution')\n\n        # Plot bar graph of groups\n        keys = OrderedDict()\n        keys['cds_exons_tag_count'] = {'name': \"CDS_Exons\"}\n        keys['5_utr_exons_tag_count'] = {'name': \"5'UTR_Exons\"}\n        keys['3_utr_exons_tag_count'] = {'name': \"3'UTR_Exons\"}\n        keys['introns_tag_count'] = {'name': \"Introns\"}\n        keys['tss_up_1kb_tag_count'] = {'name': \"TSS_up_1kb\"}\n        keys['tss_up_5kb_tag_count'] = {'name': \"TSS_up_5kb\"}\n        keys['tss_up_10kb_tag_count'] = {'name': \"TSS_up_10kb\"}\n        keys['tes_down_1kb_tag_count'] = {'name': \"TES_down_1kb\"}\n        keys['tes_down_5kb_tag_count'] = {'name': \"TES_down_5kb\"}\n        keys['tes_down_10kb_tag_count'] = {'name': \"TES_down_10kb\"}\n        keys['other_intergenic_tag_count'] = {'name': \"Other_intergenic\"}\n\n        # Config for the plot\n        pconfig = {\n            'id': 'rseqc_read_distribution_plot',\n            'title': 'RSeQC: Read Distribution',\n            'ylab': '# Tags',\n            'cpswitch_counts_label': 'Number of Tags',\n            'cpswitch_c_active': False\n        }\n\n        self.add_section (\n            name = 'Read Distribution',\n            anchor = 'rseqc-read_distribution',\n            description = '<a href=\"http://rseqc.sourceforge.net/#read-distribution-py\" target=\"_blank\">Read Distribution</a>' \\\n                \" calculates how mapped reads are distributed over genome features.\",\n            plot = bargraph.plot(self.read_dist, keys, pconfig)\n        )\n\n    # Return number of samples found\n    return len(self.read_dist)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntake the parsed stats from the ClipAndMerge report and add it to the basic stats table at the top of the report", "response": "def clipandmerge_general_stats_table(self):\n        \"\"\" Take the parsed stats from the ClipAndMerge report and add it to the\n        basic stats table at the top of the report \"\"\"\n\n        headers = OrderedDict()\n        headers['percentage'] = {\n            'title': '% Merged',\n            'description': 'Percentage of reads merged',\n            'min': 0,\n            'max': 100,\n            'suffix': '%',\n            'scale': 'Greens',\n            'format': '{:,.2f}',\n        }\n        self.general_stats_addcols(self.clipandmerge_data, headers)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake the HTML to plot the duplication rates for the current read.", "response": "def clipandmerge_alignment_plot (self):\n        \"\"\" Make the HighCharts HTML to plot the duplication rates \"\"\"\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['merged_reads'] = { 'name': 'Merged Reads' }\n        keys['usable_not_merged_forward'] = { 'name': 'Usable, not merged (forward)'}\n        keys['usable_not_merged_reverse'] = { 'name': 'Usable, not merged (reverse)'}\n        keys['usable_forward_no_pairing_reverse'] = { 'name': 'Usable forward-only'}\n        keys['usable_reverse_no_pairing_forward'] = { 'name': 'Usable reverse-only'}\n\n        # Config for the plot\n        config = {\n            'id': 'clipandmerge_rates',\n            'title': 'ClipAndMerge: Read merging results',\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads',\n            'hide_zero_cats': False\n        }\n\n        return bargraph.plot(self.clipandmerge_data, keys, config)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates line graph plot for basic histogram data for file_type.", "response": "def plot_basic_hist(samples, file_type, **plot_args):\n    \"\"\" Create line graph plot for basic histogram data for 'file_type'.\n\n    The 'samples' parameter could be from the bbmap mod_data dictionary:\n    samples = bbmap.MultiqcModule.mod_data[file_type]\n    \"\"\"\n\n    sumy = sum([int(samples[sample]['data'][x][0])\n                for sample in samples\n                for x in samples[sample]['data']])\n\n    cutoff = sumy * 0.999\n    all_x = set()\n    for item in sorted(chain(*[samples[sample]['data'].items()\n                                for sample in samples])):\n        all_x.add(item[0])\n        cutoff -= item[1][0]\n        if cutoff < 0:\n            xmax = item[0]\n            break\n    else:\n        xmax = max(all_x)\n\n    data = {\n        sample: {\n            x: samples[sample]['data'][x][0] if x in samples[sample]['data'] else 0\n            for x in all_x\n        }\n        for sample in samples\n    }\n\n    plot_params = {\n            'id': 'bbmap-' + file_type + '_plot',\n            'title': 'BBTools: ' + plot_args['plot_title'],\n            'xmax': xmax\n    }\n    plot_params.update(plot_args['plot_params'])\n    plot = linegraph.plot(\n        data,\n        plot_params\n    )\n\n    return plot"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding bamPEFragmentSize output. Supports the table option.", "response": "def parse_bamPEFragmentSize(self):\n        \"\"\"Find bamPEFragmentSize output. Supports the --table option\"\"\"\n        self.deeptools_bamPEFragmentSize = dict()\n        for f in self.find_log_files('deeptools/bamPEFragmentSizeTable'):\n            parsed_data = self.parseBamPEFile(f)\n            for k, v in parsed_data.items():\n                if k in self.deeptools_bamPEFragmentSize:\n                    log.warning(\"Replacing duplicate sample {}.\".format(k))\n                self.deeptools_bamPEFragmentSize[k] = v\n\n            if len(parsed_data) > 0:\n                self.add_data_source(f, section='bamPEFragmentSize')\n        if len(self.deeptools_bamPEFragmentSize) > 0:\n            headersSE = OrderedDict()\n            headersSE[\"Reads Sampled\"] = {\n                'title': '# Sampled',\n                'description': 'Number of reads sampled',\n                'format': '{:,.0f}'\n            }\n            headersSE[\"Read Len. Min.\"] = {\n                'title': 'Min',\n                'description': 'Minimum read length',\n                'format': '{:,.0f}',\n                'shared_key': 'read_length'\n            }\n            headersSE[\"Read Len. 1st. Qu.\"] = {\n                'title': '1st Quartile',\n                'description': '1st quartile read length',\n                'format': '{:,.0f}',\n                'shared_key': 'read_length'\n            }\n            headersSE[\"Read Len. Mean\"] = {\n                'title': 'Mean',\n                'description': 'Mean read length',\n                'shared_key': 'read_length'\n            }\n            headersSE[\"Read Len. Median\"] = {\n                'title': 'Median',\n                'description': 'Median read length',\n                'format': '{:,.0f}',\n                'shared_key': 'read_length'\n            }\n            headersSE[\"Read Len. 3rd Qu.\"] = {\n                'title': '3rd Quartile',\n                'description': '3rd quartile read length',\n                'format': '{:,.0f}',\n                'shared_key': 'read_length'\n            }\n            headersSE[\"Read Len. Max\"] = {\n                'title': 'Max',\n                'description': 'Maximum read length',\n                'format': '{:,.0f}',\n                'shared_key': 'read_length'\n            }\n            headersSE[\"Read Len. Std.\"] = {\n                'title': 'Std. Dev.',\n                'description': 'read length standard deviation',\n                'shared_key': 'read_length'\n            }\n            headersSE[\"Read Med. Abs. Dev.\"] = {\n                'title': 'MAD',\n                'description': 'read length median absolute deviation',\n                'shared_key': 'read_length'\n            }\n            config = {'namespace': 'deepTools bamPEFragmentSize'}\n            self.add_section(\n                name = \"Read length metrics\",\n                anchor = \"deeptools_readlengths\",\n                plot = table.plot(self.deeptools_bamPEFragmentSize, headersSE, config)\n            )\n\n            headersPE = OrderedDict()\n            headersPE[\"Frag. Sampled\"] = {\n                'title': '# Sampled',\n                'description': 'Number of fragments sampled',\n                'format': '{:,.0f}'\n            }\n            headersPE[\"Frag. Len. Min.\"] = {\n                'title': 'Min',\n                'description': 'Minimum fragment length',\n                'format': '{:,.0f}',\n                'shared_key': 'frag_length'\n            }\n            headersPE[\"Frag. Len. 1st. Qu.\"] = {\n                'title': '1st Quartile',\n                'description': '1st quartile fragment length',\n                'format': '{:,.0f}',\n                'shared_key': 'frag_length'\n            }\n            headersPE[\"Frag. Len. Mean\"] = {\n                'title': 'Mean',\n                'description': 'Mean fragment length',\n                'format': '{:,.0f}',\n                'shared_key': 'frag_length'\n            }\n            headersPE[\"Frag. Len. Median\"] = {\n                'title': 'Median',\n                'description': 'Median fragment length',\n                'format': '{:,.0f}',\n                'shared_key': 'frag_length'\n            }\n            headersPE[\"Frag. Len. 3rd Qu.\"] = {\n                'title': '3rd Quartile',\n                'description': '3rd quartile fragment length',\n                'format': '{:,.0f}',\n                'shared_key': 'frag_length'\n            }\n            headersPE[\"Frag. Len. Max\"] = {\n                'title': 'Max',\n                'description': 'Maximum fragment length',\n                'format': '{:,.0f}',\n                'shared_key': 'frag_length'\n            }\n            headersPE[\"Frag. Len. Std.\"] = {\n                'title': 'Std. Dev.',\n                'description': 'Fragment length standard deviation',\n                'shared_key': 'frag_length'\n            }\n            headersPE[\"Frag. Med. Abs. Dev.\"] = {\n                'title': 'MAD',\n                'description': 'Fragment length median absolute deviation',\n                'shared_key': 'frag_length'\n            }\n\n            # Are there any PE datasets?\n            PE = False\n            for k, v in self.deeptools_bamPEFragmentSize.items():\n                if 'Frag. Len. Min.' in v:\n                    PE = True\n                    break\n            if PE:\n                self.add_section(name=\"Fragment length metrics\",\n                                 anchor=\"deeptools_fragmentlengths\",\n                                 plot=table.plot(self.deeptools_bamPEFragmentSize, headersPE, config))\n\n            # Read length plot\n            config = {\n                'data_labels': [\n                    {\n                        'name': \"Read length distribution\",\n                        'title': \"Read length distribution\",\n                        'ylab': \"Read length (bases)\"\n                    },\n                    {\n                        'name': \"Fragment length distribution\",\n                        'title': \"Fragment length distribution\",\n                        'ylab': \"Fragment length (bases)\"\n                    }\n                ],\n                'id': 'deeptools_readlengthsPlot',\n                'title': 'deepTools: Read/Fragment length distribution',\n                'namespace': 'deepTools bamPEFragmentSize',\n                'ylab': \"Read length (bases)\",\n                'xlab': \"Percentile\"\n            }\n            SE = dict()\n            PE = dict()\n            for k, v in self.deeptools_bamPEFragmentSize.items():\n                SE[k] = {0: v['Read Len. Min.'],\n                         10: v['Read Len. 10%'],\n                         20: v['Read Len. 20%'],\n                         25: v['Read Len. 1st. Qu.'],\n                         30: v['Read Len. 30%'],\n                         40: v['Read Len. 40%'],\n                         50: v['Read Len. Median'],\n                         60: v['Read Len. 60%'],\n                         70: v['Read Len. 70%'],\n                         75: v['Read Len. 3rd Qu.'],\n                         80: v['Read Len. 80%'],\n                         90: v['Read Len. 90%'],\n                         99: v['Read Len. 99%'],\n                         100: v['Read Len. Max']}\n                if 'Frag. Len. Min.' not in v:\n                    continue\n                PE[k] = {0: v['Frag. Len. Min.'],\n                         10: v['Frag. Len. 10%'],\n                         20: v['Frag. Len. 20%'],\n                         25: v['Frag. Len. 1st. Qu.'],\n                         30: v['Frag. Len. 30%'],\n                         40: v['Frag. Len. 40%'],\n                         50: v['Frag. Len. Median'],\n                         60: v['Frag. Len. 60%'],\n                         70: v['Frag. Len. 70%'],\n                         75: v['Frag. Len. 3rd Qu.'],\n                         80: v['Frag. Len. 80%'],\n                         90: v['Frag. Len. 90%'],\n                         99: v['Frag. Len. 99%'],\n                         100: v['Frag. Len. Max']}\n            self.add_section(\n                name = \"Read/fragment length distribution\",\n                anchor = \"deeptools_fragmentlengths_dist\",\n                plot = linegraph.plot([SE, PE], config)\n            )\n\n        return len(self.deeptools_bamPEFragmentSize)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_gatk_base_recalibrator(self):\n\n        report_table_headers = {\n            '#:GATKTable:Arguments:Recalibration argument collection values used in this run': 'arguments',\n            '#:GATKTable:Quantized:Quality quantization map': 'quality_quantization_map',\n            '#:GATKTable:RecalTable0:': 'recal_table_0',\n            '#:GATKTable:RecalTable1:': 'recal_table_1',\n            '#:GATKTable:RecalTable2:': 'recal_table_2',\n        }\n        samples_kept = {rt_type: set() for rt_type in recal_table_type}\n        self.gatk_base_recalibrator = {recal_type:\n                                           {table_name: {}\n                                            for table_name\n                                            in report_table_headers.values()}\n                                       for recal_type in recal_table_type}\n\n        for f in self.find_log_files('gatk/base_recalibrator', filehandles=True):\n            parsed_data = self.parse_report(f['f'].readlines(), report_table_headers)\n            rt_type = determine_recal_table_type(parsed_data)\n            if len(parsed_data) > 0:\n                if f['s_name'] in samples_kept[rt_type]:\n                    log.debug(\"Duplicate sample name found! Overwriting: {}\".format(f['s_name']))\n                else:\n                    samples_kept[rt_type].add(f['s_name'])\n\n                self.add_data_source(f, section='base_recalibrator')\n                for table_name, sample_tables in parsed_data.items():\n                    self.gatk_base_recalibrator[rt_type][table_name][\n                        f['s_name']] = sample_tables\n\n        # Filter to strip out ignored sample names\n        for rt_type in recal_table_type:\n            for table_name, sample_tables in self.gatk_base_recalibrator[rt_type].items():\n                self.gatk_base_recalibrator[rt_type][table_name] = self.ignore_samples(\n                    sample_tables)\n\n        n_reports_found = sum([len(samples_kept[rt_type]) for rt_type in recal_table_type])\n\n        if n_reports_found > 0:\n            log.info(\"Found {} BaseRecalibrator reports\".format(n_reports_found))\n\n            self.add_quality_score_vs_no_of_observations_section()\n\n        return n_reports_found", "response": "Find GATK BaseRecalibrator logs and parse their data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a section for the quality score vs number of observations line plot", "response": "def add_quality_score_vs_no_of_observations_section(self):\n        \"\"\" Add a section for the quality score vs number of observations line plot \"\"\"\n\n        sample_data = []\n        data_labels = []\n        for rt_type_name, rt_type in recal_table_type._asdict().items():\n            sample_tables = self.gatk_base_recalibrator[rt_type]['quality_quantization_map']\n            if len(sample_tables) == 0:\n                continue\n\n            sample_data.append({\n                sample: {int(x): int(y) for x, y in zip(table['QualityScore'], table['Count'])}\n                for sample, table in sample_tables.items()\n            })\n\n            sample_y_sums = {\n                sample: sum(int(y) for y in table['Count'])\n                for sample, table\n                in sample_tables.items()\n            }\n\n            sample_data.append({\n                sample: {\n                    int(x): float(y) / sample_y_sums[sample]\n                    for x, y in zip(table['QualityScore'], table['Count'])\n                }\n                for sample, table in sample_tables.items()\n            })\n\n            flat_proportions = [float(y) / sample_y_sums[sample]\n                                for sample, table in sample_tables.items()\n                                for y in table['Count']]\n            prop_ymax = max(flat_proportions)\n            data_labels.append({'name': \"{} Count\".format(rt_type_name.capitalize().replace('_', '-')),\n                                'ylab': 'Count'})\n            data_labels.append({'ymax': prop_ymax,\n                                'name': \"{} Percent\".format(rt_type_name.capitalize().replace('_', '-')),\n                                'ylab': 'Percent'})\n\n        plot = linegraph.plot(\n            sample_data,\n            pconfig={\n                'title': \"Observed Quality Score Counts\",\n                'id': 'gatk-base-recalibrator-quality-score-vs-number-of-observations',\n                'xlab': 'Observed Quality Score',\n                'ylab': 'Count',\n                'xDecimals': False,\n                'data_labels': data_labels,\n            })\n\n        # Reported vs empirical quality scores\n        self.add_section(\n            name='Observed Quality Scores',\n            description=(\n                'This plot shows the distribution of base quality scores in each sample before and '\n                'after base quality score recalibration (BQSR). Applying BQSR should broaden the '\n                'distribution of base quality scores.'\n            ),\n            helptext=(\n                'For more information see '\n                '[the Broad\\'s description of BQSR]'\n                '(https://gatkforums.broadinstitute.org/gatk/discussion/44/base-quality-score-recalibration-bqsr)'\n                '.'\n            ),\n            plot=plot,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_bbt(self, fh):\n        parsed_data = OrderedDict()\n        headers = None\n        for l in fh:\n            s = l.split(\"\\t\")\n            if headers is None:\n                headers = s\n            else:\n                parsed_data[s[0]] = dict()\n                for i, h in enumerate(headers[1:]):\n                    parsed_data[s[0]][h] = float(s[i+1])\n\n        return parsed_data", "response": "Parse the BioBloom Tools output into a 3D dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bbt_simple_plot(self):\n\n        # First, sum the different types of alignment counts\n        data = OrderedDict()\n        cats = OrderedDict()\n        for s_name in self.bbt_data:\n            data[s_name] = OrderedDict()\n            for org in self.bbt_data[s_name]:\n                data[s_name][org] = self.bbt_data[s_name][org]['hits'] - self.bbt_data[s_name][org]['shared']\n                if org not in cats and org != 'multiMatch' and org != 'noMatch':\n                    if org.lower().endswith('.fa'):\n                        cname = org[:-3]\n                    elif org.lower().endswith('.fasta'):\n                        cname = org[:-6]\n                    else:\n                        cname = org\n                    cats[org] = { 'name': cname }\n\n        pconfig = {\n            'id': 'biobloom_tools',\n            'title': 'BioBloom Tools: Alignment counts per species',\n            'ylab': 'Number of hits',\n            'hide_zero_cats': False\n        }\n        cats['multiMatch'] = { 'name': 'Multiple Genomes', 'color': '#820000' }\n        cats['noMatch'] = { 'name': 'No Match', 'color': '#cccccc' }\n\n        return bargraph.plot(data, cats, pconfig)", "response": "Makes a simple bar plot with summed alignment counts for each species stacked."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the FastQ Screen output into a 3D dict", "response": "def parse_fqscreen(self, f):\n        \"\"\" Parse the FastQ Screen output into a 3D dict \"\"\"\n        parsed_data = OrderedDict()\n        reads_processed = None\n        nohits_pct = None\n        for l in f['f']:\n            if l.startswith('%Hit_no_genomes:') or l.startswith('%Hit_no_libraries:'):\n                nohits_pct = float(l.split(':', 1)[1])\n                parsed_data['No hits'] = {'percentages': {'one_hit_one_library': nohits_pct }}\n            else:\n                fqs = re.search(r\"^(\\S+)\\s+(\\d+)\\s+(\\d+)\\s+([\\d\\.]+)\\s+(\\d+)\\s+([\\d\\.]+)\\s+(\\d+)\\s+([\\d\\.]+)\\s+(\\d+)\\s+([\\d\\.]+)\\s+(\\d+)\\s+([\\d\\.]+)$\", l)\n                if fqs:\n                    org = fqs.group(1)\n                    parsed_data[org] = {'percentages':{}, 'counts':{}}\n                    reads_processed = int(fqs.group(2))\n                    parsed_data[org]['counts']['reads_processed'] = int(fqs.group(2))\n                    parsed_data[org]['counts']['unmapped'] = int(fqs.group(3))\n                    parsed_data[org]['percentages']['unmapped'] = float(fqs.group(4))\n                    parsed_data[org]['counts']['one_hit_one_library'] = int(fqs.group(5))\n                    parsed_data[org]['percentages']['one_hit_one_library'] = float(fqs.group(6))\n                    parsed_data[org]['counts']['multiple_hits_one_library'] = int(fqs.group(7))\n                    parsed_data[org]['percentages']['multiple_hits_one_library'] = float(fqs.group(8))\n                    parsed_data[org]['counts']['one_hit_multiple_libraries'] = int(fqs.group(9))\n                    parsed_data[org]['percentages']['one_hit_multiple_libraries'] = float(fqs.group(10))\n                    parsed_data[org]['counts']['multiple_hits_multiple_libraries'] = int(fqs.group(11))\n                    parsed_data[org]['percentages']['multiple_hits_multiple_libraries'] = float(fqs.group(12))\n                    # Can't use #Reads in subset as varies. #Reads_processed should be same for all orgs in a sample\n                    parsed_data['total_reads'] = int(fqs.group(2))\n\n        if len(parsed_data) == 0:\n            return None\n\n        # Calculate no hits counts\n        if reads_processed and nohits_pct:\n            parsed_data['No hits']['counts'] = {'one_hit_one_library': int((nohits_pct/100.0) * float(reads_processed)) }\n        else:\n            log.warn(\"Couldn't find number of reads with no hits for '{}'\".format(f['s_name']))\n\n        self.num_orgs = max(len(parsed_data), self.num_orgs)\n        return parsed_data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fqscreen_plot (self):\n\n        categories = list()\n        getCats = True\n        data = list()\n        p_types = OrderedDict()\n        p_types['multiple_hits_multiple_libraries'] = {'col': '#7f0000', 'name': 'Multiple Hits, Multiple Genomes' }\n        p_types['one_hit_multiple_libraries'] = {'col': '#ff0000', 'name': 'One Hit, Multiple Genomes' }\n        p_types['multiple_hits_one_library'] = {'col': '#00007f', 'name': 'Multiple Hits, One Genome' }\n        p_types['one_hit_one_library'] = {'col': '#0000ff', 'name': 'One Hit, One Genome' }\n        for k, t in p_types.items():\n            first = True\n            for s in sorted(self.fq_screen_data.keys()):\n                thisdata = list()\n                if len(categories) > 0:\n                    getCats = False\n                for org in sorted(self.fq_screen_data[s]):\n                    if org == 'total_reads':\n                        continue\n                    try:\n                        thisdata.append(self.fq_screen_data[s][org]['percentages'][k])\n                    except KeyError:\n                        thisdata.append(None)\n                    if getCats:\n                        categories.append(org)\n                td = {\n                    'name': t['name'],\n                    'stack': s,\n                    'data': thisdata,\n                    'color': t['col']\n                }\n                if first:\n                    first = False\n                else:\n                    td['linkedTo'] = ':previous'\n                data.append(td)\n\n        html = '<div id=\"fq_screen_plot\" class=\"hc-plot\"></div> \\n\\\n        <script type=\"text/javascript\"> \\n\\\n            fq_screen_data = {};\\n\\\n            fq_screen_categories = {};\\n\\\n            $(function () {{ \\n\\\n                $(\"#fq_screen_plot\").highcharts({{ \\n\\\n                    chart: {{ type: \"column\", backgroundColor: null }}, \\n\\\n                    title: {{ text: \"FastQ Screen Results\" }}, \\n\\\n                    xAxis: {{ categories: fq_screen_categories }}, \\n\\\n                    yAxis: {{ \\n\\\n                        max: 100, \\n\\\n                        min: 0, \\n\\\n                        title: {{ text: \"Percentage Aligned\" }} \\n\\\n                    }}, \\n\\\n                    tooltip: {{ \\n\\\n                        formatter: function () {{ \\n\\\n                            return \"<b>\" + this.series.stackKey.replace(\"column\",\"\") + \" - \" + this.x + \"</b><br/>\" + \\n\\\n                                this.series.name + \": \" + this.y + \"%<br/>\" + \\n\\\n                                \"Total Alignment: \" + this.point.stackTotal + \"%\"; \\n\\\n                        }}, \\n\\\n                    }}, \\n\\\n                    plotOptions: {{ \\n\\\n                        column: {{ \\n\\\n                            pointPadding: 0, \\n\\\n                            groupPadding: 0.02, \\n\\\n                            stacking: \"normal\" }} \\n\\\n                    }}, \\n\\\n                    series: fq_screen_data \\n\\\n                }}); \\n\\\n            }}); \\n\\\n        </script>'.format(json.dumps(data), json.dumps(categories))\n\n        return html", "response": "Makes a fancy custom plot which replicates the plot seen in the main FastQ Screen program."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes a simple bar plot with summed alignment counts for each species stacked.", "response": "def fqscreen_simple_plot(self):\n        \"\"\" Makes a simple bar plot with summed alignment counts for\n        each species, stacked. \"\"\"\n\n        # First, sum the different types of alignment counts\n        data = OrderedDict()\n        cats = OrderedDict()\n        for s_name in sorted(self.fq_screen_data):\n            data[s_name] = OrderedDict()\n            sum_alignments = 0\n            for org in self.fq_screen_data[s_name]:\n                if org == 'total_reads':\n                    continue\n                try:\n                    data[s_name][org] = self.fq_screen_data[s_name][org]['counts']['one_hit_one_library']\n                except KeyError:\n                    log.error(\"No counts found for '{}' ('{}'). Could be malformed or very old FastQ Screen results.\".format(org, s_name))\n                    continue\n                try:\n                    data[s_name][org] += self.fq_screen_data[s_name][org]['counts']['multiple_hits_one_library']\n                except KeyError:\n                    pass\n                sum_alignments += data[s_name][org]\n                if org not in cats and org != 'No hits':\n                    cats[org] = { 'name': org }\n\n            # Calculate hits in multiple genomes\n            if 'total_reads' in self.fq_screen_data[s_name]:\n                data[s_name]['Multiple Genomes'] = self.fq_screen_data[s_name]['total_reads'] - sum_alignments\n\n        # Strip empty dicts\n        for s_name in list(data.keys()):\n            if len(data[s_name]) == 0:\n                del data[s_name]\n\n        pconfig = {\n            'id': 'fastq_screen',\n            'title': 'FastQ Screen',\n            'cpswitch_c_active': False,\n            'hide_zero_cats': False\n        }\n        cats['Multiple Genomes'] = { 'name': 'Multiple Genomes', 'color': '#820000' }\n        cats['No hits'] = { 'name': 'No hits', 'color': '#cccccc' }\n\n        return bargraph.plot(data, cats, pconfig)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_reports(self):\n\n    # Set up vars\n    self.picard_alignment_metrics = dict()\n\n    # Go through logs and find Metrics\n    for f in self.find_log_files('picard/alignment_metrics', filehandles=True):\n        parsed_data = dict()\n        s_name = None\n        keys = None\n        for l in f['f']:\n            # New log starting\n            if 'AlignmentSummaryMetrics' in l and 'INPUT' in l:\n                s_name = None\n                keys = None\n                # Pull sample name from input\n                fn_search = re.search(r\"INPUT(?:=|\\s+)(\\[?[^\\s]+\\]?)\", l, flags=re.IGNORECASE)\n                if fn_search:\n                    s_name = os.path.basename(fn_search.group(1).strip('[]'))\n                    s_name = self.clean_s_name(s_name, f['root'])\n                    parsed_data[s_name] = dict()\n\n            if s_name is not None:\n                if 'AlignmentSummaryMetrics' in l and '## METRICS CLASS' in l:\n                    keys = f['f'].readline().strip(\"\\n\").split(\"\\t\")\n                elif keys:\n                    vals = l.strip(\"\\n\").split(\"\\t\")\n                    if len(vals) == len(keys):\n                        # Ignore the FIRST_OF_PAIR / SECOND_OF_PAIR data to simplify things\n                        if vals[0] == 'PAIR' or vals[0] == 'UNPAIRED':\n                            for i, k in enumerate(keys):\n                                try:\n                                    parsed_data[s_name][k] = float(vals[i])\n                                except ValueError:\n                                    parsed_data[s_name][k] = vals[i]\n                    else:\n                        s_name = None\n                        keys = None\n\n        # Remove empty dictionaries\n        for s_name in list(parsed_data.keys()):\n            if len(parsed_data[s_name]) == 0:\n                parsed_data.pop(s_name, None)\n\n        # Manipulate sample names if multiple baits found\n        for s_name in parsed_data.keys():\n            if s_name in self.picard_alignment_metrics:\n                log.debug(\"Duplicate sample name found in {}! Overwriting: {}\".format(f['fn'], s_name))\n            self.add_data_source(f, s_name, section='AlignmentSummaryMetrics')\n            self.picard_alignment_metrics[s_name] = parsed_data[s_name]\n\n    # Filter to strip out ignored sample names\n    self.picard_alignment_metrics = self.ignore_samples(self.picard_alignment_metrics)\n\n    if len(self.picard_alignment_metrics) > 0:\n\n        # Write parsed data to a file\n        self.write_data_file(self.picard_alignment_metrics, 'multiqc_picard_AlignmentSummaryMetrics')\n\n        # Add to general stats table\n        self.general_stats_headers['PCT_PF_READS_ALIGNED'] = {\n            'title': '% Aligned',\n            'description': 'Percent of aligned reads',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'format': '{:,.0f}',\n            'scale': 'RdYlGn',\n            'modify': lambda x: self.multiply_hundred(x)\n        }\n        for s_name in self.picard_alignment_metrics:\n            if s_name not in self.general_stats_data:\n                self.general_stats_data[s_name] = dict()\n            self.general_stats_data[s_name].update( self.picard_alignment_metrics[s_name] )\n\n\n\n        # Make the bar plot of alignment read count\n        pdata = dict()\n        for s_name in self.picard_alignment_metrics.keys():\n            pdata[s_name] = dict()\n            # Picard reports both reads for PE data. Divide it by two as most people will expect # clusters\n            if self.picard_alignment_metrics[s_name]['CATEGORY'] == 'PAIR':\n                pdata[s_name]['total_reads'] = self.picard_alignment_metrics[s_name]['TOTAL_READS'] / 2\n                pdata[s_name]['aligned_reads'] = self.picard_alignment_metrics[s_name]['PF_READS_ALIGNED'] / 2\n            else:\n                pdata[s_name]['total_reads'] = self.picard_alignment_metrics[s_name]['TOTAL_READS']\n                pdata[s_name]['aligned_reads'] = self.picard_alignment_metrics[s_name]['PF_READS_ALIGNED']\n            pdata[s_name]['unaligned_reads'] = pdata[s_name]['total_reads'] - pdata[s_name]['aligned_reads']\n\n        keys = OrderedDict()\n        keys['aligned_reads'] = {'name': 'Aligned Reads'}\n        keys['unaligned_reads'] = {'name': 'Unaligned Reads'}\n\n        # Config for the plot\n        pconfig = {\n            'id': 'picard_aligned_reads',\n            'title': 'Picard: Aligned Reads',\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads',\n        }\n\n        self.add_section (\n            name = 'Alignment Summary',\n            anchor = 'picard-alignmentsummary',\n            description = \"Plase note that Picard's read counts are divided by two for paired-end data.\",\n            plot = bargraph.plot(pdata, keys, pconfig)\n        )\n\n    # Return the number of detected samples to the parent module\n    return len(self.picard_alignment_metrics)", "response": "Find Picard AlignmentSummaryMetrics reports and parse their data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_minionqc_report(self, s_name, f):\n        '''\n        Parses minionqc's 'summary.yaml' report file for results.\n        Uses only the \"All reads\" stats. Ignores \"Q>=x\" part.\n        '''\n        try:\n            # Parsing as OrderedDict is slightly messier with YAML\n            # http://stackoverflow.com/a/21048064/713980\n            def dict_constructor(loader, node):\n                return OrderedDict(loader.construct_pairs(node))\n            yaml.add_constructor(yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG, dict_constructor)\n            summary_dict = yaml.safe_load(f)\n        except Exception as e:\n            log.error(\"Error parsing MinIONQC input file: {}\".format(f))\n            return\n\n        # Do a deep copy as dicts are immutable\n        self.minionqc_raw_data[s_name] = copy.deepcopy(summary_dict)\n\n        # get q value threshold used for reads\n        q_threshold = None\n        for k in summary_dict.keys():\n            if k.startswith('Q>='):\n                q_threshold = k\n\n        data_dict = {}\n        data_dict['all'] = summary_dict['All reads']        # all reads\n        data_dict['q_filt'] = summary_dict[q_threshold]     # quality filtered reads\n\n        for q_key in ['all', 'q_filt']:\n            for key_1 in ['reads', 'gigabases']:\n                for key_2 in data_dict[q_key][key_1]:\n                    new_key = '{} {}'.format(key_1, key_2)\n                    data_dict[q_key][new_key] = data_dict[q_key][key_1][key_2]\n                data_dict[q_key].pop(key_1)    # removes key after flattening\n\n        self.minionqc_data[s_name] = data_dict['all']     # stats for all reads\n        self.qfilt_data[s_name] = data_dict['q_filt']     # stats for q-filtered reads\n        self.q_threshold_list.add(q_threshold)", "response": "Parses a MinIONQC summary. yaml file for results."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef headers_to_use(self):\n        '''\n        Defines features of columns to be used in multiqc table\n        '''\n        headers = OrderedDict()\n\n        headers['total.reads'] = {\n            'title': 'Total reads',\n            'description': 'Total number of reads',\n            'format': '{:,.0f}',\n            'scale': 'Greys'\n        }\n        headers['total.gigabases'] = {\n            'title': 'Total bases (GB)',\n            'description': 'Total bases',\n            'format': '{:,.2f}',\n            'scale': 'Blues'\n        }\n        headers['N50.length'] = {\n            'title': 'Reads N50',\n            'description': 'Minimum read length needed to cover 50% of all reads',\n            'format': '{:,.0f}',\n            'scale': 'Purples',\n        }\n        headers['mean.q'] = {\n            'title': 'Mean Q score',\n            'description': 'Mean quality of reads',\n            'min': 0,\n            'max': 15,\n            'format': '{:,.1f}',\n            'hidden': True,\n            'scale': 'Greens',\n        }\n        headers['median.q'] = {\n            'title': 'Median Q score',\n            'description': 'Median quality of reads',\n            'min': 0,\n            'max': 15,\n            'format': '{:,.1f}',\n            'scale': 'Greens',\n        }\n        headers['mean.length'] = {\n            'title': 'Mean length (bp)',\n            'description': 'Mean read length',\n            'format': '{:,.0f}',\n            'hidden': True,\n            'scale': 'Blues',\n        }\n        headers['median.length'] = {\n            'title': 'Median length (bp)',\n            'description': 'Median read length',\n            'format': '{:,.0f}',\n            'scale': 'Blues',\n        }\n\n        # Add row ID to avoid duplicates\n        for k in headers:\n            h_id = re.sub('[^0-9a-zA-Z]+', '_', headers[k]['title'])\n            headers[k]['rid'] = \"rid_{}\".format(h_id)\n\n        return headers", "response": "Defines features of columns to be used in multiqc table\n            is a dict of the keys that are used in the multiqc table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef table_qALL(self):\n\n        self.add_section (\n            name = 'Stats: All reads',\n            anchor = 'minionqc-stats-qAll',\n            description = 'MinIONQC statistics for all reads',\n            plot = table.plot(\n                self.minionqc_data,\n                self.headers_to_use(),\n                {\n                    'namespace': 'MinIONQC',\n                    'id': 'minionqc-stats-qAll-table',\n                    'table_title': 'MinIONQC Stats: All reads'\n                }\n            )\n        )", "response": "Table showing stats for all reads"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef table_qfiltered(self):\n\n        description = 'MinIONQC statistics for quality filtered reads. ' + \\\n                        'Quailty threshold used: {}.'.format(', '.join(list(self.q_threshold_list)))\n        if len(self.q_threshold_list) > 1:\n            description += '''\n            <div class=\"alert alert-warning\">\n              <span class=\"glyphicon glyphicon-warning-sign\"></span>\n              <strong>Warning!</strong> More than one quality thresholds were present.\n            </div>\n            '''\n            log.warning('More than one quality thresholds were present. Thresholds: {}.'.format(', '.join(list(self.q_threshold_list))))\n\n        self.add_section (\n            name = 'Stats: Quality filtered reads',\n            anchor = 'minionqc-stats-qFilt',\n            description = description,\n            plot = table.plot(\n                self.qfilt_data,\n                self.headers_to_use(),\n                {\n                    'namespace': 'MinIONQC',\n                    'id': 'minionqc-stats-qFilt-table',\n                    'table_title': 'MinIONQC Stats: Quality filtered reads'\n                }\n            )\n        )", "response": "Table showing stats for quality filtered reads"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_samtools_rmdup(self):\n\n        self.samtools_rmdup = dict()\n        for f in self.find_log_files('samtools/rmdup', filehandles=True):\n            # Example below:\n            # [bam_rmdupse_core] 26602816 / 103563641 = 0.2569 in library '   '\n            dups_regex = \"\\[bam_rmdups?e?_core\\] (\\d+) / (\\d+) = (\\d+\\.\\d+) in library '(.*)'\"\n            s_name = f['s_name']\n            for l in f['f']:\n                match = re.search(dups_regex, l)\n                if match:\n                    library_name = match.group(4).strip()\n                    if library_name != '':\n                        s_name = library_name\n                    if s_name in self.samtools_rmdup:\n                        log.debug(\"Duplicate sample name found in {}! Overwriting: {}\".format(f['fn'], s_name))\n                    self.add_data_source(f, s_name)\n                    self.samtools_rmdup[s_name] = dict()\n                    self.samtools_rmdup[s_name]['n_dups'] = int(match.group(1))\n                    self.samtools_rmdup[s_name]['n_tot'] = int(match.group(2))\n                    self.samtools_rmdup[s_name]['n_unique'] = int(match.group(2)) - int(match.group(1))\n                    self.samtools_rmdup[s_name]['pct_dups'] = float(match.group(3))*100\n\n        # Filter to strip out ignored sample names\n        self.samtools_rmdup = self.ignore_samples(self.samtools_rmdup)\n\n        if len(self.samtools_rmdup) > 0:\n            # Write parsed report data to a file\n            self.write_data_file(self.samtools_rmdup, 'multiqc_samtools_rmdup')\n\n            # Make a bar plot showing duplicates\n            keys = OrderedDict()\n            keys['n_unique'] = {'name': 'Non-duplicated reads'}\n            keys['n_dups'] = {'name': 'Duplicated reads'}\n            pconfig = {\n                'id': 'samtools_rmdup_plot',\n                'title': 'Samtools rmdup: Duplicate alignments',\n                'ylab': 'Number of reads',\n                'yDecimals': False\n            }\n            self.add_section (\n                name = 'Duplicates removed',\n                anchor = 'samtools-rmdup',\n                plot = bargraph.plot(self.samtools_rmdup, keys, pconfig)\n            )\n\n            # Add a column to the General Stats table\n            # General Stats Table\n            stats_headers = OrderedDict()\n            stats_headers['pct_dups'] = {\n                'title': '% Dups',\n                'description': 'Percent of duplicate alignments',\n                'min': 0,\n                'max': 100,\n                'suffix': '%',\n                'scale': 'OrRd'\n            }\n            self.general_stats_addcols(self.samtools_rmdup, stats_headers, 'Samtools rmdup')\n\n        return len(self.samtools_rmdup)", "response": "Find Samtools rmdup logs and parse their data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the summary file into a dictionary of counts.", "response": "def parse_summary(self, contents):\n        \"\"\"Parses summary file into a dictionary of counts.\"\"\"\n\n        lines = contents.strip().split('\\n')\n\n        data = {}\n        for row in lines[1:]:\n            split = row.strip().split('\\t')\n\n            sample = split[0]\n\n            data[sample] = {\n                'species_a': int(split[1]),\n                'species_b': int(split[2]),\n                'ambiguous': int(split[3])\n            }\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd stats to general stats table.", "response": "def add_stats_table(self):\n        \"\"\"Adds stats to general table.\"\"\"\n\n        totals = {sample: sum(counts.values())\n                  for sample, counts in self.data.items()}\n\n        percentages = {sample: {k: (v / totals[sample]) * 100\n                                for k, v in counts.items()}\n                       for sample, counts in self.data.items()}\n\n        headers = {\n            'species_a': {\n                'title': '% Species a',\n                'description': 'Percentage of reads mapping to species a',\n                'max': 100,\n                'min': 0,\n                'suffix': '%',\n                'scale': 'YlGn'\n            }\n        }\n\n        self.general_stats_addcols(percentages, headers)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_stats_plot(self):\n\n        keys = OrderedDict()\n        keys['species_a'] = {'color': '#437bb1', 'name': 'Species a'}\n        keys['species_b'] = {'color': '#b1084c', 'name': 'Species b'}\n        keys['ambiguous'] = {'color': '#333333', 'name': 'Ambiguous'}\n\n        plot_config = {\n            'id': \"disambiguated_alignments\",\n            'title': \"Disambiguate: Alignment Counts\",\n            'cpswitch_counts_label': \"# Reads\",\n            'ylab': \"# Reads\"\n        }\n\n        self.add_section(\n            plot=bargraph.plot(self.data, keys, plot_config)\n        )", "response": "Plots alignment stats as bargraph."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_headers_in_order(self):\n        res = list()\n        #Scan through self.headers_in_order and just bolt on the actual header info\n        for bucket in sorted(self.headers_in_order):\n            for idx, k in self.headers_in_order[bucket]:\n                res.append( (idx, k, self.headers[idx][k]) )\n        return res", "response": "Gets the headers in the order they want to be displayed."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the HTSeq Count log file.", "response": "def parse_htseq_report (self, f):\n        \"\"\" Parse the HTSeq Count log file. \"\"\"\n        keys = [ '__no_feature', '__ambiguous', '__too_low_aQual', '__not_aligned', '__alignment_not_unique' ]\n        parsed_data = dict()\n        assigned_counts = 0\n        for l in f['f']:\n            s = l.split(\"\\t\")\n            if s[0] in keys:\n                parsed_data[s[0][2:]] = int(s[-1])\n            else:\n                try:\n                    assigned_counts += int(s[-1])\n                except (ValueError, IndexError):\n                    pass\n        if len(parsed_data) > 0:\n            parsed_data['assigned'] = assigned_counts\n            parsed_data['total_count'] = sum([v for v in parsed_data.values()])\n            parsed_data['percent_assigned'] = (float(parsed_data['assigned']) / float(parsed_data['total_count'])) * 100.0\n            return parsed_data\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntake the parsed stats from the HTSeq Count report and add them to the basic stats table at the top of the report", "response": "def htseq_stats_table(self):\n        \"\"\" Take the parsed stats from the HTSeq Count report and add them to the\n        basic stats table at the top of the report \"\"\"\n\n        headers = OrderedDict()\n        headers['percent_assigned'] = {\n            'title': '% Assigned',\n            'description': '% Assigned reads',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'RdYlGn'\n        }\n        headers['assigned'] = {\n            'title': '{} Assigned'.format(config.read_count_prefix),\n            'description': 'Assigned Reads ({})'.format(config.read_count_desc),\n            'min': 0,\n            'scale': 'PuBu',\n            'modify': lambda x: float(x) * config.read_count_multiplier,\n            'shared_key': 'read_count'\n        }\n        self.general_stats_addcols(self.htseq_data, headers)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking the HTSeq Count assignment rates plot", "response": "def htseq_counts_chart (self):\n        \"\"\" Make the HTSeq Count assignment rates plot \"\"\"\n        cats = OrderedDict()\n        cats['assigned'] =      { 'name': 'Assigned' }\n        cats['ambiguous'] =     { 'name': 'Ambiguous' }\n        cats['alignment_not_unique'] = { 'name': 'Alignment Not Unique' }\n        cats['no_feature'] =    { 'name': 'No Feature' }\n        cats['too_low_aQual'] = { 'name': 'Too Low aQual' }\n        cats['not_aligned'] =   { 'name': 'Not Aligned' }\n        config = {\n            'id': 'htseq_assignment_plot',\n            'title': 'HTSeq: Count Assignments',\n            'ylab': '# Reads',\n            'hide_zero_cats': False,\n            'cpswitch_counts_label': 'Number of Reads'\n        }\n        return bargraph.plot(self.htseq_data, cats, config)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_reports(self):\n\n    # Set up vars\n    self.picard_OxoGMetrics_data = dict()\n\n    # Go through logs and find Metrics\n    for f in self.find_log_files('picard/oxogmetrics', filehandles=True):\n        # We use lists to make sure that we don't overwrite when no data will be parsed\n        parsed_data = list()\n        sample_names = list()\n        s_files = list()\n        s_name = None\n        keys = None\n        for l in f['f']:\n            # New log starting\n            if 'CollectOxoGMetrics' in l and 'INPUT' in l:\n                s_name = None\n                keys = None\n                context_col = None\n\n                # Pull sample name from input\n                fn_search = re.search(r\"INPUT(?:=|\\s+)(\\[?[^\\s]+\\]?)\", l, flags=re.IGNORECASE)\n                if fn_search:\n                    s_name = os.path.basename(fn_search.group(1).strip('[]'))\n                    s_name = self.clean_s_name(s_name, f['root'])\n                    parsed_data.append(dict())\n                    sample_names.append(s_name)\n                    s_files.append(f)\n\n\n            if s_name is not None:\n                if 'CollectOxoGMetrics$CpcgMetrics' in l and '## METRICS CLASS' in l:\n                    keys = f['f'].readline().strip(\"\\n\").split(\"\\t\")\n                    context_col = keys.index('CONTEXT')\n                elif keys:\n                    vals = l.strip(\"\\n\").split(\"\\t\")\n                    if len(vals) == len(keys) and context_col is not None:\n                        context = vals[context_col]\n                        parsed_data[-1][context] = dict()\n                        for i, k in enumerate(keys):\n                            k = k.strip()\n                            try:\n                                parsed_data[-1][context][k] = float(vals[i])\n                            except ValueError:\n                                vals[i] = vals[i].strip()\n                                parsed_data[-1][context][k] = vals[i]\n                    else:\n                        s_name = None\n                        keys = None\n\n        # Remove empty dictionaries\n        for idx, s_name in enumerate(sample_names):\n            if len(parsed_data[idx]) > 0:\n                if s_name in self.picard_OxoGMetrics_data:\n                    log.debug(\"Duplicate sample name found in {}! Overwriting: {}\".format(s_files[idx], s_name))\n                self.add_data_source(s_files[idx], s_name, section='OxoGMetrics')\n                self.picard_OxoGMetrics_data[s_name] = parsed_data[idx]\n\n\n    # Filter to strip out ignored sample names\n    self.picard_OxoGMetrics_data = self.ignore_samples(self.picard_OxoGMetrics_data)\n\n    if len(self.picard_OxoGMetrics_data) > 0:\n\n        # Write parsed data to a file\n        # Collapse into 2D structure with sample_context keys\n        print_data = {\n            '{}_{}'.format(s, c):v\n            for s in self.picard_OxoGMetrics_data.keys()\n            for c,v in self.picard_OxoGMetrics_data[s].items()\n        }\n        self.write_data_file(print_data, 'multiqc_picard_OxoGMetrics')\n\n        # Add to general stats table\n        data = dict()\n        for s_name in self.picard_OxoGMetrics_data:\n            data[s_name] = dict()\n            try:\n                data[s_name]['CCG_OXIDATION_ERROR_RATE'] = self.picard_OxoGMetrics_data[s_name]['CCG']['OXIDATION_ERROR_RATE']\n            except KeyError:\n                log.warn(\"Couldn't find picard CCG oxidation error rate for {}\".format(s_name))\n\n        self.general_stats_headers['CCG_OXIDATION_ERROR_RATE'] = {\n            'title': 'CCG Oxidation',\n            'description': 'CCG-CAG Oxidation Error Rate',\n            'max': 1,\n            'min': 0,\n            'suffix': '%',\n            'format': '{:,.0f}',\n            'scale': 'RdYlGn-rev',\n            'modify': lambda x: self.multiply_hundred(x)\n        }\n        for s_name in data:\n            if s_name not in self.general_stats_data:\n                self.general_stats_data[s_name] = dict()\n            self.general_stats_data[s_name].update( data[s_name] )\n\n    # Return the number of detected samples to the parent module\n    return len(self.picard_OxoGMetrics_data)", "response": "Find Picard OxoGMetrics reports and parse their data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_reports(self):\n\n    # Set up vars\n    self.picard_rrbs_metrics = dict()\n\n    # Go through logs and find Metrics\n    for f in self.find_log_files('picard/rrbs_metrics', filehandles=True):\n        parsed_data = dict()\n        s_name = None\n        keys = None\n        for l in f['f']:\n            # New log starting\n            if 'CollectRrbsMetrics' in l and 'INPUT' in l:\n                s_name = None\n                keys = None\n                # Pull sample name from input\n                fn_search = re.search(r\"INPUT(?:=|\\s+)(\\[?[^\\s]+\\]?)\", l, flags=re.IGNORECASE)\n                if fn_search:\n                    s_name = os.path.basename(fn_search.group(1).strip('[]'))\n                    s_name = self.clean_s_name(s_name, f['root'])\n                    parsed_data[s_name] = dict()\n\n            if s_name is not None:\n                if 'RrbsSummaryMetrics' in l and '## METRICS CLASS' in l:\n                    keys = f['f'].readline().strip(\"\\n\").split(\"\\t\")\n                elif keys:\n                    vals = l.strip(\"\\n\").split(\"\\t\")\n                    if len(vals) == len(keys):\n                        for i, k in enumerate(keys):\n                            try:\n                                parsed_data[s_name][k] = float(vals[i])\n                            except ValueError:\n                                parsed_data[s_name][k] = vals[i]\n                    else:\n                        s_name = None\n                        keys = None\n\n        # Remove empty dictionaries\n        for s_name in list(parsed_data.keys()):\n            if len(parsed_data[s_name]) == 0:\n                parsed_data.pop(s_name, None)\n\n        # Collect parsed data\n        for s_name in parsed_data.keys():\n            if s_name in self.picard_rrbs_metrics:\n                log.debug(\"Duplicate sample name found in {}! Overwriting: {}\".format(f['fn'], s_name))\n            self.add_data_source(f, s_name, section='RrbsSummaryMetrics')\n            self.picard_rrbs_metrics[s_name] = parsed_data[s_name]\n\n    # Filter to strip out ignored sample names\n    self.picard_rrbs_metrics = self.ignore_samples(self.picard_rrbs_metrics)\n\n    if len(self.picard_rrbs_metrics) > 0:\n\n        # Write parsed data to a file\n        self.write_data_file(self.picard_rrbs_metrics, 'multiqc_picard_RrbsSummaryMetrics')\n\n        # Add to general stats table\n        self.general_stats_headers['PCT_CPG_BASES_CONVERTED'] = {\n            'title': '% CpG Methylated',\n            'description': 'Percentage of times a CpG cytosine was converted',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'format': '{:,.0f}',\n            'scale': 'RdYlGn-rev',\n            'modify': lambda x: 100 - self.multiply_hundred(x)\n        }\n        self.general_stats_headers['PCT_NON_CPG_BASES_CONVERTED'] = {\n            'title': '% Non-CpG Methylated',\n            'description': 'Percentage of times a non-CpG cytosine was converted',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'format': '{:,.0f}',\n            'scale': 'RdYlGn',\n            'modify': lambda x: 100 - self.multiply_hundred(x)\n        }\n        self.general_stats_headers['MEDIAN_CPG_COVERAGE'] = {\n            'title': 'Median CpG Cov',\n            'description': 'Median coverage of CpG sites',\n            'min': 0\n        }\n        for s_name in self.picard_rrbs_metrics:\n            if s_name not in self.general_stats_data:\n                self.general_stats_data[s_name] = dict()\n            self.general_stats_data[s_name].update( self.picard_rrbs_metrics[s_name] )\n\n\n\n        # Make the bar plot of converted bases\n        pdata_cpg = dict()\n        pdata_noncpg = dict()\n        for s_name in self.picard_rrbs_metrics.keys():\n            pdata_cpg[s_name] = dict()\n            pdata_cpg[s_name]['converted'] = self.picard_rrbs_metrics[s_name]['CPG_BASES_CONVERTED']\n            pdata_cpg[s_name]['not_converted'] = self.picard_rrbs_metrics[s_name]['CPG_BASES_SEEN'] - self.picard_rrbs_metrics[s_name]['CPG_BASES_CONVERTED']\n            pdata_noncpg[s_name] = dict()\n            pdata_noncpg[s_name]['converted'] = self.picard_rrbs_metrics[s_name]['NON_CPG_BASES']\n            pdata_noncpg[s_name]['not_converted'] = self.picard_rrbs_metrics[s_name]['NON_CPG_BASES'] - self.picard_rrbs_metrics[s_name]['NON_CPG_CONVERTED_BASES']\n\n        keys = OrderedDict()\n        keys['not_converted'] = {'name': 'Unconverted Bases (Methylated)'}\n        keys['converted'] = {'name': 'Converted Bases (Unmethylated)'}\n\n        # Config for the plot\n        pconfig = {\n            'id': 'picard_rrbs_converted_bases_plot',\n            'title': 'Picard: Converted Bases',\n            'ylab': '# CpG Bases',\n            'cpswitch_counts_label': 'Number of Bases',\n            'data_labels': [\n                {'name': 'CpG', 'ylab': '# CpG Bases'},\n                {'name': 'Non-CpG', 'ylab': '# Non-CpG Bases'}\n            ]\n        }\n\n        self.add_section (\n            name = 'RRBS Converted Bases',\n            anchor = 'picard-rrbssummary-convertedbases',\n            plot = bargraph.plot([pdata_cpg, pdata_noncpg], [keys, keys], pconfig)\n        )\n\n\n        # Make the bar plot of processed reads\n        pdata = dict()\n        for s_name in self.picard_rrbs_metrics.keys():\n            pdata[s_name] = dict()\n            pdata[s_name]['with_no_cpg'] = self.picard_rrbs_metrics[s_name]['READS_WITH_NO_CPG']\n            pdata[s_name]['ignored_short'] = self.picard_rrbs_metrics[s_name]['READS_IGNORED_SHORT']\n            pdata[s_name]['ignored_mismatches'] = self.picard_rrbs_metrics[s_name]['READS_IGNORED_MISMATCHES']\n            pdata[s_name]['not_ignored'] = (\n                self.picard_rrbs_metrics[s_name]['READS_ALIGNED'] -\n                pdata[s_name]['with_no_cpg'] -\n                pdata[s_name]['ignored_short'] -\n                pdata[s_name]['ignored_mismatches']\n            )\n\n        keys = OrderedDict()\n        keys['not_ignored'] = {'name': 'Utilised reads'}\n        keys['with_no_cpg'] = {'name': 'Ignored (no CpG sites)'}\n        keys['ignored_short'] = {'name': 'Ignored (too short)'}\n        keys['ignored_mismatches'] = {'name': 'Ignored (exceeded mismatch threshold)'}\n\n        # Config for the plot\n        pconfig = {\n            'id': 'picard_rrbs_ignored_reads_plot',\n            'title': 'Picard: RRBS Read Counts',\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads'\n        }\n\n        self.add_section (\n            name = 'RRBS Read Counts',\n            anchor = 'picard-rrbssummary-readcounts',\n            plot = bargraph.plot(pdata, keys, pconfig)\n        )\n\n    # Return the number of detected samples to the parent module\n    return len(self.picard_rrbs_metrics)", "response": "Find Picard RrbsSummaryMetrics reports and parse their data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_selfsm(self, f):\n\t\t#create a dictionary to populate from this sample's file\n\t\tparsed_data = dict()\n\t\t# set a empty variable which denotes if the headers have been read\n\t\theaders = None\n\t\t# for each line in the file\n\t\tfor l in f['f'].splitlines():\n\t\t\t# split the line on tab\n\t\t\ts = l.split(\"\\t\")\n\t\t\t# if we haven't already read the header line\n\t\t\tif headers is None:\n\t\t\t\t# assign this list to headers variable\n\t\t\t\theaders = s\n\t\t\t# for all rows after the first\n\t\t\telse:\n\t\t\t\t# clean the sample name (first column) and assign to s_name\n\t\t\t\ts_name = self.clean_s_name(s[0], f['root'])\n\t\t\t\t# create a dictionary entry with the first column as a key (sample name) and empty dictionary as a value\n\t\t\t\tparsed_data[s_name] = {}\n\t\t\t\t# for each item in list of items in the row\n\t\t\t\tfor i, v in enumerate(s):\n\t\t\t\t\t# if it's not the first element (if it's not the name)\n\t\t\t\t\tif i != 0:\n\t\t\t\t\t\t# see if CHIP is in the column header and the value is not NA\n\t\t\t\t\t\tif \"CHIP\" in [headers[i]] and v != \"NA\":\n\t\t\t\t\t\t\t# set hide_chip_columns = False so they are not hidden\n\t\t\t\t\t\t\tself.hide_chip_columns=False\n\t\t\t\t\t\t# try and convert the value into a float\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t# and add to the dictionary the key as the corrsponding item from the header and the value from the list\n\t\t\t\t\t\t\tparsed_data[s_name][headers[i]] = float(v)\n\t\t\t\t\t\t#if can't convert to float...\n\t\t\t\t\t\texcept ValueError:\n\t\t\t\t\t\t\t# add to the dictionary the key as the corrsponding item from the header and the value from the list\n\t\t\t\t\t\t\tparsed_data[s_name][headers[i]] = v\n\n\t\t# else return the dictionary\n\t\treturn parsed_data", "response": "Go through the selfSM file and create a dictionary to populate from this sample s file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef verifybamid_general_stats_table(self):\n\n\t\t# create a dictionary to hold the columns to add to the general stats table\n\t\theaders = OrderedDict()\n\t\t# available columns are:\n\t\t#SEQ_ID RG  CHIP_ID #SNPS   #READS  AVG_DP  FREEMIX FREELK1 FREELK0 FREE_RH FREE_RA CHIPMIX CHIPLK1 CHIPLK0 CHIP_RH CHIP_RA DPREF   RDPHET  RDPALT\n\t\t#see https://genome.sph.umich.edu/wiki/VerifyBamID#Interpreting_output_files\n\n\t\t# add the CHIPMIX column if we have data\n\t\tif not self.hide_chip_columns:\n\t\t\theaders['CHIPMIX'] = dict(self.col_config_defaults, **{\n\t\t\t\t'title': 'Contamination (S+A)',\n\t\t\t\t'description': 'VerifyBamID: CHIPMIX -   Sequence+array estimate of contamination (NA if the external genotype is unavailable)'\n\t\t\t})\n\n\t\t# add the FREEMIX column. set the title and description\n\t\theaders['FREEMIX'] = dict(self.col_config_defaults, **{\n\t\t\t'title': 'Contamination (S)',\n\t\t\t'description': 'VerifyBamID: FREEMIX -   Sequence-only estimate of contamination.',\n\t\t})\n\n\t\t# pass the data dictionary and header dictionary to function to add to table.\n\t\tself.general_stats_addcols(self.verifybamid_data, headers)", "response": "This function will take the percentage of contamination from all the parsed *. SELFSM files and add it to the basic stats table at the top of the report"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef verifybamid_table(self):\n\n\t\t# create an ordered dictionary to preserve the order of columns\n\t\theaders = OrderedDict()\n\t\t# add each column and the title and description (taken from verifyBAMID website)\n\t\theaders['RG'] = {\n\t\t\t'title': 'Read Group',\n\t\t\t'description': 'ReadGroup ID of sequenced lane.',\n\t\t\t'hidden': all( [ s['RG'] == 'ALL' for s in self.verifybamid_data.values() ] )\n\t\t}\n\t\tif not self.hide_chip_columns:\n\t\t\theaders['CHIP_ID'] = {\n\t\t\t\t'title': 'Chip ID',\n\t\t\t\t'description': 'ReadGroup ID of sequenced lane.'\n\t\t\t}\n\t\theaders['#SNPS'] = {\n\t\t\t'title': 'SNPS',\n\t\t\t'description': '# SNPs passing the criteria from the VCF file',\n\t\t\t'format': '{:,.0f}',\n\t\t\t'min': 0,\n\t\t\t'scale': 'BuPu'\n\t\t}\n\t\theaders['#READS'] = {\n\t\t\t'title': '{} Reads'.format(config.read_count_prefix),\n\t\t\t'description': 'Number of reads loaded from the BAM file ({})'.format(config.read_count_desc),\n\t\t\t'format': '{:,.1f}',\n\t\t\t'modify': lambda x: x * config.read_count_multiplier if x != \"NA\" else x,\n\t\t\t'shared_key': 'read_count',\n\t\t\t'min': 0,\n\t\t\t'scale': 'GnBu'\n\t\t}\n\t\theaders['AVG_DP'] = {\n\t\t\t'title': 'Average Depth',\n\t\t\t'description': 'Average sequencing depth at the sites in the VCF file',\n\t\t\t'suffix': ' X',\n\t\t\t'min': 0,\n\t\t\t'scale': 'YlGn'\n\t\t}\n\t\t# use default columns\n\t\theaders['FREEMIX'] = dict(self.col_config_defaults, **{\n\t\t\t'title': 'Contamination (Seq)',\n\t\t\t'description': 'VerifyBamID: FREEMIX -   Sequence-only estimate of contamination.',\n\t\t})\n\t\theaders['FREELK1'] = {\n\t\t\t'title': 'FREEELK1',\n\t\t\t'format': '{:,.0f}',\n\t\t\t'description': 'Maximum log-likelihood of the sequence reads given estimated contamination under sequence-only method',\n\t\t\t'min': 0,\n\t\t\t'scale': 'RdYlGn'\n\t\t}\n\t\theaders['FREELK0'] = {\n\t\t\t'title': 'FREELK0',\n\t\t\t'format': '{:,.0f}',\n\t\t\t'description': 'Log-likelihood of the sequence reads given no contamination under sequence-only method',\n\t\t\t'min': 0,\n\t\t\t'scale': 'RdYlGn'\n\t\t}\n\t\theaders['FREE_RH'] = {\n\t\t\t'title': 'FREE_RH',\n\t\t\t'description': 'Estimated reference bias parameter Pr(refBase|HET) (when --free-refBias or --free-full is used)',\n\t\t\t'hidden': all( [ s['FREE_RH'] == 'NA' for s in self.verifybamid_data.values() ] ),\n\t\t}\n\t\theaders['FREE_RA'] = {\n\t\t\t'title': 'FREE_RA',\n\t\t\t'description': 'Estimated reference bias parameter Pr(refBase|HOMALT) (when --free-refBias or --free-full is used)',\n\t\t\t'hidden': all( [ s['FREE_RA'] == 'NA' for s in self.verifybamid_data.values() ] ),\n\t\t}\n\n\t\t# Only print Chip columns to the report if we have data\n\t\tif not self.hide_chip_columns:\n\t\t\theaders['CHIPMIX'] = dict(self.col_config_defaults, **{\n\t\t\t\t'title': 'Contamination S+A',\n\t\t\t\t'description': 'VerifyBamID: CHIPMIX -   Sequence+array estimate of contamination (NA if the external genotype is unavailable)'\n\t\t\t})\n\t\t\theaders['CHIPLK1'] = {\n\t\t\t\t'title': 'CHIPLK1',\n\t\t\t\t'description': 'Maximum log-likelihood of the sequence reads given estimated contamination under sequence+array method (NA if the external genotypes are unavailable)'\n\t\t\t}\n\t\t\theaders['CHIPLK0'] = {\n\t\t\t\t'title': 'CHIPLK0',\n\t\t\t\t'description': ' Log-likelihood of the sequence reads given no contamination under sequence+array method (NA if the external genotypes are unavailable)'\n\t\t\t\t}\n\t\t\theaders['CHIP_RH'] = {\n\t\t\t\t'title': 'CHIP_RH',\n\t\t\t\t'description': 'Estimated reference bias parameter Pr(refBase|HET) (when --chip-refBias or --chip-full is used)'\n\t\t\t}\n\t\t\theaders['CHIP_RA'] = {\n\t\t\t\t'title': 'CHIP_RA',\n\t\t\t\t'description': 'Estimated reference bias parameter Pr(refBase|HOMALT) (when --chip-refBias or --chip-full is used)'\n\t\t\t}\n\n\t\theaders['DPREF'] = {\n\t\t\t'title': 'DPREF',\n\t\t\t'description': 'Depth (Coverage) of HomRef site (based on the genotypes of (SELF_SM/BEST_SM), passing mapQ, baseQual, maxDepth thresholds.',\n\t\t\t'hidden': all( [ s['DPREF'] == 'NA' for s in self.verifybamid_data.values() ] ),\n\t\t}\n\t\theaders['RDPHET'] = {\n\t\t\t'title': 'RDPHET',\n\t\t\t'description': 'DPHET/DPREF, Relative depth to HomRef site at Heterozygous site.',\n\t\t\t'hidden': all( [ s['RDPHET'] == 'NA' for s in self.verifybamid_data.values() ] ),\n\t\t}\n\t\theaders['RDPALT'] = {\n\t\t\t'title': 'RDPALT',\n\t\t\t'description': 'DPHET/DPREF, Relative depth to HomRef site at HomAlt site.',\n\t\t\t'hidden': all( [ s['RDPALT'] == 'NA' for s in self.verifybamid_data.values() ] ),\n\t\t}\n\n\t\ttconfig = {\n\t\t\t'namespace': 'VerifyBAMID',\n\t\t\t'id': 'verifybamid-results',\n\t\t}\n\n\t\t# send the plot to add section function with data dict and headers\n\t\tself.add_section (\n\t\t\tanchor = 'verifybamid-table',\n\t\t\tdescription = 'The following values provide estimates of sample contamination. Click help for more information.',\n\t\t\thelptext = '''\n\t\t\t**Please note that `FREEMIX` is named _Contamination (Seq)_ and `CHIPMIX`\n\t\t\tis named _Contamination (S+A)_ in this MultiQC report.**\n\n\t\t\tVerifyBamID provides a series of information that is informative to determine\n\t\t\twhether the sample is possibly contaminated or swapped, but there is no single\n\t\t\tcriteria that works for every circumstances. There are a few unmodeled factor\n\t\t\tin the estimation of `[SELF-IBD]/[BEST-IBD]` and `[%MIX]`, so please note that the\n\t\t\tMLE estimation may not always exactly match to the true amount of contamination.\n\t\t\tHere we provide a guideline to flag potentially contaminated/swapped samples:\n\n\t\t\t* Each sample or lane can be checked in this way.\n\t\t\t  When `[CHIPMIX] >> 0.02` and/or `[FREEMIX] >> 0.02`, meaning 2% or more of\n\t\t\t  non-reference bases are observed in reference sites, we recommend to examine\n\t\t\t  the data more carefully for the possibility of contamination.\n\t\t\t* We recommend to check each lane for the possibility of sample swaps.\n\t\t\t  When `[CHIPMIX] ~ 1` AND `[FREEMIX] ~ 0`, then it is possible that the sample\n\t\t\t  is swapped with another sample. When `[CHIPMIX] ~ 0` in `.bestSM` file,\n\t\t\t  `[CHIP_ID]` might be actually the swapped sample. Otherwise, the swapped\n\t\t\t  sample may not exist in the genotype data you have compared.\n\t\t\t* When genotype data is not available but allele-frequency-based estimates of\n\t\t\t  `[FREEMIX] >= 0.03` and `[FREELK1]-[FREELK0]` is large, then it is possible\n\t\t\t  that the sample is contaminated with other sample. We recommend to use\n\t\t\t  per-sample data rather than per-lane data for checking this for low coverage\n\t\t\t  data, because the inference will be more confident when there are large number\n\t\t\t  of bases with depth 2 or higher.\n\n\t\t\t_Copied from the [VerifyBAMID documentation](https://genome.sph.umich.edu/wiki/VerifyBamID) - see the link for more details._\n\t\t\t''',\n\t\t\tplot = table.plot(self.verifybamid_data, headers, tconfig)\n\t\t)", "response": "Create a table with all the columns from verify BAM ID"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a GATK report and return a dictionary of tables.", "response": "def parse_report(self, lines, table_names):\n        \"\"\" Parse a GATK report https://software.broadinstitute.org/gatk/documentation/article.php?id=1244\n\n        Only GATTable entries are parsed.  Tables are returned as a dict of tables.\n        Each table is a dict of arrays, where names correspond to column names, and arrays\n        correspond to column values.\n\n        Args:\n            lines (file handle): an iterable over the lines of a GATK report.\n            table_names (dict): a dict with keys that are GATK report table names\n                (e.g. \"#:GATKTable:Quantized:Quality quantization map\"), and values that are the\n                keys in the returned dict.\n\n        Returns:\n            {\n                table_1:\n                    {\n                        col_1: [ val_1, val_2, ... ]\n                        col_2: [ val_1, val_2, ... ]\n                        ...\n                    }\n                table_2:\n                    ...\n            }\n        \"\"\"\n\n        report = dict()\n\n        lines = (l for l in lines)\n        for line in lines:\n            line = line.rstrip()\n            if line in table_names.keys():\n                report[table_names[line]] = self.parse_gatk_report_table(lines)\n        return report"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the log file for HISAT2 and return a dictionary of the parsed data.", "response": "def parse_hisat2_logs(self, f):\n        \"\"\"\n        Parse statistics generated by HISAT2 >= v2.1.0 that has been run\n        with the --new-summary option. Older versions or logs from runs without\n        that option are identical to that from bowtie2 and will be parsed\n        by that module.\n        \"\"\"\n\n        # Regexes\n        regexes = {\n            'unpaired_total': r\"Total(?: unpaired)? reads: (\\d+)\",\n            'unpaired_aligned_none': r\"Aligned 0 times?: (\\d+) \\([\\d\\.]+%\\)\",\n            'unpaired_aligned_one': r\"Aligned 1 time: (\\d+) \\([\\d\\.]+%\\)\",\n            'unpaired_aligned_multi': r\"Aligned >1 times: (\\d+) \\([\\d\\.]+%\\)\",\n    \t\t'paired_total': r\"Total pairs: (\\d+)\",\n    \t\t'paired_aligned_none': r\"Aligned concordantly or discordantly 0 time: (\\d+) \\([\\d\\.]+%\\)\",\n    \t\t'paired_aligned_one': r\"Aligned concordantly 1 time: (\\d+) \\([\\d\\.]+%\\)\",\n    \t\t'paired_aligned_multi': r\"Aligned concordantly >1 times: (\\d+) \\([\\d\\.]+%\\)\",\n    \t\t'paired_aligned_discord_one': r\"Aligned discordantly 1 time: (\\d+) \\([\\d\\.]+%\\)\",\n        }\n\n        # Go through log file line by line\n        s_name = f['s_name']\n        parsed_data = {}\n\n        for l in f['f']:\n\n            # Attempt in vain to find original hisat2 command, logged by another program\n            hscmd = re.search(r\"hisat2 .+ -[1U] ([^\\s,]+)\", l)\n            if hscmd:\n                s_name = self.clean_s_name(hscmd.group(1), f['root'])\n                log.debug(\"Found a HISAT2 command, updating sample name to '{}'\".format(s_name))\n\n            # Run through all regexes\n            for k, r in regexes.items():\n                match = re.search(r, l)\n                if match:\n                    parsed_data[k] = int(match.group(1))\n\n            # Overall alignment rate\n            overall = re.search(r\"Overall alignment rate: ([\\d\\.]+)%\", l)\n            if overall:\n                parsed_data['overall_alignment_rate'] = float(overall.group(1))\n\n                # Save parsed data\n                if s_name in self.hisat2_data:\n                    log.debug(\"Duplicate sample name found! Overwriting: {}\".format(s_name))\n                self.add_data_source(f, s_name)\n                self.hisat2_data[s_name] = parsed_data\n\n                # Reset in case we find more in this log file\n                s_name = f['s_name']\n                parsed_data = {}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hisat2_general_stats_table(self):\n\n        headers = OrderedDict()\n        headers['overall_alignment_rate'] = {\n            'title': '% Aligned',\n            'description': 'overall alignment rate',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'YlGn'\n        }\n        self.general_stats_addcols(self.hisat2_data, headers)", "response": "Add the parsed stats from the HISAT2 report to the basic stats table at the top of the report"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake the HTML to plot the alignment rates for the current HISAT2 record.", "response": "def hisat2_alignment_plot (self):\n        \"\"\" Make the HighCharts HTML to plot the alignment rates \"\"\"\n\n        # Split the data into SE and PE\n        sedata = {}\n        pedata = {}\n        for s_name, data in self.hisat2_data.items():\n            if 'paired_total' in data:\n                # Save half 'pairs' of mate counts\n                m_keys = ['unpaired_total', 'unpaired_aligned_none', 'unpaired_aligned_one', 'unpaired_aligned_multi']\n                for k in m_keys:\n                    if k in data:\n                        data[k] = float(data[k]) / 2.0\n                pedata[s_name] = data\n            else:\n                sedata[s_name] = data\n\n        # Two plots, don't mix SE with PE\n        if len(sedata) > 0:\n            sekeys = OrderedDict()\n            sekeys['unpaired_aligned_one'] = { 'color': '#20568f', 'name': 'SE mapped uniquely' }\n            sekeys['unpaired_aligned_multi'] = { 'color': '#f7a35c', 'name': 'SE multimapped' }\n            sekeys['unpaired_aligned_none'] = { 'color': '#981919', 'name': 'SE not aligned' }\n            pconfig = {\n                'id': 'hisat2_se_plot',\n                'title': 'HISAT2: SE Alignment Scores',\n                'ylab': '# Reads',\n                'cpswitch_counts_label': 'Number of Reads'\n            }\n            self.add_section(\n                plot = bargraph.plot(sedata, sekeys, pconfig)\n            )\n\n        if len(pedata) > 0:\n            pekeys = OrderedDict()\n            pekeys['paired_aligned_one'] = { 'color': '#20568f', 'name': 'PE mapped uniquely' }\n            pekeys['paired_aligned_discord_one'] = { 'color': '#5c94ca', 'name': 'PE mapped discordantly uniquely' }\n            pekeys['unpaired_aligned_one'] = { 'color': '#95ceff', 'name': 'PE one mate mapped uniquely' }\n            pekeys['paired_aligned_multi'] = { 'color': '#f7a35c', 'name': 'PE multimapped' }\n            pekeys['unpaired_aligned_multi'] = { 'color': '#ffeb75', 'name': 'PE one mate multimapped' }\n            pekeys['unpaired_aligned_none'] = { 'color': '#981919', 'name': 'PE neither mate aligned' }\n            pconfig = {\n                'id': 'hisat2_pe_plot',\n                'title': 'HISAT2: PE Alignment Scores',\n                'ylab': '# Reads',\n                'cpswitch_counts_label': 'Number of Reads'\n            }\n            self.add_section(\n                description = '<em>Please note that single mate alignment counts are halved to tally with pair counts properly.</em>',\n                plot = bargraph.plot(pedata, pekeys, pconfig)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds matches log files of interest.", "response": "def find_log_files(self, sp_key, filecontents=True, filehandles=False):\n        \"\"\"\n        Return matches log files of interest.\n        :param sp_key: Search pattern key specified in config\n        :param filehandles: Set to true to return a file handle instead of slurped file contents\n        :return: Yields a dict with filename (fn), root directory (root), cleaned sample name\n                 generated from the filename (s_name) and either the file contents or file handle\n                 for the current matched file (f).\n                 As yield is used, the results can be iterated over without loading all files at once\n        \"\"\"\n\n        # Pick up path filters if specified.\n        # Allows modules to be called multiple times with different sets of files\n        path_filters = getattr(self, 'mod_cust_config', {}).get('path_filters')\n        path_filters_exclude = getattr(self, 'mod_cust_config', {}).get('path_filters_exclude')\n\n        # Old, depreciated syntax support. Likely to be removed in a future version.\n        if isinstance(sp_key, dict):\n            report.files[self.name] = list()\n            for sf in report.searchfiles:\n                if report.search_file(sp_key, {'fn': sf[0], 'root': sf[1]}):\n                    report.files[self.name].append({'fn': sf[0], 'root': sf[1]})\n            sp_key = self.name\n            logwarn = \"Depreciation Warning: {} - Please use new style for find_log_files()\".format(self.name)\n            if len(report.files[self.name]) > 0:\n                logger.warn(logwarn)\n            else:\n                logger.debug(logwarn)\n        elif not isinstance(sp_key, str):\n            logger.warn(\"Did not understand find_log_files() search key\")\n            return\n\n        for f in report.files[sp_key]:\n            # Make a note of the filename so that we can report it if something crashes\n            report.last_found_file = os.path.join(f['root'], f['fn'])\n\n            # Filter out files based on exclusion patterns\n            if path_filters_exclude and len(path_filters_exclude) > 0:\n                exlusion_hits = (fnmatch.fnmatch(report.last_found_file, pfe) for pfe in path_filters_exclude)\n                if any(exlusion_hits):\n                    logger.debug(\"{} - Skipping '{}' as it matched the path_filters_exclude for '{}'\".format(sp_key, f['fn'], self.name))\n                    continue\n\n            # Filter out files based on inclusion patterns\n            if path_filters and len(path_filters) > 0:\n                inclusion_hits = (fnmatch.fnmatch(report.last_found_file, pf) for pf in path_filters)\n                if not any(inclusion_hits):\n                    logger.debug(\"{} - Skipping '{}' as it didn't match the path_filters for '{}'\".format(sp_key, f['fn'], self.name))\n                    continue\n                else:\n                    logger.debug(\"{} - Selecting '{}' as it matched the path_filters for '{}'\".format(sp_key, f['fn'], self.name))\n\n            # Make a sample name from the filename\n            f['s_name'] = self.clean_s_name(f['fn'], f['root'])\n            if filehandles or filecontents:\n                try:\n                    # Custom content module can now handle image files\n                    (ftype, encoding) = mimetypes.guess_type(os.path.join(f['root'], f['fn']))\n                    if ftype is not None and ftype.startswith('image'):\n                        with io.open (os.path.join(f['root'],f['fn']), \"rb\") as fh:\n                            # always return file handles\n                            f['f'] = fh\n                            yield f\n                    else:\n                        # Everything else - should be all text files\n                        with io.open (os.path.join(f['root'],f['fn']), \"r\", encoding='utf-8') as fh:\n                            if filehandles:\n                                f['f'] = fh\n                                yield f\n                            elif filecontents:\n                                f['f'] = fh.read()\n                                yield f\n                except (IOError, OSError, ValueError, UnicodeDecodeError) as e:\n                    if config.report_readerrors:\n                        logger.debug(\"Couldn't open filehandle when returning file: {}\\n{}\".format(f['fn'], e))\n                        f['f'] = None\n            else:\n                yield f"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a section to the module report output", "response": "def add_section(self, name=None, anchor=None, description='', comment='', helptext='', plot='', content='', autoformat=True, autoformat_type='markdown'):\n        \"\"\" Add a section to the module report output \"\"\"\n\n        # Default anchor\n        if anchor is None:\n            if name is not None:\n                nid = name.lower().strip().replace(' ','-')\n                anchor = '{}-{}'.format(self.anchor, nid)\n            else:\n                sl = len(self.sections) + 1\n                anchor = '{}-section-{}'.format(self.anchor, sl)\n\n        # Skip if user has a config to remove this module section\n        if anchor in config.remove_sections:\n            logger.debug(\"Skipping section '{}' because specified in user config\".format(anchor))\n            return\n\n        # Sanitise anchor ID and check for duplicates\n        anchor = report.save_htmlid(anchor)\n\n        # See if we have a user comment in the config\n        if anchor in config.section_comments:\n            comment = config.section_comments[anchor]\n\n        # Format the content\n        if autoformat:\n            if len(description) > 0:\n                description = textwrap.dedent(description)\n                if autoformat_type == 'markdown':\n                    description = markdown.markdown(description)\n            if len(comment) > 0:\n                comment = textwrap.dedent(comment)\n                if autoformat_type == 'markdown':\n                    comment = markdown.markdown(comment)\n            if len(helptext) > 0:\n                helptext = textwrap.dedent(helptext)\n                if autoformat_type == 'markdown':\n                    helptext = markdown.markdown(helptext)\n\n        # Strip excess whitespace\n        description = description.strip()\n        comment = comment.strip()\n        helptext = helptext.strip()\n\n        self.sections.append({\n            'name': name,\n            'anchor': anchor,\n            'description': description,\n            'comment': comment,\n            'helptext': helptext,\n            'plot': plot,\n            'content': content,\n            'print_section': any([ n is not None and len(n) > 0 for n in [description, comment, helptext, plot, content] ])\n        })"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ignore_samples(self, data):\n        try:\n            if isinstance(data, OrderedDict):\n                newdata = OrderedDict()\n            elif isinstance(data, dict):\n                newdata = dict()\n            else:\n                return data\n            for k,v in data.items():\n                # Match ignore glob patterns\n                glob_match = any( fnmatch.fnmatch(k, sn) for sn in config.sample_names_ignore )\n                re_match = any( re.match(sn, k) for sn in config.sample_names_ignore_re )\n                if not glob_match and not re_match:\n                    newdata[k] = v\n            return newdata\n        except (TypeError, AttributeError):\n            return data", "response": "Strip out samples which match sample_names_ignore"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds columns to the General Statistics variable.", "response": "def general_stats_addcols(self, data, headers=None, namespace=None):\n        \"\"\" Helper function to add to the General Statistics variable.\n        Adds to report.general_stats and does not return anything. Fills\n        in required config variables if not supplied.\n        :param data: A dict with the data. First key should be sample name,\n                     then the data key, then the data.\n        :param headers: Dict / OrderedDict with information for the headers,\n                        such as colour scales, min and max values etc.\n                        See docs/writing_python.md for more information.\n        :return: None\n        \"\"\"\n        if headers is None:\n            headers = {}\n        # Use the module namespace as the name if not supplied\n        if namespace is None:\n            namespace = self.name\n\n        # Guess the column headers from the data if not supplied\n        if headers is None or len(headers) == 0:\n            hs = set()\n            for d in data.values():\n                hs.update(d.keys())\n            hs = list(hs)\n            hs.sort()\n            headers = OrderedDict()\n            for k in hs:\n                headers[k] = dict()\n\n        # Add the module name to the description if not already done\n        keys = headers.keys()\n        for k in keys:\n            if 'namespace' not in headers[k]:\n                headers[k]['namespace'] = namespace\n            if 'description' not in headers[k]:\n                headers[k]['description'] = headers[k].get('title', k)\n\n        # Append to report.general_stats for later assembly into table\n        report.general_stats_data.append(data)\n        report.general_stats_headers.append(headers)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsaving raw data to a file in the report directory", "response": "def write_data_file(self, data, fn, sort_cols=False, data_format=None):\n        \"\"\" Saves raw data to a dictionary for downstream use, then redirects\n        to report.write_data_file() to create the file in the report directory \"\"\"\n        report.saved_raw_data[fn] = data\n        util_functions.write_data_file(data, fn, sort_cols, data_format)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeprecating function. Forwards to new location. Forwards to new location.", "response": "def plot_bargraph (self, data, cats=None, pconfig=None):\n        \"\"\" Depreciated function. Forwards to new location. \"\"\"\n        from multiqc.plots import bargraph\n        if pconfig is None:\n            pconfig = {}\n        return bargraph.plot(data, cats, pconfig)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeprecates function. Forwards to new location. Forwards to new location.", "response": "def plot_xy_data(self, data, pconfig=None):\n        \"\"\" Depreciated function. Forwards to new location. \"\"\"\n        from multiqc.plots import linegraph\n        if pconfig is None:\n            pconfig = {}\n        return linegraph.plot(data, pconfig)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_single_report(f):\n\n    parsed_data = OrderedDict()\n    for l in f.splitlines():\n        s = l.split(\"\\t\")\n        try:\n            parsed_data[s[0]] = int(s[2])\n        except (IndexError, ValueError):\n            pass\n    return parsed_data", "response": "Parse a samtools idxstats file into a dict"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_samtools_idxstats(self):\n\n        self.samtools_idxstats = dict()\n        for f in self.find_log_files('samtools/idxstats'):\n            parsed_data = parse_single_report(f['f'])\n            if len(parsed_data) > 0:\n                if f['s_name'] in self.samtools_idxstats:\n                    log.debug(\"Duplicate sample name found! Overwriting: {}\".format(f['s_name']))\n                self.add_data_source(f, section='idxstats')\n                self.samtools_idxstats[f['s_name']] = parsed_data\n\n        # Filter to strip out ignored sample names\n        self.samtools_idxstats = self.ignore_samples(self.samtools_idxstats)\n\n        if len(self.samtools_idxstats) > 0:\n\n            # Write parsed report data to a file (restructure first)\n            self.write_data_file(self.samtools_idxstats, 'multiqc_samtools_idxstats')\n\n            # Prep the data for the plots\n            keys = list()\n            pdata = dict()\n            pdata_norm = dict()\n            xy_counts = dict()\n            # Count the total mapped reads for every chromosome\n            chrs_mapped = defaultdict(lambda:0)\n            sample_mapped = defaultdict(lambda:0)\n            total_mapped = 0\n            # Cutoff, can be customised in config\n            cutoff = float(getattr(config, 'samtools_idxstats_fraction_cutoff', 0.001))\n            if cutoff != 0.001:\n                log.info('Setting idxstats cutoff to: {}%'.format(cutoff*100.0))\n            for s_name in self.samtools_idxstats:\n                for chrom in self.samtools_idxstats[s_name]:\n                    chrs_mapped[chrom] += self.samtools_idxstats[s_name][chrom]\n                    sample_mapped[s_name] += self.samtools_idxstats[s_name][chrom]\n                    total_mapped += self.samtools_idxstats[s_name][chrom]\n            req_reads = float(total_mapped)*cutoff\n            chr_always = getattr(config, 'samtools_idxstats_always', [])\n            if len(chr_always) > 0:\n                log.info('Trying to include these chromosomes in idxstats: {}'.format(', '.join(chr_always)))\n            chr_ignore = getattr(config, 'samtools_idxstats_ignore', [])\n            if len(chr_ignore) > 0:\n                log.info('Excluding these chromosomes from idxstats: {}'.format(', '.join(chr_ignore)))\n            xchr = getattr(config, 'samtools_idxstats_xchr', False)\n            if xchr:\n                log.info('Using \"{}\" as X chromosome name'.format(xchr))\n            ychr = getattr(config, 'samtools_idxstats_ychr', False)\n            if ychr:\n                log.info('Using \"{}\" as Y chromosome name'.format(ychr))\n            # Go through again and collect all of the keys that have enough counts\n            # Also get the X/Y counts if we find them\n            for s_name in self.samtools_idxstats:\n                x_count = False\n                y_count = False\n                for chrom in self.samtools_idxstats[s_name]:\n                    if float(chrs_mapped[chrom]) > req_reads or chrom in chr_always:\n                        if chrom not in chr_ignore and chrom not in keys:\n                            keys.append(chrom)\n                    # Collect X and Y counts if we have them\n                    mapped = self.samtools_idxstats[s_name][chrom]\n                    if xchr is not False :\n                        if str(xchr) == str(chrom):\n                            x_count = mapped\n                    else:\n                        if chrom.lower() == 'x' or chrom.lower() == 'chrx':\n                            x_count = mapped\n                    if ychr is not False :\n                        if str(ychr) == str(chrom):\n                            y_count = mapped\n                    else:\n                        if chrom.lower() == 'y' or chrom.lower() == 'chry':\n                            y_count = mapped\n                # Only save these counts if we have both x and y\n                if x_count and y_count:\n                    xy_counts[s_name] = {'x': x_count, 'y': y_count }\n            # Ok, one last time. We have the chromosomes that we want to plot,\n            # now collect the counts\n            for s_name in self.samtools_idxstats:\n                pdata[s_name] = OrderedDict()\n                pdata_norm[s_name] = OrderedDict()\n                for k in keys:\n                    try:\n                        pdata[s_name][k] = self.samtools_idxstats[s_name][k]\n                        pdata_norm[s_name][k] = float(self.samtools_idxstats[s_name][k]) / sample_mapped[s_name]\n                    except (KeyError, ZeroDivisionError):\n                        pdata[s_name][k] = 0\n                        pdata_norm[s_name][k] = 0\n\n            # X/Y ratio plot\n            if len(xy_counts) > 0:\n                xy_keys = OrderedDict()\n                xy_keys['x'] = { 'name': xchr if xchr else 'Chromosome X' }\n                xy_keys['y'] = { 'name': ychr if ychr else 'Chromosome Y' }\n                pconfig = {\n                    'id': 'samtools-idxstats-xy-plot',\n                    'title': 'Samtools idxstats: chrXY mapped reads',\n                    'ylab': 'Percent of X+Y Reads',\n                    'cpswitch_counts_label': 'Number of Reads',\n                    'cpswitch_percent_label': 'Percent of X+Y Reads',\n                    'cpswitch_c_active': False\n                }\n                self.add_section (\n                    name = 'XY counts',\n                    anchor = 'samtools-idxstats-xy-counts',\n                    plot = bargraph.plot(xy_counts, xy_keys, pconfig)\n                )\n\n\n            # Mapped reads per chr line plot\n            pconfig = {\n                'id': 'samtools-idxstats-mapped-reads-plot',\n                'title': 'Samtools idxstats: Mapped reads per contig',\n                'ylab': '# mapped reads',\n                'xlab': 'Chromosome Name',\n                'categories': True,\n                'tt_label': '<strong>{point.category}:</strong> {point.y:.2f}',\n                'data_labels': [\n                    {'name': 'Normalised Counts', 'ylab': 'Fraction of total count'},\n                    {'name': 'Counts', 'ylab': '# mapped reads'}\n                ]\n            }\n            self.add_section (\n                name = 'Mapped reads per contig',\n                anchor = 'samtools-idxstats',\n                description = 'The <code>samtools idxstats</code> tool counts the number of mapped reads per chromosome / contig. ' +\n                    'Chromosomes with &lt; {}% of the total aligned reads are omitted from this plot.'.format(cutoff*100),\n                plot = linegraph.plot([pdata_norm, pdata], pconfig)\n            )\n\n        # Return the number of logs that were found\n        return len(self.samtools_idxstats)", "response": "Find Samtools idxstats logs and parse their data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a given HiCExplorer log file from hicBuildMatrix.", "response": "def parse_logs(self, f):\n        \"\"\"Parse a given HiCExplorer log file from hicBuildMatrix.\"\"\"\n        data = {}\n        for l in f.splitlines():\n            # catch empty lines\n            if len(l) == 0:\n                continue\n            s = l.split(\"\\t\")\n            data_ = []\n            # catch lines with descriptive content: \"Of pairs used:\"\n            for i in s[1:]:\n                if len(i) == 0:\n                    continue\n                try:\n                    i.replace('(', '')\n                    i.replace(')', '')\n                    i.replace(',', '')\n\n                    data_.append(float(i))\n                except ValueError:\n                    data_.append(i)\n            if len(data_) == 0:\n                continue\n            if s[0].startswith('short range'):\n                s[0] = 'short range'\n            elif s[0].startswith('same fragment'):\n                s[0] = 'same fragment'\n            s[0] = s[0].capitalize()\n            data[s[0]] = data_\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate the general statistics for HiCExplorer.", "response": "def hicexplorer_basic_statistics(self):\n        \"\"\"Create the general statistics for HiCExplorer.\"\"\"\n        data = {}\n        for file in self.mod_data:\n            max_distance_key = 'Max rest. site distance'\n            total_pairs = self.mod_data[file]['Pairs considered'][0]\n            try:\n                self.mod_data[file][max_distance_key][0]\n            except KeyError:\n                max_distance_key = 'Max library insert size'\n            data_ = {\n                'Pairs considered': self.mod_data[file]['Pairs considered'][0],\n                'Pairs used': self.mod_data[file]['Pairs used'][0] / total_pairs,\n                'Mapped': self.mod_data[file]['One mate unmapped'][0] / total_pairs,\n                'Min rest. site distance': self.mod_data[file]['Min rest. site distance'][0],\n                max_distance_key: self.mod_data[file][max_distance_key][0],\n            }\n            data[self.mod_data[file]['File'][0]] = data_\n        headers = OrderedDict()\n        headers['Pairs considered'] = {\n            'title': '{} Pairs'.format(config.read_count_prefix),\n            'description': 'Total number of read pairs ({})'.format(config.read_count_desc),\n            'shared_key': 'read_count'\n        }\n        headers['Pairs used'] = {\n            'title': '% Used pairs',\n            'max': 100,\n            'min': 0,\n            'modify': lambda x: x * 100,\n            'suffix': '%'\n        }\n        headers['Mapped'] = {\n            'title': '% Mapped',\n            'max': 100,\n            'min': 0,\n            'modify': lambda x: (1 - x) * 100,\n            'scale': 'RdYlGn',\n            'suffix': '%'\n        }\n        headers['Min rest. site distance'] = {\n            'title': 'Min RE dist',\n            'description': 'Minimum restriction site distance (bp)',\n            'format': '{:.0f}',\n            'suffix': ' bp'\n        }\n        headers[max_distance_key] = {\n            'title': 'Max RE dist',\n            'description': max_distance_key + ' (bp)',\n            'format': '{:.0f}',\n            'suffix': ' bp'\n        }\n\n        self.general_stats_addcols(data, headers)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate the graphics containing information about the read quality.", "response": "def hicexplorer_create_plot(self, pKeyList, pTitle, pId):\n        \"\"\"Create the graphics containing information about the read quality.\"\"\"\n\n        keys = OrderedDict()\n\n        for i, key_ in enumerate(pKeyList):\n            keys[key_] = {'color': self.colors[i]}\n\n        data = {}\n\n        for data_ in self.mod_data:\n            data['{}'.format(self.mod_data[data_]['File'][0])] = {}\n            for key_ in pKeyList:\n                data['{}'.format(self.mod_data[data_]['File'][0])][key_] = self.mod_data[data_][key_][0]\n\n        config = {\n            'id': 'hicexplorer_' + pId + '_plot',\n            'title': pTitle,\n            'ylab': 'Number of Reads',\n            'cpswitch_counts_label': 'Number of Reads'\n        }\n\n        return bargraph.plot(data, keys, config)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the Clusterflow logs file and store the information in the self. clusterflow_commands dictionary.", "response": "def parse_clusterflow_logs(self, f):\n        \"\"\" Parse Clusterflow logs \"\"\"\n        module = None\n        job_id = None\n        pipeline_id = None\n        for l in f['f']:\n\n            # Get pipeline ID\n            module_r = re.match(r'Module:\\s+(.+)$', l)\n            if module_r:\n                module = module_r.group(1)\n            job_id_r = re.match(r'Job ID:\\s+(.+)$', l)\n            if job_id_r:\n                job_id = job_id_r.group(1)\n                if module is not None:\n                    pipeline_r = re.match(r\"(cf_.+)_\"+re.escape(module)+r\"_\\d+$\", job_id)\n                    if pipeline_r:\n                        pipeline_id = pipeline_r.group(1)\n\n            # Get commands that have been run\n            if l.startswith('###CFCMD'):\n                if pipeline_id is None:\n                    pipeline_id = 'unknown'\n                if pipeline_id not in self.clusterflow_commands.keys():\n                    self.clusterflow_commands[pipeline_id] = list()\n                self.clusterflow_commands[pipeline_id].append(l[8:])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking a table of the Cluster Flow commands.", "response": "def clusterflow_commands_table (self):\n        \"\"\" Make a table of the Cluster Flow commands \"\"\"\n\n        # I wrote this when I was tired. Sorry if it's incomprehensible.\n\n        desc = '''Every Cluster Flow run will have many different commands.\n            MultiQC splits these by whitespace, collects by the tool name\n            and shows the first command found. Any terms not found in <em>all</em> subsequent\n            calls are replaced with <code>[variable]</code>\n            <em>(typically input and ouput filenames)</em>. Each column is for one Cluster Flow run.'''\n\n        # Loop through pipelines\n        tool_cmds = OrderedDict()\n        headers = dict()\n        for pipeline_id, commands in self.clusterflow_commands.items():\n            headers[pipeline_id] = {'scale': False}\n            self.var_html = '<span style=\"background-color:#dedede; color:#999;\">[variable]</span>'\n            tool_cmd_parts = OrderedDict()\n            for cmd in commands:\n                s = cmd.split()\n                tool = self._guess_cmd_name(s)\n                if tool not in tool_cmd_parts.keys():\n                    tool_cmd_parts[tool] = list()\n                tool_cmd_parts[tool].append(s)\n\n\n            for tool, cmds in tool_cmd_parts.items():\n                cons_cmd = self._replace_variable_chunks(cmds)\n                # Try again with first two blocks if all variable\n                variable_count = cons_cmd.count(self.var_html)\n                if variable_count == len(cmds[0]) - 1 and len(cmds[0]) > 2:\n                    for subcmd in set([x[1] for x in cmds]):\n                        sub_cons_cmd = self._replace_variable_chunks([cmd for cmd in cmds if cmd[1] == subcmd])\n                        tool = \"{} {}\".format(tool, subcmd)\n                        if tool not in tool_cmds:\n                            tool_cmds[tool] = dict()\n                        tool_cmds[tool][pipeline_id] = '<code style=\"white-space:nowrap;\">{}</code>'.format(\" \".join(sub_cons_cmd) )\n                else:\n                    if tool not in tool_cmds:\n                        tool_cmds[tool] = dict()\n                    tool_cmds[tool][pipeline_id] = '<code style=\"white-space:nowrap;\">{}</code>'.format(\" \".join(cons_cmd) )\n\n        table_config = {\n            'namespace': 'Cluster Flow',\n            'id': 'clusterflow-commands-table',\n            'table_title': 'Cluster Flow Commands',\n            'col1_header': 'Tool',\n            'sortRows': False,\n            'no_beeswarm': True\n        }\n        self.add_section (\n            name = 'Commands',\n            anchor = 'clusterflow-commands',\n            description = desc,\n            plot = table.plot(tool_cmds, headers, table_config)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _replace_variable_chunks(self, cmds):\n\n        cons_cmd = None\n        while cons_cmd is None:\n            for cmd in cmds:\n                if cons_cmd is None:\n                    cons_cmd = cmd[:]\n                else:\n                    for idx, s in enumerate(cons_cmd):\n                        if s not in cmd:\n                            cons_cmd[idx] = self.var_html\n        return cons_cmd", "response": "Return a list of command chunks with any variable bits blanked out."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _guess_cmd_name(self, cmd):\n\n        # zcat to bowtie\n        if cmd[0] == 'zcat' and 'bowtie' in cmd:\n            return 'bowtie'\n        # samtools\n        if cmd[0] == 'samtools':\n            return ' '.join(cmd[0:2])\n        # java (eg. picard)\n        if cmd[0] == 'java':\n            jars = [s for s in cmd if '.jar' in s]\n            return os.path.basename(jars[0].replace('.jar', ''))\n        return cmd[0]", "response": "Try to guess the command name from the command line."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_clusterflow_runfiles(self, f):\n\n        data = dict()\n        in_comment = False\n        seen_pipeline = False\n        cf_file = False\n        for l in f['f']:\n            l = l.rstrip()\n            # Check that this is from Cluster Flow\n            if 'Cluster Flow' in l:\n                cf_file = True\n            # Header\n            if l.startswith('Pipeline: '):\n                data['pipeline_name'] = l[10:]\n            if l.startswith('Pipeline ID: '):\n                data['pipeline_id'] = l[13:]\n            if l.startswith('Created at '):\n                data['pipeline_start'] = l[11:]\n            # Config settings\n            if l.startswith('@'):\n                s = l.split(None, 1)\n                key = s[0].replace('@', '').strip()\n                try:\n                    data[key] = \"\\t\".join(s[1:])\n                except IndexError:\n                    data[key] = True\n            # Comments\n            if l.startswith('/*'):\n                in_comment = True\n            if l.startswith('*/'):\n                in_comment = False\n            if in_comment:\n                if 'comment' not in data:\n                    data['comment'] = ''\n                data['comment'] += l+\"\\n\"\n            # Pipeline steps\n            if l.strip().startswith('#'):\n                if 'pipeline_steps' not in data:\n                    data['pipeline_steps'] = []\n                data['pipeline_steps'].append(l)\n                seen_pipeline = True\n            # Step output files\n            elif seen_pipeline:\n                s = l.split(\"\\t\")\n                if len(s) > 1:\n                    if 'files' not in data:\n                        data['files'] = OrderedDict()\n                    if s[0] not in data['files']:\n                        data['files'][s[0]] = []\n                    data['files'][s[0]].append(s[1:])\n        # Parse the start date\n        dt = None\n        if 'pipeline_id' in data:\n            s = data['pipeline_id'].split('_')\n            dt = datetime.datetime.fromtimestamp(int(s[-1]))\n        elif 'pipeline_start' in data:\n            dt_r = re.match(r'(\\d{2}):(\\d{2}), (\\d{2})-(\\d{2})-(\\d{4})', data['pipeline_start'])\n            if dt_r:\n                dt = datetime.datetime(\n                    int(dt_r.group(5)), # year\n                    int(dt_r.group(4)), # month\n                    int(dt_r.group(3)), # day\n                    int(dt_r.group(1)), # hour\n                    int(dt_r.group(2))  # minute\n                )\n\n        # Not a Cluster Flow file (eg. Nextflow .run file)\n        if not cf_file:\n            return None\n\n        if dt is not None:\n            data['pipeline_start_dateparts'] = {\n                'year':        dt.year,\n                'month':       dt.month,\n                'day':         dt.day,\n                'hour':        dt.hour,\n                'minute':      dt.minute,\n                'second':      dt.second,\n                'microsecond': dt.microsecond,\n                'timestamp':   time.mktime(dt.timetuple())\n            }\n        # Cluster Flow v0.4 and before did not print the pipeline ID in run files\n        # Try to guess - will be wrong as no microsecond info, but hopefully unique\n        # and reproducible for other run files\n        if 'pipeline_id' not in data:\n            if 'pipeline_name' in data and 'pipeline_start_dateparts' in data:\n                log.debug('Trying to guess pipeline ID for file \"{}\"'.format(f['fn']))\n                data['pipeline_id'] = 'cf_{}_{}'.format(data['pipeline_name'], data['pipeline_start_dateparts']['timestamp'])\n        return data", "response": "Parse run files generated by Cluster Flow. Currently gets pipeline IDs and associated steps."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clusterflow_pipelines_section(self):\n        data = dict()\n        pids_guessed = ''\n        for f,d in self.clusterflow_runfiles.items():\n            pid = d.get('pipeline_id', 'unknown')\n            if d.get('pipeline_id_guess', False) is True:\n                pid += '*'\n                pids_guessed = ' Project IDs with an asterisk may be inaccurate.'\n            # Count the number of files going into the first module\n            num_starting_files = 0\n            for step_name, files in d.get('files',{}).items():\n                if step_name.startswith('start'):\n                    num_starting_files += len(files)\n            # Reformat the date so that column sorting works nicely\n            if 'pipeline_start_dateparts' in d:\n                dt = d['pipeline_start_dateparts']\n                d['pipeline_start'] = '{}-{:02d}-{:02d} {:02d}:{:02d}'.format(dt['year'], dt['month'], dt['day'], dt['hour'], dt['minute'])\n            if pid not in data:\n                data[pid] = d\n                data[pid]['num_starting_files'] = int(num_starting_files)\n            else:\n                data[pid]['num_starting_files'] += int(num_starting_files)\n\n        headers = OrderedDict()\n        headers['pipeline_name'] = {'title': 'Pipeline Name'}\n        headers['pipeline_start'] = {'title': 'Date Started', 'description': 'Date and time that pipeline was started (YYYY-MM-DD HH:SS)'}\n        headers['genome'] = {'title': 'Genome ID', 'description': 'ID of reference genome used'}\n        headers['num_starting_files'] = {'title': '# Starting Files', 'format': '{:,.0f}', 'description': 'Number of input files at start of pipeline run.'}\n        table_config = {\n            'namespace': 'Cluster Flow',\n            'id': 'clusterflow-pipelines-table',\n            'table_title': 'Cluster Flow Pipelines',\n            'col1_header': 'Pipeline ID',\n            'no_beeswarm': True,\n            'save_file': True\n        }\n        self.add_section (\n            name = 'Pipelines',\n            anchor = 'clusterflow-pipelines',\n            description = 'Information about pipelines is parsed from <code>*.run</code> files. {}'.format(pids_guessed),\n            plot = table.plot(data, headers, table_config),\n            content = self.clusterflow_pipelines_printout()\n        )", "response": "Generate HTML for section about pipelines."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprinting the steps used in each Cluster Flow pipeline.", "response": "def clusterflow_pipelines_printout(self):\n        \"\"\" Print the steps used in each Cluster Flow pipeline \"\"\"\n        data = dict()\n        html = ''\n        for f,d in self.clusterflow_runfiles.items():\n            pid = d.get('pipeline_id', 'unknown')\n            data[pid] = [\n                d.get('pipeline_name'),\n                \"\\n\".join(d.get('pipeline_steps', []))\n            ]\n        for pid, d in data.items():\n            html += '''\n                <div class=\"panel panel-default\">\n                    <div class=\"panel-heading\"><h3 class=\"panel-title\">Pipeline Steps: {} (<code>{}</code>)</h3></div>\n                    <pre class=\"panel-body\" style=\"border:0; background-color:transparent; padding:0 15px; margin:0; color:#666; font-size:90%;\">{}</pre>\n                </div>\n                '''.format(pid, d[0], d[1])\n        return html"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sortmerna_detailed_barplot (self):\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        metrics = set()\n        for sample in self.sortmerna:\n            for key in self.sortmerna[sample]:\n                if not key in [\"total\", \"rRNA\", \"non_rRNA\"] and not \"_pct\" in key:\n                    metrics.add(key)\n\n        for key in metrics:\n            keys[key] = { 'name': key.replace(\"_count\",\"\") }\n\n        # Config for the plot\n        pconfig = {\n            'id': 'sortmerna-detailed-plot',\n            'title': 'SortMeRNA: Hit Counts',\n            'ylab': 'Reads'\n        }\n\n        self.add_section( plot = bargraph.plot(self.sortmerna, keys, pconfig) )", "response": "Make the highcharts HTML to plot the sortmerna rates"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds RSeQC inner distance frequency reports and parse their data", "response": "def parse_reports(self):\n    \"\"\" Find RSeQC inner_distance frequency reports and parse their data \"\"\"\n\n    # Set up vars\n    self.inner_distance = dict()\n    self.inner_distance_pct = dict()\n\n    # Go through files and parse data\n    for f in self.find_log_files('rseqc/inner_distance'):\n        if f['s_name'] in self.inner_distance:\n            log.debug(\"Duplicate sample name found! Overwriting: {}\".format(f['s_name']))\n        self.add_data_source(f, section='inner_distance')\n        #saving to temporary variable fro SE checking later\n        parsed_data = OrderedDict()\n        for l in f['f'].splitlines():\n            s = l.split()\n            try:\n                avg_pos = (float(s[0]) +  float(s[1])) / 2.0\n                parsed_data[avg_pos] = float(s[2])\n            except:\n                # Don't bother running through whole file if wrong\n                break\n\n        # Only add if we actually found something i,e it was PE data\n        if len(parsed_data) > 0:\n            self.inner_distance[f['s_name']] = parsed_data\n\n    # Filter to strip out ignored sample names\n    self.inner_distance = self.ignore_samples(self.inner_distance)\n\n    if len(self.inner_distance) > 0:\n\n        # Make a normalised percentage version of the data\n        for s_name in self.inner_distance:\n            self.inner_distance_pct[s_name] = OrderedDict()\n            total = sum( self.inner_distance[s_name].values() )\n            for k, v in self.inner_distance[s_name].items():\n                self.inner_distance_pct[s_name][k] = (v/total)*100\n\n        # Add line graph to section\n        pconfig = {\n            'id': 'rseqc_inner_distance_plot',\n            'title': 'RSeQC: Inner Distance',\n            'ylab': 'Counts',\n            'xlab': \"Inner Distance (bp)\",\n            'tt_label': \"<strong>{point.x} bp</strong>: {point.y:.2f}\",\n            'data_labels': [\n                {'name': 'Counts', 'ylab': 'Counts'},\n                {'name': 'Percentages', 'ylab': 'Percentage'}\n            ]\n        }\n        self.add_section (\n            name = 'Inner Distance',\n            anchor = 'rseqc-inner_distance',\n            description = '<a href=\"http://rseqc.sourceforge.net/#inner-distance-py\" target=\"_blank\">Inner Distance</a>' \\\n                \" calculates the inner distance\" \\\n                \" (or insert size) between two paired RNA reads.\" \\\n                \" Note that this can be negative if fragments overlap.\",\n            plot = linegraph.plot([self.inner_distance, self.inner_distance_pct], pconfig)\n        )\n\n    # Return number of samples found\n    return len(self.inner_distance)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_unknown_barcodes(self, lane_unknown_barcode):\n        try:\n            sorted_barcodes = OrderedDict(\n                sorted(\n                    lane_unknown_barcode.items(),\n                    key = operator.itemgetter(1),\n                    reverse = True\n                )\n            )\n        except AttributeError:\n            sorted_barcodes = None\n        return sorted_barcodes", "response": "This function returns an OrderedDict that is not sorted."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef lane_stats_table(self):\n        headers = OrderedDict()\n        headers['total_yield'] = {\n            'title': '{} Total Yield'.format(config.base_count_prefix),\n            'description': 'Number of bases ({})'.format(config.base_count_desc),\n            'scale': 'Greens',\n            'shared_key': 'base_count'\n        }\n        headers['total'] = {\n            'title': '{} Total Clusters'.format(config.read_count_prefix),\n            'description': 'Total number of clusters for this lane ({})'.format(config.read_count_desc),\n            'scale': 'Blues',\n            'shared_key': 'read_count'\n        }\n        headers['percent_Q30'] = {\n            'title': '% bases &ge; Q30',\n            'description': 'Percentage of bases with greater than or equal to Q30 quality score',\n            'suffix': '%',\n            'max': 100,\n            'min': 0,\n            'scale': 'RdYlGn'\n        }\n        headers['mean_qscore'] = {\n            'title': 'Mean Quality',\n            'description': 'Average phred qualty score',\n            'min': 0,\n            'scale': 'Spectral'\n        }\n        headers['percent_perfectIndex'] = {\n            'title': '% Perfect Index',\n            'description': 'Percent of reads with perfect index (0 mismatches)',\n            'max': 100,\n            'min': 0,\n            'scale': 'RdYlGn',\n            'suffix': '%'\n        }\n        table_config = {\n            'namespace': 'bcl2fastq',\n            'id': 'bcl2fastq-lane-stats-table',\n            'table_title': 'bcl2fastq Lane Statistics',\n            'col1_header': 'Run ID - Lane',\n            'no_beeswarm': True\n        }\n        return table.plot(self.bcl2fastq_bylane, headers, table_config)", "response": "Return a table with overview stats for each bcl2fastq lane for a single flow cell"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets data to plot for undetermined barcodes.", "response": "def get_bar_data_from_undetermined(self, flowcells):\n        \"\"\" Get data to plot for undetermined barcodes.\n        \"\"\"\n        bar_data = defaultdict(dict)\n        # get undetermined barcodes for each lanes\n        for lane_id, lane in flowcells.items():\n            try:\n                for barcode, count in islice(lane['unknown_barcodes'].items(), 20):\n                    bar_data[barcode][lane_id] = count\n            except AttributeError:\n                pass\n\n        # sort results\n        bar_data = OrderedDict(sorted(\n            bar_data.items(),\n            key=lambda x: sum(x[1].values()),\n            reverse=True\n        ))\n        return OrderedDict(\n            (key, value) for key, value in islice(bar_data.items(), 20)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef kallisto_general_stats_table(self):\n\n        headers = OrderedDict()\n        headers['fragment_length'] = {\n            'title': 'Frag Length',\n            'description': 'Estimated average fragment length',\n            'min': 0,\n            'suffix': 'bp',\n            'scale': 'RdYlGn'\n        }\n        headers['percent_aligned'] = {\n            'title': '% Aligned',\n            'description': '% processed reads that were pseudoaligned',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'YlGn'\n        }\n        headers['pseudoaligned_reads'] = {\n            'title': '{} Aligned'.format(config.read_count_prefix),\n            'description': 'Pseudoaligned reads ({})'.format(config.read_count_desc),\n            'min': 0,\n            'scale': 'PuRd',\n            'modify': lambda x: x * config.read_count_multiplier,\n            'shared_key': 'read_count'\n        }\n        self.general_stats_addcols(self.kallisto_data, headers)", "response": "Take the parsed stats from the Kallisto report and add it to the basic stats table at the top of the report"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes the HighCharts HTML to plot the alignment rates for the current Kallisto category", "response": "def kallisto_alignment_plot (self):\n        \"\"\" Make the HighCharts HTML to plot the alignment rates \"\"\"\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['pseudoaligned_reads'] = { 'color': '#437bb1', 'name': 'Pseudoaligned' }\n        keys['not_pseudoaligned_reads'] =   { 'color': '#b1084c', 'name': 'Not aligned' }\n\n        # Config for the plot\n        config = {\n            'id': 'kallisto_alignment',\n            'title': 'Kallisto: Alignment Scores',\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads'\n        }\n\n        return bargraph.plot(self.kallisto_data, keys, config)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a HiC - Pro stat file and store the data in self. hicpro_data.", "response": "def parse_hicpro_stats(self, f, rsection):\n        \"\"\" Parse a HiC-Pro stat file \"\"\"\n        s_name = self.clean_s_name(os.path.basename(f['root']), os.path.dirname(f['root']))\n\n        if s_name not in self.hicpro_data.keys():\n            self.hicpro_data[s_name] = {}\n\n        self.add_data_source(f, s_name, section=rsection)\n        for l in f['f'].splitlines():\n            if not l.startswith('#'):\n                s = l.split(\"\\t\")\n                if s[0] in self.hicpro_data[s_name]:\n                    log.debug(\"Duplicated keys found! Overwriting: {}\".format(s[0]))\n                self.hicpro_data[s_name][s[0]] = int(s[1])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hicpro_stats_table(self):\n         headers = OrderedDict()\n\n         headers['percent_duplicates'] = {\n             'title': '% Duplicates',\n             'description': 'Percent of duplicated valid pairs (%)',\n             'max': 100,\n             'min': 0,\n             'suffix': '%',\n             'scale': 'YlOrRd',\n             'hidden': True\n         }\n            \n         headers['valid_interaction_rmdup'] = {\n             'title': '{} Valid Pairs Unique'.format(config.read_count_prefix),\n             'description': 'Number of valid pairs after duplicates removal ({})'.format(config.read_count_desc),\n             'min': 0,\n             'scale': 'RdYlBu',\n             'modify': lambda x: x * config.read_count_multiplier,\n             'shared_key': 'read_count',\n         }\n                 \n         headers['percent_valid'] = {\n             'title': '% Valid Pairs',\n             'description': 'Percentage of valid pairs over reported ones (%)',\n             'max': 100,\n             'min': 0,\n             'suffix': '%',\n             'scale': 'YlGn',\n             'hidden': True\n         }\n\n         headers['valid_interaction'] = {\n             'title': '{} Valid Pairs'.format(config.read_count_prefix),\n             'description': 'Number of valid pairs ({})'.format(config.read_count_desc),\n             'min': 0,\n             'scale': 'RdYlBu',\n             'modify': lambda x: x * config.read_count_multiplier,\n             'shared_key': 'read_count',\n             'hidden': True\n         }\n \n         headers['percent_paired_reads'] = {\n             'title': '% Reported',\n             'description': 'Percentage of paired reads (%) passing the mapping filters',\n             'max': 100,\n             'min': 0,\n             'suffix': '%',\n             'scale': 'YlGn',\n             'hidden': True\n         }\n\n         headers['paired_reads'] = {\n             'title': 'Reported Read Pairs',\n             'description' : 'Total number of read pairs ({}) passing the mapping filters'.format(config.read_count_desc),\n             'min' : '0',\n             'scale' : 'RdYlBu',\n             'modify': lambda x: x * config.read_count_multiplier,\n             'shared_key' : 'read_count',\n         }\n\n         headers['percent_mapped_R2'] = {\n             'title': '% Aligned [R2]',\n             'description': 'Percentage of aligned reads [R2] (%)',\n             'max': 100,\n             'min': 0,\n             'suffix': '%',\n             'scale': 'YlGn',\n             'hidden': True\n         }\n\n         headers['mapped_R2'] = {\n             'title': 'Aligned [R2]',\n             'description' : 'Total number of aligned reads [R2] ({})'.format(config.read_count_desc),\n             'min' : '0',\n             'scale' : 'RdYlBu',\n             'modify': lambda x: x * config.read_count_multiplier,\n             'shared_key' : 'read_count',\n             'hidden': True\n         }\n\n         headers['percent_mapped_R1'] = {\n             'title': '% Aligned [R1]',\n             'description': 'Percentage of aligned reads [R1] (%)',\n             'max': 100,\n             'min': 0,\n             'suffix': '%',\n             'scale': 'YlGn',\n             'hidden': True\n         }\n\n         headers['mapped_R1'] = {\n             'title': 'Aligned [R1]',\n             'description' : 'Total number of aligned reads [R1] ({})'.format(config.read_count_desc),\n             'min' : '0',\n             'scale' : 'RdYlBu',\n             'modify': lambda x: x * config.read_count_multiplier,\n             'shared_key' : 'read_count',\n             'hidden': True\n         }\n         \n         headers['total_R1'] = {\n             'title': 'Total',\n             'description' : 'Total Number of Read Pairs',\n             'min' : '0',\n             'scale' : 'RdYlBu',\n             'modify': lambda x: x * config.read_count_multiplier,\n             'shared_key' : 'read_count',\n         }\n\n         self.general_stats_addcols(self.hicpro_data, headers, 'HiC-Pro')", "response": "Add HiC - Pro stats to the general stats table"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hicpro_mapping_chart (self):\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['Full_Alignments_Read']   = { 'color': '#005ce6', 'name': 'Full reads Alignments' }\n        keys['Trimmed_Alignments_Read'] = { 'color': '#3385ff', 'name': 'Trimmed reads Alignments' }\n        keys['Failed_To_Align_Read']     = { 'color': '#a9a2a2', 'name': 'Failed To Align' }\n                               \n        data = [{},{}]\n        for s_name in self.hicpro_data:\n            for r in [1,2]:\n                data[r-1]['{} [R{}]'.format(s_name, r)] = {\n                    'Full_Alignments_Read': self.hicpro_data[s_name]['global_R{}'.format(r)],\n                    'Trimmed_Alignments_Read': self.hicpro_data[s_name]['local_R{}'.format(r)],\n                    'Failed_To_Align_Read': int(self.hicpro_data[s_name]['total_R{}'.format(r)]) - int(self.hicpro_data[s_name]['mapped_R{}'.format(r)])\n                }\n        \n        # Config for the plot\n        config = {\n            'id': 'hicpro_mapping_stats_plot',\n            'title': 'HiC-Pro: Mapping Statistics',\n            'ylab': '# Reads',\n            'ylab': '# Reads: Read 1',\n            'data_labels': [\n                {'name': 'Read 1', 'ylab': '# Reads: Read 1'},\n                {'name': 'Read 2', 'ylab': '# Reads: Read 2'}\n            ]\n        }\n\n        return bargraph.plot(data, [keys, keys], config)", "response": "Generate the HiC - Pro Aligned reads plot"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hicpro_pairing_chart (self):\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['Unique_paired_alignments'] = { 'color': '#005ce6', 'name': 'Uniquely Aligned' }\n        keys['Low_qual_pairs'] = { 'color': '#b97b35', 'name': 'Low Quality' }\n        keys['Pairs_with_singleton'] = { 'color': '#ff9933', 'name': 'Singleton' }\n        keys['Multiple_pairs_alignments'] = { 'color': '#e67300', 'name': 'Multi Aligned' }\n        keys['Unmapped_airs'] = { 'color': '#a9a2a2', 'name': 'Failed To Align' }\n    \n        # Config for the plot\n        config = {\n            'id': 'hicpro_pairing_stats_plot',\n            'title': 'HiC-Pro: Pairing Statistics',\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads'\n        }\n\n        return bargraph.plot(self.hicpro_data, keys, config)", "response": "Generate the HiC - Pro Pairing chart"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating the HiC - Pro filtering plot", "response": "def hicpro_filtering_chart (self):\n        \"\"\" Generate the HiC-Pro filtering plot \"\"\"\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['Valid_interaction_pairs_FF'] = { 'color': '#ccddff', 'name': 'Valid Pairs FF' }\n        keys['Valid_interaction_pairs_RR'] = { 'color': '#6699ff', 'name': 'Valid Pairs RR' }\n        keys['Valid_interaction_pairs_RF'] = { 'color': '#0055ff', 'name': 'Valid Pairs RF' }\n        keys['Valid_interaction_pairs_FR'] = { 'color': '#003399', 'name': 'Valid Pairs FR' }\n        keys['Self_Cycle_pairs'] = { 'color': '#ffad99', 'name': 'Same Fragment - Self-Circle' }\n        keys['Dangling_end_pairs'] = { 'color': '#ff5c33', 'name': 'Same Fragment - Dangling Ends' }\n        keys['Religation_pairs'] = { 'color': '#cc2900', 'name': 'Re-ligation' }\n        keys['Filtered_pairs'] = { 'color': '#661400', 'name': 'Filtered pairs' }\n        keys['Dumped_pairs'] = { 'color': '#330a00', 'name': 'Dumped pairs' }\n\n        # Config for the plot\n        config = {\n            'id': 'hicpro_filtering_plot',\n            'title': 'HiC-Pro: Filtering Statistics',\n            'ylab': '# Read Pairs',\n            'cpswitch_counts_label': 'Number of Read Pairs'\n        }\n        \n        return bargraph.plot(self.hicpro_data, keys, config)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hicpro_contact_chart (self):\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['cis_shortRange'] = { 'color': '#0039e6', 'name': 'Unique: cis <= 20Kbp' }\n        keys['cis_longRange'] = { 'color': '#809fff', 'name': 'Unique: cis > 20Kbp' }\n        keys['trans_interaction']  = { 'color': '#009933', 'name': 'Unique: trans' }\n        keys['duplicates'] = { 'color': '#a9a2a2', 'name': 'Duplicate read pairs' }\n        \n        # Config for the plot\n        config = {\n            'id': 'hicpro_contact_plot',\n            'title': 'HiC-Pro: Contact Statistics',\n            'ylab': '# Pairs',\n            'cpswitch_counts_label': 'Number of Pairs'\n        }\n        \n        return bargraph.plot(self.hicpro_data, keys, config)", "response": "Generate the HiC - Pro contact plot"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate the Capture Hi - C plot", "response": "def hicpro_capture_chart (self):\n        \"\"\" Generate Capture Hi-C plot\"\"\"\n\n        keys = OrderedDict()\n        keys['valid_pairs_on_target_cap_cap'] = { 'color': '#0039e6', 'name': 'Capture-Capture interactions' }\n        keys['valid_pairs_on_target_cap_rep']  = { 'color': '#809fff', 'name': 'Capture-Reporter interactions' }\n        keys['valid_pairs_off_target'] = { 'color': '#cccccc', 'name': 'Off-target valid pairs' }\n  \n        # Check capture info are available\n        num_samples = 0\n        for s_name in self.hicpro_data:\n            for k in keys:\n                num_samples += sum([1 if k in self.hicpro_data[s_name] else 0])\n        if num_samples == 0:\n            return False\n\n        # Config for the plot\n        config = {\n            'id': 'hicpro_cap_plot',\n            'title': 'HiC-Pro: Capture Statistics',\n            'ylab': '# Pairs',\n            'cpswitch_counts_label': 'Number of Pairs'\n        }\n\n        return bargraph.plot(self.hicpro_data, keys, config)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_reports(self):\n\n    # Set up vars\n    self.picard_RnaSeqMetrics_data = dict()\n    self.picard_RnaSeqMetrics_histogram = dict()\n\n    # Go through logs and find Metrics\n    for f in self.find_log_files('picard/rnaseqmetrics', filehandles=True):\n        s_name = None\n        in_hist = False\n        for l in f['f']:\n            # Catch the histogram values\n            if s_name is not None and in_hist is True:\n                try:\n                    sections = l.split(\"\\t\")\n                    pos = int(sections[0])\n                    coverage = float(sections[1])\n                    self.picard_RnaSeqMetrics_histogram[s_name][pos] = coverage\n                except ValueError:\n                    # Reset in case we have more in this log file\n                    s_name = None\n                    in_hist = False\n\n            # New log starting\n            if 'rnaseqmetrics' in l.lower() and 'INPUT' in l:\n                s_name = None\n                # Pull sample name from input\n                fn_search = re.search(r\"INPUT(?:=|\\s+)(\\[?[^\\s]+\\]?)\", l, flags=re.IGNORECASE)\n                if fn_search:\n                    s_name = os.path.basename(fn_search.group(1).strip('[]'))\n                    s_name = self.clean_s_name(s_name, f['root'])\n\n            if s_name is not None:\n                if 'rnaseqmetrics' in l.lower() and '## METRICS CLASS' in l:\n                    if s_name in self.picard_RnaSeqMetrics_data:\n                        log.debug(\"Duplicate sample name found in {}! Overwriting: {}\".format(f['fn'], s_name))\n                    self.picard_RnaSeqMetrics_data[s_name] = dict()\n                    self.picard_RnaSeqMetrics_histogram[s_name] = dict()\n                    self.add_data_source(f, s_name, section='RnaSeqMetrics')\n                    keys = f['f'].readline().strip(\"\\n\").split(\"\\t\")\n                    vals = f['f'].readline().strip(\"\\n\").split(\"\\t\")\n                    for i, k in enumerate(keys):\n                        # Multiply percentages by 100\n                        if k.startswith('PCT_'):\n                            try:\n                                vals[i] = float(vals[i]) * 100.0\n                            except (ValueError, IndexError):\n                                pass\n                        # Save the key:value pairs\n                        try:\n                            self.picard_RnaSeqMetrics_data[s_name][k] = float(vals[i])\n                        except ValueError:\n                            self.picard_RnaSeqMetrics_data[s_name][k] = vals[i]\n                        except IndexError:\n                            pass # missing data\n                    # Calculate some extra numbers\n                    if 'PF_BASES' in keys and 'PF_ALIGNED_BASES' in keys:\n                        self.picard_RnaSeqMetrics_data[s_name]['PF_NOT_ALIGNED_BASES'] = \\\n                            self.picard_RnaSeqMetrics_data[s_name]['PF_BASES'] - self.picard_RnaSeqMetrics_data[s_name]['PF_ALIGNED_BASES']\n\n            if s_name is not None and 'normalized_position\tAll_Reads.normalized_coverage' in l:\n                self.picard_RnaSeqMetrics_histogram[s_name] = dict()\n                in_hist = True\n\n        for key in list(self.picard_RnaSeqMetrics_data.keys()):\n            if len(self.picard_RnaSeqMetrics_data[key]) == 0:\n                self.picard_RnaSeqMetrics_data.pop(key, None)\n        for s_name in list(self.picard_RnaSeqMetrics_histogram.keys()):\n            if len(self.picard_RnaSeqMetrics_histogram[s_name]) == 0:\n                self.picard_RnaSeqMetrics_histogram.pop(s_name, None)\n                log.debug(\"Ignoring '{}' histogram as no data parsed\".format(s_name))\n\n    # Filter to strip out ignored sample names\n    self.picard_RnaSeqMetrics_data = self.ignore_samples(self.picard_RnaSeqMetrics_data)\n\n    if len(self.picard_RnaSeqMetrics_data) > 0:\n\n        # Write parsed data to a file\n        self.write_data_file(self.picard_RnaSeqMetrics_data, 'multiqc_picard_RnaSeqMetrics')\n\n        # Add to general stats table\n        GenStatsHeaders = OrderedDict()\n        GenStatsHeaders['PCT_RIBOSOMAL_BASES'] = {\n            'title': '% rRNA',\n            'description': 'Percent of aligned bases overlapping ribosomal RNA regions',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'Reds',\n        }\n        GenStatsHeaders['PCT_MRNA_BASES'] = {\n            'title': '% mRNA',\n            'description': 'Percent of aligned bases overlapping UTRs and coding regions of mRNA transcripts',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'Greens',\n        }\n        self.general_stats_addcols(self.picard_RnaSeqMetrics_data, GenStatsHeaders)\n\n        # Bar plot of bases assignment\n        bg_cats = OrderedDict()\n        bg_cats['CODING_BASES'] = { 'name': 'Coding' }\n        bg_cats['UTR_BASES'] = { 'name': 'UTR' }\n        bg_cats['INTRONIC_BASES'] = { 'name': 'Intronic' }\n        bg_cats['INTERGENIC_BASES'] = { 'name': 'Intergenic' }\n        bg_cats['RIBOSOMAL_BASES'] = { 'name': 'Ribosomal' }\n        bg_cats['PF_NOT_ALIGNED_BASES'] = { 'name': 'PF not aligned' }\n\n        # Warn user if any samples are missing 'RIBOSOMAL_BASES' data; ie picard was run without an rRNA interval file.\n        warn_rrna = ''\n        rrna_missing = []\n        for s_name, metrics in self.picard_RnaSeqMetrics_data.items():\n            if metrics['RIBOSOMAL_BASES'] == '':\n                rrna_missing.append(s_name)\n        if len(rrna_missing):\n            if len(rrna_missing) < 5:\n                missing_samples = 'for samples <code>{}</code>'.format('</code>, <code>'.join(rrna_missing))\n            else:\n                missing_samples = '<strong>{} samples</strong>'.format(len(rrna_missing))\n            warn_rrna = '''\n            <div class=\"alert alert-warning\">\n              <span class=\"glyphicon glyphicon-warning-sign\"></span>\n              Picard was run without an rRNA annotation file {}, therefore the ribosomal assignment is not available. To correct, rerun with the <code>RIBOSOMAL_INTERVALS</code> parameter, as documented <a href=\"https://broadinstitute.github.io/picard/command-line-overview.html#CollectRnaSeqMetrics\" target=\"_blank\">here</a>.\n            </div>\n            '''.format(missing_samples)\n\n        pconfig = {\n            'id': 'picard_rnaseqmetrics_assignment_plot',\n            'title': 'Picard: RnaSeqMetrics Read Assignments',\n            'ylab': 'Number of reads'\n        }\n        self.add_section (\n            name = 'RnaSeqMetrics Assignment',\n            anchor = 'picard-rna-assignment',\n            description = 'Number of bases in primary alignments that align to regions in the reference genome.' + warn_rrna,\n            plot = bargraph.plot(self.picard_RnaSeqMetrics_data, bg_cats, pconfig)\n        )\n\n        # Bar plot of strand mapping\n        bg_cats = OrderedDict()\n        bg_cats['CORRECT_STRAND_READS'] = { 'name': 'Correct' }\n        bg_cats['INCORRECT_STRAND_READS'] = { 'name': 'Incorrect', 'color': '#8e123c' }\n\n        pdata = dict()\n        for s_name, d in self.picard_RnaSeqMetrics_data.items():\n            if d['CORRECT_STRAND_READS'] > 0 and d['INCORRECT_STRAND_READS'] > 0:\n                pdata[s_name] = d\n        if len(pdata) > 0:\n            pconfig = {\n                'id': 'picard_rnaseqmetrics_strand_plot',\n                'title': 'Picard: RnaSeqMetrics Strand Mapping',\n                'ylab': 'Number of reads',\n                'hide_zero_cats': False\n            }\n            self.add_section (\n                name = 'RnaSeqMetrics Strand Mapping',\n                anchor = 'picard-rna-strand',\n                description = 'Number of aligned reads that map to the correct strand.',\n                plot = bargraph.plot(self.picard_RnaSeqMetrics_data, bg_cats, pconfig)\n            )\n\n        # Section with histogram plot\n        if len(self.picard_RnaSeqMetrics_histogram) > 0:\n            # Plot the data and add section\n            pconfig = {\n                'smooth_points': 500,\n                'smooth_points_sumcounts': [True, False],\n                'id': 'picard_rna_coverage',\n                'title': 'Picard: Normalized Gene Coverage',\n                'ylab': 'Coverage',\n                'xlab': 'Percent through gene',\n                'xDecimals': False,\n                'tt_label': '<b>{point.x}%</b>: {point.y:.0f}',\n                'ymin': 0,\n            }\n            self.add_section (\n                name = 'Gene Coverage',\n                anchor = 'picard-rna-coverage',\n                plot = linegraph.plot(self.picard_RnaSeqMetrics_histogram, pconfig)\n            )\n\n\n    # Return the number of detected samples to the parent module\n    return len(self.picard_RnaSeqMetrics_data)", "response": "Find Picard RnaSeqMetrics reports and parse their data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind bamPEFragmentSize output. Supports the bamPEFDistribution option. Returns the number of paired - end fragment sizes.", "response": "def parse_bamPEFragmentSizeDistribution(self):\n        \"\"\"Find bamPEFragmentSize output. Supports the --outRawFragmentLengths option\"\"\"\n        self.deeptools_bamPEFragmentSizeDistribution = dict()\n        for f in self.find_log_files('deeptools/bamPEFragmentSizeDistribution', filehandles=False):\n            parsed_data = self.parseBamPEFDistributionFile(f)\n            for k, v in parsed_data.items():\n                if k in self.deeptools_bamPEFragmentSizeDistribution:\n                    log.warning(\"Replacing duplicate sample {}.\".format(k))\n                self.deeptools_bamPEFragmentSizeDistribution[k] = v\n            if len(parsed_data) > 0:\n                self.add_data_source(f, section='bamPEFragmentSizeDistribution')\n\n        if len(self.deeptools_bamPEFragmentSizeDistribution) > 0:\n            config = {\n                'id': 'fragment_size_distribution_plot',\n                'title': 'deeptools: Fragment Size Distribution Plot',\n                'ylab': 'Occurrence',\n                'xlab': 'Fragment Size (bp)',\n                'smooth_points': 50,\n                'xmax': 1000,\n                'xDecimals': False,\n                'tt_label': '<b>Fragment Size (bp) {point.x}</b>: {point.y} Occurrence',\n            }\n\n            self.add_section (\n                name = 'Fragment size distribution',\n                anchor = 'fragment_size_distribution',\n                description=\"Distribution of paired-end fragment sizes\",\n                plot=linegraph.plot(self.deeptools_bamPEFragmentSizeDistribution, config)\n            )\n\n        return len(self.deeptools_bamPEFragmentSizeDistribution)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_featurecounts_report (self, f):\n\n        file_names = list()\n        parsed_data = dict()\n        for l in f['f'].splitlines():\n            thisrow = list()\n            s = l.split(\"\\t\")\n            if len(s) < 2:\n                continue\n            if s[0] == 'Status':\n                for f_name in s[1:]:\n                    file_names.append(f_name)\n            else:\n                k = s[0]\n                if k not in self.featurecounts_keys:\n                    self.featurecounts_keys.append(k)\n                for val in s[1:]:\n                    try:\n                        thisrow.append(int(val))\n                    except ValueError:\n                        pass\n            if len(thisrow) > 0:\n                parsed_data[k] = thisrow\n        # Check that this actually is a featureCounts file, as format and parsing is quite general\n        if 'Assigned' not in parsed_data.keys():\n            return None\n        for idx, f_name in enumerate(file_names):\n\n            # Clean up sample name\n            s_name = self.clean_s_name(f_name, f['root'])\n\n            # Reorganised parsed data for this sample\n            # Collect total count number\n            data = dict()\n            data['Total'] = 0\n            for k in parsed_data:\n                data[k] = parsed_data[k][idx]\n                data['Total'] += parsed_data[k][idx]\n\n            # Calculate the percent aligned if we can\n            try:\n                data['percent_assigned'] = (float(data['Assigned'])/float(data['Total'])) * 100.0\n            except (KeyError, ZeroDivisionError):\n                pass\n\n            # Add to the main dictionary\n            if len(data) > 1:\n                if s_name in self.featurecounts_data:\n                    log.debug(\"Duplicate sample name found! Overwriting: {}\".format(s_name))\n                self.add_data_source(f, s_name)\n                self.featurecounts_data[s_name] = data", "response": "Parse the featureCounts log file and store the data in the main dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef featurecounts_stats_table(self):\n\n        headers = OrderedDict()\n        headers['percent_assigned'] = {\n            'title': '% Assigned',\n            'description': '% Assigned reads',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'RdYlGn'\n        }\n        headers['Assigned'] = {\n            'title': '{} Assigned'.format(config.read_count_prefix),\n            'description': 'Assigned reads ({})'.format(config.read_count_desc),\n            'min': 0,\n            'scale': 'PuBu',\n            'modify': lambda x: float(x) * config.read_count_multiplier,\n            'shared_key': 'read_count'\n        }\n        self.general_stats_addcols(self.featurecounts_data, headers)", "response": "Take the parsed stats from the featureCounts report and add them to the basic stats table at the top of the report"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking the featureCounts assignment rates plot", "response": "def featureCounts_chart (self):\n        \"\"\" Make the featureCounts assignment rates plot \"\"\"\n\n        # Config for the plot\n        config = {\n            'id': 'featureCounts_assignment_plot',\n            'title': 'featureCounts: Assignments',\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads'\n        }\n\n        return bargraph.plot(self.featurecounts_data, self.featurecounts_keys, config)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplotting a line graph with X and Y data.", "response": "def plot (data, pconfig=None):\n    \"\"\" Plot a line graph with X,Y data.\n    :param data: 2D dict, first keys as sample names, then x:y data pairs\n    :param pconfig: optional dict with config key:value pairs. See CONTRIBUTING.md\n    :return: HTML and JS, ready to be inserted into the page\n    \"\"\"\n    # Don't just use {} as the default argument as it's mutable. See:\n    # http://python-guide-pt-br.readthedocs.io/en/latest/writing/gotchas/\n    if pconfig is None:\n        pconfig = {}\n\n    # Allow user to overwrite any given config for this plot\n    if 'id' in pconfig and pconfig['id'] and pconfig['id'] in config.custom_plot_config:\n        for k, v in config.custom_plot_config[pconfig['id']].items():\n            pconfig[k] = v\n\n    # Given one dataset - turn it into a list\n    if type(data) is not list:\n        data = [data]\n\n    # Smooth dataset if requested in config\n    if pconfig.get('smooth_points', None) is not None:\n        sumcounts = pconfig.get('smooth_points_sumcounts', True)\n        for i, d in enumerate(data):\n            if type(sumcounts) is list:\n                sumc = sumcounts[i]\n            else:\n                sumc = sumcounts\n            data[i] = smooth_line_data(d, pconfig['smooth_points'], sumc)\n\n    # Add sane plotting config defaults\n    for idx, yp in enumerate(pconfig.get('yPlotLines', [])):\n        pconfig['yPlotLines'][idx][\"width\"] = pconfig['yPlotLines'][idx].get(\"width\", 2)\n\n    # Add initial axis labels if defined in `data_labels` but not main config\n    if pconfig.get('ylab') is None:\n        try:\n            pconfig['ylab'] = pconfig['data_labels'][0]['ylab']\n        except (KeyError, IndexError):\n            pass\n    if pconfig.get('xlab') is None:\n        try:\n            pconfig['xlab'] = pconfig['data_labels'][0]['xlab']\n        except (KeyError, IndexError):\n            pass\n\n    # Generate the data dict structure expected by HighCharts series\n    plotdata = list()\n    for data_index, d in enumerate(data):\n        thisplotdata = list()\n\n        for s in sorted(d.keys()):\n\n            # Ensure any overwritting conditionals from data_labels (e.g. ymax) are taken in consideration\n            series_config = pconfig.copy()\n            if 'data_labels' in pconfig and type(pconfig['data_labels'][data_index]) is dict:  # if not a dict: only dataset name is provided\n                series_config.update(pconfig['data_labels'][data_index])\n\n            pairs = list()\n            maxval = 0\n            if 'categories' in series_config:\n                pconfig['categories'] = list()\n                for k in d[s].keys():\n                    pconfig['categories'].append(k)\n                    pairs.append(d[s][k])\n                    maxval = max(maxval, d[s][k])\n            else:\n                for k in sorted(d[s].keys()):\n                    if k is not None:\n                        if 'xmax' in series_config and float(k) > float(series_config['xmax']):\n                            continue\n                        if 'xmin' in series_config and float(k) < float(series_config['xmin']):\n                            continue\n                    if d[s][k] is not None:\n                        if 'ymax' in series_config and float(d[s][k]) > float(series_config['ymax']):\n                            continue\n                        if 'ymin' in series_config and float(d[s][k]) < float(series_config['ymin']):\n                            continue\n                    pairs.append([k, d[s][k]])\n                    try:\n                        maxval = max(maxval, d[s][k])\n                    except TypeError:\n                        pass\n            if maxval > 0 or series_config.get('hide_empty') is not True:\n                this_series = { 'name': s, 'data': pairs }\n                try:\n                    this_series['color'] = series_config['colors'][s]\n                except:\n                    pass\n                thisplotdata.append(this_series)\n        plotdata.append(thisplotdata)\n\n    # Add on annotation data series\n    try:\n        if pconfig.get('extra_series'):\n            extra_series = pconfig['extra_series']\n            if type(pconfig['extra_series']) == dict:\n                extra_series = [[ pconfig['extra_series'] ]]\n            elif type(pconfig['extra_series']) == list and type(pconfig['extra_series'][0]) == dict:\n                extra_series = [ pconfig['extra_series'] ]\n            for i, es in enumerate(extra_series):\n                for s in es:\n                    plotdata[i].append(s)\n    except (KeyError, IndexError):\n        pass\n\n    # Make a plot - template custom, or interactive or flat\n    try:\n        return get_template_mod().linegraph(plotdata, pconfig)\n    except (AttributeError, TypeError):\n        if config.plots_force_flat or (not config.plots_force_interactive and len(plotdata[0]) > config.plots_flat_numseries):\n            try:\n                return matplotlib_linegraph(plotdata, pconfig)\n            except:\n                logger.error(\"############### Error making MatPlotLib figure! Falling back to HighCharts.\")\n                return highcharts_linegraph(plotdata, pconfig)\n        else:\n            # Use MatPlotLib to generate static plots if requested\n            if config.export_plots:\n                matplotlib_linegraph(plotdata, pconfig)\n            # Return HTML for HighCharts dynamic plot\n            return highcharts_linegraph(plotdata, pconfig)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef highcharts_linegraph (plotdata, pconfig=None):\n    if pconfig is None:\n        pconfig = {}\n\n    # Get the plot ID\n    if pconfig.get('id') is None:\n        pconfig['id'] = 'mqc_hcplot_'+''.join(random.sample(letters, 10))\n\n    # Sanitise plot ID and check for duplicates\n    pconfig['id'] = report.save_htmlid(pconfig['id'])\n\n    # Build the HTML for the page\n    html = '<div class=\"mqc_hcplot_plotgroup\">'\n\n    # Buttons to cycle through different datasets\n    if len(plotdata) > 1:\n        html += '<div class=\"btn-group hc_switch_group\">\\n'\n        for k, p in enumerate(plotdata):\n            active = 'active' if k == 0 else ''\n            try:\n                name = pconfig['data_labels'][k]['name']\n            except:\n                name = k+1\n            try:\n                ylab = 'data-ylab=\"{}\"'.format(pconfig['data_labels'][k]['ylab'])\n            except:\n                ylab = 'data-ylab=\"{}\"'.format(name) if name != k+1 else ''\n            try:\n                ymax = 'data-ymax=\"{}\"'.format(pconfig['data_labels'][k]['ymax'])\n            except:\n                ymax = ''\n            try:\n                xlab = 'data-xlab=\"{}\"'.format(pconfig['data_labels'][k]['xlab'])\n            except:\n                xlab = ''\n            html += '<button class=\"btn btn-default btn-sm {a}\" data-action=\"set_data\" {y} {ym} {x} data-newdata=\"{k}\" data-target=\"{id}\">{n}</button>\\n'.format(a=active, id=pconfig['id'], n=name, y=ylab, ym=ymax, x=xlab, k=k)\n        html += '</div>\\n\\n'\n\n    # The plot div\n    html += '<div class=\"hc-plot-wrapper\"><div id=\"{id}\" class=\"hc-plot not_rendered hc-line-plot\"><small>loading..</small></div></div></div> \\n'.format(id=pconfig['id'])\n\n    report.num_hc_plots += 1\n\n    report.plot_data[pconfig['id']] = {\n        'plot_type': \"xy_line\",\n        'datasets': plotdata,\n        'config': pconfig\n    }\n\n    return html", "response": "Build the HTML needed for a HighCharts line graph."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef matplotlib_linegraph (plotdata, pconfig=None):\n    if pconfig is None:\n        pconfig = {}\n\n    # Plot group ID\n    if pconfig.get('id') is None:\n        pconfig['id'] = 'mqc_mplplot_'+''.join(random.sample(letters, 10))\n\n    # Sanitise plot ID and check for duplicates\n    pconfig['id'] = report.save_htmlid(pconfig['id'])\n\n    # Individual plot IDs\n    pids = []\n    for k in range(len(plotdata)):\n        try:\n            name = pconfig['data_labels'][k]['name']\n        except:\n            name = k+1\n        pid = 'mqc_{}_{}'.format(pconfig['id'], name)\n        pid = report.save_htmlid(pid, skiplint=True)\n        pids.append(pid)\n\n    html = '<p class=\"text-info\"><small><span class=\"glyphicon glyphicon-picture\" aria-hidden=\"true\"></span> ' + \\\n          'Flat image plot. Toolbox functions such as highlighting / hiding samples will not work ' + \\\n          '(see the <a href=\"http://multiqc.info/docs/#flat--interactive-plots\" target=\"_blank\">docs</a>).</small></p>'\n    html += '<div class=\"mqc_mplplot_plotgroup\" id=\"{}\">'.format(pconfig['id'])\n\n    # Same defaults as HighCharts for consistency\n    default_colors = ['#7cb5ec', '#434348', '#90ed7d', '#f7a35c', '#8085e9',\n                      '#f15c80', '#e4d354', '#2b908f', '#f45b5b', '#91e8e1']\n\n    # Buttons to cycle through different datasets\n    if len(plotdata) > 1 and not config.simple_output:\n        html += '<div class=\"btn-group mpl_switch_group mqc_mplplot_bargraph_switchds\">\\n'\n        for k, p in enumerate(plotdata):\n            pid = pids[k]\n            active = 'active' if k == 0 else ''\n            try:\n                name = pconfig['data_labels'][k]['name']\n            except:\n                name = k+1\n            html += '<button class=\"btn btn-default btn-sm {a}\" data-target=\"#{pid}\">{n}</button>\\n'.format(a=active, pid=pid, n=name)\n        html += '</div>\\n\\n'\n\n    # Go through datasets creating plots\n    for pidx, pdata in enumerate(plotdata):\n\n        # Plot ID\n        pid = pids[pidx]\n\n        # Save plot data to file\n        fdata = OrderedDict()\n        lastcats = None\n        sharedcats = True\n        for d in pdata:\n            fdata[d['name']] = OrderedDict()\n            for i, x in enumerate(d['data']):\n                if type(x) is list:\n                    fdata[d['name']][str(x[0])] = x[1]\n                    # Check to see if all categories are the same\n                    if lastcats is None:\n                        lastcats = [x[0] for x in d['data']]\n                    elif lastcats != [x[0] for x in d['data']]:\n                        sharedcats = False\n                else:\n                    try:\n                        fdata[d['name']][pconfig['categories'][i]] = x\n                    except (KeyError, IndexError):\n                        fdata[d['name']][str(i)] = x\n\n        # Custom tsv output if the x axis varies\n        if not sharedcats and config.data_format == 'tsv':\n            fout = ''\n            for d in pdata:\n                fout += \"\\t\"+\"\\t\".join([str(x[0]) for x in d['data']])\n                fout += \"\\n{}\\t\".format(d['name'])\n                fout += \"\\t\".join([str(x[1]) for x in d['data']])\n                fout += \"\\n\"\n            with io.open (os.path.join(config.data_dir, '{}.txt'.format(pid)), 'w', encoding='utf-8') as f:\n                print( fout.encode('utf-8', 'ignore').decode('utf-8'), file=f )\n        else:\n            util_functions.write_data_file(fdata, pid)\n\n        # Set up figure\n        fig = plt.figure(figsize=(14, 6), frameon=False)\n        axes = fig.add_subplot(111)\n\n        # Go through data series\n        for idx, d in enumerate(pdata):\n\n            # Default colour index\n            cidx = idx\n            while cidx >= len(default_colors):\n                cidx -= len(default_colors)\n\n            # Line style\n            linestyle = 'solid'\n            if d.get('dashStyle', None) == 'Dash':\n                linestyle = 'dashed'\n\n            # Reformat data (again)\n            try:\n                axes.plot([x[0] for x in d['data']], [x[1] for x in d['data']], label=d['name'], color=d.get('color', default_colors[cidx]), linestyle=linestyle, linewidth=1, marker=None)\n            except TypeError:\n                # Categorical data on x axis\n                axes.plot(d['data'], label=d['name'], color=d.get('color', default_colors[cidx]), linewidth=1, marker=None)\n\n        # Tidy up axes\n        axes.tick_params(labelsize=8, direction='out', left=False, right=False, top=False, bottom=False)\n        axes.set_xlabel(pconfig.get('xlab', ''))\n        axes.set_ylabel(pconfig.get('ylab', ''))\n\n        # Dataset specific y label\n        try:\n            axes.set_ylabel(pconfig['data_labels'][pidx]['ylab'])\n        except:\n            pass\n\n        # Axis limits\n        default_ylimits = axes.get_ylim()\n        ymin = default_ylimits[0]\n        if 'ymin' in pconfig:\n            ymin = pconfig['ymin']\n        elif 'yFloor' in pconfig:\n            ymin = max(pconfig['yFloor'], default_ylimits[0])\n        ymax = default_ylimits[1]\n        if 'ymax' in pconfig:\n            ymax = pconfig['ymax']\n        elif 'yCeiling' in pconfig:\n            ymax = min(pconfig['yCeiling'], default_ylimits[1])\n        if (ymax - ymin) < pconfig.get('yMinRange', 0):\n            ymax = ymin + pconfig['yMinRange']\n        axes.set_ylim((ymin, ymax))\n\n        # Dataset specific ymax\n        try:\n            axes.set_ylim((ymin, pconfig['data_labels'][pidx]['ymax']))\n        except:\n            pass\n\n        default_xlimits = axes.get_xlim()\n        xmin = default_xlimits[0]\n        if 'xmin' in pconfig:\n            xmin = pconfig['xmin']\n        elif 'xFloor' in pconfig:\n            xmin = max(pconfig['xFloor'], default_xlimits[0])\n        xmax = default_xlimits[1]\n        if 'xmax' in pconfig:\n            xmax = pconfig['xmax']\n        elif 'xCeiling' in pconfig:\n            xmax = min(pconfig['xCeiling'], default_xlimits[1])\n        if (xmax - xmin) < pconfig.get('xMinRange', 0):\n            xmax = xmin + pconfig['xMinRange']\n        axes.set_xlim((xmin, xmax))\n\n        # Plot title\n        if 'title' in pconfig:\n            plt.text(0.5, 1.05, pconfig['title'], horizontalalignment='center', fontsize=16, transform=axes.transAxes)\n        axes.grid(True, zorder=10, which='both', axis='y', linestyle='-', color='#dedede', linewidth=1)\n\n        # X axis categories, if specified\n        if 'categories' in pconfig:\n            axes.set_xticks([i for i,v in enumerate(pconfig['categories'])])\n            axes.set_xticklabels(pconfig['categories'])\n\n        # Axis lines\n        xlim = axes.get_xlim()\n        axes.plot([xlim[0], xlim[1]], [0, 0], linestyle='-', color='#dedede', linewidth=2)\n        axes.set_axisbelow(True)\n        axes.spines['right'].set_visible(False)\n        axes.spines['top'].set_visible(False)\n        axes.spines['bottom'].set_visible(False)\n        axes.spines['left'].set_visible(False)\n\n        # Background colours, if specified\n        if 'yPlotBands' in pconfig:\n            xlim = axes.get_xlim()\n            for pb in pconfig['yPlotBands']:\n                axes.barh(pb['from'], xlim[1], height = pb['to']-pb['from'], left=xlim[0], color=pb['color'], linewidth=0, zorder=0)\n        if 'xPlotBands' in pconfig:\n            ylim = axes.get_ylim()\n            for pb in pconfig['xPlotBands']:\n                axes.bar(pb['from'], ylim[1], width = pb['to']-pb['from'], bottom=ylim[0], color=pb['color'], linewidth=0, zorder=0)\n\n        # Tight layout - makes sure that legend fits in and stuff\n        if len(pdata) <= 15:\n            axes.legend(loc='lower center', bbox_to_anchor=(0, -0.22, 1, .102), ncol=5, mode='expand', fontsize=8, frameon=False)\n            plt.tight_layout(rect=[0,0.08,1,0.92])\n        else:\n            plt.tight_layout(rect=[0,0,1,0.92])\n\n        # Should this plot be hidden on report load?\n        hidediv = ''\n        if pidx > 0:\n            hidediv = ' style=\"display:none;\"'\n\n        # Save the plot to the data directory if export is requests\n        if config.export_plots:\n            for fformat in config.export_plot_formats:\n                # Make the directory if it doesn't already exist\n                plot_dir = os.path.join(config.plots_dir, fformat)\n                if not os.path.exists(plot_dir):\n                    os.makedirs(plot_dir)\n                # Save the plot\n                plot_fn = os.path.join(plot_dir, '{}.{}'.format(pid, fformat))\n                fig.savefig(plot_fn, format=fformat, bbox_inches='tight')\n\n        # Output the figure to a base64 encoded string\n        if getattr(get_template_mod(), 'base64_plots', True) is True:\n            img_buffer = io.BytesIO()\n            fig.savefig(img_buffer, format='png', bbox_inches='tight')\n            b64_img = base64.b64encode(img_buffer.getvalue()).decode('utf8')\n            img_buffer.close()\n            html += '<div class=\"mqc_mplplot\" id=\"{}\"{}><img src=\"data:image/png;base64,{}\" /></div>'.format(pid, hidediv, b64_img)\n\n        # Save to a file and link <img>\n        else:\n            plot_relpath = os.path.join(config.plots_dir_name, 'png', '{}.png'.format(pid))\n            html += '<div class=\"mqc_mplplot\" id=\"{}\"{}><img src=\"{}\" /></div>'.format(pid, hidediv, plot_relpath)\n\n        plt.close(fig)\n\n\n    # Close wrapping div\n    html += '</div>'\n\n    report.num_mpl_plots += 1\n\n    return html", "response": "Plot a line graph with Matplot lib and return a HTML string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef smooth_line_data(data, numpoints, sumcounts=True):\n    smoothed = {}\n    for s_name, d in data.items():\n\n        # Check that we need to smooth this data\n        if len(d) <= numpoints:\n            smoothed[s_name] = d\n            continue\n\n        smoothed[s_name] = OrderedDict();\n        p = 0\n        binsize = len(d) / numpoints\n        if binsize < 1:\n            binsize = 1\n        binvals = []\n        for x in sorted(d):\n            y = d[x]\n            if p < binsize:\n                binvals.append(y)\n                p += 1\n            else:\n                if sumcounts is True:\n                    v = sum(binvals)\n                else:\n                    v = sum(binvals) / binsize\n                smoothed[s_name][x] = v\n                p = 0\n                binvals = []\n    return smoothed", "response": "Function to take an x - y dataset and use binning to calculate the maximum number of datapoints."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_mods_docs_readme():\n\n    docs_mods = []\n\n    readme_fn = os.path.join( os.path.dirname(config.MULTIQC_DIR), 'docs', 'README.md')\n    if not os.path.isfile(readme_fn):\n        if os.environ.get('TRAVIS_BUILD_DIR') is not None:\n            readme_fn = os.path.join( os.environ.get('TRAVIS_BUILD_DIR'), 'docs', 'README.md')\n        else:\n            logger.warn(\"Can't check docs readme in lint test as file doesn't exist: {}\".format(readme_fn))\n            return None\n    with open(readme_fn) as f:\n        fm = next(yaml.load_all(f, Loader=yaml.SafeLoader))\n\n    for section in fm['MultiQC Modules']:\n        for name, fn in fm['MultiQC Modules'][section].items():\n            # remove modules/ and .md\n            docs_mods.append(fn[8:-3])\n\n    # Check that installed modules are listed in docs/README.md\n    for m in config.avail_modules.keys():\n        if m not in docs_mods and m != 'custom_content':\n            errmsg = \"LINT: Module '{}' found in installed modules, but not docs/README.md\".format(m)\n            logger.error(errmsg)\n            report.lint_errors.append(errmsg)\n\n    # Check that modules in docs/README.md are installed\n    for m in docs_mods:\n        if m not in config.avail_modules.keys() and m != 'custom_content':\n            errmsg = \"LINT: Module '{}' found in docs/README.md, but not installed modules\".format(m)\n            logger.error(errmsg)\n            report.lint_errors.append(errmsg)", "response": "Check that all modules in the README. md are listed in the YAML index\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds Picard ValidateSamFile reports and parse their data based on the type of report we have found and return the number of parsed reports", "response": "def parse_reports(self):\n    \"\"\"\n        Find Picard ValidateSamFile reports and parse their data based on wether we\n        think it's a VERBOSE or SUMMARY report\n    \"\"\"\n\n    # Get data\n    data = _parse_reports_by_type(self)\n\n    if data:\n        #  Filter to strip out ignored sample names (REQUIRED)\n        data = self.ignore_samples(data)\n\n        # Populate the general stats table\n        _add_data_to_general_stats(self, data)\n\n        # Add any found data to the report\n        _add_section_to_report(self, data)\n\n        # Write parsed data to a file\n        self.write_data_file(data, 'multiqc_picard_validatesamfile')\n\n    self.picard_ValidateSamFile_data = data  # Seems like the right thing to do\n    return len(data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dictionary of all reports based on the type of error found verbose or summary.", "response": "def _parse_reports_by_type(self):\n    \"\"\" Returns a data dictionary\n\n        Goes through logs and parses them based on 'No errors found', VERBOSE or SUMMARY type.\n    \"\"\"\n\n    data = dict()\n\n    for file_meta in self.find_log_files('picard/sam_file_validation', filehandles=True):\n        sample = file_meta['s_name']\n\n        if sample in data:\n            log.debug(\"Duplicate sample name found! Overwriting: {}\".format(sample))\n\n        filehandle = file_meta['f']\n        first_line = filehandle.readline().rstrip()\n        filehandle.seek(0)  # Rewind reading of the file\n\n        if 'No errors found' in first_line:\n            sample_data = _parse_no_error_report()\n        elif first_line.startswith('ERROR') or first_line.startswith('WARNING'):\n            sample_data = _parse_verbose_report(filehandle)\n        else:\n            sample_data = _parse_summary_report(filehandle)\n\n        data[sample] = sample_data\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _histogram_data(iterator):\n    histogram_started = False\n    header_passed = False\n    for l in iterator:\n        if '## HISTOGRAM' in l:\n            histogram_started = True\n        elif histogram_started:\n            if header_passed:\n                values = l.rstrip().split(\"\\t\")\n                problem_type, name = values[0].split(':')\n                yield problem_type, name, int(values[1])\n            elif l.startswith('Error Type'):\n                header_passed = True", "response": "Yields only the row contents that contain the histogram entries"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding found data to the report as a section of HTML.", "response": "def _add_section_to_report(self, data):\n    \"\"\"\n        Adds found data to the report via several HTML generators\n    \"\"\"\n\n    # Count samples that have errors and/or warnings\n    pass_count = error_count = only_warning_count = 0\n    for sample_data in data.values():\n        if sample_data['file_validation_status'] == 'fail':\n            error_count += 1\n        elif sample_data['file_validation_status'] == 'warn':\n            only_warning_count += 1\n        else:\n            pass_count += 1\n\n    # Add overview note\n    plot_html = []\n    note_html = _generate_overview_note(\n                        pass_count=pass_count,\n                        only_warning_count=only_warning_count,\n                        error_count=error_count,\n                        total_count=len(data)\n                    )\n    plot_html.append(note_html)\n\n    # Add the detailed table, but only if we have something to show\n    if error_count or only_warning_count:\n        table_html = _generate_detailed_table(data)\n        plot_html.append(table_html)\n\n    # Finally, add the html to the report as a section\n    self.add_section(\n            name='SAM/BAM File Validation',\n            anchor='picard_validatesamfile',\n            description=('This tool reports on the validity of a SAM or BAM '\n                         'file relative to the SAM-format specification.'),\n            helptext='''\n            A detailed table is only shown if errors or warnings are found. Details about the errors and warnings are only shown if a `SUMMARY` report was parsed.\n\n            For more information on the warnings, errors and possible fixes please read [this broadinstitute article](https://software.broadinstitute.org/gatk/documentation/article.php?id=7571).''',\n            plot=\"\\n\".join(plot_html),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding data for the general stats in a Picard - module specific manner", "response": "def _add_data_to_general_stats(self, data):\n    \"\"\"\n        Add data for the general stats in a Picard-module specific manner\n    \"\"\"\n    headers = _get_general_stats_headers()\n\n    self.general_stats_headers.update(headers)\n\n    header_names = ('ERROR_count', 'WARNING_count', 'file_validation_status')\n\n    general_data = dict()\n    for sample in data:\n        general_data[sample] = {column: data[sample][column] for column in header_names}\n        if sample not in self.general_stats_data:\n            self.general_stats_data[sample] = dict()\n        if data[sample]['file_validation_status'] != 'pass':\n            headers['file_validation_status']['hidden'] = False\n        self.general_stats_data[sample].update(general_data[sample])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate and returns an overview HTML note that provides a summary of validation status.", "response": "def _generate_overview_note(pass_count, only_warning_count, error_count, total_count):\n    \"\"\" Generates and returns the HTML note that provides a summary of validation status. \"\"\"\n\n    note_html = ['<div class=\"progress\">']\n    pbars = [\n        [ float(error_count), 'danger', 'had errors' ],\n        [ float(only_warning_count), 'warning', 'had warnings' ],\n        [ float(pass_count), 'success', 'passed' ]\n    ]\n    for b in pbars:\n        if b[0]:\n            note_html.append(\n                '<div class=\"progress-bar progress-bar-{pbcol}\" style=\"width: {pct}%\" data-toggle=\"tooltip\" title=\"{count} {sample} {txt}\">{count}</div>'. \\\n                format(\n                    pbcol = b[1],\n                    count = int(b[0]),\n                    pct = (b[0]/float(total_count))*100.0,\n                    txt = b[2],\n                    sample = 'samples' if b[0] > 1 else 'sample'\n                )\n            )\n    note_html.append('</div>')\n\n    return \"\\n\".join(note_html)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _generate_detailed_table(data):\n    headers = _get_general_stats_headers()\n\n    # Only add headers for errors/warnings we have found\n    for problems in data.values():\n        for problem in problems:\n            if problem not in headers and problem in WARNING_DESCRIPTIONS:\n                headers['WARNING_count']['hidden'] = False\n                headers[problem] = {\n                    'description': WARNING_DESCRIPTIONS[problem],\n                    'namespace': 'WARNING',\n                    'scale': headers['WARNING_count']['scale'],\n                    'format': '{:.0f}',\n                    'shared_key': 'warnings',\n                    'hidden': True,  # Hide by default; to unclutter things.\n                }\n            if problem not in headers and problem in ERROR_DESCRIPTIONS:\n                headers['ERROR_count']['hidden'] = False\n                headers[problem] = {\n                    'description': ERROR_DESCRIPTIONS[problem],\n                    'namespace': 'ERROR',\n                    'scale': headers['ERROR_count']['scale'],\n                    'format': '{:.0f}',\n                    'shared_key': 'errors',\n                    'hidden': True,  # Hide by default; to unclutter things.\n                }\n\n    table_config = {\n        'table_title': 'Picard: SAM/BAM File Validation',\n    }\n\n    return table.plot(data=data, headers=headers, pconfig=table_config)", "response": "Generates and returns the HTML table that overviews the details found."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef busco_plot (self, lin):\n\n        data = {}\n        for s_name in self.busco_data:\n            if self.busco_data[s_name].get('lineage_dataset') == lin:\n                data[s_name] = self.busco_data[s_name]\n\n        plot_keys = ['complete_single_copy','complete_duplicated','fragmented','missing']\n        plot_cols = ['#7CB5EC', '#434348', '#F7A35C', '#FF3C50']\n        keys = OrderedDict()\n        for k, col in zip(plot_keys, plot_cols):\n            keys[k] = {'name': self.busco_keys[k], 'color': col}\n\n        # Config for the plot\n        config = {\n            'id': 'busco_plot_{}'.format(re.sub('\\W+', '_', str(lin))),\n            'title': 'BUSCO: Assessment Results' if lin is None else 'BUSCO Assessment Results: {}'.format(lin),\n            'ylab': '# BUSCOs',\n            'cpswitch_counts_label': 'Number of BUSCOs'\n        }\n\n        return bargraph.plot(data, keys, config)", "response": "Make the HTML for the BUSCO plot for a particular lineage"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake the HighCharts HTML to plot the trimmomatic rates", "response": "def trimmomatic_barplot (self):\n        \"\"\" Make the HighCharts HTML to plot the trimmomatic rates \"\"\"\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['surviving'] =              { 'color': '#437bb1', 'name': 'Surviving Reads' }\n        keys['both_surviving'] =         { 'color': '#f7a35c', 'name': 'Both Surviving' }\n        keys['forward_only_surviving'] = { 'color': '#e63491', 'name': 'Forward Only Surviving' }\n        keys['reverse_only_surviving'] = { 'color': '#b1084c', 'name': 'Reverse Only Surviving' }\n        keys['dropped'] =                { 'color': '#7f0000', 'name': 'Dropped' }\n\n        # Config for the plot\n        pconfig = {\n            'id': 'trimmomatic_plot',\n            'title': 'Trimmomatic: Surviving Reads',\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads'\n        }\n\n        self.add_section( plot = bargraph.plot(self.trimmomatic, keys, pconfig) )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngoing through the peddy output file looking for peddy output", "response": "def parse_peddy_summary(self, f):\n        \"\"\" Go through log file looking for peddy output \"\"\"\n        parsed_data = dict()\n        headers = None\n        for l in f['f'].splitlines():\n            s = l.split(\"\\t\")\n            if headers is None:\n                s[0] = s[0].lstrip('#')\n                headers = s\n            else:\n                parsed_data[s[1]] = dict()\n                for i, v in enumerate(s):\n                    if i != 1:\n                        try:\n                            parsed_data[s[1]][headers[i]] = float(v)\n                        except ValueError:\n                            parsed_data[s[1]][headers[i]] = v\n        if len(parsed_data) == 0:\n            return None\n        return parsed_data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses Peddy output from Peddy.", "response": "def parse_peddy_csv(self, f, pattern):\n        \"\"\" Parse csv output from peddy \"\"\"\n        parsed_data = dict()\n        headers = None\n        s_name_idx = None\n        for l in f['f'].splitlines():\n            s = l.split(\",\")\n            if headers is None:\n                headers = s\n                try:\n                    s_name_idx = [headers.index(\"sample_id\")]\n                except ValueError:\n                    try:\n                        s_name_idx = [headers.index(\"sample_a\"), headers.index(\"sample_b\")]\n                    except ValueError:\n                        log.warn(\"Could not find sample name in Peddy output: {}\".format(f['fn']))\n                        return None\n            else:\n                s_name = '-'.join([s[idx] for idx in s_name_idx])\n                parsed_data[s_name] = dict()\n                for i, v in enumerate(s):\n                    if i not in s_name_idx:\n                        if headers[i] == \"error\" and pattern == \"sex_check\":\n                            v = \"True\" if v == \"False\" else \"False\"\n                        try:\n                            # add the pattern as a suffix to key\n                            parsed_data[s_name][headers[i] + \"_\" + pattern] = float(v)\n                        except ValueError:\n                            # add the pattern as a suffix to key\n                            parsed_data[s_name][headers[i] + \"_\" + pattern] = v\n        if len(parsed_data) == 0:\n            return None\n        return parsed_data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntakes the parsed stats from the Peddy report and add it to the basic stats table at the top of the report", "response": "def peddy_general_stats_table(self):\n        \"\"\" Take the parsed stats from the Peddy report and add it to the\n        basic stats table at the top of the report \"\"\"\n\n        family_ids = [ x.get('family_id') for x in self.peddy_data.values() ]\n\n        headers = OrderedDict()\n        headers['family_id'] = {\n            'title': 'Family ID',\n            'hidden': True if all([v == family_ids[0] for v in family_ids]) else False\n        }\n        headers['ancestry-prediction'] = {\n            'title': 'Ancestry',\n            'description': 'Ancestry Prediction',\n        }\n        headers['ancestry-prob_het_check'] = {\n            'title': 'P(Ancestry)',\n            'description': 'Probability predicted ancestry is correct.'\n        }\n        headers['sex_het_ratio'] = {\n            'title': 'Sex / Het Ratio',\n        }\n        headers['error_sex_check'] = {\n            'title': 'Correct Sex',\n            'description': 'Displays False if error in sample sex prediction',\n        }\n        headers['predicted_sex_sex_check'] = {\n            'title': 'Sex',\n            'description': 'Predicted sex'\n        }\n        self.general_stats_addcols(self.peddy_data, headers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef peddy_het_check_plot(self):\n        # empty dictionary to add sample names, and dictionary of values\n        data = {}\n\n        # for each sample, and list in self.peddy_data\n        for s_name, d in self.peddy_data.items():\n            # check the sample contains the required columns\n            if 'median_depth_het_check' in d and 'het_ratio_het_check' in d:\n                # add sample to dictionary with value as a dictionary of points to plot\n                data[s_name] = {\n                    'x': d['median_depth_het_check'],\n                    'y': d['het_ratio_het_check']\n                }\n\n        pconfig = {\n            'id': 'peddy_het_check_plot',\n            'title': 'Peddy: Het Check',\n            'xlab': 'median depth',\n            'ylab': 'proportion het calls',\n        }\n\n        self.add_section (\n            name = 'Het Check',\n            description = \"Proportion of sites that were heterozygous against median depth.\",\n            helptext = \"\"\"\n            A high proportion of heterozygous sites suggests contamination, a low proportion suggests consanguinity.\n            \n            See [the main peddy documentation](https://peddy.readthedocs.io/en/latest/output.html#het-check) for more details about the `het_check` command.\n            \"\"\",\n            anchor = 'peddy-hetcheck-plot',\n            plot = scatter.plot(data, pconfig)\n        )", "response": "add the het_check scatter plot"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind Picard TargetedPcrMetrics reports and parse their data", "response": "def parse_reports(self):\n    \"\"\" Find Picard TargetedPcrMetrics reports and parse their data \"\"\"\n\n    # Set up vars\n    self.picard_pcrmetrics_data = dict()\n    self.picard_pcrmetrics_samplestats = dict()\n\n    # Go through logs and find Metrics\n    for f in self.find_log_files('picard/pcr_metrics', filehandles=True):\n        s_name = None\n        for l in f['f']:\n            # New log starting\n            if 'TargetedPcrMetrics' in l and 'INPUT' in l:\n                s_name = None\n                # Pull sample name from input\n                fn_search = re.search(r\"INPUT(?:=|\\s+)(\\[?[^\\s]+\\]?)\", l, flags=re.IGNORECASE)\n                if fn_search:\n                    s_name = os.path.basename(fn_search.group(1).strip('[]'))\n                    s_name = self.clean_s_name(s_name, f['root'])\n\n            if s_name is not None:\n                if 'TargetedPcrMetrics' in l and '## METRICS CLASS' in l:\n                    keys = f['f'].readline().strip(\"\\n\").split(\"\\t\")\n                    vals = f['f'].readline().strip(\"\\n\").split(\"\\t\")\n                    if len(vals) == len(keys):\n                        if s_name in self.picard_pcrmetrics_data:\n                            log.debug(\"Duplicate sample name found in {}! Overwriting: {}\".format(f['fn'], s_name))\n                        self.add_data_source(f, s_name, section='TargetedPcrMetrics')\n                        self.picard_pcrmetrics_data[s_name] = dict()\n                        for i, k in enumerate(keys):\n                            try:\n                                # Multiply percentages by 100\n                                if k.startswith('PCT_'):\n                                    vals[i] = float(vals[i]) * 100.0\n                                self.picard_pcrmetrics_data[s_name][k] = float(vals[i])\n                            except ValueError:\n                                self.picard_pcrmetrics_data[s_name][k] = vals[i]\n\n    # Filter to strip out ignored sample names\n    self.picard_pcrmetrics_data = self.ignore_samples(self.picard_pcrmetrics_data)\n\n    if len(self.picard_pcrmetrics_data) > 0:\n\n        # Write parsed data to a file\n        self.write_data_file(self.picard_pcrmetrics_data, 'multiqc_picard_pcrmetrics')\n\n        # Add to general stats table\n        self.general_stats_headers['PCT_AMPLIFIED_BASES'] = {\n            'title': '% Amplified Bases',\n            'description': 'The fraction of aligned bases that mapped to or near an amplicon.',\n            'min': 0,\n            'max': 100,\n            'suffix': '%',\n            'scale': 'BrBG'\n        }\n        self.general_stats_headers['MEDIAN_TARGET_COVERAGE'] = {\n            'title': 'Median Target Coverage',\n            'description': 'The median coverage of reads that mapped to target regions of an experiment.',\n            'min': 0,\n            'suffix': 'X',\n            'scale': 'GnBu',\n        }\n\n        for s_name in self.picard_pcrmetrics_data:\n            if s_name not in self.general_stats_data:\n                self.general_stats_data[s_name] = dict()\n            self.general_stats_data[s_name].update( self.picard_pcrmetrics_data[s_name] )\n\n        # Bar plot of ignored bases\n        keys = OrderedDict()\n        keys['ON_AMPLICON_BASES'] = {'name': 'On-amplicon bases'}\n        keys['NEAR_AMPLICON_BASES'] = {'name': 'Near-amplicon bases'}\n        keys['OFF_AMPLICON_BASES'] = {'name': 'Off-amplicon bases', 'color': '#f28f43'}\n\n        # Config for the plot\n        pconfig = {\n            'id': 'picard_pcr_metrics_bases',\n            'title': 'Picard: PCR Amplicon Bases',\n            'ylab': '# Bases',\n            'cpswitch_counts_label': '# Bases',\n            'hide_zero_cats': False\n        }\n\n        self.add_section (\n            name = 'PCR Amplicon Bases',\n            anchor = 'picard-pcrmetrics-bases',\n            description = 'Metrics about reads obtained from targeted PCR experiments.',\n            helptext = '''\n            This plot shows the number of bases aligned on or near to amplified regions of the genome.\n\n            * `ON_AMPLICON_BASES`: The number of `PF_BASES_ALIGNED` that mapped to an amplified region of the genome.\n            * `NEAR_AMPLICON_BASES`: The number of `PF_BASES_ALIGNED` that mapped to within a fixed interval of an amplified region, but not on a baited region.\n            * `OFF_AMPLICON_BASES`: The number of `PF_BASES_ALIGNED` that mapped neither on or near an amplicon.\n\n            For more information see the [Picard documentation](https://broadinstitute.github.io/picard/picard-metric-definitions.html#TargetedPcrMetrics).''',\n            plot = bargraph.plot(self.picard_pcrmetrics_data, keys, pconfig)\n        )\n\n\n    # Return the number of detected samples to the parent module\n    return len(self.picard_pcrmetrics_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates the Samblaster bar plot.", "response": "def add_barplot(self):\n        \"\"\" Generate the Samblaster bar plot. \"\"\"\n        cats = OrderedDict()\n        cats['n_nondups'] = {'name': 'Non-duplicates'}\n        cats['n_dups'] = {'name': 'Duplicates'}\n\n        pconfig = {\n            'id': 'samblaster_duplicates',\n            'title': 'Samblaster: Number of duplicate reads',\n            'ylab': 'Number of reads'\n        }\n        self.add_section( plot = bargraph.plot(self.samblaster_data, cats, pconfig) )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_samblaster(self, f):\n        dups_regex = \"samblaster: (Removed|Marked) (\\d+) of (\\d+) \\((\\d+.\\d+)%\\) read ids as duplicates\"\n        input_file_regex = \"samblaster: Opening (\\S+) for read.\"\n        rgtag_name_regex = \"\\\\\\\\tID:(\\S*?)\\\\\\\\t\"\n        data = {}\n        s_name = None\n        fh = f['f']\n        for l in fh:\n            # try to find name from RG-tag. If bwa mem is used upstream samblaster with pipes, then the bwa mem command\n            # including the read group will be written in the log\n            match = re.search(rgtag_name_regex, l)\n            if match:\n                s_name = self.clean_s_name( match.group(1), f['root'])\n\n            # try to find name from the input file name, if used\n            match = re.search(input_file_regex, l)\n            if match:\n                basefn = os.path.basename(match.group(1))\n                fname, ext = os.path.splitext(basefn)\n                # if it's stdin, then try bwa RG-tag instead\n                if fname != 'stdin':\n                    s_name = self.clean_s_name( fname, f['root'])\n\n            match = re.search(dups_regex, l)\n            if match:\n                data['n_dups'] = int(match.group(2))\n                data['n_tot'] = int(match.group(3))\n                data['n_nondups'] = data['n_tot'] - data['n_dups']\n                data['pct_dups'] = float(match.group(4))\n\n        if s_name is None:\n            s_name = f['s_name']\n\n        if len(data) > 0:\n            if s_name in self.samblaster_data:\n                log.debug(\"Duplicate sample name found in {}! Overwriting: {}\".format(f['fn'], s_name))\n            self.add_data_source(f, s_name)\n            self.samblaster_data[s_name] = data", "response": "Go through the log file looking for samblaster output. If the samblaster output is not found the samblaster output is returned."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef robust_rmtree(path, logger=None, max_retries=10):\n\n    for i in range(max_retries):\n        try:\n            shutil.rmtree(path)\n            return\n        except OSError:\n            if logger:\n                logger.info('Unable to remove path: {}'.format(path))\n                logger.info('Retrying after {} seconds'.format(i**2))\n            else:\n                print('Unable to remove path: {}'.format(path), file=sys.stderr)\n                print('Retrying after {} seconds'.format(i**2), file=sys.stderr)\n            time.sleep(i**2)\n\n    # Final attempt, pass any Exceptions up to caller.\n    shutil.rmtree(path)", "response": "Robustly deletes a path."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites a 2D dict to the report directory. Will not do anything.", "response": "def write_data_file(data, fn, sort_cols=False, data_format=None):\n    \"\"\" Write a data file to the report directory. Will not do anything\n    if config.data_dir is not set.\n    :param: data - a 2D dict, first key sample name (row header),\n            second key field (column header).\n    :param: fn - Desired filename. Directory will be prepended automatically.\n    :param: sort_cols - Sort columns alphabetically\n    :param: data_format - Output format. Defaults to config.data_format (usually tsv)\n    :return: None \"\"\"\n\n    if config.data_dir is not None:\n\n        # Add relevant file extension to filename\n        if data_format is None:\n            data_format = config.data_format\n        fn = '{}.{}'.format(fn, config.data_format_extensions[data_format])\n\n        # JSON encoder class to handle lambda functions\n        class MQCJSONEncoder(json.JSONEncoder):\n            def default(self, obj):\n                if callable(obj):\n                    try:\n                        return obj(1)\n                    except:\n                        return None\n                return json.JSONEncoder.default(self, obj)\n\n        # Save file\n        with io.open (os.path.join(config.data_dir, fn), 'w', encoding='utf-8') as f:\n            if data_format == 'json':\n                jsonstr = json.dumps(data, indent=4, cls=MQCJSONEncoder, ensure_ascii=False)\n                print( jsonstr.encode('utf-8', 'ignore').decode('utf-8'), file=f)\n            elif data_format == 'yaml':\n                yaml.dump(data, f, default_flow_style=False)\n            else:\n                # Default - tab separated output\n                # Get all headers\n                h = ['Sample']\n                for sn in sorted(data.keys()):\n                    for k in data[sn].keys():\n                        if type(data[sn][k]) is not dict and k not in h:\n                            h.append(str(k))\n                if sort_cols:\n                    h = sorted(h)\n\n                # Get the rows\n                rows = [ \"\\t\".join(h) ]\n                for sn in sorted(data.keys()):\n                    # Make a list starting with the sample name, then each field in order of the header cols\n                    l = [str(sn)] + [ str(data[sn].get(k, '')) for k in h[1:] ]\n                    rows.append( \"\\t\".join(l) )\n\n                body = '\\n'.join(rows)\n\n                print( body.encode('utf-8', 'ignore').decode('utf-8'), file=f)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef view_all_tags(ctx, param, value):\n    # To make sure this function executed only when the flag was called\n    if not value or ctx.resilient_parsing:\n        return\n    avail_tags = dict()\n    print(\"\\nMultiQC Available module tag groups:\\n\")\n    for mod_dict in filter(lambda mod:isinstance(mod, dict), config.module_order):\n        mod_key, mod_val = list(mod_dict.items())[0]\n        tags = list(mod_val.get('module_tag', []))\n        for t in tags:\n            if t not in avail_tags:\n                avail_tags[t] = []\n            avail_tags[t].append(mod_key)\n    for t in sorted(avail_tags.keys(), key=lambda s: s.lower()):\n        print (\" - {}:\".format(t))\n        for ttgs in avail_tags[t]:\n            print (\"   - {}\".format(ttgs))\n    ctx.exit()", "response": "List available tags and associated modules\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _short_chrom(self, chrom):\n        default_allowed = set([\"X\"])\n        allowed_chroms = set(getattr(config, \"goleft_indexcov_config\", {}).get(\"chromosomes\", []))\n\n        chrom_clean = chrom.replace(\"chr\", \"\")\n        try:\n            chrom_clean = int(chrom_clean)\n        except ValueError:\n            if chrom_clean not in default_allowed and chrom_clean not in allowed_chroms:\n                chrom_clean = None\n\n        if allowed_chroms:\n            if chrom in allowed_chroms or chrom_clean in allowed_chroms:\n                return chrom_clean\n        elif isinstance(chrom_clean, int) or chrom_clean in default_allowed:\n            return chrom_clean", "response": "Plot standard chromosomes + X sorted numerically."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_conpair_logs(self, f):\n\n        conpair_regexes = {\n            'concordance_concordance': r\"Concordance: ([\\d\\.]+)%\",\n            'concordance_used_markers': r\"Based on (\\d+)/\\d+ markers\",\n            'concordance_total_markers': r\"Based on \\d+/(\\d+) markers\",\n            'concordance_marker_threshold': r\"\\(coverage per marker threshold : (\\d+) reads\\)\",\n            'concordance_min_mapping_quality': r\"Minimum mappinq quality: (\\d+)\",\n            'concordance_min_base_quality': r\"Minimum base quality: (\\d+)\",\n            'contamination_normal': r\"Normal sample contamination level: ([\\d\\.]+)%\",\n            'contamination_tumor': r\"Tumor sample contamination level: ([\\d\\.]+)%\"\n        }\n\n        parsed_data = {}\n        for k, r in conpair_regexes.items():\n            match = re.search(r, f['f'])\n            if match:\n                parsed_data[k] = float(match.group(1))\n\n        def _cp_type(data):\n            if 'concordance_concordance' in parsed_data:\n                return 'concordance'\n            elif 'contamination_normal' in parsed_data:\n                return 'contamination'\n\n        if len(parsed_data) > 0:\n            if f['s_name'] in self.conpair_data:\n                if(_cp_type(self.conpair_data[f['s_name']]) == _cp_type(parsed_data)):\n                    log.debug(\"Duplicate sample name found! Overwriting: {}\".format(f['s_name']))\n            else:\n                self.conpair_data[f['s_name']] = dict()\n            self.add_data_source(f, section=_cp_type(parsed_data))\n            self.conpair_data[f['s_name']].update(parsed_data)", "response": "Parse the conpair output file looking for concordance or contamination output and adding the data to self. conpair_data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntaking the parsed stats from the Conpair report and add it to the basic stats table at the top of the report", "response": "def conpair_general_stats_table(self):\n        \"\"\" Take the parsed stats from the Conpair report and add it to the\n        basic stats table at the top of the report \"\"\"\n\n        headers = {}\n        headers['concordance_concordance'] = {\n            'title': 'Concordance',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'format': '{:,.2f}',\n            'scale': 'RdYlGn'\n        }\n        headers['contamination_normal'] = {\n            'title': 'N Contamination',\n            'description': 'Normal sample contamination level',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'format': '{:,.3f}',\n            'scale': 'RdYlBu-rev'\n        }\n        headers['contamination_tumor'] = {\n            'title': 'T Contamination',\n            'description': 'Tumor sample contamination level',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'format': '{:,.3f}',\n            'scale': 'RdYlBu-rev'\n        }\n        self.general_stats_addcols(self.conpair_data, headers)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind plotCoverage output. Both stdout and outRawCounts.", "response": "def parse_plotCoverage(self):\n        \"\"\"Find plotCoverage output. Both stdout and --outRawCounts\"\"\"\n        self.deeptools_plotCoverageStdout = dict()\n        for f in self.find_log_files('deeptools/plotCoverageStdout'):\n            parsed_data = self.parsePlotCoverageStdout(f)\n            for k, v in parsed_data.items():\n                if k in self.deeptools_plotCoverageStdout:\n                    log.warning(\"Replacing duplicate sample {}.\".format(k))\n                self.deeptools_plotCoverageStdout[k] = v\n\n            if len(parsed_data) > 0:\n                self.add_data_source(f, section='plotCoverage')\n\n        self.deeptools_plotCoverageOutRawCounts= dict()\n        for f in self.find_log_files('deeptools/plotCoverageOutRawCounts'):\n            parsed_data = self.parsePlotCoverageOutRawCounts(f)\n            for k, v in parsed_data.items():\n                if k in self.deeptools_plotCoverageOutRawCounts:\n                    log.warning(\"Replacing duplicate sample {}.\".format(k))\n                self.deeptools_plotCoverageOutRawCounts[k] = v\n\n            if len(parsed_data) > 0:\n                self.add_data_source(f, section='plotCoverage')\n\n        if len(self.deeptools_plotCoverageStdout) > 0:\n            header = OrderedDict()\n            header[\"min\"] = {\n                'title': 'Min',\n                'description': 'Minimum Coverage',\n                'shared_key': 'coverage'\n            }\n            header[\"25%\"] = {\n                'rid': 'first_quartile',\n                'title': '1st Quartile',\n                'description': 'First quartile coverage',\n                'shared_key': 'coverage'\n            }\n            header[\"50%\"] = {\n                'rid': 'median',\n                'title': 'Median',\n                'description': 'Median coverage (second quartile)',\n                'shared_key': 'coverage'\n            }\n            header[\"mean\"] = {\n                'title': 'Mean',\n                'description': 'Mean coverage',\n                'shared_key': 'coverage'\n            }\n            header[\"75%\"] = {\n                'rid': 'third_quartile',\n                'title': '3rd Quartile',\n                'description': 'Third quartile coverage',\n                'shared_key': 'coverage'\n            }\n            header[\"max\"] = {\n                'title': 'Max',\n                'description': 'Maximum coverage',\n                'shared_key': 'coverage'\n            }\n            header[\"std\"] = {\n                'title': 'Std. Dev.',\n                'description': 'Coverage standard deviation',\n                'shared_key': 'coverage'\n            }\n            config = {'namespace': 'deepTools plotCoverage'}\n            self.add_section(\n                name = \"Coverage metrics\",\n                anchor = \"deeptools_coverage_metrics\",\n                plot = table.plot(self.deeptools_plotCoverageStdout, header, config)\n            )\n\n        if len(self.deeptools_plotCoverageOutRawCounts) > 0:\n            config = {\n                'id': 'deeptools_coverage_metrics_plot',\n                'title': 'deepTools: Coverage distribution',\n                'xlab': 'Coverage',\n                'ylab': 'Fraction of bases sampled'\n            }\n            self.add_section(\n                name = \"Coverage distribution\",\n                anchor = \"deeptools_coverage_distribution\",\n                description = \"The fraction of bases with a given number of read/fragment coverage\",\n                plot = linegraph.plot(self.deeptools_plotCoverageOutRawCounts, config)\n            )\n\n        return len(self.deeptools_plotCoverageStdout), len(self.deeptools_plotCoverageOutRawCounts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds estimateReadFiltering output. Only the output from the log files is supported.", "response": "def parse_estimateReadFiltering(self):\n        \"\"\"Find estimateReadFiltering output. Only the output from --table is supported.\"\"\"\n        self.deeptools_estimateReadFiltering = dict()\n        for f in self.find_log_files('deeptools/estimateReadFiltering'):\n            parsed_data = self.parseEstimateReadFilteringFile(f)\n            for k, v in parsed_data.items():\n                if k in self.deeptools_estimateReadFiltering:\n                    log.warning(\"Replacing duplicate sample {}.\".format(k))\n                self.deeptools_estimateReadFiltering[k] = v\n\n            if len(parsed_data) > 0:\n                self.add_data_source(f, section='estimateReadFiltering')\n\n        if len(self.deeptools_estimateReadFiltering) > 0:\n            header = OrderedDict()\n            header[\"M Entries\"] = {\n                'title': 'M entries',\n                'description': 'Number of entries in the file (millions)'}\n            header[\"pct_Aligned\"] = {\n                'title': '% Aligned',\n                'description': 'Percent of aligned entries',\n                'scale': 'YlGn',\n                'min': 0,\n                'max': 100\n            }\n            header[\"pct_Filtered\"] = {\n                'title': '% Tot. Filtered',\n                'description': 'Percent of alignment that would be filtered for any reason.',\n                'scale': 'OrRd',\n                'min': 0,\n                'max': 100\n            }\n            header[\"pct_Blacklisted\"] = {\n                'title': '% Blacklisted',\n                'description': 'Percent of alignments falling (at least partially) inside a blacklisted region',\n                'scale': 'YlOrRd',\n                'min': 0,\n                'max': 100\n            }\n            header[\"pct_MAPQ\"] = {\n                'title': '% MAPQ', 'description':\n                'Percent of alignments having MAPQ scores below the specified threshold',\n                'scale': 'YlOrBn',\n                'min': 0,\n                'max': 100\n            }\n            header[\"pct_Missing_Flags\"] = {\n                'title': '% Missing Flags',\n                'description': 'Percent of alignments lacking at least on flag specified by --samFlagInclude',\n                'scale': 'PuRd',\n                'min': 0,\n                'max': 100\n            }\n            header[\"pct_Forbidden_Flags\"] = {\n                'title': '% Forbidden Flags',\n                'description': 'Percent of alignments having at least one flag specified by --samFlagExclude',\n                'scale': 'OrRd',\n                'min': 0,\n                'max': 100\n            }\n            header[\"pct_deepTools_Dupes\"] = {\n                'title': '% deepTools Dupes',\n                'description': 'Percent of alignments marked by deepTools as being duplicates',\n                'scale': 'PuRd',\n                'min': 0,\n                'max': 100\n            }\n            header[\"pct_Duplication\"] = {\n                'title': '% Duplication',\n                'description': 'Percent of alignments originally marked as being duplicates',\n                'scale': 'OrRd',\n                'min': 0,\n                'max': 100\n            }\n            header[\"pct_Singletons\"] = {\n                'title': '% Singletons',\n                 'description': 'Percent of alignments that are singletons (i.e., paired-end reads where the mates don\\'t align as a pair',\n                 'scale': 'OrRd',\n                 'min': 0,\n                 'max': 100\n                }\n            header[\"pct_Strand_Filtered\"] = {\n                'title': '% Strand Filtered',\n                'description': 'Percent of alignments arising from the wrong strand',\n                'scale': 'OrRd',\n                'min': 0,\n                'max': 100\n            }\n\n            tdata = dict()\n            for k, v in self.deeptools_estimateReadFiltering.items():\n                tdata[k] = {\n                    'M Entries': v['total'] / 1000000.0,\n                    'pct_Aligned': 100. * v['mapped'] / float(v['total']),\n                    'pct_Filtered': 100. * v['filtered'] / float(v['total']),\n                    'pct_Blacklisted': 100. * v['blacklisted'] / float(v['total']),\n                    'pct_Below_MAPQ': 100. * v['mapq'] / float(v['total']),\n                    'pct_Missing_Flags': 100. * v['required flags'] / float(v['total']),\n                    'pct_Forbidden_Flags': 100. * v['excluded flags'] / float(v['total']),\n                    'pct_deepTools_Dupes': 100. * v['internal dupes'] / float(v['total']),\n                    'pct_Duplication': 100. * v['dupes'] / float(v['total']),\n                    'pct_Singletons': 100. * v['singletons'] / float(v['total']),\n                    'pct_Strand_Filtered': 100. * v['strand'] / float(v['total'])\n                }\n\n            config = {'namespace': 'deepTools bamPEFragmentSize'}\n            self.add_section(\n                name = \"Filtering metrics\",\n                anchor = \"estimateReadFiltering\",\n                description = \"Estimated percentages of alignments filtered independently for each setting in `estimateReadFiltering`\",\n                plot = table.plot(tdata, header, config)\n            )\n\n        return len(self.deeptools_estimateReadFiltering)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_plotPCA(self):\n        self.deeptools_plotPCAData = dict()\n        for f in self.find_log_files('deeptools/plotPCAData', filehandles=False):\n            parsed_data = self.parsePlotPCAData(f)\n            for k, v in parsed_data.items():\n                if k in self.deeptools_plotPCAData:\n                    log.warning(\"Replacing duplicate sample {}.\".format(k))\n                self.deeptools_plotPCAData[k] = v\n            if len(parsed_data) > 0:\n                self.add_data_source(f, section='plotPCA')\n\n        if len(self.deeptools_plotPCAData) > 0:\n            config = {\n                'id': 'deeptools_pca_plot',\n                'title': 'deeptools: PCA Plot',\n                'xlab': 'PC1',\n                'ylab': 'PC2',\n                'tt_label': 'PC1 {point.x:.2f}: PC2 {point.y:.2f}',\n            }\n            data = dict()\n            for s_name in self.deeptools_plotPCAData:\n                try:\n                    data[s_name] = {'x': self.deeptools_plotPCAData[s_name][1], 'y': self.deeptools_plotPCAData[s_name][2]}\n                except KeyError:\n                    pass\n            if len(data) == 0:\n                log.debug('No valid data for PCA plot')\n                return None\n\n            self.add_section(\n                name=\"PCA plot\",\n                anchor=\"deeptools_pca\",\n                description=\"PCA plot with the top two principal components calculated based on genome-wide distribution of sequence reads\",\n                plot=scatter.plot(data, config)\n            )\n\n        return len(self.deeptools_plotPCAData)", "response": "Find plotPCA output and parse it"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading and parse a user config file in the current working directory and command line.", "response": "def mqc_load_userconfig(paths=()):\n    \"\"\" Overwrite config defaults with user config files \"\"\"\n\n    # Load and parse installation config file if we find it\n    mqc_load_config(os.path.join( os.path.dirname(MULTIQC_DIR), 'multiqc_config.yaml'))\n\n    # Load and parse a user config file if we find it\n    mqc_load_config(os.path.expanduser('~/.multiqc_config.yaml'))\n\n    # Load and parse a config file path set in an ENV variable if we find it\n    if os.environ.get('MULTIQC_CONFIG_PATH') is not None:\n        mqc_load_config( os.environ.get('MULTIQC_CONFIG_PATH') )\n\n    # Load and parse a config file in this working directory if we find it\n    mqc_load_config('multiqc_config.yaml')\n\n    # Custom command line config\n    for p in paths:\n        mqc_load_config(p)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mqc_load_config(yaml_config):\n    if os.path.isfile(yaml_config):\n        try:\n            with open(yaml_config) as f:\n                new_config = yaml.safe_load(f)\n                logger.debug(\"Loading config settings from: {}\".format(yaml_config))\n                mqc_add_config(new_config, yaml_config)\n        except (IOError, AttributeError) as e:\n            logger.debug(\"Config error: {}\".format(e))\n        except yaml.scanner.ScannerError as e:\n            logger.error(\"Error parsing config YAML: {}\".format(e))\n            sys.exit(1)\n    else:\n        logger.debug(\"No MultiQC config found: {}\".format(yaml_config))", "response": "Load and parse a config file if we find it"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a new config dict to the MultiQC global config.", "response": "def mqc_add_config(conf, conf_path=None):\n    \"\"\" Add to the global config with given MultiQC config dict \"\"\"\n    global fn_clean_exts, fn_clean_trim\n    for c, v in conf.items():\n        if c == 'sp':\n            # Merge filename patterns instead of replacing\n            sp.update(v)\n            logger.debug(\"Added to filename patterns: {}\".format(v))\n        elif c == 'extra_fn_clean_exts':\n            # Prepend to filename cleaning patterns instead of replacing\n            fn_clean_exts[0:0] = v\n            logger.debug(\"Added to filename clean extensions: {}\".format(v))\n        elif c == 'extra_fn_clean_trim':\n            # Prepend to filename cleaning patterns instead of replacing\n            fn_clean_trim[0:0] = v\n            logger.debug(\"Added to filename clean trimmings: {}\".format(v))\n        elif c in ['custom_logo'] and v:\n            # Resolve file paths - absolute or cwd, or relative to config file\n            fpath = v\n            if os.path.exists(v):\n                fpath = os.path.abspath(v)\n            elif conf_path is not None and os.path.exists(os.path.join(os.path.dirname(conf_path), v)):\n                fpath = os.path.abspath(os.path.join(os.path.dirname(conf_path), v))\n            else:\n                logger.error(\"Config '{}' path not found, skipping ({})\".format(c, fpath))\n                continue\n            logger.debug(\"New config '{}': {}\".format(c, fpath))\n            update({c: fpath})\n        else:\n            logger.debug(\"New config '{}': {}\".format(c, v))\n            update({c: v})"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_preseq_logs(f):\n\n    lines = f['f'].splitlines()\n    header = lines.pop(0)\n\n    data_is_bases = False\n    if header.startswith('TOTAL_READS\tEXPECTED_DISTINCT'):\n        pass\n    elif header.startswith('TOTAL_BASES\tEXPECTED_DISTINCT'):\n        data_is_bases = True\n    elif header.startswith('total_reads\tdistinct_reads'):\n        pass\n    else:\n        log.debug(\"First line of preseq file {} did not look right\".format(f['fn']))\n        return None, None\n\n    data = dict()\n    for l in lines:\n        s = l.split()\n        # Sometimes the Expected_distinct count drops to 0, not helpful\n        if float(s[1]) == 0 and float(s[0]) > 0:\n            continue\n        data[float(s[0])] = float(s[1])\n\n    return data, data_is_bases", "response": "Go through the log file looking for preseq output"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmodify counts or base pairs according to read_count_multiplier and base_count_multiplier.", "response": "def _modify_raw_data(sample_data, is_basepairs):\n    \"\"\" Modify counts or base pairs according to `read_count_multiplier`\n        or `base_count_multiplier`.\n    \"\"\"\n    return OrderedDict(\n        (_modify_raw_val(x, is_basepairs), _modify_raw_val(y, is_basepairs))\n        for x, y in sample_data.items()\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _modify_raw_val(val, is_basepairs):\n    return float(val) * (config.base_count_multiplier if is_basepairs else config.read_count_multiplier)", "response": "Modify raw value according to read_count_multiplier."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting raw counts to coverages.", "response": "def _counts_to_coverages(sample_data, counts_in_1x):\n    \"\"\" If the user specified read length and genome size in the config,\n        convert the raw counts/bases into the depth of coverage.\n    \"\"\"\n    if not counts_in_1x:\n        return {None: None}\n\n    return OrderedDict(\n        (_count_to_coverage(x, counts_in_1x), _count_to_coverage(y, counts_in_1x))\n        for x, y in sample_data.items()\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_counts_in_1x(data_is_basepairs):\n    read_length = float(getattr(config, 'preseq', {}).get('read_length', 0))\n    genome_size = getattr(config, 'preseq', {}).get('genome_size')\n    if genome_size:\n        try:\n            genome_size = float(genome_size)\n        except ValueError:\n            presets = {'hg19_genome': 2897310462,\n                       'hg38_genome': 3049315783,\n                       'mm10_genome': 2652783500}\n            if genome_size in presets:\n                genome_size = presets[genome_size]\n            else:\n                log.warn('The size for genome ' + genome_size + ' is unknown to MultiQC, ' +\n                         'please specify it explicitly or choose one of the following: ' +\n                         ', '.join(presets.keys()) + '. Falling back to molecule counts.')\n                genome_size = None\n\n    if genome_size:\n        if data_is_basepairs:\n            return genome_size\n        elif read_length:\n            return genome_size / read_length\n    else:\n        return None", "response": "Get approximate number of counts in 1x of depth\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates the preseq length trimmed plot.", "response": "def _make_preseq_length_trimmed_plot(self, data_raw, is_basepairs):\n        \"\"\" Generate the preseq plot.\n\n            For Y axis, plot coverages if `config.preseq.read_length` and `config.preseq.genome_size`\n            specified (unless requested otherwise in `config.preseq.y_axis`).\n\n            For X axis, plot counts (or base pairs) in X axis (unless coverages requested\n            explicitly in `config.preseq.x_axis`).\n        \"\"\"\n        counts_in_1x = _get_counts_in_1x(is_basepairs)\n        x_axis = getattr(config, 'preseq', {}).get('x_axis', 'counts')\n        y_axis = getattr(config, 'preseq', {}).get('y_axis', 'coverage' if counts_in_1x else 'counts')\n\n        # Modify counts\n        d_cnts = {sn: _modify_raw_data(sample_data, is_basepairs)\n                  for sn, sample_data in data_raw.items()}\n\n        # Convert counts (base pairs) -> depths\n        d_covs = {sn: _counts_to_coverages(sample_data, counts_in_1x) if counts_in_1x else None\n                  for sn, sample_data in data_raw.items()}\n\n        # Prepare final dataset for plotting\n        data = dict()\n        for sn, s_d_cnts, s_d_covs in zip(d_cnts, d_cnts.values(), d_covs.values()):\n            keys = s_d_covs.keys() if x_axis == 'coverage' else s_d_cnts.keys()\n            values = s_d_covs.values() if y_axis == 'coverage' else s_d_cnts.values()\n            data[sn] = dict(zip(keys, values))\n\n        # Count maximum values to draw the \"ideal\" line\n        max_y_raw, max_sn = max((max(sd.values()), sn) for sn, sd in data_raw.items())\n        max_y_cnt = list(_modify_raw_data({max_y_raw: max_y_raw}, is_basepairs).items())[0][0]\n        max_y_cov = list(_counts_to_coverages({max_y_raw: max_y_raw}, counts_in_1x).items())[0][0]\n        max_y = max_y_cov if y_axis == 'coverage' else max_y_cnt\n        max_yx = max_y_cov if x_axis == 'coverage' else max_y_cnt\n\n        # Preparing axis and tooltip labels\n        x_lbl, y_tt_lbl, x_axis_name, y_lbl, x_tt_lbl, y_axis_name = \\\n            _prepare_labels(is_basepairs, max_y_cov, x_axis, y_axis)\n\n        description = ''\n        pconfig = {\n            'id': 'preseq_plot',\n            'title': 'Preseq: Complexity curve',\n            'xlab': x_axis_name,\n            'ylab': y_axis_name,\n            'xmin': 0,\n            'ymin': 0,\n            'tt_label': '<b>' + y_tt_lbl + '</b>: ' + x_tt_lbl,\n            'xLabelFormat': x_lbl,\n            'yLabelFormat': y_lbl,\n            'extra_series': []\n        }\n\n        # Parse the real counts if we have them\n        real_cnts_all, real_cnts_unq = self._parse_real_counts(data.keys())\n        real_vals_all, real_vals_unq = _prep_real_counts(real_cnts_all, real_cnts_unq,\n                                                         is_basepairs, counts_in_1x, x_axis, y_axis)\n        pconfig['extra_series'].extend(_real_counts_to_plot_series(data, real_vals_unq, real_vals_all))\n        if real_vals_unq:\n            description += '<p>Points show read count versus deduplicated read counts (externally calculated).</p>'\n        elif real_vals_all:\n            description += '<p>Points show externally calculated read counts on the curves.</p>'\n\n        # Trim the data to not have a ridiculous x-axis (10Gbp anyone?)\n        if getattr(config, 'preseq', {}).get('notrim', False) is not True:\n            max_y *= 0.8\n            max_yx *= 0.8\n            max_x = 0\n            for x in sorted(list(data[max_sn].keys())):\n                max_x = max(max_x, x)\n                if data[max_sn][x] > max_y and x > real_vals_all.get(max_sn, 0) and x > real_vals_unq.get(max_sn, 0):\n                    break\n            pconfig['xmax'] = max_x\n            description += \"<p>Note that the x axis is trimmed at the point where all the datasets \\\n                show 80% of their maximum y-value, to avoid ridiculous scales.</p>\"\n\n        # Plot perfect library as dashed line\n        pconfig['extra_series'].append({\n            'name': 'a perfect library where each read is unique',\n            'data': [[0, 0], [max_yx, max_y]],\n            'dashStyle': 'Dash',\n            'lineWidth': 1,\n            'color': '#000000',\n            'marker': { 'enabled': False },\n            'showInLegend': False,\n        })\n\n        self.add_section(\n            name='Complexity curve',\n            description=description,\n            anchor='preseq-plot',\n            plot=linegraph.plot(data, pconfig)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_reports(self):\n\n    self.qualimap_rnaseq_genome_results = dict()\n    regexes = {\n        'reads_aligned': r\"read(?:s| pairs) aligned\\s*=\\s*([\\d,]+)\",\n        'total_alignments': r\"total alignments\\s*=\\s*([\\d,]+)\",\n        'non_unique_alignments': r\"non-unique alignments\\s*=\\s*([\\d,]+)\",\n        'reads_aligned_genes': r\"aligned to genes\\s*=\\s*([\\d,]+)\",\n        'ambiguous_alignments': r\"ambiguous alignments\\s*=\\s*([\\d,]+)\",\n        'not_aligned': r\"not aligned\\s*=\\s*([\\d,]+)\",\n        '5_3_bias': r\"5'-3' bias\\s*=\\s*([\\d,\\.]+)$\",\n        'reads_aligned_exonic': r\"exonic\\s*=\\s*([\\d,]+)\",\n        'reads_aligned_intronic': r\"intronic\\s*=\\s*([\\d,]+)\",\n        'reads_aligned_intergenic': r\"intergenic\\s*=\\s*([\\d,]+)\",\n        'reads_aligned_overlapping_exon': r\"overlapping exon\\s*=\\s*([\\d,]+)\",\n    }\n    for f in self.find_log_files('qualimap/rnaseq/rnaseq_results'):\n        d = dict()\n\n        # Get the sample name\n        s_name_regex = re.search(r\"bam file\\s*=\\s*(.+)\", f['f'], re.MULTILINE)\n        if s_name_regex:\n            d['bam_file'] = s_name_regex.group(1)\n            s_name = self.clean_s_name(d['bam_file'], f['root'])\n        else:\n            log.warn(\"Couldn't find an input filename in genome_results file {}/{}\".format(f['root'], f['fn']))\n            return None\n\n        # Check for and 'fix' European style decimal places / thousand separators\n        comma_regex = re.search(r\"exonic\\s*=\\s*[\\d\\.]+ \\(\\d{1,3},\\d+%\\)\", f['f'], re.MULTILINE)\n        if comma_regex:\n            log.debug(\"Trying to fix European comma style syntax in Qualimap report {}/{}\".format(f['root'], f['fn']))\n            f['f'] = f['f'].replace('.','')\n            f['f'] = f['f'].replace(',','.')\n\n        # Go through all numeric regexes\n        for k, r in regexes.items():\n            r_search = re.search(r, f['f'], re.MULTILINE)\n            if r_search:\n                try:\n                    d[k] = float(r_search.group(1).replace(',',''))\n                except UnicodeEncodeError:\n                    # Qualimap reports infinity (\\u221e) when 3' bias denominator is zero\n                    pass\n                except ValueError:\n                    d[k] = r_search.group(1)\n\n        # Add to general stats table\n        for k in ['5_3_bias', 'reads_aligned']:\n            try:\n                self.general_stats_data[s_name][k] = d[k]\n            except KeyError:\n                pass\n\n        # Save results\n        if s_name in self.qualimap_rnaseq_genome_results:\n            log.debug(\"Duplicate genome results sample name found! Overwriting: {}\".format(s_name))\n        self.qualimap_rnaseq_genome_results[s_name] = d\n        self.add_data_source(f, s_name=s_name, section='rna_genome_results')\n\n\n    #### Coverage profile\n    self.qualimap_rnaseq_cov_hist = dict()\n    for f in self.find_log_files('qualimap/rnaseq/coverage', filehandles=True):\n        s_name = self.get_s_name(f)\n        d = dict()\n        for l in f['f']:\n            if l.startswith('#'):\n                continue\n            coverage, count = l.split(None, 1)\n            coverage = int(round(float(coverage)))\n            count = float(count)\n            d[coverage] = count\n\n        if len(d) == 0:\n            log.debug(\"Couldn't parse contents of coverage histogram file {}\".format(f['fn']))\n            return None\n\n        # Save results\n        if s_name in self.qualimap_rnaseq_cov_hist:\n            log.debug(\"Duplicate coverage histogram sample name found! Overwriting: {}\".format(s_name))\n        self.qualimap_rnaseq_cov_hist[s_name] = d\n        self.add_data_source(f, s_name=s_name, section='rna_coverage_histogram')\n\n    # Filter to strip out ignored sample names\n    self.qualimap_rnaseq_genome_results = self.ignore_samples(self.qualimap_rnaseq_genome_results)\n    self.qualimap_rnaseq_cov_hist = self.ignore_samples(self.qualimap_rnaseq_cov_hist)\n\n    #### Plots\n\n    # Genomic Origin Bar Graph\n    # NB: Ignore 'Overlapping Exon' in report - these make the numbers add up to > 100%\n    if len(self.qualimap_rnaseq_genome_results) > 0:\n        gorigin_cats = OrderedDict()\n        gorigin_cats['reads_aligned_exonic'] = {'name': 'Exonic'}\n        gorigin_cats['reads_aligned_intronic'] = {'name': 'Intronic'}\n        gorigin_cats['reads_aligned_intergenic'] = {'name': 'Intergenic'}\n        gorigin_pconfig = {\n            'id': 'qualimap_genomic_origin',\n            'title': 'Qualimap RNAseq: Genomic Origin',\n            'ylab': 'Number of reads',\n            'cpswitch_c_active': False\n        }\n        genomic_origin_helptext = '''\n        There are currently three main approaches to map reads to transcripts in an\n        RNA-seq experiment: mapping reads to a reference genome to identify expressed\n        transcripts that are annotated (and discover those that are unknown), mapping\n        reads to a reference transcriptome, and <i>de novo</i> assembly of transcript\n        sequences (<a href=\"https://doi.org/10.1186/s13059-016-0881-8\"\n        target=\"_blank\">Conesa et al. 2016</a>).\n\n        For RNA-seq QC analysis, QualiMap can be used to assess alignments produced by\n        the first of these approaches. For input, it requires a GTF annotation file\n        along with a reference genome, which can be used to reconstruct the exon\n        structure of known transcripts. This allows mapped reads to be grouped by\n        whether they originate in an exonic region (for QualiMap, this may include\n        5&#8242; and 3&#8242; UTR regions as well as protein-coding exons), an intron,\n        or an intergenic region (see the <a href=\"http://qualimap.bioinfo.cipf.es/doc_html/index.html\"\n        target=\"_blank\">Qualimap 2 documentation</a>).\n\n        The inferred genomic origins of RNA-seq reads are presented here as a bar graph\n        showing either the number or percentage of mapped reads in each read dataset\n        that have been assigned to each type of genomic region. This graph can be used\n        to assess the proportion of useful reads in an RNA-seq experiment. That\n        proportion can be reduced by the presence of intron sequences, especially if\n        depletion of ribosomal RNA was used during sample preparation (<a href=\"https://doi.org/10.1038/nrg3642\"\n        target=\"_blank\">Sims et al. 2014</a>). It can also be reduced by off-target\n        transcripts, which are detected in greater numbers at the sequencing depths\n        needed to detect poorly-expressed transcripts (<a href=\"https://doi.org/10.1101/gr.124321.111\"\n        target=\"_blank\">Tarazona et al. 2011</a>).'''\n        self.add_section (\n            name = 'Genomic origin of reads',\n            anchor = 'qualimap-reads-genomic-origin',\n            description = 'Classification of mapped reads as originating in exonic, intronic or intergenic regions. These can be displayed as either the number or percentage of mapped reads.',\n            helptext = genomic_origin_helptext,\n            plot = bargraph.plot(self.qualimap_rnaseq_genome_results, gorigin_cats, gorigin_pconfig)\n        )\n\n    if len(self.qualimap_rnaseq_cov_hist) > 0:\n        coverage_profile_helptext = '''\n        There are currently three main approaches to map reads to transcripts in an\n        RNA-seq experiment: mapping reads to a reference genome to identify expressed\n        transcripts that are annotated (and discover those that are unknown), mapping\n        reads to a reference transcriptome, and <i>de novo</i> assembly of transcript\n        sequences (<a href=\"https://doi.org/10.1186/s13059-016-0881-8\"\n        target=\"_blank\">Conesa et al. 2016</a>).\n\n        For RNA-seq QC analysis, QualiMap can be used to assess alignments produced by\n        the first of these approaches. For input, it requires a GTF annotation file\n        along with a reference genome, which can be used to reconstruct the exon\n        structure of known transcripts. QualiMap uses this information to calculate the\n        depth of coverage along the length of each annotated transcript. For a set of\n        reads mapped to a transcript, the depth of coverage at a given base position is\n        the number of high-quality reads that map to the transcript at that position\n        (<a href=\"https://doi.org/10.1038/nrg3642\" target=\"_blank\">Sims et al. 2014</a>).\n\n        QualiMap calculates coverage depth at every base position of each annotated\n        transcript. To enable meaningful comparison between transcripts, base positions\n        are rescaled to relative positions expressed as percentage distance along each\n        transcript (*0%, 1%, &#8230;, 99%*). For the set of transcripts with at least\n        one mapped read, QualiMap plots the cumulative mapped-read depth (y-axis) at\n        each relative transcript position (x-axis). This plot shows the gene coverage\n        profile across all mapped transcripts for each read dataset. It provides a\n        visual way to assess positional biases, such as an accumulation of mapped reads\n        at the 3&#8242; end of transcripts, which may indicate poor RNA quality in the\n        original sample (<a href=\"https://doi.org/10.1186/s13059-016-0881-8\"\n        target=\"_blank\">Conesa et al. 2016</a>).'''\n        self.add_section (\n            name = 'Gene Coverage Profile',\n            anchor = 'qualimap-genome-fraction-coverage',\n            description = 'Mean distribution of coverage depth across the length of all mapped transcripts.',\n            helptext = coverage_profile_helptext,\n            plot = linegraph.plot(self.qualimap_rnaseq_cov_hist, {\n                'id': 'qualimap_gene_coverage_profile',\n                'title': 'Qualimap RNAseq: Coverage Profile Along Genes (total)',\n                'ylab': 'Coverage',\n                'xlab': 'Transcript Position (%)',\n                'ymin': 0,\n                'xmin': 0,\n                'xmax': 100,\n                'tt_label': '<b>{point.x} bp</b>: {point.y:.0f}%',\n            })\n        )\n\n\n    #### General Stats\n    self.general_stats_headers['5_3_bias'] = {\n        'title': \"5'-3' bias\",\n        'format': '{:,.2f}',\n    }\n    self.general_stats_headers['reads_aligned'] = {\n        'title': '{} Aligned'.format(config.read_count_prefix),\n        'description': 'Reads Aligned ({})'.format(config.read_count_desc),\n        'min': 0,\n        'scale': 'RdBu',\n        'shared_key': 'read_count',\n        'modify': lambda x: x * config.read_count_multiplier\n    }\n\n    # Return the number of reports we found\n    return len(self.qualimap_rnaseq_genome_results.keys())", "response": "Find Qualimap RNASeq reports and parse their data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_colour(self, val, colformat='hex'):\n\t\ttry:\n\t\t\t# Sanity checks\n\t\t\tval = re.sub(\"[^0-9\\.]\", \"\", str(val))\n\t\t\tif val == '':\n\t\t\t\tval = self.minval\n\t\t\tval = float(val)\n\t\t\tval = max(val, self.minval)\n\t\t\tval = min(val, self.maxval)\n\n\t\t\tdomain_nums = list( np.linspace(self.minval, self.maxval, len(self.colours)) )\n\t\t\tmy_scale = spectra.scale(self.colours).domain(domain_nums)\n\n\t\t\t# Weird, I know. I ported this from the original JavaScript for continuity\n\t\t\t# Seems to work better than adjusting brightness / saturation / luminosity\n\t\t\trgb_converter = lambda x: max(0, min(1, 1+((x-1)*0.3)))\n\t\t\tthecolour = spectra.rgb( *[rgb_converter(v) for v in my_scale(val).rgb] )\n\n\t\t\treturn thecolour.hexcode\n\n\t\texcept:\n\t\t\t# Shouldn't crash all of MultiQC just for colours\n\t\t\treturn ''", "response": "Given a value return a colour within the colour scale"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_colours(self, name='GnBu'):\n\t\t# ColorBrewer colours, taken from Chroma.js source code\n\t\t# https://github.com/gka/chroma.js\n\n\t\t###\tCopyright (c) 2002 Cynthia Brewer, Mark Harrower, and The\n\t\t###\tPennsylvania State University.\n\t\t###\n\t\t###\tLicensed under the Apache License, Version 2.0 (the \"License\");\n\t\t###\tyou may not use this file except in compliance with the License.\n\t\t###\tYou may obtain a copy of the License at\n\t\t###\thttp://www.apache.org/licenses/LICENSE-2.0\n\t\t###\n\t\t###\tUnless required by applicable law or agreed to in writing, software distributed\n\t\t###\tunder the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR\n\t\t###\tCONDITIONS OF ANY KIND, either express or implied. See the License for the\n\t\t###\tspecific language governing permissions and limitations under the License.\n\n\t\tcolorbrewer_scales = {\n\t\t\t# sequential\n\t\t\t'OrRd': ['#fff7ec', '#fee8c8', '#fdd49e', '#fdbb84', '#fc8d59', '#ef6548', '#d7301f', '#b30000', '#7f0000'],\n\t\t\t'PuBu': ['#fff7fb', '#ece7f2', '#d0d1e6', '#a6bddb', '#74a9cf', '#3690c0', '#0570b0', '#045a8d', '#023858'],\n\t\t\t'BuPu': ['#f7fcfd', '#e0ecf4', '#bfd3e6', '#9ebcda', '#8c96c6', '#8c6bb1', '#88419d', '#810f7c', '#4d004b'],\n\t\t\t'Oranges': ['#fff5eb', '#fee6ce', '#fdd0a2', '#fdae6b', '#fd8d3c', '#f16913', '#d94801', '#a63603', '#7f2704'],\n\t\t\t'BuGn': ['#f7fcfd', '#e5f5f9', '#ccece6', '#99d8c9', '#66c2a4', '#41ae76', '#238b45', '#006d2c', '#00441b'],\n\t\t\t'YlOrBr': ['#ffffe5', '#fff7bc', '#fee391', '#fec44f', '#fe9929', '#ec7014', '#cc4c02', '#993404', '#662506'],\n\t\t\t'YlGn': ['#ffffe5', '#f7fcb9', '#d9f0a3', '#addd8e', '#78c679', '#41ab5d', '#238443', '#006837', '#004529'],\n\t\t\t'Reds': ['#fff5f0', '#fee0d2', '#fcbba1', '#fc9272', '#fb6a4a', '#ef3b2c', '#cb181d', '#a50f15', '#67000d'],\n\t\t\t'RdPu': ['#fff7f3', '#fde0dd', '#fcc5c0', '#fa9fb5', '#f768a1', '#dd3497', '#ae017e', '#7a0177', '#49006a'],\n\t\t\t'Greens': ['#f7fcf5', '#e5f5e0', '#c7e9c0', '#a1d99b', '#74c476', '#41ab5d', '#238b45', '#006d2c', '#00441b'],\n\t\t\t'YlGnBu': ['#ffffd9', '#edf8b1', '#c7e9b4', '#7fcdbb', '#41b6c4', '#1d91c0', '#225ea8', '#253494', '#081d58'],\n\t\t\t'Purples': ['#fcfbfd', '#efedf5', '#dadaeb', '#bcbddc', '#9e9ac8', '#807dba', '#6a51a3', '#54278f', '#3f007d'],\n\t\t\t'GnBu': ['#f7fcf0', '#e0f3db', '#ccebc5', '#a8ddb5', '#7bccc4', '#4eb3d3', '#2b8cbe', '#0868ac', '#084081'],\n\t\t\t'Greys': ['#ffffff', '#f0f0f0', '#d9d9d9', '#bdbdbd', '#969696', '#737373', '#525252', '#252525', '#000000'],\n\t\t\t'YlOrRd': ['#ffffcc', '#ffeda0', '#fed976', '#feb24c', '#fd8d3c', '#fc4e2a', '#e31a1c', '#bd0026', '#800026'],\n\t\t\t'PuRd': ['#f7f4f9', '#e7e1ef', '#d4b9da', '#c994c7', '#df65b0', '#e7298a', '#ce1256', '#980043', '#67001f'],\n\t\t\t'Blues': ['#f7fbff', '#deebf7', '#c6dbef', '#9ecae1', '#6baed6', '#4292c6', '#2171b5', '#08519c', '#08306b'],\n\t\t\t'PuBuGn': ['#fff7fb', '#ece2f0', '#d0d1e6', '#a6bddb', '#67a9cf', '#3690c0', '#02818a', '#016c59', '#014636'],\n\n\t\t\t# diverging\n\t\t\t'Spectral': ['#9e0142', '#d53e4f', '#f46d43', '#fdae61', '#fee08b', '#ffffbf', '#e6f598', '#abdda4', '#66c2a5', '#3288bd', '#5e4fa2'],\n\t\t\t'RdYlGn': ['#a50026', '#d73027', '#f46d43', '#fdae61', '#fee08b', '#ffffbf', '#d9ef8b', '#a6d96a', '#66bd63', '#1a9850', '#006837'],\n\t\t\t'RdBu': ['#67001f', '#b2182b', '#d6604d', '#f4a582', '#fddbc7', '#f7f7f7', '#d1e5f0', '#92c5de', '#4393c3', '#2166ac', '#053061'],\n\t\t\t'PiYG': ['#8e0152', '#c51b7d', '#de77ae', '#f1b6da', '#fde0ef', '#f7f7f7', '#e6f5d0', '#b8e186', '#7fbc41', '#4d9221', '#276419'],\n\t\t\t'PRGn': ['#40004b', '#762a83', '#9970ab', '#c2a5cf', '#e7d4e8', '#f7f7f7', '#d9f0d3', '#a6dba0', '#5aae61', '#1b7837', '#00441b'],\n\t\t\t'RdYlBu': ['#a50026', '#d73027', '#f46d43', '#fdae61', '#fee090', '#ffffbf', '#e0f3f8', '#abd9e9', '#74add1', '#4575b4', '#313695'],\n\t\t\t'BrBG': ['#543005', '#8c510a', '#bf812d', '#dfc27d', '#f6e8c3', '#f5f5f5', '#c7eae5', '#80cdc1', '#35978f', '#01665e', '#003c30'],\n\t\t\t'RdGy': ['#67001f', '#b2182b', '#d6604d', '#f4a582', '#fddbc7', '#ffffff', '#e0e0e0', '#bababa', '#878787', '#4d4d4d', '#1a1a1a'],\n\t\t\t'PuOr': ['#7f3b08', '#b35806', '#e08214', '#fdb863', '#fee0b6', '#f7f7f7', '#d8daeb', '#b2abd2', '#8073ac', '#542788', '#2d004b'],\n\n\t\t\t# qualitative\n\t\t\t'Set2': ['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3', '#a6d854', '#ffd92f', '#e5c494', '#b3b3b3'],\n\t\t\t'Accent': ['#7fc97f', '#beaed4', '#fdc086', '#ffff99', '#386cb0', '#f0027f', '#bf5b17', '#666666'],\n\t\t\t'Set1': ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00', '#ffff33', '#a65628', '#f781bf', '#999999'],\n\t\t\t'Set3': ['#8dd3c7', '#ffffb3', '#bebada', '#fb8072', '#80b1d3', '#fdb462', '#b3de69', '#fccde5', '#d9d9d9', '#bc80bd', '#ccebc5', '#ffed6f'],\n\t\t\t'Dark2': ['#1b9e77', '#d95f02', '#7570b3', '#e7298a', '#66a61e', '#e6ab02', '#a6761d', '#666666'],\n\t\t\t'Paired': ['#a6cee3', '#1f78b4', '#b2df8a', '#33a02c', '#fb9a99', '#e31a1c', '#fdbf6f', '#ff7f00', '#cab2d6', '#6a3d9a', '#ffff99', '#b15928'],\n\t\t\t'Pastel2': ['#b3e2cd', '#fdcdac', '#cbd5e8', '#f4cae4', '#e6f5c9', '#fff2ae', '#f1e2cc', '#cccccc'],\n\t\t\t'Pastel1': ['#fbb4ae', '#b3cde3', '#ccebc5', '#decbe4', '#fed9a6', '#ffffcc', '#e5d8bd', '#fddaec', '#f2f2f2'],\n\t\t}\n\n\t\t# Detect reverse colour scales\n\t\treverse = False\n\t\tif str(name).endswith('-rev'):\n\t\t\treverse = True\n\t\t\tname = name[:-4]\n\n\t\t# Default colour scale\n\t\tif name not in colorbrewer_scales:\n\t\t\tname = 'GnBu'\n\n\t\t# Return colours\n\t\tif reverse:\n\t\t\treturn list(reversed(colorbrewer_scales[name]))\n\t\telse:\n\t\t\treturn colorbrewer_scales[name]", "response": "Function to get a colour scale by name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_tophat_log (self, raw_data):\n\n        if 'Aligned pairs' in raw_data:\n            # Paired end data\n            regexes = {\n                'overall_aligned_percent': r\"([\\d\\.]+)% overall read mapping rate.\",\n                'concordant_aligned_percent': r\"([\\d\\.]+)% concordant pair alignment rate.\",\n                'aligned_total': r\"Aligned pairs:\\s+(\\d+)\",\n                'aligned_multimap': r\"Aligned pairs:\\s+\\d+\\n\\s+of these:\\s+(\\d+)\",\n                'aligned_discordant': r\"(\\d+) \\([\\s\\d\\.]+%\\) are discordant alignments\",\n                'total_reads': r\"[Rr]eads:\\n\\s+Input\\s*:\\s+(\\d+)\",\n            }\n        else:\n            # Single end data\n            regexes = {\n                'total_reads': r\"[Rr]eads:\\n\\s+Input\\s*:\\s+(\\d+)\",\n                'aligned_total': r\"Mapped\\s*:\\s+(\\d+)\",\n                'aligned_multimap': r\"of these\\s*:\\s+(\\d+)\",\n                'overall_aligned_percent': r\"([\\d\\.]+)% overall read mapping rate.\",\n            }\n\n        parsed_data = {}\n        for k, r in regexes.items():\n            r_search = re.search(r, raw_data, re.MULTILINE)\n            if r_search:\n                parsed_data[k] = float(r_search.group(1))\n        if len(parsed_data) == 0: return None\n        parsed_data['concordant_aligned_percent'] = parsed_data.get('concordant_aligned_percent', 0)\n        parsed_data['aligned_total'] = parsed_data.get('aligned_total', 0)\n        parsed_data['aligned_multimap'] = parsed_data.get('aligned_multimap', 0)\n        parsed_data['aligned_discordant'] = parsed_data.get('aligned_discordant', 0)\n        parsed_data['unaligned_total'] = parsed_data['total_reads'] - parsed_data['aligned_total']\n        parsed_data['aligned_not_multimapped_discordant'] = parsed_data['aligned_total'] - parsed_data['aligned_multimap'] - parsed_data['aligned_discordant']\n        return parsed_data", "response": "Parses the Tophat alignment log file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntakes the parsed stats from the Tophat report and add it to the basic stats table at the top of the report", "response": "def tophat_general_stats_table(self):\n        \"\"\" Take the parsed stats from the Tophat report and add it to the\n        basic stats table at the top of the report \"\"\"\n\n        headers = OrderedDict()\n        headers['overall_aligned_percent'] = {\n            'title': '% Aligned',\n            'description': 'overall read mapping rate',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'YlGn'\n        }\n        headers['aligned_not_multimapped_discordant'] = {\n            'title': '{} Aligned'.format(config.read_count_prefix),\n            'description': 'Aligned reads, not multimapped or discordant ({})'.format(config.read_count_desc),\n            'min': 0,\n            'scale': 'PuRd',\n            'modify': lambda x: x * config.read_count_multiplier,\n            'shared_key': 'read_count'\n        }\n        self.general_stats_addcols(self.tophat_data, headers)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake the HTML to plot the alignment rates", "response": "def tophat_alignment_plot (self):\n        \"\"\" Make the HighCharts HTML to plot the alignment rates \"\"\"\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['aligned_not_multimapped_discordant'] = { 'color': '#437bb1', 'name': 'Aligned' }\n        keys['aligned_multimap'] =   { 'color': '#f7a35c', 'name': 'Multimapped' }\n        keys['aligned_discordant'] = { 'color': '#e63491', 'name': 'Discordant mappings' }\n        keys['unaligned_total'] =    { 'color': '#7f0000', 'name': 'Not aligned' }\n\n        # Config for the plot\n        config = {\n            'id': 'tophat_alignment',\n            'title': 'Tophat: Alignment Scores',\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads'\n        }\n\n        self.add_section( plot =  bargraph.plot(self.tophat_data, keys, config) )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntaking a filename parse the data assuming it s a flagstat file Returns a dictionary containing the data that is a dictionary containing the key - value pairs that are used to identify the flagstat file and the value of the flagstat file that is used to identify the flagstat file.", "response": "def parse_single_report(file_obj):\n    \"\"\"\n    Take a filename, parse the data assuming it's a flagstat file\n    Returns a dictionary {'lineName_pass' : value, 'lineName_fail' : value}\n    \"\"\"\n    parsed_data = {}\n\n    re_groups = ['passed', 'failed', 'passed_pct', 'failed_pct']\n    for k, r in flagstat_regexes.items():\n        r_search = re.search(r, file_obj, re.MULTILINE)\n        if r_search:\n            for i,j in enumerate(re_groups):\n                try:\n                    key = \"{}_{}\".format(k, j)\n                    val = r_search.group(i+1).strip('% ')\n                    parsed_data[key] = float(val) if ('.' in val) else int(val)\n                except IndexError:\n                    pass # Not all regexes have percentages\n                except ValueError:\n                    parsed_data[key] = float('nan')\n    # Work out the total read count\n    try:\n        parsed_data['flagstat_total'] = parsed_data['total_passed'] + parsed_data['total_failed']\n    except KeyError:\n        pass\n    return parsed_data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind Samtools flagstat logs and parse their data", "response": "def parse_samtools_flagstats(self):\n        \"\"\" Find Samtools flagstat logs and parse their data \"\"\"\n\n        self.samtools_flagstat = dict()\n        for f in self.find_log_files('samtools/flagstat'):\n            parsed_data = parse_single_report(f['f'])\n            if len(parsed_data) > 0:\n                if f['s_name'] in self.samtools_flagstat:\n                    log.debug(\"Duplicate sample name found! Overwriting: {}\".format(f['s_name']))\n                self.add_data_source(f, section='flagstat')\n                self.samtools_flagstat[f['s_name']] = parsed_data\n\n        # Filter to strip out ignored sample names\n        self.samtools_flagstat = self.ignore_samples(self.samtools_flagstat)\n\n        if len(self.samtools_flagstat) > 0:\n\n            # Write parsed report data to a file (restructure first)\n            self.write_data_file(self.samtools_flagstat, 'multiqc_samtools_flagstat')\n\n            # General Stats Table\n            flagstats_headers = dict()\n            flagstats_headers['mapped_passed'] = {\n                'title': '{} Reads Mapped'.format(config.read_count_prefix),\n                'description': 'Reads Mapped in the bam file ({})'.format(config.read_count_desc),\n                'min': 0,\n                'modify': lambda x: x * config.read_count_multiplier,\n                'shared_key': 'read_count',\n                'placement' : 100.0\n            }\n            self.general_stats_addcols(self.samtools_flagstat, flagstats_headers, 'Samtools Flagstat')\n\n            # Make dot plot of counts\n            keys = OrderedDict()\n            reads = {\n                'min': 0,\n                'modify': lambda x: float(x) * config.read_count_multiplier,\n                 'suffix': '{} reads'.format(config.read_count_prefix),\n                'decimalPlaces': 2,\n                'shared_key': 'read_count'\n            }\n            keys['flagstat_total']              = dict(reads, title = 'Total Reads' )\n            keys['total_passed']                = dict(reads, title = 'Total Passed QC' )\n            keys['mapped_passed']               = dict(reads, title = 'Mapped' )\n\n            if any(v.get('secondary_passed') for v in self.samtools_flagstat.values()):\n                keys['secondary_passed']        = dict(reads, title = 'Secondary Alignments' )\n\n            if any(v.get('supplementary_passed') for v in self.samtools_flagstat.values()):\n                keys['supplementary_passed']    = dict(reads, title = 'Supplementary Alignments' )\n\n            keys['duplicates_passed']           = dict(reads, title = 'Duplicates' )\n            keys['paired in sequencing_passed'] = dict(reads, title = 'Paired in Sequencing' )\n            keys['properly paired_passed']      = dict(reads, title = 'Properly Paired' )\n            keys['with itself and mate mapped_passed'] = \\\n                                                  dict(reads, title = 'Self and mate mapped',\n                                                              description = 'Reads with itself and mate mapped' )\n            keys['singletons_passed']           = dict(reads, title = 'Singletons' )\n            keys['with mate mapped to a different chr_passed'] = \\\n                                                  dict(reads, title = 'Mate mapped to diff chr',\n                                                              description = 'Mate mapped to different chromosome' )\n            keys['with mate mapped to a different chr (mapQ >= 5)_passed'] = \\\n                                                  dict(reads, title = 'Diff chr (mapQ >= 5)',\n                                                              description = 'Mate mapped to different chromosome (mapQ >= 5)' )\n\n            self.add_section (\n                name = 'Samtools Flagstat',\n                anchor = 'samtools-flagstat',\n                description = 'This module parses the output from <code>samtools flagstat</code>. All numbers in millions.',\n                plot = beeswarm.plot(self.samtools_flagstat, keys, {'id': 'samtools-flagstat-dp'})\n            )\n\n        # Return the number of logs that were found\n        return len(self.samtools_flagstat)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the cutadapt log file looking for the cutadapt output", "response": "def parse_cutadapt_logs(self, f):\n        \"\"\" Go through log file looking for cutadapt output \"\"\"\n        fh = f['f']\n        regexes = {\n            '1.7': {\n                'bp_processed': \"Total basepairs processed:\\s*([\\d,]+) bp\",\n                'bp_written': \"Total written \\(filtered\\):\\s*([\\d,]+) bp\",\n                'quality_trimmed': \"Quality-trimmed:\\s*([\\d,]+) bp\",\n                'r_processed': \"Total reads processed:\\s*([\\d,]+)\",\n                'r_with_adapters': \"Reads with adapters:\\s*([\\d,]+)\"\n            },\n            '1.6': {\n                'r_processed': \"Processed reads:\\s*([\\d,]+)\",\n                'bp_processed': \"Processed bases:\\s*([\\d,]+) bp\",\n                'r_trimmed': \"Trimmed reads:\\s*([\\d,]+)\",\n                'quality_trimmed': \"Quality-trimmed:\\s*([\\d,]+) bp\",\n                'bp_trimmed': \"Trimmed bases:\\s*([\\d,]+) bp\",\n                'too_short': \"Too short reads:\\s*([\\d,]+)\",\n                'too_long': \"Too long reads:\\s*([\\d,]+)\",\n            }\n        }\n        s_name = None\n        cutadapt_version = '1.7'\n        log_section = None\n        for l in fh:\n            # New log starting\n            if 'cutadapt' in l:\n                s_name = None\n                c_version = re.match(r'This is cutadapt ([\\d\\.]+)', l)\n                if c_version:\n                    try:\n                        assert(StrictVersion(c_version.group(1)) <= StrictVersion('1.6'))\n                        cutadapt_version = '1.6'\n                    except:\n                        cutadapt_version = '1.7'\n                c_version_old = re.match(r'cutadapt version ([\\d\\.]+)', l)\n                if c_version_old:\n                    try:\n                        assert(StrictVersion(c_version.group(1)) <= StrictVersion('1.6'))\n                        cutadapt_version = '1.6'\n                    except:\n                        # I think the pattern \"cutadapt version XX\" is only pre-1.6?\n                        cutadapt_version = '1.6'\n            # Get sample name from end of command line params\n            if l.startswith('Command line parameters'):\n                s_name = l.split()[-1]\n                # Manage case where sample name is '-' (reading from stdin)\n                if s_name == '-':\n                    s_name = f['s_name']\n                else:\n                    s_name = self.clean_s_name(s_name, f['root'])\n                if s_name in self.cutadapt_data:\n                    log.debug(\"Duplicate sample name found! Overwriting: {}\".format(s_name))\n                self.cutadapt_data[s_name] = dict()\n\n            if s_name is not None:\n                self.add_data_source(f, s_name)\n\n                # Search regexes for overview stats\n                for k, r in regexes[cutadapt_version].items():\n                    match = re.search(r, l)\n                    if match:\n                        self.cutadapt_data[s_name][k] = int(match.group(1).replace(',', ''))\n\n                # Starting a new section\n                if '===' in l:\n                    log_section = l.strip().strip('=').strip()\n\n                # Histogram showing lengths trimmed\n                if 'length' in l and 'count' in l and 'expect' in l:\n                    plot_sname = s_name\n                    if log_section is not None:\n                        plot_sname = '{} - {}'.format(s_name, log_section)\n                    self.cutadapt_length_counts[plot_sname] = dict()\n                    self.cutadapt_length_exp[plot_sname] = dict()\n                    self.cutadapt_length_obsexp[plot_sname] = dict()\n                    # Nested loop to read this section while the regex matches\n                    for l in fh:\n                        r_seqs = re.search(\"^(\\d+)\\s+(\\d+)\\s+([\\d\\.]+)\", l)\n                        if r_seqs:\n                            a_len = int(r_seqs.group(1))\n                            self.cutadapt_length_counts[plot_sname][a_len] = int(r_seqs.group(2))\n                            self.cutadapt_length_exp[plot_sname][a_len] = float(r_seqs.group(3))\n                            if float(r_seqs.group(3)) > 0:\n                                self.cutadapt_length_obsexp[plot_sname][a_len] = float(r_seqs.group(2)) / float(r_seqs.group(3))\n                            else:\n                                # Cheating, I know. Infinity is difficult to plot.\n                                self.cutadapt_length_obsexp[plot_sname][a_len] = float(r_seqs.group(2))\n                        else:\n                            break\n\n        # Calculate a few extra numbers of our own\n        for s_name, d in self.cutadapt_data.items():\n            if 'bp_processed' in d and 'bp_written' in d:\n                self.cutadapt_data[s_name]['percent_trimmed'] = (float(d['bp_processed'] - d['bp_written']) / d['bp_processed']) * 100\n            elif 'bp_processed' in d and 'bp_trimmed' in d:\n                self.cutadapt_data[s_name]['percent_trimmed'] = ((float(d.get('bp_trimmed', 0)) + float(d.get('quality_trimmed', 0))) / d['bp_processed']) * 100"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates the trimming length plot", "response": "def cutadapt_length_trimmed_plot (self):\n        \"\"\" Generate the trimming length plot \"\"\"\n\n        description = 'This plot shows the number of reads with certain lengths of adapter trimmed. \\n\\\n        Obs/Exp shows the raw counts divided by the number expected due to sequencing errors. A defined peak \\n\\\n        may be related to adapter length. See the \\n\\\n        <a href=\"http://cutadapt.readthedocs.org/en/latest/guide.html#how-to-read-the-report\" target=\"_blank\">cutadapt documentation</a> \\n\\\n        for more information on how these numbers are generated.'\n\n        pconfig = {\n            'id': 'cutadapt_plot',\n            'title': 'Cutadapt: Lengths of Trimmed Sequences',\n            'ylab': 'Counts',\n            'xlab': 'Length Trimmed (bp)',\n            'xDecimals': False,\n            'ymin': 0,\n            'tt_label': '<b>{point.x} bp trimmed</b>: {point.y:.0f}',\n            'data_labels': [{'name': 'Counts', 'ylab': 'Count'},\n                            {'name': 'Obs/Exp', 'ylab': 'Observed / Expected'}]\n        }\n\n        self.add_section(\n            description = description,\n            plot = linegraph.plot([self.cutadapt_length_counts, self.cutadapt_length_obsexp], pconfig)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_bowtie2_logs(self, f):\n\n        # Regexes\n        regexes = {\n            'unpaired': {\n                'unpaired_aligned_none': r\"(\\d+) \\([\\d\\.]+%\\) aligned 0 times\",\n                'unpaired_aligned_one': r\"(\\d+) \\([\\d\\.]+%\\) aligned exactly 1 time\",\n                'unpaired_aligned_multi': r\"(\\d+) \\([\\d\\.]+%\\) aligned >1 times\"\n            },\n            'paired': {\n                'paired_aligned_none': r\"(\\d+) \\([\\d\\.]+%\\) aligned concordantly 0 times\",\n                'paired_aligned_one': r\"(\\d+) \\([\\d\\.]+%\\) aligned concordantly exactly 1 time\",\n                'paired_aligned_multi': r\"(\\d+) \\([\\d\\.]+%\\) aligned concordantly >1 times\",\n                'paired_aligned_discord_one': r\"(\\d+) \\([\\d\\.]+%\\) aligned discordantly 1 time\",\n                'paired_aligned_discord_multi': r\"(\\d+) \\([\\d\\.]+%\\) aligned discordantly >1 times\",\n                'paired_aligned_mate_one': r\"(\\d+) \\([\\d\\.]+%\\) aligned exactly 1 time\",\n                'paired_aligned_mate_multi': r\"(\\d+) \\([\\d\\.]+%\\) aligned >1 times\",\n                'paired_aligned_mate_none': r\"(\\d+) \\([\\d\\.]+%\\) aligned 0 times\"\n            }\n        }\n\n        # Go through log file line by line\n        s_name = f['s_name']\n        parsed_data = {}\n\n        for l in f['f']:\n            # Attempt in vain to find original bowtie2 command, logged by another program\n            btcmd = re.search(r\"bowtie2 .+ -[1U] ([^\\s,]+)\", l)\n            if btcmd:\n                s_name = self.clean_s_name(btcmd.group(1), f['root'])\n                log.debug(\"Found a bowtie2 command, updating sample name to '{}'\".format(s_name))\n\n            # Total reads\n            total = re.search(r\"(\\d+) reads; of these:\", l)\n            if total:\n                parsed_data['total_reads'] = int(total.group(1))\n\n            # Single end reads\n            unpaired = re.search(r\"(\\d+) \\([\\d\\.]+%\\) were unpaired; of these:\", l)\n            if unpaired:\n                parsed_data['unpaired_total'] = int(unpaired.group(1))\n                self.num_se += 1\n\n                # Do nested loop whilst we have this level of indentation\n                l = f['f'].readline()\n                while l.startswith('    '):\n                    for k, r in regexes['unpaired'].items():\n                        match = re.search(r, l)\n                        if match:\n                            parsed_data[k] = int(match.group(1))\n                    l = f['f'].readline()\n\n            # Paired end reads\n            paired = re.search(r\"(\\d+) \\([\\d\\.]+%\\) were paired; of these:\", l)\n            if paired:\n                parsed_data['paired_total'] = int(paired.group(1))\n                self.num_pe += 1\n\n                # Do nested loop whilst we have this level of indentation\n                l = f['f'].readline()\n                while l.startswith('    '):\n                    for k, r in regexes['paired'].items():\n                        match = re.search(r, l)\n                        if match:\n                            parsed_data[k] = int(match.group(1))\n                    l = f['f'].readline()\n\n            # Overall alignment rate\n            overall = re.search(r\"([\\d\\.]+)% overall alignment rate\", l)\n            if overall:\n                parsed_data['overall_alignment_rate'] = float(overall.group(1))\n\n                # End of log section\n                # Save half 'pairs' of mate counts\n                m_keys = ['paired_aligned_mate_multi', 'paired_aligned_mate_none', 'paired_aligned_mate_one']\n                for k in m_keys:\n                    if k in parsed_data:\n                        parsed_data['{}_halved'.format(k)] = float(parsed_data[k]) / 2.0\n                # Save parsed data\n                if s_name in self.bowtie2_data:\n                    log.debug(\"Duplicate sample name found! Overwriting: {}\".format(s_name))\n                self.add_data_source(f, s_name)\n                self.bowtie2_data[s_name] = parsed_data\n                # Reset in case we find more in this log file\n                s_name = f['s_name']\n                parsed_data = {}", "response": "Parse the bowtie2 log file f and return the key bits of the log file as a list of key bits."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bowtie2_general_stats_table(self):\n\n        headers = OrderedDict()\n        headers['overall_alignment_rate'] = {\n            'title': '% Aligned',\n            'description': 'overall alignment rate',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'YlGn'\n        }\n        self.general_stats_addcols(self.bowtie2_data, headers)", "response": "Take the parsed stats from the Bowtie 2 report and add it to the basic stats table at the top of the report"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes the HTML to plot the alignment rates for the Bowtie 2.", "response": "def bowtie2_alignment_plot (self):\n        \"\"\" Make the HighCharts HTML to plot the alignment rates \"\"\"\n\n        half_warning = ''\n        for s_name in self.bowtie2_data:\n            if 'paired_aligned_mate_one_halved' in self.bowtie2_data[s_name] or 'paired_aligned_mate_multi_halved' in self.bowtie2_data[s_name] or 'paired_aligned_mate_none_halved' in self.bowtie2_data[s_name]:\n                half_warning = '<em>Please note that single mate alignment counts are halved to tally with pair counts properly.</em>'\n        description_text = 'This plot shows the number of reads aligning to the reference in different ways.'\n\n        # Config for the plot\n        config = {\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads'\n        }\n\n        # Two plots, don't mix SE with PE\n        if self.num_se > 0:\n            sekeys = OrderedDict()\n            sekeys['unpaired_aligned_one'] = { 'color': '#20568f', 'name': 'SE mapped uniquely' }\n            sekeys['unpaired_aligned_multi'] = { 'color': '#f7a35c', 'name': 'SE multimapped' }\n            sekeys['unpaired_aligned_none'] = { 'color': '#981919', 'name': 'SE not aligned' }\n            config['id'] = 'bowtie2_se_plot'\n            config['title'] = 'Bowtie 2: SE Alignment Scores'\n            self.add_section(\n                description = description_text,\n                helptext = '''\n                There are 3 possible types of alignment:\n                * **SE Mapped uniquely**: Read has only one occurence in the reference genome.\n                * **SE Multimapped**: Read has multiple occurence.\n                * **SE No aligned**: Read has no occurence.\n                ''',\n                plot = bargraph.plot(self.bowtie2_data, sekeys, config)\n            )\n\n        if self.num_pe > 0:\n            pekeys = OrderedDict()\n            pekeys['paired_aligned_one'] = { 'color': '#20568f', 'name': 'PE mapped uniquely' }\n            pekeys['paired_aligned_discord_one'] = { 'color': '#5c94ca', 'name': 'PE mapped discordantly uniquely' }\n            pekeys['paired_aligned_mate_one_halved'] = { 'color': '#95ceff', 'name': 'PE one mate mapped uniquely' }\n            pekeys['paired_aligned_multi'] = { 'color': '#f7a35c', 'name': 'PE multimapped' }\n            pekeys['paired_aligned_discord_multi'] = { 'color': '#dce333', 'name': 'PE discordantly multimapped' }\n            pekeys['paired_aligned_mate_multi_halved'] = { 'color': '#ffeb75', 'name': 'PE one mate multimapped' }\n            pekeys['paired_aligned_mate_none_halved'] = { 'color': '#981919', 'name': 'PE neither mate aligned' }\n            config['id'] = 'bowtie2_pe_plot'\n            config['title'] = 'Bowtie 2: PE Alignment Scores'\n            self.add_section(\n                description = \"<br>\".join([description_text,half_warning]),\n                helptext = '''\n                There are 6 possible types of alignment:\n                * **PE mapped uniquely**: Pair has only one occurence in the reference genome.\n                * **PE mapped discordantly uniquely**: Pair has only one occurence but not in proper pair.\n                * **PE one mate mapped uniquely**: One read of a pair has one occurence.\n                * **PE multimapped**: Pair has multiple occurence.\n                * **PE one mate multimapped**: One read of a pair has multiple occurence.\n                * **PE neither mate aligned**: Pair has no occurence.\n                ''',\n                plot = bargraph.plot(self.bowtie2_data, pekeys, config)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the JSON output from DamageProfiler and save the summary statistics", "response": "def parseJSON(self, f):\n        \"\"\" Parse the JSON output from DamageProfiler and save the summary statistics \"\"\"\n        try:\n            parsed_json = json.load(f['f'])\n        except Exception as e:\n            print(e)\n            log.warn(\"Could not parse DamageProfiler JSON: '{}'\".format(f['fn']))\n            return None\n\n        #Get sample name from JSON first\n        s_name = self.clean_s_name(parsed_json['metadata']['sample_name'],'')\n        self.add_data_source(f, s_name)\n\n\n        #Add 3' G to A data\n        self.threepGtoAfreq_data[s_name] = parsed_json['dmg_3p']\n\n        #Add 5' C to T data\n        self.fivepCtoTfreq_data[s_name] = parsed_json['dmg_5p']\n\n        #Add lendist forward\n        self.lgdist_fw_data[s_name] = parsed_json['lendist_fw']\n\n        #Add lendist reverse\n        self.lgdist_rv_data[s_name] = parsed_json['lendist_rv']\n\n        #Add summary metrics\n        self.summary_metrics_data[s_name] = parsed_json['summary_stats']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntake the parsed stats from the DamageProfiler and add it to the basic stats table at the top of the report", "response": "def dmgprof_misinc_stats(self, dict_to_plot, readend, substitution):\n        \"\"\" Take the parsed stats from the DamageProfiler and add it to the\n        basic stats table at the top of the report \"\"\"\n\n        headers = OrderedDict()\n        headers['{}1'.format(readend)] = {\n            'id': 'misinc-stats-1st-{}-{}'.format(readend, substitution),\n            'title': '{} {} 1st base'.format(readend, substitution),\n            'description': '{} 1st base substitution frequency for {}'.format(readend, substitution),\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'YlGnBu',\n            'modify': lambda x: x * 100.0\n        }\n        headers['{}2'.format(readend)] = {\n            'id': 'misinc-stats-2nd-{}-{}'.format(readend, substitution),\n            'title': '{} {} 2nd base'.format(readend, substitution),\n            'description': '{} 2nd base substitution frequency for {}'.format(readend, substitution),\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'BuGn',\n            'hidden': True,\n            'modify': lambda x: x * 100.0\n        }\n\n\n        # Create new small subset dictionary for entries (we need just the first two data (k,v) pairs from each report)\n        # Only the first two parts are informative from both 3' and 5' ends of reads (1st, 2nd base damage pattern)\n        data = OrderedDict()\n        dict_to_add = dict()\n\n        for key in dict_to_plot.keys():\n            tmp = dict_to_plot[key]\n            pos = [readend + '1', readend + '2']\n            strlist = tmp[:2]\n            tuples = list(zip(pos,strlist))\n            data = dict((x, y) for x, y in tuples)\n            #Extract first two elements from list\n            dict_to_add[key] = data\n\n        self.general_stats_addcols(dict_to_add,headers)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding the summary metrics to the basic stats table at the top of the report", "response": "def addSummaryMetrics(self, dict_to_plot):\n        \"\"\" Take the parsed stats from the DamageProfiler and add it to the\n        basic stats table at the top of the report \"\"\"\n\n        headers = OrderedDict()\n\n        headers['std'] = {\n            'title': 'Read length std. dev.',\n            'description': 'Read length std. dev.',\n            'suffix': 'bp',\n            'scale': 'PuBu',\n            'format': '{:,.2f}',\n            'shared_key': 'read_length',\n            'hidden': True\n        }\n\n        headers['median'] = {\n            'title': 'Median read length',\n            'description': 'Median read length',\n            'suffix': 'bp',\n            'scale': 'YlGnBu',\n            'format': '{:,.2f}',\n            'shared_key': 'read_length'\n        }\n        headers['mean_readlength'] = {\n            'title': 'Mean read length',\n            'description': 'Mean read length',\n            'suffix': 'bp',\n            'scale': 'PuBuGn',\n            'format': '{:,.2f}',\n            'shared_key': 'read_length',\n            'hidden': True\n        }\n\n        self.general_stats_addcols(dict_to_plot, headers)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a read length distribution plot", "response": "def lgdistplot(self,dict_to_use,orientation):\n        \"\"\"Generate a read length distribution plot\"\"\"\n\n        data = dict()\n        for s_name in dict_to_use:\n            try:\n                data[s_name] = {int(d): int (dict_to_use[s_name][d]) for d in dict_to_use[s_name]}\n            except KeyError:\n                pass\n        if len(data) == 0:\n            log.debug('No valid data for forward read lgdist input!')\n            return None\n\n        config = {\n            'id': 'length-distribution-{}'.format(orientation),\n            'title': 'DamageProfiler: Read length distribution: {} '.format(orientation),\n            'ylab': 'Number of reads',\n            'xlab': 'Readlength (bp)',\n            'xDecimals': False,\n            'tt_label': '{point.y} reads of length {point.x}',\n            'ymin': 0,\n            'xmin': 0\n        }\n        return linegraph.plot(data,config)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a 3 G > A linegraph plot", "response": "def threeprime_plot(self):\n        \"\"\"Generate a 3' G>A linegraph plot\"\"\"\n\n        data = dict()\n        dict_to_add = dict()\n        # Create tuples out of entries\n        for key in self.threepGtoAfreq_data:\n            pos = list(range(1,len(self.threepGtoAfreq_data.get(key))))\n            #Multiply values by 100 to get %\n            tmp = [i * 100.0 for i in self.threepGtoAfreq_data.get(key)]\n            tuples = list(zip(pos,tmp))\n            # Get a dictionary out of it\n            data = dict((x, y) for x, y in tuples)\n            dict_to_add[key] = data\n\n        config = {\n            'id': 'threeprime_misinc_plot',\n            'title': 'DamageProfiler: 3P G>A misincorporation plot',\n            'ylab': '% G to A substituted',\n            'xlab': 'Nucleotide position from 3\\'',\n            'tt_label': '{point.y:.2f} % G>A misincorporations at nucleotide position {point.x}',\n            'ymin': 0,\n            'xmin': 1\n        }\n\n        return linegraph.plot(dict_to_add,config)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot (data, cats = None, pconfig = None):\n\n    if pconfig is None:\n        pconfig = {}\n\n    # Allow user to overwrite any given config for this plot\n    if 'id' in pconfig and pconfig['id'] and pconfig['id'] in config.custom_plot_config:\n        for k, v in config.custom_plot_config[pconfig['id']].items():\n            pconfig[k] = v\n\n    # Validate config if linting\n    if config.lint:\n        # Get module name\n        modname = ''\n        callstack = inspect.stack()\n        for n in callstack:\n            if 'multiqc/modules/' in n[1] and 'base_module.py' not in n[1]:\n                callpath = n[1].split('multiqc/modules/',1)[-1]\n                modname = '>{}< '.format(callpath)\n                break\n        # Look for essential missing pconfig keys\n        for k in ['id', 'title', 'ylab']:\n            if k not in pconfig:\n                errmsg = \"LINT: {}Bargraph pconfig was missing key '{}'\".format(modname, k)\n                logger.error(errmsg)\n                report.lint_errors.append(errmsg)\n        # Check plot title format\n        if not re.match( r'^[^:]*\\S: \\S[^:]*$', pconfig.get('title', '')):\n            errmsg = \"LINT: {} Bargraph title did not match format 'Module: Plot Name' (found '{}')\".format(modname, pconfig.get('title', ''))\n            logger.error(errmsg)\n            report.lint_errors.append(errmsg)\n\n    # Given one dataset - turn it into a list\n    if type(data) is not list:\n        data = [data]\n\n    # Make list of cats from different inputs\n    if cats is None:\n        cats = list()\n    elif type(cats) is not list:\n        cats = [cats]\n    else:\n        try: # Py2\n            if type(cats[0]) is str or type(cats[0]) is unicode:\n                cats = [cats]\n        except NameError: # Py3\n            if type(cats[0]) is str:\n                cats = [cats]\n    # Generate default categories if not supplied\n    for idx in range(len(data)):\n        try:\n            cats[idx]\n        except (IndexError):\n            cats.append(list())\n            for s in data[idx].keys():\n                for k in data[idx][s].keys():\n                    if k not in cats[idx]:\n                        cats[idx].append(k)\n\n    # If we have cats in lists, turn them into dicts\n    for idx, cat in enumerate(cats):\n        if type(cat) is list:\n            newcats = OrderedDict()\n            for c in cat:\n                newcats[c] = {'name': c}\n            cats[idx] = newcats\n        else:\n            for c in cat:\n                if 'name' not in cat[c]:\n                    cats[idx][c]['name'] = c\n\n    # Parse the data into a chart friendly format\n    plotsamples = list()\n    plotdata = list()\n    for idx, d in enumerate(data):\n        if isinstance(d, OrderedDict):\n            hc_samples = list(d.keys())\n        else:\n            hc_samples = sorted(list(d.keys()))\n        hc_data = list()\n        sample_dcount = dict()\n        for c in cats[idx].keys():\n            thisdata = list()\n            catcount = 0\n            for s in hc_samples:\n                if s not in sample_dcount:\n                    sample_dcount[s] = 0\n                try:\n                    thisdata.append(float(d[s][c]))\n                    catcount += 1\n                    sample_dcount[s] += 1\n                except (KeyError, ValueError):\n                    # Pad with NaNs when we have missing categories in a sample\n                    thisdata.append(float('nan'))\n            if catcount > 0:\n                if pconfig.get('hide_zero_cats', True) is False or max(x for x in thisdata if not math.isnan(x)) > 0:\n                    thisdict = { 'name': cats[idx][c]['name'], 'data': thisdata }\n                    if 'color' in cats[idx][c]:\n                        thisdict['color'] = cats[idx][c]['color']\n                    hc_data.append(thisdict)\n\n        # Remove empty samples\n        for s, c in sample_dcount.items():\n            if c == 0:\n                idx = hc_samples.index(s)\n                del hc_samples[idx]\n                for j, d in enumerate(hc_data):\n                    del hc_data[j]['data'][idx]\n        if len(hc_data) > 0:\n            plotsamples.append(hc_samples)\n            plotdata.append(hc_data)\n\n    if len(plotdata) == 0:\n        logger.warning('Tried to make bar plot, but had no data')\n        return '<p class=\"text-danger\">Error - was not able to plot data.</p>'\n\n    # Make a plot - custom, interactive or flat\n    try:\n        return get_template_mod().bargraph(plotdata, plotsamples, pconfig)\n    except (AttributeError, TypeError):\n        if config.plots_force_flat or (not config.plots_force_interactive and len(plotsamples[0]) > config.plots_flat_numseries):\n            try:\n                return matplotlib_bargraph(plotdata, plotsamples, pconfig)\n            except:\n                logger.error(\"############### Error making MatPlotLib figure! Falling back to HighCharts.\")\n                return highcharts_bargraph(plotdata, plotsamples, pconfig)\n        else:\n            # Use MatPlotLib to generate static plots if requested\n            if config.export_plots:\n                matplotlib_bargraph(plotdata, plotsamples, pconfig)\n            # Return HTML for HighCharts dynamic plot\n            return highcharts_bargraph(plotdata, plotsamples, pconfig)", "response": "Plot a horizontal bar graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds the HTML needed for a HighCharts bar graph.", "response": "def highcharts_bargraph (plotdata, plotsamples=None, pconfig=None):\n    \"\"\"\n    Build the HTML needed for a HighCharts bar graph. Should be\n    called by plot_bargraph, which properly formats input data.\n    \"\"\"\n    if pconfig is None:\n        pconfig = {}\n    if pconfig.get('id') is None:\n        pconfig['id'] = 'mqc_hcplot_'+''.join(random.sample(letters, 10))\n\n    # Sanitise plot ID and check for duplicates\n    pconfig['id'] = report.save_htmlid(pconfig['id'])\n\n    html = '<div class=\"mqc_hcplot_plotgroup\">'\n\n    # Counts / Percentages / Log Switches\n    if pconfig.get('cpswitch') is not False or pconfig.get('logswitch') is True:\n        if pconfig.get('cpswitch_c_active', True) is True:\n            c_active = 'active'\n            p_active = ''\n            l_active = ''\n        elif pconfig.get('logswitch_active') is True:\n            c_active = ''\n            p_active = ''\n            l_active = 'active'\n        else:\n            c_active = ''\n            p_active = 'active'\n            l_active = ''\n            pconfig['stacking'] = 'percent'\n        c_label = pconfig.get('cpswitch_counts_label', 'Counts')\n        p_label = pconfig.get('cpswitch_percent_label', 'Percentages')\n        l_label = pconfig.get('logswitch_label', 'Log10')\n        html += '<div class=\"btn-group hc_switch_group\"> \\n'\n        html += '<button class=\"btn btn-default btn-sm {c_a}\" data-action=\"set_numbers\" data-target=\"{id}\" data-ylab=\"{c_l}\">{c_l}</button> \\n'.format(id=pconfig['id'], c_a=c_active, c_l=c_label)\n        if pconfig.get('cpswitch', True) is True:\n            html += '<button class=\"btn btn-default btn-sm {p_a}\" data-action=\"set_percent\" data-target=\"{id}\" data-ylab=\"{p_l}\">{p_l}</button> \\n'.format(id=pconfig['id'], p_a=p_active, p_l=p_label)\n        if pconfig.get('logswitch') is True:\n            html += '<button class=\"btn btn-default btn-sm {l_a}\" data-action=\"set_log\" data-target=\"{id}\" data-ylab=\"{l_l}\">{l_l}</button> \\n'.format(id=pconfig['id'], l_a=l_active, l_l=l_label)\n            pconfig['reversedStacks'] = True\n        html += '</div> '\n        if len(plotdata) > 1:\n            html += ' &nbsp; &nbsp; '\n\n    # Buttons to cycle through different datasets\n    if len(plotdata) > 1:\n        html += '<div class=\"btn-group hc_switch_group\">\\n'\n        for k, p in enumerate(plotdata):\n            active = 'active' if k == 0 else ''\n            try:\n                name = pconfig['data_labels'][k]['name']\n            except:\n                try:\n                    name = pconfig['data_labels'][k]\n                except:\n                    name = k+1\n            try:\n                ylab = 'data-ylab=\"{}\"'.format(pconfig['data_labels'][k]['ylab'])\n            except:\n                ylab = 'data-ylab=\"{}\"'.format(name) if name != k+1 else ''\n            try:\n                ymax = 'data-ymax=\"{}\"'.format(pconfig['data_labels'][k]['ymax'])\n            except:\n                ymax = ''\n            html += '<button class=\"btn btn-default btn-sm {a}\" data-action=\"set_data\" {y} {ym} data-newdata=\"{k}\" data-target=\"{id}\">{n}</button>\\n'.format(a=active, id=pconfig['id'], n=name, y=ylab, ym=ymax, k=k)\n        html += '</div>\\n\\n'\n\n    # Plot HTML\n    html += \"\"\"<div class=\"hc-plot-wrapper\">\n        <div id=\"{id}\" class=\"hc-plot not_rendered hc-bar-plot\"><small>loading..</small></div>\n    </div></div>\"\"\".format(id=pconfig['id']);\n\n    report.num_hc_plots += 1\n\n    report.plot_data[pconfig['id']] = {\n        'plot_type': 'bar_graph',\n        'samples': plotsamples,\n        'datasets': plotdata,\n        'config': pconfig\n    }\n\n    return html"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef matplotlib_bargraph (plotdata, plotsamples, pconfig=None):\n\n    if pconfig is None:\n        pconfig = {}\n\n    # Plot group ID\n    if pconfig.get('id') is None:\n        pconfig['id'] = 'mqc_mplplot_'+''.join(random.sample(letters, 10))\n\n    # Sanitise plot ID and check for duplicates\n    pconfig['id'] = report.save_htmlid(pconfig['id'])\n\n    # Individual plot IDs\n    pids = []\n    for k in range(len(plotdata)):\n        try:\n            name = pconfig['data_labels'][k]\n        except:\n            name = k+1\n        pid = 'mqc_{}_{}'.format(pconfig['id'], name)\n        pid = report.save_htmlid(pid, skiplint=True)\n        pids.append(pid)\n\n    html = '<p class=\"text-info\"><small><span class=\"glyphicon glyphicon-picture\" aria-hidden=\"true\"></span> ' + \\\n          'Flat image plot. Toolbox functions such as highlighting / hiding samples will not work ' + \\\n          '(see the <a href=\"http://multiqc.info/docs/#flat--interactive-plots\" target=\"_blank\">docs</a>).</small></p>'\n    html += '<div class=\"mqc_mplplot_plotgroup\" id=\"{}\">'.format(pconfig['id'])\n\n    # Same defaults as HighCharts for consistency\n    default_colors = ['#7cb5ec', '#434348', '#90ed7d', '#f7a35c', '#8085e9',\n                      '#f15c80', '#e4d354', '#2b908f', '#f45b5b', '#91e8e1']\n\n    # Counts / Percentages Switch\n    if pconfig.get('cpswitch') is not False and not config.simple_output:\n        if pconfig.get('cpswitch_c_active', True) is True:\n            c_active = 'active'\n            p_active = ''\n        else:\n            c_active = ''\n            p_active = 'active'\n            pconfig['stacking'] = 'percent'\n        c_label = pconfig.get('cpswitch_counts_label', 'Counts')\n        p_label = pconfig.get('cpswitch_percent_label', 'Percentages')\n        html += '<div class=\"btn-group mpl_switch_group mqc_mplplot_bargraph_setcountspcnt\"> \\n\\\n            <button class=\"btn btn-default btn-sm {c_a} counts\">{c_l}</button> \\n\\\n            <button class=\"btn btn-default btn-sm {p_a} pcnt\">{p_l}</button> \\n\\\n        </div> '.format(c_a=c_active, p_a=p_active, c_l=c_label, p_l=p_label)\n        if len(plotdata) > 1:\n            html += ' &nbsp; &nbsp; '\n\n    # Buttons to cycle through different datasets\n    if len(plotdata) > 1 and not config.simple_output:\n        html += '<div class=\"btn-group mpl_switch_group mqc_mplplot_bargraph_switchds\">\\n'\n        for k, p in enumerate(plotdata):\n            pid = pids[k]\n            active = 'active' if k == 0 else ''\n            try:\n                name = pconfig['data_labels'][k]\n            except:\n                name = k+1\n            html += '<button class=\"btn btn-default btn-sm {a}\" data-target=\"#{pid}\">{n}</button>\\n'.format(a=active, pid=pid, n=name)\n        html += '</div>\\n\\n'\n\n    # Go through datasets creating plots\n    for pidx, pdata in enumerate(plotdata):\n\n        # Save plot data to file\n        fdata = {}\n        for d in pdata:\n            for didx, dval in enumerate(d['data']):\n                s_name = plotsamples[pidx][didx]\n                if s_name not in fdata:\n                    fdata[s_name] = dict()\n                fdata[s_name][d['name']] = dval\n        util_functions.write_data_file(fdata, pids[pidx])\n\n        # Plot percentage as well as counts\n        plot_pcts = [False]\n        if pconfig.get('cpswitch') is not False:\n            plot_pcts = [False, True]\n\n        # Switch out NaN for 0s so that MatPlotLib doesn't ignore stuff\n        for idx, d in enumerate(pdata):\n            pdata[idx]['data'] = [x if not math.isnan(x) else 0 for x in d['data'] ]\n\n        for plot_pct in plot_pcts:\n\n            # Plot ID\n            pid = pids[pidx]\n            hide_plot = False\n            if plot_pct is True:\n                pid = '{}_pc'.format(pid)\n                if pconfig.get('cpswitch_c_active', True) is True:\n                    hide_plot = True\n            else:\n                if pconfig.get('cpswitch_c_active', True) is not True:\n                    hide_plot = True\n\n            # Set up figure\n            plt_height = len(plotsamples[pidx]) / 2.3\n            plt_height = max(6, plt_height) # At least 6\" tall\n            plt_height = min(30, plt_height) # Cap at 30\" tall\n            bar_width = 0.8\n\n            fig = plt.figure(figsize=(14, plt_height), frameon=False)\n            axes = fig.add_subplot(111)\n            y_ind = range(len(plotsamples[pidx]))\n\n            # Count totals for each sample\n            if plot_pct is True:\n                s_totals = [0 for _ in pdata[0]['data']]\n                for series_idx, d in enumerate(pdata):\n                    for sample_idx, v in enumerate(d['data']):\n                        s_totals[sample_idx] += v\n\n            # Plot bars\n            dlabels = []\n            prev_values = None\n            for idx, d in enumerate(pdata):\n                # Plot percentages\n                values = [x for x in d['data']]\n                if len(values) < len(y_ind):\n                    values.extend([0] * (len(y_ind) - len(values)))\n                if plot_pct is True:\n                    for (key,var) in enumerate(values):\n                        s_total = s_totals[key]\n                        if s_total == 0:\n                            values[key] = 0\n                        else:\n                            values[key] = (float(var+0.0)/float(s_total))*100\n\n                # Get offset for stacked bars\n                if idx == 0:\n                    prevdata = [0] * len(plotsamples[pidx])\n                else:\n                    for i, p in enumerate(prevdata):\n                        prevdata[i] += prev_values[i]\n                # Default colour index\n                cidx = idx\n                while cidx >= len(default_colors):\n                    cidx -= len(default_colors)\n                # Save the name of this series\n                dlabels.append(d['name'])\n                # Add the series of bars to the plot\n                axes.barh(\n                    y_ind,\n                    values,\n                    bar_width,\n                    left = prevdata,\n                    color = d.get('color', default_colors[cidx]),\n                    align = 'center',\n                    linewidth = pconfig.get('borderWidth', 0)\n                )\n                prev_values = values\n\n            # Tidy up axes\n            axes.tick_params(labelsize=8, direction='out', left=False, right=False, top=False, bottom=False)\n            axes.set_xlabel(pconfig.get('ylab', '')) # I know, I should fix the fact that the config is switched\n            axes.set_ylabel(pconfig.get('xlab', ''))\n            axes.set_yticks(y_ind) # Specify where to put the labels\n            axes.set_yticklabels(plotsamples[pidx]) # Set y axis sample name labels\n            axes.set_ylim((-0.5, len(y_ind)-0.5)) # Reduce padding around plot area\n            if plot_pct is True:\n                axes.set_xlim((0, 100))\n                # Add percent symbols\n                vals = axes.get_xticks()\n                axes.set_xticklabels(['{:.0f}%'.format(x) for x in vals])\n            else:\n                default_xlimits = axes.get_xlim()\n                axes.set_xlim((pconfig.get('ymin', default_xlimits[0]),pconfig.get('ymax', default_xlimits[1])))\n            if 'title' in pconfig:\n                top_gap = 1 + (0.5 / plt_height)\n                plt.text(0.5, top_gap, pconfig['title'], horizontalalignment='center', fontsize=16, transform=axes.transAxes)\n            axes.grid(True, zorder=0, which='both', axis='x', linestyle='-', color='#dedede', linewidth=1)\n            axes.set_axisbelow(True)\n            axes.spines['right'].set_visible(False)\n            axes.spines['top'].set_visible(False)\n            axes.spines['bottom'].set_visible(False)\n            axes.spines['left'].set_visible(False)\n            plt.gca().invert_yaxis() # y axis is reverse sorted otherwise\n\n            # Hide some labels if we have a lot of samples\n            show_nth = max(1, math.ceil(len(pdata[0]['data'])/150))\n            for idx, label in enumerate(axes.get_yticklabels()):\n                if idx % show_nth != 0:\n                    label.set_visible(False)\n\n            # Legend\n            bottom_gap = -1 * (1 - ((plt_height - 1.5) / plt_height))\n            lgd = axes.legend(dlabels, loc='lower center', bbox_to_anchor=(0, bottom_gap, 1, .102), ncol=5, mode='expand', fontsize=8, frameon=False)\n\n            # Should this plot be hidden on report load?\n            hidediv = ''\n            if pidx > 0 or hide_plot:\n                hidediv = ' style=\"display:none;\"'\n\n            # Save the plot to the data directory if export is requested\n            if config.export_plots:\n                for fformat in config.export_plot_formats:\n                    # Make the directory if it doesn't already exist\n                    plot_dir = os.path.join(config.plots_dir, fformat)\n                    if not os.path.exists(plot_dir):\n                        os.makedirs(plot_dir)\n                    # Save the plot\n                    plot_fn = os.path.join(plot_dir, '{}.{}'.format(pid, fformat))\n                    fig.savefig(plot_fn, format=fformat, bbox_extra_artists=(lgd,), bbox_inches='tight')\n\n            # Output the figure to a base64 encoded string\n            if getattr(get_template_mod(), 'base64_plots', True) is True:\n                img_buffer = io.BytesIO()\n                fig.savefig(img_buffer, format='png', bbox_inches='tight')\n                b64_img = base64.b64encode(img_buffer.getvalue()).decode('utf8')\n                img_buffer.close()\n                html += '<div class=\"mqc_mplplot\" id=\"{}\"{}><img src=\"data:image/png;base64,{}\" /></div>'.format(pid, hidediv, b64_img)\n\n            # Link to the saved image\n            else:\n                plot_relpath = os.path.join(config.plots_dir_name, 'png', '{}.png'.format(pid))\n                html += '<div class=\"mqc_mplplot\" id=\"{}\"{}><img src=\"{}\" /></div>'.format(pid, hidediv, plot_relpath)\n\n            plt.close(fig)\n\n\n    # Close wrapping div\n    html += '</div>'\n\n    report.num_mpl_plots += 1\n\n    return html", "response": "Generates a bargraph of the plot data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a fastq_data. txt file and returns a dict with keys stats and data.", "response": "def parse_fastqc_report(self, file_contents, s_name=None, f=None):\n        \"\"\" Takes contents from a fastq_data.txt file and parses out required\n        statistics and data. Returns a dict with keys 'stats' and 'data'.\n        Data is for plotting graphs, stats are for top table. \"\"\"\n\n        # Make the sample name from the input filename if we find it\n        fn_search = re.search(r\"Filename\\s+(.+)\", file_contents)\n        if fn_search:\n            s_name = self.clean_s_name(fn_search.group(1) , f['root'])\n\n        if s_name in self.fastqc_data.keys():\n            log.debug(\"Duplicate sample name found! Overwriting: {}\".format(s_name))\n        self.add_data_source(f, s_name)\n        self.fastqc_data[s_name] = { 'statuses': dict() }\n\n        # Parse the report\n        section = None\n        s_headers = None\n        self.dup_keys = []\n        for l in file_contents.splitlines():\n            if l == '>>END_MODULE':\n                section = None\n                s_headers = None\n            elif l.startswith('>>'):\n                (section, status) = l[2:].split(\"\\t\", 1)\n                section = section.lower().replace(' ', '_')\n                self.fastqc_data[s_name]['statuses'][section] = status\n            elif section is not None:\n                if l.startswith('#'):\n                    s_headers = l[1:].split(\"\\t\")\n                    # Special case: Total Deduplicated Percentage header line\n                    if s_headers[0] == 'Total Deduplicated Percentage':\n                        self.fastqc_data[s_name]['basic_statistics'].append({\n                            'measure': 'total_deduplicated_percentage',\n                            'value': float(s_headers[1])\n                        })\n                    else:\n                        # Special case: Rename dedup header in old versions of FastQC (v10)\n                        if s_headers[1] == 'Relative count':\n                            s_headers[1] = 'Percentage of total'\n                        s_headers = [s.lower().replace(' ', '_') for s in s_headers]\n                        self.fastqc_data[s_name][section] = list()\n\n                elif s_headers is not None:\n                    s = l.split(\"\\t\")\n                    row = dict()\n                    for (i, v) in enumerate(s):\n                        v.replace('NaN','0')\n                        try:\n                            v = float(v)\n                        except ValueError:\n                            pass\n                        row[s_headers[i]] = v\n                    self.fastqc_data[s_name][section].append(row)\n                    # Special case - need to remember order of duplication keys\n                    if section == 'sequence_duplication_levels':\n                        try:\n                            self.dup_keys.append(float(s[0]))\n                        except ValueError:\n                            self.dup_keys.append(s[0])\n\n        # Tidy up the Basic Stats\n        self.fastqc_data[s_name]['basic_statistics'] = {d['measure']: d['value'] for d in self.fastqc_data[s_name]['basic_statistics']}\n\n        # Calculate the average sequence length (Basic Statistics gives a range)\n        length_bp = 0\n        total_count = 0\n        for d in self.fastqc_data[s_name].get('sequence_length_distribution', {}):\n            length_bp += d['count'] * self.avg_bp_from_range(d['length'])\n            total_count += d['count']\n        if total_count > 0:\n            self.fastqc_data[s_name]['basic_statistics']['avg_sequence_length'] = length_bp / total_count"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fastqc_general_stats(self):\n\n        # Prep the data\n        data = dict()\n        for s_name in self.fastqc_data:\n            bs = self.fastqc_data[s_name]['basic_statistics']\n            data[s_name] = {\n                'percent_gc': bs['%GC'],\n                'avg_sequence_length': bs['avg_sequence_length'],\n                'total_sequences': bs['Total Sequences'],\n            }\n            try:\n                data[s_name]['percent_duplicates'] = 100 - bs['total_deduplicated_percentage']\n            except KeyError:\n                pass # Older versions of FastQC don't have this\n            # Add count of fail statuses\n            num_statuses = 0\n            num_fails = 0\n            for s in self.fastqc_data[s_name]['statuses'].values():\n                num_statuses += 1\n                if s == 'fail':\n                    num_fails += 1\n            data[s_name]['percent_fails'] = (float(num_fails)/float(num_statuses))*100.0\n\n        # Are sequence lengths interesting?\n        seq_lengths = [x['avg_sequence_length'] for x in data.values()]\n        hide_seq_length = False if max(seq_lengths) - min(seq_lengths) > 10 else True\n\n        headers = OrderedDict()\n        headers['percent_duplicates'] = {\n            'title': '% Dups',\n            'description': '% Duplicate Reads',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'RdYlGn-rev'\n        }\n        headers['percent_gc'] = {\n            'title': '% GC',\n            'description': 'Average % GC Content',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'Set1',\n            'format': '{:,.0f}'\n        }\n        headers['avg_sequence_length'] = {\n            'title': 'Length',\n            'description': 'Average Sequence Length (bp)',\n            'min': 0,\n            'suffix': ' bp',\n            'scale': 'RdYlGn',\n            'format': '{:,.0f}',\n            'hidden': hide_seq_length\n        }\n        headers['percent_fails'] = {\n            'title': '% Failed',\n            'description': 'Percentage of modules failed in FastQC report (includes those not plotted here)',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'Reds',\n            'format': '{:,.0f}',\n            'hidden': True\n        }\n        headers['total_sequences'] = {\n            'title': '{} Seqs'.format(config.read_count_prefix),\n            'description': 'Total Sequences ({})'.format(config.read_count_desc),\n            'min': 0,\n            'scale': 'Blues',\n            'modify': lambda x: x * config.read_count_multiplier,\n            'shared_key': 'read_count'\n        }\n        self.general_stats_addcols(data, headers)", "response": "Add some stats to the basic statistics table at the top of the report"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating the read count plot for each sample in the FastQC.", "response": "def read_count_plot (self):\n        \"\"\" Stacked bar plot showing counts of reads \"\"\"\n        pconfig = {\n            'id': 'fastqc_sequence_counts_plot',\n            'title': 'FastQC: Sequence Counts',\n            'ylab': 'Number of reads',\n            'cpswitch_counts_label': 'Number of reads',\n            'hide_zero_cats': False\n        }\n        pdata = dict()\n        has_dups = False\n        has_total = False\n        for s_name in self.fastqc_data:\n            pd = self.fastqc_data[s_name]['basic_statistics']\n            pdata[s_name] = dict()\n            try:\n                pdata[s_name]['Duplicate Reads'] = int(((100.0 - float(pd['total_deduplicated_percentage']))/100.0) * pd['Total Sequences'])\n                pdata[s_name]['Unique Reads'] = pd['Total Sequences'] - pdata[s_name]['Duplicate Reads']\n                has_dups = True\n            except KeyError:\n                # Older versions of FastQC don't have duplicate reads\n                pdata[s_name] = { 'Total Sequences': pd['Total Sequences'] }\n                has_total = True\n        pcats = list()\n        duptext = ''\n        if has_total:\n            pcats.append('Total Sequences')\n        if has_dups:\n            pcats.extend(['Unique Reads', 'Duplicate Reads'])\n            duptext = ' Duplicate read counts are an estimate only.'\n        if has_total and not has_dups:\n            pconfig['use_legend'] = False\n            pconfig['cpswitch'] = False\n        self.add_section (\n            name = 'Sequence Counts',\n            anchor = 'fastqc_sequence_counts',\n            description = 'Sequence counts for each sample.'+duptext,\n            helptext = '''\n            This plot show the total number of reads, broken down into unique and duplicate\n            if possible (only more recent versions of FastQC give duplicate info).\n\n            You can read more about duplicate calculation in the\n            [FastQC documentation](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/3%20Analysis%20Modules/8%20Duplicate%20Sequences.html).\n            A small part has been copied here for convenience:\n\n            _Only sequences which first appear in the first 100,000 sequences\n            in each file are analysed. This should be enough to get a good impression\n            for the duplication levels in the whole file. Each sequence is tracked to\n            the end of the file to give a representative count of the overall duplication level._\n\n            _The duplication detection requires an exact sequence match over the whole length of\n            the sequence. Any reads over 75bp in length are truncated to 50bp for this analysis._\n            ''',\n            plot = bargraph.plot(pdata, pcats, pconfig)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sequence_quality_plot (self):\n\n        data = dict()\n        for s_name in self.fastqc_data:\n            try:\n                data[s_name] = {self.avg_bp_from_range(d['base']): d['mean'] for d in self.fastqc_data[s_name]['per_base_sequence_quality']}\n            except KeyError:\n                pass\n        if len(data) == 0:\n            log.debug('sequence_quality not found in FastQC reports')\n            return None\n\n        pconfig = {\n            'id': 'fastqc_per_base_sequence_quality_plot',\n            'title': 'FastQC: Mean Quality Scores',\n            'ylab': 'Phred Score',\n            'xlab': 'Position (bp)',\n            'ymin': 0,\n            'xDecimals': False,\n            'tt_label': '<b>Base {point.x}</b>: {point.y:.2f}',\n            'colors': self.get_status_cols('per_base_sequence_quality'),\n            'yPlotBands': [\n                {'from': 28, 'to': 100, 'color': '#c3e6c3'},\n                {'from': 20, 'to': 28, 'color': '#e6dcc3'},\n                {'from': 0, 'to': 20, 'color': '#e6c3c3'},\n            ]\n        }\n        self.add_section (\n            name = 'Sequence Quality Histograms',\n            anchor = 'fastqc_per_base_sequence_quality',\n            description = 'The mean quality value across each base position in the read.',\n            helptext = '''\n            To enable multiple samples to be plotted on the same graph, only the mean quality\n            scores are plotted (unlike the box plots seen in FastQC reports).\n\n            Taken from the [FastQC help](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/3%20Analysis%20Modules/2%20Per%20Base%20Sequence%20Quality.html):\n\n            _The y-axis on the graph shows the quality scores. The higher the score, the better\n            the base call. The background of the graph divides the y axis into very good quality\n            calls (green), calls of reasonable quality (orange), and calls of poor quality (red).\n            The quality of calls on most platforms will degrade as the run progresses, so it is\n            common to see base calls falling into the orange area towards the end of a read._\n            ''',\n            plot = linegraph.plot(data, pconfig)\n        )", "response": "Create the HTML for the sequence quality histogram plot"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate the HTML for the sequence content heatmap for the FastQC report", "response": "def sequence_content_plot (self):\n        \"\"\" Create the epic HTML for the FastQC sequence content heatmap \"\"\"\n\n        # Prep the data\n        data = OrderedDict()\n        for s_name in sorted(self.fastqc_data.keys()):\n            try:\n                data[s_name] = {self.avg_bp_from_range(d['base']): d for d in self.fastqc_data[s_name]['per_base_sequence_content']}\n            except KeyError:\n                pass\n            # Old versions of FastQC give counts instead of percentages\n            for b in data[s_name]:\n                tot = sum([data[s_name][b][base] for base in ['a','c','t','g']])\n                if tot == 100.0:\n                    break\n                else:\n                    for base in ['a','c','t','g']:\n                        data[s_name][b][base] = (float(data[s_name][b][base])/float(tot)) * 100.0\n        if len(data) == 0:\n            log.debug('sequence_content not found in FastQC reports')\n            return None\n\n        html = '''<div id=\"fastqc_per_base_sequence_content_plot_div\">\n            <div class=\"alert alert-info\">\n               <span class=\"glyphicon glyphicon-hand-up\"></span>\n               Click a sample row to see a line plot for that dataset.\n            </div>\n            <h5><span class=\"s_name text-primary\"><span class=\"glyphicon glyphicon-info-sign\"></span> Rollover for sample name</span></h5>\n            <button id=\"fastqc_per_base_sequence_content_export_btn\"><span class=\"glyphicon glyphicon-download-alt\"></span> Export Plot</button>\n            <div class=\"fastqc_seq_heatmap_key\">\n                Position: <span id=\"fastqc_seq_heatmap_key_pos\">-</span>\n                <div><span id=\"fastqc_seq_heatmap_key_t\"> %T: <span>-</span></span></div>\n                <div><span id=\"fastqc_seq_heatmap_key_c\"> %C: <span>-</span></span></div>\n                <div><span id=\"fastqc_seq_heatmap_key_a\"> %A: <span>-</span></span></div>\n                <div><span id=\"fastqc_seq_heatmap_key_g\"> %G: <span>-</span></span></div>\n            </div>\n            <div id=\"fastqc_seq_heatmap_div\" class=\"fastqc-overlay-plot\">\n                <div id=\"fastqc_per_base_sequence_content_plot\" class=\"hc-plot has-custom-export\">\n                    <canvas id=\"fastqc_seq_heatmap\" height=\"100%\" width=\"800px\" style=\"width:100%;\"></canvas>\n                </div>\n            </div>\n            <div class=\"clearfix\"></div>\n        </div>\n        <script type=\"text/javascript\">\n            fastqc_seq_content_data = {d};\n            $(function () {{ fastqc_seq_content_heatmap(); }});\n        </script>'''.format(d=json.dumps(data))\n\n        self.add_section (\n            name = 'Per Base Sequence Content',\n            anchor = 'fastqc_per_base_sequence_content',\n            description = 'The proportion of each base position for which each of the four normal DNA bases has been called.',\n            helptext = '''\n            To enable multiple samples to be shown in a single plot, the base composition data\n            is shown as a heatmap. The colours represent the balance between the four bases:\n            an even distribution should give an even muddy brown colour. Hover over the plot\n            to see the percentage of the four bases under the cursor.\n\n            **To see the data as a line plot, as in the original FastQC graph, click on a sample track.**\n\n            From the [FastQC help](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/3%20Analysis%20Modules/4%20Per%20Base%20Sequence%20Content.html):\n\n            _Per Base Sequence Content plots out the proportion of each base position in a\n            file for which each of the four normal DNA bases has been called._\n\n            _In a random library you would expect that there would be little to no difference\n            between the different bases of a sequence run, so the lines in this plot should\n            run parallel with each other. The relative amount of each base should reflect\n            the overall amount of these bases in your genome, but in any case they should\n            not be hugely imbalanced from each other._\n\n            _It's worth noting that some types of library will always produce biased sequence\n            composition, normally at the start of the read. Libraries produced by priming\n            using random hexamers (including nearly all RNA-Seq libraries) and those which\n            were fragmented using transposases inherit an intrinsic bias in the positions\n            at which reads start. This bias does not concern an absolute sequence, but instead\n            provides enrichement of a number of different K-mers at the 5' end of the reads.\n            Whilst this is a true technical bias, it isn't something which can be corrected\n            by trimming and in most cases doesn't seem to adversely affect the downstream\n            analysis._\n            ''',\n            content = html\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gc_content_plot (self):\n\n        data = dict()\n        data_norm = dict()\n        for s_name in self.fastqc_data:\n            try:\n                data[s_name] = {d['gc_content']: d['count'] for d in self.fastqc_data[s_name]['per_sequence_gc_content']}\n            except KeyError:\n                pass\n            else:\n                data_norm[s_name] = dict()\n                total = sum( [ c for c in data[s_name].values() ] )\n                for gc, count in data[s_name].items():\n                    data_norm[s_name][gc] = (count / total) * 100\n        if len(data) == 0:\n            log.debug('per_sequence_gc_content not found in FastQC reports')\n            return None\n\n        pconfig = {\n            'id': 'fastqc_per_sequence_gc_content_plot',\n            'title': 'FastQC: Per Sequence GC Content',\n            'xlab': '% GC',\n            'ymin': 0,\n            'xmax': 100,\n            'xmin': 0,\n            'yDecimals': False,\n            'tt_label': '<b>{point.x}% GC</b>: {point.y}',\n            'colors': self.get_status_cols('per_sequence_gc_content'),\n            'data_labels': [\n                {'name': 'Percentages', 'ylab': 'Percentage'},\n                {'name': 'Counts', 'ylab': 'Count'}\n            ]\n        }\n\n        # Try to find and plot a theoretical GC line\n        theoretical_gc = None\n        theoretical_gc_raw = None\n        theoretical_gc_name = None\n        for f in self.find_log_files('fastqc/theoretical_gc'):\n            if theoretical_gc_raw is not None:\n                log.warn(\"Multiple FastQC Theoretical GC Content files found, now using {}\".format(f['fn']))\n            theoretical_gc_raw = f['f']\n            theoretical_gc_name = f['fn']\n        if theoretical_gc_raw is None:\n            tgc = getattr(config, 'fastqc_config', {}).get('fastqc_theoretical_gc', None)\n            if tgc is not None:\n                theoretical_gc_name = os.path.basename(tgc)\n                tgc_fn = 'fastqc_theoretical_gc_{}.txt'.format(tgc)\n                tgc_path = os.path.join(os.path.dirname(__file__), 'fastqc_theoretical_gc', tgc_fn)\n                if not os.path.isfile(tgc_path):\n                    tgc_path = tgc\n                try:\n                    with io.open (tgc_path, \"r\", encoding='utf-8') as f:\n                        theoretical_gc_raw = f.read()\n                except IOError:\n                    log.warn(\"Couldn't open FastQC Theoretical GC Content file {}\".format(tgc_path))\n                    theoretical_gc_raw = None\n        if theoretical_gc_raw is not None:\n            theoretical_gc = list()\n            for l in theoretical_gc_raw.splitlines():\n                if '# FastQC theoretical GC content curve:' in l:\n                    theoretical_gc_name = l[39:]\n                elif not l.startswith('#'):\n                    s = l.split()\n                    try:\n                        theoretical_gc.append([float(s[0]), float(s[1])])\n                    except (TypeError, IndexError):\n                        pass\n\n        desc = '''The average GC content of reads. Normal random library typically have a\n        roughly normal distribution of GC content.'''\n        if theoretical_gc is not None:\n            # Calculate the count version of the theoretical data based on the largest data store\n            max_total = max([sum (d.values()) for d in data.values() ])\n            esconfig = {\n                'name': 'Theoretical GC Content',\n                'dashStyle': 'Dash',\n                'lineWidth': 2,\n                'color': '#000000',\n                'marker': { 'enabled': False },\n                'enableMouseTracking': False,\n                'showInLegend': False,\n            }\n            pconfig['extra_series'] = [ [dict(esconfig)], [dict(esconfig)] ]\n            pconfig['extra_series'][0][0]['data'] = theoretical_gc\n            pconfig['extra_series'][1][0]['data'] = [ [ d[0], (d[1]/100.0)*max_total ] for d in theoretical_gc ]\n            desc = \" **The dashed black line shows theoretical GC content:** `{}`\".format(theoretical_gc_name)\n\n        self.add_section (\n            name = 'Per Sequence GC Content',\n            anchor = 'fastqc_per_sequence_gc_content',\n            description = desc,\n            helptext = '''\n            From the [FastQC help](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/3%20Analysis%20Modules/5%20Per%20Sequence%20GC%20Content.html):\n\n            _This module measures the GC content across the whole length of each sequence\n            in a file and compares it to a modelled normal distribution of GC content._\n\n            _In a normal random library you would expect to see a roughly normal distribution\n            of GC content where the central peak corresponds to the overall GC content of\n            the underlying genome. Since we don't know the the GC content of the genome the\n            modal GC content is calculated from the observed data and used to build a\n            reference distribution._\n\n            _An unusually shaped distribution could indicate a contaminated library or\n            some other kinds of biased subset. A normal distribution which is shifted\n            indicates some systematic bias which is independent of base position. If there\n            is a systematic bias which creates a shifted normal distribution then this won't\n            be flagged as an error by the module since it doesn't know what your genome's\n            GC content should be._\n            ''',\n            plot = linegraph.plot([data_norm, data], pconfig)\n        )", "response": "Create the HTML for the FastQC GC content plot"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating the HTML for the Sequence Length Distribution plot", "response": "def seq_length_dist_plot (self):\n        \"\"\" Create the HTML for the Sequence Length Distribution plot \"\"\"\n\n        data = dict()\n        seq_lengths = set()\n        multiple_lenths = False\n        for s_name in self.fastqc_data:\n            try:\n                data[s_name] = {self.avg_bp_from_range(d['length']): d['count'] for d in self.fastqc_data[s_name]['sequence_length_distribution']}\n                seq_lengths.update(data[s_name].keys())\n                if len(set(data[s_name].keys())) > 1:\n                    multiple_lenths = True\n            except KeyError:\n                pass\n        if len(data) == 0:\n            log.debug('sequence_length_distribution not found in FastQC reports')\n            return None\n\n        if not multiple_lenths:\n            lengths = 'bp , '.join([str(l) for l in list(seq_lengths)])\n            desc = 'All samples have sequences of a single length ({}bp).'.format(lengths)\n            if len(seq_lengths) > 1:\n                desc += ' See the <a href=\"#general_stats\">General Statistics Table</a>.'\n            self.add_section (\n                name = 'Sequence Length Distribution',\n                anchor = 'fastqc_sequence_length_distribution',\n                description = '<div class=\"alert alert-info\">{}</div>'.format(desc)\n            )\n        else:\n            pconfig = {\n                'id': 'fastqc_sequence_length_distribution_plot',\n                'title': 'FastQC: Sequence Length Distribution',\n                'ylab': 'Read Count',\n                'xlab': 'Sequence Length (bp)',\n                'ymin': 0,\n                'yMinTickInterval': 0.1,\n                'xDecimals': False,\n                'colors': self.get_status_cols('sequence_length_distribution'),\n                'tt_label': '<b>{point.x} bp</b>: {point.y}',\n            }\n            self.add_section (\n                name = 'Sequence Length Distribution',\n                anchor = 'fastqc_sequence_length_distribution',\n                description = '''The distribution of fragment sizes (read lengths) found.\n                    See the [FastQC help](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/3%20Analysis%20Modules/7%20Sequence%20Length%20Distribution.html)''',\n                plot = linegraph.plot(data, pconfig)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef seq_dup_levels_plot (self):\n\n        data = dict()\n        max_dupval = 0\n        for s_name in self.fastqc_data:\n            try:\n                thisdata = {}\n                for d in self.fastqc_data[s_name]['sequence_duplication_levels']:\n                    thisdata[d['duplication_level']] = d['percentage_of_total']\n                    max_dupval = max(max_dupval, d['percentage_of_total'])\n                data[s_name] = OrderedDict()\n                for k in self.dup_keys:\n                    try:\n                        data[s_name][k] = thisdata[k]\n                    except KeyError:\n                        pass\n            except KeyError:\n                pass\n        if len(data) == 0:\n            log.debug('sequence_length_distribution not found in FastQC reports')\n            return None\n        pconfig = {\n            'id': 'fastqc_sequence_duplication_levels_plot',\n            'title': 'FastQC: Sequence Duplication Levels',\n            'categories': True,\n            'ylab': '% of Library',\n            'xlab': 'Sequence Duplication Level',\n            'ymax': 100 if max_dupval <= 100.0 else None,\n            'ymin': 0,\n            'yMinTickInterval': 0.1,\n            'colors': self.get_status_cols('sequence_duplication_levels'),\n            'tt_label': '<b>{point.x}</b>: {point.y:.1f}%',\n        }\n\n        self.add_section (\n            name = 'Sequence Duplication Levels',\n            anchor = 'fastqc_sequence_duplication_levels',\n            description = 'The relative level of duplication found for every sequence.',\n            helptext = '''\n            From the [FastQC Help](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/3%20Analysis%20Modules/8%20Duplicate%20Sequences.html):\n\n            _In a diverse library most sequences will occur only once in the final set.\n            A low level of duplication may indicate a very high level of coverage of the\n            target sequence, but a high level of duplication is more likely to indicate\n            some kind of enrichment bias (eg PCR over amplification). This graph shows\n            the degree of duplication for every sequence in a library: the relative\n            number of sequences with different degrees of duplication._\n\n            _Only sequences which first appear in the first 100,000 sequences\n            in each file are analysed. This should be enough to get a good impression\n            for the duplication levels in the whole file. Each sequence is tracked to\n            the end of the file to give a representative count of the overall duplication level._\n\n            _The duplication detection requires an exact sequence match over the whole length of\n            the sequence. Any reads over 75bp in length are truncated to 50bp for this analysis._\n\n            _In a properly diverse library most sequences should fall into the far left of the\n            plot in both the red and blue lines. A general level of enrichment, indicating broad\n            oversequencing in the library will tend to flatten the lines, lowering the low end\n            and generally raising other categories. More specific enrichments of subsets, or\n            the presence of low complexity contaminants will tend to produce spikes towards the\n            right of the plot._\n            ''',\n            plot = linegraph.plot(data, pconfig)\n        )", "response": "Create the HTML for the Sequence Duplication Levels plot"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate the plot of the over - represented sequences in a bar plot", "response": "def overrepresented_sequences (self):\n        \"\"\"Sum the percentages of overrepresented sequences and display them in a bar plot\"\"\"\n\n        data = dict()\n        for s_name in self.fastqc_data:\n            data[s_name] = dict()\n            try:\n                max_pcnt   = max( [ float(d['percentage']) for d in self.fastqc_data[s_name]['overrepresented_sequences']] )\n                total_pcnt = sum( [ float(d['percentage']) for d in self.fastqc_data[s_name]['overrepresented_sequences']] )\n                data[s_name]['total_overrepresented'] = total_pcnt\n                data[s_name]['top_overrepresented'] = max_pcnt\n                data[s_name]['remaining_overrepresented'] = total_pcnt - max_pcnt\n            except KeyError:\n                if self.fastqc_data[s_name]['statuses']['overrepresented_sequences'] == 'pass':\n                    data[s_name]['total_overrepresented'] = 0\n                    data[s_name]['top_overrepresented'] = 0\n                    data[s_name]['remaining_overrepresented'] = 0\n                else:\n                    log.debug(\"Couldn't find data for {}, invalid Key\".format(s_name))\n\n        cats = OrderedDict()\n        cats['top_overrepresented'] = { 'name': 'Top over-represented sequence' }\n        cats['remaining_overrepresented'] = { 'name': 'Sum of remaining over-represented sequences' }\n\n        # Config for the plot\n        pconfig = {\n            'id': 'fastqc_overrepresented_sequencesi_plot',\n            'title': 'FastQC: Overrepresented sequences',\n            'ymin': 0,\n            'yCeiling': 100,\n            'yMinRange': 20,\n            'tt_decimals': 2,\n            'tt_suffix': '%',\n            'tt_percentages': False,\n            'ylab_format': '{value}%',\n            'cpswitch': False,\n            'ylab': 'Percentage of Total Sequences'\n        }\n\n        # Check if any samples have more than 1% overrepresented sequences, else don't make plot.\n        if max([ x['total_overrepresented'] for x in data.values()]) < 1:\n            plot_html = '<div class=\"alert alert-info\">{} samples had less than 1% of reads made up of overrepresented sequences</div>'.format(len(data))\n        else:\n            plot_html = bargraph.plot(data, cats, pconfig)\n\n        self.add_section (\n            name = 'Overrepresented sequences',\n            anchor = 'fastqc_overrepresented_sequences',\n            description = 'The total amount of overrepresented sequences found in each library.',\n            helptext = '''\n            FastQC calculates and lists overrepresented sequences in FastQ files. It would not be\n            possible to show this for all samples in a MultiQC report, so instead this plot shows\n            the _number of sequences_ categorized as over represented.\n\n            Sometimes, a single sequence  may account for a large number of reads in a dataset.\n            To show this, the bars are split into two: the first shows the overrepresented reads\n            that come from the single most common sequence. The second shows the total count\n            from all remaining overrepresented sequences.\n\n            From the [FastQC Help](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/3%20Analysis%20Modules/9%20Overrepresented%20Sequences.html):\n\n            _A normal high-throughput library will contain a diverse set of sequences, with no\n            individual sequence making up a tiny fraction of the whole. Finding that a single\n            sequence is very overrepresented in the set either means that it is highly biologically\n            significant, or indicates that the library is contaminated, or not as diverse as you expected._\n\n            _FastQC lists all of the sequences which make up more than 0.1% of the total.\n            To conserve memory only sequences which appear in the first 100,000 sequences are tracked\n            to the end of the file. It is therefore possible that a sequence which is overrepresented\n            but doesn't appear at the start of the file for some reason could be missed by this module._\n            ''',\n            plot  = plot_html\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate the HTML for the FastQC adapter content plot", "response": "def adapter_content_plot (self):\n        \"\"\" Create the HTML for the FastQC adapter plot \"\"\"\n\n        data = dict()\n        for s_name in self.fastqc_data:\n            try:\n                for d in self.fastqc_data[s_name]['adapter_content']:\n                    pos = self.avg_bp_from_range(d['position'])\n                    for r in self.fastqc_data[s_name]['adapter_content']:\n                        pos = self.avg_bp_from_range(r['position'])\n                        for a in r.keys():\n                            k = \"{} - {}\".format(s_name, a)\n                            if a != 'position':\n                                try:\n                                    data[k][pos] = r[a]\n                                except KeyError:\n                                    data[k] = {pos: r[a]}\n            except KeyError:\n                pass\n        if len(data) == 0:\n            log.debug('adapter_content not found in FastQC reports')\n            return None\n\n        # Lots of these datasets will be all zeros.\n        # Only take datasets with > 0.1% adapter contamination\n        data = {k:d for k,d in data.items() if max(data[k].values()) >= 0.1 }\n\n        pconfig = {\n            'id': 'fastqc_adapter_content_plot',\n            'title': 'FastQC: Adapter Content',\n            'ylab': '% of Sequences',\n            'xlab': 'Position (bp)',\n            'yCeiling': 100,\n            'yMinRange': 5,\n            'ymin': 0,\n            'xDecimals': False,\n            'tt_label': '<b>Base {point.x}</b>: {point.y:.2f}%',\n            'hide_empty': True,\n            'yPlotBands': [\n                {'from': 20, 'to': 100, 'color': '#e6c3c3'},\n                {'from': 5, 'to': 20, 'color': '#e6dcc3'},\n                {'from': 0, 'to': 5, 'color': '#c3e6c3'},\n            ],\n        }\n\n        if len(data) > 0:\n            plot_html = linegraph.plot(data, pconfig)\n        else:\n            plot_html = '<div class=\"alert alert-info\">No samples found with any adapter contamination > 0.1%</div>'\n\n        # Note - colours are messy as we've added adapter names here. Not\n        # possible to break down pass / warn / fail for each adapter, which\n        # could lead to misleading labelling (fails on adapter types with\n        # little or no contamination)\n\n        self.add_section (\n            name = 'Adapter Content',\n            anchor = 'fastqc_adapter_content',\n            description = '''The cumulative percentage count of the proportion of your\n            library which has seen each of the adapter sequences at each position.''',\n            helptext = '''\n            Note that only samples with &ge; 0.1% adapter contamination are shown.\n\n            There may be several lines per sample, as one is shown for each adapter\n            detected in the file.\n\n            From the [FastQC Help](http://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/3%20Analysis%20Modules/10%20Adapter%20Content.html):\n\n            _The plot shows a cumulative percentage count of the proportion\n            of your library which has seen each of the adapter sequences at each position.\n            Once a sequence has been seen in a read it is counted as being present\n            right through to the end of the read so the percentages you see will only\n            increase as the read length goes on._\n            ''',\n            plot = plot_html\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef avg_bp_from_range(self, bp):\n\n        try:\n            if '-' in bp:\n                maxlen = float(bp.split(\"-\",1)[1])\n                minlen = float(bp.split(\"-\",1)[0])\n                bp = ((maxlen - minlen)/2) + minlen\n        except TypeError:\n            pass\n        return(int(bp))", "response": "This function returns the average base pair number from a range of base pair numbers."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of colours according to the FastQC status of this module for each sample.", "response": "def get_status_cols(self, section):\n        \"\"\" Helper function - returns a list of colours according to the FastQC\n        status of this module for each sample. \"\"\"\n        colours = dict()\n        for s_name in self.fastqc_data:\n            status = self.fastqc_data[s_name]['statuses'].get(section, 'default')\n            colours[s_name] = self.status_colours[status]\n        return colours"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind HOMER tagdirectory logs and parse their data", "response": "def homer_tagdirectory(self):\n        \"\"\" Find HOMER tagdirectory logs and parse their data \"\"\"\n        self.parse_gc_content()\n        self.parse_re_dist()\n        self.parse_tagLength_dist()\n        self.parse_tagInfo_data()\n        self.parse_FreqDistribution_data()\n        self.homer_stats_table_tagInfo()\n\n        return sum([len(v) for v in self.tagdir_data.values()])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_gc_content(self):\n        # Find and parse GC content:\n        for f in self.find_log_files('homer/GCcontent', filehandles=True):\n            # Get the s_name from the parent directory\n            s_name = os.path.basename(f['root'])\n            s_name = self.clean_s_name(s_name, f['root'])\n            parsed_data = self.parse_twoCol_file(f)\n            if parsed_data is not None:\n                if s_name in self.tagdir_data['GCcontent']:\n                    log.debug(\"Duplicate GCcontent sample log found! Overwriting: {}\".format(s_name))\n\n                self.add_data_source(f, s_name, section='GCcontent')\n                self.tagdir_data['GCcontent'][s_name] = parsed_data\n\n        ## get esimated genome content distribution:\n        for f in self.find_log_files('homer/genomeGCcontent', filehandles=True):\n            parsed_data = self.parse_twoCol_file(f)\n            if parsed_data is not None:\n                if s_name + \"_genome\" in self.tagdir_data['GCcontent']:\n                    log.debug(\"Duplicate genome GCcontent sample log found! Overwriting: {}\".format(s_name+ \"_genome\"))\n\n                self.add_data_source(f, s_name + \"_genome\", section='GCcontent')\n                self.tagdir_data['GCcontent'][s_name + \"_genome\"] = parsed_data\n\n        self.tagdir_data['GCcontent'] = self.ignore_samples(self.tagdir_data['GCcontent'])\n\n        if len(self.tagdir_data['GCcontent']) > 0:\n            self.add_section (\n                name = 'Per Sequence GC Content',\n                anchor = 'homer_per_sequence_gc_content',\n                description = 'This plot shows the distribution of GC content.',\n                plot = self.GCcontent_plot()\n            )", "response": "parses and plots the GC content and genome GC content files"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_re_dist(self):\n        # Find and parse homer restriction distribution reports\n        for f in self.find_log_files('homer/RestrictionDistribution', filehandles=True):\n            s_name = os.path.basename(f['root'])\n            s_name = self.clean_s_name(s_name, f['root'])\n            parsed_data = self.parse_restriction_dist(f)\n            if parsed_data is not None:\n                if s_name in self.tagdir_data['restriction']:\n                    log.debug(\"Duplicate Restriction Distribution sample log found! Overwriting: {}\".format(s_name))\n\n                self.add_data_source(f, s_name, section='restriction')\n                self.tagdir_data['restriction'][s_name] = parsed_data\n                self.tagdir_data['restriction_norm'][s_name] = self.normalize(parsed_data)\n\n        self.tagdir_data['restriction'] = self.ignore_samples(self.tagdir_data['restriction'])\n        self.tagdir_data['restriction_norm'] = self.ignore_samples(self.tagdir_data['restriction_norm'])\n\n        if len(self.tagdir_data['restriction']) > 0:\n            self.add_section (\n                name = 'Restriction Site Tag Dist',\n                anchor = 'homer-restrictionDist',\n                description = 'This plot shows the distribution of tags around restriction enzyme cut sites.',\n                helptext = '''Hi-C data often involves the digestion of DNA using a restriction enzyme.\n                    A good quality control for the experiment is the centering of reads around the\n                    restriction enzyme cut site.''',\n                plot = self.restriction_dist_chart()\n            )", "response": "parses and plots the Restriction Distribution report"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing and plots the length distribution of the homer tag length.", "response": "def parse_tagLength_dist(self):\n        \"\"\"parses and plots tag length distribution files\"\"\"\n        # Find and parse homer tag length distribution reports\n        for f in self.find_log_files('homer/LengthDistribution', filehandles=True):\n            s_name = os.path.basename(f['root'])\n            s_name = self.clean_s_name(s_name, f['root'])\n            parsed_data = self.parse_length_dist(f)\n            if parsed_data is not None:\n                if s_name in self.tagdir_data['length']:\n                    log.debug(\"Duplicate Length Distribution sample log found! Overwriting: {}\".format(s_name))\n                self.add_data_source(f, s_name, section='length')\n                self.tagdir_data['length'][s_name] = parsed_data\n\n        self.tagdir_data['length'] = self.ignore_samples(self.tagdir_data['length'])\n        if len(self.tagdir_data['length']) > 0:\n            self.add_section (\n                name = 'Tag Length Distribution',\n                anchor = 'homer-tagLength',\n                description = 'This plot shows the distribution of tag length.',\n                helptext = 'This is a good quality control for tag length inputed into Homer.',\n                plot = self.length_dist_chart()\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_tagInfo_data(self):\n        # Find and parse homer taginfo reports\n        for f in self.find_log_files('homer/tagInfo', filehandles=True):\n            s_name = os.path.basename(f['root'])\n            s_name = self.clean_s_name(s_name, f['root'])\n            parsed_data = self.parse_tag_info_chrs(f)\n            if parsed_data is not None:\n                if s_name in self.tagdir_data['taginfo_total']:\n                    log.debug(\"Duplicate tag info sample log found! Overwriting: {}\".format(s_name))\n                self.add_data_source(f, s_name, section='taginfo')\n                self.tagdir_data['taginfo_total'][s_name] = parsed_data[0]\n                self.tagdir_data['taginfo_total_norm'][s_name] = self.normalize(parsed_data[0])\n                self.tagdir_data['taginfo_uniq'][s_name] = parsed_data[1]\n                self.tagdir_data['taginfo_uniq_norm'][s_name] = self.normalize(parsed_data[1])\n\n\n        for f in self.find_log_files('homer/tagInfo', filehandles=True):\n            s_name = os.path.basename(f['root'])\n            s_name = self.clean_s_name(s_name, f['root'])\n            ## collected tag_info data for general stats table and store under 'header'\n            parsed_data = self.parse_tag_info(f)\n            if parsed_data is not None:\n                self.tagdir_data['header'][s_name] = parsed_data\n\n        self.tagdir_data['taginfo_total'] = self.ignore_samples(self.tagdir_data['taginfo_total'])\n        self.tagdir_data['taginfo_total_norm'] = self.ignore_samples(self.tagdir_data['taginfo_total_norm'])\n        self.tagdir_data['taginfo_uniq'] = self.ignore_samples(self.tagdir_data['taginfo_uniq'])\n        self.tagdir_data['taginfo_uniq_norm'] = self.ignore_samples(self.tagdir_data['taginfo_uniq_norm'])\n\n        if len(self.tagdir_data['taginfo_total']) > 0:\n            self.add_section (\n                name = 'Chromosomal Coverage',\n                anchor = 'homer-tagInfo',\n                description = 'This plot shows the distribution of tags along chromosomes.',\n                helptext = '''This is a good quality control for tag distribution and\n                        could be a good indication of large duplications or deletions.''',\n                plot = self.tag_info_chart()\n            )", "response": "parses and plots taginfo data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds core HOMER stats to the general stats table from the tagInfo file", "response": "def homer_stats_table_tagInfo(self):\n        \"\"\" Add core HOMER stats to the general stats table from tagInfo file\"\"\"\n\n        if len(self.tagdir_data['header']) == 0:\n            return None\n\n        headers = OrderedDict()\n        headers['UniqPositions'] = {\n            'title': 'Uniq Pos',\n            'description': 'Numer of Unique Di-Tags Passed Through HOMER',\n            'format': '{:,.0f}',\n            'modify': lambda x: x * 0.000001,\n            'suffix': \"M\"\n\n        }\n        headers['TotalPositions'] = {\n            'title': 'Total Pos',\n            'description': 'Numer of Total Di-Tags Passed Through HOMER',\n            'format': '{:,.0f}',\n            'modify': lambda x: x * 0.000001,\n            'suffix': \"M\"\n        }\n        headers['fragmentLengthEstimate'] = {\n            'title': 'fragment Length',\n            'description': 'Estimate of Fragnment Length',\n            'format': '{:,.0f}'\n        }\n        headers['peakSizeEstimate'] = {\n            'title': 'Peak Size',\n            'description': 'Estimate of Peak Size',\n            'format': '{:,.0f}'\n        }\n        headers['tagsPerBP'] = {\n            'title': 'tagsPerBP',\n            'description': 'average tags Per basepair',\n            'format': '{:,.3f}',\n        }\n        headers['TagsPerPosition'] = {\n            'title': 'averageTagsPerPosition',\n            'description': 'Average Tags Per Position',\n            'format': '{:,.2f}'\n        }\n        headers['averageTagLength'] = {\n            'title': 'TagLength',\n            'description': 'Average Tag Length',\n            'format': '{:,.0f}'\n        }\n        headers['averageFragmentGCcontent'] = {\n            'title': 'GCcontent',\n            'description': 'Average Fragment GC content',\n            'max': 1,\n            'min': 0,\n            'format': '{:,.2f}'\n        }\n        self.general_stats_addcols(self.tagdir_data['header'], headers, 'HOMER')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef homer_stats_table_interChr(self):\n\n        headers = OrderedDict()\n        headers['InterChr'] = {\n            'title': 'InterChr',\n            'description': 'Fraction of Reads forming inter chromosomal interactions',\n            'format': '{:,.4f}'\n        }\n        self.general_stats_addcols(self.tagdir_data['FreqDistribution'], headers, 'Homer-InterChr')", "response": "Add core HOMER stats to the general stats table from FrequencyDistribution file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_restriction_dist(self, f):\n        parsed_data = dict()\n        firstline = True\n        for l in f['f']:\n            if firstline:    #skip first line\n                firstline = False\n                continue\n            s = l.split(\"\\t\")\n            if len(s) > 1:\n                nuc = float(s[0].strip())\n                v1 = float(s[1].strip())\n                v2 = float(s[2].strip())\n                v = v1 + v2\n                #parsed_data.update({nuc:v1})\n                #parsed_data.update({nuc:v2})\n                parsed_data.update({nuc:v})\n        return parsed_data", "response": "Parse HOMER tagdirectory petagRestrictionDistribution file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_length_dist(self, f):\n        parsed_data = dict()\n        firstline = True\n        for l in f['f']:\n            if firstline:    #skip first line\n                firstline = False\n                continue\n            s = l.split(\"\\t\")\n            if len(s) > 1:\n                k = float(s[0].strip())\n                v = float(s[1].strip())\n                parsed_data[k] = v\n\n        return parsed_data", "response": "Parse HOMER tagdirectory tagLengthDistribution file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing HOMER tagdirectory taginfo. txt file to extract statistics in the first 11 lines.", "response": "def parse_tag_info(self, f):\n        \"\"\" Parse HOMER tagdirectory taginfo.txt file to extract statistics in the first 11 lines. \"\"\"\n        # General Stats Table\n        tag_info = dict()\n        for l in f['f']:\n            s = l.split(\"=\")\n            if len(s) > 1:\n                if s[0].strip() == 'genome':\n                    ss = s[1].split(\"\\t\")\n                    if len(ss) > 2:\n                        tag_info['genome'] = ss[0].strip()\n                        try:\n                            tag_info['UniqPositions'] = float(ss[1].strip())\n                            tag_info['TotalPositions'] = float(ss[2].strip())\n                        except:\n                            tag_info['UniqPositions'] = ss[1].strip()\n                            tag_info['TotalPositions'] = ss[2].strip()\n                try:\n                    tag_info[s[0].strip()] = float(s[1].strip())\n                except ValueError:\n                    tag_info[s[0].strip()] = s[1].strip()\n        return tag_info"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_tag_info_chrs(self, f, convChr=True):\n        parsed_data_total = OrderedDict()\n        parsed_data_uniq = OrderedDict()\n        remove = [\"hap\", \"random\", \"chrUn\", \"cmd\", \"EBV\", \"GL\", \"NT_\"]\n        for l in f['f']:\n            s = l.split(\"\\t\")\n            key = s[0].strip()\n            # skip header\n            if '=' in l or len(s) != 3:\n                continue\n            if convChr:\n                if any(x in key for x in remove):\n                    continue\n            try:\n                vT = float(s[1].strip())\n                vU = float(s[2].strip())\n            except ValueError:\n                continue\n\n            parsed_data_total[key] = vT\n            parsed_data_uniq[key] = vU\n\n        return [parsed_data_total, parsed_data_uniq]", "response": "Parse HOMER tagdirectory taginfo. txt file to extract chromosome coverage."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses HOMER tagdirectory petag. FreqDistribution_1000 file.", "response": "def parse_FreqDist(self, f):\n        \"\"\" Parse HOMER tagdirectory petag.FreqDistribution_1000 file. \"\"\"\n        parsed_data = dict()\n        firstline = True\n        for l in f['f']:\n            if firstline:\n                firstline = False\n                continue\n            else:\n                s = l.split(\"\\t\")\n                if len(s) > 1:\n                    k = s[0].strip()\n                    if k.startswith(\"More than \"):\n                        k = re.sub(\"More than \", \"\", k)\n\n                    k = float(k)\n                    v = float(s[1].strip())\n                    parsed_data[k] = v\n        return parsed_data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing HOMER tagdirectory petag. FreqDistribution_1000 file to get inter - chromosomal interactions.", "response": "def parse_FreqDist_interChr(self, f):\n        \"\"\" Parse HOMER tagdirectory petag.FreqDistribution_1000 file to get inter-chromosomal interactions. \"\"\"\n        parsed_data = dict()\n        firstline = True\n        for l in f['f']:\n            if firstline:\n                firstline = False\n                interChr = float(re.sub(\"\\)\", \"\", l.split(\":\")[1]))\n            else:\n                break\n        parsed_data['interChr'] = interChr\n        return parsed_data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking the petagRestrictionDistribution plot", "response": "def restriction_dist_chart (self):\n\n        \"\"\" Make the petagRestrictionDistribution plot \"\"\"\n\n        pconfig = {\n            'id': 'petagRestrictionDistribution',\n            'title': 'Restriction Distribution',\n            'ylab': 'Reads',\n            'xlab': 'Distance from cut site (bp)',\n            'data_labels': [\n                        {'name': 'Number of Tags'},\n                        {'name': 'Percenatge'}\n                            ]\n       }\n        datasets = [\n            self.tagdir_data['restriction'],\n            self.tagdir_data['restriction_norm']\n        ]\n\n        return linegraph.plot(datasets, pconfig)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate the HTML for the Homer GC content plot", "response": "def GCcontent_plot (self):\n        \"\"\" Create the HTML for the Homer GC content plot \"\"\"\n\n        pconfig = {\n            'id': 'homer-tag-directory-gc-content',\n            'title': 'Homer: Tag Directory Per Sequence GC Content',\n            'smooth_points': 200,\n            'smooth_points_sumcounts': False,\n            'ylab': 'Normalized Count',\n            'xlab': '% GC',\n            'ymin': 0,\n            'xmax': 1,\n            'xmin': 0,\n            'yDecimals': True,\n            'tt_label': '<b>{point.x}% GC</b>: {point.y}'\n        }\n        return linegraph.plot(self.tagdir_data['GCcontent'], pconfig)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tag_info_chart (self):\n\n        \"\"\" Make the taginfo.txt plot \"\"\"\n\n        ## TODO: human chrs on hg19. How will this work with GRCh genome or other, non human, genomes?\n        # nice if they are ordered by size\n        ucsc = [\"chr\" + str(i) for i in range(1,23)].append([ \"chrX\", \"chrY\", \"chrM\"])\n        ensembl = list(range(1,23)).append([ \"X\", \"Y\", \"MT\"])\n        pconfig = {\n            'id': 'tagInfo',\n            'title': 'Homer: Tag Info Distribution',\n            'ylab': 'Tags',\n            'cpswitch_counts_label': 'Number of Tags'\n        }\n\n        ## check if chromosomes starts with \"chr\" (UCSC) or \"#\" (ensembl)\n        sample1 = next(iter(self.tagdir_data['taginfo_total']))\n        chrFormat = next(iter(self.tagdir_data['taginfo_total'][sample1]))\n\n        if (\"chr\" in chrFormat):\n            chrs = ucsc\n        else:\n            chrs = ensembl\n\n        return bargraph.plot(self.tagdir_data['taginfo_total'], chrs, pconfig)", "response": "Make the taginfo. txt plot"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake the frequency distribution plot", "response": "def FreqDist_chart (self):\n        \"\"\" Make the petag.FreqDistribution_1000 plot \"\"\"\n        # Take a log of the data before plotting so that we can\n        # reduce the number of points to plot evenly\n        pdata = {}\n        for idx, s_name in enumerate(self.tagdir_data['FreqDistribution']):\n            pdata[s_name] = {}\n            for x, y in self.tagdir_data['FreqDistribution'][s_name].items():\n                try:\n                    pdata[s_name][math.log(float(x))] = y\n                except ValueError:\n                    pass\n        pconfig = {\n            'id': 'FreqDistribution',\n            'title': 'Frequency Distribution',\n            'ylab': 'Fraction of Reads',\n            'xlab': 'Log10(Distance between regions)',\n            'data_labels': ['Reads', 'Percent'],\n            'smooth_points': 500,\n            'smooth_points_sumcounts': False,\n            'yLog' : True\n        }\n        return linegraph.plot(pdata, pconfig)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_bismark_report(self, report, regexes):\n        parsed_data = {}\n        for k, r in regexes.items():\n            r_search = re.search(r, report, re.MULTILINE)\n            if r_search:\n                try:\n                    parsed_data[k] = float(r_search.group(1))\n                except ValueError:\n                    parsed_data[k] = r_search.group(1) # NaN\n        if len(parsed_data) == 0: return None\n        return parsed_data", "response": "Search a bismark report with a set of regexes and return a dictionary of the keys and values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_bismark_mbias(self, f):\n        s = f['s_name']\n        self.bismark_mbias_data['meth']['CpG_R1'][s] = {}\n        self.bismark_mbias_data['meth']['CHG_R1'][s] = {}\n        self.bismark_mbias_data['meth']['CHH_R1'][s] = {}\n        self.bismark_mbias_data['cov']['CpG_R1'][s] = {}\n        self.bismark_mbias_data['cov']['CHG_R1'][s] = {}\n        self.bismark_mbias_data['cov']['CHH_R1'][s] = {}\n        self.bismark_mbias_data['meth']['CpG_R2'][s] = {}\n        self.bismark_mbias_data['meth']['CHG_R2'][s] = {}\n        self.bismark_mbias_data['meth']['CHH_R2'][s] = {}\n        self.bismark_mbias_data['cov']['CpG_R2'][s] = {}\n        self.bismark_mbias_data['cov']['CHG_R2'][s] = {}\n        self.bismark_mbias_data['cov']['CHH_R2'][s] = {}\n        key = None\n        for l in f['f']:\n            if 'context' in l:\n                if 'CpG' in l:\n                    key = 'CpG'\n                elif 'CHG' in l:\n                    key = 'CHG'\n                elif 'CHH' in l:\n                    key = 'CHH'\n                if '(R1)' in l:\n                    key += '_R1'\n                elif '(R2)' in l:\n                    key += '_R2'\n                else:\n                    key += '_R1'\n            if key is not None:\n                sections = l.split()\n                try:\n                    pos = int(sections[0])\n                    self.bismark_mbias_data['meth'][key][s][pos] = float(sections[3])\n                    self.bismark_mbias_data['cov'][key][s][pos] = int(sections[4])\n                except (IndexError, ValueError):\n                    continue\n\n        # Remove empty dicts (eg. R2 for SE data)\n        for t in self.bismark_mbias_data:\n            for k in self.bismark_mbias_data[t]:\n                self.bismark_mbias_data[t][k] = {\n                    s_name: self.bismark_mbias_data[t][k][s_name]\n                    for s_name in self.bismark_mbias_data[t][k]\n                    if len(self.bismark_mbias_data[t][k][s_name]) > 0\n                }", "response": "Parse the Bismark M - Bias plot data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_bismark_bam2nuc(self, f):\n        if f['s_name'] in self.bismark_data['bam2nuc']:\n            log.debug(\"Duplicate deduplication sample log found! Overwriting: {}\".format(f['s_name']))\n        self.add_data_source(f, section='bam2nuc')\n        self.bismark_data['bam2nuc'][f['s_name']] = dict()\n\n        headers = None\n        for l in f['f']:\n            sections = l.rstrip().split(\"\\t\")\n            if headers is None:\n                headers = sections\n            else:\n                for i, h in enumerate(headers):\n                    if i == 0:\n                        k = sections[0]\n                    else:\n                        key = \"{}_{}\".format(k, h.lower().replace(' ','_'))\n                        self.bismark_data['bam2nuc'][f['s_name']][key] = sections[i]", "response": "Parse the Bismark bam2nuc report"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bismark_stats_table(self):\n\n        headers = {\n            'alignment': OrderedDict(),\n            'dedup': OrderedDict(),\n            'methextract': OrderedDict(),\n            'bam2nuc': OrderedDict()\n        }\n        headers['methextract']['percent_cpg_meth'] = {\n            'title': '% mCpG',\n            'description': '% Cytosines methylated in CpG context',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'Greens'\n        }\n        headers['methextract']['percent_chg_meth'] = {\n            'title': '% mCHG',\n            'description': '% Cytosines methylated in CHG context',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'Oranges'\n        }\n        headers['methextract']['percent_chh_meth'] = {\n            'title': '% mCHH',\n            'description': '% Cytosines methylated in CHH context',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'Oranges'\n        }\n        headers['methextract']['total_c'] = {\n            'title': \"M C's\",\n            'description': 'Total number of C\\'s analysed, in millions',\n            'min': 0,\n            'scale': 'Purples',\n            'modify': lambda x: x / 1000000\n        }\n        headers['bam2nuc']['C_coverage'] = {\n            'title': 'C Coverage',\n            'description': 'Cyotosine Coverage',\n            'min': 0,\n            'suffix': 'X',\n            'scale': 'Greens',\n            'format': '{:,.2f}'\n        }\n        headers['dedup']['dup_reads_percent'] = {\n            'title': '% Dups',\n            'description': 'Percent Duplicated Alignments',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'RdYlGn-rev',\n        }\n        headers['dedup']['dedup_reads'] = {\n            'title': '{} Unique'.format(config.read_count_prefix),\n            'description': 'Deduplicated Alignments ({})'.format(config.read_count_desc),\n            'min': 0,\n            'scale': 'Greens',\n            'modify': lambda x: x * config.read_count_multiplier,\n            'shared_key': 'read_count',\n            'hidden': True\n        }\n        headers['alignment']['aligned_reads'] = {\n            'title': '{} Aligned'.format(config.read_count_prefix),\n            'description': 'Total Aligned Sequences ({})'.format(config.read_count_desc),\n            'min': 0,\n            'scale': 'PuRd',\n            'modify': lambda x: x * config.read_count_multiplier,\n            'shared_key': 'read_count',\n            'hidden': True\n        }\n        headers['alignment']['percent_aligned'] = {\n            'title': '% Aligned',\n            'description': 'Percent Aligned Sequences',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'YlGn'\n        }\n\n        self.general_stats_addcols(self.bismark_data['methextract'], headers['methextract'])\n        self.general_stats_addcols(self.bismark_data['bam2nuc'], headers['bam2nuc'])\n        self.general_stats_addcols(self.bismark_data['dedup'], headers['dedup'])\n        self.general_stats_addcols(self.bismark_data['alignment'], headers['alignment'])", "response": "Take the parsed stats from the Bismark reports and add them to the basic stats table at the top of the report"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bismark_alignment_chart (self):\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['aligned_reads']   = { 'color': '#2f7ed8', 'name': 'Aligned Uniquely' }\n        keys['ambig_reads']     = { 'color': '#492970', 'name': 'Aligned Ambiguously' }\n        keys['no_alignments']   = { 'color': '#0d233a', 'name': 'Did Not Align' }\n        keys['discarded_reads'] = { 'color': '#f28f43', 'name': 'No Genomic Sequence' }\n\n        # Config for the plot\n        config = {\n            'id': 'bismark_alignment',\n            'title': 'Bismark: Alignment Scores',\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads'\n        }\n\n        self.add_section (\n            name = 'Alignment Rates',\n            anchor = 'bismark-alignment',\n            plot = bargraph.plot(self.bismark_data['alignment'], keys, config)\n        )", "response": "Make the alignment plot"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bismark_strand_chart (self):\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['strand_ob']   = { 'name': 'Original bottom strand' }\n        keys['strand_ctob'] = { 'name': 'Complementary to original bottom strand' }\n        keys['strand_ctot'] = { 'name': 'Complementary to original top strand' }\n        keys['strand_ot']   = { 'name': 'Original top strand' }\n\n        # See if we have any directional samples\n        directional = 0\n        d_mode = ''\n        for sn in self.bismark_data['alignment'].values():\n            if 'strand_directional' in sn.keys():\n                directional += 1\n        if directional == len(self.bismark_data['alignment']):\n            keys.pop('strand_ctob', None)\n            keys.pop('strand_ctot', None)\n            d_mode = 'All samples were run with <code>--directional</code> mode; alignments to complementary strands (CTOT, CTOB) were ignored.'\n        elif directional > 0:\n            d_mode = '{} samples were run with <code>--directional</code> mode; alignments to complementary strands (CTOT, CTOB) were ignored.'.format(directional)\n\n        # Config for the plot\n        config = {\n            'id': 'bismark_strand_alignment',\n            'title': 'Bismark: Alignment to Individual Bisulfite Strands',\n            'ylab': '% Reads',\n            'cpswitch_c_active': False,\n            'cpswitch_counts_label': 'Number of Reads'\n        }\n\n        self.add_section (\n            name = 'Strand Alignment',\n            anchor = 'bismark-strands',\n            description = d_mode,\n            plot = bargraph.plot(self.bismark_data['alignment'], keys, config)\n        )", "response": "Make the Bismark Strand alignment plot"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes the deduplication plot", "response": "def bismark_dedup_chart (self):\n        \"\"\" Make the deduplication plot \"\"\"\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['dedup_reads'] = { 'name': 'Deduplicated reads (remaining)' }\n        keys['dup_reads']   = { 'name': 'Duplicate reads (removed)' }\n\n        # Config for the plot\n        config = {\n            'id': 'bismark_deduplication',\n            'title': 'Bismark: Deduplication',\n            'ylab': '% Reads',\n            'cpswitch_c_active': False,\n            'cpswitch_counts_label': 'Number of Reads'\n        }\n\n        self.add_section (\n            name = 'Deduplication',\n            anchor = 'bismark-deduplication',\n            plot = bargraph.plot(self.bismark_data['dedup'], keys, config)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bismark_methlyation_chart (self):\n\n        # Config for the plot\n        keys = OrderedDict()\n        defaults = {\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'decimalPlaces': 1\n        }\n        keys['percent_cpg_meth'] = dict(defaults, **{ 'title': 'Methylated CpG' })\n        keys['percent_chg_meth'] = dict(defaults, **{ 'title': 'Methylated CHG' })\n        keys['percent_chh_meth'] = dict(defaults, **{ 'title': 'Methylated CHH' })\n\n        self.add_section (\n            name = 'Cytosine Methylation',\n            anchor = 'bismark-methylation',\n            plot = beeswarm.plot(self.bismark_data['methextract'], keys, {'id': 'bismark-methylation-dp'})\n        )", "response": "Make the methylation plot"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bismark_mbias_plot (self):\n\n        description = '<p>This plot shows the average percentage methylation and coverage across reads. See the \\n\\\n        <a href=\"https://rawgit.com/FelixKrueger/Bismark/master/Docs/Bismark_User_Guide.html#m-bias-plot\" target=\"_blank\">bismark user guide</a> \\n\\\n        for more information on how these numbers are generated.</p>'\n\n        pconfig = {\n            'id': 'bismark_mbias',\n            'title': 'Bismark: M-Bias',\n            'ylab': '% Methylation',\n            'xlab': 'Position (bp)',\n            'xDecimals': False,\n            'ymax': 100,\n            'ymin': 0,\n            'tt_label': '<b>{point.x} bp</b>: {point.y:.1f}%',\n            'data_labels': [\n                {'name': 'CpG R1', 'ylab': '% Methylation', 'ymax': 100},\n                {'name': 'CHG R1', 'ylab': '% Methylation', 'ymax': 100},\n                {'name': 'CHH R1', 'ylab': '% Methylation', 'ymax': 100}\n            ]\n        }\n        datasets = [\n            self.bismark_mbias_data['meth']['CpG_R1'],\n            self.bismark_mbias_data['meth']['CHG_R1'],\n            self.bismark_mbias_data['meth']['CHH_R1']\n        ]\n\n        if len(self.bismark_mbias_data['meth']['CpG_R2']) > 0:\n            pconfig['data_labels'].append({'name': 'CpG R2', 'ylab': '% Methylation', 'ymax': 100})\n            pconfig['data_labels'].append({'name': 'CHG R2', 'ylab': '% Methylation', 'ymax': 100})\n            pconfig['data_labels'].append({'name': 'CHH R2', 'ylab': '% Methylation', 'ymax': 100})\n            datasets.append(self.bismark_mbias_data['meth']['CpG_R2'])\n            datasets.append(self.bismark_mbias_data['meth']['CHG_R2'])\n            datasets.append(self.bismark_mbias_data['meth']['CHH_R2'])\n\n        self.add_section (\n            name = 'M-Bias',\n            anchor = 'bismark-mbias',\n            description = description,\n            plot = linegraph.plot(datasets, pconfig)\n        )", "response": "Make the M - Bias plot"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_reports(self):\n\n    # Set up vars\n    self.picard_GCbias_data = dict()\n    self.picard_GCbiasSummary_data = dict()\n\n    # Go through logs and find Metrics\n    for f in self.find_log_files('picard/gcbias', filehandles=True):\n        s_name = None\n        gc_col = None\n        cov_col = None\n        for l in f['f']:\n            # New log starting\n            if 'GcBiasMetrics' in l and 'INPUT' in l:\n                s_name = None\n\n                # Pull sample name from input\n                fn_search = re.search(r\"INPUT(?:=|\\s+)(\\[?[^\\s]+\\]?)\", l, flags=re.IGNORECASE)\n                if fn_search:\n                    s_name = os.path.basename(fn_search.group(1).strip('[]'))\n                    s_name = self.clean_s_name(s_name, f['root'])\n\n            if s_name is not None:\n                if gc_col is not None and cov_col is not None :\n                    try:\n                        # Note that GC isn't always the first column.\n                        s = l.strip(\"\\n\").split(\"\\t\")\n                        self.picard_GCbias_data[s_name][ int(s[gc_col]) ] = float(s[cov_col])\n                    except IndexError:\n                        s_name = None\n                        gc_col = None\n                        cov_col = None\n\n                if 'GcBiasDetailMetrics' in l and '## METRICS CLASS' in l:\n                    if s_name in self.picard_GCbias_data:\n                        log.debug(\"Duplicate sample name found in {}! Overwriting: {}\".format(f['fn'], s_name))\n                    self.add_data_source(f, s_name, section='GcBiasDetailMetrics')\n                    self.picard_GCbias_data[s_name] = dict()\n                    # Get header - find columns with the data we want\n                    l = f['f'].readline()\n                    s = l.strip(\"\\n\").split(\"\\t\")\n                    gc_col = s.index('GC')\n                    cov_col = s.index('NORMALIZED_COVERAGE')\n\n                if 'GcBiasSummaryMetrics' in l and '## METRICS CLASS' in l:\n                    if s_name in self.picard_GCbias_data:\n                        log.debug(\"Duplicate sample name found in {}! Overwriting: {}\".format(f['fn'], s_name))\n                    self.add_data_source(f, s_name, section='GcBiasSummaryMetrics')\n                    self.picard_GCbiasSummary_data[s_name] = dict()\n\n                    keys = f['f'].readline().rstrip(\"\\n\").split(\"\\t\")\n                    vals = f['f'].readline().rstrip(\"\\n\").split(\"\\t\")\n                    for i, k in enumerate(keys):\n                        try:\n                            self.picard_GCbiasSummary_data[s_name][k] = float(vals[i])\n                        except ValueError:\n                            self.picard_GCbiasSummary_data[s_name][k] = vals[i]\n\n\n        for s_name in list(self.picard_GCbias_data.keys()):\n            if len(self.picard_GCbias_data[s_name]) == 0:\n                self.picard_GCbias_data.pop(s_name, None)\n                log.debug(\"Removing {} as no data parsed\".format(s_name))\n\n        for s_name in list(self.picard_GCbiasSummary_data.keys()):\n            if len(self.picard_GCbiasSummary_data[s_name]) == 0:\n                self.picard_GCbiasSummary_data.pop(s_name, None)\n                log.debug(\"Removing {} as no data parsed\".format(s_name))\n\n\n    # Filter to strip out ignored sample names\n    self.picard_GCbias_data = self.ignore_samples(self.picard_GCbias_data)\n\n    if len(self.picard_GCbias_data) > 0:\n\n        # Plot the graph\n\n        pconfig = {\n            'id': 'picard_gcbias_plot',\n            'title': 'Picard: GC Coverage Bias',\n            'ylab': 'Normalized Coverage',\n            'xlab': '% GC',\n            'xmin': 0,\n            'xmax': 100,\n            'xDecimals': False,\n            'ymin': 0,\n            'yCeiling': 10,\n            'tt_label': '<b>{point.x} %GC</b>: {point.y:.2f}',\n            'yPlotLines': [\n                {'value': 1, 'color': '#999999', 'width': 2, 'dashStyle': 'LongDash'},\n            ]\n        }\n        self.add_section (\n            name = 'GC Coverage Bias',\n            anchor = 'picard-gcbias',\n            description = 'This plot shows bias in coverage across regions of the genome with varying GC content.'\\\n                ' A perfect library would be a flat line at <code>y = 1</code>.',\n            plot = linegraph.plot(self.picard_GCbias_data, pconfig)\n        )\n\n    if len(self.picard_GCbiasSummary_data) > 0:\n        # Write parsed summary data to a file\n        self.write_data_file(self.picard_GCbiasSummary_data, 'multiqc_picard_gcbias')\n\n\n    # Return the number of detected samples to the parent module\n    return len(self.picard_GCbias_data)", "response": "Find Picard InsertSizeMetrics reports and parse their data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the JSON output from AfterQC and save the summary statistics", "response": "def parse_afterqc_log(self, f):\n        \"\"\" Parse the JSON output from AfterQC and save the summary statistics \"\"\"\n        try:\n            parsed_json = json.load(f['f'])\n        except:\n            log.warn(\"Could not parse AfterQC JSON: '{}'\".format(f['fn']))\n            return None\n\n        # AfterQC changed the name of their summary key at some point\n        if 'summary' in parsed_json:\n            summaryk = 'summary'\n        elif 'afterqc_main_summary' in parsed_json:\n            summaryk = 'afterqc_main_summary'\n        else:\n            log.warn(\"AfterQC JSON did not have a 'summary' or 'afterqc_main_summary' key, skipping: '{}'\".format(f['fn']))\n            return None\n\n        s_name = f['s_name']\n        self.add_data_source(f, s_name)\n        self.afterqc_data[s_name] = {}\n        for k in parsed_json[summaryk]:\n            try:\n                self.afterqc_data[s_name][k] = float(parsed_json[summaryk][k])\n            except ValueError:\n                self.afterqc_data[s_name][k] = parsed_json[summaryk][k]\n        try:\n            self.afterqc_data[s_name]['pct_good_bases'] = (self.afterqc_data[s_name]['good_bases'] / self.afterqc_data[s_name]['total_bases']) * 100.0\n        except KeyError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntaking the parsed stats from the Afterqc report and add it to the General Statistics table at the top of the report", "response": "def afterqc_general_stats_table(self):\n        \"\"\" Take the parsed stats from the Afterqc report and add it to the\n        General Statistics table at the top of the report \"\"\"\n\n        headers = OrderedDict()\n        headers['pct_good_bases'] = {\n            'title': '% Good Bases',\n            'description': 'Percent Good Bases',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'BuGn',\n        }\n        headers['good_reads'] = {\n            'title': '{} Good Reads'.format(config.read_count_prefix),\n            'description': 'Good Reads ({})'.format(config.read_count_desc),\n            'min': 0,\n            'modify': lambda x: x * config.read_count_multiplier,\n            'scale': 'GnBu',\n            'shared_key': 'read_count'\n        }\n        headers['total_reads'] = {\n            'title': '{} Total Reads'.format(config.read_count_prefix),\n            'description': 'Total Reads ({})'.format(config.read_count_desc),\n            'min': 0,\n            'modify': lambda x: x * config.read_count_multiplier,\n            'scale': 'Blues',\n            'shared_key': 'read_count'\n        }\n        headers['readlen'] = {\n            'title': 'Read Length',\n            'description': 'Read Length',\n            'min': 0,\n            'suffix': ' bp',\n            'format': '{:,.0f}',\n            'scale': 'YlGn'\n        }\n        self.general_stats_addcols(self.afterqc_data, headers)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef after_qc_bad_reads_chart(self):\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['good_reads'] =                     { 'name': 'Good Reads' }\n        keys['bad_reads_with_bad_barcode'] =     { 'name': 'Bad Barcode' }\n        keys['bad_reads_with_bad_overlap'] =     { 'name': 'Bad Overlap' }\n        keys['bad_reads_with_bad_read_length'] = { 'name': 'Bad Read Length' }\n        keys['bad_reads_with_low_quality'] =     { 'name': 'Low Quality' }\n        keys['bad_reads_with_polyX'] =           { 'name': 'PolyX' }\n        keys['bad_reads_with_reads_in_bubble'] = { 'name': 'Reads In Bubble' }\n        keys['bad_reads_with_too_many_N'] =      { 'name': 'Too many N' }\n\n        # Config for the plot\n        pconfig = {\n            'id': 'afterqc_bad_reads_plot',\n            'title': 'AfterQC: Filtered Reads',\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads',\n            'hide_zero_cats': False,\n        }\n        return bargraph.plot(self.afterqc_data, keys, pconfig)", "response": "Function to generate the AfterQC bad reads bar plot"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot(self, file_type):\n\n        samples = self.mod_data[file_type]\n        plot_title = file_types[file_type]['title']\n        plot_func = file_types[file_type]['plot_func']\n        plot_params = file_types[file_type]['plot_params']\n        return plot_func(samples,\n                    file_type,\n                    plot_title=plot_title,\n                    plot_params=plot_params)", "response": "Call file_type plotting function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating basic table of key - value items in file_type.", "response": "def make_basic_table(self, file_type):\n        \"\"\"  Create table of key-value items in 'file_type'.\n        \"\"\"\n\n        table_data = {sample: items['kv']\n                for sample, items\n                in self.mod_data[file_type].items()\n        }\n        table_headers = {}\n        for column_header, (description, header_options) in file_types[file_type]['kv_descriptions'].items():\n            table_headers[column_header] = {\n                    'rid': '{}_{}_bbmstheader'.format(file_type, column_header),\n                    'title': column_header,\n                    'description': description,\n            }\n            table_headers[column_header].update(header_options)\n\n        tconfig = {\n            'id': file_type + '_bbm_table',\n            'namespace': 'BBTools'\n        }\n        for sample in table_data:\n            for key, value in table_data[sample].items():\n                try:\n                    table_data[sample][key] = float(value)\n                except ValueError:\n                    pass\n        return table.plot(table_data, table_headers, tconfig)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating the HTML for the TsTv by count linegraph plot.", "response": "def parse_tstv_by_count(self):\n        \"\"\" Create the HTML for the TsTv by alternative allele count linegraph plot. \"\"\"\n\n        self.vcftools_tstv_by_count = dict()\n        for f in self.find_log_files('vcftools/tstv_by_count', filehandles=True):\n            d = {}\n            for line in f['f'].readlines()[1:]: # don't add the header line (first row)\n                key = float(line.split()[0]) # taking the first column (alternative allele count) as key\n                val = float(line.split()[3]) # taking Ts/Tv as value\n                d[key] = val\n            self.vcftools_tstv_by_count[f['s_name']] = d\n\n        # Filter out ignored sample names\n        self.vcftools_tstv_by_count = self.ignore_samples(self.vcftools_tstv_by_count)\n\n        if len(self.vcftools_tstv_by_count) == 0:\n            return 0\n\n        pconfig = {\n            'id': 'vcftools_tstv_by_count',\n            'title': 'VCFTools: TsTv by Count',\n            'ylab': 'TsTv Ratio',\n            'xlab': 'Alternative Allele Count',\n            'xmin': 0,\n            'ymin': 0,\n            'smooth_points': 400, # this limits huge filesizes and prevents browser crashing\n            'smooth_points_sumcounts': False\n        }\n\n        helptext = '''\n        `Transition` is a purine-to-purine or pyrimidine-to-pyrimidine point mutations.\n        `Transversion` is a purine-to-pyrimidine or pyrimidine-to-purine point mutation.\n        `Alternative allele count` is the number of alternative alleles at the site.\n        Note: only bi-allelic SNPs are used (multi-allelic sites and INDELs are skipped.)\n        Refer to Vcftools's manual (https://vcftools.github.io/man_latest.html) on `--TsTv-by-count`\n        '''\n\n        self.add_section(\n            name = 'TsTv by Count',\n            anchor = 'vcftools-tstv-by-count',\n            description = \"Plot of `TSTV-BY-COUNT` - the transition to transversion ratio as a function of alternative allele count from the output of vcftools TsTv-by-count.\",\n            helptext = helptext,\n            plot = linegraph.plot(self.vcftools_tstv_by_count,pconfig)\n        )\n\n        return len(self.vcftools_tstv_by_count)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef alignment_chart(data):\n    keys = OrderedDict()\n    keys['reads_mapped'] = {'color': '#437bb1', 'name': 'Mapped'}\n    keys['reads_unmapped'] = {'color': '#b1084c', 'name': 'Unmapped'}\n\n    # Config for the plot\n    plot_conf = {\n        'id': 'samtools_alignment_plot',\n        'title': 'Samtools stats: Alignment Scores',\n        'ylab': '# Reads',\n        'cpswitch_counts_label': 'Number of Reads'\n    }\n    return bargraph.plot(data, keys, plot_conf)", "response": "Make the HighCharts HTML to plot the alignment rates"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_samtools_stats(self):\n\n        self.samtools_stats = dict()\n        for f in self.find_log_files('samtools/stats'):\n            parsed_data = dict()\n            for line in f['f'].splitlines():\n                if not line.startswith(\"SN\"):\n                    continue\n                sections = line.split(\"\\t\")\n                field = sections[1].strip()[:-1]\n                field = field.replace(' ', '_')\n                value = float(sections[2].strip())\n                parsed_data[field] = value\n\n            if len(parsed_data) > 0:\n                # Work out some percentages\n                if 'raw_total_sequences' in parsed_data:\n                    for k in list(parsed_data.keys()):\n                        if k.startswith('reads_') and k != 'raw_total_sequences' and parsed_data['raw_total_sequences'] > 0:\n                            parsed_data['{}_percent'.format(k)] = (parsed_data[k] / parsed_data['raw_total_sequences']) * 100\n\n                if f['s_name'] in self.samtools_stats:\n                    log.debug(\"Duplicate sample name found! Overwriting: {}\"\n                              .format(f['s_name']))\n                self.add_data_source(f, section='stats')\n                self.samtools_stats[f['s_name']] = parsed_data\n\n        # Filter to strip out ignored sample names\n        self.samtools_stats = self.ignore_samples(self.samtools_stats)\n\n        if len(self.samtools_stats) > 0:\n\n            # Write parsed report data to a file\n            self.write_data_file(self.samtools_stats, 'multiqc_samtools_stats')\n\n            # General Stats Table\n            stats_headers = OrderedDict()\n            stats_headers['error_rate'] = {\n                'title': 'Error rate',\n                'description': 'Error rate: mismatches (NM) / bases mapped (CIGAR)',\n                'min': 0,\n                'max': 100,\n                'suffix': '%',\n                'scale': 'OrRd',\n                'format': '{:,.2f}',\n                'modify': lambda x: x * 100.0\n            }\n            stats_headers['non-primary_alignments'] = {\n                'title': '{} Non-Primary'.format(config.read_count_prefix),\n                'description': 'Non-primary alignments ({})'.format(config.read_count_desc),\n                'min': 0,\n                'scale': 'PuBu',\n                'modify': lambda x: x * config.read_count_multiplier,\n                'shared_key': 'read_count'\n            }\n            stats_headers['reads_mapped'] = {\n                'title': '{} Reads Mapped'.format(config.read_count_prefix),\n                'description': 'Reads Mapped in the bam file ({})'.format(config.read_count_desc),\n                'min': 0,\n                'modify': lambda x: x * config.read_count_multiplier,\n                'shared_key': 'read_count'\n            }\n            stats_headers['reads_mapped_percent'] = {\n                'title': '% Mapped',\n                'description': '% Mapped Reads',\n                'max': 100,\n                'min': 0,\n                'suffix': '%',\n                'scale': 'RdYlGn'\n            }\n            stats_headers['reads_properly_paired_percent'] = {\n                'title': '% Proper Pairs',\n                'description': '% Properly Paired Reads',\n                'max': 100,\n                'min': 0,\n                'suffix': '%',\n                'scale': 'RdYlGn',\n                'hidden': True if (max([x['reads_mapped_and_paired'] for x in self.samtools_stats.values()]) == 0) else False\n            }\n            stats_headers['reads_MQ0_percent'] = {\n                'title': '% MapQ 0 Reads',\n                'description': '% of Reads that are Ambiguously Placed (MapQ=0)',\n                'max': 100,\n                'min': 0,\n                'suffix': '%',\n                'scale': 'OrRd',\n                'hidden': True\n            }\n            stats_headers['raw_total_sequences'] = {\n                'title': '{} Total seqs'.format(config.read_count_prefix),\n                'description': 'Total sequences in the bam file ({})'.format(config.read_count_desc),\n                'min': 0,\n                'modify': lambda x: x * config.read_count_multiplier,\n                'shared_key': 'read_count'\n            }\n            self.general_stats_addcols(self.samtools_stats, stats_headers, 'Samtools Stats')\n\n            # Make bargraph plot of mapped/unmapped reads\n            self.alignment_section(self.samtools_stats)\n\n            # Make dot plot of counts\n            keys = OrderedDict()\n            reads = {\n                'min': 0,\n                'modify': lambda x: float(x) / 1000000.0,\n                'suffix': 'M reads',\n                'decimalPlaces': 2,\n                'shared_key': 'read_count'\n            }\n            bases = {\n                'min': 0,\n                'modify': lambda x: float(x) / 1000000.0,\n                'suffix': 'M bases',\n                'decimalPlaces': 2,\n                'shared_key': 'base_count'\n            }\n            keys['raw_total_sequences'] = dict(reads, **{'title': 'Total sequences'})\n            keys['reads_mapped_and_paired'] = dict(reads, **{'title': 'Mapped &amp; paired', 'description': 'Paired-end technology bit set + both mates mapped' })\n            keys['reads_properly_paired'] = dict(reads, **{'title': 'Properly paired', 'description': 'Proper-pair bit set'})\n            keys['reads_duplicated'] = dict(reads, **{'title': 'Duplicated', 'description': 'PCR or optical duplicate bit set'})\n            keys['reads_QC_failed'] = dict(reads, **{'title': 'QC Failed'})\n            keys['reads_MQ0'] = dict(reads, **{'title': 'Reads MQ0', 'description': 'Reads mapped and MQ=0'})\n            keys['bases_mapped_(cigar)'] = dict(bases, **{'title': 'Mapped bases (CIGAR)', 'description': 'Mapped bases (CIGAR)'})\n            keys['bases_trimmed'] = dict(bases, **{'title': 'Bases Trimmed'})\n            keys['bases_duplicated'] = dict(bases, **{'title': 'Duplicated bases'})\n            keys['pairs_on_different_chromosomes'] = dict(reads, **{'title': 'Diff chromosomes', 'description': 'Pairs on different chromosomes'})\n            keys['pairs_with_other_orientation'] = dict(reads, **{'title': 'Other orientation', 'description': 'Pairs with other orientation'})\n            keys['inward_oriented_pairs'] = dict(reads, **{'title': 'Inward pairs', 'description': 'Inward oriented pairs'})\n            keys['outward_oriented_pairs'] = dict(reads, **{'title': 'Outward pairs', 'description': 'Outward oriented pairs'})\n\n            self.add_section (\n                name = 'Alignment metrics',\n                anchor = 'samtools-stats',\n                description = \"This module parses the output from <code>samtools stats</code>. All numbers in millions.\",\n                plot = beeswarm.plot(self.samtools_stats, keys, {'id': 'samtools-stats-dp'})\n            )\n\n        # Return the number of logs that were found\n        return len(self.samtools_stats)", "response": "Find Samtools stats logs and parse their data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind plotFingerprint output. Both plotFingerprintOutQualityMetrics and plotFingerprintOutRawCounts are parsed and added to self.deeptools_plotFingerprintOutQualityMetrics and self.deeptools_plotFingerprintOutRawCounts.", "response": "def parse_plotFingerprint(self):\n        \"\"\"Find plotFingerprint output. Both --outQualityMetrics and --outRawCounts\"\"\"\n        self.deeptools_plotFingerprintOutQualityMetrics = dict()\n        for f in self.find_log_files('deeptools/plotFingerprintOutQualityMetrics'):\n            parsed_data = self.parsePlotFingerprintOutQualityMetrics(f)\n            for k, v in parsed_data.items():\n                if k in self.deeptools_plotFingerprintOutQualityMetrics:\n                    log.warning(\"Replacing duplicate sample {}.\".format(k))\n                # Values are fractions - convert to percentages for consistency with other MultiQC output\n                self.deeptools_plotFingerprintOutQualityMetrics[k] = { i:float(j)*100.0 for i,j in v.items() }\n\n            if len(parsed_data) > 0:\n                self.add_data_source(f, section='plotFingerprint')\n\n        self.deeptools_plotFingerprintOutRawCounts= dict()\n        for f in self.find_log_files('deeptools/plotFingerprintOutRawCounts'):\n            parsed_data = self.parsePlotFingerprintOutRawCounts(f)\n            for k, v in parsed_data.items():\n                if k in self.deeptools_plotFingerprintOutRawCounts:\n                    log.warning(\"Replacing duplicate sample {}.\".format(k))\n                self.deeptools_plotFingerprintOutRawCounts[k] = v\n\n            if len(parsed_data) > 0:\n                self.add_data_source(f, section='plotFingerprint')\n\n        if len(self.deeptools_plotFingerprintOutRawCounts) > 0:\n            self.add_section(name=\"Fingerprint plot\",\n                             anchor=\"deeptools_fingerprint\",\n                             description=\"Signal fingerprint according to plotFingerprint\",\n                             plot=linegraph.plot(\n                                self.deeptools_plotFingerprintOutRawCounts,\n                                {\n                                    'id': 'deeptools_fingerprint_plot',\n                                    'title': 'deepTools: Fingerprint plot',\n                                    'xmin': 0.0,\n                                    'xmax': 1.0,\n                                    'ymin': 0.0,\n                                    'ymax': 1.0,\n                                    'xlab': 'rank',\n                                    'ylab': 'Fraction w.r.t. bin with highest coverage'\n                                }\n                             ))\n\n        if len(self.deeptools_plotFingerprintOutQualityMetrics) > 0:\n            self.add_section(name=\"Fingerprint quality metrics\",\n                             anchor=\"plotFingerprint\",\n                             description=\"Various quality metrics returned by plotFingerprint\",\n                             plot=linegraph.plot(\n                                self.deeptools_plotFingerprintOutQualityMetrics,\n                                {\n                                    'id': 'plotFingerprint_quality_metrics',\n                                    'title': 'deepTools: Fingerprint quality metrics',\n                                    'stacking': None,\n                                    'ymin': 0,\n                                    'ymax': 100,\n                                    'yLabelFormat': '{value}%',\n                                    'ylab': 'Percentage of fragments',\n                                    'categories': True,\n                                    'tt_label': '<strong>{point.x}</strong>: {point.y:.2f}%'\n                                }\n                             ))\n\n        return len(self.deeptools_plotFingerprintOutQualityMetrics), len(self.deeptools_plotFingerprintOutRawCounts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef slamdunkGeneralStatsTable(self):\n\n\n        headers = OrderedDict()\n        headers['counted'] = {\n            'title': '{} Counted'.format(config.read_count_prefix),\n            'description': '# reads counted within 3\\'UTRs ({})'.format(config.read_count_desc),\n            'shared_key': 'read_count',\n            'min': 0,\n            'format': '{:,.2f}',\n            'scale': 'YlGn',\n            'modify': lambda x: float(x) * config.read_count_multiplier,\n        }\n        headers['retained'] = {\n            'title': '{} Retained'.format(config.read_count_prefix),\n            'description': '# retained reads after filtering ({})'.format(config.read_count_desc),\n            'shared_key': 'read_count',\n            'min': 0,\n            'format': '{:,.2f}',\n            'scale': 'YlGn',\n            'modify': lambda x: float(x) * config.read_count_multiplier,\n        }\n        headers['mapped'] = {\n            'title': '{} Mapped'.format(config.read_count_prefix),\n            'description': '# mapped reads ({})'.format(config.read_count_desc),\n            'shared_key': 'read_count',\n            'min': 0,\n            'format': '{:,.2f}',\n            'scale': 'YlGn',\n            'modify': lambda x: float(x) * config.read_count_multiplier,\n        }\n        headers['sequenced'] = {\n            'title': '{} Sequenced'.format(config.read_count_prefix),\n            'description': '# sequenced reads ({})'.format(config.read_count_desc),\n            'shared_key': 'read_count',\n            'min': 0,\n            'format': '{:,.2f}',\n            'scale': 'YlGn',\n            'modify': lambda x: float(x) * config.read_count_multiplier,\n        }\n\n        self.general_stats_addcols(self.slamdunk_data, headers)", "response": "Take the parsed summary stats from Slamdunk and add it to the basic stats table at the top of the report"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntake the parsed filter stats from Slamdunk and add it to a separate table", "response": "def slamdunkFilterStatsTable(self):\n        \"\"\" Take the parsed filter stats from Slamdunk and add it to a separate table \"\"\"\n\n        headers = OrderedDict()\n        headers['mapped'] = {\n            'namespace': 'Slamdunk',\n            'title': '{} Mapped'.format(config.read_count_prefix),\n            'description': '# mapped reads ({})'.format(config.read_count_desc),\n            'shared_key': 'read_count',\n            'min': 0,\n            'format': '{:,.2f}',\n            'suffix': config.read_count_prefix,\n            'scale': 'YlGn',\n            'modify': lambda x: float(x) * config.read_count_multiplier,\n        }\n        headers['multimapper'] = {\n            'namespace': 'Slamdunk',\n            'title': '{} Multimap-Filtered'.format(config.read_count_prefix),\n            'description': '# multimap-filtered reads ({})'.format(config.read_count_desc),\n            'shared_key': 'read_count',\n            'min': 0,\n            'format': '{:,.2f}',\n            'suffix': config.read_count_prefix,\n            'scale': 'OrRd',\n            'modify': lambda x: float(x) * config.read_count_multiplier,\n        }\n        headers['nmfiltered'] = {\n            'namespace': 'Slamdunk',\n            'title': '{} NM-Filtered'.format(config.read_count_prefix),\n            'description': '# NM-filtered reads ({})'.format(config.read_count_desc),\n            'shared_key': 'read_count',\n            'min': 0,\n            'format': '{:,.2f}',\n            'suffix': config.read_count_prefix,\n            'scale': 'OrRd',\n            'modify': lambda x: float(x) * config.read_count_multiplier,\n        }\n        headers['idfiltered'] = {\n            'namespace': 'Slamdunk',\n            'title': '{} Identity-Filtered'.format(config.read_count_prefix),\n            'description': '# identity-filtered reads ({})'.format(config.read_count_desc),\n            'shared_key': 'read_count',\n            'min': 0,\n            'format': '{:,.2f}',\n            'suffix': config.read_count_prefix,\n            'scale': 'OrRd',\n            'modify': lambda x: float(x) * config.read_count_multiplier,\n        }\n        headers['mqfiltered'] = {\n            'namespace': 'Slamdunk',\n            'title': '{} MQ-Filtered'.format(config.read_count_prefix),\n            'description': '# MQ-filtered reads ({})'.format(config.read_count_desc),\n            'shared_key': 'read_count',\n            'min': 0,\n            'format': '{:,.2f}',\n            'suffix': config.read_count_prefix,\n            'scale': 'OrRd',\n            'modify': lambda x: float(x) * config.read_count_multiplier,\n        }\n        pconfig = {\n            'id': 'slamdunk_filtering_table',\n            'min': 0,\n        }\n\n        self.add_section (\n            name = 'Filter statistics',\n            anchor = 'slamdunk_filtering',\n            description = 'This table shows the number of reads filtered with each filter criterion during filtering phase of slamdunk.',\n            plot = table.plot(self.slamdunk_data, headers, pconfig)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef slamdunkOverallRatesPlot (self):\n\n        pconfig = {\n            'id': 'overallratesplot',\n            'title': 'Slamdunk: Overall conversion rates in reads',\n            'cpswitch': False,\n            'cpswitch_c_active': False,\n            'ylab': 'Number of reads',\n            'stacking': 'normal',\n            'tt_decimals': 2,\n            'tt_suffix': '%',\n            'tt_percentages': False,\n            'hide_zero_cats': False,\n            'data_labels': [\n                \"Plus Strand +\",\n                \"Minus Strand -\",\n            ]\n        }\n\n        cats = [OrderedDict(), OrderedDict()]\n        keys = [\n            ['T>C', 'A>T', 'A>G', 'A>C', 'T>A', 'T>G', 'G>A', 'G>T', 'G>C', 'C>A', 'C>T', 'C>G'],\n            ['A>G','A>T','A>C','T>A','T>G','T>C','G>A','G>T','G>C','C>A','C>T','C>G']\n        ]\n        for i, k in enumerate(keys):\n            for j, v in enumerate(k):\n                cats[i][v] = { 'color': self.plot_cols[j] }\n\n        self.add_section (\n            name = 'Conversion rates per read',\n            anchor = 'slamdunk_overall_rates',\n            description = \"\"\"This plot shows the individual conversion rates over all reads.\n                        It shows these conversion rates strand-specific: This means for a properly labeled\n                        sample you would see a T&gt;C excess on the plus-strand and an A&gt;G excess on the minus strand\n                        (see the <a href=\"http://t-neumann.github.io/slamdunk/docs.html#rates\" target=\"_blank\">slamdunk docs</a>).\"\"\",\n            plot = bargraph.plot([self.rates_data_plus,self.rates_data_minus], cats, pconfig)\n        )", "response": "Generate the overall rates plot for the specified object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef slamdunkUtrRatesPlot (self):\n\n        cats = OrderedDict()\n        keys = ['T>C', 'A>T', 'A>G', 'A>C', 'T>A', 'T>G', 'G>A', 'G>T', 'G>C', 'C>A', 'C>T', 'C>G']\n        for i, v in enumerate(keys):\n            cats[v] = { 'color': self.plot_cols[i] }\n\n        pconfig = {\n            'id': 'slamdunk_utrratesplot',\n            'title': 'Slamdunk: Overall conversion rates per UTR',\n            'cpswitch': False,\n            'cpswitch_c_active': False,\n            'ylab': 'Number of conversions',\n            'stacking': 'normal',\n            'tt_decimals': 2,\n            'tt_suffix': '%',\n            'tt_percentages': False,\n            'hide_zero_cats': False\n        }\n\n        self.add_section (\n            name = 'Conversion rates per UTR',\n            anchor = 'slamdunk_utr_rates',\n            description = \"\"\"This plot shows the individual conversion rates for all UTRs\n                        (see the <a href=\"http://t-neumann.github.io/slamdunk/docs.html#utrrates\" target=\"_blank\">slamdunk docs</a>).\"\"\",\n            plot = bargraph.plot(self.utrates_data, cats, pconfig)\n        )", "response": "Generate the UTR rates plot"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef slamdunkPCAPlot (self):\n\n        pconfig = {\n            'id': 'slamdunk_pca',\n            'title': 'Slamdunk: PCA',\n            'xlab': 'PC1',\n            'ylab': 'PC2',\n            'tt_label': 'PC1 {point.x:.2f}: PC2 {point.y:.2f}'\n        }\n\n        self.add_section (\n            name = 'PCA (T&gt;C based)',\n            anchor = 'slamdunk_PCA',\n            description = \"\"\"This plot shows the principal components of samples based\n                        on the distribution of reads with T&gt;C conversions within UTRs\n                        (see the <a href=\"http://t-neumann.github.io/slamdunk/docs.html#summary\" target=\"_blank\">slamdunk docs</a>).\"\"\",\n            plot = scatter.plot(self.PCA_data, pconfig)\n        )", "response": "Generate the PCA plots for the UTRs based on UTRs\n                       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef slamdunkTcPerReadPosPlot (self):\n\n        pconfig_nontc = {\n            'id': 'slamdunk_nontcperreadpos_plot',\n            'title': 'Slamdunk: Non-T>C mismatches over reads',\n            'ylab': 'Percent mismatches %',\n            'xlab': 'Position in read',\n            'xDecimals': False,\n            'ymin': 0,\n            'tt_label': '<b>Pos {point.x}</b>: {point.y:.2f} %',\n            'data_labels': [{'name': 'Forward reads +', 'ylab': 'Percent mismatches %'},\n                            {'name': 'Reverse reads -', 'ylab': 'Percent mismatches %'}]\n        }\n\n        pconfig_tc = {\n            'id': 'slamdunk_tcperreadpos_plot',\n            'title': 'Slamdunk: T>C conversions over reads',\n            'ylab': 'Percent converted %',\n            'xlab': 'Position in read',\n            'xDecimals': False,\n            'ymin': 0,\n            'tt_label': '<b>Pos {point.x}</b>: {point.y:.2f} %',\n            'data_labels': [{'name': 'Forward reads +', 'ylab': 'Percent converted %'},\n                            {'name': 'Reverse reads -', 'ylab': 'Percent converted %'}]\n        }\n\n        self.add_section (\n            name = 'Non T&gt;C mismatches over read positions',\n            anchor = 'slamdunk_nontcperreadpos',\n            description = \"\"\"This plot shows the distribution of non T&gt;C mismatches across read positions\n                        (see the <a href=\"http://t-neumann.github.io/slamdunk/docs.html#tcperreadpos\" target=\"_blank\">slamdunk docs</a>).\"\"\",\n            plot = linegraph.plot([self.nontc_per_readpos_plus, self.nontc_per_readpos_minus], pconfig_nontc)\n        )\n\n        self.add_section (\n            name = 'T&gt;C conversions over read positions',\n            anchor = 'slamdunk_tcperreadpos',\n            description = \"\"\"This plot shows the distribution of T&gt;C conversions across read positions\n                        (see the <a href=\"http://t-neumann.github.io/slamdunk/docs.html#tcperreadpos\" target=\"_blank\">slamdunk docs</a>).\"\"\",\n            plot = linegraph.plot([self.tc_per_readpos_plus, self.tc_per_readpos_minus], pconfig_tc)\n        )", "response": "Generate the tc per read pos plot for the current read."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating the tc per UTR pos plot", "response": "def slamdunkTcPerUTRPosPlot (self):\n        \"\"\" Generate the tc per UTR pos plots \"\"\"\n\n        pconfig_nontc = {\n            'id': 'slamdunk_slamdunk_nontcperutrpos_plot',\n            'title': 'Slamdunk: Non-T>C mutations over 3\\' UTR ends',\n            'ylab': 'Percent mismatches %',\n            'xlab': 'Position in the static last 250bp window of 3\\' UTR',\n            'xDecimals': False,\n            'ymin': 0,\n            'tt_label': '<b>Pos {point.x}</b>: {point.y:.2f} %',\n            'data_labels': [{'name': 'UTRs on plus strand', 'ylab': 'Percent mismatches %'},\n                            {'name': 'UTRs on minus strand', 'ylab': 'Percent mismatches %'}]\n        }\n\n        pconfig_tc = {\n            'id': 'slamdunk_slamdunk_tcperutrpos_plot',\n            'title': 'Slamdunk: T>C conversions over 3\\' UTR ends',\n            'ylab': 'Percent converted %',\n            'xlab': 'Position in the static last 250bp window of 3\\' UTR',\n            'xDecimals': False,\n            'ymin': 0,\n            'tt_label': '<b>Pos {point.x}</b>: {point.y:.2f} %',\n            'data_labels': [{'name': 'UTRs on plus strand', 'ylab': 'Percent converted %'},\n                            {'name': 'UTRs on minus strand', 'ylab': 'Percent converted %'}]\n        }\n\n        self.add_section (\n            name = 'Non T&gt;C mismatches over UTR positions',\n            anchor = 'slamdunk_nontcperutrpos',\n            description = \"\"\"This plot shows the distribution of non T&gt;C mismatches across UTR positions for the last 250 bp from the 3\\' UTR end\n                        (see the <a href=\"http://t-neumann.github.io/slamdunk/docs.html#tcperutrpos\" target=\"_blank\">slamdunk docs</a>).\"\"\",\n            plot = linegraph.plot([self.nontc_per_utrpos_plus, self.nontc_per_utrpos_minus], pconfig_nontc)\n        )\n\n        self.add_section (\n            name = 'T&gt;C conversions over UTR positions',\n            anchor = 'tcperutrpos',\n            description = \"\"\"This plot shows the distribution of T&gt;C conversions across UTR positions for the last 250 bp from the 3\\' UTR end\n                        (see the <a href=\"http://t-neumann.github.io/slamdunk/docs.html#tcperutrpos\" target=\"_blank\">slamdunk docs</a>).\"\"\",\n            plot = linegraph.plot([self.tc_per_utrpos_plus, self.tc_per_utrpos_minus], pconfig_tc)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntake the parsed stats from the Bowtie report and add it to the basic stats table at the top of the report", "response": "def bowtie_general_stats_table(self):\n        \"\"\" Take the parsed stats from the Bowtie report and add it to the\n        basic stats table at the top of the report \"\"\"\n\n        headers = OrderedDict()\n        headers['reads_aligned_percentage'] = {\n            'title': '% Aligned',\n            'description': '% reads with at least one reported alignment',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'YlGn'\n        }\n        headers['reads_aligned'] = {\n            'title': '{} Aligned'.format(config.read_count_prefix),\n            'description': 'reads with at least one reported alignment ({})'.format(config.read_count_desc),\n            'min': 0,\n            'scale': 'PuRd',\n            'modify': lambda x: x * config.read_count_multiplier,\n            'shared_key': 'read_count'\n        }\n        self.general_stats_addcols(self.bowtie_data, headers)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bowtie_alignment_plot (self):\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['reads_aligned'] = { 'color': '#8bbc21', 'name': 'Aligned' }\n        keys['multimapped'] =   { 'color': '#2f7ed8', 'name': 'Multimapped' }\n        keys['not_aligned'] =   { 'color': '#0d233a', 'name': 'Not aligned' }\n\n        # Config for the plot\n        config = {\n            'id': 'bowtie1_alignment',\n            'title': 'Bowtie 1: Alignment Scores',\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads'\n        }\n\n        self.add_section(\n            description = 'This plot shows the number of reads aligning to the reference in different ways.',\n            helptext = '''\n            There are 3 possible types of alignment:\n            * **Aligned**: Read has only one occurence in the reference genome.\n            * **Multimapped**: Read has multiple occurence.\n            * **Not aligned**: Read has no occurence.\n            ''',\n            plot = bargraph.plot(self.bowtie_data, keys, config)\n        )", "response": "Make the HTML to plot the alignment rates for the current bowtie entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a master list of files to search for.", "response": "def get_filelist(run_module_names):\n    \"\"\"\n    Go through all supplied search directories and assembly a master\n    list of files to search. Then fire search functions for each file.\n    \"\"\"\n    # Prep search patterns\n    spatterns = [{},{},{},{},{},{},{}]\n    epatterns = [{}, {}]\n    ignored_patterns = []\n    for key, sps in config.sp.items():\n        mod_name = key.split('/', 1)[0]\n        if mod_name.lower() not in [m.lower() for m in run_module_names]:\n            ignored_patterns.append(key)\n            continue\n        files[key] = list()\n        if not isinstance(sps, list):\n            sps = [sps]\n\n        # Warn if we have any unrecognised search pattern keys\n        expected_sp_keys = [\n            'fn',\n            'fn_re',\n            'contents',\n            'contents_re',\n            'num_lines',\n            'shared',\n            'max_filesize',\n            'exclude_fn',\n            'exclude_fn_re',\n            'exclude_contents',\n            'exclude_contents_re'\n        ]\n        unrecognised_keys = [y for x in sps for y in x.keys() if y not in expected_sp_keys]\n        if len(unrecognised_keys) > 0:\n            logger.warn(\"Unrecognised search pattern keys for '{}': {}\".format(key, ', '.join(unrecognised_keys)))\n\n        # Split search patterns according to speed of execution.\n        if any([x for x in sps if 'contents_re' in x]):\n            if any([x for x in sps if 'num_lines' in x]):\n                spatterns[4][key] = sps\n            elif any([x for x in sps if 'max_filesize' in x]):\n                spatterns[5][key] = sps\n            else:\n                spatterns[6][key] = sps\n        elif any([x for x in sps if 'contents' in x]):\n            if any([x for x in sps if 'num_lines' in x]):\n                spatterns[1][key] = sps\n            elif any([x for x in sps if 'max_filesize' in x]):\n                spatterns[2][key] = sps\n            else:\n                spatterns[3][key] = sps\n        else:\n            spatterns[0][key] = sps\n\n    if len(ignored_patterns) > 0:\n        logger.debug(\"Ignored {} search patterns as didn't match running modules.\".format(len(ignored_patterns)))\n\n    def add_file(fn, root):\n        \"\"\"\n        Function applied to each file found when walking the analysis\n        directories. Runs through all search patterns and returns True\n        if a match is found.\n        \"\"\"\n        f = {'fn': fn, 'root': root}\n\n        # Check that this is a file and not a pipe or anything weird\n        if not os.path.isfile(os.path.join(root, fn)):\n            return None\n\n        # Check that we don't want to ignore this file\n        i_matches = [n for n in config.fn_ignore_files if fnmatch.fnmatch(fn, n)]\n        if len(i_matches) > 0:\n            logger.debug(\"Ignoring file as matched an ignore pattern: {}\".format(fn))\n            return None\n\n        # Limit search to small files, to avoid 30GB FastQ files etc.\n        try:\n            f['filesize'] = os.path.getsize(os.path.join(root,fn))\n        except (IOError, OSError, ValueError, UnicodeDecodeError):\n            logger.debug(\"Couldn't read file when checking filesize: {}\".format(fn))\n        else:\n            if f['filesize'] > config.log_filesize_limit:\n                return False\n\n        # Test file for each search pattern\n        for patterns in spatterns:\n            for key, sps in patterns.items():\n                for sp in sps:\n                    if search_file (sp, f):\n                        # Check that we shouldn't exclude this file\n                        if not exclude_file(sp, f):\n                            # Looks good! Remember this file\n                            files[key].append(f)\n                        # Don't keep searching this file for other modules\n                        if not sp.get('shared', False):\n                            return\n                        # Don't look at other patterns for this module\n                        else:\n                            break\n\n    # Go through the analysis directories and get file list\n    for path in config.analysis_dir:\n        if os.path.islink(path) and config.ignore_symlinks:\n            continue\n        elif os.path.isfile(path):\n            searchfiles.append([os.path.basename(path), os.path.dirname(path)])\n        elif os.path.isdir(path):\n            for root, dirnames, filenames in os.walk(path, followlinks=(not config.ignore_symlinks), topdown=True):\n                bname = os.path.basename(root)\n\n                # Skip any sub-directories matching ignore params\n                orig_dirnames = dirnames[:]\n                for n in config.fn_ignore_dirs:\n                    dirnames[:] = [d for d in dirnames if not fnmatch.fnmatch(d, n.rstrip(os.sep))]\n                    if len(orig_dirnames) != len(dirnames):\n                        removed_dirs = [os.path.join(root, d) for d in set(orig_dirnames).symmetric_difference(set(dirnames))]\n                        logger.debug(\"Ignoring directory as matched fn_ignore_dirs: {}\".format(\", \".join(removed_dirs)))\n                        orig_dirnames = dirnames[:]\n                for n in config.fn_ignore_paths:\n                    dirnames[:] = [d for d in dirnames if not fnmatch.fnmatch(os.path.join(root, d), n.rstrip(os.sep))]\n                    if len(orig_dirnames) != len(dirnames):\n                        removed_dirs = [os.path.join(root, d) for d in set(orig_dirnames).symmetric_difference(set(dirnames))]\n                        logger.debug(\"Ignoring directory as matched fn_ignore_paths: {}\".format(\", \".join(removed_dirs)))\n\n                # Skip *this* directory if matches ignore params\n                d_matches = [n for n in config.fn_ignore_dirs if fnmatch.fnmatch(bname, n.rstrip(os.sep))]\n                if len(d_matches) > 0:\n                    logger.debug(\"Ignoring directory as matched fn_ignore_dirs: {}\".format(bname))\n                    continue\n                p_matches = [n for n in config.fn_ignore_paths if fnmatch.fnmatch(root, n.rstrip(os.sep))]\n                if len(p_matches) > 0:\n                    logger.debug(\"Ignoring directory as matched fn_ignore_paths: {}\".format(root))\n                    continue\n                # Search filenames in this directory\n                for fn in filenames:\n                    searchfiles.append([fn, root])\n    # Search through collected files\n    with click.progressbar(searchfiles, label=\"Searching {} files..\".format(len(searchfiles))) as sfiles:\n        for sf in sfiles:\n            add_file(sf[0], sf[1])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef search_file (pattern, f):\n\n    fn_matched = False\n    contents_matched = False\n\n    # Use mimetypes to exclude binary files where possible\n    if not re.match(r'.+_mqc\\.(png|jpg|jpeg)', f['fn']):\n        (ftype, encoding) = mimetypes.guess_type(os.path.join(f['root'], f['fn']))\n        if encoding is not None:\n            return False\n        if ftype is not None and ftype.startswith('image'):\n            return False\n\n    # Search pattern specific filesize limit\n    if pattern.get('max_filesize') is not None and 'filesize' in f:\n        if f['filesize'] > pattern.get('max_filesize'):\n            logger.debug(\"Ignoring because exceeded search pattern filesize limit: {}\".format(f['fn']))\n            return False\n\n    # Search by file name (glob)\n    if pattern.get('fn') is not None:\n        if fnmatch.fnmatch(f['fn'], pattern['fn']):\n            fn_matched = True\n            if pattern.get('contents') is None and pattern.get('contents_re') is None:\n                return True\n\n    # Search by file name (regex)\n    if pattern.get('fn_re') is not None:\n        if re.match( pattern['fn_re'], f['fn']):\n            fn_matched = True\n            if pattern.get('contents') is None and pattern.get('contents_re') is None:\n                return True\n\n    # Search by file contents\n    if pattern.get('contents') is not None or pattern.get('contents_re') is not None:\n        if pattern.get('contents_re') is not None:\n            repattern = re.compile(pattern['contents_re'])\n        try:\n            with io.open (os.path.join(f['root'],f['fn']), \"r\", encoding='utf-8') as f:\n                l = 1\n                for line in f:\n                    # Search by file contents (string)\n                    if pattern.get('contents') is not None:\n                        if pattern['contents'] in line:\n                            contents_matched = True\n                            if pattern.get('fn') is None and pattern.get('fn_re') is None:\n                                return True\n                            break\n                    # Search by file contents (regex)\n                    elif pattern.get('contents_re') is not None:\n                        if re.search(repattern, line):\n                            contents_matched = True\n                            if pattern.get('fn') is None and pattern.get('fn_re') is None:\n                                return True\n                            break\n                    # Break if we've searched enough lines for this pattern\n                    if pattern.get('num_lines') and l >= pattern.get('num_lines'):\n                        break\n                    l += 1\n        except (IOError, OSError, ValueError, UnicodeDecodeError):\n            if config.report_readerrors:\n                logger.debug(\"Couldn't read file when looking for output: {}\".format(f['fn']))\n                return False\n\n    return fn_matched and contents_matched", "response": "Search a single file for a single search pattern."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexcluding discovered files if they match the special exclude_file search pattern keys", "response": "def exclude_file(sp, f):\n    \"\"\"\n    Exclude discovered files if they match the special exclude_\n    search pattern keys\n    \"\"\"\n    # Make everything a list if it isn't already\n    for k in sp:\n        if k in ['exclude_fn', 'exclude_fn_re' 'exclude_contents', 'exclude_contents_re']:\n            if not isinstance(sp[k], list):\n                sp[k] = [sp[k]]\n\n    # Search by file name (glob)\n    if 'exclude_fn' in sp:\n        for pat in sp['exclude_fn']:\n            if fnmatch.fnmatch(f['fn'], pat):\n                return True\n\n    # Search by file name (regex)\n    if 'exclude_fn_re' in sp:\n        for pat in sp['exclude_fn_re']:\n            if re.match( pat, f['fn']):\n                return True\n\n    # Search the contents of the file\n    if 'exclude_contents' in sp or 'exclude_contents_re' in sp:\n        # Compile regex patterns if we have any\n        if 'exclude_contents_re' in sp:\n            sp['exclude_contents_re'] = [re.compile(pat) for pat in sp['exclude_contents_re']]\n        with io.open (os.path.join(f['root'],f['fn']), \"r\", encoding='utf-8') as fh:\n            for line in fh:\n                if 'exclude_contents' in sp:\n                    for pat in sp['exclude_contents']:\n                        if pat in line:\n                            return True\n                if 'exclude_contents_re' in sp:\n                    for pat in sp['exclude_contents_re']:\n                        if re.search(pat, line):\n                            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_htmlid(html_id, skiplint=False):\n    global html_ids\n    global lint_errors\n\n    # Trailing whitespace\n    html_id_clean = html_id.strip()\n\n    # Trailing underscores\n    html_id_clean = html_id_clean.strip('_')\n\n    # Must begin with a letter\n    if re.match(r'^[a-zA-Z]', html_id_clean) is None:\n        html_id_clean = 'mqc_{}'.format(html_id_clean)\n\n    # Replace illegal characters\n    html_id_clean = re.sub('[^a-zA-Z0-9_-]+', '_', html_id_clean)\n\n    # Validate if linting\n    if config.lint and not skiplint:\n        modname = ''\n        codeline = ''\n        callstack = inspect.stack()\n        for n in callstack:\n            if 'multiqc/modules/' in n[1] and 'base_module.py' not in n[1]:\n                callpath = n[1].split('multiqc/modules/',1)[-1]\n                modname = '>{}< '.format(callpath)\n                codeline = n[4][0].strip()\n                break\n    if config.lint and not skiplint and html_id != html_id_clean:\n        errmsg = \"LINT: {}HTML ID was not clean ('{}' -> '{}') ## {}\".format(modname, html_id, html_id_clean, codeline)\n        logger.error(errmsg)\n        lint_errors.append(errmsg)\n\n    # Check for duplicates\n    i = 1\n    html_id_base = html_id_clean\n    while html_id_clean in html_ids:\n        html_id_clean = '{}-{}'.format(html_id_base, i)\n        i += 1\n        if config.lint and not skiplint:\n            errmsg = \"LINT: {}HTML ID was a duplicate ({}) ## {}\".format(modname, html_id_clean, codeline)\n            logger.error(errmsg)\n            lint_errors.append(errmsg)\n\n    # Remember and return\n    html_ids.append(html_id_clean)\n    return html_id_clean", "response": "Save a HTML ID sanitise for HTML check for duplicates and save."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compress_json(data):\n    json_string = json.dumps(data).encode('utf-8', 'ignore').decode('utf-8')\n    # JSON.parse() doesn't handle `NaN`, but it does handle `null`.\n    json_string = json_string.replace('NaN', 'null');\n    x = lzstring.LZString()\n    return x.compressToBase64(json_string)", "response": "Take a Python data object. Convert to JSON and compress using lzstring."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding the parsed stats to the methylQA basic stats table", "response": "def methylqa_general_stats_table(self):\n        \"\"\" Take the parsed stats from the methylQA report and add it to the\n        basic stats table at the top of the report \"\"\"\n\n        headers = OrderedDict()\n        headers['coverage'] = {\n            'title': 'Fold Coverage',\n            'min': 0,\n            'suffix': 'X',\n            'scale': 'YlGn'\n        }\n        self.general_stats_addcols(self.methylqa_data, headers)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef methylqa_alignment_plot (self):\n\n        if len(self.methylqa_coverage_counts) == 0:\n            return '<div class=\"alert alert-danger\">No histogram data found.</div>'\n\n        pconfig = {\n            'id': 'methylqa_coverage',\n            'title': 'methylQA: CpG Coverage',\n            'categories': True,\n            'ylab': 'CpG Counts',\n            'xlab': 'Times covered',\n            'ymin': 0,\n            'data_labels': [\n                {'name': 'Counts', 'ylab': 'CpG Counts' },\n                {'name': 'Percentages', 'ylab': '% CpGs', 'ymax': 100 }\n            ]\n        }\n        return linegraph.plot([self.methylqa_coverage_counts, self.methylqa_coverage_percentages], pconfig)", "response": "Make the HighCharts HTML to plot the alignment rates for a single class of class methylQA."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_rsem_report (self, f):\n        data = dict()\n        multimapping_hist = dict()\n        in_hist = False\n        for l in f['f'].splitlines():\n            s = l.split()\n            if len(s) > 3:\n                # Line: N0 N1 N2 N_tot\n                # N0, number of unalignable reads;\n                # N1, number of alignable reads; (nUnique + nMulti)\n                # N2, number of filtered reads due to too many alignments\n                # N_tot = N0 + N1 + N2\n                data['Unalignable'] = int(s[0])\n                data['Alignable'] = int(s[1])\n                data['Filtered'] = int(s[2])\n                data['Total'] = int(s[3])\n                data['alignable_percent'] = (float(s[1]) / float(s[3])) * 100.0\n            elif len(s) == 3:\n                # Line: nUnique nMulti nUncertain\n                # nUnique, number of reads aligned uniquely to a gene\n                # nMulti, number of reads aligned to multiple genes; nUnique + nMulti = N1;\n                # nUncertain, number of reads aligned to multiple locations in the given reference sequences, which include isoform-level multi-mapping reads\n                data['Unique'] = int(s[0])\n                data['Multi'] = int(s[1])\n                data['Uncertain'] = int(s[2])\n            elif len(s) == 2:\n                # Number of gene alignments and read count\n                if in_hist or int(s[0]) == 0:\n                    in_hist = True\n                    try:\n                        multimapping_hist[int(s[0])] = int(s[1])\n                    except ValueError:\n                        pass\n            else:\n                break\n        try:\n            assert data['Unique'] + data['Multi'] == data['Alignable']\n        except AssertionError:\n            log.warn(\"Unique + Multimapping read counts != alignable reads! '{}'\".format(f['fn']))\n            return None\n        except KeyError:\n            log.warn(\"Error parsing RSEM counts file '{}'\".format(f['fn']))\n            return None\n\n        # Save parsed data\n        if len(data) > 0:\n            if f['s_name'] in self.rsem_mapped_data:\n                log.debug(\"Duplicate sample name found! Overwriting: {}\".format(f['s_name']))\n            self.rsem_mapped_data[f['s_name']] = data\n            self.add_data_source(f)\n        if len(multimapping_hist) > 0:\n            self.rsem_multimapping_data[f['s_name']] = multimapping_hist", "response": "Parse the rsem cnt stat file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntake the parsed stats from the rsem report and add them to the basic stats table at the top of the report", "response": "def rsem_stats_table(self):\n        \"\"\" Take the parsed stats from the rsem report and add them to the\n        basic stats table at the top of the report \"\"\"\n        headers = OrderedDict()\n        headers['alignable_percent'] = {\n            'title': '% Alignable'.format(config.read_count_prefix),\n            'description': '% Alignable reads'.format(config.read_count_desc),\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'YlGn'\n        }\n        self.general_stats_addcols(self.rsem_mapped_data, headers)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking the rsem assignment rates plot", "response": "def rsem_mapped_reads_plot(self):\n        \"\"\" Make the rsem assignment rates plot \"\"\"\n\n        # Plot categories\n        keys = OrderedDict()\n        keys['Unique'] =      { 'color': '#437bb1', 'name': 'Aligned uniquely to a gene' }\n        keys['Multi'] =       { 'color': '#e63491', 'name': 'Aligned to multiple genes' }\n        keys['Filtered'] =    { 'color': '#b1084c', 'name': 'Filtered due to too many alignments' }\n        keys['Unalignable'] = { 'color': '#7f0000', 'name': 'Unalignable reads' }\n\n        # Config for the plot\n        config = {\n            'id': 'rsem_assignment_plot',\n            'title': 'RSEM: Mapped reads',\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads',\n            'hide_zero_cats': False\n        }\n\n        self.add_section(\n            name = 'Mapped Reads',\n            anchor = 'rsem_mapped_reads',\n            description = 'A breakdown of how all reads were aligned for each sample.',\n            plot = bargraph.plot(self.rsem_mapped_data, keys, config)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rsem_multimapping_plot(self):\n\n        pconfig = {\n            'id': 'rsem_multimapping_rates',\n            'title': 'RSEM: Multimapping Rates',\n            'ylab': 'Counts',\n            'xlab': 'Number of alignments',\n            'xDecimals': False,\n            'ymin': 0,\n            'tt_label': '<b>{point.x} alignments</b>: {point.y:.0f}',\n        }\n\n        self.add_section(\n            name = 'Multimapping rates',\n            anchor = 'rsem_multimapping',\n            description = 'A frequency histogram showing how many reads were aligned to `n` reference regions.',\n            helptext = '''In an ideal world, every sequence reads would align uniquely to a single location in the\n                reference. However, due to factors such as repeititve sequences, short reads and sequencing errors,\n                reads can be align to the reference 0, 1 or more times. This plot shows the frequency of each factor\n                of multimapping. Good samples should have the majority of reads aligning once.''',\n            plot = linegraph.plot(self.rsem_multimapping_data, pconfig)\n        )", "response": "Make a line plot showing the multimapping rates for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates the HTML for the TsTv by quality linegraph plot.", "response": "def parse_tstv_by_qual(self):\n        \"\"\" Create the HTML for the TsTv by quality linegraph plot. \"\"\"\n\n        self.vcftools_tstv_by_qual = dict()\n        for f in self.find_log_files('vcftools/tstv_by_qual', filehandles=True):\n            d = {}\n            for line in f['f'].readlines()[1:]: # don't add the header line (first row)\n                key = float(line.split()[0]) # taking the first column (QUAL_THRESHOLD) as key\n                val = float(line.split()[6]) # taking Ts/Tv_GT_QUAL_THRESHOLD as value\n                if (val == float('inf')) or (val == float('-inf')):\n                    val = float('nan')\n                d[key] = val\n            self.vcftools_tstv_by_qual[f['s_name']] = d\n\n        # Filter out ignored sample names\n        self.vcftools_tstv_by_qual = self.ignore_samples(self.vcftools_tstv_by_qual)\n\n        if len(self.vcftools_tstv_by_qual) == 0:\n            return 0\n\n        pconfig = {\n            'id': 'vcftools_tstv_by_qual',\n            'title': 'VCFTools: TsTv by Qual',\n            'ylab': 'TsTv Ratio',\n            'xlab': 'SNP Quality Threshold',\n            'xmin': 0,\n            'ymin': 0,\n            'smooth_points': 400, # this limits huge filesizes and prevents browser crashing\n            'smooth_points_sumcounts': False\n        }\n\n        helptext = '''\n        `Transition` is a purine-to-purine or pyrimidine-to-pyrimidine point mutations.\n        `Transversion` is a purine-to-pyrimidine or pyrimidine-to-purine point mutation.\n        `Quality` here is the Phred-scaled quality score as given in the QUAL column of VCF.\n        Note: only bi-allelic SNPs are used (multi-allelic sites and INDELs are skipped.)\n        Refer to Vcftools's manual (https://vcftools.github.io/man_latest.html) on `--TsTv-by-qual`\n        '''\n\n        self.add_section(\n            name = 'TsTv by Qual',\n            anchor = 'vcftools-tstv-by-qual',\n            description = \"Plot of `TSTV-BY-QUAL` - the transition to transversion ratio as a function of SNP quality from the output of vcftools TsTv-by-qual.\",\n            helptext = helptext,\n            plot = linegraph.plot(self.vcftools_tstv_by_qual,pconfig)\n        )\n\n        return len(self.vcftools_tstv_by_qual)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_snpeff_log(self, f):\n\n        keys = {\n            '# Summary table': [\n                'Genome', 'Number_of_variants_before_filter',\n                'Number_of_known_variants', 'Number_of_effects',\n                'Genome_total_length', 'Change_rate'\n            ],\n            '# Effects by impact': [ 'HIGH', 'LOW', 'MODERATE', 'MODIFIER' ],\n            '# Effects by functional class': [ 'MISSENSE', 'NONSENSE', 'SILENT', 'Missense_Silent_ratio' ],\n            '# Hom/Het table': ['Het', 'Hom', 'Missing'],\n            '# Ts/Tv summary': [ 'Transitions', 'Transversions', 'Ts_Tv_ratio' ],\n            '# Count by effects': 'all',\n            '# Count by genomic region': 'all'\n        }\n        parsed_data = {}\n        section = None\n        for l in f['f']:\n            l = l.strip()\n            if l[:1] == '#':\n                section = l\n                self.snpeff_section_totals[section] = dict()\n                continue\n            s = l.split(',')\n\n            # Quality values / counts\n            if section == '# Quality':\n                quals = OrderedDict()\n                if l.startswith('Values'):\n                    values = [int(c) for c in l.split(',')[1:] ]\n                    counts = f['f'].readline()\n                    counts = [int(c) for c in counts.split(',')[1:] ]\n                    c = 0\n                    total = sum(counts)\n                    for i, v in enumerate(values):\n                        if c < (total * 0.995):\n                            quals[v] = counts[i]\n                            c += counts[i]\n                if len(quals) > 0:\n                    self.snpeff_qualities[f['s_name']] = quals\n\n            # Everything else\n            elif section in keys:\n                if keys[section] == 'all' or any([k in s[0].strip() for k in keys[section]]):\n                    try:\n                        parsed_data[ s[0].strip() ] = float(s[1].strip())\n                    except ValueError:\n                        parsed_data[ s[0].strip() ] = s[1].strip()\n                    except IndexError:\n                        pass\n                    else:\n                        # Parsing the number worked - add to totals\n                        try:\n                            self.snpeff_section_totals[section][s[0].strip()] += parsed_data[ s[0].strip() ]\n                        except KeyError:\n                            self.snpeff_section_totals[section][s[0].strip()] = parsed_data[ s[0].strip() ]\n                    if len(s) > 2 and s[2][-1:] == '%':\n                        parsed_data[ '{}_percent'.format(s[0].strip()) ] = float(s[2][:-1])\n\n        if len(parsed_data) > 0:\n            if f['s_name'] in self.snpeff_data:\n                log.debug(\"Duplicate sample name found! Overwriting: {}\".format(f['s_name']))\n            self.add_data_source(f)\n            self.snpeff_data[f['s_name']] = parsed_data", "response": "Parse the snpeff log file looking for the information in the file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding SnpEff stats to the general stats table", "response": "def general_stats(self):\n        \"\"\" Add key SnpEff stats to the general stats table \"\"\"\n\n        headers = OrderedDict()\n        headers['Change_rate'] = {\n            'title': 'Change rate',\n            'scale': 'RdYlBu-rev',\n            'min': 0,\n            'format': '{:,.0f}'\n        }\n        headers['Ts_Tv_ratio'] = {\n            'title': 'Ts/Tv',\n            'description': 'Transitions / Transversions ratio',\n            'format': '{:,.3f}'\n        }\n        headers['Number_of_variants_before_filter'] = {\n            'title': 'M Variants',\n            'description': 'Number of variants before filter (millions)',\n            'scale': 'PuRd',\n            'modify': lambda x: x / 1000000,\n            'min': 0,\n            'format': '{:,.2f}'\n        }\n        self.general_stats_addcols(self.snpeff_data, headers)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate the SnpEff Counts by Genomic Region plot", "response": "def count_genomic_region_plot(self):\n        \"\"\" Generate the SnpEff Counts by Genomic Region plot \"\"\"\n\n        # Sort the keys based on the total counts\n        keys = self.snpeff_section_totals['# Count by genomic region']\n        sorted_keys = sorted(keys, reverse=True, key=keys.get)\n\n        # Make nicer label names\n        pkeys = OrderedDict()\n        for k in sorted_keys:\n            pkeys[k] = {'name': k.replace('_', ' ').title().replace('Utr', 'UTR') }\n\n        # Config for the plot\n        pconfig = {\n            'id': 'snpeff_variant_effects_region',\n            'title': 'SnpEff: Counts by Genomic Region',\n            'ylab': '# Reads',\n            'logswitch': True\n        }\n\n        return bargraph.plot(self.snpeff_data, pkeys, pconfig)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating the SnpEff Counts by Effects Impact plot", "response": "def effects_impact_plot(self):\n        \"\"\" Generate the SnpEff Counts by Effects Impact plot \"\"\"\n\n        # Put keys in a more logical order\n        keys = [ 'MODIFIER', 'LOW', 'MODERATE', 'HIGH' ]\n\n        # Make nicer label names\n        pkeys = OrderedDict()\n        for k in keys:\n            pkeys[k] = {'name': k.title() }\n\n        # Config for the plot\n        pconfig = {\n            'id': 'snpeff_variant_effects_impact',\n            'title': 'SnpEff: Counts by Effects Impact',\n            'ylab': '# Reads',\n            'logswitch': True\n        }\n\n        return bargraph.plot(self.snpeff_data, pkeys, pconfig)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_prokka(self, f):\n\n        s_name = None\n\n        # Look at the first three lines, they are always the same\n        first_line = f['f'].readline()\n        contigs_line = f['f'].readline()\n        bases_line = f['f'].readline()\n        # If any of these fail, it's probably not a prokka summary file\n        if not all((first_line.startswith(\"organism:\"),\n                   contigs_line.startswith(\"contigs:\"),\n                   bases_line.startswith(\"bases:\"))):\n            return\n\n        # Get organism and sample name from the first line\n        # Assumes organism name only consists of two words,\n        # i.e. 'Genusname speciesname', and that the remaining\n        # text on the organism line is the sample name.\n        try:\n            organism = \" \".join(first_line.strip().split(\":\", 1)[1].split()[:2])\n            s_name = self.clean_s_name(\" \".join(first_line.split()[3:]), f['root'])\n        except KeyError:\n            organism = first_line.strip().split(\":\", 1)[1]\n            s_name = f['s_name']\n        # Don't try to guess sample name if requested in the config\n        if getattr(config, 'prokka_fn_snames', False):\n            s_name = f['s_name']\n\n        if s_name in self.prokka:\n            log.debug(\"Duplicate sample name found! Overwriting: {}\".format(s_name))\n        self.prokka[s_name] = dict()\n        self.prokka[s_name]['organism'] = organism\n        self.prokka[s_name]['contigs'] = int(contigs_line.split(\":\")[1])\n        self.prokka[s_name]['bases'] = int(bases_line.split(\":\")[1])\n\n        # Get additional info from remaining lines\n        for line in f['f']:\n            description, value = line.split(\":\")\n            try:\n                self.prokka[s_name][description] = int(value)\n            except ValueError:\n                log.warning(\"Unable to parse line: '%s'\", line)\n\n        self.add_data_source(f, s_name)", "response": "Parse the prokka summary files and return a dict of the keys organism and sample name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef prokka_table(self):\n\n        # Specify the order of the different possible categories\n        headers = OrderedDict()\n        headers['organism'] = {\n                'title': 'Organism',\n                'description': 'Organism name',\n        }\n        headers['contigs'] = {\n                'title': '# contigs',\n                'description': 'Number of contigs in assembly',\n                'format': '{:i}',\n        }\n        headers['bases'] = {\n                'title': '# bases',\n                'description': 'Number of nucleotide bases in assembly',\n                'format': '{:i}',\n        }\n        headers['CDS'] = {\n                'title': '# CDS',\n                'description': 'Number of annotated CDS',\n                'format': '{:i}',\n        }\n        headers['rRNA'] = {\n                'title': '# rRNA',\n                'description': 'Number of annotated rRNA',\n                'format': '{:i}',\n        }\n        headers['tRNA'] = {\n                'title': '# tRNA',\n                'description': 'Number of annotated tRNA',\n                'format': '{:i}',\n        }\n        headers['tmRNA'] = {\n                'title': '# tmRNA',\n                'description': 'Number of annotated tmRNA',\n                'format': '{:i}',\n        }\n        headers['misc_RNA'] = {\n                'title': '# misc RNA',\n                'description': 'Number of annotated misc. RNA',\n                'format': '{:i}',\n        }\n        headers['sig_peptide'] = {\n                'title': '# sig_peptide',\n                'description': 'Number of annotated sig_peptide',\n                'format': '{:i}',\n        }\n        headers['repeat_region'] = {\n                'title': '# CRISPR arrays',\n                'description': 'Number of annotated CRSIPR arrays',\n                'format': '{:i}',\n        }\n        table_config = {\n            'namespace': 'prokka',\n            'min': 0,\n        }\n\n        return table.plot(self.prokka, headers, table_config)", "response": "Make basic table of the annotation stats"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes a basic bar plot of the annotation stats", "response": "def prokka_barplot(self):\n        \"\"\" Make a basic plot of the annotation stats \"\"\"\n\n        # Specify the order of the different categories\n        keys = OrderedDict()\n        keys['CDS'] =           { 'name': 'CDS' }\n        keys['rRNA'] =          { 'name': 'rRNA' }\n        keys['tRNA'] =          { 'name': 'tRNA' }\n        keys['tmRNA'] =         { 'name': 'tmRNA' }\n        keys['misc_RNA'] =      { 'name': 'misc RNA' }\n        keys['sig_peptide'] =   { 'name': 'Signal peptides' }\n        keys['repeat_region'] = { 'name': 'CRISPR array'}\n\n        plot_config = {\n            'id': 'prokka_plot',\n            'title': 'Prokka: Feature Types',\n            'ylab': '# Counts',\n            'cpswitch_counts_label': 'Features'\n        }\n\n        return bargraph.plot(self.prokka, keys, plot_config)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntry to guess the file format based on the file extension.", "response": "def _guess_file_format(f):\n    \"\"\"\n    Tries to guess file format, first based on file extension (csv / tsv),\n    then by looking for common column separators in the first 10 non-commented lines.\n    Splits by tab / comma / space and counts resulting number of columns. Finds the most\n    common column count, then comparsed how many lines had this number.\n    eg. if tab, all 10 lines should have x columns when split by tab.\n    Returns: csv | tsv | spaces   (spaces by default if all else fails)\n    \"\"\"\n    filename, file_extension = os.path.splitext(f['fn'])\n    tabs = []\n    commas = []\n    spaces = []\n    j = 0\n    for l in f['f'].splitlines():\n        if not l.startswith('#'):\n            j += 1\n            tabs.append(len(l.split(\"\\t\")))\n            commas.append(len(l.split(\",\")))\n            spaces.append(len(l.split()))\n        if j == 10:\n            break\n    tab_mode = max(set(tabs), key=tabs.count)\n    commas_mode = max(set(commas), key=commas.count)\n    spaces_mode = max(set(spaces), key=spaces.count)\n    tab_lc = tabs.count(tab_mode) if tab_mode > 1 else 0\n    commas_lc = commas.count(commas_mode) if commas_mode > 1 else 0\n    spaces_lc = spaces.count(spaces_mode) if spaces_mode > 1 else 0\n    if tab_lc == j:\n        return 'tsv'\n    elif commas_lc == j:\n        return 'csv'\n    else:\n        if tab_lc > commas_lc and tab_lc > spaces_lc:\n            return 'tsv'\n        elif commas_lc > tab_lc and commas_lc > spaces_lc:\n            return 'csv'\n        elif spaces_lc > tab_lc and spaces_lc > commas_lc:\n            return 'spaces'\n        else:\n            if tab_mode == commas_lc and tab_mode > spaces_lc:\n                if tab_mode > commas_mode:\n                    return 'tsv'\n                else:\n                    return 'csv'\n    return 'spaces'"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind RSeQC bam_stat reports and parse their data", "response": "def parse_reports(self):\n    \"\"\" Find RSeQC bam_stat reports and parse their data \"\"\"\n\n    # Set up vars\n    self.bam_stat_data = dict()\n    regexes = {\n        'total_records': r\"Total records:\\s*(\\d+)\",\n        'qc_failed': r\"QC failed:\\s*(\\d+)\",\n        'optical_pcr_duplicate': r\"Optical/PCR duplicate:\\s*(\\d+)\",\n        'non_primary_hits': r\"Non primary hits\\s*(\\d+)\",\n        'unmapped_reads': r\"Unmapped reads:\\s*(\\d+)\",\n        'mapq_lt_mapq_cut_non-unique': r\"mapq < mapq_cut \\(non-unique\\):\\s*(\\d+)\",\n        'mapq_gte_mapq_cut_unique': r\"mapq >= mapq_cut \\(unique\\):\\s*(\\d+)\",\n        'read_1': r\"Read-1:\\s*(\\d+)\",\n        'read_2': r\"Read-2:\\s*(\\d+)\",\n        'reads_map_to_sense': r\"Reads map to '\\+':\\s*(\\d+)\",\n        'reads_map_to_antisense': r\"Reads map to '-':\\s*(\\d+)\",\n        'non-splice_reads': r\"Non-splice reads:\\s*(\\d+)\",\n        'splice_reads': r\"Splice reads:\\s*(\\d+)\",\n        'reads_mapped_in_proper_pairs': r\"Reads mapped in proper pairs:\\s*(\\d+)\",\n        'proper-paired_reads_map_to_different_chrom': r\"Proper-paired reads map to different chrom:\\s*(\\d+)\",\n    }\n\n    #intiate PE check\n    is_paired_end=False\n\n    # Go through files and parse data using regexes\n    for f in self.find_log_files('rseqc/bam_stat'):\n        d = dict()\n        for k, r in regexes.items():\n            r_search = re.search(r, f['f'], re.MULTILINE)\n            if r_search:\n                d[k] = int(r_search.group(1))\n\n        # Calculate some percentages\n        if 'total_records' in d:\n            t = float(d['total_records'])\n            if 'mapq_gte_mapq_cut_unique' in d:\n                d['unique_percent'] = (float(d['mapq_gte_mapq_cut_unique']) / t)*100.0\n            if 'reads_mapped_in_proper_pairs' in d:\n                d['proper_pairs_percent'] = (float(d['reads_mapped_in_proper_pairs']) / t)*100.0\n\n        if len(d) > 0:\n            if f['s_name'] in self.bam_stat_data:\n                log.debug(\"Duplicate sample name found! Overwriting: {}\".format(f['s_name']))\n            self.add_data_source(f, section='bam_stat')\n            #Check if SE or PE\n            if d['read_2'] != 0:\n                is_paired_end = True\n            self.bam_stat_data[f['s_name']] = d\n\n    # Filter to strip out ignored sample names\n    self.bam_stat_data = self.ignore_samples(self.bam_stat_data)\n\n    if len(self.bam_stat_data) > 0:\n        # Write to file\n        self.write_data_file(self.bam_stat_data, 'multiqc_rseqc_bam_stat')\n\n        # Add to general stats table\n        self.general_stats_headers['proper_pairs_percent'] = {\n            'title': '% Proper Pairs',\n            'description': '% Reads mapped in proper pairs',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'RdYlGn'\n        }\n        for s_name in self.bam_stat_data:\n            if s_name not in self.general_stats_data:\n                self.general_stats_data[s_name] = dict()\n\n                #Only write if PE, i.e. there is something to write\n                if is_paired_end:\n                    self.general_stats_data[s_name].update( self.bam_stat_data[s_name] )\n\n        # Make dot plot of counts\n        pconfig = {\n            'id': 'rseqc_bam_stat'\n        }\n        keys = OrderedDict()\n        defaults = {\n            'min': 0,\n            'shared_key': 'read_count',\n            'decimalPlaces': 2,\n            'modify': lambda x: float(x) / 1000000.0,\n        }\n        keys['total_records'] = dict(defaults, **{'title': 'Total records' })\n        keys['qc_failed'] = dict(defaults, **{'title': 'QC failed' })\n        keys['optical_pcr_duplicate'] = dict(defaults, **{'title': 'Duplicates', 'description': 'Optical/PCR duplicate' })\n        keys['non_primary_hits'] = dict(defaults, **{'title': 'Non primary hit' })\n        keys['unmapped_reads'] = dict(defaults, **{'title': 'Unmapped', 'description': 'Unmapped reads' })\n        keys['mapq_lt_mapq_cut_non'] = dict(defaults, **{'title': 'Non-unique', 'description': 'mapq < mapq_cut (non-unique)' })\n        keys['mapq_gte_mapq_cut_unique'] = dict(defaults, **{'title': 'Unique', 'description': 'mapq >= mapq_cut (unique)' })\n        if is_paired_end:\n            keys['read_1'] = dict(defaults, **{'title': 'Read-1' })\n            keys['read_2'] = dict(defaults, **{'title': 'Read-2' })\n        keys['reads_map_to_sense'] = dict(defaults, **{'title': '+ve strand', 'description': \"Reads map to '+'\" })\n        keys['reads_map_to_antisense'] = dict(defaults, **{'title': '-ve strand', 'description': \"Reads map to '-'\" })\n        keys['non-splice_reads'] = dict(defaults, **{'title': 'Non-splice reads' })\n        keys['splice_reads'] = dict(defaults, **{'title': 'Splice reads' })\n        if is_paired_end:\n            keys['reads_mapped_in_proper_pairs'] = dict(defaults, **{\n                        'title': 'Proper pairs', 'description':'Reads mapped in proper pairs' })\n            keys['proper-paired_reads_map_to_different_chrom'] = dict(defaults, **{\n                        'title': 'Different chrom', 'description': 'Proper-paired reads map to different chrom' })\n\n        self.add_section (\n            name = 'Bam Stat',\n            anchor = 'rseqc-bam_stat',\n            description = 'All numbers reported in millions.',\n            plot = beeswarm.plot(self.bam_stat_data, keys, pconfig)\n        )\n\n    # Return number of samples found\n    return len(self.bam_stat_data)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_reports(self):\n\n    # Set up vars\n    self.picard_HsMetrics_data = dict()\n\n    # Go through logs and find Metrics\n    for f in self.find_log_files('picard/hsmetrics', filehandles=True):\n        parsed_data = dict()\n        s_name = None\n        keys = None\n        commadecimal = None\n        for l in f['f']:\n            # New log starting\n            if 'CalculateHsMetrics' in l or 'CollectHsMetrics' in l and 'INPUT' in l:\n                s_name = None\n                keys = None\n\n                # Pull sample name from input\n                fn_search = re.search(r\"INPUT(?:=|\\s+)(\\[?[^\\s]+\\]?)\", l, flags=re.IGNORECASE)\n                if fn_search:\n                    s_name = os.path.basename(fn_search.group(1).strip('[]'))\n                    s_name = self.clean_s_name(s_name, f['root'])\n                    parsed_data[s_name] = dict()\n\n            if s_name is not None:\n                if 'HsMetrics' in l and '## METRICS CLASS' in l:\n                    keys = f['f'].readline().strip(\"\\n\").split(\"\\t\")\n                elif keys:\n                    vals = l.strip(\"\\n\").split(\"\\t\")\n                    if len(vals) == len(keys):\n                        j = 'NA'\n                        if keys[0] == 'BAIT_SET':\n                            j = vals[0]\n                        parsed_data[s_name][j] = dict()\n                        # Check that we're not using commas for decimal places\n                        if commadecimal is None:\n                            for i, k in enumerate(keys):\n                                if k.startswith('PCT_'):\n                                    if ',' in vals[i]:\n                                        commadecimal = True\n                                    else:\n                                        commadecimal = False\n                        for i, k in enumerate(keys):\n                            try:\n                                if commadecimal:\n                                    vals[i] = vals[i].replace('.', '')\n                                    vals[i] = vals[i].replace(',', '.')\n                                parsed_data[s_name][j][k] = float(vals[i])\n                            except ValueError:\n                                parsed_data[s_name][j][k] = vals[i]\n                    else:\n                        s_name = None\n                        keys = None\n\n        # Remove empty dictionaries\n        for s_name in list(parsed_data.keys()):\n            for j in parsed_data[s_name].keys():\n                if len(parsed_data[s_name][j]) == 0:\n                    parsed_data[s_name].pop(j, None)\n            if len(parsed_data[s_name]) == 0:\n                parsed_data.pop(s_name, None)\n\n        # Manipulate sample names if multiple baits found\n        for s_name in parsed_data.keys():\n            for j in parsed_data[s_name].keys():\n                this_s_name = s_name\n                if(len(parsed_data[s_name]) > 1):\n                    this_s_name = \"{}: {}\".format(s_name, j)\n                if this_s_name in self.picard_HsMetrics_data:\n                    log.debug(\"Duplicate sample name found in {}! Overwriting: {}\".format(f['fn'], this_s_name))\n                self.add_data_source(f, this_s_name, section='HsMetrics')\n                self.picard_HsMetrics_data[this_s_name] = parsed_data[s_name][j]\n\n\n    # Filter to strip out ignored sample names\n    self.picard_HsMetrics_data = self.ignore_samples(self.picard_HsMetrics_data)\n\n    if len(self.picard_HsMetrics_data) > 0:\n\n        # Write parsed data to a file\n        self.write_data_file(self.picard_HsMetrics_data, 'multiqc_picard_HsMetrics')\n\n        # Add to general stats table\n        # Swap question marks with -1\n        data = self.picard_HsMetrics_data\n        for s_name in data:\n            if data[s_name]['FOLD_ENRICHMENT'] == '?':\n                data[s_name]['FOLD_ENRICHMENT'] = -1\n\n        self.general_stats_headers['FOLD_ENRICHMENT'] = {\n            'title': 'Fold Enrichment',\n            'min': 0,\n            'format': '{:,.0f}',\n            'scale': 'Blues',\n        }\n        try:\n            covs = config.picard_config['general_stats_target_coverage']\n            assert type(covs) == list\n            assert len(covs) > 0\n            covs = [str(i) for i in covs]\n            log.debug(\"Custom Picard coverage thresholds: {}\".format(\", \".join([i for i in covs])))\n        except (AttributeError, TypeError, AssertionError):\n            covs = ['30']\n        for c in covs:\n            self.general_stats_headers['PCT_TARGET_BASES_{}X'.format(c)] = {\n                'id': 'picard_target_bases_{}X'.format(c),\n                'title': 'Target Bases {}X'.format(c),\n                'description': 'Percent of target bases with coverage &ge; {}X'.format(c),\n                'max': 100,\n                'min': 0,\n                'suffix': '%',\n                'format': '{:,.0f}',\n                'scale': 'RdYlGn',\n                'modify': lambda x: self.multiply_hundred(x)\n            }\n        for s_name in data:\n            if s_name not in self.general_stats_data:\n                self.general_stats_data[s_name] = dict()\n            self.general_stats_data[s_name].update( data[s_name] )\n        data_table = _clean_table(data)\n        self.add_section (\n                name = 'HSMetrics',\n                anchor = 'picard_hsmetrics',\n                plot = table.plot(data_table, _get_headers(data_table))\n        )\n        tbases = _add_target_bases(data)\n        self.add_section (\n            name = tbases['name'],\n            anchor = tbases['anchor'],\n            description = tbases['description'],\n            plot = tbases['plot']\n        )\n        hs_pen = _add_hs_penalty(data)\n        if hs_pen is not None:\n            self.add_section (\n                name = hs_pen['name'],\n                anchor = hs_pen['anchor'],\n                description = hs_pen['description'],\n                plot = hs_pen['plot']\n            )\n\n    # Return the number of detected samples to the parent module\n    return len(self.picard_HsMetrics_data)", "response": "Find Picard HsMetrics reports and parse their data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_bhist(samples, file_type, **plot_args):\n\n    all_x = set()\n    for item in sorted(chain(*[samples[sample]['data'].items()\n                                for sample in samples])):\n        all_x.add(item[0])\n\n    columns_to_plot = {\n        'GC': {\n            1: 'C',\n            2: 'G',\n        },\n        'AT': {\n            0: 'A',\n            3: 'T',\n        },\n        'N': {\n            4: 'N'\n        },\n    }\n    nucleotide_data = []\n    for column_type in columns_to_plot:\n        nucleotide_data.append(\n            {\n                sample+'.'+column_name: {\n                    x: samples[sample]['data'][x][column]*100 if x in samples[sample]['data'] else 0\n                    for x in all_x\n            }\n            for sample in samples\n            for column, column_name in columns_to_plot[column_type].items()\n        }\n    )\n\n    plot_params = {\n            'id': 'bbmap-' + file_type + '_plot',\n            'title': 'BBTools: ' + plot_args['plot_title'],\n            'xlab': 'Read position',\n            'ymin': 0,\n            'ymax': 100,\n            'data_labels': [\n                {'name': 'Percentage of G+C bases'},\n                {'name': 'Percentage of A+T bases'},\n                {'name': 'Percentage of N bases'},\n            ]\n    }\n    plot_params.update(plot_args['plot_params'])\n    plot = linegraph.plot(\n        nucleotide_data,\n        plot_params\n    )\n\n    return plot", "response": "Create line graph plot of histogram data for BBMap bhist output."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates HTML for read length distribution plot.", "response": "def add_readlen_dist_plot(self):\n        \"\"\" Generate plot HTML for read length distribution plot. \"\"\"\n        pconfig = {\n            'id': 'skewer_read_length_histogram',\n            'title': 'Skewer: Read Length Distribution after trimming',\n            'xDecimals': False,\n            'ylab': '% of Reads',\n            'xlab': 'Read Length',\n            'xmin': 0,\n            'ymin': 0,\n            'tt_label': '<b>{point.x}</b>: {point.y:.1f}%',\n        }\n        self.add_section(\n            plot = linegraph.plot(self.skewer_readlen_dist, pconfig)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_skewer_log(self, f):\n        fh = f['f']\n        regexes = {\n            'fq1': \"Input file:\\s+(.+)\",\n            'fq2': \"Paired file:\\s+(.+)\",\n            'r_processed': \"(\\d+) read|reads pairs? processed\",\n            'r_short_filtered': \"(\\d+) \\(\\s*\\d+.\\d+%\\) short read\",\n            'r_empty_filtered': \"(\\d+) \\(\\s*\\d+.\\d+%\\) empty read\",\n            'r_avail': \"(\\d+) \\(\\s*\\d+.\\d+%\\) read\",\n            'r_trimmed': \"(\\d+) \\(\\s*\\d+.\\d+%\\) trimmed read\",\n            'r_untrimmed': \"(\\d+) \\(\\s*\\d+.\\d+%\\) untrimmed read\"\n        }\n        regex_hist = \"\\s?(\\d+)\\s+(\\d+)\\s+(\\d+.\\d+)%\"\n\n        data = dict()\n        for k, v in regexes.items():\n            data[k] = 0\n        data['fq1'] = None\n        data['fq2'] = None\n        readlen_dist = OrderedDict()\n\n        for l in fh:\n            for k, r in regexes.items():\n                match = re.search(r, l)\n                if match:\n                    data[k] = match.group(1).replace(',', '')\n\n            match = re.search(regex_hist, l)\n            if match:\n                read_length = int(match.group(1))\n                pct_at_rl = float(match.group(3))\n                readlen_dist[read_length] = pct_at_rl\n\n        if data['fq1'] is not None:\n            s_name = self.clean_s_name(data['fq1'], f['root'])\n            if s_name in self.skewer_readlen_dist:\n                log.debug(\"Duplicate sample name found in {}! Overwriting: {}\".format(f['fn'], s_name))\n            self.add_data_source(f, s_name)\n            self.add_skewer_data(s_name, data, f)\n            self.skewer_readlen_dist[s_name] = readlen_dist\n\n        if data['fq2'] is not None:\n            s_name = self.clean_s_name(data['fq1'], f['root'])\n            if s_name in self.skewer_readlen_dist:\n                log.debug(\"Duplicate sample name found in {}! Overwriting: {}\".format(f['fn'], s_name))\n            self.add_data_source(f, s_name)\n            self.add_skewer_data(s_name, data, f)\n            self.skewer_readlen_dist[s_name] = readlen_dist", "response": "Go through the log file looking for skewer output"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_reports(self):\n\n    # Set up vars\n    self.read_dups = dict()\n\n    # Go through files and parse data\n    for f in self.find_log_files('rseqc/read_duplication_pos'):\n        if f['f'].startswith('Occurrence\tUniqReadNumber'):\n            if f['s_name'] in self.read_dups:\n                log.debug(\"Duplicate sample name found! Overwriting: {}\".format(f['s_name']))\n            self.add_data_source(f, section='read_duplication')\n            self.read_dups[f['s_name']] = OrderedDict()\n            for l in f['f'].splitlines():\n                s = l.split()\n                try:\n                    if int(s[0]) <= 500:\n                        self.read_dups[f['s_name']][int(s[0])] = int(s[1])\n                except:\n                    pass\n\n    # Filter to strip out ignored sample names\n    self.read_dups = self.ignore_samples(self.read_dups)\n\n    if len(self.read_dups) > 0:\n\n        # Add line graph to section\n        pconfig = {\n            'smooth_points': 200,\n            'id': 'rseqc_read_dups_plot',\n            'title': 'RSeQC: Read Duplication',\n            'ylab': 'Number of Reads (log10)',\n            'xlab': \"Occurrence of read\",\n            'yLog': True,\n            'tt_label': \"<strong>{point.x} occurrences</strong>: {point.y} reads\",\n        }\n\n        self.add_section (\n            name = 'Read Duplication',\n            anchor = 'rseqc-read_dups',\n            description = '<a href=\"http://rseqc.sourceforge.net/#read-duplication-py\" target=\"_blank\">read_duplication.py</a>' \\\n                \" calculates how many alignment positions have a certain number of exact duplicates.\"\\\n                \" Note - plot truncated at 500 occurrences and binned.</p>\",\n            plot = linegraph.plot(self.read_dups, pconfig)\n        )\n\n    # Return number of samples found\n    return len(self.read_dups)", "response": "Find RSeQC read_duplication reports and parse their data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the sample name from the input file and returns it.", "response": "def read_sample_name(line_iter, clean_fn):\n    \"\"\"\n    Consumes lines from the provided line_iter and parses those lines\n    as a header for the picard base distribution file.  The header\n    file is assumed to contain a line with both 'INPUT' and\n    'BaseDistributionByCycle'.\n\n    If the header parses correctly, the sample name is returned.  If\n    the header does not parse correctly, None is returned.\n    \"\"\"\n    try:\n        while True:\n            new_line = next(line_iter)\n            new_line = new_line.strip()\n            if 'BaseDistributionByCycle' in new_line and 'INPUT' in new_line:\n                # Pull sample name from input\n                fn_search = re.search(r\"INPUT=?\\s*(\\[?[^\\s]+\\]?)\", new_line, flags=re.IGNORECASE)\n                if fn_search:\n                    s_name = os.path.basename(fn_search.group(1).strip('[]'))\n                    s_name = clean_fn(s_name)\n                    return s_name\n    except StopIteration:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the base distribution data from the provided line_iter and returns the data dict mapping cycles to tuples.", "response": "def read_base_distrib_data(line_iter):\n    \"\"\"\n    Consumes lines from the provided line_iter and parses those lines\n    for base distribution data.  Data should be tab separated and\n    immediately preceded by a line of headers:\n\n    READ_END  CYCLE  PCT_A  PCT_C  PCT_G  PCT_T  PCT_N\n\n    Returns either None or a dict mapping cycles to tuples\n      (read_end pct_a pct_c pct_g pct_t pct_n)\n    where all values are numbers.\n\n    A None indicates that no lines matching the expected format\n    were found.\n    \"\"\"\n    try:\n        line = next(line_iter)\n        while 'BaseDistributionByCycle' not in line and '## METRICS CLASS' not in line:\n            line = next(line_iter)\n        line = next(line_iter)\n        headers = line.strip().split(\"\\t\")\n        assert headers == ['READ_END', 'CYCLE', 'PCT_A', 'PCT_C', 'PCT_G', 'PCT_T', 'PCT_N']\n\n        # read base distribution by cycle\n        data = {}\n\n        row = next(line_iter).strip()\n        max_cycle_r1 = None\n        while row:\n            row_data = list(map(float, row.strip().split(\"\\t\")))\n            read_end, cycle, pct_a, pct_c, pct_g, pct_t, pct_n = row_data\n            cycle = int(cycle)\n            if read_end == 1.0:\n                if max_cycle_r1 is None or cycle > max_cycle_r1:\n                    max_cycle_r1 = cycle\n            elif max_cycle_r1 is not None:\n                cycle = cycle - max_cycle_r1\n            data_by_cycle = data.setdefault(read_end, dict())\n            data_by_cycle[cycle] = (\n                pct_a, pct_c, pct_g, pct_t, pct_n\n            )\n            row = next(line_iter).strip()\n        return data\n    except (StopIteration, AssertionError):\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_reports(self):\n\n    # Set up vars\n    self.picard_baseDistributionByCycle_data = dict()\n    self.picard_baseDistributionByCycle_samplestats = dict()\n\n    # Go through logs and find Metrics\n    base_dist_files = self.find_log_files('picard/basedistributionbycycle', filehandles=True)\n\n    for f in base_dist_files:\n        try:\n            lines = iter(f['f'])\n\n            # read through the header of the file to obtain the\n            # sample name\n            clean_fn = lambda n: self.clean_s_name(n, f['root'])\n            s_name = read_sample_name(lines, clean_fn)\n            assert s_name is not None\n\n            # pull out the data\n            data = read_base_distrib_data(lines)\n            assert data is not None\n\n            # data should be a hierarchical dict\n            # data[read_end][cycle]\n            assert not (set(data) - set([1, 2]))\n\n            # set up the set of s_names\n            if 2 in set(data):\n                s_names = {\n                    1:\"%s_R1\" % s_name,\n                    2:\"%s_R2\" % s_name\n                }\n            else:\n                s_names = { 1:s_name }\n\n            previously_used = (\n                set(s_names.values())&set(self.picard_baseDistributionByCycle_data)\n            )\n\n            if previously_used:\n                for duped_name in previously_used:\n                    log.debug(\n                        \"Duplicate sample name found in {}! \"\n                        \"Overwriting: {}\".format(f['fn'], duped_name)\n                    )\n            for name in s_names.values():\n                self.add_data_source(f, name, section='BaseDistributionByCycle')\n\n            for read_end in s_names:\n                data_by_cycle = data[read_end]\n                s_name = s_names[read_end]\n                self.picard_baseDistributionByCycle_data[s_name] = data_by_cycle\n                samplestats = {\n                    'sum_pct_a':0,\n                    'sum_pct_c':0,\n                    'sum_pct_g':0,\n                    'sum_pct_t':0,\n                    'sum_pct_n':0,\n                    'cycle_count':0,\n                }\n                self.picard_baseDistributionByCycle_samplestats[s_name] = samplestats\n                for c, row in data_by_cycle.items():\n                    pct_a, pct_c, pct_g, pct_t, pct_n = row\n                    samplestats['sum_pct_a'] += pct_a\n                    samplestats['sum_pct_c'] += pct_c\n                    samplestats['sum_pct_g'] += pct_g\n                    samplestats['sum_pct_t'] += pct_t\n                    samplestats['sum_pct_n'] += pct_n\n                samplestats['cycle_count'] += len(data_by_cycle.keys())\n        except AssertionError:\n            pass\n\n    # Calculate summed mean values for all read orientations\n    for s_name, v in self.picard_baseDistributionByCycle_samplestats.items():\n        v['mean_pct_a'] = v['sum_pct_a'] / v['cycle_count']\n        v['mean_pct_c'] = v['sum_pct_c'] / v['cycle_count']\n        v['mean_pct_g'] = v['sum_pct_g'] / v['cycle_count']\n        v['mean_pct_t'] = v['sum_pct_t'] / v['cycle_count']\n\n    # Filter to strip out ignored sample names\n    self.picard_baseDistributionByCycle_data = self.ignore_samples(self.picard_baseDistributionByCycle_data)\n\n    if len(self.picard_baseDistributionByCycle_data) > 0:\n\n        # Write parsed data to a file\n        self.write_data_file(self.picard_baseDistributionByCycle_samplestats, 'multiqc_picard_baseContent')\n\n        # Plot the data and add section\n        pconfig = {\n            'id': 'picard_base_distribution_by_cycle',\n            'title': 'Picard: Base Distribution',\n            'ylab': '%',\n            'xlab': 'Cycle #',\n            'xDecimals': False,\n            'tt_label': '<b>cycle {point.x}</b>: {point.y:.2f} %',\n            'ymax': 100,\n            'ymin': 0,\n            'data_labels': [\n                {'name': '% Adenine', 'ylab': '% Adenine'},\n                {'name': '% Cytosine', 'ylab': '% Cytosine'},\n                {'name': '% Guanine', 'ylab': '% Guanine'},\n                {'name': '% Thymine', 'ylab': '% Thymine'},\n                {'name': '% Undetermined', 'ylab': '% Undetermined'},\n            ]\n        }\n\n        # build list of linegraphs\n        linegraph_data = [{}, {}, {}, {}, {}]\n        for s_name, cycles in self.picard_baseDistributionByCycle_data.items():\n            reformat_items = lambda n: {\n                cycle : tup[n] for cycle, tup in cycles.items()\n            }\n            for lg, index in zip(linegraph_data, range(5)):\n                lg[s_name] = reformat_items(index)\n\n        self.add_section (\n            name = 'Base Distribution',\n            anchor = 'picard-base-distribution-by-cycle',\n            description = 'Plot shows the distribution of bases by cycle.',\n            plot = linegraph.plot(linegraph_data, pconfig)\n        )\n\n\n    # Return the number of detected samples to the parent module\n    return len(self.picard_baseDistributionByCycle_data)", "response": "Find Picard BaseDistributionByCycleMetrics reports and parse their data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_fastp_log(self, f):\n        try:\n            parsed_json = json.load(f['f'])\n        except:\n            log.warn(\"Could not parse fastp JSON: '{}'\".format(f['fn']))\n            return None\n\n        # Fetch a sample name from the command\n        s_name = f['s_name']\n        cmd = parsed_json['command'].split()\n        for i,v in enumerate(cmd):\n            if v == '-i':\n                s_name = self.clean_s_name(cmd[i+1], f['root'])\n        if s_name == 'fastp':\n            log.warn('Could not parse sample name from fastp command: {}'.format(f['fn']))\n\n        self.add_data_source(f, s_name)\n        self.fastp_data[s_name] = {}\n        self.fastp_duplication_plotdata[s_name] = {}\n        self.fastp_insert_size_data[s_name] = {}\n        self.fastp_all_data[s_name] = parsed_json\n        for k in ['read1_before_filtering','read2_before_filtering','read1_after_filtering','read2_after_filtering']:\n            self.fastp_qual_plotdata[k][s_name] = {}\n            self.fastp_gc_content_data[k][s_name] = {}\n            self.fastp_n_content_data[k][s_name] = {}\n\n        # Parse filtering_result\n        try:\n            for k in parsed_json['filtering_result']:\n                self.fastp_data[s_name]['filtering_result_{}'.format(k)] = float(parsed_json['filtering_result'][k])\n        except KeyError:\n            log.debug(\"fastp JSON did not have 'filtering_result' key: '{}'\".format(f['fn']))\n\n        # Parse duplication\n        try:\n            self.fastp_data[s_name]['pct_duplication'] = float(parsed_json['duplication']['rate'] * 100.0)\n        except KeyError:\n            log.debug(\"fastp JSON did not have a 'duplication' key: '{}'\".format(f['fn']))\n\n        # Parse after_filtering\n        try:\n            for k in parsed_json['summary']['after_filtering']:\n                self.fastp_data[s_name]['after_filtering_{}'.format(k)] = float(parsed_json['summary']['after_filtering'][k])\n        except KeyError:\n            log.debug(\"fastp JSON did not have a 'summary'-'after_filtering' keys: '{}'\".format(f['fn']))\n\n\n        # Parse data required to calculate Pct reads surviving\n        try:\n            self.fastp_data[s_name]['before_filtering_total_reads'] = float(parsed_json['summary']['before_filtering']['total_reads'])\n        except KeyError:\n            log.debug(\"Could not find pre-filtering # reads: '{}'\".format(f['fn']))\n\n        try:\n            self.fastp_data[s_name]['pct_surviving'] = (self.fastp_data[s_name]['after_filtering_total_reads'] / self.fastp_data[s_name]['before_filtering_total_reads']) * 100.0\n        except KeyError:\n            log.debug(\"Could not calculate 'pct_surviving': {}\".format(f['fn']))\n\n        # Parse adapter_cutting\n        try:\n            for k in parsed_json['adapter_cutting']:\n                try:\n                    self.fastp_data[s_name]['adapter_cutting_{}'.format(k)] = float(parsed_json['adapter_cutting'][k])\n                except (ValueError, TypeError):\n                    pass\n        except KeyError:\n            log.debug(\"fastp JSON did not have a 'adapter_cutting' key, skipping: '{}'\".format(f['fn']))\n\n        try:\n            self.fastp_data[s_name]['pct_adapter'] = (self.fastp_data[s_name]['adapter_cutting_adapter_trimmed_reads'] / self.fastp_data[s_name]['before_filtering_total_reads']) * 100.0\n        except KeyError:\n            log.debug(\"Could not calculate 'pct_adapter': {}\".format(f['fn']))\n\n        # Duplication rate plot data\n        try:\n            # First count the total read count in the dup analysis\n            total_reads = 0\n            for v in parsed_json['duplication']['histogram']:\n                total_reads += v\n            if total_reads == 0:\n                raise KeyError\n            # Calculate percentages\n            for i, v in enumerate(parsed_json['duplication']['histogram']):\n                self.fastp_duplication_plotdata[s_name][i+1] = (float(v) / float(total_reads)) * 100.0\n        except KeyError:\n            log.debug(\"No duplication rate plot data: {}\".format(f['fn']))\n\n        # Insert size plot data\n        try:\n            # First count the total read count in the insert size analysis\n            total_reads = 0\n            max_i = 0\n            for i, v in enumerate(parsed_json['insert_size']['histogram']):\n                total_reads += v\n                if float(v) > 0:\n                    max_i = i\n            if total_reads == 0:\n                raise KeyError\n            # Calculate percentages\n            for i, v in enumerate(parsed_json['insert_size']['histogram']):\n                if i <= max_i:\n                    self.fastp_insert_size_data[s_name][i+1] = (float(v) / float(total_reads)) * 100.0\n        except KeyError:\n            log.debug(\"No insert size plot data: {}\".format(f['fn']))\n\n        for k in ['read1_before_filtering','read2_before_filtering','read1_after_filtering','read2_after_filtering']:\n            # Read quality data\n            try:\n                for i, v in enumerate(parsed_json[k]['quality_curves']['mean']):\n                    self.fastp_qual_plotdata[k][s_name][i+1] = float(v)\n            except KeyError:\n                log.debug(\"Read quality {} not found: {}\".format(k, f['fn']))\n\n            # GC and N content plots\n            try:\n                for i, v in enumerate(parsed_json[k]['content_curves']['GC']):\n                    self.fastp_gc_content_data[k][s_name][i+1] = float(v) * 100.0\n                for i, v in enumerate(parsed_json[k]['content_curves']['N']):\n                    self.fastp_n_content_data[k][s_name][i+1] = float(v) * 100.0\n            except KeyError:\n                log.debug(\"Content curve data {} not found: {}\".format(k, f['fn']))\n\n\n        # Remove empty dicts\n        if len(self.fastp_data[s_name]) == 0:\n            del self.fastp_data[s_name]\n        if len(self.fastp_duplication_plotdata[s_name]) == 0:\n            del self.fastp_duplication_plotdata[s_name]\n        if len(self.fastp_insert_size_data[s_name]) == 0:\n            del self.fastp_insert_size_data[s_name]\n        if len(self.fastp_all_data[s_name]) == 0:\n            del self.fastp_all_data[s_name]", "response": "Parse the fastp output from fastp and save the summary statistics"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fastp_general_stats_table(self):\n\n        headers = OrderedDict()\n        headers['pct_duplication'] = {\n            'title': '% Duplication',\n            'description': 'Duplication rate in filtered reads',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'RdYlGn-rev'\n        }\n        headers['after_filtering_q30_rate'] = {\n            'title': '% > Q30',\n            'description': 'Percentage of reads > Q30 after filtering',\n            'min': 0,\n            'max': 100,\n            'modify': lambda x: x * 100.0,\n            'scale': 'GnBu',\n            'suffix': '%',\n            'hidden': True\n        }\n        headers['after_filtering_q30_bases'] = {\n            'title': '{} Q30 bases'.format(config.base_count_prefix),\n            'description': 'Bases > Q30 after filtering ({})'.format(config.base_count_desc),\n            'min': 0,\n            'modify': lambda x: x * config.base_count_multiplier,\n            'scale': 'GnBu',\n            'shared_key': 'base_count',\n            'hidden': True\n        }\n        headers['after_filtering_gc_content'] = {\n            'title': 'GC content',\n            'description': 'GC content after filtering',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'Blues',\n            'modify': lambda x: x * 100.0\n        }\n        headers['pct_surviving'] = {\n            'title': '% PF',\n            'description': 'Percent reads passing filter',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'BuGn',\n        }\n        headers['pct_adapter'] = {\n            'title': '% Adapter',\n            'description': 'Percentage adapter-trimmed reads',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'RdYlGn-rev',\n        }\n\n        self.general_stats_addcols(self.fastp_data, headers)", "response": "Take the parsed stats from the fastp report and add it to the General Statistics table at the top of the report"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fastp_filtered_reads_chart(self):\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['filtering_result_passed_filter_reads'] =  { 'name': 'Passed Filter' }\n        keys['filtering_result_low_quality_reads'] =    { 'name': 'Low Quality' }\n        keys['filtering_result_too_many_N_reads'] =     { 'name': 'Too Many N' }\n        keys['filtering_result_too_short_reads'] =      { 'name': 'Too short' }\n\n        # Config for the plot\n        pconfig = {\n            'id': 'fastp_filtered_reads_plot',\n            'title': 'Fastp: Filtered Reads',\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads',\n            'hide_zero_cats': False,\n        }\n        return bargraph.plot(self.fastp_data, keys, pconfig)", "response": "Function to generate the fastp filtered reads bar plot"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fastp_read_qual_plot(self):\n        data_labels, pdata = self.filter_pconfig_pdata_subplots(self.fastp_qual_plotdata, 'Sequence Quality')\n        pconfig = {\n            'id': 'fastp-seq-quality-plot',\n            'title': 'Fastp: Sequence Quality',\n            'xlab': 'Read Position',\n            'ylab': 'R1 Before filtering: Sequence Quality',\n            'ymin': 0,\n            'xDecimals': False,\n            'data_labels': data_labels\n        }\n        return linegraph.plot(pdata, pconfig)", "response": "Make the read quality plot for Fastp"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fastp_read_gc_plot(self):\n        data_labels, pdata = self.filter_pconfig_pdata_subplots(self.fastp_gc_content_data, 'Base Content Percent')\n        pconfig = {\n            'id': 'fastp-seq-content-gc-plot',\n            'title': 'Fastp: Read GC Content',\n            'xlab': 'Read Position',\n            'ylab': 'R1 Before filtering: Base Content Percent',\n            'ymax': 100,\n            'ymin': 0,\n            'xDecimals': False,\n            'yLabelFormat': '{value}%',\n            'tt_label': '{point.x}: {point.y:.2f}%',\n            'data_labels': data_labels\n        }\n        return linegraph.plot(pdata, pconfig)", "response": "Make the read GC plot for Fastp"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake the read N content plot for Fastp", "response": "def fastp_read_n_plot(self):\n        \"\"\" Make the read N content plot for Fastp \"\"\"\n        data_labels, pdata = self.filter_pconfig_pdata_subplots(self.fastp_n_content_data, 'Base Content Percent')\n        pconfig = {\n            'id': 'fastp-seq-content-n-plot',\n            'title': 'Fastp: Read N Content',\n            'xlab': 'Read Position',\n            'ylab': 'R1 Before filtering: Base Content Percent',\n            'yCeiling': 100,\n            'yMinRange': 5,\n            'ymin': 0,\n            'xDecimals': False,\n            'yLabelFormat': '{value}%',\n            'tt_label': '{point.x}: {point.y:.2f}%',\n            'data_labels': data_labels\n        }\n        return linegraph.plot(pdata, pconfig)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_plotProfile(self):\n        self.deeptools_plotProfile = dict()\n        for f in self.find_log_files('deeptools/plotProfile', filehandles=False):\n            parsed_data, bin_labels, converted_bin_labels = self.parsePlotProfileData(f)\n            for k, v in parsed_data.items():\n                if k in self.deeptools_plotProfile:\n                    log.warning(\"Replacing duplicate sample {}.\".format(k))\n                self.deeptools_plotProfile[k] = v\n            if len(parsed_data) > 0:\n                self.add_data_source(f, section='plotProfile')\n\n        if len(self.deeptools_plotProfile) > 0:\n            config = {\n                'id': 'read_distribution_profile',\n                'title': 'deeptools: Read Distribution Profile after Annotation',\n                'ylab': 'Occurrence',\n                'xlab': None,\n                'smooth_points': 100,\n                'xPlotBands': [\n                    {'from': converted_bin_labels[bin_labels.index('TES')], 'to': converted_bin_labels[-1], 'color': '#f7cfcf'},\n                    {'from': converted_bin_labels[bin_labels.index('TSS')], 'to': converted_bin_labels[bin_labels.index('TES')], 'color': '#ffffe2'},\n                    {'from': converted_bin_labels[0], 'to': converted_bin_labels[bin_labels.index('TSS')], 'color': '#e5fce0'},\n                ],\n                'xPlotLines': [\n                    {'width': 1, 'value': converted_bin_labels[bin_labels.index('TES')], 'dashStyle': 'Dash', 'color': '#000000'},\n                    {'width': 1, 'value': converted_bin_labels[bin_labels.index('TSS')], 'dashStyle': 'Dash', 'color': '#000000'},\n                ],\n            }\n\n            self.add_section (\n                name = 'Read Distribution Profile after Annotation',\n                anchor = 'read_distribution_profile_plot',\n                description=\"Accumulated view of the distribution of sequence reads related to the closest annotated gene. All annotated genes have been normalized to the same size. Green: {} upstream of gene to {}; Yellow: {} to {}; Pink: {} to {} downstream of gene\".format(list(filter(None,bin_labels))[0], list(filter(None,bin_labels))[1], list(filter(None,bin_labels))[1], list(filter(None,bin_labels))[2], list(filter(None,bin_labels))[2], list(filter(None,bin_labels))[3]),\n                plot=linegraph.plot(self.deeptools_plotProfile, config)\n            )\n\n        return len(self.deeptools_bamPEFragmentSizeDistribution)", "response": "Find plotProfile output and parse it"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef qorts_general_stats (self):\n        headers = OrderedDict()\n        headers['Genes_PercentWithNonzeroCounts'] = {\n            'title': '% Genes with Counts',\n            'description': 'Percent of Genes with Non-Zero Counts',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'YlGn'\n        }\n        headers['NumberOfChromosomesCovered'] = {\n            'title': 'Chrs Covered',\n            'description': 'Number of Chromosomes Covered',\n            'format': '{:,.0f}'\n        }\n        self.general_stats_addcols(self.qorts_data, headers)", "response": "Add columns to the General Statistics table"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef qorts_alignment_barplot (self):\n        # Specify the order of the different possible categories\n        keys = [\n            'ReadPairs_UniqueGene_CDS',\n            'ReadPairs_UniqueGene_UTR',\n            'ReadPairs_AmbigGene',\n            'ReadPairs_NoGene_Intron',\n            'ReadPairs_NoGene_OneKbFromGene',\n            'ReadPairs_NoGene_TenKbFromGene',\n            'ReadPairs_NoGene_MiddleOfNowhere'\n        ]\n        cats = OrderedDict()\n        for k in keys:\n            name = k.replace('ReadPairs_', '').replace('_',': ')\n            name = re.sub(\"([a-z])([A-Z])\",\"\\g<1> \\g<2>\",name)\n            cats[k] = { 'name': name }\n\n        # Config for the plot\n        pconfig = {\n            'id': 'qorts_alignments',\n            'title': 'QoRTs: Alignment Locations',\n            'ylab': '# Read Pairs',\n            'cpswitch_counts_label': 'Number of Read Pairs',\n            'hide_zero_cats': False\n        }\n\n        self.add_section(\n            name = \"Alignments\",\n            description = \"This plot displays the rate for which the sample's read-pairs are assigned to the different categories.\",\n            helptext = '''\n            The [QoRTs vignette](http://hartleys.github.io/QoRTs/doc/QoRTs-vignette.pdf) describes the categories in this plot as follows:\n\n            * **Unique Gene**: The read-pair overlaps with the exonic segments of one and only one gene. For many\n              downstream analyses tools, such as DESeq, DESeq2 and EdgeR, only read-pairs in this category\n              are used.\n            * **Ambig Gene**: The read-pair overlaps with the exons of more than one gene.\n            * **No Gene: Intronic**: The read-pair does not overlap with the exons of any annotated gene, but appears\n              in a region that is bridged by an annotated splice junction.\n            * **No Gene: One kb From Gene**: The read-pair does not overlap with the exons of any annotated gene, but is\n              within 1 kilobase from the nearest annotated gene.\n            * **No Gene: Ten kb From Gene**: The read-pair does not overlap with the exons of any annotated gene, but\n              is within 10 kilobases from the nearest annotated gene.\n            * **No Gene: Middle Of Nowhere**: The read-pair does not overlap with the exons of any annotated gene,\n              and is more than 10 kilobases from the nearest annotated gene.\n\n            _What it means and what to look for:_\n\n            Outliers in these plots can indicate biological variations or the presence of large mapping problems.\n            They may also suggest the presence of large, highly-expressed, unannotated transcripts or genes.\n            ''',\n            plot = bargraph.plot(self.qorts_data, cats, pconfig)\n        )", "response": "Make the alignment statistics bar plot for the current sample."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning HTML for a MultiQC table.", "response": "def plot (data, headers=None, pconfig=None):\n    \"\"\" Return HTML for a MultiQC table.\n    :param data: 2D dict, first keys as sample names, then x:y data pairs\n    :param headers: list of optional dicts with column config in key:value pairs.\n    :return: HTML ready to be inserted into the page\n    \"\"\"\n    if headers is None:\n        headers = []\n    if pconfig is None:\n        pconfig = {}\n\n    # Allow user to overwrite any given config for this plot\n    if 'id' in pconfig and pconfig['id'] and pconfig['id'] in config.custom_plot_config:\n        for k, v in config.custom_plot_config[pconfig['id']].items():\n            pconfig[k] = v\n\n    # Make a datatable object\n    dt = table_object.datatable(data, headers, pconfig)\n\n    # Collect unique sample names\n    s_names = set()\n    for d in dt.data:\n        for s_name in d.keys():\n            s_names.add(s_name)\n\n    # Make a beeswarm plot if we have lots of samples\n    if len(s_names) >= config.max_table_rows and pconfig.get('no_beeswarm') is not True:\n        logger.debug('Plotting beeswarm instead of table, {} samples'.format(len(s_names)))\n        warning = '<p class=\"text-muted\"><span class=\"glyphicon glyphicon-exclamation-sign\" ' \\\n            'title=\"A beeswarm plot has been generated instead because of the large number of samples. '\\\n            'See http://multiqc.info/docs/#tables--beeswarm-plots\"'\\\n            ' data-toggle=\"tooltip\"></span> Showing {} samples.</p>'.format(len(s_names))\n        return warning + beeswarm.make_plot( dt )\n    else:\n        return make_table ( dt )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_table (dt):\n\n    table_id = dt.pconfig.get('id', 'table_{}'.format(''.join(random.sample(letters, 4))) )\n    table_id = report.save_htmlid(table_id)\n    t_headers = OrderedDict()\n    t_modal_headers = OrderedDict()\n    t_rows = OrderedDict()\n    dt.raw_vals = defaultdict(lambda: dict())\n    empty_cells = dict()\n    hidden_cols = 1\n    table_title = dt.pconfig.get('table_title')\n    if table_title is None:\n        table_title = table_id.replace(\"_\", \" \").title()\n\n    for idx, k, header in dt.get_headers_in_order():\n\n        rid = header['rid']\n\n        # Build the table header cell\n        shared_key = ''\n        if header.get('shared_key', None) is not None:\n            shared_key = ' data-shared-key={}'.format(header['shared_key'])\n\n        hide = ''\n        muted = ''\n        checked = ' checked=\"checked\"'\n        if header.get('hidden', False) is True:\n            hide = 'hidden'\n            muted = ' text-muted'\n            checked = ''\n            hidden_cols += 1\n\n        data_attr = 'data-dmax=\"{}\" data-dmin=\"{}\" data-namespace=\"{}\" {}' \\\n            .format(header['dmax'], header['dmin'], header['namespace'], shared_key)\n\n        cell_contents = '<span class=\"mqc_table_tooltip\" title=\"{}: {}\">{}</span>' \\\n            .format(header['namespace'], header['description'], header['title'])\n\n        t_headers[rid] = '<th id=\"header_{rid}\" class=\"{rid} {h}\" {da}>{c}</th>' \\\n            .format(rid=rid, h=hide, da=data_attr, c=cell_contents)\n\n        empty_cells[rid] = '<td class=\"data-coloured {rid} {h}\"></td>'.format(rid=rid, h=hide)\n\n        # Build the modal table row\n        t_modal_headers[rid] = \"\"\"\n        <tr class=\"{rid}{muted}\" style=\"background-color: rgba({col}, 0.15);\">\n          <td class=\"sorthandle ui-sortable-handle\">||</span></td>\n          <td style=\"text-align:center;\">\n            <input class=\"mqc_table_col_visible\" type=\"checkbox\" {checked} value=\"{rid}\" data-target=\"#{tid}\">\n          </td>\n          <td>{name}</td>\n          <td>{title}</td>\n          <td>{desc}</td>\n          <td>{col_id}</td>\n          <td>{sk}</td>\n        </tr>\"\"\".format(\n                rid = rid,\n                muted = muted,\n                checked = checked,\n                tid = table_id,\n                col = header['colour'],\n                name = header['namespace'],\n                title = header['title'],\n                desc = header['description'],\n                col_id = '<code>{}</code>'.format(k),\n                sk = header.get('shared_key', '')\n            )\n\n        # Make a colour scale\n        if header['scale'] == False:\n            c_scale = None\n        else:\n            c_scale = mqc_colour.mqc_colour_scale(header['scale'], header['dmin'], header['dmax'])\n\n        # Add the data table cells\n        for (s_name, samp) in dt.data[idx].items():\n            if k in samp:\n                val = samp[k]\n                kname = '{}_{}'.format(header['namespace'], rid)\n                dt.raw_vals[s_name][kname] = val\n\n                if 'modify' in header and callable(header['modify']):\n                    val = header['modify'](val)\n\n                try:\n                    dmin = header['dmin']\n                    dmax = header['dmax']\n                    percentage = ((float(val) - dmin) / (dmax - dmin)) * 100\n                    percentage = min(percentage, 100)\n                    percentage = max(percentage, 0)\n                except (ZeroDivisionError,ValueError):\n                    percentage = 0\n\n                try:\n                    valstring = str(header['format'].format(val))\n                except ValueError:\n                    try:\n                        valstring = str(header['format'].format(float(val)))\n                    except ValueError:\n                        valstring = str(val)\n                except:\n                    valstring = str(val)\n\n                # This is horrible, but Python locale settings are worse\n                if config.thousandsSep_format is None:\n                    config.thousandsSep_format = '<span class=\"mqc_thousandSep\"></span>'\n                if config.decimalPoint_format is None:\n                    config.decimalPoint_format = '.'\n                valstring = valstring.replace('.', 'DECIMAL').replace(',', 'THOUSAND')\n                valstring = valstring.replace('DECIMAL', config.decimalPoint_format).replace('THOUSAND', config.thousandsSep_format)\n\n                # Percentage suffixes etc\n                valstring += header.get('suffix', '')\n\n                # Conditional formatting\n                cmatches = { cfck: False for cfc in config.table_cond_formatting_colours for cfck in cfc }\n                # Find general rules followed by column-specific rules\n                for cfk in ['all_columns', rid]:\n                    if cfk in config.table_cond_formatting_rules:\n                        # Loop through match types\n                        for ftype in cmatches.keys():\n                            # Loop through array of comparison types\n                            for cmp in config.table_cond_formatting_rules[cfk].get(ftype, []):\n                                try:\n                                    # Each comparison should be a dict with single key: val\n                                    if 's_eq' in cmp and str(cmp['s_eq']).lower() == str(val).lower():\n                                        cmatches[ftype] = True\n                                    if 's_contains' in cmp and str(cmp['s_contains']).lower() in str(val).lower():\n                                        cmatches[ftype] = True\n                                    if 's_ne' in cmp and str(cmp['s_ne']).lower() != str(val).lower():\n                                        cmatches[ftype] = True\n                                    if 'eq' in cmp and float(cmp['eq']) == float(val):\n                                        cmatches[ftype] = True\n                                    if 'ne' in cmp and float(cmp['ne']) != float(val):\n                                        cmatches[ftype] = True\n                                    if 'gt' in cmp and float(cmp['gt']) < float(val):\n                                        cmatches[ftype] = True\n                                    if 'lt' in cmp and float(cmp['lt']) > float(val):\n                                        cmatches[ftype] = True\n                                except:\n                                    logger.warn(\"Not able to apply table conditional formatting to '{}' ({})\".format(val, cmp))\n                # Apply HTML in order of config keys\n                bgcol = None\n                for cfc in config.table_cond_formatting_colours:\n                    for cfck in cfc: # should always be one, but you never know\n                        if cmatches[cfck]:\n                            bgcol = cfc[cfck]\n                if bgcol is not None:\n                    valstring = '<span class=\"badge\" style=\"background-color:{}\">{}</span>'.format(bgcol, valstring)\n\n                # Build HTML\n                if not header['scale']:\n                    if s_name not in t_rows:\n                        t_rows[s_name] = dict()\n                    t_rows[s_name][rid] = '<td class=\"{rid} {h}\">{v}</td>'.format(rid=rid, h=hide, v=valstring)\n                else:\n                    if c_scale is not None:\n                        col = ' background-color:{};'.format(c_scale.get_colour(val))\n                    else:\n                        col = ''\n                    bar_html = '<span class=\"bar\" style=\"width:{}%;{}\"></span>'.format(percentage, col)\n                    val_html = '<span class=\"val\">{}</span>'.format(valstring)\n                    wrapper_html = '<div class=\"wrapper\">{}{}</div>'.format(bar_html, val_html)\n\n                    if s_name not in t_rows:\n                        t_rows[s_name] = dict()\n                    t_rows[s_name][rid] = '<td class=\"data-coloured {rid} {h}\">{c}</td>'.format(rid=rid, h=hide, c=wrapper_html)\n\n        # Remove header if we don't have any filled cells for it\n        if sum([len(rows) for rows in t_rows.values()]) == 0:\n            t_headers.pop(rid, None)\n            t_modal_headers.pop(rid, None)\n            logger.debug('Removing header {} from general stats table, as no data'.format(k))\n\n    #\n    # Put everything together\n    #\n\n    # Buttons above the table\n    html = ''\n    if not config.simple_output:\n\n        # Copy Table Button\n        html += \"\"\"\n        <button type=\"button\" class=\"mqc_table_copy_btn btn btn-default btn-sm\" data-clipboard-target=\"#{tid}\">\n            <span class=\"glyphicon glyphicon-copy\"></span> Copy table\n        </button>\n        \"\"\".format(tid=table_id)\n\n        # Configure Columns Button\n        if len(t_headers) > 1:\n            html += \"\"\"\n            <button type=\"button\" class=\"mqc_table_configModal_btn btn btn-default btn-sm\" data-toggle=\"modal\" data-target=\"#{tid}_configModal\">\n                <span class=\"glyphicon glyphicon-th\"></span> Configure Columns\n            </button>\n            \"\"\".format(tid=table_id)\n\n        # Sort By Highlight button\n        html += \"\"\"\n        <button type=\"button\" class=\"mqc_table_sortHighlight btn btn-default btn-sm\" data-target=\"#{tid}\" data-direction=\"desc\" style=\"display:none;\">\n            <span class=\"glyphicon glyphicon-sort-by-attributes-alt\"></span> Sort by highlight\n        </button>\n        \"\"\".format(tid=table_id)\n\n        # Scatter Plot Button\n        if len(t_headers) > 1:\n            html += \"\"\"\n            <button type=\"button\" class=\"mqc_table_makeScatter btn btn-default btn-sm\" data-toggle=\"modal\" data-target=\"#tableScatterModal\" data-table=\"#{tid}\">\n                <span class=\"glyphicon glyphicon glyphicon-stats\"></span> Plot\n            </button>\n            \"\"\".format(tid=table_id)\n\n        # \"Showing x of y columns\" text\n        html += \"\"\"\n        <small id=\"{tid}_numrows_text\" class=\"mqc_table_numrows_text\">Showing <sup id=\"{tid}_numrows\" class=\"mqc_table_numrows\">{nrows}</sup>/<sub>{nrows}</sub> rows and <sup id=\"{tid}_numcols\" class=\"mqc_table_numcols\">{ncols_vis}</sup>/<sub>{ncols}</sub> columns.</small>\n        \"\"\".format(tid=table_id, nrows=len(t_rows), ncols_vis = (len(t_headers)+1)-hidden_cols, ncols=len(t_headers))\n\n    # Build the table itself\n    collapse_class = 'mqc-table-collapse' if len(t_rows) > 10 and config.collapse_tables else ''\n    html += \"\"\"\n        <div id=\"{tid}_container\" class=\"mqc_table_container\">\n            <div class=\"table-responsive mqc-table-responsive {cc}\">\n                <table id=\"{tid}\" class=\"table table-condensed mqc_table\" data-title=\"{title}\">\n        \"\"\".format( tid=table_id, title=table_title, cc=collapse_class)\n\n    # Build the header row\n    col1_header = dt.pconfig.get('col1_header', 'Sample Name')\n    html += '<thead><tr><th class=\"rowheader\">{}</th>{}</tr></thead>'.format(col1_header, ''.join(t_headers.values()))\n\n    # Build the table body\n    html += '<tbody>'\n    t_row_keys = t_rows.keys()\n    if dt.pconfig.get('sortRows') is not False:\n        t_row_keys = sorted(t_row_keys)\n    for s_name in t_row_keys:\n        html += '<tr>'\n        # Sample name row header\n        html += '<th class=\"rowheader\" data-original-sn=\"{sn}\">{sn}</th>'.format(sn=s_name)\n        for k in t_headers:\n            html += t_rows[s_name].get(k, empty_cells[k])\n        html += '</tr>'\n    html += '</tbody></table></div>'\n    if len(t_rows) > 10 and config.collapse_tables:\n        html += '<div class=\"mqc-table-expand\"><span class=\"glyphicon glyphicon-chevron-down\" aria-hidden=\"true\"></span></div>'\n    html += '</div>'\n\n    # Build the bootstrap modal to customise columns and order\n    if not config.simple_output:\n        html += \"\"\"\n    <!-- MultiQC Table Columns Modal -->\n    <div class=\"modal fade\" id=\"{tid}_configModal\" tabindex=\"-1\">\n      <div class=\"modal-dialog modal-lg\">\n        <div class=\"modal-content\">\n          <div class=\"modal-header\">\n            <button type=\"button\" class=\"close\" data-dismiss=\"modal\" aria-label=\"Close\"><span aria-hidden=\"true\">&times;</span></button>\n            <h4 class=\"modal-title\">{title}: Columns</h4>\n          </div>\n          <div class=\"modal-body\">\n            <p>Uncheck the tick box to hide columns. Click and drag the handle on the left to change order.</p>\n            <p>\n                <button class=\"btn btn-default btn-sm mqc_configModal_bulkVisible\" data-target=\"#{tid}\" data-action=\"showAll\">Show All</button>\n                <button class=\"btn btn-default btn-sm mqc_configModal_bulkVisible\" data-target=\"#{tid}\" data-action=\"showNone\">Show None</button>\n            </p>\n            <table class=\"table mqc_table mqc_sortable mqc_configModal_table\" id=\"{tid}_configModal_table\" data-title=\"{title}\">\n              <thead>\n                <tr>\n                  <th class=\"sorthandle\" style=\"text-align:center;\">Sort</th>\n                  <th style=\"text-align:center;\">Visible</th>\n                  <th>Group</th>\n                  <th>Column</th>\n                  <th>Description</th>\n                  <th>ID</th>\n                  <th>Scale</th>\n                </tr>\n              </thead>\n              <tbody>\n                {trows}\n              </tbody>\n            </table>\n        </div>\n        <div class=\"modal-footer\"> <button type=\"button\" class=\"btn btn-default\" data-dismiss=\"modal\">Close</button> </div>\n    </div> </div> </div>\"\"\".format( tid=table_id, title=table_title, trows=''.join(t_modal_headers.values()) )\n\n    # Save the raw values to a file if requested\n    if dt.pconfig.get('save_file') is True:\n        fn = dt.pconfig.get('raw_data_fn', 'multiqc_{}'.format(table_id) )\n        util_functions.write_data_file(dt.raw_vals, fn )\n        report.saved_raw_data[fn] = dt.raw_vals\n\n    return html", "response": "Builds the HTML needed for a MultiQC table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_metrics(self, f):\n        headers = None\n        for l in f['f'].splitlines():\n            s = l.strip().split(\"\\t\")\n            if headers is None:\n                headers = s\n            else:\n                s_name = s[ headers.index('Sample') ]\n                data = dict()\n                for idx, h in enumerate(headers):\n                    try:\n                        data[h] = float(s[idx])\n                    except ValueError:\n                        data[h] = s[idx]\n                self.rna_seqc_metrics[s_name] = data", "response": "Parse the metrics. tsv file from RNA - SeQC\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd alignment rate to the general stats table at the current level", "response": "def rnaseqc_general_stats (self):\n        \"\"\"\n        Add alignment rate to the general stats table\n        \"\"\"\n        headers = OrderedDict()\n        headers['Expression Profiling Efficiency'] = {\n            'title': '% Expression Efficiency',\n            'description': 'Expression Profiling Efficiency: Ratio of exon reads to total reads',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'YlGn',\n            'modify': lambda x: float(x) * 100.0\n        }\n        headers['Genes Detected'] = {\n            'title': '# Genes',\n            'description': 'Number of genes detected with at least 5 reads.',\n            'min': 0,\n            'scale': 'Bu',\n            'format': '{:,.0f}'\n        }\n        headers['rRNA rate'] = {\n            'title': '% rRNA Alignment',\n            'description': ' rRNA reads (non-duplicate and duplicate reads) per total reads',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'Reds',\n            'modify': lambda x: float(x) * 100.0\n        }\n\n        self.general_stats_addcols(self.rna_seqc_metrics, headers)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef transcript_associated_plot (self):\n\n        # Plot bar graph of groups\n        keys = OrderedDict()\n        keys['Exonic Rate'] = { 'name': 'Exonic', 'color': '#2f7ed8' }\n        keys['Intronic Rate'] = { 'name': 'Intronic', 'color': '#8bbc21' }\n        keys['Intergenic Rate'] = { 'name': 'Intergenic', 'color': '#0d233a'}\n\n        # Config for the plot\n        pconfig = {\n            'id': 'rna_seqc_position_plot',\n            'title': 'RNA-SeQC: Transcript-associated reads',\n            'ylab': 'Ratio of Reads',\n            'cpswitch': False,\n            'ymax': 1,\n            'ymin': 0,\n            'tt_decimals': 3,\n            'cpswitch_c_active': False\n        }\n        self.add_section (\n            name = 'Transcript-associated reads',\n            anchor = 'Transcript_associated',\n            helptext = 'All of the above rates are per mapped read. Exonic Rate is the fraction mapping within exons. '\n                       'Intronic Rate is the fraction mapping within introns. '\n                       'Intergenic Rate is the fraction mapping in the genomic space between genes. ',\n            plot = bargraph.plot(self.rna_seqc_metrics, keys, pconfig)\n        )", "response": "Make a bar plot showing the transcript - associated reads"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef strand_barplot(self):\n        # Plot bar graph of groups\n        keys = [ 'End 1 Sense', 'End 1 Antisense', 'End 2 Sense', 'End 2 Antisense' ]\n        # Config for the plot\n        pconfig = {\n            'id': 'rna_seqc_strandedness_plot',\n            'title': 'RNA-SeQC: Strand Specificity',\n            'ylab': '% Reads',\n            'cpswitch_counts_label': '# Reads',\n            'cpswitch_percent_label': '% Reads',\n            'ymin': 0,\n            'cpswitch_c_active': False\n        }\n        self.add_section (\n            name = 'Strand Specificity',\n            anchor = 'rna_seqc_strand_specificity',\n            helptext = 'End 1/2 Sense are the number of End 1 or 2 reads that were sequenced in the sense direction. '\n                       'Similarly, End 1/2 Antisense are the number of End 1 or 2 reads that were sequenced in the '\n                       'antisense direction',\n            plot = bargraph.plot(self.rna_seqc_metrics, keys, pconfig)\n        )", "response": "Plot a bar graph showing the strandedness of alignments of the sequenced reads in the sense direction and the antisense direction."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_coverage (self, f):\n        data = dict()\n        s_names = None\n        j = 1\n        for l in f['f'].splitlines():\n            s = l.strip().split(\"\\t\")\n            if s_names is None:\n                s_names = s\n                for s_name in s_names:\n                    data[s_name] = dict()\n            else:\n                for i, v in enumerate(s):\n                    data[s_names[i]][j] = float(v)\n                j += 1\n        if f['fn'] == 'meanCoverageNorm_high.txt':\n            self.rna_seqc_norm_high_cov.update(data)\n        elif f['fn'] == 'meanCoverageNorm_medium.txt':\n            self.rna_seqc_norm_medium_cov.update(data)\n        elif f['fn'] == 'meanCoverageNorm_low.txt':\n            self.rna_seqc_norm_low_cov.update(data)", "response": "Parse the RNA - SeQC Normalised Coverage Files"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef coverage_lineplot (self):\n        # Add line graph to section\n        data = list()\n        data_labels = list()\n        if len(self.rna_seqc_norm_high_cov) > 0:\n            data.append(self.rna_seqc_norm_high_cov)\n            data_labels.append({'name': 'High Expressed'})\n        if len(self.rna_seqc_norm_medium_cov) > 0:\n            data.append(self.rna_seqc_norm_medium_cov)\n            data_labels.append({'name': 'Medium Expressed'})\n        if len(self.rna_seqc_norm_low_cov) > 0:\n            data.append(self.rna_seqc_norm_low_cov)\n            data_labels.append({'name': 'Low Expressed'})\n        pconfig = {\n            'id': 'rna_seqc_mean_coverage_plot',\n            'title': 'RNA-SeQC: Gene Body Coverage',\n            'ylab': '% Coverage',\n            'xlab': \"Gene Body Percentile (5' -> 3')\",\n            'xmin': 0,\n            'xmax': 100,\n            'tt_label': \"<strong>{point.x}% from 5'</strong>: {point.y:.2f}\",\n            'data_labels': data_labels\n        }\n        if len(data) > 0:\n            self.add_section (\n                name = 'Gene Body Coverage',\n                anchor = 'rseqc-rna_seqc_mean_coverage',\n                helptext = 'The metrics are calculated across the transcripts with tiered expression levels.',\n                plot = linegraph.plot(data, pconfig)\n            )", "response": "Make HTML for coverage line plots"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_correlation(self, f):\n        s_names = None\n        data = list()\n        for l in f['f'].splitlines():\n            s = l.strip().split(\"\\t\")\n            if s_names is None:\n                s_names = [ x for x in s if x != '' ]\n            else:\n                data.append(s[1:])\n        if f['fn'] == 'corrMatrixPearson.txt':\n            self.rna_seqc_pearson = (s_names, data)\n        elif f['fn'] == 'corrMatrixSpearman.txt':\n            self.rna_seqc_spearman = (s_names, data)", "response": "Parse the RNA - SeQC correlation matrices from a. txt file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_correlation_heatmap(self):\n        data = None\n        corr_type = None\n        correlation_type = getattr(config, 'rna_seqc' ,{}).get('default_correlation', 'spearman')\n        if self.rna_seqc_spearman is not None and correlation_type != 'pearson':\n            data = self.rna_seqc_spearman\n            corr_type = 'Spearman'\n        elif self.rna_seqc_pearson is not None:\n            data = self.rna_seqc_pearson\n            corr_type = 'Pearson'\n        if data is not None:\n            pconfig = {\n                'id': 'rna_seqc_correlation_heatmap',\n                'title': 'RNA-SeQC: {} Sample Correlation'.format(corr_type)\n            }\n            self.add_section (\n                name = '{} Correlation'.format(corr_type),\n                anchor = 'rseqc-rna_seqc_correlation',\n                plot = heatmap.plot(data[1], data[0], data[0], pconfig)\n            )", "response": "Return HTML for correlation heatmap"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_theta2_report (self, fh):\n        parsed_data = {}\n        for l in fh:\n            if l.startswith('#'):\n                continue\n            else:\n                s = l.split(\"\\t\")\n                purities = s[1].split(',')\n                parsed_data['proportion_germline'] = float(purities[0]) * 100.0\n                for i, v in enumerate(purities[1:]):\n                    if i <= 5:\n                        parsed_data['proportion_tumour_{}'.format(i+1)] = float(v) * 100.0\n                    else:\n                        parsed_data['proportion_tumour_gt5'] = (float(v) * 100.0) + parsed_data.get('proportion_tumour_gt5', 0)\n                break\n        return parsed_data", "response": "Parse the THetA2 log file and return a dictionary of the data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef theta2_purities_chart (self):\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['proportion_germline'] =   { 'name': 'Germline' }\n        keys['proportion_tumour_1'] =   { 'name': 'Tumour Subclone 1' }\n        keys['proportion_tumour_2'] =   { 'name': 'Tumour Subclone 2' }\n        keys['proportion_tumour_3'] =   { 'name': 'Tumour Subclone 3' }\n        keys['proportion_tumour_4'] =   { 'name': 'Tumour Subclone 4' }\n        keys['proportion_tumour_5'] =   { 'name': 'Tumour Subclone 5' }\n        keys['proportion_tumour_gt5'] = { 'name': 'Tumour Subclones > 5' }\n\n        # Config for the plot\n        pconfig = {\n            'id': 'theta2_purity_plot',\n            'title': 'THetA2: Tumour Subclone Purities',\n            'cpswitch': False,\n            'ymin': 0,\n            'ymax': 100,\n            'ylab': '% Purity',\n            'tt_suffix': '%'\n        }\n\n        return bargraph.plot(self.theta2_data, keys, pconfig)", "response": "Make the plot showing the alignment rates of the Tumour Subclone 2 and the Tumour Subclone 3 and the Tumour Subclone 4 and the Tumour Subclone 5."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a single GATK report.", "response": "def parse_single_report(f):\n    \"\"\" Parse a gatk varianteval varianteval \"\"\"\n    # Fixme: Separate GATKReport parsing and data subsetting. A GATKReport parser now available from the GATK MultiqcModel.\n\n    data = dict()\n    in_CompOverlap = False\n    in_CountVariants = False\n    in_TiTv = False\n    for l in f:\n        # Detect section headers\n        if '#:GATKTable:CompOverlap' in l:\n            in_CompOverlap = True\n        elif '#:GATKTable:CountVariants' in l:\n            in_CountVariants = True\n        elif '#:GATKTable:TiTvVariantEvaluator' in l:\n            in_TiTv = True\n        else:\n            # Parse contents using nested loops\n            if in_CompOverlap:\n                headers = l.split()\n                while in_CompOverlap:\n                    l = f.readline().strip(\"\\n\")\n                    d = dict()\n                    try:\n                        for i, s in enumerate(l.split()):\n                            d[headers[i]] = s\n                        if d['Novelty'] == 'all':\n                            data['reference'] = d['CompRod']\n                            data['comp_rate'] = float(d['compRate'])\n                            data['concordant_rate'] = float(d['concordantRate'])\n                            data['eval_variants'] = int(d['nEvalVariants'])\n                            data['novel_sites'] = int(d['novelSites'])\n                        elif d['Novelty'] == 'known':\n                            data['known_sites'] = int(d['nEvalVariants'])\n                    except KeyError:\n                        in_CompOverlap = False\n            elif in_CountVariants:\n                headers = l.split()\n                while in_CountVariants:\n                    l = f.readline().strip(\"\\n\")\n                    d = dict()\n                    try:\n                        for i, s in enumerate(l.split()):\n                            d[headers[i]] = s\n                        if d['Novelty'] == 'all':\n                            data['snps'] = int(d['nSNPs'])\n                            data['mnps'] = int(d['nMNPs'])\n                            data['insertions'] = int(d['nInsertions'])\n                            data['deletions'] = int(d['nDeletions'])\n                            data['complex'] = int(d['nComplex'])\n                            data['symbolic'] = int(d['nSymbolic'])\n                            data['mixed'] = int(d['nMixed'])\n                            data['nocalls'] = int(d['nNoCalls'])\n                    except KeyError:\n                        in_CountVariants = False\n            elif in_TiTv:\n                headers = l.split()\n                data['titv_reference'] = 'unknown'\n                while in_TiTv:\n                    l = f.readline().strip(\"\\n\")\n                    d = dict()\n                    try:\n                        for i, s in enumerate(l.split()):\n                            d[headers[i]] = s\n                        if d['Novelty'] == 'known':\n                            data['titv_reference'] = d['CompRod']\n                            data['known_titv'] = float(d['tiTvRatio'])\n                        elif d['Novelty'] == 'novel':\n                            data['novel_titv'] = float(d['tiTvRatio'])\n                    except KeyError:\n                        in_TiTv = False\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef count_variants_barplot(data):\n    keys = OrderedDict()\n    keys['snps'] = {'name': 'SNPs'}\n    keys['mnps'] = {'name': 'MNPs'}\n    keys['insertions'] = {'name': 'Insertions'}\n    keys['deletions'] = {'name': 'Deletions'}\n    keys['complex'] = {'name': 'Complex'}\n    keys['symbolic'] = {'name': 'Symbolic'}\n    keys['mixed'] = {'name': 'Mixed'}\n    keys['nocalls'] = {'name': 'No-calls'}\n\n    plot_conf = {\n        'id': 'gatk_varianteval_variant_plot',\n        'title': 'GATK VariantEval: Variant Counts',\n        'ylab': '# Variants',\n        'cpswitch_counts_label': 'Number of Variants'\n    }\n    return bargraph.plot(data, keys, plot_conf)", "response": "Generate the HTML for the Variant Counts barplot"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a table from the comp_overlap output.", "response": "def comp_overlap_table(data):\n    \"\"\"Build a table from the comp overlaps output.\"\"\"\n    headers = OrderedDict()\n    headers['comp_rate'] = {\n        'title': 'Compare rate',\n        'description': 'Ratio of known variants found in the reference set.',\n        'namespace': 'GATK',\n        'min': 0,\n        'max': 100,\n        'suffix': '%',\n        'format': '{:,.2f}',\n        'scale': 'Blues',\n    }\n    headers['concordant_rate'] = {\n        'title': 'Concordant rate',\n        'description': 'Ratio of variants matching alleles in the reference set.',\n        'namespace': 'GATK',\n        'min': 0,\n        'max': 100,\n        'suffix': '%',\n        'format': '{:,.2f}',\n        'scale': 'Blues',\n    }\n    headers['eval_variants'] = {\n        'title': 'M Evaluated variants',\n        'description': 'Number of called variants (millions)',\n        'namespace': 'GATK',\n        'min': 0,\n        'modify': lambda x: float(x) / 1000000.0\n    }\n    headers['known_sites'] = {\n        'title': 'M Known sites',\n        'description': 'Number of known variants (millions)',\n        'namespace': 'GATK',\n        'min': 0,\n        'modify': lambda x: float(x) / 1000000.0\n    }\n    headers['novel_sites'] = {\n        'title': 'M Novel sites',\n        'description': 'Number of novel variants (millions)',\n        'namespace': 'GATK',\n        'min': 0,\n        'modify': lambda x: float(x) / 1000000.0\n    }\n    table_html = table.plot(data, headers, {'id': 'gatk_compare_overlap', 'table_title': 'GATK - Compare Overlap'})\n    return table_html"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_gatk_varianteval(self):\n\n        self.gatk_varianteval = dict()\n        for f in self.find_log_files('gatk/varianteval', filehandles=True):\n            parsed_data = parse_single_report(f['f'])\n            if len(parsed_data) > 0:\n                if f['s_name'] in self.gatk_varianteval:\n                    log.debug(\"Duplicate sample name found! Overwriting: {}\".format(f['s_name']))\n                self.add_data_source(f, section='varianteval')\n                self.gatk_varianteval[f['s_name']] = parsed_data\n\n        # Filter to strip out ignored sample names\n        self.gatk_varianteval = self.ignore_samples(self.gatk_varianteval)\n\n        n_reports_found = len(self.gatk_varianteval)\n        if n_reports_found > 0:\n            log.info(\"Found {} VariantEval reports\".format(n_reports_found))\n\n            # Write parsed report data to a file (restructure first)\n            self.write_data_file(self.gatk_varianteval, 'multiqc_gatk_varianteval')\n\n            # Get consensus TiTv references\n            titv_ref = None\n            for s_name in self.gatk_varianteval:\n                if titv_ref is None:\n                    titv_ref = self.gatk_varianteval[s_name]['titv_reference']\n                elif titv_ref != self.gatk_varianteval[s_name]['titv_reference']:\n                    titv_ref = 'Multiple'\n                    break\n\n            # General Stats Table\n            varianteval_headers = dict()\n            varianteval_headers['known_titv'] = {\n                'title': 'TiTV ratio (known)',\n                'description': \"TiTV ratio from variants found in '{}'\".format(titv_ref),\n                'min': 0,\n                'scale': 'Blues',\n                'shared_key': 'titv_ratio'\n            }\n            varianteval_headers['novel_titv'] = {\n                'title': 'TiTV ratio (novel)',\n                'description': \"TiTV ratio from variants NOT found in '{}'\".format(titv_ref),\n                'min': 0,\n                'scale': 'Blues',\n                'shared_key': 'titv_ratio'\n            }\n            self.general_stats_addcols(self.gatk_varianteval, varianteval_headers, 'GATK VariantEval')\n\n            # Variant Counts plot\n            self.add_section (\n                name = 'Variant Counts',\n                anchor = 'gatk-count-variants',\n                plot = count_variants_barplot(self.gatk_varianteval)\n            )\n\n            # Compare Overlap Table\n            self.add_section (\n                name = 'Compare Overlap',\n                anchor = 'gatk-compare-overlap',\n                plot = comp_overlap_table(self.gatk_varianteval)\n            )\n\n        return n_reports_found", "response": "Find GATK varianteval logs and parse their data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the sargasso log file and store the data in self. sargasso_data.", "response": "def parse_sargasso_logs(self, f):\n        \"\"\" Parse the sargasso log file. \"\"\"\n        species_name = list()\n        items = list()\n        header = list()\n        is_first_line = True\n        for l in f['f'].splitlines():\n            s = l.split(\",\")\n            # Check that this actually is a Sargasso file\n            if is_first_line and s[0]!='Sample':\n                return None\n\n            if len(s) < 7:\n                continue\n            if is_first_line:\n                #prepare header\n                is_first_line = False\n                header = s\n                for i in header[1:]:\n                    #find out what species included\n                    sname = i.split('-')[-1]\n                    if sname not in species_name:\n                        species_name.append(sname)\n                    #find out what is being counted\n                    kname = (\"-\".join(i.split('-')[-3:-1]))\n                    if kname not in items:\n                        items.append(kname)\n            else:\n                #start sample lines.\n                sample_name = s.pop(0)\n\n                chunk_by_species = [s[i:i + len(items)] for i in range(0, len(s), len(items))];\n                for idx,v in enumerate(chunk_by_species):\n                    #adding species name to the sample name for easy interpretation\n                    new_sample_name = '_'.join([sample_name,species_name[idx]])\n\n                    # Clean up sample name\n                    new_sample_name = self.clean_s_name(new_sample_name, f['root'])\n\n                    if new_sample_name in self.sargasso_data.keys():\n                        log.debug(\"Duplicate sample name found! Overwriting: {}\".format(new_sample_name))\n\n                    try:\n                        self.sargasso_data[new_sample_name] = dict(zip(items,map(int, v)))\n                    except ValueError:\n                        pass\n\n\n\n        self.sargasso_keys = items\n\n        for idx, f_name in enumerate(self.sargasso_data.keys()):\n\n\n\n            # Reorganised parsed data for this sample\n            # Collect total READ count number\n            self.sargasso_data[f_name]['Total'] = 0;\n            for key, value in list(self.sargasso_data[f_name].items()):   # iter on both keys and values\n                if key.endswith(\"Reads\"):\n                    self.sargasso_data[f_name]['Total'] += value\n\n            # Calculate the percent aligned if we can\n            try:\n                self.sargasso_data[f_name]['sargasso_percent_assigned'] = (float(self.sargasso_data[f_name]['Assigned-Reads'])/float(self.sargasso_data[f_name]['Total'])) * 100.0\n            except (KeyError, ZeroDivisionError):\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sargasso_stats_table(self):\n\n        headers = OrderedDict()\n        headers['sargasso_percent_assigned'] = {\n            'title': '% Assigned',\n            'description': 'Sargasso % Assigned reads',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'RdYlGn'\n        }\n        headers['Assigned-Reads'] = {\n            'title': '{} Assigned'.format(config.read_count_prefix),\n            'description': 'Sargasso Assigned reads ({})'.format(config.read_count_desc),\n            'min': 0,\n            'scale': 'PuBu',\n            'modify': lambda x: float(x) * config.read_count_multiplier,\n            'shared_key': 'read_count'\n        }\n        self.general_stats_addcols(self.sargasso_data, headers)", "response": "Take the parsed stats from the sargasso report and add them to the basic stats table at the top of the report"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes the sargasso plot", "response": "def sargasso_chart (self):\n        \"\"\" Make the sargasso plot \"\"\"\n\n        # Config for the plot\n        config = {\n            'id': 'sargasso_assignment_plot',\n            'title': 'Sargasso: Assigned Reads',\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads'\n        }\n\n        #We only want to plot the READs at the moment\n        return bargraph.plot(self.sargasso_data, [name for name in self.sargasso_keys if 'Reads' in name], config)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind RSeQC gene_body_coverage reports and parse their data", "response": "def parse_reports(self):\n    \"\"\" Find RSeQC gene_body_coverage reports and parse their data \"\"\"\n\n    # Set up vars\n    self.gene_body_cov_hist_counts = dict()\n    self.gene_body_cov_hist_percent = dict()\n\n    # TODO - Do separate parsing step to find skewness values\n    # and add these to the general stats table?\n\n    # Go through files and parse data\n    for f in self.find_log_files('rseqc/gene_body_coverage'):\n\n        # RSeQC >= v2.4\n        if f['f'].startswith('Percentile'):\n            keys = []\n            nrows = 0\n            for l in f['f'].splitlines():\n                s = l.split()\n                if len(keys) == 0:\n                    keys = s[1:]\n                else:\n                    nrows += 1\n                    s_name = self.clean_s_name(s[0], f['root'])\n                    if s_name in self.gene_body_cov_hist_counts:\n                        log.debug(\"Duplicate sample name found! Overwriting: {}\".format(s_name))\n                    self.add_data_source(f, s_name, section='gene_body_coverage')\n                    self.gene_body_cov_hist_counts[s_name] = OrderedDict()\n                    for k, var in enumerate(s[1:]):\n                        self.gene_body_cov_hist_counts[s_name][int(keys[k])] = float(var)\n            if nrows == 0:\n                log.warning(\"Empty geneBodyCoverage file found: {}\".format(f['fn']))\n\n\n        # RSeQC < v2.4\n        elif f['f'].startswith('Total reads'):\n            if f['s_name'].endswith('.geneBodyCoverage'):\n                f['s_name'] = f['s_name'][:-17]\n            if f['s_name'] in self.gene_body_cov_hist_counts:\n                log.debug(\"Duplicate sample name found! Overwriting: {}\".format(f['s_name']))\n            self.add_data_source(f, section='gene_body_coverage')\n            self.gene_body_cov_hist_counts[f['s_name']] = OrderedDict()\n            nrows = 0\n            for l in f['f'].splitlines():\n                s = l.split()\n                try:\n                    nrows += 1\n                    self.gene_body_cov_hist_counts[f['s_name']][int(s[0])] = float(s[1])\n                except:\n                    pass\n            if nrows == 0:\n                del self.gene_body_cov_hist_counts[f['s_name']]\n                log.warning(\"Empty geneBodyCoverage file found: {}\".format(f['fn']))\n\n    # Filter to strip out ignored sample names\n    self.gene_body_cov_hist_counts = self.ignore_samples(self.gene_body_cov_hist_counts)\n\n    if len(self.gene_body_cov_hist_counts) > 0:\n\n        # Make a normalised percentage version of the data\n        for s_name in self.gene_body_cov_hist_counts:\n            self.gene_body_cov_hist_percent[s_name] = OrderedDict()\n            total = sum( self.gene_body_cov_hist_counts[s_name].values() )\n            for k, v in self.gene_body_cov_hist_counts[s_name].items():\n                self.gene_body_cov_hist_percent[s_name][k] = (v/total)*100\n\n        # Add line graph to section\n        pconfig = {\n            'id': 'rseqc_gene_body_coverage_plot',\n            'title': 'RSeQC: Gene Body Coverage',\n            'ylab': '% Coverage',\n            'xlab': \"Gene Body Percentile (5' -> 3')\",\n            'xmin': 0,\n            'xmax': 100,\n            'tt_label': \"<strong>{point.x}% from 5'</strong>: {point.y:.2f}\",\n            'data_labels': [\n                {'name': 'Percentages', 'ylab': 'Percentage Coverage'},\n                {'name': 'Counts', 'ylab': 'Coverage'}\n            ]\n        }\n        self.add_section (\n            name = 'Gene Body Coverage',\n            anchor = 'rseqc-gene_body_coverage',\n            description = '<a href=\"http://rseqc.sourceforge.net/#genebody-coverage-py\" target=\"_blank\">Gene Body Coverage</a>' \\\n                \" calculates read coverage over gene bodies.\" \\\n                \" This is used to check if reads coverage is uniform and\" \\\n                \" if there is any 5' or 3' bias.\",\n            plot = linegraph.plot([self.gene_body_cov_hist_percent, self.gene_body_cov_hist_counts], pconfig)\n        )\n\n    # Return number of samples found\n    return len(self.gene_body_cov_hist_counts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds bamtools stats reports and parse their data", "response": "def parse_reports(self):\n    \"\"\" Find bamtools stats reports and parse their data \"\"\"\n\n    # Set up vars\n    self.bamtools_stats_data = dict()\n    regexes = {\n        'total_reads': r\"Total reads:\\s*(\\d+)\",\n        'mapped_reads': r\"Mapped reads:\\s*(\\d+)\",\n        'mapped_reads_pct': r\"Mapped reads:\\s*\\d+\\s+\\(([\\d\\.]+)%\\)\",\n        'forward_strand': r\"Forward strand:\\s*(\\d+)\",\n        'forward_strand_pct': r\"Forward strand:\\s*\\d+\\s+\\(([\\d\\.]+)%\\)\",\n        'reverse_strand': r\"Reverse strand:\\s*(\\d+)\",\n        'reverse_strand_pct': r\"Reverse strand:\\s*\\d+\\s+\\(([\\d\\.]+)%\\)\",\n        'failed_qc': r\"Failed QC:\\s*(\\d+)\",\n        'failed_qc_pct': r\"Failed QC:\\s*\\d+\\s+\\(([\\d\\.]+)%\\)\",\n        'duplicates': r\"Duplicates:\\s*(\\d+)\",\n        'duplicates_pct': r\"Duplicates:\\s*\\d+\\s+\\(([\\d\\.]+)%\\)\",\n        'paired_end': r\"Paired-end reads:\\s*(\\d+)\",\n        'paired_end_pct': r\"Paired-end reads:\\s*\\d+\\s+\\(([\\d\\.]+)%\\)\",\n        'proper_pairs': r\"'Proper-pairs'\\s*(\\d+)\",\n        'proper_pairs_pct': r\"'Proper-pairs'\\s*\\d+\\s+\\(([\\d\\.]+)%\\)\",\n        'both_mapped': r\"Both pairs mapped:\\s*(\\d+)\",\n        'both_mapped_pct': r\"Both pairs mapped:\\s*\\d+\\s+\\(([\\d\\.]+)%\\)\",\n        'read_1': r\"Read 1:\\s*(\\d+)\",\n        'read_2': r\"Read 2:\\s*(\\d+)\",\n        'singletons': r\"Singletons:\\s*(\\d+)\",\n        'singletons_pct': r\"Singletons:\\s*\\d+\\s+\\(([\\d\\.]+)%\\)\",\n    }\n\n    # Go through files and parse data using regexes\n    for f in self.find_log_files('bamtools/stats'):\n        d = dict()\n        for k, r in regexes.items():\n            r_search = re.search(r, f['f'], re.MULTILINE)\n            if r_search:\n                d[k] = float(r_search.group(1))\n\n        if len(d) > 0:\n            if f['s_name'] in self.bamtools_stats_data:\n                log.debug(\"Duplicate sample name found! Overwriting: {}\".format(f['s_name']))\n            self.add_data_source(f, section='stats')\n            self.bamtools_stats_data[f['s_name']] = d\n\n    # Filter to strip out ignored sample names\n    self.bamtools_stats_data = self.ignore_samples(self.bamtools_stats_data)\n\n    if len(self.bamtools_stats_data) > 0:\n\n        # Write to file\n        self.write_data_file(self.bamtools_stats_data, 'multiqc_bamtools_stats')\n\n        # Add to general stats table\n        self.general_stats_headers['duplicates_pct'] = {\n            'title': '% Duplicates',\n            'description': '% Duplicate Reads',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'OrRd'\n        }\n        self.general_stats_headers['mapped_reads_pct'] = {\n            'title': '% Mapped',\n            'description': '% Mapped Reads',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'RdYlGn'\n        }\n        for s_name in self.bamtools_stats_data:\n            if s_name not in self.general_stats_data:\n                self.general_stats_data[s_name] = dict()\n            self.general_stats_data[s_name].update( self.bamtools_stats_data[s_name] )\n\n        # Make dot plot of counts\n        keys = OrderedDict()\n        defaults = {\n            'min': 0,\n            'max': 100,\n            'decimalPlaces': 2,\n            'suffix': '%'\n        }\n        num_defaults = {\n            'min': 0,\n            'modify': lambda x: float(x) / 1000000.0,\n            'decimalPlaces': 2\n        }\n\n        keys['total_reads'] = dict(num_defaults, **{'title': 'Total reads', 'description': 'Total reads (millions)' });\n        keys['mapped_reads_pct'] = dict(defaults, **{'title': 'Mapped reads' })\n        keys['forward_strand_pct'] = dict(defaults, **{'title': 'Forward strand' })\n        keys['reverse_strand_pct'] = dict(defaults, **{'title': 'Reverse strand' })\n        keys['failed_qc_pct'] = dict(defaults, **{'title': 'Failed QC' })\n        keys['duplicates_pct'] = dict(defaults, **{'title': 'Duplicates' })\n        keys['paired_end_pct'] = dict(defaults, **{'title': 'Paired-end', 'description': 'Paired-end reads' })\n        keys['proper_pairs_pct'] = dict(defaults, **{'title': 'Proper-pairs' })\n        keys['both_mapped_pct'] = dict(defaults, **{'title': 'Both mapped', 'description': 'Both pairs mapped' })\n        keys['bt_read_1'] = dict(num_defaults, **{'title': 'Read 1', 'description': 'Read 1 (millions)' });\n        keys['bt_read_2'] = dict(num_defaults, **{'title': 'Read 2', 'description': 'Read 2 (millions)' });\n        keys['singletons_pct'] = dict(defaults, **{'title': 'Singletons' })\n\n        self.add_section (\n            name = 'Bamtools Stats',\n            anchor = 'bamtools-stats',\n            plot = beeswarm.plot(self.bamtools_stats_data, keys)\n        )\n\n    # Return number of samples found\n    return len(self.bamtools_stats_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot (data, xcats, ycats=None, pconfig=None):\n\n    if pconfig is None:\n        pconfig = {}\n\n    # Allow user to overwrite any given config for this plot\n    if 'id' in pconfig and pconfig['id'] and pconfig['id'] in config.custom_plot_config:\n        for k, v in config.custom_plot_config[pconfig['id']].items():\n            pconfig[k] = v\n\n    if ycats is None:\n        ycats = xcats\n\n    # Make a plot\n    return highcharts_heatmap(data, xcats, ycats, pconfig)", "response": "Plot a 2D heatmap."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild the HTML needed for a HighCharts heatmap plot. Should be called by plot_xy_data which properly formats input data.", "response": "def highcharts_heatmap (data, xcats, ycats, pconfig=None):\n    \"\"\"\n    Build the HTML needed for a HighCharts line graph. Should be\n    called by plot_xy_data, which properly formats input data.\n    \"\"\"\n    if pconfig is None:\n        pconfig = {}\n\n    # Reformat the data for highcharts\n    pdata = []\n    for i, arr in enumerate(data):\n        for j, val in enumerate(arr):\n            pdata.append([j,i,val])\n\n    # Get the plot ID\n    if pconfig.get('id') is None:\n        pconfig['id'] = 'mqc_hcplot_'+''.join(random.sample(letters, 10))\n\n    # Sanitise plot ID and check for duplicates\n    pconfig['id'] = report.save_htmlid(pconfig['id'])\n\n    # Build the HTML for the page\n    html = '<div class=\"mqc_hcplot_plotgroup\">'\n\n    # The 'sort by highlights button'\n    html += \"\"\"<div class=\"btn-group hc_switch_group\">\n        <button type=\"button\" class=\"mqc_heatmap_sortHighlight btn btn-default btn-sm\" data-target=\"#{id}\" disabled=\"disabled\">\n            <span class=\"glyphicon glyphicon-sort-by-attributes-alt\"></span> Sort by highlight\n        </button>\n    </div>\"\"\".format(id=pconfig['id'])\n\n    # The plot div\n    html += '<div class=\"hc-plot-wrapper\"><div id=\"{id}\" class=\"hc-plot not_rendered hc-heatmap\"><small>loading..</small></div></div></div> \\n'.format(id=pconfig['id'])\n\n    report.num_hc_plots += 1\n\n    report.plot_data[pconfig['id']] = {\n        'plot_type': 'heatmap',\n        'data': pdata,\n        'xcats': xcats,\n        'ycats': ycats,\n        'config': pconfig\n    }\n\n    return html"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating line graph plot of histogram data for BBMap aqhist output.", "response": "def plot_aqhist(samples, file_type, **plot_args):\n    \"\"\" Create line graph plot of histogram data for BBMap 'aqhist' output.\n\n    The 'samples' parameter could be from the bbmap mod_data dictionary:\n    samples = bbmap.MultiqcModule.mod_data[file_type]\n    \"\"\"\n\n    sumy = sum([int(samples[sample]['data'][x][0])\n                for sample in samples\n                for x in samples[sample]['data']])\n\n    cutoff = sumy * 0.999\n    all_x = set()\n    for item in sorted(chain(*[samples[sample]['data'].items()\n                                for sample in samples])):\n        all_x.add(item[0])\n        cutoff -= item[1][0]\n        if cutoff < 0:\n            xmax = item[0]\n            break\n    else:\n        xmax = max(all_x)\n\n\n    columns_to_plot = {\n        'Counts': {\n            0: 'Read1',\n            2: 'Read2'\n        },\n        'Proportions': {\n            1: 'Read1',\n            3: 'Read2'\n        }\n    }\n\n    plot_data = []\n    for column_type in columns_to_plot:\n        plot_data.append(\n            {\n                sample+'.'+column_name: {\n                    x: samples[sample]['data'][x][column] if x in samples[sample]['data'] else 0\n                    for x in all_x\n                }\n                for sample in samples\n                for column, column_name in columns_to_plot[column_type].items()\n            }\n        )\n\n    plot_params = {\n            'id': 'bbmap-' + file_type + '_plot',\n            'title': 'BBTools: ' + plot_args['plot_title'],\n            'xmax': xmax,\n            'xlab': 'Quality score',\n            'data_labels': [\n                {'name': 'Count data', 'ylab': 'Read count'},\n                {'name': 'Proportion data', 'ylab': 'Proportion of reads'},\n            ]\n\n    }\n    plot_params.update(plot_args['plot_params'])\n    plot = linegraph.plot(\n        plot_data,\n        plot_params\n    )\n\n    return plot"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_reports(self,\n    log_key='picard/markdups',\n    section_name='Mark Duplicates',\n    section_anchor='picard-markduplicates',\n    plot_title='Picard: Deduplication Stats',\n    plot_id='picard_deduplication',\n    data_filename='multiqc_picard_dups'):\n    \"\"\" Find Picard MarkDuplicates reports and parse their dataself.\n    This function is also used by the biobambam2 module, hence the parameters.\n    \"\"\"\n\n    # Set up vars\n    self.picard_dupMetrics_data = dict()\n\n    # Go through logs and find Metrics\n    for f in self.find_log_files(log_key, filehandles=True):\n        s_name = f['s_name']\n        for l in f['f']:\n            # New log starting\n            if 'markduplicates' in l.lower() and 'input' in l.lower():\n                s_name = None\n\n                # Pull sample name from input\n                fn_search = re.search(r\"INPUT(?:=|\\s+)(\\[?[^\\s]+\\]?)\", l, flags=re.IGNORECASE)\n                if fn_search:\n                    s_name = os.path.basename(fn_search.group(1).strip('[]'))\n                    s_name = self.clean_s_name(s_name, f['root'])\n\n            if s_name is not None:\n                if 'UNPAIRED_READ_DUPLICATES' in l:\n                    if s_name in self.picard_dupMetrics_data:\n                        log.debug(\"Duplicate sample name found in {}! Overwriting: {}\".format(f['fn'], s_name))\n                    self.add_data_source(f, s_name, section='DuplicationMetrics')\n                    self.picard_dupMetrics_data[s_name] = dict()\n                    keys = l.rstrip(\"\\n\").split(\"\\t\")\n                    vals = f['f'].readline().rstrip(\"\\n\").split(\"\\t\")\n                    for i, k in enumerate(keys):\n                        try:\n                            self.picard_dupMetrics_data[s_name][k] = float(vals[i])\n                        except ValueError:\n                            self.picard_dupMetrics_data[s_name][k] = vals[i]\n                    # Check that this sample had some reads\n                    if self.picard_dupMetrics_data[s_name].get('READ_PAIRS_EXAMINED', 0) == 0 and \\\n                       self.picard_dupMetrics_data[s_name].get('UNPAIRED_READS_EXAMINED', 0) == 0:\n                        self.picard_dupMetrics_data.pop(s_name, None)\n                        log.warn(\"Skipping MarkDuplicates sample '{}' as log contained no reads\".format(s_name))\n                    s_name = None\n\n        for s_name in list(self.picard_dupMetrics_data.keys()):\n            if len(self.picard_dupMetrics_data[s_name]) == 0:\n                self.picard_dupMetrics_data.pop(s_name, None)\n                log.debug(\"Removing {} as no data parsed\".format(s_name))\n\n\n    # Filter to strip out ignored sample names\n    self.picard_dupMetrics_data = self.ignore_samples(self.picard_dupMetrics_data)\n\n    if len(self.picard_dupMetrics_data) > 0:\n\n        # Write parsed data to a file\n        self.write_data_file(self.picard_dupMetrics_data, data_filename)\n\n        # Add to general stats table\n        self.general_stats_headers['PERCENT_DUPLICATION'] = {\n            'title': '% Dups',\n            'description': '{} - Percent Duplication'.format(section_name),\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'OrRd',\n            'modify': lambda x: self.multiply_hundred(x)\n        }\n        for s_name in self.picard_dupMetrics_data:\n            if s_name not in self.general_stats_data:\n                self.general_stats_data[s_name] = dict()\n            self.general_stats_data[s_name].update( self.picard_dupMetrics_data[s_name] )\n\n\n        # Make the bar plot and add to the MarkDuplicates section\n        # NOTE: I had a hard time getting these numbers to add up as expected.\n        # If you think I've done something wrong, let me know! Please add an\n        # issue here: https://github.com/ewels/MultiQC/issues\n        for sn in self.picard_dupMetrics_data.keys():\n            self.picard_dupMetrics_data[sn]['UNPAIRED_READ_UNIQUE'] = self.picard_dupMetrics_data[sn]['UNPAIRED_READS_EXAMINED'] - self.picard_dupMetrics_data[sn]['UNPAIRED_READ_DUPLICATES']\n            self.picard_dupMetrics_data[sn]['READ_PAIR_NOT_OPTICAL_DUPLICATES'] = self.picard_dupMetrics_data[sn]['READ_PAIR_DUPLICATES'] - self.picard_dupMetrics_data[sn]['READ_PAIR_OPTICAL_DUPLICATES']\n            self.picard_dupMetrics_data[sn]['READ_PAIR_UNIQUE'] = self.picard_dupMetrics_data[sn]['READ_PAIRS_EXAMINED'] - self.picard_dupMetrics_data[sn]['READ_PAIR_DUPLICATES']\n\n        keys = OrderedDict()\n        keys_r = ['READ_PAIR_UNIQUE', 'UNPAIRED_READ_UNIQUE', 'READ_PAIR_NOT_OPTICAL_DUPLICATES',\n                'READ_PAIR_OPTICAL_DUPLICATES', 'UNPAIRED_READ_DUPLICATES', 'UNMAPPED_READS']\n        for k in keys_r:\n            keys[k] = {'name': k.replace('_',' ').title()}\n\n        # Config for the plot\n        pconfig = {\n            'id': plot_id,\n            'title': plot_title,\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads',\n            'cpswitch_c_active': False\n        }\n\n        self.add_section (\n            name = section_name,\n            anchor = section_anchor,\n            plot = bargraph.plot(self.picard_dupMetrics_data, keys, pconfig)\n        )\n\n    # Return the number of detected samples to the parent module\n    return len(self.picard_dupMetrics_data)", "response": "Find Picard MarkDuplicates reports and parse their dataself."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_tstv_summary(self):\n\n        self.vcftools_tstv_summary = dict()\n        for f in self.find_log_files('vcftools/tstv_summary', filehandles=True):\n            d = {}\n            for line in f['f'].readlines()[1:]: # don't add the header line (first row)\n                key = line.split()[0] # taking the first column (MODEL) as key\n                val = int(line.split()[1]) # taking the second column (COUNT) as value\n                d[key] = val\n            self.vcftools_tstv_summary[f['s_name']] = d\n\n        # Filter out ignored sample names\n        self.vcftools_tstv_summary = self.ignore_samples(self.vcftools_tstv_summary)\n\n        if len(self.vcftools_tstv_summary) == 0:\n            return 0\n\n        # Specifying the categories of the bargraph\n        keys = OrderedDict()\n        keys = ['AC', 'AG', 'AT', 'CG', 'CT', 'GT', 'Ts', 'Tv']\n\n        pconfig = {\n            'id': 'vcftools_tstv_summary',\n            'title': 'VCFTools: TsTv Summary',\n            'ylab': 'Counts',\n        }\n\n        self.add_section(\n            name = 'TsTv Summary',\n            anchor = 'vcftools-tstv-summary',\n            description = \"Plot of `TSTV-SUMMARY` - count of different types of transition and transversion SNPs.\",\n            plot = bargraph.plot(self.vcftools_tstv_summary,keys,pconfig)\n        )\n\n        return len(self.vcftools_tstv_summary)", "response": "Create the HTML for the TsTv summary plot."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_reports(self):\n\n    # Set up vars\n    self.junction_saturation_all = dict()\n    self.junction_saturation_known = dict()\n    self.junction_saturation_novel = dict()\n\n    # Go through files and parse data\n    for f in self.find_log_files('rseqc/junction_saturation'):\n        parsed = dict()\n        for l in f['f'].splitlines():\n            r = re.search(r\"^([xyzw])=c\\(([\\d,]+)\\)$\", l)\n            if r:\n                parsed[r.group(1)] = [float(i) for i in r.group(2).split(',')]\n        if len(parsed) == 4:\n            if parsed['z'][-1] == 0:\n                log.warn(\"Junction saturation data all zeroes, skipping: '{}'\".format(f['s_name']))\n            else:\n                if f['s_name'] in self.junction_saturation_all:\n                    log.debug(\"Duplicate sample name found! Overwriting: {}\".format(f['s_name']))\n                self.add_data_source(f, section='junction_saturation')\n                self.junction_saturation_all[f['s_name']] = OrderedDict()\n                self.junction_saturation_known[f['s_name']] = OrderedDict()\n                self.junction_saturation_novel[f['s_name']] = OrderedDict()\n                for k, v in enumerate(parsed['x']):\n                    self.junction_saturation_all[f['s_name']][v] = parsed['z'][k]\n                    self.junction_saturation_known[f['s_name']][v] = parsed['y'][k]\n                    self.junction_saturation_novel[f['s_name']][v] = parsed['w'][k]\n\n    # Filter to strip out ignored sample names\n    self.junction_saturation_all = self.ignore_samples(self.junction_saturation_all)\n    self.junction_saturation_known = self.ignore_samples(self.junction_saturation_known)\n    self.junction_saturation_novel = self.ignore_samples(self.junction_saturation_novel)\n\n    if len(self.junction_saturation_all) > 0:\n\n        # Add line graph to section\n        pconfig = {\n            'id': 'rseqc_junction_saturation_plot',\n            'title': 'RSeQC: Junction Saturation',\n            'ylab': 'Number of Junctions',\n            'ymin': 0,\n            'xlab': \"Percent of reads\",\n            'xmin': 0,\n            'xmax': 100,\n            'tt_label': \"<strong>{point.x}% of reads</strong>: {point.y:.2f}\",\n            'data_labels': [\n                {'name': 'Known Junctions'},\n                {'name': 'Novel Junctions'},\n                {'name': 'All Junctions'}\n            ],\n            'cursor': 'pointer',\n            'click_func': plot_single()\n        }\n        self.add_section (\n            name = 'Junction Saturation',\n            anchor = 'rseqc-junction_saturation',\n            description = '''<a href=\"http://rseqc.sourceforge.net/#junction-saturation-py\" target=\"_blank\">Junction Saturation</a>\n                counts the number of known splicing junctions that are observed\n                in each dataset. If sequencing depth is sufficient, all (annotated) splice junctions should\n                be rediscovered, resulting in a curve that reaches a plateau. Missing low abundance splice\n                junctions can affect downstream analysis.</p>\n                <div class=\"alert alert-info\" id=\"rseqc-junction_sat_single_hint\">\n                  <span class=\"glyphicon glyphicon-hand-up\"></span>\n                  Click a line to see the data side by side (as in the original RSeQC plot).\n                </div><p>''',\n            plot = linegraph.plot([\n                    self.junction_saturation_known,\n                    self.junction_saturation_novel,\n                    self.junction_saturation_all\n                ], pconfig)\n        )\n\n    # Return number of samples found\n    return len(self.junction_saturation_all)", "response": "Find RSeQC junction_saturation frequency reports and parse their data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_homer_findpeaks(self):\n\n        self.homer_findpeaks = dict()\n        for f in self.find_log_files('homer/findpeaks', filehandles=True):\n            self.parse_findPeaks(f)\n\n        # Filter to strip out ignored sample names\n        self.homer_findpeaks = self.ignore_samples(self.homer_findpeaks)\n\n        if len(self.homer_findpeaks) > 0:\n\n            # Write parsed report data to a file\n            self.write_data_file(self.homer_findpeaks, 'multiqc_homer_findpeaks')\n\n            # General Stats Table\n            stats_headers = OrderedDict()\n            stats_headers['approximate_ip_efficiency'] = {\n                'title': '% Efficiency',\n                'description': 'Approximate IP efficiency',\n                'min': 0,\n                'max': 100,\n                'suffix': '%',\n                'scale': 'RdYlGn'\n            }\n            stats_headers['total_peaks'] = {\n                'title': 'Total Peaks',\n                'min': 0,\n                'format': '{:,.0f}',\n                'scale': 'GnBu'\n            }\n            stats_headers['expected_tags_per_peak'] = {\n                'title': 'Tags/Peak',\n                'description': 'Expected tags per peak',\n                'min': 0,\n                'format': '{:,.0f}',\n                'scale': 'PuRd'\n            }\n            self.general_stats_addcols(self.homer_findpeaks, stats_headers, 'HOMER findpeaks')\n\n        return len(self.homer_findpeaks)", "response": "Find HOMER findpeaks logs and parse their data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing HOMER findPeaks file headers.", "response": "def parse_findPeaks(self, f):\n        \"\"\" Parse HOMER findPeaks file headers. \"\"\"\n        parsed_data = dict()\n        s_name = f['s_name']\n        for l in f['f']:\n            # Start of data\n            if l.strip() and not l.strip().startswith('#'):\n                break\n            # Automatically parse header lines by = symbol\n            s = l[2:].split('=')\n            if len(s) > 1:\n                k = s[0].strip().replace(' ','_').lower()\n                v = s[1].strip().replace('%','')\n                try:\n                    parsed_data[k] = float(v)\n                except ValueError:\n                    parsed_data[k] = v\n                if k == 'tag_directory':\n                    s_name = self.clean_s_name(os.path.basename(v), os.path.dirname(v))\n\n        if len(parsed_data) > 0:\n            if s_name in self.homer_findpeaks:\n                log.debug(\"Duplicate sample name found in {}! Overwriting: {}\".format(f['fn'], s_name))\n            self.add_data_source(f, s_name, section='findPeaks')\n            self.homer_findpeaks[s_name] = parsed_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_reports(self):\n\n    # Set up vars\n    self.picard_insertSize_data = dict()\n    self.picard_insertSize_histogram = dict()\n    self.picard_insertSize_samplestats = dict()\n\n    # Go through logs and find Metrics\n    for f in self.find_log_files('picard/insertsize', filehandles=True):\n        s_name = None\n        in_hist = False\n        for l in f['f']:\n\n            # Catch the histogram values\n            if s_name is not None and in_hist is True:\n                try:\n                    sections = l.split(\"\\t\")\n                    ins = int(sections[0])\n                    tot_count = sum( [int(x) for x in sections[1:]] )\n                    self.picard_insertSize_histogram[s_name][ins] = tot_count\n                    self.picard_insertSize_samplestats[s_name]['total_count'] += tot_count\n                except ValueError:\n                    # Reset in case we have more in this log file\n                    s_name = None\n                    in_hist = False\n\n            # New log starting\n            if 'InsertSizeMetrics' in l and 'INPUT' in l:\n                s_name = None\n                # Pull sample name from input\n                fn_search = re.search(r\"INPUT(?:=|\\s+)(\\[?[^\\s]+\\]?)\", l, flags=re.IGNORECASE)\n                if fn_search:\n                    s_name = os.path.basename(fn_search.group(1).strip('[]'))\n                    s_name = self.clean_s_name(s_name, f['root'])\n\n            if s_name is not None:\n                if 'InsertSizeMetrics' in l and '## METRICS CLASS' in l:\n                    if s_name in self.picard_insertSize_data:\n                        log.debug(\"Duplicate sample name found in {}! Overwriting: {}\".format(f['fn'], s_name))\n                    self.add_data_source(f, s_name, section='InsertSizeMetrics')\n                    keys = f['f'].readline().strip(\"\\n\").split(\"\\t\")\n                    vals = f['f'].readline().strip(\"\\n\").split(\"\\t\")\n                    self.picard_insertSize_samplestats[s_name] = {'total_count': 0, 'meansum':0, 'total_pairs':0 }\n                    orientation_idx = keys.index('PAIR_ORIENTATION')\n                    while len(vals) == len(keys):\n                        pair_orientation = vals[orientation_idx]\n                        rowkey = '{}_{}'.format(s_name, pair_orientation)\n                        self.picard_insertSize_data[rowkey] = OrderedDict()\n                        self.picard_insertSize_data[rowkey]['SAMPLE_NAME'] = s_name\n                        for i, k in enumerate(keys):\n                            try:\n                                self.picard_insertSize_data[rowkey][k] = float(vals[i])\n                            except ValueError:\n                                try:\n                                    self.picard_insertSize_data[rowkey][k] = float(vals[i].replace(',','.'))\n                                    log.debug(\"Switching commas for points in '{}': {} - {}\".format(f['fn'], vals[i], vals[i].replace(',','.')))\n                                except ValueError:\n                                    self.picard_insertSize_data[rowkey][k] = vals[i]\n                            except IndexError:\n                                pass # missing data\n                        # Add to mean sums\n                        rp = self.picard_insertSize_data[rowkey]['READ_PAIRS']\n                        mis = self.picard_insertSize_data[rowkey]['MEAN_INSERT_SIZE']\n                        self.picard_insertSize_samplestats[s_name]['meansum'] += (rp * mis)\n                        self.picard_insertSize_samplestats[s_name]['total_pairs'] += rp\n\n                        vals = f['f'].readline().strip(\"\\n\").split(\"\\t\")\n\n                    # Skip lines on to histogram\n                    l = f['f'].readline().strip(\"\\n\")\n                    l = f['f'].readline().strip(\"\\n\")\n\n                    self.picard_insertSize_histogram[s_name] = OrderedDict()\n                    in_hist = True\n\n        for key in list(self.picard_insertSize_data.keys()):\n            if len(self.picard_insertSize_data[key]) == 0:\n                self.picard_insertSize_data.pop(key, None)\n        for s_name in list(self.picard_insertSize_histogram.keys()):\n            if len(self.picard_insertSize_histogram[s_name]) == 0:\n                self.picard_insertSize_histogram.pop(s_name, None)\n                log.debug(\"Ignoring '{}' histogram as no data parsed\".format(s_name))\n\n    # Calculate summed mean values for all read orientations\n    for s_name, v in self.picard_insertSize_samplestats.items():\n        self.picard_insertSize_samplestats[s_name]['summed_mean'] = v['meansum'] / v['total_pairs']\n\n    # Calculate summed median values for all read orientations\n    for s_name in self.picard_insertSize_histogram:\n        j = 0\n        for idx, c in self.picard_insertSize_histogram[s_name].items():\n            j += c\n            if j > (self.picard_insertSize_samplestats[s_name]['total_count'] / 2):\n                self.picard_insertSize_samplestats[s_name]['summed_median'] = idx\n                break\n\n\n    # Filter to strip out ignored sample names\n    self.picard_insertSize_data = self.ignore_samples(self.picard_insertSize_data)\n\n    if len(self.picard_insertSize_data) > 0:\n\n        # Write parsed data to a file\n        self.write_data_file(self.picard_insertSize_data, 'multiqc_picard_insertSize')\n\n        # Do we have median insert sizes?\n        missing_medians = False\n        for v in self.picard_insertSize_samplestats.values():\n            if 'summed_median' not in v:\n                missing_medians = True\n\n        # Add to general stats table\n        self.general_stats_headers['summed_median'] = {\n            'title': 'Insert Size',\n            'description': 'Median Insert Size, all read orientations (bp)',\n            'min': 0,\n            'suffix': ' bp',\n            'format': '{:,.0f}',\n            'scale': 'GnBu',\n        }\n        self.general_stats_headers['summed_mean'] = {\n            'title': 'Mean Insert Size',\n            'description': 'Mean Insert Size, all read orientations (bp)',\n            'min': 0,\n            'suffix': ' bp',\n            'format': '{:,.0f}',\n            'scale': 'GnBu',\n            'hidden': False if missing_medians else True\n        }\n        for s_name in self.picard_insertSize_samplestats:\n            if s_name not in self.general_stats_data:\n                self.general_stats_data[s_name] = dict()\n            self.general_stats_data[s_name].update( self.picard_insertSize_samplestats[s_name] )\n\n        # Section with histogram plot\n        if len(self.picard_insertSize_histogram) > 0:\n            # Make a normalised percentage version of the data\n            data_percent = {}\n            for s_name, data in self.picard_insertSize_histogram.items():\n                data_percent[s_name] = OrderedDict()\n                total = float( sum( data.values() ) )\n                for k, v in data.items():\n                    data_percent[s_name][k] = (v/total)*100\n\n            # Allow customisation of how smooth the the plot is\n            try:\n                insertsize_smooth_points = int(config.picard_config['insertsize_smooth_points'])\n                log.debug(\"Custom Picard insert size smoothing: {}\".format(insertsize_smooth_points))\n            except (AttributeError, KeyError, ValueError):\n                insertsize_smooth_points = 500\n\n            # Plot the data and add section\n            pconfig = {\n                'smooth_points': insertsize_smooth_points,\n                'smooth_points_sumcounts': [True, False],\n                'id': 'picard_insert_size',\n                'title': 'Picard: Insert Size',\n                'ylab': 'Count',\n                'xlab': 'Insert Size (bp)',\n                'xDecimals': False,\n                'tt_label': '<b>{point.x} bp</b>: {point.y:.0f}',\n                'ymin': 0,\n                'data_labels': [\n                    {'name': 'Counts', 'ylab': 'Coverage'},\n                    {'name': 'Percentages', 'ylab': 'Percentage of Counts'}\n                ]\n            }\n            self.add_section (\n                name = 'Insert Size',\n                anchor = 'picard-insertsize',\n                description = 'Plot shows the number of reads at a given insert size. Reads with different orientations are summed.',\n                plot = linegraph.plot([self.picard_insertSize_histogram, data_percent], pconfig)\n            )\n\n\n    # Return the number of detected samples to the parent module\n    return len(self.picard_insertSize_data)", "response": "Find Picard InsertSizeMetrics reports and parse their data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind Picard WgsMetrics reports and parse their data", "response": "def parse_reports(self):\n    \"\"\" Find Picard WgsMetrics reports and parse their data \"\"\"\n\n    # Set up vars\n    self.picard_wgsmetrics_data = dict()\n    self.picard_wgsmetrics_histogram = dict()\n    self.picard_wgsmetrics_samplestats = dict()\n\n    # Go through logs and find Metrics\n    for f in self.find_log_files('picard/wgs_metrics', filehandles=True):\n        s_name = None\n        in_hist = False\n        for l in f['f']:\n\n            # Catch the histogram values\n            if s_name is not None and in_hist is True:\n                try:\n                    sections = l.split(\"\\t\")\n                    cov = int(sections[0])\n                    count = int(sections[1])\n                    self.picard_wgsmetrics_histogram[s_name][cov] = count\n                except ValueError:\n                    # Reset in case we have more in this log file\n                    s_name = None\n                    in_hist = False\n\n            # New log starting\n            if 'WgsMetrics' in l and 'INPUT' in l:\n                s_name = None\n                # Pull sample name from input\n                fn_search = re.search(r\"INPUT(?:=|\\s+)(\\[?[^\\s]+\\]?)\", l, flags=re.IGNORECASE)\n                if fn_search:\n                    s_name = os.path.basename(fn_search.group(1).strip('[]'))\n                    s_name = self.clean_s_name(s_name, f['root'])\n\n            if s_name is not None:\n                if 'CollectWgsMetrics$WgsMetrics' in l and '## METRICS CLASS' in l:\n                    if s_name in self.picard_wgsmetrics_data:\n                        log.debug(\"Duplicate sample name found in {}! Overwriting: {}\".format(f['fn'], s_name))\n                    self.add_data_source(f, s_name, section='WgsMetrics')\n                    self.picard_wgsmetrics_data[s_name] = dict()\n                    keys = f['f'].readline().strip(\"\\n\").split(\"\\t\")\n                    vals = f['f'].readline().strip(\"\\n\").split(\"\\t\")\n                    if len(vals) == len(keys):\n                        for i, k in enumerate(keys):\n                            try:\n                                self.picard_wgsmetrics_data[s_name][k] = float(vals[i])\n                            except ValueError:\n                                self.picard_wgsmetrics_data[s_name][k] = vals[i]\n\n                    # Skip lines on to histogram\n                    next(f['f'])\n                    next(f['f'])\n                    next(f['f'])\n\n                    self.picard_wgsmetrics_histogram[s_name] = OrderedDict()\n                    in_hist = True\n\n        for key in list(self.picard_wgsmetrics_data.keys()):\n            if len(self.picard_wgsmetrics_data[key]) == 0:\n                self.picard_wgsmetrics_data.pop(key, None)\n        for s_name in list(self.picard_wgsmetrics_histogram.keys()):\n            if len(self.picard_wgsmetrics_histogram[s_name]) == 0:\n                self.picard_wgsmetrics_histogram.pop(s_name, None)\n                log.debug(\"Ignoring '{}' histogram as no data parsed\".format(s_name))\n\n    # Filter to strip out ignored sample names\n    self.picard_wgsmetrics_data = self.ignore_samples(self.picard_wgsmetrics_data)\n\n    if len(self.picard_wgsmetrics_data) > 0:\n\n        # Write parsed data to a file\n        self.write_data_file(self.picard_wgsmetrics_data, 'multiqc_picard_wgsmetrics')\n\n        # Add to general stats table\n        self.general_stats_headers['MEDIAN_COVERAGE'] = {\n            'title': 'Median Coverage',\n            'description': 'The median coverage in bases of the genome territory, after all filters are applied.',\n            'min': 0,\n            'suffix': 'X',\n            'scale': 'GnBu',\n        }\n        self.general_stats_headers['MEAN_COVERAGE'] = {\n            'title': 'Mean Coverage',\n            'description': 'The mean coverage in bases of the genome territory, after all filters are applied.',\n            'min': 0,\n            'suffix': 'X',\n            'scale': 'GnBu',\n            'hidden': True,\n        }\n        self.general_stats_headers['SD_COVERAGE'] = {\n            'title': 'Median Coverage',\n            'description': 'The standard deviation coverage in bases of the genome territory, after all filters are applied.',\n            'min': 0,\n            'suffix': 'X',\n            'scale': 'GnBu',\n            'hidden': True,\n        }\n        # user configurable coverage level\n        try:\n            covs = config.picard_config['general_stats_target_coverage']\n            assert type(covs) == list\n            assert len(covs) > 0\n            covs = [str(i) for i in covs]\n            log.debug(\"Custom Picard coverage thresholds: {}\".format(\", \".join([i for i in covs])))\n        except (AttributeError, TypeError, AssertionError):\n            covs = ['30']\n        for c in covs:\n            self.general_stats_headers['PCT_{}X'.format(c)] = {\n                'id': 'picard_target_bases_{}X'.format(c),\n                'title': 'Bases &ge; {}X'.format(c),\n                'description': 'Percent of target bases with coverage &ge; {}X'.format(c),\n                'max': 100,\n                'min': 0,\n                'suffix': '%',\n                'format': '{:,.0f}',\n                'scale': 'RdYlGn',\n                'modify': lambda x: self.multiply_hundred(x)\n            }\n\n        for s_name in self.picard_wgsmetrics_data:\n            if s_name not in self.general_stats_data:\n                self.general_stats_data[s_name] = dict()\n            self.general_stats_data[s_name].update( self.picard_wgsmetrics_data[s_name] )\n\n        # Section with histogram plot\n        if len(self.picard_wgsmetrics_histogram) > 0:\n\n            # Figure out where to cut histogram tail\n            max_cov = 10\n            for s_name, samp in self.picard_wgsmetrics_histogram.items():\n                total = float( sum( samp.values() ) )\n                running_total = 0\n                for k, v in samp.items():\n                    running_total += v\n                    if running_total > total * 0.99:\n                        max_cov = max(k, max_cov)\n                        break\n\n            # Cut histogram tail and make a normalised percentage version of the data plus dropoff\n            data = {}\n            data_percent = {}\n            maxval = 0\n            for s_name, samp in self.picard_wgsmetrics_histogram.items():\n                data[s_name] = OrderedDict()\n                data_percent[s_name] = OrderedDict()\n                total = float( sum( samp.values() ) )\n                cumulative = 0\n                for k, v in samp.items():\n                    if k <= max_cov:\n                        cumulative += v\n                        data[s_name][k] = v\n                        maxval = max(maxval, v)\n                        data_percent[s_name][k] = 100 - (cumulative/total)*100\n                    else:\n                        break\n\n            # Plot the data and add section\n            pconfig = {\n                'id': 'picard_wgs_metrics_histogram',\n                'title': 'Picard: WGS Coverage',\n                'ylab': 'Percentage of Bases',\n                'xlab': 'Fold Coverage',\n                'xDecimals': False,\n                'tt_label': '<b>{point.x} X</b>: {point.y:.1f}',\n                'ymin': 0,\n                'ymax': 100,\n                'data_labels': [\n                    {'name': 'Percentage Drop-Off', 'ylab': 'Percentage of Bases', 'ymax': 100},\n                    {'name': 'Counts Histogram', 'ylab': 'Coverage', 'ymax': maxval}\n                ]\n            }\n            self.add_section (\n                name = 'WGS Coverage',\n                anchor = 'picard-wgsmetrics-cov',\n                description = 'The number of bases in the genome territory for each fold coverage. ' +\n                            'Note that final 1% of data is hidden to prevent very long tails.',\n                plot = linegraph.plot([data_percent, data], pconfig)\n            )\n\n            # Bar plot of ignored bases\n            pdata = dict()\n            for s_name, data in self.picard_wgsmetrics_data.items():\n                pdata[s_name] = dict()\n                pdata[s_name]['PCT_EXC_MAPQ'] = data['PCT_EXC_MAPQ'] * 100.0\n                pdata[s_name]['PCT_EXC_DUPE'] = data['PCT_EXC_DUPE'] * 100.0\n                pdata[s_name]['PCT_EXC_UNPAIRED'] = data['PCT_EXC_UNPAIRED'] * 100.0\n                pdata[s_name]['PCT_EXC_BASEQ'] = data['PCT_EXC_BASEQ'] * 100.0\n                pdata[s_name]['PCT_EXC_OVERLAP'] = data['PCT_EXC_OVERLAP'] * 100.0\n                pdata[s_name]['PCT_EXC_CAPPED'] = data['PCT_EXC_CAPPED'] * 100.0\n\n            keys = OrderedDict()\n            keys['PCT_EXC_MAPQ'] = {'name': 'Low mapping quality'}\n            keys['PCT_EXC_DUPE'] = {'name': 'Duplicates reads'}\n            keys['PCT_EXC_UNPAIRED'] = {'name': 'No mapped mate pair'}\n            keys['PCT_EXC_BASEQ'] = {'name': 'Low base quality'}\n            keys['PCT_EXC_OVERLAP'] = {'name': 'Overlapping insert'}\n            keys['PCT_EXC_CAPPED'] = {'name': 'Over capped coverage'}\n\n            # Config for the plot\n            pconfig = {\n                'id': 'picard_wgs_metrics_bases',\n                'title': 'Picard: WGS Filtered Bases',\n                'cpswitch': False,\n                'ylab': '% Bases',\n                'ymax': 100\n            }\n\n            self.add_section (\n                name = 'WGS Filtered Bases',\n                anchor = 'picard-wgsmetrics-bases',\n                description = 'For more information about the filtered categories, see the '+\n                           '<a href=\"http://broadinstitute.github.io/picard/picard-metric-definitions.html#CollectWgsMetrics.WgsMetrics\" target=\"_blank\">Picard documentation</a>.',\n                plot = bargraph.plot(pdata, keys, pconfig)\n            )\n\n\n    # Return the number of detected samples to the parent module\n    return len(self.picard_wgsmetrics_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize logging. Prints logs to console with level defined by loglevel Also prints verbose log to the multiqc data directory if available. (multiqc_data/multiqc.log) Args: loglevel (str): Determines the level of the log output.", "response": "def init_log(logger, loglevel=0):\n    \"\"\"\n    Initializes logging.\n    Prints logs to console with level defined by loglevel\n    Also prints verbose log to the multiqc data directory if available.\n    (multiqc_data/multiqc.log)\n\n    Args:\n        loglevel (str): Determines the level of the log output.\n    \"\"\"\n    # File for logging\n    global log_tmp_dir, log_tmp_fn\n    log_tmp_dir = tempfile.mkdtemp()\n    log_tmp_fn = os.path.join(log_tmp_dir, 'multiqc.log')\n\n    # Logging templates\n    debug_template = '[%(asctime)s] %(name)-50s [%(levelname)-7s]  %(message)s'\n    info_template = '[%(levelname)-7s] %(module)15s : %(message)s'\n\n    # Base level setup\n    logger.setLevel(getattr(logging, 'DEBUG'))\n\n    # Set up the console logging stream\n    console = logging.StreamHandler()\n    console.setLevel(getattr(logging, loglevel))\n    if loglevel == 'DEBUG':\n        console.setFormatter(logging.Formatter(debug_template))\n    else:\n        console.setFormatter(logging.Formatter(info_template))\n    logger.addHandler(console)\n\n    # Now set up the file logging stream if we have a data directory\n    file_handler = logging.FileHandler(log_tmp_fn, encoding='utf-8')\n    file_handler.setLevel(getattr(logging, 'DEBUG')) # always DEBUG for the file\n    file_handler.setFormatter(logging.Formatter(debug_template))\n    logger.addHandler(file_handler)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef move_tmp_log(logger):\n\n    try:\n        # https://stackoverflow.com/questions/15435652/python-does-not-release-filehandles-to-logfile\n        logging.shutdown()\n        shutil.move(log_tmp_fn, os.path.join(config.data_dir, 'multiqc.log'))\n        util_functions.robust_rmtree(log_tmp_dir)\n    except (AttributeError, TypeError, IOError):\n        pass", "response": "Move the temporary log file to the MultiQC data directory\n    if it exists."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a stream to the root log file or stderr log stream.", "response": "def get_log_stream(logger):\n    \"\"\"\n    Returns a stream to the root log file.\n    If there is no logfile return the stderr log stream\n\n    Returns:\n        A stream to the root log file or stderr stream.\n    \"\"\"\n\n    file_stream = None\n    log_stream = None\n    for handler in logger.handlers:\n        if isinstance(handler, logging.FileHandler):\n            file_stream = handler.stream\n        else:\n            log_stream = handler.stream\n\n    if file_stream:\n        return file_stream\n\n    return log_stream"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the jellyfish data file and store it in self. jellyfish_data.", "response": "def parse_jellyfish_data(self, f):\n        \"\"\" Go through the hist file and memorise it \"\"\"\n        histogram = {}\n        occurence = 0\n        for line in f['f']:\n            line = line.rstrip('\\n')\n            occurence = int(line.split(\" \")[0])\n            count = int(line.split(\" \")[1])\n            histogram[occurence] = occurence*count\n        #delete last occurnece as it is the sum of all kmer occuring more often than it.\n        del histogram[occurence]\n        #sanity check\n        self.max_key  = max(histogram, key=histogram.get)\n        self.jellyfish_max_x = max(self.jellyfish_max_x, self.max_key)\n        if len(histogram) > 0:\n            if f['s_name'] in self.jellyfish_data:\n                log.debug(\"Duplicate sample name found! Overwriting: {}\".format(f['s_name']))\n            self.add_data_source(f)\n            self.jellyfish_data[f['s_name']] = histogram"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef frequencies_plot(self, xmin=0, xmax=200):\n\n        helptext = '''\n            A possible way to assess the complexity of a library even in\n            absence of a reference sequence is to look at the kmer profile of the reads.\n            The idea is to count all the kmers (_i.e._, sequence of length `k`) that occur\n            in the reads. In this way it is possible to know how many kmers occur\n            `1,2,.., N` times and represent this as a plot.\n            This plot tell us for each x, how many k-mers (y-axis) are present in the\n            dataset in exactly x-copies.\n\n            In an ideal world (no errors in sequencing, no bias, no  repeated regions)\n            this plot should be as close as  possible to a gaussian distribution.\n            In reality we will always see a peak for `x=1` (_i.e._, the errors)\n            and another peak close to the expected coverage. If the genome is highly\n            heterozygous a second peak at half of the coverage can be expected.'''\n\n        pconfig = {\n            'id': 'Jellyfish_kmer_plot',\n            'title': 'Jellyfish: K-mer plot',\n            'ylab': 'Counts',\n            'xlab': 'k-mer frequency',\n            'xDecimals': False,\n            'xmin': xmin,\n            'xmax': xmax\n        }\n\n        self.add_section(\n            anchor = 'jellyfish_kmer_plot',\n            description = 'The K-mer plot lets you estimate library complexity and coverage from k-mer content.',\n            helptext = helptext,\n            plot = linegraph.plot(self.jellyfish_data, pconfig)\n        )", "response": "Generate the qualities plot for the specified reference sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_reports(self):\n\n    # Set up vars\n    self.infer_exp = dict()\n    regexes = {\n        'pe_sense': r\"\\\"1\\+\\+,1--,2\\+-,2-\\+\\\": (\\d\\.\\d+)\",\n        'pe_antisense': r\"\\\"1\\+-,1-\\+,2\\+\\+,2--\\\": (\\d\\.\\d+)\",\n        'se_sense': r\"\\\"\\+\\+,--\\\": (\\d\\.\\d+)\",\n        'se_antisense': r\"\\+-,-\\+\\\": (\\d\\.\\d+)\",\n        'failed': r\"Fraction of reads failed to determine: (\\d\\.\\d+)\"\n    }\n\n    # Go through files and parse data using regexes\n    for f in self.find_log_files('rseqc/infer_experiment'):\n        d = dict()\n        for k, r in regexes.items():\n            r_search = re.search(r, f['f'], re.MULTILINE)\n            if r_search:\n                d[k] = float(r_search.group(1))\n\n        if len(d) > 0:\n            if f['s_name'] in self.infer_exp:\n                log.debug(\"Duplicate sample name found! Overwriting: {}\".format(f['s_name']))\n            self.add_data_source(f, section='infer_experiment')\n            self.infer_exp[f['s_name']] = d\n\n    # Filter to strip out ignored sample names\n    self.infer_exp = self.ignore_samples(self.infer_exp)\n\n    if len(self.infer_exp) > 0:\n\n        # Write to file\n        self.write_data_file(self.infer_exp, 'multiqc_rseqc_infer_experiment')\n\n        # Merge PE and SE for plot\n        pdata = dict()\n        for s_name, vals in self.infer_exp.items():\n            pdata[s_name] = dict()\n            for k, v in vals.items():\n                v *= 100.0 # Multiply to get percentage\n                if k[:2] == 'pe' or k[:2] == 'se':\n                    k = k[3:]\n                pdata[s_name][k] = v + pdata[s_name].get(k, 0)\n\n        # Plot bar graph of groups\n        keys = OrderedDict()\n        keys['sense'] = {'name': \"Sense\"}\n        keys['antisense'] = {'name': \"Antisense\"}\n        keys['failed'] = {'name': \"Undetermined\"}\n\n        # Config for the plot\n        pconfig = {\n            'id': 'rseqc_infer_experiment_plot',\n            'title': 'RSeQC: Infer experiment',\n            'ylab': '% Tags',\n            'ymin': 0,\n            'ymax': 100,\n            'tt_percentages': False,\n            'ylab_format': '{value}%',\n            'cpswitch': False\n        }\n\n        self.add_section (\n            name = 'Infer experiment',\n            anchor = 'rseqc-infer_experiment',\n            description = '<a href=\"http://rseqc.sourceforge.net/#infer-experiment-py\" target=\"_blank\">Infer experiment</a>' \\\n                \" counts the percentage of reads and read pairs that match the strandedness of overlapping transcripts.\" \\\n                \" It can be used to infer whether RNA-seq library preps are stranded (sense or antisense).\",\n            plot = bargraph.plot(pdata, keys, pconfig)\n        )\n\n    # Return number of samples found\n    return len(self.infer_exp)", "response": "Find RSeQC infer_experiment reports and parse their data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_leehom_logs(self, f):\n        regexes = {\n            'total': r\"Total reads[\\s\\:]+(\\d+)\",\n            'merged_trimming': r\"Merged \\(trimming\\)\\s+(\\d+)\",\n            'merged_overlap': r\"Merged \\(overlap\\)\\s+(\\d+)\",\n            'kept': r\"Kept PE/SR\\s+(\\d+)\",\n            'trimmed': r\"Trimmed SR\\s+(\\d+)\",\n            'adapter_dimers_chimeras': r\"Adapter dimers/chimeras\\s+(\\d+)\",\n            'failed_key': r\"Failed Key\\s+(\\d+)\"\n        }\n        parsed_data = dict()\n        for l in f['f']:\n            # Search regexes for overview stats\n            for k, r in regexes.items():\n                match = re.search(r, l)\n                if match:\n                    parsed_data[k] = int(match.group(1))\n        return parsed_data", "response": "Parse the leehom logs and return a dictionary of the data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef leehom_general_stats_table(self):\n\n        headers = {}\n        headers['merged_trimming'] = {\n            'title': '{} Merged (Trimming)'.format(config.read_count_prefix),\n            'description': 'Merged clusters from trimming ({})'.format(config.read_count_desc),\n            'min': 0,\n            'scale': 'PuRd',\n            'modify': lambda x: x * config.read_count_multiplier,\n            'shared_key': 'read_count'\n        }\n        headers['merged_overlap'] = {\n            'title': '{} Merged (Overlap)'.format(config.read_count_prefix),\n            'description': 'Merged clusters from overlapping reads ({})'.format(config.read_count_desc),\n            'min': 0,\n            'scale': 'PuRd',\n            'modify': lambda x: x * config.read_count_multiplier,\n            'shared_key': 'read_count'\n        }\n        self.general_stats_addcols(self.leehom_data, headers)", "response": "Take the parsed stats from the leeHom report and add it to the basic stats table at the top of the report"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntake the parsed stats from the DeDup report and add it to the basic stats table at the top of the report", "response": "def dedup_general_stats_table(self):\n        \"\"\" Take the parsed stats from the DeDup report and add it to the\n        basic stats table at the top of the report \"\"\"\n\n        headers = OrderedDict()\n        headers['duplication_rate'] = {\n            'title': 'Duplication Rate',\n            'description': 'Percentage of reads categorised as a technical duplicate',\n            'min': 0,\n            'max': 100,\n            'suffix': '%',\n            'scale': 'OrRd',\n            'format': '{:,.0f}',\n            'modify': lambda x: x * 100.0\n        }\n        self.general_stats_addcols(self.dedup_data, headers)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dedup_alignment_plot (self):\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['not_removed'] = { 'name': 'Not Removed' }\n        keys['reverse_removed'] = { 'name': 'Reverse Removed' }\n        keys['forward_removed'] =   { 'name': 'Forward Removed' }\n        keys['merged_removed'] =   { 'name': 'Merged Removed' }\n\n        # Config for the plot\n        config = {\n            'id': 'dedup_rates',\n            'title': 'DeDup: Deduplicated Reads',\n            'ylab': '# Reads',\n            'cpswitch_counts_label': 'Number of Reads',\n            'hide_zero_cats': False\n        }\n\n        return bargraph.plot(self.dedup_data, keys, config)", "response": "Make the HTML to plot the duplication rates"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef macs_general_stats(self):\n        headers = OrderedDict()\n        headers['d'] = {\n            'title': 'Fragment Length',\n            'min': 0,\n            'format': '{:,.0f}'\n        }\n        headers['treatment_redundant_rate'] = {\n            'title': 'Treatment Redundancy',\n            'description': 'Redundant rate in treatment',\n            'max': 1,\n            'min': 0,\n            'format': '{:,.2f}',\n            'scale': 'RdYlBu-rev'\n        }\n        headers['control_redundant_rate'] = {\n            'title': 'Control Redundancy',\n            'description': 'Redundant rate in control',\n            'max': 1,\n            'min': 0,\n            'format': '{:,.2f}',\n            'scale': 'RdYlBu-rev'\n        }\n        self.general_stats_addcols(self.macs_data, headers)", "response": "Add columns to General Statistics table"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef macs_filtered_reads_plot(self):\n        data = dict()\n        req_cats = ['control_fragments_total', 'control_fragments_after_filtering', 'treatment_fragments_total', 'treatment_fragments_after_filtering']\n        for s_name, d in self.macs_data.items():\n            if all([c in d for c in req_cats]):\n                data['{}: Control'.format(s_name)] = dict()\n                data['{}: Treatment'.format(s_name)] = dict()\n                data['{}: Control'.format(s_name)]['fragments_filtered'] = d['control_fragments_total'] - d['control_fragments_after_filtering']\n                data['{}: Control'.format(s_name)]['fragments_not_filtered'] = d['control_fragments_after_filtering']\n                data['{}: Treatment'.format(s_name)]['fragments_filtered'] = d['treatment_fragments_total'] - d['treatment_fragments_after_filtering']\n                data['{}: Treatment'.format(s_name)]['fragments_not_filtered'] = d['treatment_fragments_after_filtering']\n\n        # Check that we have something to plot\n        if len(data) == 0:\n            return\n\n        # Specify the order of the different possible categories\n        keys = OrderedDict()\n        keys['fragments_not_filtered'] = { 'color': '#437BB1', 'name': 'Remaining fragments' }\n        keys['fragments_filtered'] =     { 'color': '#B1084C', 'name': 'Filtered fragments' }\n\n        # Config for the plot\n        pconfig = {\n            'id': 'macs2_filtered',\n            'title': 'MACS2: Filtered Fragments',\n            'ylab': '# Fragments',\n            'cpswitch_counts_label': 'Number of Fragments',\n            'hide_zero_cats': False\n        }\n\n        self.add_section(\n            plot = bargraph.plot(data, keys, pconfig)\n        )", "response": "Make the macs2 filtered reads plot"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsplit concat log into individual samples", "response": "def split_log(logf):\n        \"\"\"split concat log into individual samples\"\"\"\n        flashpatt = re.compile(\n            r'\\[FLASH\\] Fast Length Adjustment of SHort reads\\n(.+?)\\[FLASH\\] FLASH', flags=re.DOTALL)\n        return flashpatt.findall(logf)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_field(field, slog, fl=False):\n        field += r'\\:\\s+([\\d\\.]+)'\n        match = re.search(field, slog)\n        if match:\n            if fl:\n                return float(match.group(1))\n            return int(match.group(1))\n        return 0", "response": "parse sample log for field\n           set fl = True to return a float otherwise returns int\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clean_pe_name(self, nlog, root):\n        use_output_name = getattr(config, 'flash', {}).get('use_output_name', False)\n        if use_output_name:\n            name = re.search(r'Output files\\:\\n\\[FLASH\\]\\s+(.+?)\\n', nlog)\n        else:\n            name = re.search(r'Input files\\:\\n\\[FLASH\\]\\s+(.+?)\\n', nlog)\n        if not name:\n            return None\n        name = name.group(1)\n        name = self.clean_s_name(name, root)\n        return name", "response": "additional name cleaning for paired end data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_flash_log(self, logf):\n        data = OrderedDict()\n        samplelogs = self.split_log(logf['f'])\n        for slog in samplelogs:\n            try:\n                sample = dict()\n                ## Sample name ##\n                s_name = self.clean_pe_name(slog, logf['root'])\n                if s_name is None:\n                    continue\n                sample['s_name'] = s_name\n\n                ## Log attributes ##\n                sample['totalpairs'] = self.get_field('Total pairs', slog)\n                sample['discardpairs'] = self.get_field('Discarded pairs', slog)\n                sample['percdiscard'] = self.get_field('Percent Discarded', slog, fl=True)\n                sample['combopairs'] = self.get_field('Combined pairs', slog)\n                sample['inniepairs'] = self.get_field('Innie pairs', slog)\n                sample['outiepairs'] = self.get_field('Outie pairs', slog)\n                sample['uncombopairs'] = self.get_field('Uncombined pairs', slog)\n                sample['perccombo'] = self.get_field('Percent combined', slog, fl=True)\n\n                data[s_name] = sample\n            except Exception as err:\n                log.warning(\"Error parsing record in {}. {}\".format(logf['fn'], err))\n                log.debug(traceback.format_exc())\n                continue\n        return data", "response": "parse flash logs into a dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stats_table(self, data):\n        headers = OrderedDict()\n        headers['combopairs'] = {\n            'title': 'Combined pairs',\n            'description': 'Num read pairs combined',\n            'shared_key': 'read_count',\n            'hidden': True,\n            'scale': False\n        }\n        headers['perccombo'] = {\n            'title': '% Combined',\n            'description': '% read pairs combined',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'PiYG'\n        }\n        self.general_stats_addcols(data, headers)", "response": "Add percent combined to general stats table"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_hist_files(histf):\n        nameddata = dict()\n        data = dict()\n        try:\n            for l in histf['f'].splitlines():\n                s = l.split()\n                if s:\n                    if len(s) != 2:\n                        raise RuntimeError(\"invalid format: \" + str(len(s)) + \" column(s) found in row. must be exactly 2.\")\n                    data[int(s[0])] = int(s[1])\n        except Exception as err:\n            log.warning(\"Error parsing %s. %s\", histf['fn'], err)\n            log.debug(traceback.format_exc())\n        else:\n            if data:\n                nameddata[histf['s_name']] = data\n            else:\n                log.debug(\"%s is empty.\", histf['fn'])\n        finally:\n            return nameddata", "response": "parse histogram files and return a dict of named data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget colors for freqpoly graph", "response": "def get_colors(n):\n        \"\"\"get colors for freqpoly graph\"\"\"\n        cb_palette = [\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\",\n                      \"#CC79A7\",\"#001F3F\", \"#0074D9\", \"#7FDBFF\", \"#39CCCC\", \"#3D9970\", \"#2ECC40\",\n                       \"#01FF70\", \"#FFDC00\", \"#FF851B\", \"#FF4136\", \"#F012BE\", \"#B10DC9\",\n                       \"#85144B\", \"#AAAAAA\", \"#000000\"]\n\n        whole = int(n/22)\n        extra = (n % 22)\n        cols = cb_palette * whole\n        if extra >= 0:\n            cols.extend(cb_palette[0:extra])\n        return cols"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking freqpoly plot of merged read lengths", "response": "def freqpoly_plot(data):\n        \"\"\"make freqpoly plot of merged read lengths\"\"\"\n        rel_data = OrderedDict()\n        for key, val in data.items():\n            tot = sum(val.values(), 0)\n            rel_data[key] = {k: v / tot for k, v in val.items()}\n        fplotconfig = {\n            'data_labels': [\n                {'name': 'Absolute', 'ylab': 'Frequency', 'xlab': 'Merged Read Length'},\n                {'name': 'Relative', 'ylab': 'Relative Frequency', 'xlab': 'Merged Read Length'}\n                ],\n            'id': 'flash_freqpoly_plot', 'title': 'FLASh: Frequency of merged read lengths',\n            'colors': dict(zip(data.keys(), MultiqcModule.get_colors(len(data))))\n            }\n        return linegraph.plot([data, rel_data], fplotconfig)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing flash numeric histograms", "response": "def hist_results(self):\n        \"\"\"process flash numeric histograms\"\"\"\n        self.hist_data = OrderedDict()\n        for histfile in self.find_log_files('flash/hist'):\n            self.hist_data.update(self.parse_hist_files(histfile))\n\n        # ignore sample names\n        self.hist_data = self.ignore_samples(self.hist_data)\n\n        try:\n            if not self.hist_data:\n                raise UserWarning\n            log.info(\"Found %d histogram reports\", len(self.hist_data))\n\n            self.add_section(\n                name='Frequency polygons of merged read lengths',\n                anchor='flash-histogram',\n                description='This plot is made from the numerical histograms output by FLASh.',\n                plot=self.freqpoly_plot(self.hist_data))\n\n        except UserWarning:\n            pass\n        except Exception as err:\n            log.error(err)\n            log.debug(traceback.format_exc())\n        return len(self.hist_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots a scatter plot with X and Y data.", "response": "def plot (data, pconfig=None):\n    \"\"\" Plot a scatter plot with X,Y data.\n    :param data: 2D dict, first keys as sample names, then x:y data pairs\n    :param pconfig: optional dict with config key:value pairs. See CONTRIBUTING.md\n    :return: HTML and JS, ready to be inserted into the page\n    \"\"\"\n    if pconfig is None:\n        pconfig = {}\n\n    # Allow user to overwrite any given config for this plot\n    if 'id' in pconfig and pconfig['id'] and pconfig['id'] in config.custom_plot_config:\n        for k, v in config.custom_plot_config[pconfig['id']].items():\n            pconfig[k] = v\n\n    # Given one dataset - turn it into a list\n    if type(data) is not list:\n        data = [data]\n\n    # Generate the data dict structure expected by HighCharts series\n    plotdata = list()\n    for data_index, ds in enumerate(data):\n        d = list()\n        for s_name in ds:\n            # Ensure any overwritting conditionals from data_labels (e.g. ymax) are taken in consideration\n            series_config = pconfig.copy()\n            if 'data_labels' in pconfig and type(pconfig['data_labels'][data_index]) is dict:  # if not a dict: only dataset name is provided\n                series_config.update(pconfig['data_labels'][data_index])\n\n            if type(ds[s_name]) is not list:\n                ds[s_name] = [ ds[s_name] ]\n            for k in ds[s_name]:\n                if k['x'] is not None:\n                    if 'xmax' in series_config and float(k['x']) > float(series_config['xmax']):\n                        continue\n                    if 'xmin' in series_config and float(k['x']) < float(series_config['xmin']):\n                        continue\n                if k['y'] is not None:\n                    if 'ymax' in series_config and float(k['y']) > float(series_config['ymax']):\n                        continue\n                    if 'ymin' in series_config and float(k['y']) < float(series_config['ymin']):\n                        continue\n                this_series = { 'x': k['x'], 'y': k['y'] }\n                try:\n                    this_series['name'] = \"{}: {}\".format(s_name, k['name'])\n                except KeyError:\n                    this_series['name'] = s_name\n                try:\n                    this_series['color'] = k['color']\n                except KeyError:\n                    try:\n                        this_series['color'] = series_config['colors'][s_name]\n                    except KeyError:\n                        pass\n                d.append(this_series)\n        plotdata.append(d)\n\n    # Add on annotation data series\n    try:\n        if pconfig.get('extra_series'):\n            extra_series = pconfig['extra_series']\n            if type(pconfig['extra_series']) == dict:\n                extra_series = [[ pconfig['extra_series'] ]]\n            elif type(pconfig['extra_series']) == list and type(pconfig['extra_series'][0]) == dict:\n                extra_series = [ pconfig['extra_series'] ]\n            for i, es in enumerate(extra_series):\n                for s in es:\n                    plotdata[i].append(s)\n    except (KeyError, IndexError):\n        pass\n\n    # Make a plot\n    return highcharts_scatter_plot(plotdata, pconfig)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a dummy graph to feed to the FIAS libraries.", "response": "def generate_dummy_graph(network):\n    \"\"\"Generate a dummy graph to feed to the FIAS libraries.\n    It adds the \"pos\" attribute and removes the 380 kV duplicate\n    buses when the buses have been split, so that all load and generation\n    is attached to the 220kV bus.\"\"\"\n    \n    graph = pypsa.descriptors.OrderedGraph()\n    \n    graph.add_nodes_from([bus for bus in network.buses.index if bus not in buses_to_split])\n    \n    #add positions to graph for voronoi cell computation\n    for node in graph.nodes():\n        graph.node[node][\"pos\"] = np.array(network.buses.loc[node,[\"x\",\"y\"]],dtype=float)\n    \n    return graph"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef voronoi_partition(G, outline):\n    #following line from vresutils.graph caused a bug\n    #G = polygon_subgraph(G, outline, copy=False)\n    points = list(vresutils.graph.get_node_attributes(G, 'pos').values())\n    regions = vresutils.graph.voronoi_partition_pts(points, outline, no_multipolygons=True)\n    nx.set_node_attributes(G, 'region', dict(zip(G.nodes(), regions)))\n\n    return G", "response": "Returns a 2D - embedded graph G with the Voronoi cell region as an additional node                                                                             \n    attribute."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef panel_capacity(panel):\n    c = vresutils.reatlas.solarpanelconf_to_solar_panel_config_object(panel)\n    return c['A'] + c['B'] * 1000 + c['C'] * np.log(1000)", "response": "Returns the panel capacity in MW."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef haversine(a0,a1):\n\n    a = [np.asarray(a0,dtype=float),np.asarray(a1,dtype=float)]\n\n    for i in range(2):\n        if len(a[i].shape) == 1:\n            a[i] = np.reshape(a[i],(1,a[i].shape[0]))\n\n        if a[i].shape[1] != 2:\n            logger.error(\"Inputs to haversine have the wrong shape!\")\n            return\n\n        a[i] = np.deg2rad(a[i])\n\n    distance_km = np.zeros((a[0].shape[0],a[1].shape[0]))\n\n    for i in range(a[1].shape[0]):\n        b = np.sin((a[0][:,1] - a[1][i,1])/2.)**2 + np.cos(a[0][:,1]) * np.cos(a[1][i,1]) * np.sin((a[0][:,0] - a[1][i,0])/2.)**2\n        distance_km[:,i] = 6371.000 * 2 * np.arctan2( np.sqrt(b), np.sqrt(1-b) )\n\n    return distance_km", "response": "Compute the distance in km between two sets of points in long / lat."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the area in km^2 of a shapely geometry in longitude and latitude.", "response": "def area_from_lon_lat_poly(geometry):\n    \"\"\"\n    Compute the area in km^2 of a shapely geometry, whose points are in longitude and latitude.\n\n    Parameters\n    ----------\n    geometry: shapely geometry\n        Points must be in longitude and latitude.\n\n    Returns\n    -------\n    area:  float\n        Area in km^2.\n\n    \"\"\"\n\n    import pyproj\n    from shapely.ops import transform\n    from functools import partial\n\n\n    project = partial(\n        pyproj.transform,\n        pyproj.Proj(init='epsg:4326'), # Source: Lon-Lat\n        pyproj.Proj(proj='aea')) # Target: Albers Equal Area Conical https://en.wikipedia.org/wiki/Albers_projection\n\n    new_geometry = transform(project, geometry)\n\n    #default area is in m^2\n    return new_geometry.area/1e6"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconstruct cycle constraints for a particular subnetwork.", "response": "def define_sub_network_cycle_constraints( subnetwork, snapshots, passive_branch_p, attribute):\n    \"\"\" Constructs cycle_constraints for a particular subnetwork\n    \"\"\"\n\n    sub_network_cycle_constraints = {}\n    sub_network_cycle_index = []\n\n    matrix = subnetwork.C.tocsc()\n    branches = subnetwork.branches()\n\n    for col_j in range( matrix.shape[1] ):\n        cycle_is = matrix.getcol(col_j).nonzero()[0]\n\n        if len(cycle_is) == 0: continue\n\n        sub_network_cycle_index.append((subnetwork.name, col_j))\n\n\n        branch_idx_attributes = []\n\n        for cycle_i in cycle_is:\n            branch_idx = branches.index[cycle_i]\n            attribute_value = 1e5 * branches.at[ branch_idx, attribute] * subnetwork.C[ cycle_i, col_j]\n            branch_idx_attributes.append( (branch_idx, attribute_value))\n\n        for snapshot in snapshots:\n            expression_list = [ (attribute_value,\n                                 passive_branch_p[branch_idx[0], branch_idx[1], snapshot]) for (branch_idx, attribute_value) in branch_idx_attributes]\n\n            lhs = LExpression(expression_list)\n            sub_network_cycle_constraints[subnetwork.name,col_j,snapshot] = LConstraint(lhs,\"==\",LExpression())\n\n    return( sub_network_cycle_index, sub_network_cycle_constraints)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef define_passive_branch_flows_with_kirchhoff(network,snapshots,skip_vars=False):\n\n    for sub_network in network.sub_networks.obj:\n        find_tree(sub_network)\n        find_cycles(sub_network)\n\n        #following is necessary to calculate angles post-facto\n        find_bus_controls(sub_network)\n        if len(sub_network.branches_i()) > 0:\n            calculate_B_H(sub_network)\n\n    passive_branches = network.passive_branches()\n\n    if not skip_vars:\n        network.model.passive_branch_p = Var(list(passive_branches.index), snapshots)\n\n    cycle_index = []\n    cycle_constraints = {}\n\n    for subnetwork in network.sub_networks.obj:\n\n        branches = subnetwork.branches()\n        attribute = \"r_pu_eff\" if network.sub_networks.at[subnetwork.name,\"carrier\"] == \"DC\" else \"x_pu_eff\"\n\n        sub_network_cycle_index, sub_network_cycle_constraints = define_sub_network_cycle_constraints( subnetwork,\n                                                                                                       snapshots,\n                                                                                                       network.model.passive_branch_p, attribute)\n\n        cycle_index.extend( sub_network_cycle_index)\n        cycle_constraints.update( sub_network_cycle_constraints)\n\n    l_constraint(network.model, \"cycle_constraints\", cycle_constraints,\n                 cycle_index, snapshots)", "response": "define passive branch flows with the kirchoff method"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef define_nodal_balances(network,snapshots):\n\n    #dictionary for constraints\n    network._p_balance = {(bus,sn) : LExpression()\n                          for bus in network.buses.index\n                          for sn in snapshots}\n\n    efficiency = get_switchable_as_dense(network, 'Link', 'efficiency', snapshots)\n\n    for cb in network.links.index:\n        bus0 = network.links.at[cb,\"bus0\"]\n        bus1 = network.links.at[cb,\"bus1\"]\n\n        for sn in snapshots:\n            network._p_balance[bus0,sn].variables.append((-1,network.model.link_p[cb,sn]))\n            network._p_balance[bus1,sn].variables.append((efficiency.at[sn,cb],network.model.link_p[cb,sn]))\n\n    #Add any other buses to which the links are attached\n    for i in [int(col[3:]) for col in network.links.columns if col[:3] == \"bus\" and col not in [\"bus0\",\"bus1\"]]:\n        efficiency = get_switchable_as_dense(network, 'Link', 'efficiency{}'.format(i), snapshots)\n        for cb in network.links.index[network.links[\"bus{}\".format(i)] != \"\"]:\n            bus = network.links.at[cb, \"bus{}\".format(i)]\n            for sn in snapshots:\n                network._p_balance[bus,sn].variables.append((efficiency.at[sn,cb],network.model.link_p[cb,sn]))\n\n\n    for gen in network.generators.index:\n        bus = network.generators.at[gen,\"bus\"]\n        sign = network.generators.at[gen,\"sign\"]\n        for sn in snapshots:\n            network._p_balance[bus,sn].variables.append((sign,network.model.generator_p[gen,sn]))\n\n    load_p_set = get_switchable_as_dense(network, 'Load', 'p_set', snapshots)\n    for load in network.loads.index:\n        bus = network.loads.at[load,\"bus\"]\n        sign = network.loads.at[load,\"sign\"]\n        for sn in snapshots:\n            network._p_balance[bus,sn].constant += sign*load_p_set.at[sn,load]\n\n    for su in network.storage_units.index:\n        bus = network.storage_units.at[su,\"bus\"]\n        sign = network.storage_units.at[su,\"sign\"]\n        for sn in snapshots:\n            network._p_balance[bus,sn].variables.append((sign,network.model.storage_p_dispatch[su,sn]))\n            network._p_balance[bus,sn].variables.append((-sign,network.model.storage_p_store[su,sn]))\n\n    for store in network.stores.index:\n        bus = network.stores.at[store,\"bus\"]\n        sign = network.stores.at[store,\"sign\"]\n        for sn in snapshots:\n            network._p_balance[bus,sn].variables.append((sign,network.model.store_p[store,sn]))", "response": "Define the nodal balance for all elements except the passive\n    branches."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef network_lopf_build_model(network, snapshots=None, skip_pre=False,\n                             formulation=\"angles\", ptdf_tolerance=0.):\n    \"\"\"\n    Build pyomo model for linear optimal power flow for a group of snapshots.\n\n    Parameters\n    ----------\n    snapshots : list or index slice\n        A list of snapshots to optimise, must be a subset of\n        network.snapshots, defaults to network.snapshots\n    skip_pre: bool, default False\n        Skip the preliminary steps of computing topology, calculating\n        dependent values and finding bus controls.\n    formulation : string\n        Formulation of the linear power flow equations to use; must be\n        one of [\"angles\",\"cycles\",\"kirchhoff\",\"ptdf\"]\n    ptdf_tolerance : float\n        Value below which PTDF entries are ignored\n\n    Returns\n    -------\n    network.model\n    \"\"\"\n\n    if not skip_pre:\n        network.determine_network_topology()\n        calculate_dependent_values(network)\n        for sub_network in network.sub_networks.obj:\n            find_slack_bus(sub_network)\n        logger.info(\"Performed preliminary steps\")\n\n\n    snapshots = _as_snapshots(network, snapshots)\n\n    logger.info(\"Building pyomo model using `%s` formulation\", formulation)\n    network.model = ConcreteModel(\"Linear Optimal Power Flow\")\n\n\n    define_generator_variables_constraints(network,snapshots)\n\n    define_storage_variables_constraints(network,snapshots)\n\n    define_store_variables_constraints(network,snapshots)\n\n    define_branch_extension_variables(network,snapshots)\n\n    define_link_flows(network,snapshots)\n\n    define_nodal_balances(network,snapshots)\n\n    define_passive_branch_flows(network,snapshots,formulation,ptdf_tolerance)\n\n    define_passive_branch_constraints(network,snapshots)\n\n    if formulation in [\"angles\", \"kirchhoff\"]:\n        define_nodal_balance_constraints(network,snapshots)\n    elif formulation in [\"ptdf\", \"cycles\"]:\n        define_sub_network_balance_constraints(network,snapshots)\n\n    define_global_constraints(network,snapshots)\n\n    define_linear_objective(network, snapshots)\n\n    #tidy up auxilliary expressions\n    del network._p_balance\n\n    #force solver to also give us the dual prices\n    network.model.dual = Suffix(direction=Suffix.IMPORT)\n\n    return network.model", "response": "Builds a linear optimal power flow model for a group of snapshots."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef network_lopf_prepare_solver(network, solver_name=\"glpk\", solver_io=None):\n\n    network.opt = SolverFactory(solver_name, solver_io=solver_io)\n\n    patch_optsolver_record_memusage_before_solving(network.opt, network)\n\n    if isinstance(network.opt, PersistentSolver):\n        network.opt.set_instance(network.model)\n\n    return network.opt", "response": "Prepare linear optimal power flow solver for linear optimal power flow."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsolve linear optimal power flow for a group of snapshots and extract results.", "response": "def network_lopf_solve(network, snapshots=None, formulation=\"angles\", solver_options={},solver_logfile=None,  keep_files=False,\n                       free_memory={'pyomo'},extra_postprocessing=None):\n    \"\"\"\n    Solve linear optimal power flow for a group of snapshots and extract results.\n\n    Parameters\n    ----------\n    snapshots : list or index slice\n        A list of snapshots to optimise, must be a subset of\n        network.snapshots, defaults to network.snapshots\n    formulation : string\n        Formulation of the linear power flow equations to use; must be one of\n        [\"angles\",\"cycles\",\"kirchhoff\",\"ptdf\"]; must match formulation used for\n        building the model.\n    solver_options : dictionary\n        A dictionary with additional options that get passed to the solver.\n        (e.g. {'threads':2} tells gurobi to use only 2 cpus)\n    solver_logfile : None|string\n        If not None, sets the logfile option of the solver.\n    keep_files : bool, default False\n        Keep the files that pyomo constructs from OPF problem\n        construction, e.g. .lp file - useful for debugging\n    free_memory : set, default {'pyomo'}\n        Any subset of {'pypsa', 'pyomo'}. Allows to stash `pypsa` time-series\n        data away while the solver runs (as a pickle to disk) and/or free\n        `pyomo` data after the solution has been extracted.\n    extra_postprocessing : callable function\n        This function must take three arguments\n        `extra_postprocessing(network,snapshots,duals)` and is called after\n        the model has solved and the results are extracted. It allows the user to\n        extract further information about the solution, such as additional shadow prices.\n\n    Returns\n    -------\n    None\n    \"\"\"\n\n    snapshots = _as_snapshots(network, snapshots)\n\n    logger.info(\"Solving model using %s\", network.opt.name)\n\n    if isinstance(network.opt, PersistentSolver):\n        args = []\n    else:\n        args = [network.model]\n\n    if isinstance(free_memory, string_types):\n        free_memory = {free_memory}\n\n    if 'pypsa' in free_memory:\n        with empty_network(network):\n            network.results = network.opt.solve(*args, suffixes=[\"dual\"], keepfiles=keep_files, logfile=solver_logfile, options=solver_options)\n    else:\n        network.results = network.opt.solve(*args, suffixes=[\"dual\"], keepfiles=keep_files, logfile=solver_logfile, options=solver_options)\n\n    if logger.isEnabledFor(logging.INFO):\n        network.results.write()\n\n    status = network.results[\"Solver\"][0][\"Status\"].key\n    termination_condition = network.results[\"Solver\"][0][\"Termination condition\"].key\n\n    if status == \"ok\" and termination_condition == \"optimal\":\n        logger.info(\"Optimization successful\")\n        extract_optimisation_results(network, snapshots, formulation,\n                                     free_pyomo='pyomo' in free_memory,\n                                     extra_postprocessing=extra_postprocessing)\n    elif status == \"warning\" and termination_condition == \"other\":\n        logger.warning(\"WARNING! Optimization might be sub-optimal. Writing output anyway\")\n        extract_optimisation_results(network, snapshots, formulation,\n                                     free_pyomo='pyomo' in free_memory,\n                                     extra_postprocessing=extra_postprocessing)\n    else:\n        logger.error(\"Optimisation failed with status %s and terminal condition %s\"\n              % (status, termination_condition))\n\n    return status, termination_condition"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef replace_gen(network,gen_to_replace):\n    \n\n    gen = network.generators.loc[gen_to_replace]\n\n    bus_name = \"{} {}\".format(gen[\"bus\"], gen[\"carrier\"])\n    link_name = \"{} converter {} to AC\".format(gen_to_replace,gen[\"carrier\"])\n    store_name = \"{} store {}\".format(gen_to_replace,gen[\"carrier\"])\n\n    network.add(\"Bus\",\n            bus_name,\n            carrier=gen[\"carrier\"])\n\n    network.add(\"Link\",\n            link_name,\n            bus0=bus_name,\n            bus1=gen[\"bus\"],\n            capital_cost=gen[\"capital_cost\"]*gen[\"efficiency\"],\n            p_nom = gen[\"p_nom\"]/gen[\"efficiency\"],\n            p_nom_extendable=gen[\"p_nom_extendable\"],\n            p_nom_max = gen[\"p_nom_max\"]/gen[\"efficiency\"],\n            p_nom_min = gen[\"p_nom_min\"]/gen[\"efficiency\"],\n            p_max_pu = network.generators_t.p_max_pu.loc[:,gen_to_replace] if gen_to_replace in network.generators_t.p_max_pu.columns else gen[\"p_max_pu\"],\n            p_min_pu = network.generators_t.p_min_pu.loc[:,gen_to_replace] if gen_to_replace in network.generators_t.p_min_pu.columns else gen[\"p_min_pu\"],\n            marginal_cost=gen[\"marginal_cost\"]*gen[\"efficiency\"],\n            efficiency=gen[\"efficiency\"])\n\n    network.add(\"Store\",\n            store_name,\n            bus=bus_name,\n            e_nom_min=-float(\"inf\"),\n            e_nom_max=0,\n            e_nom_extendable=True,\n            e_min_pu=1.,\n            e_max_pu=0.)\n\n    network.remove(\"Generator\",gen_to_replace)\n\n    return bus_name, link_name, store_name", "response": "Replace the generator gen_to_replace with a bus for the energy\n    carrier a link for the conversion from the energy carrier to electricity\n    and a store for the electricity\n    and CO2 emissions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreplacing the storage unit su_to_replace with a bus for the energy carrier and two links for the conversion of the energy carrier to and from electricity.", "response": "def replace_su(network,su_to_replace):\n    \"\"\"Replace the storage unit su_to_replace with a bus for the energy\n    carrier, two links for the conversion of the energy carrier to and from electricity,\n    a store to keep track of the depletion of the energy carrier and its\n    CO2 emissions, and a variable generator for the storage inflow.\n    \n    Because the energy size and power size are linked in the storage unit by the max_hours,\n    extra functionality must be added to the LOPF to implement this constraint.\"\"\"\n\n    su = network.storage_units.loc[su_to_replace]\n\n\n    bus_name = \"{} {}\".format(su[\"bus\"],su[\"carrier\"])\n\n    link_1_name = \"{} converter {} to AC\".format(su_to_replace,su[\"carrier\"])\n\n    link_2_name = \"{} converter AC to {}\".format(su_to_replace,su[\"carrier\"])\n\n    store_name = \"{} store {}\".format(su_to_replace,su[\"carrier\"])\n\n    gen_name = \"{} inflow\".format(su_to_replace)\n\n    network.add(\"Bus\",\n            bus_name,\n            carrier=su[\"carrier\"])\n\n    #dispatch link\n    network.add(\"Link\",\n            link_1_name,\n            bus0=bus_name,\n            bus1=su[\"bus\"],\n            capital_cost=su[\"capital_cost\"]*su[\"efficiency_dispatch\"],\n            p_nom = su[\"p_nom\"]/su[\"efficiency_dispatch\"],\n            p_nom_extendable=su[\"p_nom_extendable\"],\n            p_nom_max = su[\"p_nom_max\"]/su[\"efficiency_dispatch\"],\n            p_nom_min = su[\"p_nom_min\"]/su[\"efficiency_dispatch\"],\n            p_max_pu = su[\"p_max_pu\"],\n            marginal_cost=su[\"marginal_cost\"]*su[\"efficiency_dispatch\"],\n            efficiency=su[\"efficiency_dispatch\"])\n\n    #store link\n    network.add(\"Link\",\n            link_2_name,\n            bus1=bus_name,\n            bus0=su[\"bus\"],\n            p_nom = su[\"p_nom\"],\n            p_nom_extendable=su[\"p_nom_extendable\"],\n            p_nom_max = su[\"p_nom_max\"],\n            p_nom_min = su[\"p_nom_min\"],\n            p_max_pu = -su[\"p_min_pu\"],\n            efficiency=su[\"efficiency_store\"])\n\n    \n    if su_to_replace in network.storage_units_t.state_of_charge_set.columns and (~pd.isnull(network.storage_units_t.state_of_charge_set[su_to_replace])).any():\n        e_max_pu = pd.Series(data=1.,index=network.snapshots)\n        e_min_pu = pd.Series(data=0.,index=network.snapshots)\n        non_null = ~pd.isnull(network.storage_units_t.state_of_charge_set[su_to_replace])\n        e_max_pu[non_null] = network.storage_units_t.state_of_charge_set[su_to_replace][non_null]\n        e_min_pu[non_null] = network.storage_units_t.state_of_charge_set[su_to_replace][non_null]\n    else:\n        e_max_pu = 1.\n        e_min_pu = 0.\n    \n    network.add(\"Store\",\n            store_name,\n            bus=bus_name,\n            e_nom=su[\"p_nom\"]*su[\"max_hours\"],\n            e_nom_min=su[\"p_nom_min\"]/su[\"efficiency_dispatch\"]*su[\"max_hours\"],\n            e_nom_max=su[\"p_nom_max\"]/su[\"efficiency_dispatch\"]*su[\"max_hours\"],\n            e_nom_extendable=su[\"p_nom_extendable\"],\n            e_max_pu=e_max_pu,\n            e_min_pu=e_min_pu,\n            standing_loss=su[\"standing_loss\"],\n            e_cyclic=su['cyclic_state_of_charge'],\n            e_initial=su['state_of_charge_initial'])\n\n    network.add(\"Carrier\",\n                \"rain\",\n                co2_emissions=0.)\n\n    #inflow from a variable generator, which can be curtailed (i.e. spilled)\n    inflow_max = network.storage_units_t.inflow[su_to_replace].max()\n\n    if inflow_max == 0.:\n        inflow_pu=0.\n    else:\n        inflow_pu = network.storage_units_t.inflow[su_to_replace]/inflow_max\n\n    print(inflow_pu,type(inflow_pu),type(inflow_pu) in [pd.Series])\n        \n    network.add(\"Generator\",\n               gen_name,\n               bus=bus_name,\n               carrier=\"rain\",\n               p_nom=inflow_max,\n               p_max_pu=inflow_pu)\n\n    if su[\"p_nom_extendable\"]:\n        ratio2 = su[\"max_hours\"]\n        ratio1 = ratio2*su[\"efficiency_dispatch\"]\n        def extra_functionality(network,snapshots):\n            model = network.model\n            model.store_fix_1 = Constraint(rule=lambda model : model.store_e_nom[store_name] == model.link_p_nom[link_1_name]*ratio1)\n            model.store_fix_2 = Constraint(rule=lambda model : model.store_e_nom[store_name] == model.link_p_nom[link_2_name]*ratio2)\n\n    else:\n        extra_functionality=None\n\n    network.remove(\"StorageUnit\",su_to_replace)\n\n\n    return bus_name, link_1_name, link_2_name, store_name, gen_name, extra_functionality"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef busmap_by_stubs(network, matching_attrs=None):\n\n    busmap = pd.Series(network.buses.index, network.buses.index)\n\n    G = network.graph()\n\n    def attrs_match(u, v):\n        return (matching_attrs is None or\n                (network.buses.loc[u, matching_attrs] ==\n                 network.buses.loc[v, matching_attrs]).all())\n\n    while True:\n        stubs = []\n        for u in G.node:\n            neighbours = list(G.adj[u].keys())\n            if len(neighbours) == 1:\n                v, = neighbours\n                if attrs_match(u, v):\n                    busmap[busmap == u] = v\n                    stubs.append(u)\n        G.remove_nodes_from(stubs)\n        if len(stubs) == 0:\n            break\n    return busmap", "response": "Create a busmap by reducing stubs and stubby trees\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stubs_clustering(network,use_reduced_coordinates=True, line_length_factor=1.0):\n\n    busmap = busmap_by_stubs(network)\n\n    #reset coordinates to the new reduced guys, rather than taking an average\n    if use_reduced_coordinates:\n        # TODO : FIX THIS HACK THAT HAS UNEXPECTED SIDE-EFFECTS,\n        # i.e. network is changed in place!!\n        network.buses.loc[busmap.index,['x','y']] = network.buses.loc[busmap,['x','y']].values\n\n    return get_clustering_from_busmap(network, busmap, line_length_factor=line_length_factor)", "response": "Cluster network by reducing stubs and stubby trees\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_switchable_as_dense(network, component, attr, snapshots=None, inds=None):\n\n    df = network.df(component)\n    pnl = network.pnl(component)\n\n    index = df.index\n\n    varying_i = pnl[attr].columns\n    fixed_i = df.index.difference(varying_i)\n\n    if inds is not None:\n        index = index.intersection(inds)\n        varying_i = varying_i.intersection(inds)\n        fixed_i = fixed_i.intersection(inds)\n    if snapshots is None:\n        snapshots = network.snapshots\n    return (pd.concat([\n        pd.DataFrame(np.repeat([df.loc[fixed_i, attr].values], len(snapshots), axis=0),\n                     index=snapshots, columns=fixed_i),\n        pnl[attr].loc[snapshots, varying_i]\n    ], axis=1, sort=False).reindex(columns=index))", "response": "Returns a Dataframe for a time - varying component attribute filled in with the default values for all\n    non - time - varying components filled in with the default values for all\n    non - time - varying component attribute."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef allocate_series_dataframes(network, series):\n\n    for component, attributes in iteritems(series):\n\n        df = network.df(component)\n        pnl = network.pnl(component)\n\n        for attr in attributes:\n            pnl[attr] = pnl[attr].reindex(columns=df.index,\n                                          fill_value=network.components[component][\"attrs\"].at[attr,\"default\"])", "response": "Populate time - varying outputs with default values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef zsum(s, *args, **kwargs):\n    return 0 if s.empty else s.sum(*args, **kwargs)", "response": "Compute the sum of the values in a Series."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfulling non-linear power flow for generic network. Parameters ---------- snapshots : list-like|single snapshot A subset or an elements of network.snapshots on which to run the power flow, defaults to network.snapshots skip_pre: bool, default False Skip the preliminary steps of computing topology, calculating dependent values and finding bus controls. x_tol: float Tolerance for Newton-Raphson power flow. use_seed : bool, default False Use a seed for the initial guess for the Newton-Raphson algorithm. Returns ------- Dictionary with keys 'n_iter', 'converged', 'error' and dataframe values indicating number of iterations, convergence status, and iteration error for each snapshot (rows) and sub_network (columns)", "response": "def network_pf(network, snapshots=None, skip_pre=False, x_tol=1e-6, use_seed=False):\n    \"\"\"\n    Full non-linear power flow for generic network.\n\n    Parameters\n    ----------\n    snapshots : list-like|single snapshot\n        A subset or an elements of network.snapshots on which to run\n        the power flow, defaults to network.snapshots\n    skip_pre: bool, default False\n        Skip the preliminary steps of computing topology, calculating dependent values and finding bus controls.\n    x_tol: float\n        Tolerance for Newton-Raphson power flow.\n    use_seed : bool, default False\n        Use a seed for the initial guess for the Newton-Raphson algorithm.\n\n    Returns\n    -------\n    Dictionary with keys 'n_iter', 'converged', 'error' and dataframe\n    values indicating number of iterations, convergence status, and\n    iteration error for each snapshot (rows) and sub_network (columns)\n    \"\"\"\n\n    return _network_prepare_and_run_pf(network, snapshots, skip_pre, linear=False, x_tol=x_tol, use_seed=use_seed)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef newton_raphson_sparse(f, guess, dfdx, x_tol=1e-10, lim_iter=100):\n\n    converged = False\n    n_iter = 0\n    F = f(guess)\n    diff = norm(F,np.Inf)\n\n    logger.debug(\"Error at iteration %d: %f\", n_iter, diff)\n\n    while diff > x_tol and n_iter < lim_iter:\n\n        n_iter +=1\n\n        guess = guess - spsolve(dfdx(guess),F)\n\n        F = f(guess)\n        diff = norm(F,np.Inf)\n\n        logger.debug(\"Error at iteration %d: %f\", n_iter, diff)\n\n    if diff > x_tol:\n        logger.warning(\"Warning, we didn't reach the required tolerance within %d iterations, error is at %f. See the section \\\"Troubleshooting\\\" in the documentation for tips to fix this. \", n_iter, diff)\n    elif not np.isnan(diff):\n        converged = True\n\n    return guess, n_iter, diff, converged", "response": "Solve f with initial guess and dfdx return a sparse Jacobian."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nperform a non - linear load - flow on a single sub - network.", "response": "def sub_network_pf(sub_network, snapshots=None, skip_pre=False, x_tol=1e-6, use_seed=False):\n    \"\"\"\n    Non-linear power flow for connected sub-network.\n\n    Parameters\n    ----------\n    snapshots : list-like|single snapshot\n        A subset or an elements of network.snapshots on which to run\n        the power flow, defaults to network.snapshots\n    skip_pre: bool, default False\n        Skip the preliminary steps of computing topology, calculating dependent values and finding bus controls.\n    x_tol: float\n        Tolerance for Newton-Raphson power flow.\n    use_seed : bool, default False\n        Use a seed for the initial guess for the Newton-Raphson algorithm.\n\n    Returns\n    -------\n    Tuple of three pandas.Series indicating number of iterations,\n    remaining error, and convergence status for each snapshot\n    \"\"\"\n\n    snapshots = _as_snapshots(sub_network.network, snapshots)\n    logger.info(\"Performing non-linear load-flow on {} sub-network {} for snapshots {}\".format(sub_network.network.sub_networks.at[sub_network.name,\"carrier\"], sub_network, snapshots))\n\n    # _sub_network_prepare_pf(sub_network, snapshots, skip_pre, calculate_Y)\n    network = sub_network.network\n\n    if not skip_pre:\n        calculate_dependent_values(network)\n        find_bus_controls(sub_network)\n        _allocate_pf_outputs(network, linear=False)\n\n\n    # get indices for the components on this subnetwork\n    branches_i = sub_network.branches_i()\n    buses_o = sub_network.buses_o\n\n    if not skip_pre and len(branches_i) > 0:\n        calculate_Y(sub_network, skip_pre=True)\n\n    for n in (\"q\", \"p\"):\n        # allow all one ports to dispatch as set\n        for c in sub_network.iterate_components(network.controllable_one_port_components):\n            c_n_set = get_switchable_as_dense(network, c.name, n + '_set', snapshots, c.ind)\n            c.pnl[n].loc[snapshots, c.ind] = c_n_set\n\n        # set the power injection at each node from controllable components\n        network.buses_t[n].loc[snapshots, buses_o] = \\\n            sum([((c.pnl[n].loc[snapshots, c.ind] * c.df.loc[c.ind, 'sign'])\n                  .groupby(c.df.loc[c.ind, 'bus'], axis=1).sum()\n                  .reindex(columns=buses_o, fill_value=0.))\n                 for c in sub_network.iterate_components(network.controllable_one_port_components)])\n\n        if n == \"p\":\n            network.buses_t[n].loc[snapshots, buses_o] += sum(\n                [(- c.pnl[n+str(i)].loc[snapshots].groupby(c.df[\"bus\"+str(i)], axis=1).sum()\n                  .reindex(columns=buses_o, fill_value=0))\n                 for c in network.iterate_components(network.controllable_branch_components)\n                 for i in [int(col[3:]) for col in c.df.columns if col[:3] == \"bus\"]])\n\n    def f(guess):\n        network.buses_t.v_ang.loc[now,sub_network.pvpqs] = guess[:len(sub_network.pvpqs)]\n\n        network.buses_t.v_mag_pu.loc[now,sub_network.pqs] = guess[len(sub_network.pvpqs):]\n\n        v_mag_pu = network.buses_t.v_mag_pu.loc[now,buses_o]\n        v_ang = network.buses_t.v_ang.loc[now,buses_o]\n        V = v_mag_pu*np.exp(1j*v_ang)\n\n        mismatch = V*np.conj(sub_network.Y*V) - s\n\n        F = r_[mismatch.real[1:],mismatch.imag[1+len(sub_network.pvs):]]\n\n        return F\n\n\n    def dfdx(guess):\n\n        network.buses_t.v_ang.loc[now,sub_network.pvpqs] = guess[:len(sub_network.pvpqs)]\n\n        network.buses_t.v_mag_pu.loc[now,sub_network.pqs] = guess[len(sub_network.pvpqs):]\n\n        v_mag_pu = network.buses_t.v_mag_pu.loc[now,buses_o]\n        v_ang = network.buses_t.v_ang.loc[now,buses_o]\n\n        V = v_mag_pu*np.exp(1j*v_ang)\n\n        index = r_[:len(buses_o)]\n\n        #make sparse diagonal matrices\n        V_diag = csr_matrix((V,(index,index)))\n        V_norm_diag = csr_matrix((V/abs(V),(index,index)))\n        I_diag = csr_matrix((sub_network.Y*V,(index,index)))\n\n        dS_dVa = 1j*V_diag*np.conj(I_diag - sub_network.Y*V_diag)\n\n        dS_dVm = V_norm_diag*np.conj(I_diag) + V_diag * np.conj(sub_network.Y*V_norm_diag)\n\n        J00 = dS_dVa[1:,1:].real\n        J01 = dS_dVm[1:,1+len(sub_network.pvs):].real\n        J10 = dS_dVa[1+len(sub_network.pvs):,1:].imag\n        J11 = dS_dVm[1+len(sub_network.pvs):,1+len(sub_network.pvs):].imag\n\n        J = svstack([\n            shstack([J00, J01]),\n            shstack([J10, J11])\n        ], format=\"csr\")\n\n        return J\n\n\n    #Set what we know: slack V and v_mag_pu for PV buses\n    v_mag_pu_set = get_switchable_as_dense(network, 'Bus', 'v_mag_pu_set', snapshots)\n    network.buses_t.v_mag_pu.loc[snapshots,sub_network.pvs] = v_mag_pu_set.loc[:,sub_network.pvs]\n    network.buses_t.v_mag_pu.loc[snapshots,sub_network.slack_bus] = v_mag_pu_set.loc[:,sub_network.slack_bus]\n    network.buses_t.v_ang.loc[snapshots,sub_network.slack_bus] = 0.\n\n    if not use_seed:\n        network.buses_t.v_mag_pu.loc[snapshots,sub_network.pqs] = 1.\n        network.buses_t.v_ang.loc[snapshots,sub_network.pvpqs] = 0.\n\n    ss = np.empty((len(snapshots), len(buses_o)), dtype=np.complex)\n    roots = np.empty((len(snapshots), len(sub_network.pvpqs) + len(sub_network.pqs)))\n    iters = pd.Series(0, index=snapshots)\n    diffs = pd.Series(index=snapshots)\n    convs = pd.Series(False, index=snapshots)\n    for i, now in enumerate(snapshots):\n        p = network.buses_t.p.loc[now,buses_o]\n        q = network.buses_t.q.loc[now,buses_o]\n        ss[i] = s = p + 1j*q\n\n        #Make a guess for what we don't know: V_ang for PV and PQs and v_mag_pu for PQ buses\n        guess = r_[network.buses_t.v_ang.loc[now,sub_network.pvpqs],network.buses_t.v_mag_pu.loc[now,sub_network.pqs]]\n\n        #Now try and solve\n        start = time.time()\n        roots[i], n_iter, diff, converged = newton_raphson_sparse(f,guess,dfdx,x_tol=x_tol)\n        logger.info(\"Newton-Raphson solved in %d iterations with error of %f in %f seconds\", n_iter,diff,time.time()-start)\n        iters[now] = n_iter\n        diffs[now] = diff\n        convs[now] = converged\n\n\n    #now set everything\n    network.buses_t.v_ang.loc[snapshots,sub_network.pvpqs] = roots[:,:len(sub_network.pvpqs)]\n    network.buses_t.v_mag_pu.loc[snapshots,sub_network.pqs] = roots[:,len(sub_network.pvpqs):]\n\n    v_mag_pu = network.buses_t.v_mag_pu.loc[snapshots,buses_o].values\n    v_ang = network.buses_t.v_ang.loc[snapshots,buses_o].values\n\n    V = v_mag_pu*np.exp(1j*v_ang)\n\n    #add voltages to branches\n    buses_indexer = buses_o.get_indexer\n    branch_bus0 = []; branch_bus1 = []\n    for c in sub_network.iterate_components(network.passive_branch_components):\n        branch_bus0 += list(c.df.loc[c.ind, 'bus0'])\n        branch_bus1 += list(c.df.loc[c.ind, 'bus1'])\n    v0 = V[:,buses_indexer(branch_bus0)]\n    v1 = V[:,buses_indexer(branch_bus1)]\n\n    i0 = np.empty((len(snapshots), sub_network.Y0.shape[0]), dtype=np.complex)\n    i1 = np.empty((len(snapshots), sub_network.Y1.shape[0]), dtype=np.complex)\n    for i, now in enumerate(snapshots):\n        i0[i] = sub_network.Y0*V[i]\n        i1[i] = sub_network.Y1*V[i]\n\n    s0 = pd.DataFrame(v0*np.conj(i0), columns=branches_i, index=snapshots)\n    s1 = pd.DataFrame(v1*np.conj(i1), columns=branches_i, index=snapshots)\n    for c in sub_network.iterate_components(network.passive_branch_components):\n        s0t = s0.loc[:,c.name]\n        s1t = s1.loc[:,c.name]\n        c.pnl.p0.loc[snapshots,s0t.columns] = s0t.values.real\n        c.pnl.q0.loc[snapshots,s0t.columns] = s0t.values.imag\n        c.pnl.p1.loc[snapshots,s1t.columns] = s1t.values.real\n        c.pnl.q1.loc[snapshots,s1t.columns] = s1t.values.imag\n\n    s_calc = np.empty((len(snapshots), len(buses_o)), dtype=np.complex)\n    for i in np.arange(len(snapshots)):\n        s_calc[i] = V[i]*np.conj(sub_network.Y*V[i])\n    slack_index = buses_o.get_loc(sub_network.slack_bus)\n    network.buses_t.p.loc[snapshots,sub_network.slack_bus] = s_calc[:,slack_index].real\n    network.buses_t.q.loc[snapshots,sub_network.slack_bus] = s_calc[:,slack_index].imag\n    network.buses_t.q.loc[snapshots,sub_network.pvs] = s_calc[:,buses_indexer(sub_network.pvs)].imag\n\n    #set shunt impedance powers\n    shunt_impedances_i = sub_network.shunt_impedances_i()\n    if len(shunt_impedances_i):\n        #add voltages\n        shunt_impedances_v_mag_pu = v_mag_pu[:,buses_indexer(network.shunt_impedances.loc[shunt_impedances_i, 'bus'])]\n        network.shunt_impedances_t.p.loc[snapshots,shunt_impedances_i] = (shunt_impedances_v_mag_pu**2)*network.shunt_impedances.loc[shunt_impedances_i, 'g_pu'].values\n        network.shunt_impedances_t.q.loc[snapshots,shunt_impedances_i] = (shunt_impedances_v_mag_pu**2)*network.shunt_impedances.loc[shunt_impedances_i, 'b_pu'].values\n\n    #let slack generator take up the slack\n    network.generators_t.p.loc[snapshots,sub_network.slack_generator] += network.buses_t.p.loc[snapshots,sub_network.slack_bus] - ss[:,slack_index].real\n    network.generators_t.q.loc[snapshots,sub_network.slack_generator] += network.buses_t.q.loc[snapshots,sub_network.slack_bus] - ss[:,slack_index].imag\n\n    #set the Q of the PV generators\n    network.generators_t.q.loc[snapshots,network.buses.loc[sub_network.pvs, \"generator\"]] += np.asarray(network.buses_t.q.loc[snapshots,sub_network.pvs] - ss[:,buses_indexer(sub_network.pvs)].imag)\n\n    return iters, diffs, convs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate line electrical parameters x r b g from standard types.", "response": "def apply_line_types(network):\n    \"\"\"Calculate line electrical parameters x, r, b, g from standard\n    types.\n\n    \"\"\"\n\n    lines_with_types_b = network.lines.type != \"\"\n    if lines_with_types_b.zsum() == 0:\n        return\n\n    missing_types = (pd.Index(network.lines.loc[lines_with_types_b, 'type'].unique())\n                     .difference(network.line_types.index))\n    assert missing_types.empty, (\"The type(s) {} do(es) not exist in network.line_types\"\n                                 .format(\", \".join(missing_types)))\n\n    # Get a copy of the lines data\n    l = (network.lines.loc[lines_with_types_b, [\"type\", \"length\", \"num_parallel\"]]\n         .join(network.line_types, on='type'))\n\n    for attr in [\"r\",\"x\"]:\n        l[attr] = l[attr + \"_per_length\"] * l[\"length\"] / l[\"num_parallel\"]\n    l[\"b\"] = 2*np.pi*1e-9*l[\"f_nom\"] * l[\"c_per_length\"] * l[\"length\"] * l[\"num_parallel\"]\n\n    # now set calculated values on live lines\n    for attr in [\"r\", \"x\", \"b\"]:\n        network.lines.loc[lines_with_types_b, attr] = l[attr]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates transformer electrical parameters x r b g from transformer types.", "response": "def apply_transformer_types(network):\n    \"\"\"Calculate transformer electrical parameters x, r, b, g from\n    standard types.\n\n    \"\"\"\n\n    trafos_with_types_b = network.transformers.type != \"\"\n    if trafos_with_types_b.zsum() == 0:\n        return\n\n    missing_types = (pd.Index(network.transformers.loc[trafos_with_types_b, 'type'].unique())\n                     .difference(network.transformer_types.index))\n    assert missing_types.empty, (\"The type(s) {} do(es) not exist in network.transformer_types\"\n                                 .format(\", \".join(missing_types)))\n\n    # Get a copy of the transformers data\n    # (joining pulls in \"phase_shift\", \"s_nom\", \"tap_side\" from TransformerType)\n    t = (network.transformers.loc[trafos_with_types_b, [\"type\", \"tap_position\", \"num_parallel\"]]\n         .join(network.transformer_types, on='type'))\n\n    t[\"r\"] = t[\"vscr\"] /100.\n    t[\"x\"] = np.sqrt((t[\"vsc\"]/100.)**2 - t[\"r\"]**2)\n\n    #NB: b and g are per unit of s_nom\n    t[\"g\"] = t[\"pfe\"]/(1000. * t[\"s_nom\"])\n\n    #for some bizarre reason, some of the standard types in pandapower have i0^2 < g^2\n    t[\"b\"] = - np.sqrt(((t[\"i0\"]/100.)**2 - t[\"g\"]**2).clip(lower=0))\n\n    for attr in [\"r\",\"x\"]:\n        t[attr] /= t[\"num_parallel\"]\n\n    for attr in [\"b\",\"g\"]:\n        t[attr] *= t[\"num_parallel\"]\n\n    #deal with tap positions\n\n    t[\"tap_ratio\"] = 1. + (t[\"tap_position\"] - t[\"tap_neutral\"]) * (t[\"tap_step\"]/100.)\n\n    # now set calculated values on live transformers\n    for attr in [\"r\", \"x\", \"g\", \"b\", \"phase_shift\", \"s_nom\", \"tap_side\", \"tap_ratio\"]:\n        network.transformers.loc[trafos_with_types_b, attr] = t[attr]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wye_to_delta(z1,z2,z3):\n    summand = z1*z2 + z2*z3 + z3*z1\n    return (summand/z2,summand/z1,summand/z3)", "response": "Convert a WYE delta to a delta."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting given T - model parameters to PI - model parameters using wye - delta transformation", "response": "def apply_transformer_t_model(network):\n    \"\"\"Convert given T-model parameters to PI-model parameters using wye-delta transformation\"\"\"\n\n    z_series = network.transformers.r_pu + 1j*network.transformers.x_pu\n    y_shunt = network.transformers.g_pu + 1j*network.transformers.b_pu\n\n    ts_b = (network.transformers.model == \"t\") & (y_shunt != 0.)\n\n    if ts_b.zsum() == 0:\n        return\n\n    za,zb,zc = wye_to_delta(z_series.loc[ts_b]/2,z_series.loc[ts_b]/2,1/y_shunt.loc[ts_b])\n\n    network.transformers.loc[ts_b,\"r_pu\"] = zc.real\n    network.transformers.loc[ts_b,\"x_pu\"] = zc.imag\n    network.transformers.loc[ts_b,\"g_pu\"] = (2/za).real\n    network.transformers.loc[ts_b,\"b_pu\"] = (2/za).imag"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calculate_dependent_values(network):\n\n    apply_line_types(network)\n    apply_transformer_types(network)\n\n    network.lines[\"v_nom\"] = network.lines.bus0.map(network.buses.v_nom)\n\n    network.lines[\"x_pu\"] = network.lines.x/(network.lines.v_nom**2)\n    network.lines[\"r_pu\"] = network.lines.r/(network.lines.v_nom**2)\n    network.lines[\"b_pu\"] = network.lines.b*network.lines.v_nom**2\n    network.lines[\"g_pu\"] = network.lines.g*network.lines.v_nom**2\n    network.lines[\"x_pu_eff\"] = network.lines[\"x_pu\"]\n    network.lines[\"r_pu_eff\"] = network.lines[\"r_pu\"]\n\n\n    #convert transformer impedances from base power s_nom to base = 1 MVA\n    network.transformers[\"x_pu\"] = network.transformers.x/network.transformers.s_nom\n    network.transformers[\"r_pu\"] = network.transformers.r/network.transformers.s_nom\n    network.transformers[\"b_pu\"] = network.transformers.b*network.transformers.s_nom\n    network.transformers[\"g_pu\"] = network.transformers.g*network.transformers.s_nom\n    network.transformers[\"x_pu_eff\"] = network.transformers[\"x_pu\"]* network.transformers[\"tap_ratio\"]\n    network.transformers[\"r_pu_eff\"] = network.transformers[\"r_pu\"]* network.transformers[\"tap_ratio\"]\n\n    apply_transformer_t_model(network)\n\n    network.shunt_impedances[\"v_nom\"] = network.shunt_impedances[\"bus\"].map(network.buses.v_nom)\n    network.shunt_impedances[\"b_pu\"] = network.shunt_impedances.b*network.shunt_impedances.v_nom**2\n    network.shunt_impedances[\"g_pu\"] = network.shunt_impedances.g*network.shunt_impedances.v_nom**2", "response": "Calculate per unit impedances and append voltages to lines and shunt impedances."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the slack bus in a connected sub - network.", "response": "def find_slack_bus(sub_network):\n    \"\"\"Find the slack bus in a connected sub-network.\"\"\"\n\n    gens = sub_network.generators()\n\n    if len(gens) == 0:\n        logger.warning(\"No generators in sub-network {}, better hope power is already balanced\".format(sub_network.name))\n        sub_network.slack_generator = None\n        sub_network.slack_bus = sub_network.buses_i()[0]\n\n    else:\n\n        slacks = gens[gens.control == \"Slack\"].index\n\n        if len(slacks) == 0:\n            sub_network.slack_generator = gens.index[0]\n            sub_network.network.generators.loc[sub_network.slack_generator,\"control\"] = \"Slack\"\n            logger.debug(\"No slack generator found in sub-network {}, using {} as the slack generator\".format(sub_network.name, sub_network.slack_generator))\n\n        elif len(slacks) == 1:\n            sub_network.slack_generator = slacks[0]\n        else:\n            sub_network.slack_generator = slacks[0]\n            sub_network.network.generators.loc[slacks[1:],\"control\"] = \"PV\"\n            logger.debug(\"More than one slack generator found in sub-network {}, using {} as the slack generator\".format(sub_network.name, sub_network.slack_generator))\n\n        sub_network.slack_bus = gens.bus[sub_network.slack_generator]\n\n    #also put it into the dataframe\n    sub_network.network.sub_networks.at[sub_network.name,\"slack_bus\"] = sub_network.slack_bus\n\n    logger.info(\"Slack bus for sub-network {} is {}\".format(sub_network.name, sub_network.slack_bus))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding slack and all PV and PQ buses for a sub_network. This function also fixes sub_network. buses_o a DataFrame ordered by control type.", "response": "def find_bus_controls(sub_network):\n    \"\"\"Find slack and all PV and PQ buses for a sub_network.\n    This function also fixes sub_network.buses_o, a DataFrame\n    ordered by control type.\"\"\"\n\n    network = sub_network.network\n\n    find_slack_bus(sub_network)\n\n    gens = sub_network.generators()\n    buses_i = sub_network.buses_i()\n\n    #default bus control is PQ\n    network.buses.loc[buses_i, \"control\"] = \"PQ\"\n\n    #find all buses with one or more gens with PV\n    pvs = gens[gens.control == 'PV'].index.to_series()\n    if len(pvs) > 0:\n        pvs = pvs.groupby(gens.bus).first()\n        network.buses.loc[pvs.index, \"control\"] = \"PV\"\n        network.buses.loc[pvs.index, \"generator\"] = pvs\n\n    network.buses.loc[sub_network.slack_bus, \"control\"] = \"Slack\"\n    network.buses.loc[sub_network.slack_bus, \"generator\"] = sub_network.slack_generator\n\n    buses_control = network.buses.loc[buses_i, \"control\"]\n    sub_network.pvs = buses_control.index[buses_control == \"PV\"]\n    sub_network.pqs = buses_control.index[buses_control == \"PQ\"]\n\n    sub_network.pvpqs = sub_network.pvs.append(sub_network.pqs)\n\n    # order buses\n    sub_network.buses_o = sub_network.pvpqs.insert(0, sub_network.slack_bus)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calculate_B_H(sub_network,skip_pre=False):\n\n    network = sub_network.network\n\n    if not skip_pre:\n        calculate_dependent_values(network)\n        find_bus_controls(sub_network)\n\n    if network.sub_networks.at[sub_network.name,\"carrier\"] == \"DC\":\n        attribute=\"r_pu_eff\"\n    else:\n        attribute=\"x_pu_eff\"\n\n    #following leans heavily on pypower.makeBdc\n\n    #susceptances\n    b = 1./np.concatenate([(c.df.loc[c.ind, attribute]).values \\\n                           for c in sub_network.iterate_components(network.passive_branch_components)])\n\n\n    if np.isnan(b).any():\n        logger.warning(\"Warning! Some series impedances are zero - this will cause a singularity in LPF!\")\n    b_diag = csr_matrix((b, (r_[:len(b)], r_[:len(b)])))\n\n    #incidence matrix\n    sub_network.K = sub_network.incidence_matrix(busorder=sub_network.buses_o)\n\n    sub_network.H = b_diag*sub_network.K.T\n\n    #weighted Laplacian\n    sub_network.B = sub_network.K * sub_network.H\n\n\n    sub_network.p_branch_shift = -b*np.concatenate([(c.df.loc[c.ind, \"phase_shift\"]).values*np.pi/180. if c.name == \"Transformer\"\n                                                    else np.zeros((len(c.ind),))\n                                                    for c in sub_network.iterate_components(network.passive_branch_components)])\n\n    sub_network.p_bus_shift = sub_network.K * sub_network.p_branch_shift", "response": "Calculate B and H matrices for AC or DC sub - networks."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the Power Transfer Distribution Factor for a single node.", "response": "def calculate_PTDF(sub_network,skip_pre=False):\n    \"\"\"\n    Calculate the Power Transfer Distribution Factor (PTDF) for\n    sub_network.\n\n    Sets sub_network.PTDF as a (dense) numpy array.\n\n    Parameters\n    ----------\n    sub_network : pypsa.SubNetwork\n    skip_pre: bool, default False\n        Skip the preliminary steps of computing topology, calculating dependent values,\n        finding bus controls and computing B and H.\n\n    \"\"\"\n\n    if not skip_pre:\n        calculate_B_H(sub_network)\n\n    #calculate inverse of B with slack removed\n\n    n_pvpq = len(sub_network.pvpqs)\n    index = np.r_[:n_pvpq]\n\n    I = csc_matrix((np.ones(n_pvpq), (index, index)))\n\n    B_inverse = spsolve(sub_network.B[1:, 1:],I)\n\n    #exception for two-node networks, where B_inverse is a 1d array\n    if issparse(B_inverse):\n        B_inverse = B_inverse.toarray()\n    elif B_inverse.shape == (1,):\n        B_inverse = B_inverse.reshape((1,1))\n\n    #add back in zeroes for slack\n    B_inverse = np.hstack((np.zeros((n_pvpq,1)),B_inverse))\n    B_inverse = np.vstack((np.zeros(n_pvpq+1),B_inverse))\n\n    sub_network.PTDF = sub_network.H*B_inverse"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calculate_Y(sub_network,skip_pre=False):\n\n    if not skip_pre:\n        calculate_dependent_values(sub_network.network)\n\n    if sub_network.network.sub_networks.at[sub_network.name,\"carrier\"] != \"AC\":\n        logger.warning(\"Non-AC networks not supported for Y!\")\n        return\n\n    branches = sub_network.branches()\n    buses_o = sub_network.buses_o\n\n    network = sub_network.network\n\n    #following leans heavily on pypower.makeYbus\n    #Copyright Richard Lincoln, Ray Zimmerman, BSD-style licence\n\n    num_branches = len(branches)\n    num_buses = len(buses_o)\n\n    y_se = 1/(branches[\"r_pu\"] + 1.j*branches[\"x_pu\"])\n\n    y_sh = branches[\"g_pu\"]+ 1.j*branches[\"b_pu\"]\n\n    tau = branches[\"tap_ratio\"].fillna(1.)\n\n    #catch some transformers falsely set with tau = 0 by pypower\n    tau[tau==0] = 1.\n\n    #define the HV tap ratios\n    tau_hv = pd.Series(1.,branches.index)\n    tau_hv[branches.tap_side==0] = tau[branches.tap_side==0]\n\n    #define the LV tap ratios\n    tau_lv = pd.Series(1.,branches.index)\n    tau_lv[branches.tap_side==1] = tau[branches.tap_side==1]\n\n\n    phase_shift = np.exp(1.j*branches[\"phase_shift\"].fillna(0.)*np.pi/180.)\n\n    #build the admittance matrix elements for each branch\n    Y11 = (y_se + 0.5*y_sh)/tau_lv**2\n    Y10 = -y_se/tau_lv/tau_hv/phase_shift\n    Y01 = -y_se/tau_lv/tau_hv/np.conj(phase_shift)\n    Y00 = (y_se + 0.5*y_sh)/tau_hv**2\n\n    #bus shunt impedances\n    b_sh = network.shunt_impedances.b_pu.groupby(network.shunt_impedances.bus).sum().reindex(buses_o, fill_value = 0.)\n    g_sh = network.shunt_impedances.g_pu.groupby(network.shunt_impedances.bus).sum().reindex(buses_o, fill_value = 0.)\n    Y_sh = g_sh + 1.j*b_sh\n\n    #get bus indices\n    bus0 = buses_o.get_indexer(branches.bus0)\n    bus1 = buses_o.get_indexer(branches.bus1)\n\n    #connection matrices\n    C0 = csr_matrix((ones(num_branches), (np.arange(num_branches), bus0)), (num_branches, num_buses))\n    C1 = csr_matrix((ones(num_branches), (np.arange(num_branches), bus1)), (num_branches, num_buses))\n\n    #build Y{0,1} such that Y{0,1} * V is the vector complex branch currents\n\n    i = r_[np.arange(num_branches), np.arange(num_branches)]\n    sub_network.Y0 = csr_matrix((r_[Y00,Y01],(i,r_[bus0,bus1])), (num_branches,num_buses))\n    sub_network.Y1 = csr_matrix((r_[Y10,Y11],(i,r_[bus0,bus1])), (num_branches,num_buses))\n\n    #now build bus admittance matrix\n    sub_network.Y = C0.T * sub_network.Y0 + C1.T * sub_network.Y1 + \\\n       csr_matrix((Y_sh, (np.arange(num_buses), np.arange(num_buses))))", "response": "Calculate the Y - bus admittance matrices for AC sub - networks."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef aggregate_multi_graph(sub_network):\n\n    network = sub_network.network\n\n    count = 0\n    seen = []\n    graph = sub_network.graph()\n    for u,v in graph.edges():\n        if (u,v) in seen:\n            continue\n        line_objs = list(graph.adj[u][v].keys())\n        if len(line_objs) > 1:\n            lines = network.lines.loc[[l[1] for l in line_objs]]\n            aggregated = {}\n\n            attr_inv = [\"x\",\"r\"]\n            attr_sum = [\"s_nom\",\"b\",\"g\",\"s_nom_max\",\"s_nom_min\"]\n            attr_mean = [\"capital_cost\",\"length\",\"terrain_factor\"]\n\n            for attr in attr_inv:\n                aggregated[attr] = 1./(1./lines[attr]).sum()\n\n            for attr in attr_sum:\n                aggregated[attr] = lines[attr].sum()\n\n            for attr in attr_mean:\n                aggregated[attr] = lines[attr].mean()\n\n            count += len(line_objs) - 1\n\n            #remove all but first line\n            for line in line_objs[1:]:\n                network.remove(\"Line\",line[1])\n\n            rep = line_objs[0]\n\n            for key,value in aggregated.items():\n                setattr(rep,key,value)\n\n            seen.append((u,v))\n\n    logger.info(\"Removed %d excess lines from sub-network %s and replaced with aggregated lines\", count,sub_network.name)", "response": "Aggregate branches between same buses and replace with a single\nBranch with aggregated properties."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding the spanning tree of the network and choose the node with the highest degree as a central tree slack and then see for each node with the highest degree as a central tree slack and then see for each node with the highest degree as a central tree slack and then see for each node with each node go through each branch.", "response": "def find_tree(sub_network, weight='x_pu'):\n    \"\"\"Get the spanning tree of the graph, choose the node with the\n    highest degree as a central \"tree slack\" and then see for each\n    branch which paths from the slack to each node go through the\n    branch.\n\n    \"\"\"\n\n    branches_bus0 = sub_network.branches()[\"bus0\"]\n    branches_i = branches_bus0.index\n    buses_i = sub_network.buses_i()\n\n    graph = sub_network.graph(weight=weight, inf_weight=1.)\n    sub_network.tree = nx.minimum_spanning_tree(graph)\n\n    #find bus with highest degree to use as slack\n    tree_slack_bus, slack_degree = max(degree(sub_network.tree), key=itemgetter(1))\n    logger.info(\"Tree slack bus is %s with degree %d.\", tree_slack_bus, slack_degree)\n\n    #determine which buses are supplied in tree through branch from slack\n\n    #matrix to store tree structure\n    sub_network.T = dok_matrix((len(branches_i),len(buses_i)))\n\n    for j,bus in enumerate(buses_i):\n        path = nx.shortest_path(sub_network.tree,bus,tree_slack_bus)\n        for i in range(len(path)-1):\n            branch = next(iterkeys(graph[path[i]][path[i+1]]))\n            branch_i = branches_i.get_loc(branch)\n            sign = +1 if branches_bus0.iat[branch_i] == path[i] else -1\n            sub_network.T[branch_i,j] = sign"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_cycles(sub_network, weight='x_pu'):\n    branches_bus0 = sub_network.branches()[\"bus0\"]\n    branches_i = branches_bus0.index\n\n    #reduce to a non-multi-graph for cycles with > 2 edges\n    mgraph = sub_network.graph(weight=weight, inf_weight=False)\n    graph = nx.OrderedGraph(mgraph)\n\n    cycles = nx.cycle_basis(graph)\n\n    #number of 2-edge cycles\n    num_multi = len(mgraph.edges()) - len(graph.edges())\n\n    sub_network.C = dok_matrix((len(branches_bus0),len(cycles)+num_multi))\n\n    for j,cycle in enumerate(cycles):\n\n        for i in range(len(cycle)):\n            branch = next(iterkeys(mgraph[cycle[i]][cycle[(i+1)%len(cycle)]]))\n            branch_i = branches_i.get_loc(branch)\n            sign = +1 if branches_bus0.iat[branch_i] == cycle[i] else -1\n            sub_network.C[branch_i,j] += sign\n\n    #counter for multis\n    c = len(cycles)\n\n    #add multi-graph 2-edge cycles for multiple branches between same pairs of buses\n    for u,v in graph.edges():\n        bs = list(mgraph[u][v].keys())\n        if len(bs) > 1:\n            first = bs[0]\n            first_i = branches_i.get_loc(first)\n            for b in bs[1:]:\n                b_i = branches_i.get_loc(b)\n                sign = -1 if branches_bus0.iat[b_i] == branches_bus0.iat[first_i] else +1\n                sub_network.C[first_i,c] = 1\n                sub_network.C[b_i,c] = sign\n                c+=1", "response": "Find all cycles in the sub_network and record them in sub_network. C."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sub_network_lpf(sub_network, snapshots=None, skip_pre=False):\n\n    snapshots = _as_snapshots(sub_network.network, snapshots)\n    logger.info(\"Performing linear load-flow on %s sub-network %s for snapshot(s) %s\",\n                sub_network.network.sub_networks.at[sub_network.name,\"carrier\"], sub_network, snapshots)\n\n    network = sub_network.network\n\n\n    if not skip_pre:\n        calculate_dependent_values(network)\n        find_bus_controls(sub_network)\n        _allocate_pf_outputs(network, linear=True)\n\n\n    # get indices for the components on this subnetwork\n    buses_o = sub_network.buses_o\n    branches_i = sub_network.branches_i()\n\n    # allow all shunt impedances to dispatch as set\n    shunt_impedances_i = sub_network.shunt_impedances_i()\n    network.shunt_impedances_t.p.loc[snapshots, shunt_impedances_i] = \\\n        network.shunt_impedances.g_pu.loc[shunt_impedances_i].values\n\n    # allow all one ports to dispatch as set\n    for c in sub_network.iterate_components(network.controllable_one_port_components):\n        c_p_set = get_switchable_as_dense(network, c.name, 'p_set', snapshots, c.ind)\n        c.pnl.p.loc[snapshots, c.ind] = c_p_set\n\n    # set the power injection at each node\n    network.buses_t.p.loc[snapshots, buses_o] = \\\n        sum([((c.pnl.p.loc[snapshots, c.ind] * c.df.loc[c.ind, 'sign'])\n              .groupby(c.df.loc[c.ind, 'bus'], axis=1).sum()\n              .reindex(columns=buses_o, fill_value=0.))\n             for c in sub_network.iterate_components(network.one_port_components)]\n            +\n            [(- c.pnl[\"p\"+str(i)].loc[snapshots].groupby(c.df[\"bus\"+str(i)], axis=1).sum()\n              .reindex(columns=buses_o, fill_value=0))\n             for c in network.iterate_components(network.controllable_branch_components)\n             for i in [int(col[3:]) for col in c.df.columns if col[:3] == \"bus\"]])\n\n    if not skip_pre and len(branches_i) > 0:\n        calculate_B_H(sub_network, skip_pre=True)\n\n    v_diff = np.zeros((len(snapshots), len(buses_o)))\n    if len(branches_i) > 0:\n        p = network.buses_t['p'].loc[snapshots, buses_o].values - sub_network.p_bus_shift\n        v_diff[:,1:] = spsolve(sub_network.B[1:, 1:], p[:,1:].T).T\n        flows = pd.DataFrame(v_diff * sub_network.H.T,\n                             columns=branches_i, index=snapshots) + sub_network.p_branch_shift\n\n        for c in sub_network.iterate_components(network.passive_branch_components):\n            f = flows.loc[:, c.name]\n            c.pnl.p0.loc[snapshots, f.columns] = f\n            c.pnl.p1.loc[snapshots, f.columns] = -f\n\n    if network.sub_networks.at[sub_network.name,\"carrier\"] == \"DC\":\n        network.buses_t.v_mag_pu.loc[snapshots, buses_o] = 1 + v_diff\n        network.buses_t.v_ang.loc[snapshots, buses_o] = 0.\n    else:\n        network.buses_t.v_ang.loc[snapshots, buses_o] = v_diff\n        network.buses_t.v_mag_pu.loc[snapshots, buses_o] = 1.\n\n    # set slack bus power to pick up remained\n    slack_adjustment = (- network.buses_t.p.loc[snapshots, buses_o[1:]].sum(axis=1).fillna(0.)\n                        - network.buses_t.p.loc[snapshots, buses_o[0]])\n    network.buses_t.p.loc[snapshots, buses_o[0]] += slack_adjustment\n\n    # let slack generator take up the slack\n    if sub_network.slack_generator is not None:\n        network.generators_t.p.loc[snapshots, sub_network.slack_generator] += slack_adjustment", "response": "Linear power flow for a given sub - network."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calculate_BODF(sub_network, skip_pre=False):\n\n    if not skip_pre:\n        calculate_PTDF(sub_network)\n\n    num_branches = sub_network.PTDF.shape[0]\n\n    #build LxL version of PTDF\n    branch_PTDF = sub_network.PTDF*sub_network.K\n\n    denominator = csr_matrix((1/(1-np.diag(branch_PTDF)),(r_[:num_branches],r_[:num_branches])))\n\n    sub_network.BODF = branch_PTDF*denominator\n\n    #make sure the flow on the branch itself is zero\n    np.fill_diagonal(sub_network.BODF,-1)", "response": "Calculates the Branch Outage Distribution Factor for a single node of the branch."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the linear power flow for a selection of branch outages.", "response": "def network_lpf_contingency(network, snapshots=None, branch_outages=None):\n    \"\"\"\n    Computes linear power flow for a selection of branch outages.\n\n    Parameters\n    ----------\n    snapshots : list-like|single snapshot\n        A subset or an elements of network.snapshots on which to run\n        the power flow, defaults to network.snapshots\n        NB: currently this only works for a single snapshot\n    branch_outages : list-like\n        A list of passive branches which are to be tested for outages.\n        If None, it's take as all network.passive_branches_i()\n\n    Returns\n    -------\n    p0 : pandas.DataFrame\n        num_passive_branch x num_branch_outages DataFrame of new power flows\n\n    \"\"\"\n\n    if snapshots is None:\n        snapshots = network.snapshots\n\n    if isinstance(snapshots, collections.Iterable):\n        logger.warning(\"Apologies LPF contingency, this only works for single snapshots at the moment, taking the first snapshot.\")\n        snapshot = snapshots[0]\n    else:\n        snapshot = snapshots\n\n    network.lpf(snapshot)\n\n    # Store the flows from the base case\n\n    passive_branches = network.passive_branches()\n\n    if branch_outages is None:\n        branch_outages = passive_branches.index\n\n\n    p0_base = pd.Series(index=passive_branches.index)\n\n    for c in network.passive_branch_components:\n        pnl = network.pnl(c)\n        p0_base[c] = pnl.p0.loc[snapshot]\n\n    for sn in network.sub_networks.obj:\n        sn._branches = sn.branches()\n        sn.calculate_BODF()\n\n    p0 = pd.DataFrame(index=passive_branches.index)\n\n    p0[\"base\"] = p0_base\n\n    for branch in branch_outages:\n        if type(branch) is not tuple:\n            logger.warning(\"No type given for {}, assuming it is a line\".format(branch))\n            branch = (\"Line\",branch)\n\n        sn = network.sub_networks.obj[passive_branches.sub_network[branch]]\n\n        branch_i = sn._branches.index.get_loc(branch)\n\n        p0_new = p0_base + pd.Series(sn.BODF[:,branch_i]*p0_base[branch],sn._branches.index)\n\n        p0[branch] = p0_new\n\n    return p0"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef network_sclopf(network, snapshots=None, branch_outages=None, solver_name=\"glpk\",\n                   skip_pre=False, extra_functionality=None, solver_options={},\n                   keep_files=False, formulation=\"angles\", ptdf_tolerance=0.):\n    \"\"\"\n    Computes Security-Constrained Linear Optimal Power Flow (SCLOPF).\n\n    This ensures that no branch is overloaded even given the branch outages.\n\n    Parameters\n    ----------\n    snapshots : list or index slice\n        A list of snapshots to optimise, must be a subset of\n        network.snapshots, defaults to network.snapshots\n    branch_outages : list-like\n        A list of passive branches which are to be tested for outages.\n        If None, it's take as all network.passive_branches_i()\n    solver_name : string\n        Must be a solver name that pyomo recognises and that is\n        installed, e.g. \"glpk\", \"gurobi\"\n    skip_pre: bool, default False\n        Skip the preliminary steps of computing topology, calculating\n        dependent values and finding bus controls.\n    extra_functionality : callable function\n        This function must take two arguments\n        `extra_functionality(network,snapshots)` and is called after\n        the model building is complete, but before it is sent to the\n        solver. It allows the user to add/change constraints and\n        add/change the objective function.\n    solver_options : dictionary\n        A dictionary with additional options that get passed to the solver.\n        (e.g. {'threads':2} tells gurobi to use only 2 cpus)\n    keep_files : bool, default False\n        Keep the files that pyomo constructs from OPF problem\n        construction, e.g. .lp file - useful for debugging\n    formulation : string\n        Formulation of the linear power flow equations to use; must be\n        one of [\"angles\",\"cycles\",\"kirchoff\",\"ptdf\"]\n    ptdf_tolerance : float\n\n    Returns\n    -------\n    None\n    \"\"\"\n\n    if not skip_pre:\n        network.determine_network_topology()\n\n    snapshots = _as_snapshots(network, snapshots)\n\n    passive_branches = network.passive_branches()\n\n    if branch_outages is None:\n        branch_outages = passive_branches.index\n\n    #prepare the sub networks by calculating BODF and preparing helper DataFrames\n\n    for sn in network.sub_networks.obj:\n\n        sn.calculate_BODF()\n\n        sn._branches = sn.branches()\n        sn._branches[\"_i\"] = range(sn._branches.shape[0])\n\n        sn._extendable_branches = sn._branches[sn._branches.s_nom_extendable]\n        sn._fixed_branches = sn._branches[~ sn._branches.s_nom_extendable]\n\n\n    def add_contingency_constraints(network,snapshots):\n\n        #a list of tuples with branch_outage and passive branches in same sub_network\n        branch_outage_keys = []\n        flow_upper = {}\n        flow_lower = {}\n\n        for branch in branch_outages:\n            if type(branch) is not tuple:\n                logger.warning(\"No type given for {}, assuming it is a line\".format(branch))\n                branch = (\"Line\",branch)\n\n            sub = network.sub_networks.at[passive_branches.at[branch,\"sub_network\"],\"obj\"]\n\n            branch_i = sub._branches.at[branch,\"_i\"]\n\n            branch_outage_keys.extend([(branch[0],branch[1],b[0],b[1]) for b in sub._branches.index])\n\n            flow_upper.update({(branch[0],branch[1],b[0],b[1],sn) : [[(1,network.model.passive_branch_p[b[0],b[1],sn]),(sub.BODF[sub._branches.at[b,\"_i\"],branch_i],network.model.passive_branch_p[branch[0],branch[1],sn])],\"<=\",sub._fixed_branches.at[b,\"s_nom\"]] for b in sub._fixed_branches.index for sn in snapshots})\n\n            flow_upper.update({(branch[0],branch[1],b[0],b[1],sn) : [[(1,network.model.passive_branch_p[b[0],b[1],sn]),(sub.BODF[sub._branches.at[b,\"_i\"],branch_i],network.model.passive_branch_p[branch[0],branch[1],sn]),(-1,network.model.passive_branch_s_nom[b[0],b[1]])],\"<=\",0] for b in sub._extendable_branches.index for sn in snapshots})\n\n\n            flow_lower.update({(branch[0],branch[1],b[0],b[1],sn) : [[(1,network.model.passive_branch_p[b[0],b[1],sn]),(sub.BODF[sub._branches.at[b,\"_i\"],branch_i],network.model.passive_branch_p[branch[0],branch[1],sn])],\">=\",-sub._fixed_branches.at[b,\"s_nom\"]] for b in sub._fixed_branches.index for sn in snapshots})\n\n            flow_upper.update({(branch[0],branch[1],b[0],b[1],sn) : [[(1,network.model.passive_branch_p[b[0],b[1],sn]),(sub.BODF[sub._branches.at[b,\"_i\"],branch_i],network.model.passive_branch_p[branch[0],branch[1],sn]),(1,network.model.passive_branch_s_nom[b[0],b[1]])],\">=\",0] for b in sub._extendable_branches.index for sn in snapshots})\n\n\n        l_constraint(network.model,\"contingency_flow_upper\",flow_upper,branch_outage_keys,snapshots)\n\n\n        l_constraint(network.model,\"contingency_flow_lower\",flow_lower,branch_outage_keys,snapshots)\n\n        if extra_functionality is not None:\n            extra_functionality(network, snapshots)\n\n    #need to skip preparation otherwise it recalculates the sub-networks\n\n    network.lopf(snapshots=snapshots, solver_name=solver_name, skip_pre=True,\n                 extra_functionality=add_contingency_constraints,\n                 solver_options=solver_options, keep_files=keep_files,\n                 formulation=formulation, ptdf_tolerance=ptdf_tolerance)", "response": "Returns a new structure of a SCLOPF problem with the given network."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot(network, margin=0.05, ax=None, geomap=True, projection=None,\n         bus_colors='b', line_colors='g', bus_sizes=10, line_widths=2,\n         title=\"\", line_cmap=None, bus_cmap=None, boundaries=None,\n         geometry=False, branch_components=['Line', 'Link'], jitter=None,\n         basemap=None):\n    \"\"\"\n    Plot the network buses and lines using matplotlib and Basemap.\n\n    Parameters\n    ----------\n    margin : float\n        Margin at the sides as proportion of distance between max/min x,y\n    ax : matplotlib ax, defaults to plt.gca()\n        Axis to which to plot the network\n    geomap: bool/str, default True\n        Switch to use Basemap or Cartopy (depends on what is installed).\n        If string is passed, it will be used as a resolution argument.\n        For Basemap users 'c' (crude), 'l' (low), 'i' (intermediate),\n        'h' (high), 'f' (full) are valid resolutions options.\n        For Cartopy users '10m', '50m', '110m' are valid resolutions options.\n    projection: cartopy.crs.Projection, defaults to None\n        Define the projection of your geomap, only valid if cartopy is\n        installed. If None (default) is passed the projection for cartropy\n        is set to cartopy.crs.PlateCarree\n    bus_colors : dict/pandas.Series\n        Colors for the buses, defaults to \"b\"\n    bus_sizes : dict/pandas.Series\n        Sizes of bus points, defaults to 10\n    line_colors : dict/pandas.Series\n        Colors for the lines, defaults to \"g\" for Lines and \"cyan\" for\n        Links. Colors for branches other than Lines can be\n        specified using a pandas Series with a MultiIndex.\n    line_widths : dict/pandas.Series\n        Widths of lines, defaults to 2. Widths for branches other\n        than Lines can be specified using a pandas Series with a\n        MultiIndex.\n    title : string\n        Graph title\n    line_cmap : plt.cm.ColorMap/str|dict\n        If line_colors are floats, this color map will assign the colors.\n        Use a dict to specify colormaps for more than one branch type.\n    bus_cmap : plt.cm.ColorMap/str\n        If bus_colors are floats, this color map will assign the colors\n    boundaries : list of four floats\n        Boundaries of the plot in format [x1,x2,y1,y2]\n    branch_components : list of str\n        Branch components to be plotted, defaults to Line and Link.\n    jitter : None|float\n        Amount of random noise to add to bus positions to distinguish\n        overlapping buses\n\n    Returns\n    -------\n    bus_collection, branch_collection1, ... : tuple of Collections\n        Collections for buses and branches.\n    \"\"\"\n\n    defaults_for_branches = {\n        'Link': dict(color=\"cyan\", width=2),\n        'Line': dict(color=\"b\", width=2),\n        'Transformer': dict(color='green', width=2)\n    }\n\n    if not plt_present:\n        logger.error(\"Matplotlib is not present, so plotting won't work.\")\n        return\n\n    if basemap is not None:\n        logger.warning(\"argument `basemap` is deprecated, \"\n                       \"use `geomap` instead.\")\n        geomap = basemap\n\n    if cartopy_present and geomap:\n        if projection is None:\n            projection = get_projection_from_crs(network.srid)\n\n        if ax is None:\n            ax = plt.gca(projection=projection)\n        else:\n            assert isinstance(ax, cartopy.mpl.geoaxes.GeoAxesSubplot), (\n                    'The passed axis is not a GeoAxesSubplot. You can '\n                    'create one with: \\nimport cartopy.crs as ccrs \\n'\n                    'fig, ax = plt.subplots('\n                    'subplot_kw={\"projection\":ccrs.PlateCarree()})')\n    elif ax is None:\n        ax = plt.gca()\n\n    x, y = network.buses[\"x\"],  network.buses[\"y\"]\n\n    if jitter is not None:\n        x = x + np.random.uniform(low=-jitter, high=jitter, size=len(x))\n        y = y + np.random.uniform(low=-jitter, high=jitter, size=len(y))\n\n    if geomap:\n        transform = draw_map(network, x, y, ax, boundaries, margin, geomap)\n    else:\n        transform = ax.transData\n\n    if isinstance(bus_sizes, pd.Series) and isinstance(bus_sizes.index, pd.MultiIndex):\n        # We are drawing pies to show all the different shares\n        assert len(bus_sizes.index.levels[0].difference(network.buses.index)) == 0, \\\n            \"The first MultiIndex level of bus_sizes must contain buses\"\n        assert (isinstance(bus_colors, dict) and\n                set(bus_colors).issuperset(bus_sizes.index.levels[1])), \\\n            \"bus_colors must be a dictionary defining a color for each element \" \\\n            \"in the second MultiIndex level of bus_sizes\"\n\n        bus_sizes = bus_sizes.sort_index(level=0, sort_remaining=False)\\\n                        * projected_area_factor(ax, network.srid)**2\n\n        patches = []\n        for b_i in bus_sizes.index.levels[0]:\n            s = bus_sizes.loc[b_i]\n            radius = s.sum()**0.5\n            if radius == 0.0:\n                ratios = s\n            else:\n                ratios = s/s.sum()\n\n            start = 0.25\n            for i, ratio in ratios.iteritems():\n                patches.append(Wedge((x.at[b_i], y.at[b_i]), radius,\n                                     360*start, 360*(start+ratio),\n                                     facecolor=bus_colors[i]))\n                start += ratio\n        bus_collection = PatchCollection(patches, match_original=True, transform=transform)\n        ax.add_collection(bus_collection)\n    else:\n        c = pd.Series(bus_colors, index=network.buses.index)\n        s = pd.Series(bus_sizes, index=network.buses.index, dtype=\"float\").fillna(10)\n        bus_collection = ax.scatter(x, y, c=c, s=s, cmap=bus_cmap, edgecolor='face', transform=transform)\n\n    def as_branch_series(ser):\n        if isinstance(ser, dict) and set(ser).issubset(branch_components):\n            return pd.Series(ser)\n        elif isinstance(ser, pd.Series):\n            if isinstance(ser.index, pd.MultiIndex):\n                return ser\n            index = ser.index\n            ser = ser.values\n        else:\n            index = network.lines.index\n        return pd.Series(ser,\n                         index=pd.MultiIndex(levels=([\"Line\"], index),\n                                             codes=(np.zeros(len(index)),\n                                                    np.arange(len(index)))))\n\n    line_colors = as_branch_series(line_colors)\n    line_widths = as_branch_series(line_widths)\n    if not isinstance(line_cmap, dict):\n        line_cmap = {'Line': line_cmap}\n\n    branch_collections = []\n    for c in network.iterate_components(branch_components):\n        l_defaults = defaults_for_branches[c.name]\n        l_widths = line_widths.get(c.name, l_defaults['width'])\n        l_nums = None\n        l_colors = line_colors.get(c.name, l_defaults['color'])\n\n        if isinstance(l_colors, pd.Series):\n            if issubclass(l_colors.dtype.type, np.number):\n                l_nums = l_colors\n                l_colors = None\n            else:\n                l_colors.fillna(l_defaults['color'], inplace=True)\n\n        if not geometry:\n            segments = (np.asarray(((c.df.bus0.map(x),\n                                     c.df.bus0.map(y)),\n                                    (c.df.bus1.map(x),\n                                     c.df.bus1.map(y))))\n                        .transpose(2, 0, 1))\n        else:\n            from shapely.wkt import loads\n            from shapely.geometry import LineString\n            linestrings = c.df.geometry.map(loads)\n            assert all(isinstance(ls, LineString) for ls in linestrings), (\n                \"The WKT-encoded geometry in the 'geometry' column must be \"\n                \"composed of LineStrings\")\n            segments = np.asarray(list(linestrings.map(np.asarray)))\n\n        l_collection = LineCollection(segments,\n                                      linewidths=l_widths,\n                                      antialiaseds=(1,),\n                                      colors=l_colors,\n                                      transOffset=ax.transData,\n                                      transform=transform)\n\n        if l_nums is not None:\n            l_collection.set_array(np.asarray(l_nums))\n            l_collection.set_cmap(line_cmap.get(c.name, None))\n            l_collection.autoscale()\n\n        ax.add_collection(l_collection)\n        l_collection.set_zorder(1)\n\n        branch_collections.append(l_collection)\n\n    bus_collection.set_zorder(2)\n\n    ax.update_datalim(compute_bbox_with_margins(margin, x, y))\n    ax.autoscale_view()\n\n    if geomap:\n        if cartopy_present:\n            ax.outline_patch.set_visible(False)\n        ax.axis('off')\n\n    ax.set_title(title)\n\n    return (bus_collection,) + tuple(branch_collections)", "response": "Plot the network with matplotlib and Basemap."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplots the network buses and lines interactively using plotly.", "response": "def iplot(network, fig=None, bus_colors='blue',\n          bus_colorscale=None, bus_colorbar=None, bus_sizes=10, bus_text=None,\n          line_colors='green', line_widths=2, line_text=None, title=\"\",\n          branch_components=['Line', 'Link'], iplot=True, jitter=None):\n    \"\"\"\n    Plot the network buses and lines interactively using plotly.\n\n    Parameters\n    ----------\n    fig : dict, default None\n        If not None, figure is built upon this fig.\n    bus_colors : dict/pandas.Series\n        Colors for the buses, defaults to \"b\"\n    bus_colorscale : string\n        Name of colorscale if bus_colors are floats, e.g. 'Jet', 'Viridis'\n    bus_colorbar : dict\n        Plotly colorbar, e.g. {'title' : 'my colorbar'}\n    bus_sizes : dict/pandas.Series\n        Sizes of bus points, defaults to 10\n    bus_text : dict/pandas.Series\n        Text for each bus, defaults to bus names\n    line_colors : dict/pandas.Series\n        Colors for the lines, defaults to \"g\" for Lines and \"cyan\" for\n        Links. Colors for branches other than Lines can be\n        specified using a pandas Series with a MultiIndex.\n    line_widths : dict/pandas.Series\n        Widths of lines, defaults to 2. Widths for branches other\n        than Lines can be specified using a pandas Series with a\n        MultiIndex.\n    line_text : dict/pandas.Series\n        Text for lines, defaults to line names. Text for branches other\n        than Lines can be specified using a pandas Series with a\n        MultiIndex.\n    title : string\n        Graph title\n    branch_components : list of str\n        Branch components to be plotted, defaults to Line and Link.\n    iplot : bool, default True\n        Automatically do an interactive plot of the figure.\n    jitter : None|float\n        Amount of random noise to add to bus positions to distinguish\n        overlapping buses\n\n    Returns\n    -------\n    fig: dictionary for plotly figure\n    \"\"\"\n\n    defaults_for_branches = {\n        'Link': dict(color=\"cyan\", width=2),\n        'Line': dict(color=\"blue\", width=2),\n        'Transformer': dict(color='green', width=2)\n    }\n\n    if fig is None:\n        fig = dict(data=[],layout={})\n\n    if bus_text is None:\n        bus_text = 'Bus ' + network.buses.index\n\n    x = network.buses.x\n    y = network.buses.y\n\n    if jitter is not None:\n        x = x + np.random.uniform(low=-jitter, high=jitter, size=len(x))\n        y = y + np.random.uniform(low=-jitter, high=jitter, size=len(y))\n\n    bus_trace = dict(x=x, y=y,\n                     text=bus_text,\n                     type=\"scatter\",\n                     mode=\"markers\",\n                     hoverinfo=\"text\",\n                     marker=dict(color=bus_colors,\n                                 size=bus_sizes),\n                     )\n\n    if bus_colorscale is not None:\n        bus_trace['marker']['colorscale'] = bus_colorscale\n\n    if bus_colorbar is not None:\n        bus_trace['marker']['colorbar'] = bus_colorbar\n\n\n    def as_branch_series(ser):\n        if isinstance(ser, dict) and set(ser).issubset(branch_components):\n            return pd.Series(ser)\n        elif isinstance(ser, pd.Series):\n            if isinstance(ser.index, pd.MultiIndex):\n                return ser\n            index = ser.index\n            ser = ser.values\n        else:\n            index = network.lines.index\n        return pd.Series(ser,\n                         index=pd.MultiIndex(levels=([\"Line\"], index),\n                                             labels=(np.zeros(len(index)),\n                                                     np.arange(len(index)))))\n\n    line_colors = as_branch_series(line_colors)\n    line_widths = as_branch_series(line_widths)\n\n    if line_text is not None:\n        line_text = as_branch_series(line_text)\n\n    shapes = []\n\n    shape_traces = []\n\n    for c in network.iterate_components(branch_components):\n        l_defaults = defaults_for_branches[c.name]\n        l_widths = line_widths.get(c.name, l_defaults['width'])\n        l_colors = line_colors.get(c.name, l_defaults['color'])\n        l_nums = None\n\n        if line_text is None:\n            l_text = c.name + ' ' + c.df.index\n        else:\n            l_text = line_text.get(c.name)\n\n        if isinstance(l_colors, pd.Series):\n            if issubclass(l_colors.dtype.type, np.number):\n                l_nums = l_colors\n                l_colors = None\n            else:\n                l_colors.fillna(l_defaults['color'], inplace=True)\n\n        x0 = c.df.bus0.map(x)\n        x1 = c.df.bus1.map(x)\n\n        y0 = c.df.bus0.map(y)\n        y1 = c.df.bus1.map(y)\n\n        for line in c.df.index:\n            color = l_colors if isinstance(l_colors, string_types) else l_colors[line]\n            width = l_widths if isinstance(l_widths, (int, float)) else l_widths[line]\n\n            shapes.append(dict(type='line',\n                               x0=x0[line],\n                               y0=y0[line],\n                               x1=x1[line],\n                               y1=y1[line],\n                               opacity=0.7,\n                               line=dict(color=color, width=width)))\n\n        shape_traces.append(dict(x=0.5*(x0+x1),\n                                 y=0.5*(y0+y1),\n                                 text=l_text,\n                                 type=\"scatter\",\n                                 mode=\"markers\",\n                                 hoverinfo=\"text\",\n                                 marker=dict(opacity=0.)))\n\n    fig['data'].extend([bus_trace]+shape_traces)\n\n    fig['layout'].update(dict(shapes=shapes,\n                              title=title,\n                              hovermode='closest',\n                              showlegend=False))\n                              #xaxis=dict(range=[6,14]),\n                              #yaxis=dict(range=[47,55])\n\n\n    if iplot:\n        if not pltly_present:\n            logger.warning(\"Plotly is not present, so interactive plotting won't work.\")\n        else:\n            pltly.iplot(fig)\n\n    return fig"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _export_to_exporter(network, exporter, basename, export_standard_types=False):\n\n    #exportable component types\n    #what about None???? - nan is float?\n    allowed_types = (float,int,bool) + string_types + tuple(np.typeDict.values())\n\n    #first export network properties\n    attrs = dict((attr, getattr(network, attr))\n                 for attr in dir(network)\n                 if (not attr.startswith(\"__\") and\n                     isinstance(getattr(network,attr), allowed_types)))\n    exporter.save_attributes(attrs)\n\n    #now export snapshots\n    snapshots = pd.DataFrame(dict(weightings=network.snapshot_weightings),\n                             index=pd.Index(network.snapshots, name=\"name\"))\n    exporter.save_snapshots(snapshots)\n\n    exported_components = []\n    for component in network.all_components - {\"SubNetwork\"}:\n\n        list_name = network.components[component][\"list_name\"]\n        attrs = network.components[component][\"attrs\"]\n\n        df = network.df(component)\n        pnl = network.pnl(component)\n\n        if not export_standard_types and component in network.standard_type_components:\n            df = df.drop(network.components[component][\"standard_types\"].index)\n\n        # first do static attributes\n        df.index.name = \"name\"\n        if df.empty:\n            exporter.remove_static(list_name)\n            continue\n\n        col_export = []\n        for col in df.columns:\n            # do not export derived attributes\n            if col in [\"sub_network\", \"r_pu\", \"x_pu\", \"g_pu\", \"b_pu\"]:\n                continue\n            if col in attrs.index and pd.isnull(attrs.at[col, \"default\"]) and pd.isnull(df[col]).all():\n                continue\n            if (col in attrs.index\n                and df[col].dtype == attrs.at[col, 'dtype']\n                and (df[col] == attrs.at[col, \"default\"]).all()):\n                continue\n\n            col_export.append(col)\n\n        exporter.save_static(list_name, df[col_export])\n\n        #now do varying attributes\n        for attr in pnl:\n            if attr not in attrs.index:\n                col_export = pnl[attr].columns\n            else:\n                default = attrs.at[attr, \"default\"]\n\n                if pd.isnull(default):\n                    col_export = pnl[attr].columns[(~pd.isnull(pnl[attr])).any()]\n                else:\n                    col_export = pnl[attr].columns[(pnl[attr] != default).any()]\n\n            if len(col_export) > 0:\n                df = pnl[attr][col_export]\n                exporter.save_series(list_name, attr, df)\n            else:\n                exporter.remove_series(list_name, attr)\n\n        exported_components.append(list_name)\n\n    logger.info(\"Exported network {} has {}\".format(basename, \", \".join(exported_components)))", "response": "Exports the network to exporter."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nimport network data from CSVs in a folder.", "response": "def import_from_csv_folder(network, csv_folder_name, encoding=None, skip_time=False):\n    \"\"\"\n    Import network data from CSVs in a folder.\n\n    The CSVs must follow the standard form, see pypsa/examples.\n\n    Parameters\n    ----------\n    csv_folder_name : string\n        Name of folder\n    encoding : str, default None\n        Encoding to use for UTF when reading (ex. 'utf-8'). `List of Python\n        standard encodings\n        <https://docs.python.org/3/library/codecs.html#standard-encodings>`_\n    skip_time : bool, default False\n        Skip reading in time dependent attributes\n    \"\"\"\n\n    basename = os.path.basename(csv_folder_name)\n    with ImporterCSV(csv_folder_name, encoding=encoding) as importer:\n        _import_from_importer(network, importer, basename=basename, skip_time=skip_time)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef export_to_csv_folder(network, csv_folder_name, encoding=None, export_standard_types=False):\n\n    basename = os.path.basename(csv_folder_name)\n    with ExporterCSV(csv_folder_name=csv_folder_name, encoding=encoding) as exporter:\n        _export_to_exporter(network, exporter, basename=basename,\n                            export_standard_types=export_standard_types)", "response": "Export network and components to a folder of CSVs."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nimports network data from HDF5 store at path.", "response": "def import_from_hdf5(network, path, skip_time=False):\n    \"\"\"\n    Import network data from HDF5 store at `path`.\n\n    Parameters\n    ----------\n    path : string\n        Name of HDF5 store\n    skip_time : bool, default False\n        Skip reading in time dependent attributes\n    \"\"\"\n\n    basename = os.path.basename(path)\n    with ImporterHDF5(path) as importer:\n        _import_from_importer(network, importer, basename=basename, skip_time=skip_time)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef export_to_hdf5(network, path, export_standard_types=False, **kwargs):\n\n    kwargs.setdefault('complevel', 4)\n\n    basename = os.path.basename(path)\n    with ExporterHDF5(path, **kwargs) as exporter:\n        _export_to_exporter(network, exporter, basename=basename,\n                            export_standard_types=export_standard_types)", "response": "Exports a network and components to an HDF5 file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef import_from_netcdf(network, path, skip_time=False):\n\n    assert has_xarray, \"xarray must be installed for netCDF support.\"\n\n    basename = os.path.basename(path) if isinstance(path, string_types) else None\n    with ImporterNetCDF(path=path) as importer:\n        _import_from_importer(network, importer, basename=basename,\n                              skip_time=skip_time)", "response": "Imports network data from netCDF file or xarray dataset at path."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexports network and components to a netCDF file.", "response": "def export_to_netcdf(network, path=None, export_standard_types=False,\n                     least_significant_digit=None):\n    \"\"\"Export network and components to a netCDF file.\n\n    Both static and series attributes of components are exported, but only\n    if they have non-default values.\n\n    If path does not already exist, it is created.\n\n    If no path is passed, no file is exported, but the xarray.Dataset\n    is still returned.\n\n    Be aware that this cannot export boolean attributes on the Network\n    class, e.g. network.my_bool = False is not supported by netCDF.\n\n    Parameters\n    ----------\n    path : string|None\n        Name of netCDF file to which to export (if it exists, it is overwritten);\n        if None is passed, no file is exported.\n    least_significant_digit\n        This is passed to the netCDF exporter, but currently makes no difference\n        to file size or float accuracy. We're working on improving this...\n\n    Returns\n    -------\n    ds : xarray.Dataset\n\n    Examples\n    --------\n    >>> export_to_netcdf(network, \"my_file.nc\")\n    OR\n    >>> network.export_to_netcdf(\"my_file.nc\")\n\n    \"\"\"\n\n    assert has_xarray, \"xarray must be installed for netCDF support.\"\n\n    basename = os.path.basename(path) if path is not None else None\n    with ExporterNetCDF(path, least_significant_digit) as exporter:\n        _export_to_exporter(network, exporter, basename=basename,\n                            export_standard_types=export_standard_types)\n        return exporter.ds"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _import_from_importer(network, importer, basename, skip_time=False):\n\n    attrs = importer.get_attributes()\n\n    current_pypsa_version = [int(s) for s in network.pypsa_version.split(\".\")]\n    pypsa_version = None\n\n    if attrs is not None:\n        network.name = attrs.pop('name')\n\n        try:\n            pypsa_version = [int(s) for s in attrs.pop(\"pypsa_version\").split(\".\")]\n        except KeyError:\n            pypsa_version = None\n\n        for attr, val in iteritems(attrs):\n            setattr(network, attr, val)\n\n    ##https://docs.python.org/3/tutorial/datastructures.html#comparing-sequences-and-other-types\n    if pypsa_version is None or pypsa_version < current_pypsa_version:\n        logger.warning(dedent(\"\"\"\n                Importing PyPSA from older version of PyPSA than current version {}.\n                Please read the release notes at https://pypsa.org/doc/release_notes.html\n                carefully to prepare your network for import.\n        \"\"\").format(network.pypsa_version))\n\n    importer.pypsa_version = pypsa_version\n    importer.current_pypsa_version = current_pypsa_version\n\n    # if there is snapshots.csv, read in snapshot data\n    df = importer.get_snapshots()\n    if df is not None:\n        network.set_snapshots(df.index)\n        if \"weightings\" in df.columns:\n            network.snapshot_weightings = df[\"weightings\"].reindex(network.snapshots)\n\n    imported_components = []\n\n    # now read in other components; make sure buses and carriers come first\n    for component in [\"Bus\", \"Carrier\"] + sorted(network.all_components - {\"Bus\", \"Carrier\", \"SubNetwork\"}):\n        list_name = network.components[component][\"list_name\"]\n\n        df = importer.get_static(list_name)\n        if df is None:\n            if component == \"Bus\":\n                logger.error(\"Error, no buses found\")\n                return\n            else:\n                continue\n\n        import_components_from_dataframe(network, df, component)\n\n        if not skip_time:\n            for attr, df in importer.get_series(list_name):\n                import_series_from_dataframe(network, df, component, attr)\n\n        logger.debug(getattr(network,list_name))\n\n        imported_components.append(list_name)\n\n    logger.info(\"Imported network{} has {}\".format(\" \" + basename, \", \".join(imported_components)))", "response": "Imports network data from importer and returns a new version of the network."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nimporting components from a pandas DataFrame.", "response": "def import_components_from_dataframe(network, dataframe, cls_name):\n    \"\"\"\n    Import components from a pandas DataFrame.\n\n    If columns are missing then defaults are used.\n\n    If extra columns are added, these are left in the resulting component dataframe.\n\n    Parameters\n    ----------\n    dataframe : pandas.DataFrame\n    cls_name : string\n        Name of class of component\n\n    Examples\n    --------\n    >>> network.import_components_from_dataframe(dataframe,\"Line\")\n    \"\"\"\n\n    if cls_name == \"Generator\" and \"source\" in dataframe.columns:\n        logger.warning(\"'source' for generators is deprecated, use 'carrier' instead.\")\n    if cls_name == \"Generator\" and \"dispatch\" in dataframe.columns:\n        logger.warning(\"'dispatch' for generators is deprecated, use time-varing 'p_max_pu' for 'variable' and static 'p_max_pu' for 'flexible'.\")\n    if cls_name in [\"Generator\",\"StorageUnit\"] and \"p_max_pu_fixed\" in dataframe.columns:\n        logger.warning(\"'p_max_pu_fixed' for generators is deprecated, use static 'p_max_pu' instead.\")\n    if cls_name in [\"Generator\",\"StorageUnit\"] and \"p_min_pu_fixed\" in dataframe.columns:\n        logger.warning(\"'p_min_pu_fixed' for generators is deprecated, use static 'p_min_pu' instead.\")\n    if cls_name == \"Bus\" and \"current_type\" in dataframe.columns:\n        logger.warning(\"'current_type' for buses is deprecated, use 'carrier' instead.\")\n    if cls_name == \"Link\" and \"s_nom\" in dataframe.columns:\n        logger.warning(\"'s_nom*' for links is deprecated, use 'p_nom*' instead.\")\n\n    attrs = network.components[cls_name][\"attrs\"]\n\n    static_attrs = attrs[attrs.static].drop(\"name\")\n    non_static_attrs = attrs[~attrs.static]\n\n    # Clean dataframe and ensure correct types\n    dataframe = pd.DataFrame(dataframe)\n    dataframe.index = dataframe.index.astype(str)\n\n    for k in static_attrs.index:\n        if k not in dataframe.columns:\n            dataframe[k] = static_attrs.at[k, \"default\"]\n        else:\n            if static_attrs.at[k, \"type\"] == 'string':\n                dataframe[k] = dataframe[k].replace({np.nan: \"\"})\n\n            dataframe[k] = dataframe[k].astype(static_attrs.at[k, \"typ\"])\n\n    #check all the buses are well-defined\n    for attr in [\"bus\", \"bus0\", \"bus1\"]:\n        if attr in dataframe.columns:\n            missing = dataframe.index[~dataframe[attr].isin(network.buses.index)]\n            if len(missing) > 0:\n                logger.warning(\"The following %s have buses which are not defined:\\n%s\",\n                               cls_name, missing)\n\n    non_static_attrs_in_df = non_static_attrs.index.intersection(dataframe.columns)\n    old_df = network.df(cls_name)\n    new_df = dataframe.drop(non_static_attrs_in_df, axis=1)\n    if not old_df.empty:\n        new_df = pd.concat((old_df, new_df), sort=False)\n\n    if not new_df.index.is_unique:\n        logger.error(\"Error, new components for {} are not unique\".format(cls_name))\n        return\n\n    setattr(network, network.components[cls_name][\"list_name\"], new_df)\n\n    #now deal with time-dependent properties\n\n    pnl = network.pnl(cls_name)\n\n    for k in non_static_attrs_in_df:\n        #If reading in outputs, fill the outputs\n        pnl[k] = pnl[k].reindex(columns=new_df.index,\n                                fill_value=non_static_attrs.at[k, \"default\"])\n        pnl[k].loc[:,dataframe.index] = dataframe.loc[:,k].values\n\n    setattr(network,network.components[cls_name][\"list_name\"]+\"_t\",pnl)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nimport time series from a pandas DataFrame.", "response": "def import_series_from_dataframe(network, dataframe, cls_name, attr):\n    \"\"\"\n    Import time series from a pandas DataFrame.\n\n    Parameters\n    ----------\n    dataframe : pandas.DataFrame\n    cls_name : string\n        Name of class of component\n    attr : string\n        Name of series attribute\n\n    Examples\n    --------\n    >>> import_series_from_dataframe(dataframe,\"Load\",\"p_set\")\n    \"\"\"\n\n    df = network.df(cls_name)\n    pnl = network.pnl(cls_name)\n    list_name = network.components[cls_name][\"list_name\"]\n\n    diff = dataframe.columns.difference(df.index)\n    if len(diff) > 0:\n        logger.warning(\"Components {} for attribute {} of {} are not in main components dataframe {}\".format(diff,attr,cls_name,list_name))\n\n    attr_series = network.components[cls_name][\"attrs\"].loc[attr]\n    columns = dataframe.columns\n\n    diff = network.snapshots.difference(dataframe.index)\n    if len(diff):\n        logger.warning(\"Snapshots {} are missing from {} of {}. Filling with default value '{}'\".format(diff,attr,cls_name,attr_series[\"default\"]))\n        dataframe = dataframe.reindex(network.snapshots, fill_value=attr_series[\"default\"])\n\n    if not attr_series.static:\n        pnl[attr] = pnl[attr].reindex(columns=df.index|columns, fill_value=attr_series.default)\n    else:\n        pnl[attr] = pnl[attr].reindex(columns=(pnl[attr].columns | columns))\n\n    pnl[attr].loc[network.snapshots, columns] = dataframe.loc[network.snapshots, columns]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_from_pypower_ppc(network, ppc, overwrite_zero_s_nom=None):\n\n\n    version = ppc[\"version\"]\n    if int(version) != 2:\n        logger.warning(\"Warning, importing from PYPOWER may not work if PPC version is not 2!\")\n\n    logger.warning(\"Warning: Note that when importing from PYPOWER, some PYPOWER features not supported: areas, gencosts, component status\")\n\n\n    baseMVA = ppc[\"baseMVA\"]\n\n    #dictionary to store pandas DataFrames of PyPower data\n    pdf = {}\n\n\n    # add buses\n\n    #integer numbering will be bus names\n    index = np.array(ppc['bus'][:,0],dtype=int)\n\n    columns = [\"type\",\"Pd\",\"Qd\",\"Gs\",\"Bs\",\"area\",\"v_mag_pu_set\",\"v_ang_set\",\"v_nom\",\"zone\",\"v_mag_pu_max\",\"v_mag_pu_min\"]\n\n    pdf[\"buses\"] = pd.DataFrame(index=index,columns=columns,data=ppc['bus'][:,1:len(columns)+1])\n\n    if (pdf[\"buses\"][\"v_nom\"] == 0.).any():\n        logger.warning(\"Warning, some buses have nominal voltage of 0., setting the nominal voltage of these to 1.\")\n        pdf['buses'].loc[pdf['buses']['v_nom'] == 0.,'v_nom'] = 1.\n\n\n    #rename controls\n    controls = [\"\",\"PQ\",\"PV\",\"Slack\"]\n    pdf[\"buses\"][\"control\"] = pdf[\"buses\"].pop(\"type\").map(lambda i: controls[int(i)])\n\n    #add loads for any buses with Pd or Qd\n    pdf['loads'] = pdf[\"buses\"].loc[pdf[\"buses\"][[\"Pd\",\"Qd\"]].any(axis=1), [\"Pd\",\"Qd\"]]\n    pdf['loads']['bus'] = pdf['loads'].index\n    pdf['loads'].rename(columns={\"Qd\" : \"q_set\", \"Pd\" : \"p_set\"}, inplace=True)\n    pdf['loads'].index = [\"L\"+str(i) for i in range(len(pdf['loads']))]\n\n\n    #add shunt impedances for any buses with Gs or Bs\n\n    shunt = pdf[\"buses\"].loc[pdf[\"buses\"][[\"Gs\",\"Bs\"]].any(axis=1), [\"v_nom\",\"Gs\",\"Bs\"]]\n\n    #base power for shunt is 1 MVA, so no need to rebase here\n    shunt[\"g\"] = shunt[\"Gs\"]/shunt[\"v_nom\"]**2\n    shunt[\"b\"] = shunt[\"Bs\"]/shunt[\"v_nom\"]**2\n    pdf['shunt_impedances'] = shunt.reindex(columns=[\"g\",\"b\"])\n    pdf['shunt_impedances'][\"bus\"] = pdf['shunt_impedances'].index\n    pdf['shunt_impedances'].index = [\"S\"+str(i) for i in range(len(pdf['shunt_impedances']))]\n\n    #add gens\n\n    #it is assumed that the pypower p_max is the p_nom\n\n    #could also do gen.p_min_pu = p_min/p_nom\n\n    columns = \"bus, p_set, q_set, q_max, q_min, v_set_pu, mva_base, status, p_nom, p_min, Pc1, Pc2, Qc1min, Qc1max, Qc2min, Qc2max, ramp_agc, ramp_10, ramp_30, ramp_q, apf\".split(\", \")\n\n    index = [\"G\"+str(i) for i in range(len(ppc['gen']))]\n\n    pdf['generators'] = pd.DataFrame(index=index,columns=columns,data=ppc['gen'][:,:len(columns)])\n\n\n    #make sure bus name is an integer\n    pdf['generators']['bus'] = np.array(ppc['gen'][:,0],dtype=int)\n\n    #add branchs\n    ## branch data\n    # fbus, tbus, r, x, b, rateA, rateB, rateC, ratio, angle, status, angmin, angmax\n\n    columns = 'bus0, bus1, r, x, b, s_nom, rateB, rateC, tap_ratio, phase_shift, status, v_ang_min, v_ang_max'.split(\", \")\n\n\n    pdf['branches'] = pd.DataFrame(columns=columns,data=ppc['branch'][:,:len(columns)])\n\n    pdf['branches']['original_index'] = pdf['branches'].index\n\n    pdf['branches'][\"bus0\"] = pdf['branches'][\"bus0\"].astype(int)\n    pdf['branches'][\"bus1\"] = pdf['branches'][\"bus1\"].astype(int)\n\n    # s_nom = 0 indicates an unconstrained line\n    zero_s_nom = pdf['branches'][\"s_nom\"] == 0.\n    if zero_s_nom.any():\n        if overwrite_zero_s_nom is not None:\n            pdf['branches'].loc[zero_s_nom, \"s_nom\"] = overwrite_zero_s_nom\n        else:\n            logger.warning(\"Warning: there are {} branches with s_nom equal to zero, \"\n                  \"they will probably lead to infeasibilities and should be \"\n                  \"replaced with a high value using the `overwrite_zero_s_nom` \"\n                  \"argument.\".format(zero_s_nom.sum()))\n\n    # determine bus voltages of branches to detect transformers\n    v_nom = pdf['branches'].bus0.map(pdf['buses'].v_nom)\n    v_nom_1 = pdf['branches'].bus1.map(pdf['buses'].v_nom)\n\n    # split branches into transformers and lines\n    transformers = ((v_nom != v_nom_1)\n                    | ((pdf['branches'].tap_ratio != 0.) & (pdf['branches'].tap_ratio != 1.)) #NB: PYPOWER has strange default of 0. for tap ratio\n                    | (pdf['branches'].phase_shift != 0))\n    pdf['transformers'] = pd.DataFrame(pdf['branches'][transformers])\n    pdf['lines'] = pdf['branches'][~ transformers].drop([\"tap_ratio\", \"phase_shift\"], axis=1)\n\n    #convert transformers from base baseMVA to base s_nom\n    pdf['transformers']['r'] = pdf['transformers']['r']*pdf['transformers']['s_nom']/baseMVA\n    pdf['transformers']['x'] = pdf['transformers']['x']*pdf['transformers']['s_nom']/baseMVA\n    pdf['transformers']['b'] = pdf['transformers']['b']*baseMVA/pdf['transformers']['s_nom']\n\n    #correct per unit impedances\n    pdf['lines'][\"r\"] = v_nom**2*pdf['lines'][\"r\"]/baseMVA\n    pdf['lines'][\"x\"] = v_nom**2*pdf['lines'][\"x\"]/baseMVA\n    pdf['lines'][\"b\"] = pdf['lines'][\"b\"]*baseMVA/v_nom**2\n\n\n    if (pdf['transformers']['tap_ratio'] == 0.).any():\n        logger.warning(\"Warning, some transformers have a tap ratio of 0., setting the tap ratio of these to 1.\")\n        pdf['transformers'].loc[pdf['transformers']['tap_ratio'] == 0.,'tap_ratio'] = 1.\n\n\n    #name them nicely\n    pdf['transformers'].index = [\"T\"+str(i) for i in range(len(pdf['transformers']))]\n    pdf['lines'].index = [\"L\"+str(i) for i in range(len(pdf['lines']))]\n\n    #TODO\n\n    ##-----  OPF Data  -----##\n    ## generator cost data\n    # 1 startup shutdown n x1 y1 ... xn yn\n    # 2 startup shutdown n c(n-1) ... c0\n\n    for component in [\"Bus\",\"Load\",\"Generator\",\"Line\",\"Transformer\",\"ShuntImpedance\"]:\n        import_components_from_dataframe(network,pdf[network.components[component][\"list_name\"]],component)\n\n    network.generators[\"control\"] = network.generators.bus.map(network.buses[\"control\"])\n\n    #for consistency with pypower, take the v_mag set point from the generators\n    network.buses.loc[network.generators.bus,\"v_mag_pu_set\"] = np.asarray(network.generators[\"v_set_pu\"])", "response": "Imports network from PYPOWER PPC dictionary format version 2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nimport network from pandapower net. This import function is not yet finished (see warning below). Parameters ---------- net : pandapower network Examples -------- >>> network.import_from_pandapower_net(net)", "response": "def import_from_pandapower_net(network, net):\n    \"\"\"\n    Import network from pandapower net.\n\n    This import function is not yet finished (see warning below).\n\n    Parameters\n    ----------\n    net : pandapower network\n\n    Examples\n    --------\n    >>> network.import_from_pandapower_net(net)\n    \"\"\"\n    logger.warning(\"Warning: Importing from pandapower is still in beta; not all pandapower data is supported.\\nUnsupported features include: three-winding transformers, switches, in_service status, shunt impedances and tap positions of transformers.\")\n\n    d = {}\n\n    d[\"Bus\"] = pd.DataFrame({\"v_nom\" : net.bus.vn_kv.values,\n                             \"v_mag_pu_set\" : 1.},\n                            index=net.bus.name)\n\n    d[\"Load\"] = pd.DataFrame({\"p_set\" : (net.load.scaling*net.load.p_mw).values,\n                              \"q_set\" : (net.load.scaling*net.load.q_mvar).values,\n                              \"bus\" : net.bus.name.loc[net.load.bus].values},\n                             index=net.load.name)\n\n    #deal with PV generators\n    d[\"Generator\"] = pd.DataFrame({\"p_set\" : -(net.gen.scaling*net.gen.p_mw).values,\n                                   \"q_set\" : 0.,\n                                   \"bus\" : net.bus.name.loc[net.gen.bus].values,\n                                   \"control\" : \"PV\"},\n                                  index=net.gen.name)\n\n    d[\"Bus\"].loc[net.bus.name.loc[net.gen.bus].values,\"v_mag_pu_set\"] = net.gen.vm_pu.values\n\n\n    #deal with PQ \"static\" generators\n    d[\"Generator\"] = pd.concat((d[\"Generator\"],pd.DataFrame({\"p_set\" : -(net.sgen.scaling*net.sgen.p_mw).values,\n                                                             \"q_set\" : -(net.sgen.scaling*net.sgen.q_mvar).values,\n                                                             \"bus\" : net.bus.name.loc[net.sgen.bus].values,\n                                                             \"control\" : \"PQ\"},\n                                                            index=net.sgen.name)), sort=False)\n\n    d[\"Generator\"] = pd.concat((d[\"Generator\"],pd.DataFrame({\"control\" : \"Slack\",\n                                                             \"p_set\" : 0.,\n                                                             \"q_set\" : 0.,\n                                                             \"bus\" : net.bus.name.loc[net.ext_grid.bus].values},\n                                                            index=net.ext_grid.name.fillna(\"External Grid\"))), sort=False)\n\n    d[\"Bus\"].loc[net.bus.name.loc[net.ext_grid.bus].values,\"v_mag_pu_set\"] = net.ext_grid.vm_pu.values\n\n    d[\"Line\"] = pd.DataFrame({\"type\" : net.line.std_type.values,\n                              \"bus0\" : net.bus.name.loc[net.line.from_bus].values,\n                              \"bus1\" : net.bus.name.loc[net.line.to_bus].values,\n                              \"length\" : net.line.length_km.values,\n                              \"num_parallel\" : net.line.parallel.values},\n                             index=net.line.name)\n\n    d[\"Transformer\"] = pd.DataFrame({\"type\" : net.trafo.std_type.values,\n                                     \"bus0\" : net.bus.name.loc[net.trafo.hv_bus].values,\n                                     \"bus1\" : net.bus.name.loc[net.trafo.lv_bus].values,\n                                     \"tap_position\" : net.trafo.tap_pos.values},\n                                    index=net.trafo.name)\n\n    for c in [\"Bus\",\"Load\",\"Generator\",\"Line\",\"Transformer\"]:\n        network.import_components_from_dataframe(d[c],c)\n\n\n\n    #amalgamate buses connected by closed switches\n\n    bus_switches = net.switch[(net.switch.et==\"b\") & net.switch.closed]\n\n    bus_switches[\"stays\"] = bus_switches.bus.map(net.bus.name)\n    bus_switches[\"goes\"] = bus_switches.element.map(net.bus.name)\n\n    to_replace = pd.Series(bus_switches.stays.values,bus_switches.goes.values)\n\n    for i in to_replace.index:\n        network.remove(\"Bus\",i)\n\n        for c in network.iterate_components({\"Load\",\"Generator\"}):\n            c.df.bus.replace(to_replace,inplace=True)\n\n        for c in network.iterate_components({\"Line\",\"Transformer\"}):\n            c.df.bus0.replace(to_replace,inplace=True)\n            c.df.bus1.replace(to_replace,inplace=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef graph(network, branch_components=None, weight=None, inf_weight=False):\n\n    from . import components\n\n    if isinstance(network, components.Network):\n        if branch_components is None:\n            branch_components = network.branch_components\n        buses_i = network.buses.index\n    elif isinstance(network, components.SubNetwork):\n        if branch_components is None:\n            branch_components = network.network.passive_branch_components\n        buses_i = network.buses_i()\n    else:\n        raise TypeError(\"graph must be called with a Network or a SubNetwork\")\n\n    graph = OrderedGraph()\n\n    # add nodes first, in case there are isolated buses not connected with branches\n    graph.add_nodes_from(buses_i)\n\n    # Multigraph uses the branch type and name as key\n    def gen_edges():\n        for c in network.iterate_components(branch_components):\n            for branch in c.df.loc[slice(None) if c.ind is None\n                                               else c.ind].itertuples():\n                if weight is None:\n                    data = {}\n                else:\n                    data = dict(weight=getattr(branch, weight, 0))\n                    if np.isinf(data['weight']) and inf_weight is not True:\n                        if inf_weight is False:\n                            continue\n                        else:\n                            data['weight'] = inf_weight\n\n                yield (branch.bus0, branch.bus1, (c.name, branch.Index), data)\n\n    graph.add_edges_from(gen_edges())\n\n    return graph", "response": "Build a NetworkX graph from a network."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef adjacency_matrix(network, branch_components=None, busorder=None, weights=None):\n\n    from . import components\n\n    if isinstance(network, components.Network):\n        if branch_components is None:\n            branch_components = network.branch_components\n        if busorder is None:\n            busorder = network.buses.index\n    elif isinstance(network, components.SubNetwork):\n        if branch_components is None:\n            branch_components = network.network.passive_branch_components\n        if busorder is None:\n            busorder = network.buses_i()\n    else:\n        raise TypeError(\" must be called with a Network or a SubNetwork\")\n\n    no_buses = len(busorder)\n    no_branches = 0\n    bus0_inds = []\n    bus1_inds = []\n    weight_vals = []\n    for c in network.iterate_components(branch_components):\n        if c.ind is None:\n            sel = slice(None)\n            no_branches = len(c.df)\n        else:\n            sel = c.ind\n            no_branches = len(c.ind)\n        bus0_inds.append(busorder.get_indexer(c.df.loc[sel, \"bus0\"]))\n        bus1_inds.append(busorder.get_indexer(c.df.loc[sel, \"bus1\"]))\n        weight_vals.append(np.ones(no_branches)\n                           if weights is None\n                           else weights[c.name][sel].values)\n\n    if no_branches == 0:\n        return sp.sparse.coo_matrix((no_buses, no_buses))\n\n    bus0_inds = np.concatenate(bus0_inds)\n    bus1_inds = np.concatenate(bus1_inds)\n    weight_vals = np.concatenate(weight_vals)\n\n    return sp.sparse.coo_matrix((weight_vals, (bus0_inds, bus1_inds)),\n                                shape=(no_buses, no_buses))", "response": "Construct a sparse adjacency matrix for a single site."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef incidence_matrix(network, branch_components=None, busorder=None):\n    from . import components\n\n    if isinstance(network, components.Network):\n        if branch_components is None:\n            branch_components = network.branch_components\n        if busorder is None:\n            busorder = network.buses.index\n    elif isinstance(network, components.SubNetwork):\n        if branch_components is None:\n            branch_components = network.network.passive_branch_components\n        if busorder is None:\n            busorder = network.buses_i()\n    else:\n        raise TypeError(\" must be called with a Network or a SubNetwork\")\n\n    no_buses = len(busorder)\n    no_branches = 0\n    bus0_inds = []\n    bus1_inds = []\n    for c in network.iterate_components(branch_components):\n        if c.ind is None:\n            sel = slice(None)\n            no_branches += len(c.df)\n        else:\n            sel = c.ind\n            no_branches += len(c.ind)\n        bus0_inds.append(busorder.get_indexer(c.df.loc[sel, \"bus0\"]))\n        bus1_inds.append(busorder.get_indexer(c.df.loc[sel, \"bus1\"]))\n    bus0_inds = np.concatenate(bus0_inds)\n    bus1_inds = np.concatenate(bus1_inds)\n\n    return sp.sparse.csr_matrix((np.r_[np.ones(no_branches), -np.ones(no_branches)],\n                                 (np.r_[bus0_inds, bus1_inds], np.r_[:no_branches, :no_branches])),\n                                (no_buses, no_branches))", "response": "Construct a sparse incidence matrix for a single site."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef l_objective(model,objective=None):\n\n    if objective is None:\n        objective = LExpression()\n\n    #initialise with a dummy\n    model.objective = Objective(expr = 0.)\n    model.objective._expr = _build_sum_expression(objective.variables, constant=objective.constant)", "response": "A replacement for pyomo s Objective that quickly builds linear\n    objectives."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfunctions called when network is created to build component pandas. DataFrames.", "response": "def _build_dataframes(self):\n        \"\"\"Function called when network is created to build component pandas.DataFrames.\"\"\"\n\n        for component in self.all_components:\n\n            attrs = self.components[component][\"attrs\"]\n\n            static_dtypes = attrs.loc[attrs.static, \"dtype\"].drop([\"name\"])\n\n            df = pd.DataFrame({k: pd.Series(dtype=d) for k, d in static_dtypes.iteritems()},\n                              columns=static_dtypes.index)\n\n            df.index.name = \"name\"\n\n            setattr(self,self.components[component][\"list_name\"],df)\n\n            pnl = Dict({k : pd.DataFrame(index=self.snapshots,\n                                         columns=[],\n                                         #it's currently hard to imagine non-float series, but this could be generalised\n                                         dtype=np.dtype(float))\n                        for k in attrs.index[attrs.varying]})\n\n            setattr(self,self.components[component][\"list_name\"]+\"_t\",pnl)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the snapshots and reindex all time - dependent data.", "response": "def set_snapshots(self,snapshots):\n        \"\"\"\n        Set the snapshots and reindex all time-dependent data.\n\n        This will reindex all pandas.Panels of time-dependent data; NaNs are filled\n        with the default value for that quantity.\n\n        Parameters\n        ----------\n        snapshots : list or pandas.Index\n            All time steps.\n\n        Returns\n        -------\n        None\n        \"\"\"\n\n        self.snapshots = pd.Index(snapshots)\n\n        self.snapshot_weightings = self.snapshot_weightings.reindex(self.snapshots,fill_value=1.)\n        if isinstance(snapshots, pd.DatetimeIndex) and _pd_version < '0.18.0':\n            snapshots = pd.Index(snapshots.values)\n\n        for component in self.all_components:\n            pnl = self.pnl(component)\n            attrs = self.components[component][\"attrs\"]\n\n            for k,default in attrs.default[attrs.varying].iteritems():\n                pnl[k] = pnl[k].reindex(self.snapshots).fillna(default)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add(self, class_name, name, **kwargs):\n\n        assert class_name in self.components, \"Component class {} not found\".format(class_name)\n\n        cls_df = self.df(class_name)\n        cls_pnl = self.pnl(class_name)\n\n        name = str(name)\n\n        assert name not in cls_df.index, \"Failed to add {} component {} because there is already an object with this name in {}\".format(class_name, name, self.components[class_name][\"list_name\"])\n\n        attrs = self.components[class_name][\"attrs\"]\n\n        static_attrs = attrs[attrs.static].drop(\"name\")\n\n        #This guarantees that the correct attribute type is maintained\n        obj_df = pd.DataFrame(data=[static_attrs.default],index=[name],columns=static_attrs.index)\n        new_df = cls_df.append(obj_df, sort=False)\n\n        setattr(self, self.components[class_name][\"list_name\"], new_df)\n\n        for k,v in iteritems(kwargs):\n            if k not in attrs.index:\n                logger.warning(\"{} has no attribute {}, ignoring this passed value.\".format(class_name,k))\n                continue\n            typ = attrs.at[k, \"typ\"]\n            if not attrs.at[k,\"varying\"]:\n                new_df.at[name,k] = typ(v)\n            elif attrs.at[k,\"static\"] and not isinstance(v, (pd.Series, np.ndarray, list)):\n                new_df.at[name,k] = typ(v)\n            else:\n                cls_pnl[k][name] = pd.Series(data=v, index=self.snapshots, dtype=typ)\n\n\n        for attr in [\"bus\",\"bus0\",\"bus1\"]:\n            if attr in new_df.columns:\n                bus_name = new_df.at[name,attr]\n                if bus_name not in self.buses.index:\n                    logger.warning(\"The bus name `{}` given for {} of {} `{}` does not appear in network.buses\".format(bus_name,attr,class_name,name))", "response": "Add a single component to the network."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove a single component from the network.", "response": "def remove(self, class_name, name):\n        \"\"\"\n        Removes a single component from the network.\n\n        Removes it from component DataFrames.\n\n        Parameters\n        ----------\n        class_name : string\n            Component class name\n        name : string\n            Component name\n\n        Examples\n        --------\n        >>> network.remove(\"Line\",\"my_line 12345\")\n\n        \"\"\"\n\n        if class_name not in self.components:\n            logger.error(\"Component class {} not found\".format(class_name))\n            return None\n\n        cls_df = self.df(class_name)\n\n        cls_df.drop(name, inplace=True)\n\n        pnl = self.pnl(class_name)\n\n        for df in itervalues(pnl):\n            if name in df:\n                df.drop(name, axis=1, inplace=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding multiple components to the network along with their attributes.", "response": "def madd(self, class_name, names, suffix='', **kwargs):\n        \"\"\"\n        Add multiple components to the network, along with their attributes.\n\n        Make sure when adding static attributes as pandas Series that they are indexed\n        by names. Make sure when adding time-varying attributes as pandas DataFrames that\n        their index is a superset of network.snapshots and their columns are a\n        subset of names.\n\n        Parameters\n        ----------\n        class_name : string\n            Component class name in [\"Bus\",\"Generator\",\"Load\",\"StorageUnit\",\"Store\",\"ShuntImpedance\",\"Line\",\"Transformer\",\"Link\"]\n        names : list-like or pandas.Index\n            Component names\n        suffix : string, default ''\n            All components are named after names with this added suffix. It\n            is assumed that all Series and DataFrames are indexed by the original names.\n        kwargs\n            Component attributes, e.g. x=[0.1,0.2], can be list, pandas.Series of pandas.DataFrame for time-varying\n\n        Returns\n        --------\n        new_names : pandas.index\n            Names of new components (including suffix)\n\n        Examples\n        --------\n        >>> network.madd(\"Load\", [\"load 1\", \"load 2\"], bus=[\"1\",\"2\"], p_set=np.random.rand(len(network.snapshots),2))\n\n        \"\"\"\n\n        if class_name not in self.components:\n            logger.error(\"Component class {} not found\".format(class_name))\n            return None\n\n        if not isinstance(names, pd.Index):\n            names = pd.Index(names)\n\n        new_names = names.astype(str) + suffix\n\n        static = {}; series = {}\n        for k, v in iteritems(kwargs):\n            if isinstance(v, pd.DataFrame):\n                series[k] = v.rename(columns=lambda i: str(i)+suffix)\n            elif isinstance(v, pd.Series):\n                static[k] = v.rename(lambda i: str(i)+suffix)\n            elif isinstance(v, np.ndarray) and v.shape == (len(self.snapshots), len(names)):\n                series[k] = pd.DataFrame(v, index=self.snapshots, columns=new_names)\n            else:\n                static[k] = v\n\n        self.import_components_from_dataframe(pd.DataFrame(static, index=new_names), class_name)\n\n        for k, v in iteritems(series):\n            self.import_series_from_dataframe(v, class_name, k)\n\n        return new_names"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mremove(self, class_name, names):\n\n        if class_name not in self.components:\n            logger.error(\"Component class {} not found\".format(class_name))\n            return None\n\n        if not isinstance(names, pd.Index):\n            names = pd.Index(names)\n\n        cls_df = self.df(class_name)\n\n        cls_df.drop(names, inplace=True)\n\n        pnl = self.pnl(class_name)\n\n        for df in itervalues(pnl):\n            df.drop(df.columns.intersection(names), axis=1, inplace=True)", "response": "Removes multiple components from the network."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef copy(self, with_time=True, ignore_standard_types=False):\n\n        override_components, override_component_attrs = self._retrieve_overridden_components()\n\n        network = self.__class__(ignore_standard_types=ignore_standard_types,\n                                 override_components=override_components,\n                                 override_component_attrs=override_component_attrs)\n\n        for component in self.iterate_components([\"Bus\", \"Carrier\"] + sorted(self.all_components - {\"Bus\",\"Carrier\"})):\n            df = component.df\n            #drop the standard types to avoid them being read in twice\n            if not ignore_standard_types and component.name in self.standard_type_components:\n                df = component.df.drop(network.components[component.name][\"standard_types\"].index)\n\n            import_components_from_dataframe(network, df, component.name)\n\n        if with_time:\n            network.set_snapshots(self.snapshots)\n            for component in self.iterate_components():\n                pnl = getattr(network, component.list_name+\"_t\")\n                for k in iterkeys(component.pnl):\n                    pnl[k] = component.pnl[k].copy()\n\n        #catch all remaining attributes of network\n        for attr in [\"name\", \"srid\"]:\n            setattr(network,attr,getattr(self,attr))\n\n        network.snapshot_weightings = self.snapshot_weightings.copy()\n\n        return network", "response": "Returns a deep copy of the Network object with all components and time - dependent data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding sub_networks from topology.", "response": "def determine_network_topology(self):\n        \"\"\"\n        Build sub_networks from topology.\n        \"\"\"\n\n        adjacency_matrix = self.adjacency_matrix(self.passive_branch_components)\n        n_components, labels = csgraph.connected_components(adjacency_matrix, directed=False)\n\n        # remove all old sub_networks\n        for sub_network in self.sub_networks.index:\n            obj = self.sub_networks.at[sub_network,\"obj\"]\n            self.remove(\"SubNetwork\", sub_network)\n            del obj\n\n        for i in np.arange(n_components):\n            # index of first bus\n            buses_i = (labels == i).nonzero()[0]\n            carrier = self.buses.carrier.iat[buses_i[0]]\n\n            if carrier not in [\"AC\",\"DC\"] and len(buses_i) > 1:\n                logger.warning(\"Warning, sub network {} is not electric but contains multiple buses\\n\"\n                                \"and branches. Passive flows are not allowed for non-electric networks!\".format(i))\n\n            if (self.buses.carrier.iloc[buses_i] != carrier).any():\n                logger.warning(\"Warning, sub network {} contains buses with mixed carriers! Value counts:\\n{}\".format(i),\n                                self.buses.carrier.iloc[buses_i].value_counts())\n\n            self.add(\"SubNetwork\", i, carrier=carrier)\n\n        #add objects\n        self.sub_networks[\"obj\"] = [SubNetwork(self, name) for name in self.sub_networks.index]\n\n        self.buses.loc[:, \"sub_network\"] = labels.astype(str)\n\n        for c in self.iterate_components(self.passive_branch_components):\n            c.df[\"sub_network\"] = c.df.bus0.map(self.buses[\"sub_network\"])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck the network for consistency.", "response": "def consistency_check(self):\n        \"\"\"\n        Checks the network for consistency, including bus definitions and impedances.\n        Prints warnings if anything is potentially inconsistent.\n\n        Examples\n        --------\n        >>> network.consistency_check()\n\n        \"\"\"\n\n\n        for c in self.iterate_components(self.one_port_components):\n            missing = c.df.index[~c.df.bus.isin(self.buses.index)]\n            if len(missing) > 0:\n                logger.warning(\"The following %s have buses which are not defined:\\n%s\",\n                               c.list_name, missing)\n\n        for c in self.iterate_components(self.branch_components):\n            for attr in [\"bus0\",\"bus1\"]:\n                missing = c.df.index[~c.df[attr].isin(self.buses.index)]\n                if len(missing) > 0:\n                    logger.warning(\"The following %s have %s which are not defined:\\n%s\",\n                                   c.list_name, attr, missing)\n\n\n        for c in self.iterate_components(self.passive_branch_components):\n            for attr in [\"x\",\"r\"]:\n                bad = c.df.index[c.df[attr] == 0.]\n                if len(bad) > 0:\n                    logger.warning(\"The following %s have zero %s, which could break the linear load flow:\\n%s\",\n                                   c.list_name, attr, bad)\n\n            bad = c.df.index[(c.df[\"x\"] == 0.) & (c.df[\"r\"] == 0.)]\n            if len(bad) > 0:\n                logger.warning(\"The following %s have zero series impedance, which will break the load flow:\\n%s\",\n                               c.list_name, bad)\n\n\n        for c in self.iterate_components({\"Transformer\"}):\n            bad = c.df.index[c.df[\"s_nom\"] == 0.]\n            if len(bad) > 0:\n                logger.warning(\"The following %s have zero s_nom, which is used to define the impedance and will thus break the load flow:\\n%s\",\n                               c.list_name, bad)\n\n\n        for c in self.iterate_components(self.all_components):\n            for attr in c.attrs.index[c.attrs.varying & c.attrs.static]:\n                attr_df = c.pnl[attr]\n\n                diff = attr_df.columns.difference(c.df.index)\n                if len(diff) > 0:\n                    logger.warning(\"The following %s have time series defined for attribute %s in network.%s_t, but are not defined in network.%s:\\n%s\",\n                                   c.list_name, attr, c.list_name, c.list_name, diff)\n\n                diff = self.snapshots.difference(attr_df.index)\n                if len(diff) > 0:\n                    logger.warning(\"In the time-dependent Dataframe for attribute %s of network.%s_t the following snapshots are missing:\\n%s\",\n                                   attr, c.list_name, diff)\n\n                diff = attr_df.index.difference(self.snapshots)\n                if len(diff) > 0:\n                    logger.warning(\"In the time-dependent Dataframe for attribute %s of network.%s_t the following snapshots are defined which are not in network.snapshots:\\n%s\",\n                                   attr, c.list_name, diff)\n\n        static_attrs = ['p_nom', 's_nom', 'e_nom']\n        varying_attrs = ['p_max_pu', 'e_max_pu']\n        for c in self.iterate_components(self.all_components - {'TransformerType'}):\n            varying_attr = c.attrs.index[c.attrs.varying].intersection(varying_attrs)\n            static_attr = c.attrs.index[c.attrs.static].intersection(static_attrs)\n\n            if len(static_attr):\n                diff = (getattr(self, c.list_name)[static_attr[0] + \"_max\"] -\n                        getattr(self, c.list_name)[static_attr[0] + \"_min\"])\n                if not diff[diff < 0].empty:\n                    logger.warning(\"The following %s have smaller maximum than minimum expansion limit which can lead to infeasibilty:\\n%s\",\n                                   c.list_name, diff[diff < 0].index)\n\n            if len(varying_attr):\n                max_pu = get_switchable_as_dense(self, c.name, varying_attr[0][0] + \"_max_pu\")\n                min_pu = get_switchable_as_dense(self, c.name, varying_attr[0][0] + \"_min_pu\")\n\n                # check for NaN values:\n                if max_pu.isnull().values.any():\n                    for col in max_pu.columns[max_pu.isnull().any()]:\n                        logger.warning(\"The attribute %s of element %s of %s has NaN values for the following snapshots:\\n%s\",\n                                       varying_attr[0][0] + \"_max_pu\", col, c.list_name, max_pu.index[max_pu[col].isnull()])\n                if min_pu.isnull().values.any():\n                    for col in min_pu.columns[min_pu.isnull().any()]:\n                        logger.warning(\"The attribute %s of element %s of %s has NaN values for the following snapshots:\\n%s\",\n                                       varying_attr[0][0] + \"_min_pu\", col, c.list_name, min_pu.index[min_pu[col].isnull()])\n\n                diff = max_pu - min_pu\n                diff = diff[diff < 0].dropna(axis=1, how='all')\n                for col in diff.columns:\n                    logger.warning(\"The element %s of %s has a smaller maximum than minimum operational limit which can lead to infeasibility for the following snapshots:\\n%s\",\n                                   col, c.list_name, diff[col].dropna().index)\n\n        #check all dtypes of component attributes\n\n        for c in self.iterate_components():\n\n            #first check static attributes\n\n            dtypes_soll = c.attrs.loc[c.attrs[\"static\"], \"dtype\"].drop(\"name\")\n            unmatched = (c.df.dtypes[dtypes_soll.index] != dtypes_soll)\n\n            if unmatched.any():\n                logger.warning(\"The following attributes of the dataframe %s have the wrong dtype:\\n%s\\n\"\n                               \"They are:\\n%s\\n\"\n                               \"but should be:\\n%s\",\n                               c.list_name,\n                               unmatched.index[unmatched],\n                               c.df.dtypes[dtypes_soll.index[unmatched]],\n                               dtypes_soll[unmatched])\n\n            #now check varying attributes\n\n            types_soll = c.attrs.loc[c.attrs[\"varying\"], [\"typ\", \"dtype\"]]\n\n            for attr, typ, dtype in types_soll.itertuples():\n                if c.pnl[attr].empty:\n                    continue\n\n                unmatched = (c.pnl[attr].dtypes != dtype)\n\n                if unmatched.any():\n                    logger.warning(\"The following columns of time-varying attribute %s in %s_t have the wrong dtype:\\n%s\\n\"\n                                   \"They are:\\n%s\\n\"\n                                   \"but should be:\\n%s\",\n                                   attr,c.list_name,\n                                   unmatched.index[unmatched],\n                                   c.pnl[attr].dtypes[unmatched],\n                                   typ)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild HTML page and writes it.", "response": "def process_org(org_name):\n    \"\"\"Build HTML page and write it.\"\"\"\n\n    html_name = org_name[:-4] + \".html\"\n\n    root_path = org_name.count(\"/\")*\"../\"\n\n    hierarchy_path = []\n\n    def find_path(layer,path,hierarchy_path):\n        if len(path) > 0 and path[-1] == html_name:\n            hierarchy_path += path\n        else:\n            for item in layer:\n                new_path = path + [item[0]]\n                if len(item) > 2:\n                    new_layer = item[2]\n                else:\n                    new_layer = []\n                find_path(new_layer,new_path,hierarchy_path)\n\n    find_path(hierarchy,[],hierarchy_path)\n\n    hierarchy_path.append(\"\")\n\n    layer = hierarchy\n\n    name = \"\"\n\n    menu = \"\"\n\n    for n,item in enumerate(hierarchy_path):\n\n        bare_layer = [i[0] for i in layer]\n\n        filled_layer = [[i[0],i[0][:i[0].rfind(\".\")]] if len(i) < 2 else i for i in layer]\n\n        menu += \"<ul id=\\\"{}menu\\\">\\n\".format(\"sub\"*(n+1))\n\n        menu += \"\".join([\"<li><a href=\\\"{}\\\">{}</a></li>\\n\".format(root_path + i[0],i[1]) if i[0] != item and i[0] != html_name else \"<li><span class=\\\"current\\\">{}</span></li>\\n\".format(i[1]) for i in filled_layer])\n\n        menu += \"</ul>\\n\"\n\n        if item != \"\":\n            full_item = layer[ bare_layer.index(item)]\n            for i in filled_layer:\n                if i[0] == html_name:\n                    name = i[1]\n        else:\n            full_item = []\n\n        if len(full_item) < 3:\n            break\n\n        layer = full_item[2]\n\n\n    html = \"\"\"<!DOCTYPE html\nPUBLIC \\\"-//W3C//DTD XHTML 1.0 Transitional//EN\\\"\n\\\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\\\">\n<html>\n<head>\n<title>pypsa | {}</title>\n<link rel=\\\"stylesheet\\\" type=\\\"text/css\\\" href=\\\"{}\\\" />\n</head>\n<body>\n<div id=\"outer_box\">\n<div id=\"header\">\n\n{}\n\n\"\"\".format(name, root_path + \"theme.css\", menu)\n\n\n    html += \"\\n\\n</div>\\n\\n<div id=\\\"main\\\">\\n\\n\"\n\n\n    command = \"emacs {} --batch -f org-html-export-to-html --kill\".format(org_name)\n\n    os.system(command)\n\n    f = open(html_name,\"r\")\n\n    org_html = f.read()\n\n    f.close()\n\n\n    if \"outer_box\" in org_html:\n        print(\"File is already processed, skipping\")\n        return\n\n    start_string = '<div id=\"content\">'\n\n    if start_string not in org_html:\n        print(\"Start string not found, skipping\")\n        return\n\n\n    end_string = '<div id=\"postamble\" class=\"status\">'\n    if end_string not in org_html:\n        print(\"End string not found, skipping\")\n        return\n\n    new = org_html[org_html.find(start_string)+len(start_string):org_html.find(end_string)]\n\n\n    html  = html + new + \"\\n\\n</body>\\n</html>\\n\"\n\n    f = open(html_name,\"w\")\n\n    f.write(html)\n\n    f.close()\n\n    return html"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compatibility_mode():\n    setattr(EventsCodes, 'ALL_EVENTS', ALL_EVENTS)\n    for evname in globals():\n        if evname.startswith('IN_'):\n            setattr(EventsCodes, evname, globals()[evname])\n    global COMPATIBILITY_MODE\n    COMPATIBILITY_MODE = True", "response": "Sets the COMPATIBILITY_MODE flag to True."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncommand line interface for the command line interface.", "response": "def command_line():\n    \"\"\"\n    By default the watched path is '/tmp' and all types of events are\n    monitored. Events monitoring serves forever, type c^c to stop it.\n    \"\"\"\n    from optparse import OptionParser\n\n    usage = \"usage: %prog [options] [path1] [path2] [pathn]\"\n\n    parser = OptionParser(usage=usage)\n    parser.add_option(\"-v\", \"--verbose\", action=\"store_true\",\n                      dest=\"verbose\", help=\"Verbose mode\")\n    parser.add_option(\"-r\", \"--recursive\", action=\"store_true\",\n                      dest=\"recursive\",\n                      help=\"Add watches recursively on paths\")\n    parser.add_option(\"-a\", \"--auto_add\", action=\"store_true\",\n                      dest=\"auto_add\",\n                      help=\"Automatically add watches on new directories\")\n    parser.add_option(\"-g\", \"--glob\", action=\"store_true\",\n                      dest=\"glob\",\n                      help=\"Treat paths as globs\")\n    parser.add_option(\"-e\", \"--events-list\", metavar=\"EVENT[,...]\",\n                      dest=\"events_list\",\n                      help=(\"A comma-separated list of events to watch for - \"\n                           \"see the documentation for valid options (defaults\"\n                           \" to everything)\"))\n    parser.add_option(\"-s\", \"--stats\", action=\"store_true\",\n                      dest=\"stats\",\n                      help=\"Display dummy statistics\")\n    parser.add_option(\"-V\", \"--version\", action=\"store_true\",\n                      dest=\"version\",  help=\"Pyinotify version\")\n    parser.add_option(\"-f\", \"--raw-format\", action=\"store_true\",\n                      dest=\"raw_format\",\n                      help=\"Disable enhanced output format.\")\n    parser.add_option(\"-c\", \"--command\", action=\"store\",\n                      dest=\"command\",\n                      help=\"Shell command to run upon event\")\n\n    (options, args) = parser.parse_args()\n\n    if options.verbose:\n        log.setLevel(10)\n\n    if options.version:\n        print(__version__)\n\n    if not options.raw_format:\n        global output_format\n        output_format = ColoredOutputFormat()\n\n    if len(args) < 1:\n        path = '/tmp'  # default watched path\n    else:\n        path = args\n\n    # watch manager instance\n    wm = WatchManager()\n    # notifier instance and init\n    if options.stats:\n        notifier = Notifier(wm, default_proc_fun=Stats(), read_freq=5)\n    else:\n        notifier = Notifier(wm, default_proc_fun=PrintAllEvents())\n\n    # What mask to apply\n    mask = 0\n    if options.events_list:\n        events_list = options.events_list.split(',')\n        for ev in events_list:\n            evcode = EventsCodes.ALL_FLAGS.get(ev, 0)\n            if evcode:\n                mask |= evcode\n            else:\n                parser.error(\"The event '%s' specified with option -e\"\n                             \" is not valid\" % ev)\n    else:\n        mask = ALL_EVENTS\n\n    # stats\n    cb_fun = None\n    if options.stats:\n        def cb(s):\n            sys.stdout.write(repr(s.proc_fun()))\n            sys.stdout.write('\\n')\n            sys.stdout.write(str(s.proc_fun()))\n            sys.stdout.write('\\n')\n            sys.stdout.flush()\n        cb_fun = cb\n\n    # External command\n    if options.command:\n        def cb(s):\n            subprocess.Popen(options.command, shell=True)\n        cb_fun = cb\n\n    log.debug('Start monitoring %s, (press c^c to halt pyinotify)' % path)\n\n    wm.add_watch(path, mask, rec=options.recursive, auto_add=options.auto_add, do_glob=options.glob)\n    # Loop forever (until sigint signal get caught)\n    notifier.loop(callback=cb_fun)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new object of the right type.", "response": "def create():\n        \"\"\"\n        Factory method instanciating and returning the right wrapper.\n        \"\"\"\n        # First, try to use ctypes.\n        if ctypes:\n            inotify = _CtypesLibcINotifyWrapper()\n            if inotify.init():\n                return inotify\n        # Second, see if C extension is compiled.\n        if inotify_syscalls:\n            inotify = _INotifySyscallsWrapper()\n            if inotify.init():\n                return inotify"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_val(self):\n        with open(os.path.join(self._base, self._attr), 'r') as file_obj:\n            return int(file_obj.readline())", "response": "Gets attribute s value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset new attribute s value.", "response": "def set_val(self, nval):\n        \"\"\"\n        Sets new attribute's value.\n\n        @param nval: replaces current value by nval.\n        @type nval: int\n        @raise IOError: if corresponding file in /proc/sys cannot be written.\n        \"\"\"\n        with open(os.path.join(self._base, self._attr), 'w') as file_obj:\n            file_obj.write(str(nval) + '\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the event name associated to a given mask.", "response": "def maskname(mask):\n        \"\"\"\n        Returns the event name associated to mask. IN_ISDIR is appended to\n        the result when appropriate. Note: only one event is returned, because\n        only one event can be raised at a given time.\n\n        @param mask: mask.\n        @type mask: int\n        @return: event name.\n        @rtype: str\n        \"\"\"\n        ms = mask\n        name = '%s'\n        if mask & IN_ISDIR:\n            ms = mask - IN_ISDIR\n            name = '%s|IN_ISDIR'\n        return name % EventsCodes.ALL_VALUES[ms]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmaps the cookie with the source path + date for cleaning.", "response": "def process_IN_MOVED_FROM(self, raw_event):\n        \"\"\"\n        Map the cookie with the source path (+ date for cleaning).\n        \"\"\"\n        watch_ = self._watch_manager.get_watch(raw_event.wd)\n        path_ = watch_.path\n        src_path = os.path.normpath(os.path.join(path_, raw_event.name))\n        self._mv_cookie[raw_event.cookie] = (src_path, datetime.now())\n        return self.process_default(raw_event, {'cookie': raw_event.cookie})"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmaps the source path with the destination path (+ date for cleaning).", "response": "def process_IN_MOVED_TO(self, raw_event):\n        \"\"\"\n        Map the source path with the destination path (+ date for\n        cleaning).\n        \"\"\"\n        watch_ = self._watch_manager.get_watch(raw_event.wd)\n        path_ = watch_.path\n        dst_path = os.path.normpath(os.path.join(path_, raw_event.name))\n        mv_ = self._mv_cookie.get(raw_event.cookie)\n        to_append = {'cookie': raw_event.cookie}\n        if mv_ is not None:\n            self._mv[mv_[0]] = (dst_path, datetime.now())\n            # Let's assume that IN_MOVED_FROM event is always queued before\n            # that its associated (they share a common cookie) IN_MOVED_TO\n            # event is queued itself. It is then possible in that scenario\n            # to provide as additional information to the IN_MOVED_TO event\n            # the original pathname of the moved file/directory.\n            to_append['src_pathname'] = mv_[0]\n        elif (raw_event.mask & IN_ISDIR and watch_.auto_add and\n              not watch_.exclude_filter(dst_path)):\n            # We got a diretory that's \"moved in\" from an unknown source and\n            # auto_add is enabled. Manually add watches to the inner subtrees.\n            # The newly monitored directory inherits attributes from its\n            # parent directory.\n            self._watch_manager.add_watch(dst_path, watch_.mask,\n                                          proc_fun=watch_.proc_fun,\n                                          rec=True, auto_add=True,\n                                          exclude_filter=watch_.exclude_filter)\n        return self.process_default(raw_event, to_append)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_IN_IGNORED(self, raw_event):\n        event_ = self.process_default(raw_event)\n        self._watch_manager.del_watch(raw_event.wd)\n        return event_", "response": "Process an event in an ignored state."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing the default event.", "response": "def process_default(self, raw_event, to_append=None):\n        \"\"\"\n        Commons handling for the followings events:\n\n        IN_ACCESS, IN_MODIFY, IN_ATTRIB, IN_CLOSE_WRITE, IN_CLOSE_NOWRITE,\n        IN_OPEN, IN_DELETE, IN_DELETE_SELF, IN_UNMOUNT.\n        \"\"\"\n        watch_ = self._watch_manager.get_watch(raw_event.wd)\n        if raw_event.mask & (IN_DELETE_SELF | IN_MOVE_SELF):\n            # Unfornulately this information is not provided by the kernel\n            dir_ = watch_.dir\n        else:\n            dir_ = bool(raw_event.mask & IN_ISDIR)\n        dict_ = {'wd': raw_event.wd,\n                 'mask': raw_event.mask,\n                 'path': watch_.path,\n                 'name': raw_event.name,\n                 'dir': dir_}\n        if COMPATIBILITY_MODE:\n            dict_['is_dir'] = dir_\n        if to_append is not None:\n            dict_.update(to_append)\n        return Event(dict_)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite event string representation to file object provided to my_init.", "response": "def process_default(self, event):\n        \"\"\"\n        Writes event string representation to file object provided to\n        my_init().\n\n        @param event: Event to be processed. Can be of any type of events but\n                      IN_Q_OVERFLOW events (see method process_IN_Q_OVERFLOW).\n        @type event: Event instance\n        \"\"\"\n        self._out.write(str(event))\n        self._out.write('\\n')\n        self._out.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef my_init(self):\n        self._start_time = time.time()\n        self._stats = {}\n        self._stats_lock = threading.Lock()", "response": "Method automatically called from base class constructor."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dump(self, filename):\n        flags = os.O_WRONLY|os.O_CREAT|os.O_NOFOLLOW|os.O_EXCL\n        fd = os.open(filename, flags, 0o0600)\n        os.write(fd, bytes(self.__str__(), locale.getpreferredencoding()))\n        os.close(fd)", "response": "Dumps the current object to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef coalesce_events(self, coalesce=True):\n        self._coalesce = coalesce\n        if not coalesce:\n            self._eventset.clear()", "response": "Coalescing events. Events are usually processed by batchs, their size\n        depend on various factors. Thus, before processing them, events received\n        from inotify are aggregated in a fifo queue. If this coalescing\n        option is enabled events are filtered based on their unicity, only\n        unique events are enqueued, doublons are discarded. An event is unique\n        when the combination of its fields (wd, mask, cookie, name) is unique\n        among events of a same batch. After a batch of events is processed any\n        events is accepted again. By default this option is disabled, you have\n        to explictly call this function to turn it on.\n\n        @param coalesce: Optional new coalescing value. True by default.\n        @type coalesce: Bool"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_events(self):\n        while self._eventq:\n            raw_event = self._eventq.popleft()  # pop next event\n            if self._watch_manager.ignore_events:\n                log.debug(\"Event ignored: %s\" % repr(raw_event))\n                continue\n            watch_ = self._watch_manager.get_watch(raw_event.wd)\n            if (watch_ is None) and not (raw_event.mask & IN_Q_OVERFLOW):\n                if not (raw_event.mask & IN_IGNORED):\n                    # Not really sure how we ended up here, nor how we should\n                    # handle these types of events and if it is appropriate to\n                    # completly skip them (like we are doing here).\n                    log.warning(\"Unable to retrieve Watch object associated to %s\",\n                                repr(raw_event))\n                continue\n            revent = self._sys_proc_fun(raw_event)  # system processings\n            if watch_ and watch_.proc_fun:\n                watch_.proc_fun(revent)  # user processings\n            else:\n                self._default_proc_fun(revent)\n        self._sys_proc_fun.cleanup()  # remove olds MOVED_* events records\n        if self._coalesce:\n            self._eventset.clear()", "response": "Process events from the queue and process them."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef loop(self, callback=None, daemonize=False, **args):\n        if daemonize:\n            self.__daemonize(**args)\n\n        # Read and process events forever\n        while 1:\n            try:\n                self.process_events()\n                if (callback is not None) and (callback(self) is True):\n                    break\n                ref_time = time.time()\n                # check_events is blocking\n                if self.check_events():\n                    self._sleep(ref_time)\n                    self.read_events()\n            except KeyboardInterrupt:\n                # Stop monitoring if sigint is caught (Control-C).\n                log.debug('Pyinotify stops monitoring.')\n                break\n        # Close internals\n        self.stop()", "response": "This method is called every time a new event is received from the system."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstops the inotify instance.", "response": "def stop(self):\n        \"\"\"\n        Close inotify's instance (close its file descriptor).\n        It destroys all existing watches, pending events,...\n        This method is automatically called at the end of loop().\n        Afterward it is invalid to access this instance.\n        \"\"\"\n        if self._fd is not None:\n            self._pollobj.unregister(self._fd)\n            os.close(self._fd)\n            self._fd = None\n        self._sys_proc_fun = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stop(self):\n        self._stop_event.set()\n        os.write(self._pipe[1], b'stop')\n        threading.Thread.join(self)\n        Notifier.stop(self)\n        self._pollobj.unregister(self._pipe[0])\n        os.close(self._pipe[0])\n        os.close(self._pipe[1])", "response": "Stop the thread and unregister the thread."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef loop(self):\n        # When the loop must be terminated .stop() is called, 'stop'\n        # is written to pipe fd so poll() returns and .check_events()\n        # returns False which make evaluate the While's stop condition\n        # ._stop_event.isSet() wich put an end to the thread's execution.\n        while not self._stop_event.isSet():\n            self.process_events()\n            ref_time = time.time()\n            if self.check_events():\n                self._sleep(ref_time)\n                self.read_events()", "response": "This is the main loop for the While thread."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_read(self, *args, **kwargs):\n        self.read_events()\n        self.process_events()\n        if self.handle_read_callback is not None:\n            self.handle_read_callback(self)", "response": "Called by the event loop when a read event occurs."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves the watch entry associated to watch descriptor wd.", "response": "def del_watch(self, wd):\n        \"\"\"\n        Remove watch entry associated to watch descriptor wd.\n\n        @param wd: Watch descriptor.\n        @type wd: int\n        \"\"\"\n        try:\n            del self._wmd[wd]\n        except KeyError as err:\n            log.error('Cannot delete unknown watch descriptor %s' % str(err))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a watch on path and return the wd value.", "response": "def __add_watch(self, path, mask, proc_fun, auto_add, exclude_filter):\n        \"\"\"\n        Add a watch on path, build a Watch object and insert it in the\n        watch manager dictionary. Return the wd value.\n        \"\"\"\n        path = self.__format_path(path)\n        if auto_add and not mask & IN_CREATE:\n            mask |= IN_CREATE\n        wd = self._inotify_wrapper.inotify_add_watch(self._fd, path, mask)\n        if wd < 0:\n            return wd\n        watch = Watch(wd=wd, path=path, mask=mask, proc_fun=proc_fun,\n                      auto_add=auto_add, exclude_filter=exclude_filter)\n        # wd are _always_ indexed with their original unicode paths in wmd.\n        self._wmd[wd] = watch\n        log.debug('New %s', watch)\n        return wd"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting every wd from self. _wmd if its path is under the path of . Doesn t follow symlinks.", "response": "def __get_sub_rec(self, lpath):\n        \"\"\"\n        Get every wd from self._wmd if its path is under the path of\n        one (at least) of those in lpath. Doesn't follow symlinks.\n\n        @param lpath: list of watch descriptor\n        @type lpath: list of int\n        @return: list of watch descriptor\n        @rtype: list of int\n        \"\"\"\n        for d in lpath:\n            root = self.get_path(d)\n            if root is not None:\n                # always keep root\n                yield d\n            else:\n                # if invalid\n                continue\n\n            # nothing else to expect\n            if not os.path.isdir(root):\n                continue\n\n            # normalization\n            root = os.path.normpath(root)\n            # recursion\n            lend = len(root)\n            for iwd in self._wmd.items():\n                cur = iwd[1].path\n                pref = os.path.commonprefix([root, cur])\n                if root == os.sep or (len(pref) == lend and \\\n                                      len(cur) > lend and \\\n                                      cur[lend] == os.sep):\n                    yield iwd[1].wd"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_watch(self, wd, mask=None, proc_fun=None, rec=False,\n                     auto_add=False, quiet=True):\n        \"\"\"\n        Update existing watch descriptors |wd|. The |mask| value, the\n        processing object |proc_fun|, the recursive param |rec| and the\n        |auto_add| and |quiet| flags can all be updated.\n\n        @param wd: Watch Descriptor to update. Also accepts a list of\n                   watch descriptors.\n        @type wd: int or list of int\n        @param mask: Optional new bitmask of events.\n        @type mask: int\n        @param proc_fun: Optional new processing function.\n        @type proc_fun: function or ProcessEvent instance or instance of\n                        one of its subclasses or callable object.\n        @param rec: Optionally adds watches recursively on all\n                    subdirectories contained into |wd| directory.\n        @type rec: bool\n        @param auto_add: Automatically adds watches on newly created\n                         directories in the watch's path corresponding to |wd|.\n                         If |auto_add| is True, IN_CREATE is ored with |mask|\n                         when the watch is updated.\n        @type auto_add: bool\n        @param quiet: If False raises a WatchManagerError exception on\n                      error. See example not_quiet.py\n        @type quiet: bool\n        @return: dict of watch descriptors associated to booleans values.\n                 True if the corresponding wd has been successfully\n                 updated, False otherwise.\n        @rtype: dict of {int: bool}\n        \"\"\"\n        lwd = self.__format_param(wd)\n        if rec:\n            lwd = self.__get_sub_rec(lwd)\n\n        ret_ = {}  # return {wd: bool, ...}\n        for awd in lwd:\n            apath = self.get_path(awd)\n            if not apath or awd < 0:\n                err = 'update_watch: invalid WD=%d' % awd\n                if quiet:\n                    log.error(err)\n                    continue\n                raise WatchManagerError(err, ret_)\n\n            if mask:\n                wd_ = self._inotify_wrapper.inotify_add_watch(self._fd, apath,\n                                                              mask)\n                if wd_ < 0:\n                    ret_[awd] = False\n                    err = ('update_watch: cannot update %s WD=%d, %s' % \\\n                               (apath, wd_, self._inotify_wrapper.str_errno()))\n                    if quiet:\n                        log.error(err)\n                        continue\n                    raise WatchManagerError(err, ret_)\n\n                assert(awd == wd_)\n\n            if proc_fun or auto_add:\n                watch_ = self._wmd[awd]\n\n            if proc_fun:\n                watch_.proc_fun = proc_fun\n\n            if auto_add:\n                watch_.auto_add = auto_add\n\n            ret_[awd] = True\n            log.debug('Updated watch - %s', self._wmd[awd])\n        return ret_", "response": "Update watch descriptors |wd|."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the watch descriptor associated to the given path.", "response": "def get_wd(self, path):\n        \"\"\"\n        Returns the watch descriptor associated to path. This method\n        presents a prohibitive cost, always prefer to keep the WD\n        returned by add_watch(). If the path is unknown it returns None.\n\n        @param path: Path.\n        @type path: str\n        @return: WD or None.\n        @rtype: int or None\n        \"\"\"\n        path = self.__format_path(path)\n        for iwd in self._wmd.items():\n            if iwd[1].path == path:\n                return iwd[0]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_path(self, wd):\n        watch_ = self._wmd.get(wd)\n        if watch_ is not None:\n            return watch_.path", "response": "Returns the path associated to a WD."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __walk_rec(self, top, rec):\n        if not rec or os.path.islink(top) or not os.path.isdir(top):\n            yield top\n        else:\n            for root, dirs, files in os.walk(top):\n                yield root", "response": "Yields each directory of top recursively."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rm_watch(self, wd, rec=False, quiet=True):\n        lwd = self.__format_param(wd)\n        if rec:\n            lwd = self.__get_sub_rec(lwd)\n\n        ret_ = {}  # return {wd: bool, ...}\n        for awd in lwd:\n            # remove watch\n            wd_ = self._inotify_wrapper.inotify_rm_watch(self._fd, awd)\n            if wd_ < 0:\n                ret_[awd] = False\n                err = ('rm_watch: cannot remove WD=%d, %s' % \\\n                           (awd, self._inotify_wrapper.str_errno()))\n                if quiet:\n                    log.error(err)\n                    continue\n                raise WatchManagerError(err, ret_)\n\n            # Remove watch from our dictionary\n            if awd in self._wmd:\n                del self._wmd[awd]\n            ret_[awd] = True\n            log.debug('Watch WD=%d (%s) removed', awd, self.get_path(awd))\n        return ret_", "response": "Removes a watch from the watch manager."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef watch_transient_file(self, filename, mask, proc_class):\n        dirname = os.path.dirname(filename)\n        if dirname == '':\n            return {}  # Maintains coherence with add_watch()\n        basename = os.path.basename(filename)\n        # Assuming we are watching at least for IN_CREATE and IN_DELETE\n        mask |= IN_CREATE | IN_DELETE\n\n        def cmp_name(event):\n            if getattr(event, 'name') is None:\n                return False\n            return basename == event.name\n        return self.add_watch(dirname, mask,\n                              proc_fun=proc_class(ChainIfTrue(func=cmp_name)),\n                              rec=False,\n                              auto_add=False, do_glob=False,\n                              exclude_filter=lambda path: False)", "response": "Watch a file for changes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget attribute s value.", "response": "def get_val(self):\n        \"\"\"\n        Gets attribute's value.\n\n        @return: stored value.\n        @rtype: int\n        @raise IOError: if corresponding file in /proc/sys cannot be read.\n        \"\"\"\n        file_obj = file(os.path.join(self._base, self._attr), 'r')\n        try:\n            val = int(file_obj.readline())\n        finally:\n            file_obj.close()\n        return val"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets new attribute s value.", "response": "def set_val(self, nval):\n        \"\"\"\n        Sets new attribute's value.\n\n        @param nval: replaces current value by nval.\n        @type nval: int\n        @raise IOError: if corresponding file in /proc/sys cannot be written.\n        \"\"\"\n        file_obj = file(os.path.join(self._base, self._attr), 'w')\n        try:\n            file_obj.write(str(nval) + '\\n')\n        finally:\n            file_obj.close()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_IN_CREATE(self, raw_event):\n        if raw_event.mask & IN_ISDIR:\n            watch_ = self._watch_manager.get_watch(raw_event.wd)\n            created_dir = os.path.join(watch_.path, raw_event.name)\n            if watch_.auto_add and not watch_.exclude_filter(created_dir):\n                addw = self._watch_manager.add_watch\n                # The newly monitored directory inherits attributes from its\n                # parent directory.\n                addw_ret = addw(created_dir, watch_.mask,\n                                proc_fun=watch_.proc_fun,\n                                rec=False, auto_add=watch_.auto_add,\n                                exclude_filter=watch_.exclude_filter)\n\n                # Trick to handle mkdir -p /d1/d2/t3 where d1 is watched and\n                # d2 and t3 (directory or file) are created.\n                # Since the directory d2 is new, then everything inside it must\n                # also be new.\n                created_dir_wd = addw_ret.get(created_dir)\n                if ((created_dir_wd is not None) and (created_dir_wd > 0) and\n                    os.path.isdir(created_dir)):\n                    try:\n                        for name in os.listdir(created_dir):\n                            inner = os.path.join(created_dir, name)\n                            if self._watch_manager.get_wd(inner) is not None:\n                                continue\n                            # Generate (simulate) creation events for sub-\n                            # directories and files.\n                            if os.path.isfile(inner):\n                                # symlinks are handled as files.\n                                flags = IN_CREATE\n                            elif os.path.isdir(inner):\n                                flags = IN_CREATE | IN_ISDIR\n                            else:\n                                # This path should not be taken.\n                                continue\n                            rawevent = _RawEvent(created_dir_wd, flags, 0, name)\n                            self._notifier.append_event(rawevent)\n                    except OSError, err:\n                        msg = \"process_IN_CREATE, invalid directory %s: %s\"\n                        log.debug(msg % (created_dir, str(err)))\n        return self.process_default(raw_event)", "response": "If the event affects a directory and the auto_add flag of the\n        targetted watch is set to True, a new watch is added on this\n        new directory, with the same attribute values than those of\n        this watch."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dump(self, filename):\n        flags = os.O_WRONLY|os.O_CREAT|os.O_NOFOLLOW|os.O_EXCL\n        fd = os.open(filename, flags, 0600)\n        os.write(fd, str(self))\n        os.close(fd)", "response": "Dumps the current object to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_events(self, timeout=None):\n        while True:\n            try:\n                # blocks up to 'timeout' milliseconds\n                if timeout is None:\n                    timeout = self._timeout\n                ret = self._pollobj.poll(timeout)\n            except select.error, err:\n                if err[0] == errno.EINTR:\n                    continue # interrupted, retry\n                else:\n                    raise\n            else:\n                break\n\n        if not ret or (self._pipe[0] == ret[0][0]):\n            return False\n        # only one fd is polled\n        return ret[0][1] & select.POLLIN", "response": "Check if new events are available to read."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread events from device and build _RawEvents and enqueue them.", "response": "def read_events(self):\n        \"\"\"\n        Read events from device, build _RawEvents, and enqueue them.\n        \"\"\"\n        buf_ = array.array('i', [0])\n        # get event queue size\n        if fcntl.ioctl(self._fd, termios.FIONREAD, buf_, 1) == -1:\n            return\n        queue_size = buf_[0]\n        if queue_size < self._threshold:\n            log.debug('(fd: %d) %d bytes available to read but threshold is '\n                      'fixed to %d bytes', self._fd, queue_size,\n                      self._threshold)\n            return\n\n        try:\n            # Read content from file\n            r = os.read(self._fd, queue_size)\n        except Exception, msg:\n            raise NotifierError(msg)\n        log.debug('Event queue size: %d', queue_size)\n        rsum = 0  # counter\n        while rsum < queue_size:\n            s_size = 16\n            # Retrieve wd, mask, cookie and fname_len\n            wd, mask, cookie, fname_len = struct.unpack('iIII',\n                                                        r[rsum:rsum+s_size])\n            # Retrieve name\n            fname, = struct.unpack('%ds' % fname_len,\n                                   r[rsum + s_size:rsum + s_size + fname_len])\n            rawevent = _RawEvent(wd, mask, cookie, fname)\n            if self._coalesce:\n                # Only enqueue new (unique) events.\n                raweventstr = str(rawevent)\n                if raweventstr not in self._eventset:\n                    self._eventset.add(raweventstr)\n                    self._eventq.append(rawevent)\n            else:\n                self._eventq.append(rawevent)\n            rsum += s_size + fname_len"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __format_path(self, path):\n        # Unicode strings are converted back to strings, because it seems\n        # that inotify_add_watch from ctypes does not work well when\n        # it receives an ctypes.create_unicode_buffer instance as argument.\n        # Therefore even wd are indexed with bytes string and not with\n        # unicode paths.\n        if isinstance(path, unicode):\n            path = path.encode(sys.getfilesystemencoding())\n        return os.path.normpath(path)", "response": "Format a path to its internal representation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_watch(self, path, mask, proc_fun=None, rec=False,\n                  auto_add=False, do_glob=False, quiet=True,\n                  exclude_filter=None):\n        \"\"\"\n        Add watch(s) on the provided |path|(s) with associated |mask| flag\n        value and optionally with a processing |proc_fun| function and\n        recursive flag |rec| set to True.\n        Ideally |path| components should not be unicode objects. Note that\n        although unicode paths are accepted there are converted to byte\n        strings before a watch is put on that path. The encoding used for\n        converting the unicode object is given by sys.getfilesystemencoding().\n        If |path| is already watched it is ignored, but if it is called with\n        option rec=True a watch is put on each one of its not-watched\n        subdirectory.\n\n        @param path: Path to watch, the path can either be a file or a\n                     directory. Also accepts a sequence (list) of paths.\n        @type path: string or list of strings\n        @param mask: Bitmask of events.\n        @type mask: int\n        @param proc_fun: Processing object.\n        @type proc_fun: function or ProcessEvent instance or instance of\n                        one of its subclasses or callable object.\n        @param rec: Recursively add watches from path on all its\n                    subdirectories, set to False by default (doesn't\n                    follows symlinks in any case).\n        @type rec: bool\n        @param auto_add: Automatically add watches on newly created\n                         directories in watched parent |path| directory.\n                         If |auto_add| is True, IN_CREATE is ored with |mask|\n                         when the watch is added.\n        @type auto_add: bool\n        @param do_glob: Do globbing on pathname (see standard globbing\n                        module for more informations).\n        @type do_glob: bool\n        @param quiet: if False raises a WatchManagerError exception on\n                      error. See example not_quiet.py.\n        @type quiet: bool\n        @param exclude_filter: predicate (boolean function), which returns\n                               True if the current path must be excluded\n                               from being watched. This argument has\n                               precedence over exclude_filter passed to\n                               the class' constructor.\n        @type exclude_filter: callable object\n        @return: dict of paths associated to watch descriptors. A wd value\n                 is positive if the watch was added sucessfully, otherwise\n                 the value is negative. If the path was invalid or was already\n                 watched it is not included into this returned dictionary.\n        @rtype: dict of {str: int}\n        \"\"\"\n        ret_ = {} # return {path: wd, ...}\n\n        if exclude_filter is None:\n            exclude_filter = self._exclude_filter\n\n        # normalize args as list elements\n        for npath in self.__format_param(path):\n            # unix pathname pattern expansion\n            for apath in self.__glob(npath, do_glob):\n                # recursively list subdirs according to rec param\n                for rpath in self.__walk_rec(apath, rec):\n                    if not exclude_filter(rpath):\n                        wd = ret_[rpath] = self.__add_watch(rpath, mask,\n                                                            proc_fun,\n                                                            auto_add,\n                                                            exclude_filter)\n                        if wd < 0:\n                            err = ('add_watch: cannot watch %s WD=%d, %s' % \\\n                                       (rpath, wd,\n                                        self._inotify_wrapper.str_errno()))\n                            if quiet:\n                                log.error(err)\n                            else:\n                                raise WatchManagerError(err, ret_)\n                    else:\n                        # Let's say -2 means 'explicitely excluded\n                        # from watching'.\n                        ret_[rpath] = -2\n        return ret_", "response": "Adds a watch to the directory tree with the given path and mask."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef on_loop(notifier, counter):\n    if counter.count > 4:\n        # Loops 5 times then exits.\n        sys.stdout.write(\"Exit\\n\")\n        notifier.stop()\n        sys.exit(0)\n    else:\n        sys.stdout.write(\"Loop %d\\n\" % counter.count)\n        counter.plusone()", "response": "This function is called after each event loop. It is called after each event loop."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nensure that the redbeat_conf attribute is set to an instance of the RedBeatConfig class.", "response": "def ensure_conf(app):\n    \"\"\"\n    Ensure for the given app the the redbeat_conf\n    attribute is set to an instance of the RedBeatConfig\n    class.\n    \"\"\"\n    name = 'redbeat_conf'\n    app = app_or_default(app)\n    try:\n        config = getattr(app, name)\n    except AttributeError:\n        config = RedBeatConfig(app)\n        setattr(app, name, config)\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a string representing time in seconds since epoch t.", "response": "def time2isoz(t=None):\n    \"\"\"Return a string representing time in seconds since epoch, t.\n\n    If the function is called without an argument, it will use the current\n    time.\n\n    The format of the returned string is like \"YYYY-MM-DD hh:mm:ssZ\",\n    representing Universal Time (UTC, aka GMT).  An example of this format is:\n\n    1994-11-24 08:49:37Z\n\n    \"\"\"\n    if t is None:\n        dt = datetime.datetime.utcnow()\n    else:\n        dt = datetime.datetime.utcfromtimestamp(t)\n    return \"%04d-%02d-%02d %02d:%02d:%02dZ\" % (\n        dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef time2netscape(t=None):\n    if t is None:\n        dt = datetime.datetime.utcnow()\n    else:\n        dt = datetime.datetime.utcfromtimestamp(t)\n    return \"%s %02d-%s-%04d %02d:%02d:%02d GMT\" % (\n        DAYS[dt.weekday()], dt.day, MONTHS[dt.month-1],\n        dt.year, dt.hour, dt.minute, dt.second)", "response": "Return a string representing time in seconds since epoch t."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the time in seconds since epoch of time represented by a string.", "response": "def http2time(text):\n    \"\"\"Returns time in seconds since epoch of time represented by a string.\n\n    Return value is an integer.\n\n    None is returned if the format of str is unrecognized, the time is outside\n    the representable range, or the timezone string is not recognized.  If the\n    string contains no timezone, UTC is assumed.\n\n    The timezone in the string may be numerical (like \"-0800\" or \"+0100\") or a\n    string timezone (like \"UTC\", \"GMT\", \"BST\" or \"EST\").  Currently, only the\n    timezone strings equivalent to UTC (zero offset) are known to the function.\n\n    The function loosely parses the following formats:\n\n    Wed, 09 Feb 1994 22:23:32 GMT       -- HTTP format\n    Tuesday, 08-Feb-94 14:15:29 GMT     -- old rfc850 HTTP format\n    Tuesday, 08-Feb-1994 14:15:29 GMT   -- broken rfc850 HTTP format\n    09 Feb 1994 22:23:32 GMT            -- HTTP format (no weekday)\n    08-Feb-94 14:15:29 GMT              -- rfc850 format (no weekday)\n    08-Feb-1994 14:15:29 GMT            -- broken rfc850 format (no weekday)\n\n    The parser ignores leading and trailing whitespace.  The time may be\n    absent.\n\n    If the year is given with only 2 digits, the function will select the\n    century that makes the year closest to the current date.\n\n    \"\"\"\n    # fast exit for strictly conforming string\n    m = STRICT_DATE_RE.search(text)\n    if m:\n        g = m.groups()\n        mon = MONTHS_LOWER.index(g[1].lower()) + 1\n        tt = (int(g[2]), mon, int(g[0]),\n              int(g[3]), int(g[4]), float(g[5]))\n        return _timegm(tt)\n\n    # No, we need some messy parsing...\n\n    # clean up\n    text = text.lstrip()\n    text = WEEKDAY_RE.sub(\"\", text, 1)  # Useless weekday\n\n    # tz is time zone specifier string\n    day, mon, yr, hr, min, sec, tz = [None]*7\n\n    # loose regexp parse\n    m = LOOSE_HTTP_DATE_RE.search(text)\n    if m is not None:\n        day, mon, yr, hr, min, sec, tz = m.groups()\n    else:\n        return None  # bad format\n\n    return _str2time(day, mon, yr, hr, min, sec, tz)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef iso2time(text):\n    # clean up\n    text = text.lstrip()\n\n    # tz is time zone specifier string\n    day, mon, yr, hr, min, sec, tz = [None]*7\n\n    # loose regexp parse\n    m = ISO_DATE_RE.search(text)\n    if m is not None:\n        # XXX there's an extra bit of the timezone I'm ignoring here: is\n        #   this the right thing to do?\n        yr, mon, day, hr, min, sec, tz, _ = m.groups()\n    else:\n        return None  # bad format\n\n    return _str2time(day, mon, yr, hr, min, sec, tz)", "response": "Convert ISO 8601 time to time."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the unmatched part of a re. Match object.", "response": "def unmatched(match):\n    \"\"\"Return unmatched part of re.Match object.\"\"\"\n    start, end = match.span(0)\n    return match.string[:start]+match.string[end:]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef split_header_words(header_values):\n    assert not isinstance(header_values, str)\n    result = []\n    for text in header_values:\n        orig_text = text\n        pairs = []\n        while text:\n            m = HEADER_TOKEN_RE.search(text)\n            if m:\n                text = unmatched(m)\n                name = m.group(1)\n                m = HEADER_QUOTED_VALUE_RE.search(text)\n                if m:  # quoted value\n                    text = unmatched(m)\n                    value = m.group(1)\n                    value = HEADER_ESCAPE_RE.sub(r\"\\1\", value)\n                else:\n                    m = HEADER_VALUE_RE.search(text)\n                    if m:  # unquoted value\n                        text = unmatched(m)\n                        value = m.group(1)\n                        value = value.rstrip()\n                    else:\n                        # no value, a lone token\n                        value = None\n                pairs.append((name, value))\n            elif text.lstrip().startswith(\",\"):\n                # concatenated headers, as per RFC 2616 section 4.2\n                text = text.lstrip()[1:]\n                if pairs: result.append(pairs)\n                pairs = []\n            else:\n                # skip junk\n                non_junk, nr_junk_chars = re.subn(\"^[=\\s;]*\", \"\", text)\n                assert nr_junk_chars > 0, (\n                    \"split_header_words bug: '%s', '%s', %s\" %\n                    (orig_text, text, pairs))\n                text = non_junk\n        if pairs: result.append(pairs)\n    return result", "response": "r This function parses the header_values into a list of lists containing key value pairs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndo the inverse (almost) of the conversion done by split_header_words. Takes a list of lists of (key, value) pairs and produces a single header value. Attribute values are quoted if needed. >>> join_header_words([[(\"text/plain\", None), (\"charset\", \"iso-8859/1\")]]) 'text/plain; charset=\"iso-8859/1\"' >>> join_header_words([[(\"text/plain\", None)], [(\"charset\", \"iso-8859/1\")]]) 'text/plain, charset=\"iso-8859/1\"'", "response": "def join_header_words(lists):\n    \"\"\"Do the inverse (almost) of the conversion done by split_header_words.\n\n    Takes a list of lists of (key, value) pairs and produces a single header\n    value.  Attribute values are quoted if needed.\n\n    >>> join_header_words([[(\"text/plain\", None), (\"charset\", \"iso-8859/1\")]])\n    'text/plain; charset=\"iso-8859/1\"'\n    >>> join_header_words([[(\"text/plain\", None)], [(\"charset\", \"iso-8859/1\")]])\n    'text/plain, charset=\"iso-8859/1\"'\n\n    \"\"\"\n    headers = []\n    for pairs in lists:\n        attr = []\n        for k, v in pairs:\n            if v is not None:\n                if not re.search(r\"^\\w+$\", v):\n                    v = HEADER_JOIN_ESCAPE_RE.sub(r\"\\\\\\1\", v)  # escape \" and \\\n                    v = '\"%s\"' % v\n                k = \"%s=%s\" % (k, v)\n            attr.append(k)\n        if attr: headers.append(\"; \".join(attr))\n    return \", \".join(headers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_ns_headers(ns_headers):\n    known_attrs = (\"expires\", \"domain\", \"path\", \"secure\",\n                   # RFC 2109 attrs (may turn up in Netscape cookies, too)\n                   \"version\", \"port\", \"max-age\")\n\n    result = []\n    for ns_header in ns_headers:\n        pairs = []\n        version_set = False\n        for ii, param in enumerate(re.split(r\";\\s*\", ns_header)):\n            param = param.rstrip()\n            if param == \"\": continue\n            if \"=\" not in param:\n                k, v = param, None\n            else:\n                k, v = re.split(r\"\\s*=\\s*\", param, 1)\n                k = k.lstrip()\n            if ii != 0:\n                lc = k.lower()\n                if lc in known_attrs:\n                    k = lc\n                if k == \"version\":\n                    # This is an RFC 2109 cookie.\n                    v = strip_quotes(v)\n                    version_set = True\n                if k == \"expires\":\n                    # convert expires date to seconds since epoch\n                    v = http2time(strip_quotes(v))  # None if invalid\n            pairs.append((k, v))\n\n        if pairs:\n            if not version_set:\n                pairs.append((\"version\", \"0\"))\n            result.append(pairs)\n\n    return result", "response": "Parses the set - cookie header words into a list of lists."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_HDN(text):\n    # XXX\n    # This may well be wrong.  Which RFC is HDN defined in, if any (for\n    #  the purposes of RFC 2965)?\n    # For the current implementation, what about IPv6?  Remember to look\n    #  at other uses of IPV4_RE also, if change this.\n    if IPV4_RE.search(text):\n        return False\n    if text == \"\":\n        return False\n    if text[0] == \".\" or text[-1] == \".\":\n        return False\n    return True", "response": "Return True if text is a host domain name."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if domain A domain - matches domain B according to RFC 2965 section 1. 1.", "response": "def domain_match(A, B):\n    \"\"\"Return True if domain A domain-matches domain B, according to RFC 2965.\n\n    A and B may be host domain names or IP addresses.\n\n    RFC 2965, section 1:\n\n    Host names can be specified either as an IP address or a HDN string.\n    Sometimes we compare one host name with another.  (Such comparisons SHALL\n    be case-insensitive.)  Host A's name domain-matches host B's if\n\n         *  their host name strings string-compare equal; or\n\n         * A is a HDN string and has the form NB, where N is a non-empty\n            name string, B has the form .B', and B' is a HDN string.  (So,\n            x.y.com domain-matches .Y.com but not Y.com.)\n\n    Note that domain-match is not a commutative operation: a.b.c.com\n    domain-matches .c.com, but not the reverse.\n\n    \"\"\"\n    # Note that, if A or B are IP addresses, the only relevant part of the\n    # definition of the domain-match algorithm is the direct string-compare.\n    A = A.lower()\n    B = B.lower()\n    if A == B:\n        return True\n    if not is_HDN(A):\n        return False\n    i = A.rfind(B)\n    if i == -1 or i == 0:\n        # A does not have form NB, or N is the empty string\n        return False\n    if not B.startswith(\".\"):\n        return False\n    if not is_HDN(B[1:]):\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef user_domain_match(A, B):\n    A = A.lower()\n    B = B.lower()\n    if not (liberal_is_HDN(A) and liberal_is_HDN(B)):\n        if A == B:\n            # equal IP addresses\n            return True\n        return False\n    initial_dot = B.startswith(\".\")\n    if initial_dot and A.endswith(B):\n        return True\n    if not initial_dot and A == B:\n        return True\n    return False", "response": "Returns True if A and B are host domain names or IP addresses."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning request - host as defined by RFC 2965.", "response": "def request_host(request):\n    \"\"\"Return request-host, as defined by RFC 2965.\n\n    Variation from RFC: returned value is lowercased, for convenient\n    comparison.\n\n    \"\"\"\n    url = request.get_full_url()\n    host = urlparse(url)[1]\n    if host == \"\":\n        host = request.get_header(\"Host\", \"\")\n\n    # remove port, if present\n    host = cut_port_re.sub(\"\", host, 1)\n    return host.lower()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef eff_request_host(request):\n    erhn = req_host = request_host(request)\n    if req_host.find(\".\") == -1 and not IPV4_RE.search(req_host):\n        erhn = req_host + \".local\"\n    return req_host, erhn", "response": "Return a tuple containing the request - host and effective request - host name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the path component of the request - URI as defined by RFC 2965.", "response": "def request_path(request):\n    \"\"\"Path component of request-URI, as defined by RFC 2965.\"\"\"\n    url = request.get_full_url()\n    parts = urlsplit(url)\n    path = escape_path(parts.path)\n    if not path.startswith(\"/\"):\n        # fix bad RFC 2396 absoluteURI\n        path = \"/\" + path\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nescaping any invalid characters in HTTP URL and uppercase all escapes.", "response": "def escape_path(path):\n    \"\"\"Escape any invalid characters in HTTP URL, and uppercase all escapes.\"\"\"\n    # There's no knowing what character encoding was used to create URLs\n    # containing %-escapes, but since we have to pick one to escape invalid\n    # path characters, we pick UTF-8, as recommended in the HTML 4.0\n    # specification:\n    # http://www.w3.org/TR/REC-html40/appendix/notes.html#h-B.2.1\n    # And here, kind of: draft-fielding-uri-rfc2396bis-03\n    # (And in draft IRI specification: draft-duerst-iri-05)\n    # (And here, for new URI schemes: RFC 2718)\n    path = quote(path, HTTP_PATH_SAFE)\n    path = ESCAPED_CHAR_RE.sub(uppercase_escaped_char, path)\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reach(h):\n    i = h.find(\".\")\n    if i >= 0:\n        #a = h[:i]  # this line is only here to show what a is\n        b = h[i+1:]\n        i = b.find(\".\")\n        if is_HDN(h) and (i >= 0 or b == \"local\"):\n            return \".\"+b\n    return h", "response": "Return reach of a host name h."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef deepvalues(mapping):\n    values = vals_sorted_by_key(mapping)\n    for obj in values:\n        mapping = False\n        try:\n            obj.items\n        except AttributeError:\n            pass\n        else:\n            mapping = True\n            for subobj in deepvalues(obj):\n                yield subobj\n        if not mapping:\n            yield obj", "response": "Iterates over nested mapping depth - first in sorted order by key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning string representation of Cookie in an LWP cookie file format.", "response": "def lwp_cookie_str(cookie):\n    \"\"\"Return string representation of Cookie in an the LWP cookie file format.\n\n    Actually, the format is extended a bit -- see module docstring.\n\n    \"\"\"\n    h = [(cookie.name, cookie.value),\n         (\"path\", cookie.path),\n         (\"domain\", cookie.domain)]\n    if cookie.port is not None: h.append((\"port\", cookie.port))\n    if cookie.path_specified: h.append((\"path_spec\", None))\n    if cookie.port_specified: h.append((\"port_spec\", None))\n    if cookie.domain_initial_dot: h.append((\"domain_dot\", None))\n    if cookie.secure: h.append((\"secure\", None))\n    if cookie.expires: h.append((\"expires\",\n                               time2isoz(float(cookie.expires))))\n    if cookie.discard: h.append((\"discard\", None))\n    if cookie.comment: h.append((\"comment\", cookie.comment))\n    if cookie.comment_url: h.append((\"commenturl\", cookie.comment_url))\n\n    keys = sorted(cookie._rest.keys())\n    for k in keys:\n        h.append((k, str(cookie._rest[k])))\n\n    h.append((\"version\", str(cookie.version)))\n\n    return join_header_words([h])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the sequence of allowed domains or None.", "response": "def set_allowed_domains(self, allowed_domains):\n        \"\"\"Set the sequence of allowed domains, or None.\"\"\"\n        if allowed_domains is not None:\n            allowed_domains = tuple(allowed_domains)\n        self._allowed_domains = allowed_domains"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef return_ok(self, cookie, request):\n        # Path has already been checked by .path_return_ok(), and domain\n        # blocking done by .domain_return_ok().\n        _debug(\" - checking cookie %s=%s\", cookie.name, cookie.value)\n\n        for n in \"version\", \"verifiability\", \"secure\", \"expires\", \"port\", \"domain\":\n            fn_name = \"return_ok_\"+n\n            fn = getattr(self, fn_name)\n            if not fn(cookie, request):\n                return False\n        return True", "response": "Check if a cookie is OK."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _cookies_for_request(self, request):\n        cookies = []\n        for domain in self._cookies.keys():\n            cookies.extend(self._cookies_for_domain(domain, request))\n        return cookies", "response": "Return a list of cookies to be returned to server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _cookie_attrs(self, cookies):\n        # add cookies in order of most specific (ie. longest) path first\n        cookies.sort(key=lambda a: len(a.path), reverse=True)\n\n        version_set = False\n\n        attrs = []\n        for cookie in cookies:\n            # set version of Cookie header\n            # XXX\n            # What should it be if multiple matching Set-Cookie headers have\n            #  different versions themselves?\n            # Answer: there is no answer; was supposed to be settled by\n            #  RFC 2965 errata, but that may never appear...\n            version = cookie.version\n            if not version_set:\n                version_set = True\n                if version > 0:\n                    attrs.append(\"$Version=%s\" % version)\n\n            # quote cookie value if necessary\n            # (not for Netscape protocol, which already has any quotes\n            #  intact, due to the poorly-specified Netscape Cookie: syntax)\n            if ((cookie.value is not None) and\n                self.non_word_re.search(cookie.value) and version > 0):\n                value = self.quote_re.sub(r\"\\\\\\1\", cookie.value)\n            else:\n                value = cookie.value\n\n            # add cookie-attributes to be returned in Cookie header\n            if cookie.value is None:\n                attrs.append(cookie.name)\n            else:\n                attrs.append(\"%s=%s\" % (cookie.name, value))\n            if version > 0:\n                if cookie.path_specified:\n                    attrs.append('$Path=\"%s\"' % cookie.path)\n                if cookie.domain.startswith(\".\"):\n                    domain = cookie.domain\n                    if (not cookie.domain_initial_dot and\n                        domain.startswith(\".\")):\n                        domain = domain[1:]\n                    attrs.append('$Domain=\"%s\"' % domain)\n                if cookie.port is not None:\n                    p = \"$Port\"\n                    if cookie.port_specified:\n                        p = p + ('=\"%s\"' % cookie.port)\n                    attrs.append(p)\n\n        return attrs", "response": "Return a list of cookie - attributes to be returned to server."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_cookie_header(self, request):\n        _debug(\"add_cookie_header\")\n        self._cookies_lock.acquire()\n        try:\n\n            self._policy._now = self._now = int(time.time())\n\n            cookies = self._cookies_for_request(request)\n\n            attrs = self._cookie_attrs(cookies)\n            if attrs:\n                if not request.has_header(\"Cookie\"):\n                    request.add_unredirected_header(\n                        \"Cookie\", \"; \".join(attrs))\n\n            # if necessary, advertise that we know RFC 2965\n            if (self._policy.rfc2965 and not self._policy.hide_cookie2 and\n                not request.has_header(\"Cookie2\")):\n                for cookie in cookies:\n                    if cookie.version != 1:\n                        request.add_unredirected_header(\"Cookie2\", '$Version=\"1\"')\n                        break\n\n        finally:\n            self._cookies_lock.release()\n\n        self.clear_expired_cookies()", "response": "Add correct Cookie header to request."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _normalized_cookie_tuples(self, attrs_set):\n        cookie_tuples = []\n\n        boolean_attrs = \"discard\", \"secure\"\n        value_attrs = (\"version\",\n                       \"expires\", \"max-age\",\n                       \"domain\", \"path\", \"port\",\n                       \"comment\", \"commenturl\")\n\n        for cookie_attrs in attrs_set:\n            name, value = cookie_attrs[0]\n\n            # Build dictionary of standard cookie-attributes (standard) and\n            # dictionary of other cookie-attributes (rest).\n\n            # Note: expiry time is normalised to seconds since epoch.  V0\n            # cookies should have the Expires cookie-attribute, and V1 cookies\n            # should have Max-Age, but since V1 includes RFC 2109 cookies (and\n            # since V0 cookies may be a mish-mash of Netscape and RFC 2109), we\n            # accept either (but prefer Max-Age).\n            max_age_set = False\n\n            bad_cookie = False\n\n            standard = {}\n            rest = {}\n            for k, v in cookie_attrs[1:]:\n                lc = k.lower()\n                # don't lose case distinction for unknown fields\n                if lc in value_attrs or lc in boolean_attrs:\n                    k = lc\n                if k in boolean_attrs and v is None:\n                    # boolean cookie-attribute is present, but has no value\n                    # (like \"discard\", rather than \"port=80\")\n                    v = True\n                if k in standard:\n                    # only first value is significant\n                    continue\n                if k == \"domain\":\n                    if v is None:\n                        _debug(\"   missing value for domain attribute\")\n                        bad_cookie = True\n                        break\n                    # RFC 2965 section 3.3.3\n                    v = v.lower()\n                if k == \"expires\":\n                    if max_age_set:\n                        # Prefer max-age to expires (like Mozilla)\n                        continue\n                    if v is None:\n                        _debug(\"   missing or invalid value for expires \"\n                              \"attribute: treating as session cookie\")\n                        continue\n                if k == \"max-age\":\n                    max_age_set = True\n                    try:\n                        v = int(v)\n                    except ValueError:\n                        _debug(\"   missing or invalid (non-numeric) value for \"\n                              \"max-age attribute\")\n                        bad_cookie = True\n                        break\n                    # convert RFC 2965 Max-Age to seconds since epoch\n                    # XXX Strictly you're supposed to follow RFC 2616\n                    #   age-calculation rules.  Remember that zero Max-Age is a\n                    #   is a request to discard (old and new) cookie, though.\n                    k = \"expires\"\n                    v = self._now + v\n                if (k in value_attrs) or (k in boolean_attrs):\n                    if (v is None and\n                        k not in (\"port\", \"comment\", \"commenturl\")):\n                        _debug(\"   missing value for %s attribute\" % k)\n                        bad_cookie = True\n                        break\n                    standard[k] = v\n                else:\n                    rest[k] = v\n\n            if bad_cookie:\n                continue\n\n            cookie_tuples.append((name, value, standard, rest))\n\n        return cookie_tuples", "response": "Return list of tuples containing normalised cookie information."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns sequence of Cookie objects extracted from response object.", "response": "def make_cookies(self, response, request):\n        \"\"\"Return sequence of Cookie objects extracted from response object.\"\"\"\n        # get cookie-attributes for RFC 2965 and Netscape protocols\n        headers = response.info()\n        rfc2965_hdrs = headers.get_all(\"Set-Cookie2\", [])\n        ns_hdrs = headers.get_all(\"Set-Cookie\", [])\n\n        rfc2965 = self._policy.rfc2965\n        netscape = self._policy.netscape\n\n        if ((not rfc2965_hdrs and not ns_hdrs) or\n            (not ns_hdrs and not rfc2965) or\n            (not rfc2965_hdrs and not netscape) or\n            (not netscape and not rfc2965)):\n            return []  # no relevant cookie headers: quick exit\n\n        try:\n            cookies = self._cookies_from_attrs_set(\n                split_header_words(rfc2965_hdrs), request)\n        except Exception:\n            _warn_unhandled_exception()\n            cookies = []\n\n        if ns_hdrs and netscape:\n            try:\n                # RFC 2109 and Netscape cookies\n                ns_cookies = self._cookies_from_attrs_set(\n                    parse_ns_headers(ns_hdrs), request)\n            except Exception:\n                _warn_unhandled_exception()\n                ns_cookies = []\n            self._process_rfc2109_cookies(ns_cookies)\n\n            # Look for Netscape cookies (from Set-Cookie headers) that match\n            # corresponding RFC 2965 cookies (from Set-Cookie2 headers).\n            # For each match, keep the RFC 2965 cookie and ignore the Netscape\n            # cookie (RFC 2965 section 9.1).  Actually, RFC 2109 cookies are\n            # bundled in with the Netscape cookies for this purpose, which is\n            # reasonable behaviour.\n            if rfc2965:\n                lookup = {}\n                for cookie in cookies:\n                    lookup[(cookie.domain, cookie.path, cookie.name)] = None\n\n                def no_matching_rfc2965(ns_cookie, lookup=lookup):\n                    key = ns_cookie.domain, ns_cookie.path, ns_cookie.name\n                    return key not in lookup\n                ns_cookies = filter(no_matching_rfc2965, ns_cookies)\n\n            if ns_cookies:\n                cookies.extend(ns_cookies)\n\n        return cookies"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset a cookie if policy says it s OK to do so.", "response": "def set_cookie_if_ok(self, cookie, request):\n        \"\"\"Set a cookie if policy says it's OK to do so.\"\"\"\n        self._cookies_lock.acquire()\n        try:\n            self._policy._now = self._now = int(time.time())\n\n            if self._policy.set_ok(cookie, request):\n                self.set_cookie(cookie)\n\n\n        finally:\n            self._cookies_lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_cookie(self, cookie):\n        c = self._cookies\n        self._cookies_lock.acquire()\n        try:\n            if cookie.domain not in c: c[cookie.domain] = {}\n            c2 = c[cookie.domain]\n            if cookie.path not in c2: c2[cookie.path] = {}\n            c3 = c2[cookie.path]\n            c3[cookie.name] = cookie\n        finally:\n            self._cookies_lock.release()", "response": "Set a cookie without checking whether or not it should be set."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract cookies from response where allowable given the request.", "response": "def extract_cookies(self, response, request):\n        \"\"\"Extract cookies from response, where allowable given the request.\"\"\"\n        _debug(\"extract_cookies: %s\", response.info())\n        self._cookies_lock.acquire()\n        try:\n            self._policy._now = self._now = int(time.time())\n\n            for cookie in self.make_cookies(response, request):\n                if self._policy.set_ok(cookie, request):\n                    _debug(\" setting cookie: %s\", cookie)\n                    self.set_cookie(cookie)\n        finally:\n            self._cookies_lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clear(self, domain=None, path=None, name=None):\n        if name is not None:\n            if (domain is None) or (path is None):\n                raise ValueError(\n                    \"domain and path must be given to remove a cookie by name\")\n            del self._cookies[domain][path][name]\n        elif path is not None:\n            if domain is None:\n                raise ValueError(\n                    \"domain must be given to remove cookies by path\")\n            del self._cookies[domain][path]\n        elif domain is not None:\n            del self._cookies[domain]\n        else:\n            self._cookies = {}", "response": "Removes all cookies from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndiscarding all session cookies.", "response": "def clear_session_cookies(self):\n        \"\"\"Discard all session cookies.\n\n        Note that the .save() method won't save session cookies anyway, unless\n        you ask otherwise by passing a true ignore_discard argument.\n\n        \"\"\"\n        self._cookies_lock.acquire()\n        try:\n            for cookie in self:\n                if cookie.discard:\n                    self.clear(cookie.domain, cookie.path, cookie.name)\n        finally:\n            self._cookies_lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clear_expired_cookies(self):\n        self._cookies_lock.acquire()\n        try:\n            now = time.time()\n            for cookie in self:\n                if cookie.is_expired(now):\n                    self.clear(cookie.domain, cookie.path, cookie.name)\n        finally:\n            self._cookies_lock.release()", "response": "Discard all expired cookies."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load(self, filename=None, ignore_discard=False, ignore_expires=False):\n        if filename is None:\n            if self.filename is not None: filename = self.filename\n            else: raise ValueError(MISSING_FILENAME_TEXT)\n\n        f = open(filename)\n        try:\n            self._really_load(f, filename, ignore_discard, ignore_expires)\n        finally:\n            f.close()", "response": "Load cookies from a file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef revert(self, filename=None,\n               ignore_discard=False, ignore_expires=False):\n        \"\"\"Clear all cookies and reload cookies from a saved file.\n\n        Raises LoadError (or IOError) if reversion is not successful; the\n        object's state will not be altered if this happens.\n\n        \"\"\"\n        if filename is None:\n            if self.filename is not None: filename = self.filename\n            else: raise ValueError(MISSING_FILENAME_TEXT)\n\n        self._cookies_lock.acquire()\n        try:\n\n            old_state = copy.deepcopy(self._cookies)\n            self._cookies = {}\n            try:\n                self.load(filename, ignore_discard, ignore_expires)\n            except (LoadError, IOError):\n                self._cookies = old_state\n                raise\n\n        finally:\n            self._cookies_lock.release()", "response": "Revert all cookies and reload cookies from a saved file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef as_lwp_str(self, ignore_discard=True, ignore_expires=True):\n        now = time.time()\n        r = []\n        for cookie in self:\n            if not ignore_discard and cookie.discard:\n                continue\n            if not ignore_expires and cookie.is_expired(now):\n                continue\n            r.append(\"Set-Cookie3: %s\" % lwp_cookie_str(cookie))\n        return \"\\n\".join(r+[\"\"])", "response": "Return cookies as a string of \"\\\\n\" - separated Set - Cookie3 headers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fold(self, **_3to2kwargs):\n        policy = _3to2kwargs['policy']; del _3to2kwargs['policy']\n        \"\"\"Fold header according to policy.\n\n        The parsed representation of the header is folded according to\n        RFC5322 rules, as modified by the policy.  If the parse tree\n        contains surrogateescaped bytes, the bytes are CTE encoded using\n        the charset 'unknown-8bit\".\n\n        Any non-ASCII characters in the parse tree are CTE encoded using\n        charset utf-8. XXX: make this a policy setting.\n\n        The returned value is an ASCII-only string possibly containing linesep\n        characters, and ending with a linesep character.  The string includes\n        the header name and the ': ' separator.\n\n        \"\"\"\n        # At some point we need to only put fws here if it was in the source.\n        header = parser.Header([\n            parser.HeaderLabel([\n                parser.ValueTerminal(self.name, 'header-name'),\n                parser.ValueTerminal(':', 'header-sep')]),\n            parser.CFWSList([parser.WhiteSpaceTerminal(' ', 'fws')]),\n                             self._parse_tree])\n        return header.fold(policy=policy)", "response": "Folds the parse tree into a string according to the RFC 5322 rules."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef no(mytype, argnums=(1,)):\n    if isinstance(argnums, Integral):\n        argnums = (argnums,)\n    disallowed_types = [mytype] * len(argnums)\n    return disallow_types(argnums, disallowed_types)", "response": "Disallows only one type of a given object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef issubset(list1, list2):\n    n = len(list1)\n    for startpos in range(len(list2) - n + 1):\n        if list2[startpos:startpos+n] == list1:\n            return True\n    return False", "response": "Tests if two lists are in a subset of the ones in the list1."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a URL path remove extra slashes and. and return a colllapsed path.", "response": "def _url_collapse_path(path):\n    \"\"\"\n    Given a URL path, remove extra '/'s and '.' path elements and collapse\n    any '..' references and returns a colllapsed path.\n\n    Implements something akin to RFC-2396 5.2 step 6 to parse relative paths.\n    The utility of this function is limited to is_cgi method and helps\n    preventing some security attacks.\n\n    Returns: A tuple of (head, tail) where tail is everything after the final /\n    and head is everything before it.  Head will always start with a '/' and,\n    if it contains anything else, never have a trailing '/'.\n\n    Raises: IndexError if too many '..' occur within the path.\n\n    \"\"\"\n    # Similar to os.path.split(os.path.normpath(path)) but specific to URL\n    # path semantics rather than local operating system semantics.\n    path_parts = path.split('/')\n    head_parts = []\n    for part in path_parts[:-1]:\n        if part == '..':\n            head_parts.pop() # IndexError if more '..' than prior parts\n        elif part and part != '.':\n            head_parts.append( part )\n    if path_parts:\n        tail_part = path_parts.pop()\n        if tail_part:\n            if tail_part == '..':\n                head_parts.pop()\n                tail_part = ''\n            elif tail_part == '.':\n                tail_part = ''\n    else:\n        tail_part = ''\n\n    splitpath = ('/' + '/'.join(head_parts), tail_part)\n    collapsed_path = \"/\".join(splitpath)\n\n    return collapsed_path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef nobody_uid():\n    global nobody\n    if nobody:\n        return nobody\n    try:\n        import pwd\n    except ImportError:\n        return -1\n    try:\n        nobody = pwd.getpwnam('nobody')[2]\n    except KeyError:\n        nobody = 1 + max(x[2] for x in pwd.getpwall())\n    return nobody", "response": "Internal routine to get nobody s uid"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\noverriding server_bind to store the server name and port.", "response": "def server_bind(self):\n        \"\"\"Override server_bind to store the server name.\"\"\"\n        socketserver.TCPServer.server_bind(self)\n        host, port = self.socket.getsockname()[:2]\n        self.server_name = socket.getfqdn(host)\n        self.server_port = port"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_request(self):\n        self.command = None  # set in case of error on the first line\n        self.request_version = version = self.default_request_version\n        self.close_connection = 1\n        requestline = str(self.raw_requestline, 'iso-8859-1')\n        requestline = requestline.rstrip('\\r\\n')\n        self.requestline = requestline\n        words = requestline.split()\n        if len(words) == 3:\n            command, path, version = words\n            if version[:5] != 'HTTP/':\n                self.send_error(400, \"Bad request version (%r)\" % version)\n                return False\n            try:\n                base_version_number = version.split('/', 1)[1]\n                version_number = base_version_number.split(\".\")\n                # RFC 2145 section 3.1 says there can be only one \".\" and\n                #   - major and minor numbers MUST be treated as\n                #      separate integers;\n                #   - HTTP/2.4 is a lower version than HTTP/2.13, which in\n                #      turn is lower than HTTP/12.3;\n                #   - Leading zeros MUST be ignored by recipients.\n                if len(version_number) != 2:\n                    raise ValueError\n                version_number = int(version_number[0]), int(version_number[1])\n            except (ValueError, IndexError):\n                self.send_error(400, \"Bad request version (%r)\" % version)\n                return False\n            if version_number >= (1, 1) and self.protocol_version >= \"HTTP/1.1\":\n                self.close_connection = 0\n            if version_number >= (2, 0):\n                self.send_error(505,\n                          \"Invalid HTTP Version (%s)\" % base_version_number)\n                return False\n        elif len(words) == 2:\n            command, path = words\n            self.close_connection = 1\n            if command != 'GET':\n                self.send_error(400,\n                                \"Bad HTTP/0.9 request type (%r)\" % command)\n                return False\n        elif not words:\n            return False\n        else:\n            self.send_error(400, \"Bad request syntax (%r)\" % requestline)\n            return False\n        self.command, self.path, self.request_version = command, path, version\n\n        # Examine the headers and look for a Connection directive.\n        try:\n            self.headers = http_client.parse_headers(self.rfile,\n                                                     _class=self.MessageClass)\n        except http_client.LineTooLong:\n            self.send_error(400, \"Line too long\")\n            return False\n\n        conntype = self.headers.get('Connection', \"\")\n        if conntype.lower() == 'close':\n            self.close_connection = 1\n        elif (conntype.lower() == 'keep-alive' and\n              self.protocol_version >= \"HTTP/1.1\"):\n            self.close_connection = 0\n        # Examine the headers and look for an Expect directive\n        expect = self.headers.get('Expect', \"\")\n        if (expect.lower() == \"100-continue\" and\n                self.protocol_version >= \"HTTP/1.1\" and\n                self.request_version >= \"HTTP/1.1\"):\n            if not self.handle_expect_100():\n                return False\n        return True", "response": "Parse a request and return a tuple of command path and version."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle(self):\n        self.close_connection = 1\n\n        self.handle_one_request()\n        while not self.close_connection:\n            self.handle_one_request()", "response": "Handle multiple requests if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send_error(self, code, message=None):\n\n        try:\n            shortmsg, longmsg = self.responses[code]\n        except KeyError:\n            shortmsg, longmsg = '???', '???'\n        if message is None:\n            message = shortmsg\n        explain = longmsg\n        self.log_error(\"code %d, message %s\", code, message)\n        # using _quote_html to prevent Cross Site Scripting attacks (see bug #1100201)\n        content = (self.error_message_format %\n                   {'code': code, 'message': _quote_html(message), 'explain': explain})\n        self.send_response(code, message)\n        self.send_header(\"Content-Type\", self.error_content_type)\n        self.send_header('Connection', 'close')\n        self.end_headers()\n        if self.command != 'HEAD' and code >= 200 and code not in (204, 304):\n            self.wfile.write(content.encode('UTF-8', 'replace'))", "response": "Send an error response and log it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd the response header to the headers buffer and log the response code and message.", "response": "def send_response(self, code, message=None):\n        \"\"\"Add the response header to the headers buffer and log the\n        response code.\n\n        Also send two standard headers with the server software\n        version and the current date.\n\n        \"\"\"\n        self.log_request(code)\n        self.send_response_only(code, message)\n        self.send_header('Server', self.version_string())\n        self.send_header('Date', self.date_time_string())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send_response_only(self, code, message=None):\n        if message is None:\n            if code in self.responses:\n                message = self.responses[code][0]\n            else:\n                message = ''\n        if self.request_version != 'HTTP/0.9':\n            if not hasattr(self, '_headers_buffer'):\n                self._headers_buffer = []\n            self._headers_buffer.append((\"%s %d %s\\r\\n\" %\n                    (self.protocol_version, code, message)).encode(\n                        'latin-1', 'strict'))", "response": "Send the response header only."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_header(self, keyword, value):\n        if self.request_version != 'HTTP/0.9':\n            if not hasattr(self, '_headers_buffer'):\n                self._headers_buffer = []\n            self._headers_buffer.append(\n                (\"%s: %s\\r\\n\" % (keyword, value)).encode('latin-1', 'strict'))\n\n        if keyword.lower() == 'connection':\n            if value.lower() == 'close':\n                self.close_connection = 1\n            elif value.lower() == 'keep-alive':\n                self.close_connection = 0", "response": "Send a MIME header to the headers buffer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending the blank line ending the MIME headers.", "response": "def end_headers(self):\n        \"\"\"Send the blank line ending the MIME headers.\"\"\"\n        if self.request_version != 'HTTP/0.9':\n            self._headers_buffer.append(b\"\\r\\n\")\n            self.flush_headers()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef log_message(self, format, *args):\n\n        sys.stderr.write(\"%s - - [%s] %s\\n\" %\n                         (self.address_string(),\n                          self.log_date_time_string(),\n                          format%args))", "response": "Log an arbitrary message."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the current date and time formatted for a message header.", "response": "def date_time_string(self, timestamp=None):\n        \"\"\"Return the current date and time formatted for a message header.\"\"\"\n        if timestamp is None:\n            timestamp = time.time()\n        year, month, day, hh, mm, ss, wd, y, z = time.gmtime(timestamp)\n        s = \"%s, %02d %3s %4d %02d:%02d:%02d GMT\" % (\n                self.weekdayname[wd],\n                day, self.monthname[month], year,\n                hh, mm, ss)\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the current time formatted for logging.", "response": "def log_date_time_string(self):\n        \"\"\"Return the current time formatted for logging.\"\"\"\n        now = time.time()\n        year, month, day, hh, mm, ss, x, y, z = time.localtime(now)\n        s = \"%02d/%3s/%04d %02d:%02d:%02d\" % (\n                day, self.monthname[month], year, hh, mm, ss)\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nserves a GET request.", "response": "def do_GET(self):\n        \"\"\"Serve a GET request.\"\"\"\n        f = self.send_head()\n        if f:\n            self.copyfile(f, self.wfile)\n            f.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_head(self):\n        path = self.translate_path(self.path)\n        f = None\n        if os.path.isdir(path):\n            if not self.path.endswith('/'):\n                # redirect browser - doing basically what apache does\n                self.send_response(301)\n                self.send_header(\"Location\", self.path + \"/\")\n                self.end_headers()\n                return None\n            for index in \"index.html\", \"index.htm\":\n                index = os.path.join(path, index)\n                if os.path.exists(index):\n                    path = index\n                    break\n            else:\n                return self.list_directory(path)\n        ctype = self.guess_type(path)\n        try:\n            f = open(path, 'rb')\n        except IOError:\n            self.send_error(404, \"File not found\")\n            return None\n        self.send_response(200)\n        self.send_header(\"Content-type\", ctype)\n        fs = os.fstat(f.fileno())\n        self.send_header(\"Content-Length\", str(fs[6]))\n        self.send_header(\"Last-Modified\", self.date_time_string(fs.st_mtime))\n        self.end_headers()\n        return f", "response": "This function is used to send the response code and MIME headers to the client."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list_directory(self, path):\n        try:\n            list = os.listdir(path)\n        except os.error:\n            self.send_error(404, \"No permission to list directory\")\n            return None\n        list.sort(key=lambda a: a.lower())\n        r = []\n        displaypath = html.escape(urllib_parse.unquote(self.path))\n        enc = sys.getfilesystemencoding()\n        title = 'Directory listing for %s' % displaypath\n        r.append('<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01//EN\" '\n                 '\"http://www.w3.org/TR/html4/strict.dtd\">')\n        r.append('<html>\\n<head>')\n        r.append('<meta http-equiv=\"Content-Type\" '\n                 'content=\"text/html; charset=%s\">' % enc)\n        r.append('<title>%s</title>\\n</head>' % title)\n        r.append('<body>\\n<h1>%s</h1>' % title)\n        r.append('<hr>\\n<ul>')\n        for name in list:\n            fullname = os.path.join(path, name)\n            displayname = linkname = name\n            # Append / for directories or @ for symbolic links\n            if os.path.isdir(fullname):\n                displayname = name + \"/\"\n                linkname = name + \"/\"\n            if os.path.islink(fullname):\n                displayname = name + \"@\"\n                # Note: a link to a directory displays with @ and links with /\n            r.append('<li><a href=\"%s\">%s</a></li>'\n                    % (urllib_parse.quote(linkname), html.escape(displayname)))\n            # # Use this instead:\n            # r.append('<li><a href=\"%s\">%s</a></li>'\n            #         % (urllib.quote(linkname), cgi.escape(displayname)))\n        r.append('</ul>\\n<hr>\\n</body>\\n</html>\\n')\n        encoded = '\\n'.join(r).encode(enc)\n        f = io.BytesIO()\n        f.write(encoded)\n        f.seek(0)\n        self.send_response(200)\n        self.send_header(\"Content-type\", \"text/html; charset=%s\" % enc)\n        self.send_header(\"Content-Length\", str(len(encoded)))\n        self.end_headers()\n        return f", "response": "Helper to produce a directory listing."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntranslates a / - separated PATH to the local filename syntax.", "response": "def translate_path(self, path):\n        \"\"\"Translate a /-separated PATH to the local filename syntax.\n\n        Components that mean special things to the local file system\n        (e.g. drive or directory names) are ignored.  (XXX They should\n        probably be diagnosed.)\n\n        \"\"\"\n        # abandon query parameters\n        path = path.split('?',1)[0]\n        path = path.split('#',1)[0]\n        path = posixpath.normpath(urllib_parse.unquote(path))\n        words = path.split('/')\n        words = filter(None, words)\n        path = os.getcwd()\n        for word in words:\n            drive, word = os.path.splitdrive(word)\n            head, word = os.path.split(word)\n            if word in (os.curdir, os.pardir): continue\n            path = os.path.join(path, word)\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nguesses the type of a file.", "response": "def guess_type(self, path):\n        \"\"\"Guess the type of a file.\n\n        Argument is a PATH (a filename).\n\n        Return value is a string of the form type/subtype,\n        usable for a MIME Content-type header.\n\n        The default implementation looks the file's extension\n        up in the table self.extensions_map, using application/octet-stream\n        as a default; however it would be permissible (if\n        slow) to look inside the data to make a better guess.\n\n        \"\"\"\n\n        base, ext = posixpath.splitext(path)\n        if ext in self.extensions_map:\n            return self.extensions_map[ext]\n        ext = ext.lower()\n        if ext in self.extensions_map:\n            return self.extensions_map[ext]\n        else:\n            return self.extensions_map['']"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntests whether self. path corresponds to a CGI script.", "response": "def is_cgi(self):\n        \"\"\"Test whether self.path corresponds to a CGI script.\n\n        Returns True and updates the cgi_info attribute to the tuple\n        (dir, rest) if self.path requires running a CGI script.\n        Returns False otherwise.\n\n        If any exception is raised, the caller should assume that\n        self.path was rejected as invalid and act accordingly.\n\n        The default implementation tests whether the normalized url\n        path begins with one of the strings in self.cgi_directories\n        (and the next character is a '/' or the end of the string).\n\n        \"\"\"\n        collapsed_path = _url_collapse_path(self.path)\n        dir_sep = collapsed_path.find('/', 1)\n        head, tail = collapsed_path[:dir_sep], collapsed_path[dir_sep+1:]\n        if head in self.cgi_directories:\n            self.cgi_info = head, tail\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntest whether argument path is a Python script.", "response": "def is_python(self, path):\n        \"\"\"Test whether argument path is a Python script.\"\"\"\n        head, tail = os.path.splitext(path)\n        return tail.lower() in (\".py\", \".pyw\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute a CGI script.", "response": "def run_cgi(self):\n        \"\"\"Execute a CGI script.\"\"\"\n        path = self.path\n        dir, rest = self.cgi_info\n\n        i = path.find('/', len(dir) + 1)\n        while i >= 0:\n            nextdir = path[:i]\n            nextrest = path[i+1:]\n\n            scriptdir = self.translate_path(nextdir)\n            if os.path.isdir(scriptdir):\n                dir, rest = nextdir, nextrest\n                i = path.find('/', len(dir) + 1)\n            else:\n                break\n\n        # find an explicit query string, if present.\n        i = rest.rfind('?')\n        if i >= 0:\n            rest, query = rest[:i], rest[i+1:]\n        else:\n            query = ''\n\n        # dissect the part after the directory name into a script name &\n        # a possible additional path, to be stored in PATH_INFO.\n        i = rest.find('/')\n        if i >= 0:\n            script, rest = rest[:i], rest[i:]\n        else:\n            script, rest = rest, ''\n\n        scriptname = dir + '/' + script\n        scriptfile = self.translate_path(scriptname)\n        if not os.path.exists(scriptfile):\n            self.send_error(404, \"No such CGI script (%r)\" % scriptname)\n            return\n        if not os.path.isfile(scriptfile):\n            self.send_error(403, \"CGI script is not a plain file (%r)\" %\n                            scriptname)\n            return\n        ispy = self.is_python(scriptname)\n        if self.have_fork or not ispy:\n            if not self.is_executable(scriptfile):\n                self.send_error(403, \"CGI script is not executable (%r)\" %\n                                scriptname)\n                return\n\n        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html\n        # XXX Much of the following could be prepared ahead of time!\n        env = copy.deepcopy(os.environ)\n        env['SERVER_SOFTWARE'] = self.version_string()\n        env['SERVER_NAME'] = self.server.server_name\n        env['GATEWAY_INTERFACE'] = 'CGI/1.1'\n        env['SERVER_PROTOCOL'] = self.protocol_version\n        env['SERVER_PORT'] = str(self.server.server_port)\n        env['REQUEST_METHOD'] = self.command\n        uqrest = urllib_parse.unquote(rest)\n        env['PATH_INFO'] = uqrest\n        env['PATH_TRANSLATED'] = self.translate_path(uqrest)\n        env['SCRIPT_NAME'] = scriptname\n        if query:\n            env['QUERY_STRING'] = query\n        env['REMOTE_ADDR'] = self.client_address[0]\n        authorization = self.headers.get(\"authorization\")\n        if authorization:\n            authorization = authorization.split()\n            if len(authorization) == 2:\n                import base64, binascii\n                env['AUTH_TYPE'] = authorization[0]\n                if authorization[0].lower() == \"basic\":\n                    try:\n                        authorization = authorization[1].encode('ascii')\n                        if utils.PY3:\n                            # In Py3.3, was:\n                            authorization = base64.decodebytes(authorization).\\\n                                            decode('ascii')\n                        else:\n                            # Backport to Py2.7:\n                            authorization = base64.decodestring(authorization).\\\n                                            decode('ascii')\n                    except (binascii.Error, UnicodeError):\n                        pass\n                    else:\n                        authorization = authorization.split(':')\n                        if len(authorization) == 2:\n                            env['REMOTE_USER'] = authorization[0]\n        # XXX REMOTE_IDENT\n        if self.headers.get('content-type') is None:\n            env['CONTENT_TYPE'] = self.headers.get_content_type()\n        else:\n            env['CONTENT_TYPE'] = self.headers['content-type']\n        length = self.headers.get('content-length')\n        if length:\n            env['CONTENT_LENGTH'] = length\n        referer = self.headers.get('referer')\n        if referer:\n            env['HTTP_REFERER'] = referer\n        accept = []\n        for line in self.headers.getallmatchingheaders('accept'):\n            if line[:1] in \"\\t\\n\\r \":\n                accept.append(line.strip())\n            else:\n                accept = accept + line[7:].split(',')\n        env['HTTP_ACCEPT'] = ','.join(accept)\n        ua = self.headers.get('user-agent')\n        if ua:\n            env['HTTP_USER_AGENT'] = ua\n        co = filter(None, self.headers.get_all('cookie', []))\n        cookie_str = ', '.join(co)\n        if cookie_str:\n            env['HTTP_COOKIE'] = cookie_str\n        # XXX Other HTTP_* headers\n        # Since we're setting the env in the parent, provide empty\n        # values to override previously set values\n        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',\n                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):\n            env.setdefault(k, \"\")\n\n        self.send_response(200, \"Script output follows\")\n        self.flush_headers()\n\n        decoded_query = query.replace('+', ' ')\n\n        if self.have_fork:\n            # Unix -- fork as we should\n            args = [script]\n            if '=' not in decoded_query:\n                args.append(decoded_query)\n            nobody = nobody_uid()\n            self.wfile.flush() # Always flush before forking\n            pid = os.fork()\n            if pid != 0:\n                # Parent\n                pid, sts = os.waitpid(pid, 0)\n                # throw away additional data [see bug #427345]\n                while select.select([self.rfile], [], [], 0)[0]:\n                    if not self.rfile.read(1):\n                        break\n                if sts:\n                    self.log_error(\"CGI script exit status %#x\", sts)\n                return\n            # Child\n            try:\n                try:\n                    os.setuid(nobody)\n                except os.error:\n                    pass\n                os.dup2(self.rfile.fileno(), 0)\n                os.dup2(self.wfile.fileno(), 1)\n                os.execve(scriptfile, args, env)\n            except:\n                self.server.handle_error(self.request, self.client_address)\n                os._exit(127)\n\n        else:\n            # Non-Unix -- use subprocess\n            import subprocess\n            cmdline = [scriptfile]\n            if self.is_python(scriptfile):\n                interp = sys.executable\n                if interp.lower().endswith(\"w.exe\"):\n                    # On Windows, use python.exe, not pythonw.exe\n                    interp = interp[:-5] + interp[-4:]\n                cmdline = [interp, '-u'] + cmdline\n            if '=' not in query:\n                cmdline.append(query)\n            self.log_message(\"command: %s\", subprocess.list2cmdline(cmdline))\n            try:\n                nbytes = int(length)\n            except (TypeError, ValueError):\n                nbytes = 0\n            p = subprocess.Popen(cmdline,\n                                 stdin=subprocess.PIPE,\n                                 stdout=subprocess.PIPE,\n                                 stderr=subprocess.PIPE,\n                                 env = env\n                                 )\n            if self.command.lower() == \"post\" and nbytes > 0:\n                data = self.rfile.read(nbytes)\n            else:\n                data = None\n            # throw away additional data [see bug #427345]\n            while select.select([self.rfile._sock], [], [], 0)[0]:\n                if not self.rfile._sock.recv(1):\n                    break\n            stdout, stderr = p.communicate(data)\n            self.wfile.write(stdout)\n            if stderr:\n                self.log_error('%s', stderr)\n            p.stderr.close()\n            p.stdout.close()\n            status = p.returncode\n            if status:\n                self.log_error(\"CGI script exit status %#x\", status)\n            else:\n                self.log_message(\"CGI script exited OK\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef index(self, value):\n        try:\n            diff = value - self._start\n        except TypeError:\n            raise ValueError('%r is not in range' % value)\n        quotient, remainder = divmod(diff, self._step)\n        if remainder == 0 and 0 <= quotient < self._len:\n            return abs(quotient)\n        raise ValueError('%r is not in range' % value)", "response": "Return the 0 - based position of integer value in\n        the sequence this range represents."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a range which represents the requested slce of the sequence represented by this range.", "response": "def __getitem_slice(self, slce):\n        \"\"\"Return a range which represents the requested slce\n        of the sequence represented by this range.\n        \"\"\"\n        scaled_indices = (self._step * n for n in slce.indices(self._len))\n        start_offset, stop_offset, new_step = scaled_indices\n        return newrange(self._start + start_offset,\n                        self._start + stop_offset,\n                        new_step)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the URL referring to a robots. txt file.", "response": "def set_url(self, url):\n        \"\"\"Sets the URL referring to a robots.txt file.\"\"\"\n        self.url = url\n        self.host, self.path = urllib.parse.urlparse(url)[1:3]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read(self):\n        try:\n            f = urllib.request.urlopen(self.url)\n        except urllib.error.HTTPError as err:\n            if err.code in (401, 403):\n                self.disallow_all = True\n            elif err.code >= 400:\n                self.allow_all = True\n        else:\n            raw = f.read()\n            self.parse(raw.decode(\"utf-8\").splitlines())", "response": "Reads the robots. txt URL and feeds it to the parser."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(self, lines):\n        # states:\n        #   0: start state\n        #   1: saw user-agent line\n        #   2: saw an allow or disallow line\n        state = 0\n        entry = Entry()\n\n        for line in lines:\n            if not line:\n                if state == 1:\n                    entry = Entry()\n                    state = 0\n                elif state == 2:\n                    self._add_entry(entry)\n                    entry = Entry()\n                    state = 0\n            # remove optional comment and strip line\n            i = line.find('#')\n            if i >= 0:\n                line = line[:i]\n            line = line.strip()\n            if not line:\n                continue\n            line = line.split(':', 1)\n            if len(line) == 2:\n                line[0] = line[0].strip().lower()\n                line[1] = urllib.parse.unquote(line[1].strip())\n                if line[0] == \"user-agent\":\n                    if state == 2:\n                        self._add_entry(entry)\n                        entry = Entry()\n                    entry.useragents.append(line[1])\n                    state = 1\n                elif line[0] == \"disallow\":\n                    if state != 0:\n                        entry.rulelines.append(RuleLine(line[1], False))\n                        state = 2\n                elif line[0] == \"allow\":\n                    if state != 0:\n                        entry.rulelines.append(RuleLine(line[1], True))\n                        state = 2\n        if state == 2:\n            self._add_entry(entry)", "response": "Parse the input lines from a robots. txt file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndecides if useragent can fetch url", "response": "def can_fetch(self, useragent, url):\n        \"\"\"using the parsed robots.txt decide if useragent can fetch url\"\"\"\n        if self.disallow_all:\n            return False\n        if self.allow_all:\n            return True\n        # search for given user agent matches\n        # the first match counts\n        parsed_url = urllib.parse.urlparse(urllib.parse.unquote(url))\n        url = urllib.parse.urlunparse(('','',parsed_url.path,\n            parsed_url.params,parsed_url.query, parsed_url.fragment))\n        url = urllib.parse.quote(url)\n        if not url:\n            url = \"/\"\n        for entry in self.entries:\n            if entry.applies_to(useragent):\n                return entry.allowance(url)\n        # try the default entry last\n        if self.default_entry:\n            return self.default_entry.allowance(url)\n        # agent not found ==> access granted\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef allowance(self, filename):\n        for line in self.rulelines:\n            if line.applies_to(filename):\n                return line.allowance\n        return True", "response": "Returns True if the entry contains a valid entry for the given filename."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef recursive_repr(fillvalue='...'):\n    'Decorator to make a repr function return fillvalue for a recursive call'\n\n    def decorating_function(user_function):\n        repr_running = set()\n\n        def wrapper(self):\n            key = id(self), get_ident()\n            if key in repr_running:\n                return fillvalue\n            repr_running.add(key)\n            try:\n                result = user_function(self)\n            finally:\n                repr_running.discard(key)\n            return result\n\n        # Can't use functools.wraps() here because of bootstrap issues\n        wrapper.__module__ = getattr(user_function, '__module__')\n        wrapper.__doc__ = getattr(user_function, '__doc__')\n        wrapper.__name__ = getattr(user_function, '__name__')\n        wrapper.__annotations__ = getattr(user_function, '__annotations__', {})\n        return wrapper\n\n    return decorating_function", "response": "Decorator to make a repr function return fillvalue for a recursive call"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntallies elements from the iterable.", "response": "def _count_elements(mapping, iterable):\n    'Tally elements from the iterable.'\n    mapping_get = mapping.get\n    for elem in iterable:\n        mapping[elem] = mapping_get(elem, 0) + 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clear(self):\n        'od.clear() -> None.  Remove all items from od.'\n        root = self.__root\n        root.prev = root.next = root\n        self.__map.clear()\n        dict.clear(self)", "response": "Remove all items from od."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef move_to_end(self, key, last=True):\n        '''Move an existing element to the end (or beginning if last==False).\n\n        Raises KeyError if the element does not exist.\n        When last=True, acts like a fast version of self[key]=self.pop(key).\n\n        '''\n        link = self.__map[key]\n        link_prev = link.prev\n        link_next = link.next\n        link_prev.next = link_next\n        link_next.prev = link_prev\n        root = self.__root\n        if last:\n            last = root.prev\n            link.prev = last\n            link.next = root\n            last.next = root.prev = link\n        else:\n            first = root.next\n            link.prev = root\n            link.next = first\n            root.next = first.prev = link", "response": "Moves an existing element to the end of the list."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists the n most common elements and their counts from the most common to the least.", "response": "def most_common(self, n=None):\n        '''List the n most common elements and their counts from the most\n        common to the least.  If n is None, then list all element counts.\n\n        >>> Counter('abcdeabcdabcaba').most_common(3)\n        [('a', 5), ('b', 4), ('c', 3)]\n\n        '''\n        # Emulate Bag.sortedByCount from Smalltalk\n        if n is None:\n            return sorted(self.items(), key=_itemgetter(1), reverse=True)\n        return _heapq.nlargest(n, self.items(), key=_itemgetter(1))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef flatten(self, msg, unixfrom=False, linesep=None):\n        # We use the _XXX constants for operating on data that comes directly\n        # from the msg, and _encoded_XXX constants for operating on data that\n        # has already been converted (to bytes in the BytesGenerator) and\n        # inserted into a temporary buffer.\n        policy = msg.policy if self.policy is None else self.policy\n        if linesep is not None:\n            policy = policy.clone(linesep=linesep)\n        if self.maxheaderlen is not None:\n            policy = policy.clone(max_line_length=self.maxheaderlen)\n        self._NL = policy.linesep\n        self._encoded_NL = self._encode(self._NL)\n        self._EMPTY = ''\n        self._encoded_EMTPY = self._encode('')\n        # Because we use clone (below) when we recursively process message\n        # subparts, and because clone uses the computed policy (not None),\n        # submessages will automatically get set to the computed policy when\n        # they are processed by this code.\n        old_gen_policy = self.policy\n        old_msg_policy = msg.policy\n        try:\n            self.policy = policy\n            msg.policy = policy\n            if unixfrom:\n                ufrom = msg.get_unixfrom()\n                if not ufrom:\n                    ufrom = 'From nobody ' + time.ctime(time.time())\n                self.write(ufrom + self._NL)\n            self._write(msg)\n        finally:\n            self.policy = old_gen_policy\n            msg.policy = old_msg_policy", "response": "r Prints the message object tree rooted at msg to the output file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclone this generator with the exact same options.", "response": "def clone(self, fp):\n        \"\"\"Clone this generator with the exact same options.\"\"\"\n        return self.__class__(fp,\n                              self._mangle_from_,\n                              None, # Use policy setting, which we've adjusted\n                              policy=self.policy)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef urlretrieve(url, filename=None, reporthook=None, data=None):\n    url_type, path = splittype(url)\n\n    with contextlib.closing(urlopen(url, data)) as fp:\n        headers = fp.info()\n\n        # Just return the local path and the \"headers\" for file://\n        # URLs. No sense in performing a copy unless requested.\n        if url_type == \"file\" and not filename:\n            return os.path.normpath(path), headers\n\n        # Handle temporary file setup.\n        if filename:\n            tfp = open(filename, 'wb')\n        else:\n            tfp = tempfile.NamedTemporaryFile(delete=False)\n            filename = tfp.name\n            _url_tempfiles.append(filename)\n\n        with tfp:\n            result = filename, headers\n            bs = 1024*8\n            size = -1\n            read = 0\n            blocknum = 0\n            if \"content-length\" in headers:\n                size = int(headers[\"Content-Length\"])\n\n            if reporthook:\n                reporthook(blocknum, bs, size)\n\n            while True:\n                block = fp.read(bs)\n                if not block:\n                    break\n                read += len(block)\n                tfp.write(block)\n                blocknum += 1\n                if reporthook:\n                    reporthook(blocknum, bs, size)\n\n    if size >= 0 and read < size:\n        raise ContentTooShortError(\n            \"retrieval incomplete: got only %i out of %i bytes\"\n            % (read, size), result)\n\n    return result", "response": "Retrieve a URL into a temporary location on disk."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef request_host(request):\n\n    \"\"\"Return request-host, as defined by RFC 2965.\n\n    Variation from RFC: returned value is lowercased, for convenient\n    comparison.\n\n    \"\"\"\n    url = request.full_url\n    host = urlparse(url)[1]\n    if host == \"\":\n        host = request.get_header(\"Host\", \"\")\n\n    # remove port, if present\n    host = _cut_port_re.sub(\"\", host, 1)\n    return host.lower()", "response": "Return request - host as defined by RFC 2965."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding an opener object from a list of handlers.", "response": "def build_opener(*handlers):\n    \"\"\"Create an opener object from a list of handlers.\n\n    The opener will use several default handlers, including support\n    for HTTP, FTP and when applicable HTTPS.\n\n    If any of the handlers passed as arguments are subclasses of the\n    default handlers, the default handlers will not be used.\n    \"\"\"\n    def isclass(obj):\n        return isinstance(obj, type) or hasattr(obj, \"__bases__\")\n\n    opener = OpenerDirector()\n    default_classes = [ProxyHandler, UnknownHandler, HTTPHandler,\n                       HTTPDefaultErrorHandler, HTTPRedirectHandler,\n                       FTPHandler, FileHandler, HTTPErrorProcessor]\n    if hasattr(http_client, \"HTTPSConnection\"):\n        default_classes.append(HTTPSHandler)\n    skip = set()\n    for klass in default_classes:\n        for check in handlers:\n            if isclass(check):\n                if issubclass(check, klass):\n                    skip.add(klass)\n            elif isinstance(check, klass):\n                skip.add(klass)\n    for klass in skip:\n        default_classes.remove(klass)\n\n    for klass in default_classes:\n        opener.add_handler(klass())\n\n    for h in handlers:\n        if isclass(h):\n            h = h()\n        opener.add_handler(h)\n    return opener"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_proxy(proxy):\n    scheme, r_scheme = splittype(proxy)\n    if not r_scheme.startswith(\"/\"):\n        # authority\n        scheme = None\n        authority = proxy\n    else:\n        # URL\n        if not r_scheme.startswith(\"//\"):\n            raise ValueError(\"proxy URL with no authority: %r\" % proxy)\n        # We have an authority, so for RFC 3986-compliant URLs (by ss 3.\n        # and 3.3.), path is empty or starts with '/'\n        end = r_scheme.find(\"/\", 2)\n        if end == -1:\n            end = None\n        authority = r_scheme[2:end]\n    userinfo, hostport = splituser(authority)\n    if userinfo is not None:\n        user, password = splitpasswd(userinfo)\n    else:\n        user = password = None\n    return scheme, user, password, hostport", "response": "Parse a proxy URL or authority."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_keqv_list(l):\n    parsed = {}\n    for elt in l:\n        k, v = elt.split('=', 1)\n        if v[0] == '\"' and v[-1] == '\"':\n            v = v[1:-1]\n        parsed[k] = v\n    return parsed", "response": "Parse list of key = value strings where keys are not duplicated."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_http_list(s):\n    res = []\n    part = ''\n\n    escape = quote = False\n    for cur in s:\n        if escape:\n            part += cur\n            escape = False\n            continue\n        if quote:\n            if cur == '\\\\':\n                escape = True\n                continue\n            elif cur == '\"':\n                quote = False\n            part += cur\n            continue\n\n        if cur == ',':\n            res.append(part)\n            part = ''\n            continue\n\n        if cur == '\"':\n            quote = True\n\n        part += cur\n\n    # append last part\n    if part:\n        res.append(part)\n\n    return [part.strip() for part in res]", "response": "Parse a list of http lists."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the IP addresses of the current host.", "response": "def thishost():\n    \"\"\"Return the IP addresses of the current host.\"\"\"\n    global _thishost\n    if _thishost is None:\n        try:\n            _thishost = tuple(socket.gethostbyname_ex(socket.gethostname())[2])\n        except socket.gaierror:\n            _thishost = tuple(socket.gethostbyname_ex('localhost')[2])\n    return _thishost"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getproxies_environment():\n    proxies = {}\n    for name, value in os.environ.items():\n        name = name.lower()\n        if value and name[-6:] == '_proxy':\n            proxies[name[:-6]] = value\n    return proxies", "response": "Return a dictionary of scheme - > proxy server URLs mappings."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntest if proxies should not be used for a particular host.", "response": "def proxy_bypass_environment(host):\n    \"\"\"Test if proxies should not be used for a particular host.\n\n    Checks the environment for a variable named no_proxy, which should\n    be a list of DNS suffixes separated by commas, or '*' for all hosts.\n    \"\"\"\n    no_proxy = os.environ.get('no_proxy', '') or os.environ.get('NO_PROXY', '')\n    # '*' is special case for always bypass\n    if no_proxy == '*':\n        return 1\n    # strip port off host\n    hostonly, port = splitport(host)\n    # check if the host ends with any of the DNS suffixes\n    no_proxy_list = [proxy.strip() for proxy in no_proxy.split(',')]\n    for name in no_proxy_list:\n        if name and (hostonly.endswith(name) or host.endswith(name)):\n            return 1\n    # otherwise, don't bypass\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _proxy_bypass_macosx_sysconf(host, proxy_settings):\n    from fnmatch import fnmatch\n\n    hostonly, port = splitport(host)\n\n    def ip2num(ipAddr):\n        parts = ipAddr.split('.')\n        parts = list(map(int, parts))\n        if len(parts) != 4:\n            parts = (parts + [0, 0, 0, 0])[:4]\n        return (parts[0] << 24) | (parts[1] << 16) | (parts[2] << 8) | parts[3]\n\n    # Check for simple host names:\n    if '.' not in host:\n        if proxy_settings['exclude_simple']:\n            return True\n\n    hostIP = None\n\n    for value in proxy_settings.get('exceptions', ()):\n        # Items in the list are strings like these: *.local, 169.254/16\n        if not value: continue\n\n        m = re.match(r\"(\\d+(?:\\.\\d+)*)(/\\d+)?\", value)\n        if m is not None:\n            if hostIP is None:\n                try:\n                    hostIP = socket.gethostbyname(hostonly)\n                    hostIP = ip2num(hostIP)\n                except socket.error:\n                    continue\n\n            base = ip2num(m.group(1))\n            mask = m.group(2)\n            if mask is None:\n                mask = 8 * (m.group(1).count('.') + 1)\n            else:\n                mask = int(mask[1:])\n            mask = 32 - mask\n\n            if (hostIP >> mask) == (base >> mask):\n                return True\n\n        elif fnmatch(host, value):\n            return True\n\n    return False", "response": "Return True iff this host should be accessed using a proxy"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef open(self, fullurl, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT):\n        if isinstance(fullurl, bytes):\n            fullurl = fullurl.decode()\n        if isinstance(fullurl, str):\n            req = Request(fullurl, data)\n        else:\n            req = fullurl\n            if data is not None:\n                req.data = data\n\n        req.timeout = timeout\n        protocol = req.type\n\n        # pre-process request\n        meth_name = protocol+\"_request\"\n        for processor in self.process_request.get(protocol, []):\n            meth = getattr(processor, meth_name)\n            req = meth(req)\n\n        response = self._open(req, data)\n\n        # post-process response\n        meth_name = protocol+\"_response\"\n        for processor in self.process_response.get(protocol, []):\n            meth = getattr(processor, meth_name)\n            response = meth(req, response)\n\n        return response", "response": "Open a URL or a Request object and return a response object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a Request or None in response to a redirect.", "response": "def redirect_request(self, req, fp, code, msg, headers, newurl):\n        \"\"\"Return a Request or None in response to a redirect.\n\n        This is called by the http_error_30x methods when a\n        redirection response is received.  If a redirection should\n        take place, return a new Request to allow http_error_30x to\n        perform the redirect.  Otherwise, raise HTTPError if no-one\n        else should try to handle this url.  Return None if you can't\n        but another Handler might.\n        \"\"\"\n        m = req.get_method()\n        if (not (code in (301, 302, 303, 307) and m in (\"GET\", \"HEAD\")\n            or code in (301, 302, 303) and m == \"POST\")):\n            raise HTTPError(req.full_url, code, msg, headers, fp)\n\n        # Strictly (according to RFC 2616), 301 or 302 in response to\n        # a POST MUST NOT cause a redirection without confirmation\n        # from the user (of urllib.request, in this case).  In practice,\n        # essentially all clients do redirect in this case, so we do\n        # the same.\n        # be conciliant with URIs containing a space\n        newurl = newurl.replace(' ', '%20')\n        CONTENT_HEADERS = (\"content-length\", \"content-type\")\n        newheaders = dict((k, v) for k, v in req.headers.items()\n                          if k.lower() not in CONTENT_HEADERS)\n        return Request(newurl,\n                       headers=newheaders,\n                       origin_req_host=req.origin_req_host,\n                       unverifiable=True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\naccepting authority or URI and extract only the authority and path.", "response": "def reduce_uri(self, uri, default_port=True):\n        \"\"\"Accept authority or URI and extract only the authority and path.\"\"\"\n        # note HTTP URLs do not have a userinfo component\n        parts = urlsplit(uri)\n        if parts[1]:\n            # URI\n            scheme = parts[0]\n            authority = parts[1]\n            path = parts[2] or '/'\n        else:\n            # host or host:port\n            scheme = None\n            authority = uri\n            path = '/'\n        host, port = splitport(authority)\n        if default_port and port is None and scheme is not None:\n            dport = {\"http\": 80,\n                     \"https\": 443,\n                     }.get(scheme)\n            if dport is not None:\n                authority = \"%s:%d\" % (host, dport)\n        return authority, path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if test is below base in a URI tree", "response": "def is_suburi(self, base, test):\n        \"\"\"Check if test is below base in a URI tree\n\n        Both args must be URIs in reduced form.\n        \"\"\"\n        if base == test:\n            return True\n        if base[0] != test[0]:\n            return False\n        common = posixpath.commonprefix((base[1], test[1]))\n        if len(common) == len(base[1]):\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nopens a new HTTP connection.", "response": "def do_open(self, http_class, req, **http_conn_args):\n        \"\"\"Return an HTTPResponse object for the request, using http_class.\n\n        http_class must implement the HTTPConnection API from http.client.\n        \"\"\"\n        host = req.host\n        if not host:\n            raise URLError('no host given')\n\n        # will parse host:port\n        h = http_class(host, timeout=req.timeout, **http_conn_args)\n\n        headers = dict(req.unredirected_hdrs)\n        headers.update(dict((k, v) for k, v in req.headers.items()\n                            if k not in headers))\n\n        # TODO(jhylton): Should this be redesigned to handle\n        # persistent connections?\n\n        # We want to make an HTTP/1.1 request, but the addinfourl\n        # class isn't prepared to deal with a persistent connection.\n        # It will try to read all remaining data from the socket,\n        # which will block while the server waits for the next request.\n        # So make sure the connection gets closed after the (only)\n        # request.\n        headers[\"Connection\"] = \"close\"\n        headers = dict((name.title(), val) for name, val in headers.items())\n\n        if req._tunnel_host:\n            tunnel_headers = {}\n            proxy_auth_hdr = \"Proxy-Authorization\"\n            if proxy_auth_hdr in headers:\n                tunnel_headers[proxy_auth_hdr] = headers[proxy_auth_hdr]\n                # Proxy-Authorization should not be sent to origin\n                # server.\n                del headers[proxy_auth_hdr]\n            h.set_tunnel(req._tunnel_host, headers=tunnel_headers)\n\n        try:\n            h.request(req.get_method(), req.selector, req.data, headers)\n        except socket.error as err: # timeout error\n            h.close()\n            raise URLError(err)\n        else:\n            r = h.getresponse()\n            # If the server does not send us a 'Connection: close' header,\n            # HTTPConnection assumes the socket should be left open. Manually\n            # mark the socket to be closed when this response object goes away.\n            if h.sock:\n                h.sock.close()\n                h.sock = None\n\n\n        r.url = req.get_full_url()\n        # This line replaces the .msg attribute of the HTTPResponse\n        # with .headers, because urllib clients expect the response to\n        # have the reason in .msg.  It would be good to mark this\n        # attribute is deprecated and get then to use info() or\n        # .headers.\n        r.msg = r.reason\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef open(self, fullurl, data=None):\n        fullurl = unwrap(to_bytes(fullurl))\n        fullurl = quote(fullurl, safe=\"%/:=&?~#+!$,;'@()*[]|\")\n        if self.tempcache and fullurl in self.tempcache:\n            filename, headers = self.tempcache[fullurl]\n            fp = open(filename, 'rb')\n            return addinfourl(fp, headers, fullurl)\n        urltype, url = splittype(fullurl)\n        if not urltype:\n            urltype = 'file'\n        if urltype in self.proxies:\n            proxy = self.proxies[urltype]\n            urltype, proxyhost = splittype(proxy)\n            host, selector = splithost(proxyhost)\n            url = (host, fullurl) # Signal special case to open_*()\n        else:\n            proxy = None\n        name = 'open_' + urltype\n        self.type = urltype\n        name = name.replace('-', '_')\n        if not hasattr(self, name):\n            if proxy:\n                return self.open_unknown_proxy(proxy, fullurl, data)\n            else:\n                return self.open_unknown(fullurl, data)\n        try:\n            if data is None:\n                return getattr(self, name)(url)\n            else:\n                return getattr(self, name)(url, data)\n        except HTTPError:\n            raise\n        except socket.error as msg:\n            raise_with_traceback(IOError('socket error', msg))", "response": "Use URLopener. open instead of URLopener. open."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef open_unknown_proxy(self, proxy, fullurl, data=None):\n        type, url = splittype(fullurl)\n        raise IOError('url error', 'invalid proxy for %s' % type, proxy)", "response": "Overridable interface to open unknown proxy."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef retrieve(self, url, filename=None, reporthook=None, data=None):\n        url = unwrap(to_bytes(url))\n        if self.tempcache and url in self.tempcache:\n            return self.tempcache[url]\n        type, url1 = splittype(url)\n        if filename is None and (not type or type == 'file'):\n            try:\n                fp = self.open_local_file(url1)\n                hdrs = fp.info()\n                fp.close()\n                return url2pathname(splithost(url1)[1]), hdrs\n            except IOError as msg:\n                pass\n        fp = self.open(url, data)\n        try:\n            headers = fp.info()\n            if filename:\n                tfp = open(filename, 'wb')\n            else:\n                import tempfile\n                garbage, path = splittype(url)\n                garbage, path = splithost(path or \"\")\n                path, garbage = splitquery(path or \"\")\n                path, garbage = splitattr(path or \"\")\n                suffix = os.path.splitext(path)[1]\n                (fd, filename) = tempfile.mkstemp(suffix)\n                self.__tempfiles.append(filename)\n                tfp = os.fdopen(fd, 'wb')\n            try:\n                result = filename, headers\n                if self.tempcache is not None:\n                    self.tempcache[url] = result\n                bs = 1024*8\n                size = -1\n                read = 0\n                blocknum = 0\n                if \"content-length\" in headers:\n                    size = int(headers[\"Content-Length\"])\n                if reporthook:\n                    reporthook(blocknum, bs, size)\n                while 1:\n                    block = fp.read(bs)\n                    if not block:\n                        break\n                    read += len(block)\n                    tfp.write(block)\n                    blocknum += 1\n                    if reporthook:\n                        reporthook(blocknum, bs, size)\n            finally:\n                tfp.close()\n        finally:\n            fp.close()\n\n        # raise exception if actual size does not match content-length header\n        if size >= 0 and read < size:\n            raise ContentTooShortError(\n                \"retrieval incomplete: got only %i out of %i bytes\"\n                % (read, size), result)\n\n        return result", "response": "retrieve returns filename headers for a local object or a temporary file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _open_generic_http(self, connection_factory, url, data):\n\n        user_passwd = None\n        proxy_passwd= None\n        if isinstance(url, str):\n            host, selector = splithost(url)\n            if host:\n                user_passwd, host = splituser(host)\n                host = unquote(host)\n            realhost = host\n        else:\n            host, selector = url\n            # check whether the proxy contains authorization information\n            proxy_passwd, host = splituser(host)\n            # now we proceed with the url we want to obtain\n            urltype, rest = splittype(selector)\n            url = rest\n            user_passwd = None\n            if urltype.lower() != 'http':\n                realhost = None\n            else:\n                realhost, rest = splithost(rest)\n                if realhost:\n                    user_passwd, realhost = splituser(realhost)\n                if user_passwd:\n                    selector = \"%s://%s%s\" % (urltype, realhost, rest)\n                if proxy_bypass(realhost):\n                    host = realhost\n\n        if not host: raise IOError('http error', 'no host given')\n\n        if proxy_passwd:\n            proxy_passwd = unquote(proxy_passwd)\n            proxy_auth = base64.b64encode(proxy_passwd.encode()).decode('ascii')\n        else:\n            proxy_auth = None\n\n        if user_passwd:\n            user_passwd = unquote(user_passwd)\n            auth = base64.b64encode(user_passwd.encode()).decode('ascii')\n        else:\n            auth = None\n        http_conn = connection_factory(host)\n        headers = {}\n        if proxy_auth:\n            headers[\"Proxy-Authorization\"] = \"Basic %s\" % proxy_auth\n        if auth:\n            headers[\"Authorization\"] =  \"Basic %s\" % auth\n        if realhost:\n            headers[\"Host\"] = realhost\n\n        # Add Connection:close as we don't support persistent connections yet.\n        # This helps in closing the socket and avoiding ResourceWarning\n\n        headers[\"Connection\"] = \"close\"\n\n        for header, value in self.addheaders:\n            headers[header] = value\n\n        if data is not None:\n            headers[\"Content-Type\"] = \"application/x-www-form-urlencoded\"\n            http_conn.request(\"POST\", selector, data, headers)\n        else:\n            http_conn.request(\"GET\", selector, headers=headers)\n\n        try:\n            response = http_conn.getresponse()\n        except http_client.BadStatusLine:\n            # something went wrong with the HTTP status line\n            raise URLError(\"http protocol error: bad status line\")\n\n        # According to RFC 2616, \"2xx\" code indicates that the client's\n        # request was successfully received, understood, and accepted.\n        if 200 <= response.status < 300:\n            return addinfourl(response, response.msg, \"http:\" + url,\n                              response.status)\n        else:\n            return self.http_error(\n                url, response.fp,\n                response.status, response.reason, response.msg, data)", "response": "Open an HTTP connection using connection_factory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef open_http(self, url, data=None):\n        return self._open_generic_http(http_client.HTTPConnection, url, data)", "response": "Open an HTTP connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles http errors. Derived class can override this, or provide specific handlers named http_error_DDD where DDD is the 3-digit error code.", "response": "def http_error(self, url, fp, errcode, errmsg, headers, data=None):\n        \"\"\"Handle http errors.\n\n        Derived class can override this, or provide specific handlers\n        named http_error_DDD where DDD is the 3-digit error code.\"\"\"\n        # First check if there's a specific handler for this error\n        name = 'http_error_%d' % errcode\n        if hasattr(self, name):\n            method = getattr(self, name)\n            if data is None:\n                result = method(url, fp, errcode, errmsg, headers)\n            else:\n                result = method(url, fp, errcode, errmsg, headers, data)\n            if result: return result\n        return self.http_error_default(url, fp, errcode, errmsg, headers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef http_error_default(self, url, fp, errcode, errmsg, headers):\n        fp.close()\n        raise HTTPError(url, errcode, errmsg, headers, None)", "response": "Default error handler: close the connection and raise IOError."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef open_file(self, url):\n        if not isinstance(url, str):\n            raise URLError('file error: proxy support for file protocol currently not implemented')\n        if url[:2] == '//' and url[2:3] != '/' and url[2:12].lower() != 'localhost/':\n            raise ValueError(\"file:// scheme is supported only on localhost\")\n        else:\n            return self.open_local_file(url)", "response": "Use local file or FTP depending on form of URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef open_ftp(self, url):\n        if not isinstance(url, str):\n            raise URLError('ftp error: proxy support for ftp protocol currently not implemented')\n        import mimetypes\n        host, path = splithost(url)\n        if not host: raise URLError('ftp error: no host given')\n        host, port = splitport(host)\n        user, host = splituser(host)\n        if user: user, passwd = splitpasswd(user)\n        else: passwd = None\n        host = unquote(host)\n        user = unquote(user or '')\n        passwd = unquote(passwd or '')\n        host = socket.gethostbyname(host)\n        if not port:\n            import ftplib\n            port = ftplib.FTP_PORT\n        else:\n            port = int(port)\n        path, attrs = splitattr(path)\n        path = unquote(path)\n        dirs = path.split('/')\n        dirs, file = dirs[:-1], dirs[-1]\n        if dirs and not dirs[0]: dirs = dirs[1:]\n        if dirs and not dirs[0]: dirs[0] = '/'\n        key = user, host, port, '/'.join(dirs)\n        # XXX thread unsafe!\n        if len(self.ftpcache) > MAXFTPCACHE:\n            # Prune the cache, rather arbitrarily\n            for k in self.ftpcache.keys():\n                if k != key:\n                    v = self.ftpcache[k]\n                    del self.ftpcache[k]\n                    v.close()\n        try:\n            if key not in self.ftpcache:\n                self.ftpcache[key] = \\\n                    ftpwrapper(user, passwd, host, port, dirs)\n            if not file: type = 'D'\n            else: type = 'I'\n            for attr in attrs:\n                attr, value = splitvalue(attr)\n                if attr.lower() == 'type' and \\\n                   value in ('a', 'A', 'i', 'I', 'd', 'D'):\n                    type = value.upper()\n            (fp, retrlen) = self.ftpcache[key].retrfile(file, type)\n            mtype = mimetypes.guess_type(\"ftp:\" + url)[0]\n            headers = \"\"\n            if mtype:\n                headers += \"Content-Type: %s\\n\" % mtype\n            if retrlen is not None and retrlen >= 0:\n                headers += \"Content-Length: %d\\n\" % retrlen\n            headers = email.message_from_string(headers)\n            return addinfourl(fp, headers, \"ftp:\" + url)\n        except ftperrors() as exp:\n            raise_with_traceback(URLError('ftp error %r' % exp))", "response": "Open an ftp connection."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef open_data(self, url, data=None):\n        if not isinstance(url, str):\n            raise URLError('data error: proxy support for data protocol currently not implemented')\n        # ignore POSTed data\n        #\n        # syntax of data URLs:\n        # dataurl   := \"data:\" [ mediatype ] [ \";base64\" ] \",\" data\n        # mediatype := [ type \"/\" subtype ] *( \";\" parameter )\n        # data      := *urlchar\n        # parameter := attribute \"=\" value\n        try:\n            [type, data] = url.split(',', 1)\n        except ValueError:\n            raise IOError('data error', 'bad data URL')\n        if not type:\n            type = 'text/plain;charset=US-ASCII'\n        semi = type.rfind(';')\n        if semi >= 0 and '=' not in type[semi:]:\n            encoding = type[semi+1:]\n            type = type[:semi]\n        else:\n            encoding = ''\n        msg = []\n        msg.append('Date: %s'%time.strftime('%a, %d %b %Y %H:%M:%S GMT',\n                                            time.gmtime(time.time())))\n        msg.append('Content-type: %s' % type)\n        if encoding == 'base64':\n            # XXX is this encoding/decoding ok?\n            data = base64.decodebytes(data.encode('ascii')).decode('latin-1')\n        else:\n            data = unquote(data)\n        msg.append('Content-Length: %d' % len(data))\n        msg.append('')\n        msg.append(data)\n        msg = '\\n'.join(msg)\n        headers = email.message_from_string(msg)\n        f = io.StringIO(msg)\n        #f.fileno = None     # needed for addinfourl\n        return addinfourl(f, headers, url)", "response": "Open a data URL and return a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef http_error_default(self, url, fp, errcode, errmsg, headers):\n        return addinfourl(fp, headers, \"http:\" + url, errcode)", "response": "Default error handling -- don't raise an exception."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef http_error_302(self, url, fp, errcode, errmsg, headers, data=None):\n        self.tries += 1\n        if self.maxtries and self.tries >= self.maxtries:\n            if hasattr(self, \"http_error_500\"):\n                meth = self.http_error_500\n            else:\n                meth = self.http_error_default\n            self.tries = 0\n            return meth(url, fp, 500,\n                        \"Internal Server Error: Redirect Recursion\", headers)\n        result = self.redirect_internal(url, fp, errcode, errmsg, headers,\n                                        data)\n        self.tries = 0\n        return result", "response": "HTTP 302 error handler."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef http_error_401(self, url, fp, errcode, errmsg, headers, data=None,\n            retry=False):\n        \"\"\"Error 401 -- authentication required.\n        This function supports Basic authentication only.\"\"\"\n        if 'www-authenticate' not in headers:\n            URLopener.http_error_default(self, url, fp,\n                                         errcode, errmsg, headers)\n        stuff = headers['www-authenticate']\n        match = re.match('[ \\t]*([^ \\t]+)[ \\t]+realm=\"([^\"]*)\"', stuff)\n        if not match:\n            URLopener.http_error_default(self, url, fp,\n                                         errcode, errmsg, headers)\n        scheme, realm = match.groups()\n        if scheme.lower() != 'basic':\n            URLopener.http_error_default(self, url, fp,\n                                         errcode, errmsg, headers)\n        if not retry:\n            URLopener.http_error_default(self, url, fp, errcode, errmsg,\n                    headers)\n        name = 'retry_' + self.type + '_basic_auth'\n        if data is None:\n            return getattr(self,name)(url, realm)\n        else:\n            return getattr(self,name)(url, realm, data)", "response": "HTTP error 401. This function supports Basic authentication only."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef prompt_user_passwd(self, host, realm):\n        import getpass\n        try:\n            user = input(\"Enter username for %s at %s: \" % (realm, host))\n            passwd = getpass.getpass(\"Enter password for %s in %s at %s: \" %\n                (user, realm, host))\n            return user, passwd\n        except KeyboardInterrupt:\n            print()\n            return None, None", "response": "Prompt user and password for a specific user."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef match_division(node):\n    slash = token.SLASH\n    return node.type == slash and not node.next_sibling.type == slash and \\\n                                  not node.prev_sibling.type == slash", "response": "u Returns True if the node matches the division."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstart the fixer for the given tree.", "response": "def start_tree(self, tree, name):\n        \"\"\"\n        Skip this fixer if \"__future__.division\" is already imported.\n        \"\"\"\n        super(FixDivisionSafe, self).start_tree(tree, name)\n        self.skip = \"division\" in tree.future_features"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef match(self, node):\n        if node.type == self.syms.term:\n            div_idx = find_division(node)\n            if div_idx is not False:\n                # if expr1 or expr2 are obviously floats, we don't need to wrap in\n                # old_div, as the behavior of division between any number and a float\n                # should be the same in 2 or 3\n                if not is_floaty(node, div_idx):\n                    return clone_div_operands(node, div_idx)\n        return False", "response": "u Return True if the tree matches the node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef push(self, data):\n        # Handle any previous leftovers\n        data, self._partial = self._partial + data, ''\n        # Crack into lines, but preserve the newlines on the end of each\n        parts = NLCRE_crack.split(data)\n        # The *ahem* interesting behaviour of re.split when supplied grouping\n        # parentheses is that the last element of the resulting list is the\n        # data after the final RE.  In the case of a NL/CR terminated string,\n        # this is the empty string.\n        self._partial = parts.pop()\n        #GAN 29Mar09  bugs 1555570, 1721862  Confusion at 8K boundary ending with \\r:\n        # is there a \\n to follow later?\n        if not self._partial and parts and parts[-1].endswith('\\r'):\n            self._partial = parts.pop(-2)+parts.pop()\n        # parts is a list of strings, alternating between the line contents\n        # and the eol character(s).  Gather up a list of lines after\n        # re-attaching the newlines.\n        lines = []\n        for i in range(len(parts) // 2):\n            lines.append(parts[i*2] + parts[i*2+1])\n        self.pushlines(lines)", "response": "Push some new data into the internal list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse all remaining data and return the root message object.", "response": "def close(self):\n        \"\"\"Parse all remaining data and return the root message object.\"\"\"\n        self._input.close()\n        self._call_parse()\n        root = self._pop_message()\n        assert not self._msgstack\n        # Look for final set of defects\n        if root.get_content_maintype() == 'multipart' \\\n               and not root.is_multipart():\n            defect = errors.MultipartInvariantViolationDefect()\n            self.policy.handle_defect(root, defect)\n        return root"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new number in a new order.", "response": "def newround(number, ndigits=None):\n    \"\"\"\n    See Python 3 documentation: uses Banker's Rounding.\n\n    Delegates to the __round__ method if for some reason this exists.\n\n    If not, rounds a number to a given precision in decimal digits (default\n    0 digits). This returns an int when called with one argument,\n    otherwise the same type as the number. ndigits may be negative.\n\n    See the test_round method in future/tests/test_builtins.py for\n    examples.\n    \"\"\"\n    return_int = False\n    if ndigits is None:\n        return_int = True\n        ndigits = 0\n    if hasattr(number, '__round__'):\n        return number.__round__(ndigits)\n\n    if ndigits < 0:\n        raise NotImplementedError('negative ndigits not supported yet')\n    exponent = Decimal('10') ** (-ndigits)\n\n    if PYPY:\n        # Work around issue #24: round() breaks on PyPy with NumPy's types\n        if 'numpy' in repr(type(number)):\n            number = float(number)\n\n    if not PY26:\n        d = Decimal.from_float(number).quantize(exponent,\n                                            rounding=ROUND_HALF_EVEN)\n    else:\n        d = from_float_26(number).quantize(exponent, rounding=ROUND_HALF_EVEN)\n\n    if return_int:\n        return int(d)\n    else:\n        return float(d)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a float to a decimal number exactly.", "response": "def from_float_26(f):\n    \"\"\"Converts a float to a decimal number, exactly.\n\n    Note that Decimal.from_float(0.1) is not the same as Decimal('0.1').\n    Since 0.1 is not exactly representable in binary floating point, the\n    value is stored as the nearest representable value which is\n    0x1.999999999999ap-4.  The exact equivalent of the value in decimal\n    is 0.1000000000000000055511151231257827021181583404541015625.\n\n    >>> Decimal.from_float(0.1)\n    Decimal('0.1000000000000000055511151231257827021181583404541015625')\n    >>> Decimal.from_float(float('nan'))\n    Decimal('NaN')\n    >>> Decimal.from_float(float('inf'))\n    Decimal('Infinity')\n    >>> Decimal.from_float(-float('inf'))\n    Decimal('-Infinity')\n    >>> Decimal.from_float(-0.0)\n    Decimal('-0')\n\n    \"\"\"\n    import math as _math\n    from decimal import _dec_from_triple    # only available on Py2.6 and Py2.7 (not 3.3)\n\n    if isinstance(f, (int, long)):        # handle integer inputs\n        return Decimal(f)\n    if _math.isinf(f) or _math.isnan(f):  # raises TypeError if not a float\n        return Decimal(repr(f))\n    if _math.copysign(1.0, f) == 1.0:\n        sign = 0\n    else:\n        sign = 1\n    n, d = abs(f).as_integer_ratio()\n    # int.bit_length() method doesn't exist on Py2.6:\n    def bit_length(d):\n        if d != 0:\n            return len(bin(abs(d))) - 2\n        else:\n            return 0\n    k = bit_length(d) - 1\n    result = _dec_from_triple(sign, str(n*5**k), -k)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef transform(self, node, results):\n        params = results.get(u\"params\")\n        ret = results.get(u\"ret\")\n        if ret is not None:\n            assert ret.prev_sibling.type == token.RARROW, u\"Invalid return annotation\"\n            self.warn_once(node, reason=warning_text)\n            ret.prev_sibling.remove()\n            ret.remove()\n        if params is None: return\n        if params.type == syms.typedargslist:\n            # more than one param in a typedargslist\n            for param in params.children:\n                if param.type == syms.tname:\n                    self.warn_once(node, reason=warning_text)\n                    param.replace(param_without_annotations(param))\n        elif params.type == syms.tname:\n            # one param\n            self.warn_once(node, reason=warning_text)\n            params.replace(param_without_annotations(params))", "response": "u This just strips annotations from the funcdef completely."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts an integer to a byte array.", "response": "def to_bytes(self, length, byteorder='big', signed=False):\n        \"\"\"\n        Return an array of bytes representing an integer.\n\n        The integer is represented using length bytes.  An OverflowError is\n        raised if the integer is not representable with the given number of\n        bytes.\n\n        The byteorder argument determines the byte order used to represent the\n        integer.  If byteorder is 'big', the most significant byte is at the\n        beginning of the byte array.  If byteorder is 'little', the most\n        significant byte is at the end of the byte array.  To request the native\n        byte order of the host system, use `sys.byteorder' as the byte order value.\n\n        The signed keyword-only argument determines whether two's complement is\n        used to represent the integer.  If signed is False and a negative integer\n        is given, an OverflowError is raised.\n        \"\"\"\n        if length < 0:\n            raise ValueError(\"length argument must be non-negative\")\n        if length == 0 and self == 0:\n            return newbytes()\n        if signed and self < 0:\n            bits = length * 8\n            num = (2**bits) + self\n            if num <= 0:\n                raise OverflowError(\"int too smal to convert\")\n        else:\n            if self < 0:\n                raise OverflowError(\"can't convert negative int to unsigned\")\n            num = self\n        if byteorder not in ('little', 'big'):\n            raise ValueError(\"byteorder must be either 'little' or 'big'\")\n        h = b'%x' % num\n        s = newbytes((b'0'*(len(h) % 2) + h).zfill(length*2).decode('hex'))\n        if signed:\n            high_set = s[0] & 0x80\n            if self > 0 and high_set:\n                raise OverflowError(\"int too big to convert\")\n            if self < 0 and not high_set:\n                raise OverflowError(\"int too small to convert\")\n        if len(s) > length:\n            raise OverflowError(\"int too big to convert\")\n        return s if byteorder == 'big' else s[::-1]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the integer represented by the given bytes.", "response": "def from_bytes(cls, mybytes, byteorder='big', signed=False):\n        \"\"\"\n        Return the integer represented by the given array of bytes.\n\n        The mybytes argument must either support the buffer protocol or be an\n        iterable object producing bytes.  Bytes and bytearray are examples of\n        built-in objects that support the buffer protocol.\n\n        The byteorder argument determines the byte order used to represent the\n        integer.  If byteorder is 'big', the most significant byte is at the\n        beginning of the byte array.  If byteorder is 'little', the most\n        significant byte is at the end of the byte array.  To request the native\n        byte order of the host system, use `sys.byteorder' as the byte order value.\n\n        The signed keyword-only argument indicates whether two's complement is\n        used to represent the integer.\n        \"\"\"\n        if byteorder not in ('little', 'big'):\n            raise ValueError(\"byteorder must be either 'little' or 'big'\")\n        if isinstance(mybytes, unicode):\n            raise TypeError(\"cannot convert unicode objects to bytes\")\n        # mybytes can also be passed as a sequence of integers on Py3.\n        # Test for this:\n        elif isinstance(mybytes, collections.Iterable):\n            mybytes = newbytes(mybytes)\n        b = mybytes if byteorder == 'big' else mybytes[::-1]\n        if len(b) == 0:\n            b = b'\\x00'\n        # The encode() method has been disabled by newbytes, but Py2's\n        # str has it:\n        num = int(native(b).encode('hex'), 16)\n        if signed and (b[0] & 0x80):\n            num = num - (2 ** (len(b)*8))\n        return cls(num)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parsedate_tz(data):\n    res = _parsedate_tz(data)\n    if not res:\n        return\n    if res[9] is None:\n        res[9] = 0\n    return tuple(res)", "response": "Convert a date string to a time tuple."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert date to extended time tuple.", "response": "def _parsedate_tz(data):\n    \"\"\"Convert date to extended time tuple.\n\n    The last (additional) element is the time zone offset in seconds, except if\n    the timezone was specified as -0000.  In that case the last element is\n    None.  This indicates a UTC timestamp that explicitly declaims knowledge of\n    the source timezone, as opposed to a +0000 timestamp that indicates the\n    source timezone really was UTC.\n\n    \"\"\"\n    if not data:\n        return\n    data = data.split()\n    # The FWS after the comma after the day-of-week is optional, so search and\n    # adjust for this.\n    if data[0].endswith(',') or data[0].lower() in _daynames:\n        # There's a dayname here. Skip it\n        del data[0]\n    else:\n        i = data[0].rfind(',')\n        if i >= 0:\n            data[0] = data[0][i+1:]\n    if len(data) == 3: # RFC 850 date, deprecated\n        stuff = data[0].split('-')\n        if len(stuff) == 3:\n            data = stuff + data[1:]\n    if len(data) == 4:\n        s = data[3]\n        i = s.find('+')\n        if i == -1:\n            i = s.find('-')\n        if i > 0:\n            data[3:] = [s[:i], s[i:]]\n        else:\n            data.append('') # Dummy tz\n    if len(data) < 5:\n        return None\n    data = data[:5]\n    [dd, mm, yy, tm, tz] = data\n    mm = mm.lower()\n    if mm not in _monthnames:\n        dd, mm = mm, dd.lower()\n        if mm not in _monthnames:\n            return None\n    mm = _monthnames.index(mm) + 1\n    if mm > 12:\n        mm -= 12\n    if dd[-1] == ',':\n        dd = dd[:-1]\n    i = yy.find(':')\n    if i > 0:\n        yy, tm = tm, yy\n    if yy[-1] == ',':\n        yy = yy[:-1]\n    if not yy[0].isdigit():\n        yy, tz = tz, yy\n    if tm[-1] == ',':\n        tm = tm[:-1]\n    tm = tm.split(':')\n    if len(tm) == 2:\n        [thh, tmm] = tm\n        tss = '0'\n    elif len(tm) == 3:\n        [thh, tmm, tss] = tm\n    elif len(tm) == 1 and '.' in tm[0]:\n        # Some non-compliant MUAs use '.' to separate time elements.\n        tm = tm[0].split('.')\n        if len(tm) == 2:\n            [thh, tmm] = tm\n            tss = 0\n        elif len(tm) == 3:\n            [thh, tmm, tss] = tm\n    else:\n        return None\n    try:\n        yy = int(yy)\n        dd = int(dd)\n        thh = int(thh)\n        tmm = int(tmm)\n        tss = int(tss)\n    except ValueError:\n        return None\n    # Check for a yy specified in two-digit format, then convert it to the\n    # appropriate four-digit format, according to the POSIX standard. RFC 822\n    # calls for a two-digit yy, but RFC 2822 (which obsoletes RFC 822)\n    # mandates a 4-digit yy. For more information, see the documentation for\n    # the time module.\n    if yy < 100:\n        # The year is between 1969 and 1999 (inclusive).\n        if yy > 68:\n            yy += 1900\n        # The year is between 2000 and 2068 (inclusive).\n        else:\n            yy += 2000\n    tzoffset = None\n    tz = tz.upper()\n    if tz in _timezones:\n        tzoffset = _timezones[tz]\n    else:\n        try:\n            tzoffset = int(tz)\n        except ValueError:\n            pass\n        if tzoffset==0 and tz.startswith('-'):\n            tzoffset = None\n    # Convert a timezone offset into seconds ; -0500 -> -18000\n    if tzoffset:\n        if tzoffset < 0:\n            tzsign = -1\n            tzoffset = -tzoffset\n        else:\n            tzsign = 1\n        tzoffset = tzsign * ( (tzoffset//100)*3600 + (tzoffset % 100)*60)\n    # Daylight Saving Time flag is set to -1, since DST is unknown.\n    return [yy, mm, dd, thh, tmm, tss, 0, 1, -1, tzoffset]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a time string to a time tuple.", "response": "def parsedate(data):\n    \"\"\"Convert a time string to a time tuple.\"\"\"\n    t = parsedate_tz(data)\n    if isinstance(t, tuple):\n        return t[:9]\n    else:\n        return t"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nturns a 10 - tuple as returned by parsedate_tz() into a POSIX timestamp.", "response": "def mktime_tz(data):\n    \"\"\"Turn a 10-tuple as returned by parsedate_tz() into a POSIX timestamp.\"\"\"\n    if data[9] is None:\n        # No zone info, so localtime is better assumption than GMT\n        return time.mktime(data[:8] + (-1,))\n    else:\n        t = calendar.timegm(data)\n        return t - data[9]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nskipping white space and extract comments.", "response": "def gotonext(self):\n        \"\"\"Skip white space and extract comments.\"\"\"\n        wslist = []\n        while self.pos < len(self.field):\n            if self.field[self.pos] in self.LWS + '\\n\\r':\n                if self.field[self.pos] not in '\\n\\r':\n                    wslist.append(self.field[self.pos])\n                self.pos += 1\n            elif self.field[self.pos] == '(':\n                self.commentlist.append(self.getcomment())\n            else:\n                break\n        return EMPTYSTRING.join(wslist)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses all addresses. Returns a list containing all of the addresses.", "response": "def getaddrlist(self):\n        \"\"\"Parse all addresses.\n\n        Returns a list containing all of the addresses.\n        \"\"\"\n        result = []\n        while self.pos < len(self.field):\n            ad = self.getaddress()\n            if ad:\n                result += ad\n            else:\n                result.append(('', ''))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the next address.", "response": "def getaddress(self):\n        \"\"\"Parse the next address.\"\"\"\n        self.commentlist = []\n        self.gotonext()\n\n        oldpos = self.pos\n        oldcl = self.commentlist\n        plist = self.getphraselist()\n\n        self.gotonext()\n        returnlist = []\n\n        if self.pos >= len(self.field):\n            # Bad email address technically, no domain.\n            if plist:\n                returnlist = [(SPACE.join(self.commentlist), plist[0])]\n\n        elif self.field[self.pos] in '.@':\n            # email address is just an addrspec\n            # this isn't very efficient since we start over\n            self.pos = oldpos\n            self.commentlist = oldcl\n            addrspec = self.getaddrspec()\n            returnlist = [(SPACE.join(self.commentlist), addrspec)]\n\n        elif self.field[self.pos] == ':':\n            # address is a group\n            returnlist = []\n\n            fieldlen = len(self.field)\n            self.pos += 1\n            while self.pos < len(self.field):\n                self.gotonext()\n                if self.pos < fieldlen and self.field[self.pos] == ';':\n                    self.pos += 1\n                    break\n                returnlist = returnlist + self.getaddress()\n\n        elif self.field[self.pos] == '<':\n            # Address is a phrase then a route addr\n            routeaddr = self.getrouteaddr()\n\n            if self.commentlist:\n                returnlist = [(SPACE.join(plist) + ' (' +\n                               ' '.join(self.commentlist) + ')', routeaddr)]\n            else:\n                returnlist = [(SPACE.join(plist), routeaddr)]\n\n        else:\n            if plist:\n                returnlist = [(SPACE.join(self.commentlist), plist[0])]\n            elif self.field[self.pos] in self.specials:\n                self.pos += 1\n\n        self.gotonext()\n        if self.pos < len(self.field) and self.field[self.pos] == ',':\n            self.pos += 1\n        return returnlist"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getaddrspec(self):\n        aslist = []\n\n        self.gotonext()\n        while self.pos < len(self.field):\n            preserve_ws = True\n            if self.field[self.pos] == '.':\n                if aslist and not aslist[-1].strip():\n                    aslist.pop()\n                aslist.append('.')\n                self.pos += 1\n                preserve_ws = False\n            elif self.field[self.pos] == '\"':\n                aslist.append('\"%s\"' % quote(self.getquote()))\n            elif self.field[self.pos] in self.atomends:\n                if aslist and not aslist[-1].strip():\n                    aslist.pop()\n                break\n            else:\n                aslist.append(self.getatom())\n            ws = self.gotonext()\n            if preserve_ws and ws:\n                aslist.append(ws)\n\n        if self.pos >= len(self.field) or self.field[self.pos] != '@':\n            return EMPTYSTRING.join(aslist)\n\n        aslist.append('@')\n        self.pos += 1\n        self.gotonext()\n        return EMPTYSTRING.join(aslist) + self.getdomain()", "response": "Parse an RFC 2822 addr - spec."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getatom(self, atomends=None):\n        atomlist = ['']\n        if atomends is None:\n            atomends = self.atomends\n\n        while self.pos < len(self.field):\n            if self.field[self.pos] in atomends:\n                break\n            else:\n                atomlist.append(self.field[self.pos])\n            self.pos += 1\n\n        return EMPTYSTRING.join(atomlist)", "response": "Parse an RFC 2822 atom."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decode_header(header):\n    # If it is a Header object, we can just return the encoded chunks.\n    if hasattr(header, '_chunks'):\n        return [(_charset._encode(string, str(charset)), str(charset))\n                    for string, charset in header._chunks]\n    # If no encoding, just return the header with no charset.\n    if not ecre.search(header):\n        return [(header, None)]\n    # First step is to parse all the encoded parts into triplets of the form\n    # (encoded_string, encoding, charset).  For unencoded strings, the last\n    # two parts will be None.\n    words = []\n    for line in header.splitlines():\n        parts = ecre.split(line)\n        first = True\n        while parts:\n            unencoded = parts.pop(0)\n            if first:\n                unencoded = unencoded.lstrip()\n                first = False\n            if unencoded:\n                words.append((unencoded, None, None))\n            if parts:\n                charset = parts.pop(0).lower()\n                encoding = parts.pop(0).lower()\n                encoded = parts.pop(0)\n                words.append((encoded, encoding, charset))\n    # Now loop over words and remove words that consist of whitespace\n    # between two encoded strings.\n    import sys\n    droplist = []\n    for n, w in enumerate(words):\n        if n>1 and w[1] and words[n-2][1] and words[n-1][0].isspace():\n            droplist.append(n-1)\n    for d in reversed(droplist):\n        del words[d]\n\n    # The next step is to decode each encoded word by applying the reverse\n    # base64 or quopri transformation.  decoded_words is now a list of the\n    # form (decoded_word, charset).\n    decoded_words = []\n    for encoded_string, encoding, charset in words:\n        if encoding is None:\n            # This is an unencoded word.\n            decoded_words.append((encoded_string, charset))\n        elif encoding == 'q':\n            word = header_decode(encoded_string)\n            decoded_words.append((word, charset))\n        elif encoding == 'b':\n            paderr = len(encoded_string) % 4   # Postel's law: add missing padding\n            if paderr:\n                encoded_string += '==='[:4 - paderr]\n            try:\n                word = base64mime.decode(encoded_string)\n            except binascii.Error:\n                raise HeaderParseError('Base64 decoding error')\n            else:\n                decoded_words.append((word, charset))\n        else:\n            raise AssertionError('Unexpected encoding: ' + encoding)\n    # Now convert all words to bytes and collapse consecutive runs of\n    # similarly encoded words.\n    collapsed = []\n    last_word = last_charset = None\n    for word, charset in decoded_words:\n        if isinstance(word, str):\n            word = bytes(word, 'raw-unicode-escape')\n        if last_word is None:\n            last_word = word\n            last_charset = charset\n        elif charset != last_charset:\n            collapsed.append((last_word, last_charset))\n            last_word = word\n            last_charset = charset\n        elif last_charset is None:\n            last_word += BSPACE + word\n        else:\n            last_word += word\n    collapsed.append((last_word, last_charset))\n    return collapsed", "response": "Decode a message header value without converting charset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a Header from a sequence of pairs as returned by decode_header", "response": "def make_header(decoded_seq, maxlinelen=None, header_name=None,\n                continuation_ws=' '):\n    \"\"\"Create a Header from a sequence of pairs as returned by decode_header()\n\n    decode_header() takes a header value string and returns a sequence of\n    pairs of the format (decoded_string, charset) where charset is the string\n    name of the character set.\n\n    This function takes one of those sequence of pairs and returns a Header\n    instance.  Optional maxlinelen, header_name, and continuation_ws are as in\n    the Header constructor.\n    \"\"\"\n    h = Header(maxlinelen=maxlinelen, header_name=header_name,\n               continuation_ws=continuation_ws)\n    for s, charset in decoded_seq:\n        # None means us-ascii but we can simply pass it on to h.append()\n        if charset is not None and not isinstance(charset, Charset):\n            charset = Charset(charset)\n        h.append(s, charset)\n    return h"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef append(self, s, charset=None, errors='strict'):\n        if charset is None:\n            charset = self._charset\n        elif not isinstance(charset, Charset):\n            charset = Charset(charset)\n        if not isinstance(s, str):\n            input_charset = charset.input_codec or 'us-ascii'\n            if input_charset == _charset.UNKNOWN8BIT:\n                s = s.decode('us-ascii', 'surrogateescape')\n            else:\n                s = s.decode(input_charset, errors)\n        # Ensure that the bytes we're storing can be decoded to the output\n        # character set, otherwise an early error is raised.\n        output_charset = charset.output_codec or 'us-ascii'\n        if output_charset != _charset.UNKNOWN8BIT:\n            try:\n                s.encode(output_charset, errors)\n            except UnicodeEncodeError:\n                if output_charset!='us-ascii':\n                    raise\n                charset = UTF8\n        self._chunks.append((s, charset))", "response": "Append a string to the MIME header."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef encode(self, splitchars=';, \\t', maxlinelen=None, linesep='\\n'):\n        self._normalize()\n        if maxlinelen is None:\n            maxlinelen = self._maxlinelen\n        # A maxlinelen of 0 means don't wrap.  For all practical purposes,\n        # choosing a huge number here accomplishes that and makes the\n        # _ValueFormatter algorithm much simpler.\n        if maxlinelen == 0:\n            maxlinelen = 1000000\n        formatter = _ValueFormatter(self._headerlen, maxlinelen,\n                                    self._continuation_ws, splitchars)\n        lastcs = None\n        hasspace = lastspace = None\n        for string, charset in self._chunks:\n            if hasspace is not None:\n                hasspace = string and self._nonctext(string[0])\n                import sys\n                if lastcs not in (None, 'us-ascii'):\n                    if not hasspace or charset not in (None, 'us-ascii'):\n                        formatter.add_transition()\n                elif charset not in (None, 'us-ascii') and not lastspace:\n                    formatter.add_transition()\n            lastspace = string and self._nonctext(string[-1])\n            lastcs = charset\n            hasspace = False\n            lines = string.splitlines()\n            if lines:\n                formatter.feed('', lines[0], charset)\n            else:\n                formatter.feed('', '', charset)\n            for line in lines[1:]:\n                formatter.newline()\n                if charset.header_encoding is not None:\n                    formatter.feed(self._continuation_ws, ' ' + line.lstrip(),\n                                   charset)\n                else:\n                    sline = line.lstrip()\n                    fws = line[:len(line)-len(sline)]\n                    formatter.feed(fws, sline, charset)\n            if len(lines) > 1:\n                formatter.newline()\n        if self._chunks:\n            formatter.add_transition()\n        value = formatter._str(linesep)\n        if _embeded_header.search(value):\n            raise HeaderParseError(\"header value appears to contain \"\n                \"an embedded header: {!r}\".format(value))\n        return value", "response": "r Encode a message header into an RFC 2047 compliant format."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlikes a builtin super function but capable of magic.", "response": "def newsuper(typ=_SENTINEL, type_or_obj=_SENTINEL, framedepth=1):\n    '''Like builtin super(), but capable of magic.\n\n    This acts just like the builtin super() function, but if called\n    without any arguments it attempts to infer them at runtime.\n    '''\n    #  Infer the correct call if used without arguments.\n    if typ is _SENTINEL:\n        # We'll need to do some frame hacking.\n        f = sys._getframe(framedepth)\n\n        try:\n            # Get the function's first positional argument.\n            type_or_obj = f.f_locals[f.f_code.co_varnames[0]]\n        except (IndexError, KeyError,):\n            raise RuntimeError('super() used in a function with no args')\n\n        try:\n            # Get the MRO so we can crawl it.\n            mro = type_or_obj.__mro__\n        except (AttributeError, RuntimeError):  # see issue #160\n            try:\n                mro = type_or_obj.__class__.__mro__\n            except AttributeError:\n                raise RuntimeError('super() used with a non-newstyle class')\n\n        #   A ``for...else`` block?  Yes!  It's odd, but useful.\n        #   If unfamiliar with for...else, see:\n        #\n        #       http://psung.blogspot.com/2007/12/for-else-in-python.html\n        for typ in mro:\n            #  Find the class that owns the currently-executing method.\n            for meth in typ.__dict__.values():\n                # Drill down through any wrappers to the underlying func.\n                # This handles e.g. classmethod() and staticmethod().\n                try:\n                    while not isinstance(meth,FunctionType):\n                        if isinstance(meth, property):\n                            # Calling __get__ on the property will invoke\n                            # user code which might throw exceptions or have\n                            # side effects\n                            meth = meth.fget\n                        else:\n                            try:\n                                meth = meth.__func__\n                            except AttributeError:\n                                meth = meth.__get__(type_or_obj, typ)\n                except (AttributeError, TypeError):\n                    continue\n                if meth.func_code is f.f_code:\n                    break   # Aha!  Found you.\n            else:\n                continue    #  Not found! Move onto the next class in MRO.\n            break    #  Found! Break out of the search loop.\n        else:\n            raise RuntimeError('super() called outside a method')\n\n    #  Dispatch to builtin super().\n    if type_or_obj is not _SENTINEL:\n        return _builtin_super(typ, type_or_obj)\n    return _builtin_super(typ)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _formatparam(param, value=None, quote=True):\n    if value is not None and len(value) > 0:\n        # A tuple is used for RFC 2231 encoded parameter values where items\n        # are (charset, language, value).  charset is a string, not a Charset\n        # instance.  RFC 2231 encoded values are never quoted, per RFC.\n        if isinstance(value, tuple):\n            # Encode as per RFC 2231\n            param += '*'\n            value = utils.encode_rfc2231(value[2], value[0], value[1])\n            return '%s=%s' % (param, value)\n        else:\n            try:\n                value.encode('ascii')\n            except UnicodeEncodeError:\n                param += '*'\n                value = utils.encode_rfc2231(value, 'utf-8', '')\n                return '%s=%s' % (param, value)\n        # BAW: Please check this.  I think that if quote is set it should\n        # force quoting even if not necessary.\n        if quote or tspecials.search(value):\n            return '%s=\"%s\"' % (param, utils.quote(value))\n        else:\n            return '%s=%s' % (param, value)\n    else:\n        return param", "response": "Convenience function to format and return a key = value pair."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef as_string(self, unixfrom=False, maxheaderlen=0):\n        from future.backports.email.generator import Generator\n        fp = StringIO()\n        g = Generator(fp, mangle_from_=False, maxheaderlen=maxheaderlen)\n        g.flatten(self, unixfrom=unixfrom)\n        return fp.getvalue()", "response": "Return the entire formatted message as a ( unicode ) string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef attach(self, payload):\n        if self._payload is None:\n            self._payload = [payload]\n        else:\n            self._payload.append(payload)", "response": "Add the given payload to the current payload."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_payload(self, i=None, decode=False):\n        # Here is the logic table for this code, based on the email5.0.0 code:\n        #   i     decode  is_multipart  result\n        # ------  ------  ------------  ------------------------------\n        #  None   True    True          None\n        #   i     True    True          None\n        #  None   False   True          _payload (a list)\n        #   i     False   True          _payload element i (a Message)\n        #   i     False   False         error (not a list)\n        #   i     True    False         error (not a list)\n        #  None   False   False         _payload\n        #  None   True    False         _payload decoded (bytes)\n        # Note that Barry planned to factor out the 'decode' case, but that\n        # isn't so easy now that we handle the 8 bit data, which needs to be\n        # converted in both the decode and non-decode path.\n        if self.is_multipart():\n            if decode:\n                return None\n            if i is None:\n                return self._payload\n            else:\n                return self._payload[i]\n        # For backward compatibility, Use isinstance and this error message\n        # instead of the more logical is_multipart test.\n        if i is not None and not isinstance(self._payload, list):\n            raise TypeError('Expected list, got %s' % type(self._payload))\n        payload = self._payload\n        # cte might be a Header, so for now stringify it.\n        cte = str(self.get('content-transfer-encoding', '')).lower()\n        # payload may be bytes here.\n        if isinstance(payload, str):\n            payload = str(payload)    # for Python-Future, so surrogateescape works\n            if utils._has_surrogates(payload):\n                bpayload = payload.encode('ascii', 'surrogateescape')\n                if not decode:\n                    try:\n                        payload = bpayload.decode(self.get_param('charset', 'ascii'), 'replace')\n                    except LookupError:\n                        payload = bpayload.decode('ascii', 'replace')\n            elif decode:\n                try:\n                    bpayload = payload.encode('ascii')\n                except UnicodeError:\n                    # This won't happen for RFC compliant messages (messages\n                    # containing only ASCII codepoints in the unicode input).\n                    # If it does happen, turn the string into bytes in a way\n                    # guaranteed not to fail.\n                    bpayload = payload.encode('raw-unicode-escape')\n        if not decode:\n            return payload\n        if cte == 'quoted-printable':\n            return utils._qdecode(bpayload)\n        elif cte == 'base64':\n            # XXX: this is a bit of a hack; decode_b should probably be factored\n            # out somewhere, but I haven't figured out where yet.\n            value, defects = decode_b(b''.join(bpayload.splitlines()))\n            for defect in defects:\n                self.policy.handle_defect(self, defect)\n            return value\n        elif cte in ('x-uuencode', 'uuencode', 'uue', 'x-uue'):\n            in_file = BytesIO(bpayload)\n            out_file = BytesIO()\n            try:\n                uu.decode(in_file, out_file, quiet=True)\n                return out_file.getvalue()\n            except uu.Error:\n                # Some decoding problem\n                return bpayload\n        if isinstance(payload, str):\n            return bpayload\n        return payload", "response": "Return a reference to the payload."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_payload(self, payload, charset=None):\n        self._payload = payload\n        if charset is not None:\n            self.set_charset(charset)", "response": "Set the payload of the message."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the charset of the payload to a given character set.", "response": "def set_charset(self, charset):\n        \"\"\"Set the charset of the payload to a given character set.\n\n        charset can be a Charset instance, a string naming a character set, or\n        None.  If it is a string it will be converted to a Charset instance.\n        If charset is None, the charset parameter will be removed from the\n        Content-Type field.  Anything else will generate a TypeError.\n\n        The message will be assumed to be of type text/* encoded with\n        charset.input_charset.  It will be converted to charset.output_charset\n        and encoded properly, if needed, when generating the plain text\n        representation of the message.  MIME headers (MIME-Version,\n        Content-Type, Content-Transfer-Encoding) will be added as needed.\n        \"\"\"\n        if charset is None:\n            self.del_param('charset')\n            self._charset = None\n            return\n        if not isinstance(charset, Charset):\n            charset = Charset(charset)\n        self._charset = charset\n        if 'MIME-Version' not in self:\n            self.add_header('MIME-Version', '1.0')\n        if 'Content-Type' not in self:\n            self.add_header('Content-Type', 'text/plain',\n                            charset=charset.get_output_charset())\n        else:\n            self.set_param('charset', charset.get_output_charset())\n        if charset != charset.get_output_charset():\n            self._payload = charset.body_encode(self._payload)\n        if 'Content-Transfer-Encoding' not in self:\n            cte = charset.get_body_encoding()\n            try:\n                cte(self)\n            except TypeError:\n                self._payload = charset.body_encode(self._payload)\n                self.add_header('Content-Transfer-Encoding', cte)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of all the message s header values.", "response": "def values(self):\n        \"\"\"Return a list of all the message's header values.\n\n        These will be sorted in the order they appeared in the original\n        message, or were added to the message, and may contain duplicates.\n        Any fields deleted and re-inserted are always appended to the header\n        list.\n        \"\"\"\n        return [self.policy.header_fetch_parse(k, v)\n                for k, v in self._headers]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef items(self):\n        return [(k, self.policy.header_fetch_parse(k, v))\n                for k, v in self._headers]", "response": "Get all the message s header fields and values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a header value.", "response": "def get(self, name, failobj=None):\n        \"\"\"Get a header value.\n\n        Like __getitem__() but return failobj instead of None when the field\n        is missing.\n        \"\"\"\n        name = name.lower()\n        for k, v in self._headers:\n            if k.lower() == name:\n                return self.policy.header_fetch_parse(k, v)\n        return failobj"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_all(self, name, failobj=None):\n        values = []\n        name = name.lower()\n        for k, v in self._headers:\n            if k.lower() == name:\n                values.append(self.policy.header_fetch_parse(k, v))\n        if not values:\n            return failobj\n        return values", "response": "Return a list of all the values for the named field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a new header field to the message.", "response": "def add_header(self, _name, _value, **_params):\n        \"\"\"Extended header setting.\n\n        name is the header field to add.  keyword arguments can be used to set\n        additional parameters for the header field, with underscores converted\n        to dashes.  Normally the parameter will be added as key=\"value\" unless\n        value is None, in which case only the key will be added.  If a\n        parameter value contains non-ASCII characters it can be specified as a\n        three-tuple of (charset, language, value), in which case it will be\n        encoded according to RFC2231 rules.  Otherwise it will be encoded using\n        the utf-8 charset and a language of ''.\n\n        Examples:\n\n        msg.add_header('content-disposition', 'attachment', filename='bud.gif')\n        msg.add_header('content-disposition', 'attachment',\n                       filename=('utf-8', '', 'Fu\u00dfballer.ppt'))\n        msg.add_header('content-disposition', 'attachment',\n                       filename='Fu\u00dfballer.ppt'))\n        \"\"\"\n        parts = []\n        for k, v in _params.items():\n            if v is None:\n                parts.append(k.replace('_', '-'))\n            else:\n                parts.append(_formatparam(k.replace('_', '-'), v))\n        if _value is not None:\n            parts.insert(0, _value)\n        self[_name] = SEMISPACE.join(parts)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef replace_header(self, _name, _value):\n        _name = _name.lower()\n        for i, (k, v) in zip(range(len(self._headers)), self._headers):\n            if k.lower() == _name:\n                self._headers[i] = self.policy.header_store_parse(k, _value)\n                break\n        else:\n            raise KeyError(_name)", "response": "Replace a header.\n\n        Replace the first matching header found in the message, retaining\n        header order and case.  If no matching header was found, a KeyError is\n        raised."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_content_type(self):\n        missing = object()\n        value = self.get('content-type', missing)\n        if value is missing:\n            # This should have no parameters\n            return self.get_default_type()\n        ctype = _splitparam(value)[0].lower()\n        # RFC 2045, section 5.2 says if its invalid, use text/plain\n        if ctype.count('/') != 1:\n            return 'text/plain'\n        return ctype", "response": "Return the message s content type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the message s Content - Type parameters as a list.", "response": "def get_params(self, failobj=None, header='content-type', unquote=True):\n        \"\"\"Return the message's Content-Type parameters, as a list.\n\n        The elements of the returned list are 2-tuples of key/value pairs, as\n        split on the `=' sign.  The left hand side of the `=' is the key,\n        while the right hand side is the value.  If there is no `=' sign in\n        the parameter the value is the empty string.  The value is as\n        described in the get_param() method.\n\n        Optional failobj is the object to return if there is no Content-Type\n        header.  Optional header is the header to search instead of\n        Content-Type.  If unquote is True, the value is unquoted.\n        \"\"\"\n        missing = object()\n        params = self._get_params_preserve(missing, header)\n        if params is missing:\n            return failobj\n        if unquote:\n            return [(k, _unquotevalue(v)) for k, v in params]\n        else:\n            return params"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_param(self, param, failobj=None, header='content-type',\n                  unquote=True):\n        \"\"\"Return the parameter value if found in the Content-Type header.\n\n        Optional failobj is the object to return if there is no Content-Type\n        header, or the Content-Type header has no such parameter.  Optional\n        header is the header to search instead of Content-Type.\n\n        Parameter keys are always compared case insensitively.  The return\n        value can either be a string, or a 3-tuple if the parameter was RFC\n        2231 encoded.  When it's a 3-tuple, the elements of the value are of\n        the form (CHARSET, LANGUAGE, VALUE).  Note that both CHARSET and\n        LANGUAGE can be None, in which case you should consider VALUE to be\n        encoded in the us-ascii charset.  You can usually ignore LANGUAGE.\n        The parameter value (either the returned string, or the VALUE item in\n        the 3-tuple) is always unquoted, unless unquote is set to False.\n\n        If your application doesn't care whether the parameter was RFC 2231\n        encoded, it can turn the return value into a string as follows:\n\n            param = msg.get_param('foo')\n            param = email.utils.collapse_rfc2231_value(rawparam)\n\n        \"\"\"\n        if header not in self:\n            return failobj\n        for k, v in self._get_params_preserve(failobj, header):\n            if k.lower() == param.lower():\n                if unquote:\n                    return _unquotevalue(v)\n                else:\n                    return v\n        return failobj", "response": "Return the value of a parameter in the Content - Type header."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_param(self, param, value, header='Content-Type', requote=True,\n                  charset=None, language=''):\n        \"\"\"Set a parameter in the Content-Type header.\n\n        If the parameter already exists in the header, its value will be\n        replaced with the new value.\n\n        If header is Content-Type and has not yet been defined for this\n        message, it will be set to \"text/plain\" and the new parameter and\n        value will be appended as per RFC 2045.\n\n        An alternate header can specified in the header argument, and all\n        parameters will be quoted as necessary unless requote is False.\n\n        If charset is specified, the parameter will be encoded according to RFC\n        2231.  Optional language specifies the RFC 2231 language, defaulting\n        to the empty string.  Both charset and language should be strings.\n        \"\"\"\n        if not isinstance(value, tuple) and charset:\n            value = (charset, language, value)\n\n        if header not in self and header.lower() == 'content-type':\n            ctype = 'text/plain'\n        else:\n            ctype = self.get(header)\n        if not self.get_param(param, header=header):\n            if not ctype:\n                ctype = _formatparam(param, value, requote)\n            else:\n                ctype = SEMISPACE.join(\n                    [ctype, _formatparam(param, value, requote)])\n        else:\n            ctype = ''\n            for old_param, old_value in self.get_params(header=header,\n                                                        unquote=requote):\n                append_param = ''\n                if old_param.lower() == param.lower():\n                    append_param = _formatparam(param, value, requote)\n                else:\n                    append_param = _formatparam(old_param, old_value, requote)\n                if not ctype:\n                    ctype = append_param\n                else:\n                    ctype = SEMISPACE.join([ctype, append_param])\n        if ctype != self.get(header):\n            del self[header]\n            self[header] = ctype", "response": "Set a parameter in the Content - Type header."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef del_param(self, param, header='content-type', requote=True):\n        if header not in self:\n            return\n        new_ctype = ''\n        for p, v in self.get_params(header=header, unquote=requote):\n            if p.lower() != param.lower():\n                if not new_ctype:\n                    new_ctype = _formatparam(p, v, requote)\n                else:\n                    new_ctype = SEMISPACE.join([new_ctype,\n                                                _formatparam(p, v, requote)])\n        if new_ctype != self.get(header):\n            del self[header]\n            self[header] = new_ctype", "response": "Removes the given parameter from the Content - Type header."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_type(self, type, header='Content-Type', requote=True):\n        # BAW: should we be strict?\n        if not type.count('/') == 1:\n            raise ValueError\n        # Set the Content-Type, you get a MIME-Version\n        if header.lower() == 'content-type':\n            del self['mime-version']\n            self['MIME-Version'] = '1.0'\n        if header not in self:\n            self[header] = type\n            return\n        params = self.get_params(header=header, unquote=requote)\n        del self[header]\n        self[header] = type\n        # Skip the first param; it's the old type.\n        for p, v in params[1:]:\n            self.set_param(p, v, header, requote)", "response": "Set the main type and subtype for the Content - Type header."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_filename(self, failobj=None):\n        missing = object()\n        filename = self.get_param('filename', missing, 'content-disposition')\n        if filename is missing:\n            filename = self.get_param('name', missing, 'content-type')\n        if filename is missing:\n            return failobj\n        return utils.collapse_rfc2231_value(filename).strip()", "response": "Return the filename associated with the payload if present."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the boundary associated with the payload if present.", "response": "def get_boundary(self, failobj=None):\n        \"\"\"Return the boundary associated with the payload if present.\n\n        The boundary is extracted from the Content-Type header's `boundary'\n        parameter, and it is unquoted.\n        \"\"\"\n        missing = object()\n        boundary = self.get_param('boundary', missing)\n        if boundary is missing:\n            return failobj\n        # RFC 2046 says that boundaries may begin but not end in w/s\n        return utils.collapse_rfc2231_value(boundary).rstrip()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_boundary(self, boundary):\n        missing = object()\n        params = self._get_params_preserve(missing, 'content-type')\n        if params is missing:\n            # There was no Content-Type header, and we don't know what type\n            # to set it to, so raise an exception.\n            raise errors.HeaderParseError('No Content-Type header found')\n        newparams = list()\n        foundp = False\n        for pk, pv in params:\n            if pk.lower() == 'boundary':\n                newparams.append(('boundary', '\"%s\"' % boundary))\n                foundp = True\n            else:\n                newparams.append((pk, pv))\n        if not foundp:\n            # The original Content-Type header had no boundary attribute.\n            # Tack one on the end.  BAW: should we raise an exception\n            # instead???\n            newparams.append(('boundary', '\"%s\"' % boundary))\n        # Replace the existing Content-Type header with the new value\n        newheaders = list()\n        for h, v in self._headers:\n            if h.lower() == 'content-type':\n                parts = list()\n                for k, v in newparams:\n                    if v == '':\n                        parts.append(k)\n                    else:\n                        parts.append('%s=%s' % (k, v))\n                val = SEMISPACE.join(parts)\n                newheaders.append(self.policy.header_store_parse(h, val))\n\n            else:\n                newheaders.append((h, v))\n        self._headers = newheaders", "response": "Set the boundary parameter in Content - Type to boundary."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the charset parameter of the Content - Type header.", "response": "def get_content_charset(self, failobj=None):\n        \"\"\"Return the charset parameter of the Content-Type header.\n\n        The returned string is always coerced to lower case.  If there is no\n        Content-Type header, or if that header has no charset parameter,\n        failobj is returned.\n        \"\"\"\n        missing = object()\n        charset = self.get_param('charset', missing)\n        if charset is missing:\n            return failobj\n        if isinstance(charset, tuple):\n            # RFC 2231 encoded, so decode it, and it better end up as ascii.\n            pcharset = charset[0] or 'us-ascii'\n            try:\n                # LookupError will be raised if the charset isn't known to\n                # Python.  UnicodeError will be raised if the encoded text\n                # contains a character not in the charset.\n                as_bytes = charset[2].encode('raw-unicode-escape')\n                charset = str(as_bytes, pcharset)\n            except (LookupError, UnicodeError):\n                charset = charset[2]\n        # charset characters must be in us-ascii range\n        try:\n            charset.encode('us-ascii')\n        except UnicodeError:\n            return failobj\n        # RFC 2046, $4.1.2 says charsets are not case sensitive\n        return charset.lower()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new socket object from a file descriptor.", "response": "def fromfd(fd, family, type, proto=0):\n    \"\"\" fromfd(fd, family, type[, proto]) -> socket object\n\n    Create a socket object from a duplicate of the given file\n    descriptor.  The remaining arguments are the same as for socket().\n    \"\"\"\n    nfd = dup(fd)\n    return socket(family, type, proto, nfd)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget fully qualified domain name from name.", "response": "def getfqdn(name=''):\n    \"\"\"Get fully qualified domain name from name.\n\n    An empty argument is interpreted as meaning the local host.\n\n    First the hostname returned by gethostbyaddr() is checked, then\n    possibly existing aliases. In case no FQDN is available, hostname\n    from gethostname() is returned.\n    \"\"\"\n    name = name.strip()\n    if not name or name == '0.0.0.0':\n        name = gethostname()\n    try:\n        hostname, aliases, ipaddrs = gethostbyaddr(name)\n    except error:\n        pass\n    else:\n        aliases.insert(0, hostname)\n        for name in aliases:\n            if '.' in name:\n                break\n        else:\n            name = hostname\n    return name"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dup(self):\n        fd = dup(self.fileno())\n        sock = self.__class__(self.family, self.type, self.proto, fileno=fd)\n        sock.settimeout(self.gettimeout())\n        return sock", "response": "Return a copy of the socket object connected to the same system resource."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef accept(self):\n        fd, addr = self._accept()\n        sock = socket(self.family, self.type, self.proto, fileno=fd)\n        # Issue #7995: if no default timeout is set and the listening\n        # socket had a (non-zero) timeout, force the new socket in blocking\n        # mode to override platform-specific socket flags inheritance.\n        if getdefaulttimeout() is None and self.gettimeout():\n            sock.setblocking(True)\n        return sock, addr", "response": "accept - Returns a new socket object and the address info of the client."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a file - like object connected to the socket.", "response": "def makefile(self, mode=\"r\", buffering=None, **_3to2kwargs):\n        \"\"\"makefile(...) -> an I/O stream connected to the socket\n\n        The arguments are as for io.open() after the filename,\n        except the only mode characters supported are 'r', 'w' and 'b'.\n        The semantics are similar too.  (XXX refactor to share code?)\n        \"\"\"\n        if 'newline' in _3to2kwargs: newline = _3to2kwargs['newline']; del _3to2kwargs['newline']\n        else: newline = None\n        if 'errors' in _3to2kwargs: errors = _3to2kwargs['errors']; del _3to2kwargs['errors']\n        else: errors = None\n        if 'encoding' in _3to2kwargs: encoding = _3to2kwargs['encoding']; del _3to2kwargs['encoding']\n        else: encoding = None\n        for c in mode:\n            if c not in (\"r\", \"w\", \"b\"):\n                raise ValueError(\"invalid mode %r (only r, w, b allowed)\")\n        writing = \"w\" in mode\n        reading = \"r\" in mode or not writing\n        assert reading or writing\n        binary = \"b\" in mode\n        rawmode = \"\"\n        if reading:\n            rawmode += \"r\"\n        if writing:\n            rawmode += \"w\"\n        raw = SocketIO(self, rawmode)\n        self._io_refs += 1\n        if buffering is None:\n            buffering = -1\n        if buffering < 0:\n            buffering = io.DEFAULT_BUFFER_SIZE\n        if buffering == 0:\n            if not binary:\n                raise ValueError(\"unbuffered streams must be binary\")\n            return raw\n        if reading and writing:\n            buffer = io.BufferedRWPair(raw, raw, buffering)\n        elif reading:\n            buffer = io.BufferedReader(raw, buffering)\n        else:\n            assert writing\n            buffer = io.BufferedWriter(raw, buffering)\n        if binary:\n            return buffer\n        text = io.TextIOWrapper(buffer, encoding, errors, newline)\n        text.mode = mode\n        return text"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread up to len ( b ) bytes into the writable buffer b.", "response": "def readinto(self, b):\n        \"\"\"Read up to len(b) bytes into the writable buffer *b* and return\n        the number of bytes read.  If the socket is non-blocking and no bytes\n        are available, None is returned.\n\n        If *b* is non-empty, a 0 return value indicates that the connection\n        was shutdown at the other end.\n        \"\"\"\n        self._checkClosed()\n        self._checkReadable()\n        if self._timeout_occurred:\n            raise IOError(\"cannot read from timed out object\")\n        while True:\n            try:\n                return self._sock.recv_into(b)\n            except timeout:\n                self._timeout_occurred = True\n                raise\n            # except InterruptedError:\n            #     continue\n            except error as e:\n                if e.args[0] in _blocking_errnos:\n                    return None\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef maybe_2to3(filename, modname=None):\n    need_2to3 = False\n    filename = os.path.abspath(filename)\n    if any(filename.startswith(d) for d in DIRS):\n        need_2to3 = True\n    elif modname is not None and any(modname.startswith(p) for p in PACKAGES):\n        need_2to3 = True\n    if not need_2to3:\n        return filename\n    outfilename = '/_auto2to3_'.join(os.path.split(filename))\n    if (not os.path.exists(outfilename) or\n        os.stat(filename).st_mtime > os.stat(outfilename).st_mtime):\n        try:\n            with open(filename) as file:\n                contents = file.read()\n            contents = rt.refactor_docstring(contents, filename)\n            tree = rt.refactor_string(contents, filename)\n        except Exception as err:\n            raise ImportError(\"2to3 couldn't convert %r\" % filename)\n        outfile = open(outfilename, 'wb')\n        outfile.write(str(tree).encode('utf8'))\n        outfile.close()\n    return outfilename", "response": "Returns a python3 version of filename."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwalks over the message tree yielding each subpart.", "response": "def walk(self):\n    \"\"\"Walk over the message tree, yielding each subpart.\n\n    The walk is performed in depth-first order.  This method is a\n    generator.\n    \"\"\"\n    yield self\n    if self.is_multipart():\n        for subpart in self.get_payload():\n            for subsubpart in subpart.walk():\n                yield subsubpart"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\niterate over the parts returning string payloads line - by - line.", "response": "def body_line_iterator(msg, decode=False):\n    \"\"\"Iterate over the parts, returning string payloads line-by-line.\n\n    Optional decode (default False) is passed through to .get_payload().\n    \"\"\"\n    for subpart in msg.walk():\n        payload = subpart.get_payload(decode=decode)\n        if isinstance(payload, str):\n            for line in StringIO(payload):\n                yield line"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\niterate over the subparts with a given MIME type.", "response": "def typed_subpart_iterator(msg, maintype='text', subtype=None):\n    \"\"\"Iterate over the subparts with a given MIME type.\n\n    Use `maintype' as the main MIME type to match against; this defaults to\n    \"text\".  Optional `subtype' is the MIME subtype to match against; if\n    omitted, only the main type is matched.\n    \"\"\"\n    for subpart in msg.walk():\n        if subpart.get_content_maintype() == maintype:\n            if subtype is None or subpart.get_content_subtype() == subtype:\n                yield subpart"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _structure(msg, fp=None, level=0, include_default=False):\n    if fp is None:\n        fp = sys.stdout\n    tab = ' ' * (level * 4)\n    print(tab + msg.get_content_type(), end='', file=fp)\n    if include_default:\n        print(' [%s]' % msg.get_default_type(), file=fp)\n    else:\n        print(file=fp)\n    if msg.is_multipart():\n        for subpart in msg.get_payload():\n            _structure(subpart, fp, level+1, include_default)", "response": "A handy debugging aid"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef all_modules_subpattern():\n    names_dot_attrs = [mod.split(u\".\") for mod in MAPPING]\n    ret = u\"( \" + u\" | \".join([dotted_name % (simple_name % (mod[0]),\n                                            simple_attr % (mod[1])) for mod in names_dot_attrs])\n    ret += u\" | \"\n    ret += u\" | \".join([simple_name % (mod[0]) for mod in names_dot_attrs if mod[1] == u\"__init__\"]) + u\" )\"\n    return ret", "response": "u Builds a pattern for all top - level modules."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_charset(charset, header_enc=None, body_enc=None, output_charset=None):\n    if body_enc == SHORTEST:\n        raise ValueError('SHORTEST not allowed for body_enc')\n    CHARSETS[charset] = (header_enc, body_enc, output_charset)", "response": "Add character set properties to the global registry."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_body_encoding(self):\n        assert self.body_encoding != SHORTEST\n        if self.body_encoding == QP:\n            return 'quoted-printable'\n        elif self.body_encoding == BASE64:\n            return 'base64'\n        else:\n            return encode_7or8bit", "response": "Return the content - transfer - encoding used for the message body."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef header_encode(self, string):\n        codec = self.output_codec or 'us-ascii'\n        header_bytes = _encode(string, codec)\n        # 7bit/8bit encodings return the string unchanged (modulo conversions)\n        encoder_module = self._get_encoder(header_bytes)\n        if encoder_module is None:\n            return string\n        return encoder_module.header_encode(header_bytes, codec)", "response": "Encode a unicode string to bytes using the character set s header_encoding."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef header_encode_lines(self, string, maxlengths):\n        # See which encoding we should use.\n        codec = self.output_codec or 'us-ascii'\n        header_bytes = _encode(string, codec)\n        encoder_module = self._get_encoder(header_bytes)\n        encoder = partial(encoder_module.header_encode, charset=codec)\n        # Calculate the number of characters that the RFC 2047 chrome will\n        # contribute to each line.\n        charset = self.get_output_charset()\n        extra = len(charset) + RFC2047_CHROME_LEN\n        # Now comes the hard part.  We must encode bytes but we can't split on\n        # bytes because some character sets are variable length and each\n        # encoded word must stand on its own.  So the problem is you have to\n        # encode to bytes to figure out this word's length, but you must split\n        # on characters.  This causes two problems: first, we don't know how\n        # many octets a specific substring of unicode characters will get\n        # encoded to, and second, we don't know how many ASCII characters\n        # those octets will get encoded to.  Unless we try it.  Which seems\n        # inefficient.  In the interest of being correct rather than fast (and\n        # in the hope that there will be few encoded headers in any such\n        # message), brute force it. :(\n        lines = []\n        current_line = []\n        maxlen = next(maxlengths) - extra\n        for character in string:\n            current_line.append(character)\n            this_line = EMPTYSTRING.join(current_line)\n            length = encoder_module.header_length(_encode(this_line, charset))\n            if length > maxlen:\n                # This last character doesn't fit so pop it off.\n                current_line.pop()\n                # Does nothing fit on the first line?\n                if not lines and not current_line:\n                    lines.append(None)\n                else:\n                    separator = (' ' if lines else '')\n                    joined_line = EMPTYSTRING.join(current_line)\n                    header_bytes = _encode(joined_line, codec)\n                    lines.append(encoder(header_bytes))\n                current_line = [character]\n                maxlen = next(maxlengths) - extra\n        joined_line = EMPTYSTRING.join(current_line)\n        header_bytes = _encode(joined_line, codec)\n        lines.append(encoder(header_bytes))\n        return lines", "response": "This function encodes a unicode string into a list of header lines."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef body_encode(self, string):\n        if not string:\n            return string\n        if self.body_encoding is BASE64:\n            if isinstance(string, str):\n                string = string.encode(self.output_charset)\n            return email.base64mime.body_encode(string)\n        elif self.body_encoding is QP:\n            # quopromime.body_encode takes a string, but operates on it as if\n            # it were a list of byte codes.  For a (minimal) history on why\n            # this is so, see changeset 0cf700464177.  To correctly encode a\n            # character set, then, we must turn it into pseudo bytes via the\n            # latin1 charset, which will encode any byte as a single code point\n            # between 0 and 255, which is what body_encode is expecting.\n            if isinstance(string, str):\n                string = string.encode(self.output_charset)\n            string = string.decode('latin1')\n            return email.quoprimime.body_encode(string)\n        else:\n            if isinstance(string, str):\n                string = string.encode(self.output_charset).decode('ascii')\n            return string", "response": "Encode a string using the appropriate encoding."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nresolve a dotted attribute name to an object.", "response": "def resolve_dotted_attribute(obj, attr, allow_dotted_names=True):\n    \"\"\"resolve_dotted_attribute(a, 'b.c.d') => a.b.c.d\n\n    Resolves a dotted attribute name to an object.  Raises\n    an AttributeError if any attribute in the chain starts with a '_'.\n\n    If the optional allow_dotted_names argument is false, dots are not\n    supported and this function operates similar to getattr(obj, attr).\n    \"\"\"\n\n    if allow_dotted_names:\n        attrs = attr.split('.')\n    else:\n        attrs = [attr]\n\n    for i in attrs:\n        if i.startswith('_'):\n            raise AttributeError(\n                'attempt to access private attribute \"%s\"' % i\n                )\n        else:\n            obj = getattr(obj,i)\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nregistering an instance to respond to XML - RPC requests.", "response": "def register_instance(self, instance, allow_dotted_names=False):\n        \"\"\"Registers an instance to respond to XML-RPC requests.\n\n        Only one instance can be installed at a time.\n\n        If the registered instance has a _dispatch method then that\n        method will be called with the name of the XML-RPC method and\n        its parameters as a tuple\n        e.g. instance._dispatch('add',(2,3))\n\n        If the registered instance does not have a _dispatch method\n        then the instance will be searched to find a matching method\n        and, if found, will be called. Methods beginning with an '_'\n        are considered private and will not be called by\n        SimpleXMLRPCServer.\n\n        If a registered function matches a XML-RPC request, then it\n        will be called instead of the registered instance.\n\n        If the optional allow_dotted_names argument is true and the\n        instance does not have a _dispatch method, method names\n        containing dots are supported and resolved, as long as none of\n        the name segments start with an '_'.\n\n            *** SECURITY WARNING: ***\n\n            Enabling the allow_dotted_names options allows intruders\n            to access your module's global variables and may allow\n            intruders to execute arbitrary code on your machine.  Only\n            use this option on a secure, closed network.\n\n        \"\"\"\n\n        self.instance = instance\n        self.allow_dotted_names = allow_dotted_names"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nregistering the XML - RPC introspection methods in the system namespace.", "response": "def register_introspection_functions(self):\n        \"\"\"Registers the XML-RPC introspection methods in the system\n        namespace.\n\n        see http://xmlrpc.usefulinc.com/doc/reserved.html\n        \"\"\"\n\n        self.funcs.update({'system.listMethods' : self.system_listMethods,\n                      'system.methodSignature' : self.system_methodSignature,\n                      'system.methodHelp' : self.system_methodHelp})"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _marshaled_dispatch(self, data, dispatch_method = None, path = None):\n\n        try:\n            params, method = loads(data, use_builtin_types=self.use_builtin_types)\n\n            # generate response\n            if dispatch_method is not None:\n                response = dispatch_method(method, params)\n            else:\n                response = self._dispatch(method, params)\n            # wrap response in a singleton tuple\n            response = (response,)\n            response = dumps(response, methodresponse=1,\n                             allow_none=self.allow_none, encoding=self.encoding)\n        except Fault as fault:\n            response = dumps(fault, allow_none=self.allow_none,\n                             encoding=self.encoding)\n        except:\n            # report exception back to server\n            exc_type, exc_value, exc_tb = sys.exc_info()\n            response = dumps(\n                Fault(1, \"%s:%s\" % (exc_type, exc_value)),\n                encoding=self.encoding, allow_none=self.allow_none,\n                )\n\n        return response.encode(self.encoding)", "response": "Dispatches an XML - RPC method from the marshalled data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef system_listMethods(self):\n\n        methods = set(self.funcs.keys())\n        if self.instance is not None:\n            # Instance can implement _listMethod to return a list of\n            # methods\n            if hasattr(self.instance, '_listMethods'):\n                methods |= set(self.instance._listMethods())\n            # if the instance has a _dispatch method then we\n            # don't have enough information to provide a list\n            # of methods\n            elif not hasattr(self.instance, '_dispatch'):\n                methods |= set(list_public_methods(self.instance))\n        return sorted(methods)", "response": "System. listMethods - Returns a list of the methods supported by the server."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef system_methodHelp(self, method_name):\n\n        method = None\n        if method_name in self.funcs:\n            method = self.funcs[method_name]\n        elif self.instance is not None:\n            # Instance can implement _methodHelp to return help for a method\n            if hasattr(self.instance, '_methodHelp'):\n                return self.instance._methodHelp(method_name)\n            # if the instance has a _dispatch method then we\n            # don't have enough information to provide help\n            elif not hasattr(self.instance, '_dispatch'):\n                try:\n                    method = resolve_dotted_attribute(\n                                self.instance,\n                                method_name,\n                                self.allow_dotted_names\n                                )\n                except AttributeError:\n                    pass\n\n        # Note that we aren't checking that the method actually\n        # be a callable object of some kind\n        if method is None:\n            return \"\"\n        else:\n            return pydoc.getdoc(method)", "response": "system. methodHelp ( add ) => Adds two integers together Returns a string containing documentation for the specified method."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndispatching the XML - RPC method.", "response": "def _dispatch(self, method, params):\n        \"\"\"Dispatches the XML-RPC method.\n\n        XML-RPC calls are forwarded to a registered function that\n        matches the called XML-RPC method name. If no such function\n        exists then the call is forwarded to the registered instance,\n        if available.\n\n        If the registered instance has a _dispatch method then that\n        method will be called with the name of the XML-RPC method and\n        its parameters as a tuple\n        e.g. instance._dispatch('add',(2,3))\n\n        If the registered instance does not have a _dispatch method\n        then the instance will be searched to find a matching method\n        and, if found, will be called.\n\n        Methods beginning with an '_' are considered private and will\n        not be called.\n        \"\"\"\n\n        func = None\n        try:\n            # check to see if a matching function has been registered\n            func = self.funcs[method]\n        except KeyError:\n            if self.instance is not None:\n                # check for a _dispatch method\n                if hasattr(self.instance, '_dispatch'):\n                    return self.instance._dispatch(method, params)\n                else:\n                    # call instance method directly\n                    try:\n                        func = resolve_dotted_attribute(\n                            self.instance,\n                            method,\n                            self.allow_dotted_names\n                            )\n                    except AttributeError:\n                        pass\n\n        if func is not None:\n            return func(*params)\n        else:\n            raise Exception('method \"%s\" is not supported' % method)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling the HTTP POST request.", "response": "def do_POST(self):\n        \"\"\"Handles the HTTP POST request.\n\n        Attempts to interpret all HTTP POST requests as XML-RPC calls,\n        which are forwarded to the server's _dispatch method for handling.\n        \"\"\"\n\n        # Check that the path is legal\n        if not self.is_rpc_path_valid():\n            self.report_404()\n            return\n\n        try:\n            # Get arguments by reading body of request.\n            # We read this in chunks to avoid straining\n            # socket.read(); around the 10 or 15Mb mark, some platforms\n            # begin to have problems (bug #792570).\n            max_chunk_size = 10*1024*1024\n            size_remaining = int(self.headers[\"content-length\"])\n            L = []\n            while size_remaining:\n                chunk_size = min(size_remaining, max_chunk_size)\n                chunk = self.rfile.read(chunk_size)\n                if not chunk:\n                    break\n                L.append(chunk)\n                size_remaining -= len(L[-1])\n            data = b''.join(L)\n\n            data = self.decode_request_content(data)\n            if data is None:\n                return #response has been sent\n\n            # In previous versions of SimpleXMLRPCServer, _dispatch\n            # could be overridden in this class, instead of in\n            # SimpleXMLRPCDispatcher. To maintain backwards compatibility,\n            # check to see if a subclass implements _dispatch and dispatch\n            # using that method if present.\n            response = self.server._marshaled_dispatch(\n                    data, getattr(self, '_dispatch', None), self.path\n                )\n        except Exception as e: # This should only happen if the module is buggy\n            # internal error, report as HTTP server error\n            self.send_response(500)\n\n            # Send information about the exception if requested\n            if hasattr(self.server, '_send_traceback_header') and \\\n                    self.server._send_traceback_header:\n                self.send_header(\"X-exception\", str(e))\n                trace = traceback.format_exc()\n                trace = str(trace.encode('ASCII', 'backslashreplace'), 'ASCII')\n                self.send_header(\"X-traceback\", trace)\n\n            self.send_header(\"Content-length\", \"0\")\n            self.end_headers()\n        else:\n            self.send_response(200)\n            self.send_header(\"Content-type\", \"text/xml\")\n            if self.encode_threshold is not None:\n                if len(response) > self.encode_threshold:\n                    q = self.accept_encodings().get(\"gzip\", 0)\n                    if q:\n                        try:\n                            response = gzip_encode(response)\n                            self.send_header(\"Content-Encoding\", \"gzip\")\n                        except NotImplementedError:\n                            pass\n            self.send_header(\"Content-length\", str(len(response)))\n            self.end_headers()\n            self.wfile.write(response)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle a single XML - RPC request.", "response": "def handle_xmlrpc(self, request_text):\n        \"\"\"Handle a single XML-RPC request\"\"\"\n\n        response = self._marshaled_dispatch(request_text)\n\n        print('Content-Type: text/xml')\n        print('Content-Length: %d' % len(response))\n        print()\n        sys.stdout.flush()\n        sys.stdout.buffer.write(response)\n        sys.stdout.buffer.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles a single HTTP GET request.", "response": "def handle_get(self):\n        \"\"\"Handle a single HTTP GET request.\n\n        Default implementation indicates an error because\n        XML-RPC uses the POST method.\n        \"\"\"\n\n        code = 400\n        message, explain = BaseHTTPRequestHandler.responses[code]\n\n        response = http_server.DEFAULT_ERROR_MESSAGE % \\\n            {\n             'code' : code,\n             'message' : message,\n             'explain' : explain\n            }\n        response = response.encode('utf-8')\n        print('Status: %d %s' % (code, message))\n        print('Content-Type: %s' % http_server.DEFAULT_ERROR_CONTENT_TYPE)\n        print('Content-Length: %d' % len(response))\n        print()\n        sys.stdout.flush()\n        sys.stdout.buffer.write(response)\n        sys.stdout.buffer.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle a single XML - RPC request.", "response": "def handle_request(self, request_text=None):\n        \"\"\"Handle a single XML-RPC request passed through a CGI post method.\n\n        If no XML data is given then it is read from stdin. The resulting\n        XML-RPC response is printed to stdout along with the correct HTTP\n        headers.\n        \"\"\"\n\n        if request_text is None and \\\n            os.environ.get('REQUEST_METHOD', None) == 'GET':\n            self.handle_get()\n        else:\n            # POST data is normally available through stdin\n            try:\n                length = int(os.environ.get('CONTENT_LENGTH', None))\n            except (ValueError, TypeError):\n                length = -1\n            if request_text is None:\n                request_text = sys.stdin.read(length)\n\n            self.handle_xmlrpc(request_text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef markup(self, text, escape=None, funcs={}, classes={}, methods={}):\n        escape = escape or self.escape\n        results = []\n        here = 0\n\n        # XXX Note that this regular expression does not allow for the\n        # hyperlinking of arbitrary strings being used as method\n        # names. Only methods with names consisting of word characters\n        # and '.'s are hyperlinked.\n        pattern = re.compile(r'\\b((http|ftp)://\\S+[\\w/]|'\n                                r'RFC[- ]?(\\d+)|'\n                                r'PEP[- ]?(\\d+)|'\n                                r'(self\\.)?((?:\\w|\\.)+))\\b')\n        while 1:\n            match = pattern.search(text, here)\n            if not match: break\n            start, end = match.span()\n            results.append(escape(text[here:start]))\n\n            all, scheme, rfc, pep, selfdot, name = match.groups()\n            if scheme:\n                url = escape(all).replace('\"', '&quot;')\n                results.append('<a href=\"%s\">%s</a>' % (url, url))\n            elif rfc:\n                url = 'http://www.rfc-editor.org/rfc/rfc%d.txt' % int(rfc)\n                results.append('<a href=\"%s\">%s</a>' % (url, escape(all)))\n            elif pep:\n                url = 'http://www.python.org/dev/peps/pep-%04d/' % int(pep)\n                results.append('<a href=\"%s\">%s</a>' % (url, escape(all)))\n            elif text[end:end+1] == '(':\n                results.append(self.namelink(name, methods, funcs, classes))\n            elif selfdot:\n                results.append('self.<strong>%s</strong>' % name)\n            else:\n                results.append(self.namelink(name, classes))\n            here = end\n        results.append(escape(text[here:]))\n        return ''.join(results)", "response": "Mark up some plain text given a context of symbols to look for."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nproduce HTML documentation for a function or method object.", "response": "def docroutine(self, object, name, mod=None,\n                   funcs={}, classes={}, methods={}, cl=None):\n        \"\"\"Produce HTML documentation for a function or method object.\"\"\"\n\n        anchor = (cl and cl.__name__ or '') + '-' + name\n        note = ''\n\n        title = '<a name=\"%s\"><strong>%s</strong></a>' % (\n            self.escape(anchor), self.escape(name))\n\n        if inspect.ismethod(object):\n            args = inspect.getfullargspec(object)\n            # exclude the argument bound to the instance, it will be\n            # confusing to the non-Python user\n            argspec = inspect.formatargspec (\n                    args.args[1:],\n                    args.varargs,\n                    args.varkw,\n                    args.defaults,\n                    annotations=args.annotations,\n                    formatvalue=self.formatvalue\n                )\n        elif inspect.isfunction(object):\n            args = inspect.getfullargspec(object)\n            argspec = inspect.formatargspec(\n                args.args, args.varargs, args.varkw, args.defaults,\n                annotations=args.annotations,\n                formatvalue=self.formatvalue)\n        else:\n            argspec = '(...)'\n\n        if isinstance(object, tuple):\n            argspec = object[0] or argspec\n            docstring = object[1] or \"\"\n        else:\n            docstring = pydoc.getdoc(object)\n\n        decl = title + argspec + (note and self.grey(\n               '<font face=\"helvetica, arial\">%s</font>' % note))\n\n        doc = self.markup(\n            docstring, self.preformat, funcs, classes, methods)\n        doc = doc and '<dd><tt>%s</tt></dd>' % doc\n        return '<dl><dt>%s</dt>%s</dl>\\n' % (decl, doc)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nproduces HTML documentation for an XML - RPC server.", "response": "def docserver(self, server_name, package_documentation, methods):\n        \"\"\"Produce HTML documentation for an XML-RPC server.\"\"\"\n\n        fdict = {}\n        for key, value in methods.items():\n            fdict[key] = '#-' + key\n            fdict[value] = fdict[key]\n\n        server_name = self.escape(server_name)\n        head = '<big><big><strong>%s</strong></big></big>' % server_name\n        result = self.heading(head, '#ffffff', '#7799ee')\n\n        doc = self.markup(package_documentation, self.preformat, fdict)\n        doc = doc and '<tt>%s</tt>' % doc\n        result = result + '<p>%s</p>\\n' % doc\n\n        contents = []\n        method_items = sorted(methods.items())\n        for key, value in method_items:\n            contents.append(self.docroutine(value, key, funcs=fdict))\n        result = result + self.bigsection(\n            'Methods', '#ffffff', '#eeaa77', ''.join(contents))\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates HTML documentation for the server using introspection for the available functions and instances.", "response": "def generate_html_documentation(self):\n        \"\"\"generate_html_documentation() => html documentation for the server\n\n        Generates HTML documentation for the server using introspection for\n        installed functions and instances that do not implement the\n        _dispatch method. Alternatively, instances can choose to implement\n        the _get_method_argstring(method_name) method to provide the\n        argument string used in the documentation and the\n        _methodHelp(method_name) method to provide the help text used\n        in the documentation.\"\"\"\n\n        methods = {}\n\n        for method_name in self.system_listMethods():\n            if method_name in self.funcs:\n                method = self.funcs[method_name]\n            elif self.instance is not None:\n                method_info = [None, None] # argspec, documentation\n                if hasattr(self.instance, '_get_method_argstring'):\n                    method_info[0] = self.instance._get_method_argstring(method_name)\n                if hasattr(self.instance, '_methodHelp'):\n                    method_info[1] = self.instance._methodHelp(method_name)\n\n                method_info = tuple(method_info)\n                if method_info != (None, None):\n                    method = method_info\n                elif not hasattr(self.instance, '_dispatch'):\n                    try:\n                        method = resolve_dotted_attribute(\n                                    self.instance,\n                                    method_name\n                                    )\n                    except AttributeError:\n                        method = method_info\n                else:\n                    method = method_info\n            else:\n                assert 0, \"Could not find method in self.functions and no \"\\\n                          \"instance installed\"\n\n            methods[method_name] = method\n\n        documenter = ServerHTMLDoc()\n        documentation = documenter.docserver(\n                                self.server_name,\n                                self.server_documentation,\n                                methods\n                            )\n\n        return documenter.page(self.server_title, documentation)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_GET(self):\n        # Check that the path is legal\n        if not self.is_rpc_path_valid():\n            self.report_404()\n            return\n\n        response = self.server.generate_html_documentation().encode('utf-8')\n        self.send_response(200)\n        self.send_header(\"Content-type\", \"text/html\")\n        self.send_header(\"Content-length\", str(len(response)))\n        self.end_headers()\n        self.wfile.write(response)", "response": "Handles the HTTP GET request."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_get(self):\n\n        response = self.generate_html_documentation().encode('utf-8')\n\n        print('Content-Type: text/html')\n        print('Content-Length: %d' % len(response))\n        print()\n        sys.stdout.flush()\n        sys.stdout.buffer.write(response)\n        sys.stdout.buffer.flush()", "response": "Handles the HTTP GET request."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gen_params(raw_params):\n    assert raw_params[0].type == token.STAR and len(raw_params) > 2\n    curr_idx = 2 # the first place a keyword-only parameter name can be is index 2\n    max_idx = len(raw_params)\n    while curr_idx < max_idx:\n        curr_item = raw_params[curr_idx]\n        prev_item = curr_item.prev_sibling\n        if curr_item.type != token.NAME:\n            curr_idx += 1\n            continue\n        if prev_item is not None and prev_item.type == token.DOUBLESTAR:\n            break\n        name = curr_item.value\n        nxt = curr_item.next_sibling\n        if nxt is not None and nxt.type == token.EQUAL:\n            default_value = nxt.next_sibling\n            curr_idx += 2\n        else:\n            default_value = None\n        yield (name, default_value)\n        curr_idx += 1", "response": "u Generator that yields tuples of name default_value for each parameter in the list\n   ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef common_substring(s1, s2):\n    chunks = []\n    path1 = splitall(s1)\n    path2 = splitall(s2)\n    for (dir1, dir2) in zip(path1, path2):\n        if dir1 != dir2:\n            break\n        chunks.append(dir1)\n    return os.path.join(*chunks)", "response": "Returns the longest common substring to the two strings starting from the\n    left."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetect whether the code is Python 2 or Python 3.", "response": "def detect_python2(source, pathname):\n    \"\"\"\n    Returns a bool indicating whether we think the code is Py2\n    \"\"\"\n    RTs.setup_detect_python2()\n    try:\n        tree = RTs._rt_py2_detect.refactor_string(source, pathname)\n    except ParseError as e:\n        if e.msg != 'bad input' or e.value != '=':\n            raise\n        tree = RTs._rtp.refactor_string(source, pathname)\n\n    if source != str(tree)[:-1]:   # remove added newline\n        # The above fixers made changes, so we conclude it's Python 2 code\n        logger.debug('Detected Python 2 code: {0}'.format(pathname))\n        with open('/tmp/original_code.py', 'w') as f:\n            f.write('### Original code (detected as py2): %s\\n%s' %\n                    (pathname, source))\n        with open('/tmp/py2_detection_code.py', 'w') as f:\n            f.write('### Code after running py3 detection (from %s)\\n%s' %\n                    (pathname, str(tree)[:-1]))\n        return True\n    else:\n        logger.debug('Detected Python 3 code: {0}'.format(pathname))\n        with open('/tmp/original_code.py', 'w') as f:\n            f.write('### Original code (detected as py3): %s\\n%s' %\n                    (pathname, source))\n        try:\n            os.remove('/tmp/futurize_code.py')\n        except OSError:\n            pass\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncall this before using the refactoring tools to create them on demand if needed.", "response": "def setup():\n        \"\"\"\n        Call this before using the refactoring tools to create them on demand\n        if needed.\n        \"\"\"\n        if None in [RTs._rt, RTs._rtp]:\n            RTs._rt = RefactoringTool(myfixes)\n            RTs._rtp = RefactoringTool(myfixes, {'print_function': True})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setup_detect_python2():\n        if None in [RTs._rt_py2_detect, RTs._rtp_py2_detect]:\n            RTs._rt_py2_detect = RefactoringTool(py2_detect_fixers)\n            RTs._rtp_py2_detect = RefactoringTool(py2_detect_fixers,\n                                                  {'print_function': True})", "response": "Setups the refactoring tools for Python 2."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef all_patterns(name):\n\n    # i_ denotes an import-like node\n    # u_ denotes a node that appears to be a usage of the name\n    if u'.' in name:\n        name, attr = name.split(u'.', 1)\n        simple_name = simple_name_match % (name)\n        simple_attr = subname_match % (attr)\n        dotted_name = dotted_name_match % (simple_name, simple_attr)\n        i_from = from_import_match % (dotted_name)\n        i_from_submod = from_import_submod_match % (simple_name, simple_attr, simple_attr, simple_attr, simple_attr)\n        i_name = name_import_match % (dotted_name, dotted_name)\n        u_name = power_twoname_match % (simple_name, simple_attr)\n        u_subname = power_subname_match % (simple_attr)\n        return u' | \\n'.join((i_name, i_from, i_from_submod, u_name, u_subname))\n    else:\n        simple_name = simple_name_match % (name)\n        i_name = name_import_match % (simple_name, simple_name)\n        i_from = from_import_match % (simple_name)\n        u_name = power_onename_match % (simple_name)\n        return u' | \\n'.join((i_name, i_from, u_name))", "response": "u Returns a pattern of possible patterns involving that name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_mapping(self):\n        self.mapping = dict([(f.name, f) for f in iter(self)])", "response": "u Updates the mapping of names to features."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transform(self, node, results):\n        self.LISTNAME = self.new_name(u\"_3to2list\")\n        self.ITERNAME = self.new_name(u\"_3to2iter\")\n        expl, impl = results.get(u\"expl\"), results.get(u\"impl\")\n        if expl is not None:\n            setup_line, power_line = self.fix_explicit_context(node, results)\n            setup_line.prefix = expl.prefix\n            power_line.prefix = indentation(expl.parent)\n            setup_line.append_child(Newline())\n            parent = node.parent\n            i = node.remove()\n            parent.insert_child(i, power_line)\n            parent.insert_child(i, setup_line)\n        elif impl is not None:\n            setup_line, power_line = self.fix_implicit_context(node, results)\n            suitify(node)\n            suite = [k for k in node.children if k.type == syms.suite][0]\n            setup_line.prefix = u\"\"\n            power_line.prefix = suite.children[1].value\n            suite.children[2].prefix = indentation(suite.children[2])\n            suite.insert_child(2, Newline())\n            suite.insert_child(2, power_line)\n            suite.insert_child(2, Newline())\n            suite.insert_child(2, setup_line)\n            results.get(u\"lst\").replace(Name(self.ITERNAME, prefix=u\" \"))", "response": "uTransform a single entry node into a new entry node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef escape(s, quote=True):\n    assert not isinstance(s, bytes), 'Pass a unicode string'\n    if quote:\n        return s.translate(_escape_map_full)\n    return s.translate(_escape_map)", "response": "Replace special characters \"&\", \"<\" and \">\" to HTML-safe sequences.\n    If the optional flag quote is true (the default), the quotation mark\n    characters, both double quote (\") and single quote (') characters are also\n    translated."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef canonical_fix_name(fix, avail_fixes):\n    if \".fix_\" in fix:\n        return fix\n    else:\n        if fix.startswith('fix_'):\n            fix = fix[4:]\n        # Infer the full module name for the fixer.\n        # First ensure that no names clash (e.g.\n        # lib2to3.fixes.fix_blah and libfuturize.fixes.fix_blah):\n        found = [f for f in avail_fixes\n                 if f.endswith('fix_{0}'.format(fix))]\n        if len(found) > 1:\n            raise ValueError(\"Ambiguous fixer name. Choose a fully qualified \"\n                  \"module name instead from these:\\n\" +\n                  \"\\n\".join(\"  \" + myf for myf in found))\n        elif len(found) == 0:\n            raise ValueError(\"Unknown fixer. Use --list-fixes or -l for a list.\")\n        return found[0]", "response": "Given a fixer name return the canonical name for that fixer."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef commatize(leafs):\n    new_leafs = []\n    for leaf in leafs:\n        new_leafs.append(leaf)\n        new_leafs.append(Comma())\n    del new_leafs[-1]\n    return new_leafs", "response": "Takes a list of leafs and returns a list of new leafs"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the indentation for this node", "response": "def indentation(node):\n    \"\"\"\n    Returns the indentation for this node\n    Iff a node is in a suite, then it has indentation.\n    \"\"\"\n    while node.parent is not None and node.parent.type != syms.suite:\n        node = node.parent\n    if node.parent is None:\n        return u\"\"\n    # The first three children of a suite are NEWLINE, INDENT, (some other node)\n    # INDENT.value contains the indentation for this suite\n    # anything after (some other node) has the indentation as its prefix.\n    if node.type == token.INDENT:\n        return node.value\n    elif node.prev_sibling is not None and node.prev_sibling.type == token.INDENT:\n        return node.prev_sibling.value\n    elif node.prev_sibling is None:\n        return u\"\"\n    else:\n        return node.prefix"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef indentation_step(node):\n    r = find_root(node)\n    # Collect all indentations into one set.\n    all_indents = set(i.value for i in r.pre_order() if i.type == token.INDENT)\n    if not all_indents:\n        # nothing is indented anywhere, so we get to pick what we want\n        return u\"    \" # four spaces is a popular convention\n    else:\n        return min(all_indents)", "response": "Get the difference between each indentation level and the shortest indentation string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef suitify(parent):\n    for node in parent.children:\n        if node.type == syms.suite:\n            # already in the prefered format, do nothing\n            return\n\n    # One-liners have no suite node, we have to fake one up\n    for i, node in enumerate(parent.children):\n        if node.type == token.COLON:\n            break\n    else:\n        raise ValueError(u\"No class suite and no ':'!\")\n    # Move everything into a suite node\n    suite = Node(syms.suite, [Newline(), Leaf(token.INDENT, indentation(node) + indentation_step(node))])\n    one_node = parent.children[i+1]\n    one_node.remove()\n    one_node.prefix = u''\n    suite.append_child(one_node)\n    parent.append_child(suite)", "response": "Turn the stuff after the first colon in parent s children\n    into a suite node."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a node that imports a package.", "response": "def NameImport(package, as_name=None, prefix=None):\n    \"\"\"\n    Accepts a package (Name node), name to import it as (string), and\n    optional prefix and returns a node:\n    import <package> [as <as_name>]\n    \"\"\"\n    if prefix is None:\n        prefix = u\"\"\n    children = [Name(u\"import\", prefix=prefix), package]\n    if as_name is not None:\n        children.extend([Name(u\"as\", prefix=u\" \"),\n                         Name(as_name, prefix=u\" \")])\n    return Node(syms.import_name, children)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if the node appears to be a docstring.", "response": "def is_docstring(node):\n    \"\"\"\n    Returns True if the node appears to be a docstring\n    \"\"\"\n    return (node.type == syms.simple_stmt and\n            len(node.children) > 0 and node.children[0].type == token.STRING)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef future_import(feature, node):\n    root = find_root(node)\n\n    if does_tree_import(u\"__future__\", feature, node):\n        return\n\n    # Look for a shebang or encoding line\n    shebang_encoding_idx = None\n\n    for idx, node in enumerate(root.children):\n        # Is it a shebang or encoding line?\n        if is_shebang_comment(node) or is_encoding_comment(node):\n            shebang_encoding_idx = idx\n        if is_docstring(node):\n            # skip over docstring\n            continue\n        names = check_future_import(node)\n        if not names:\n            # not a future statement; need to insert before this\n            break\n        if feature in names:\n            # already imported\n            return\n\n    import_ = FromImport(u'__future__', [Leaf(token.NAME, feature, prefix=\" \")])\n    if shebang_encoding_idx == 0 and idx == 0:\n        # If this __future__ import would go on the first line,\n        # detach the shebang / encoding prefix from the current first line.\n        # and attach it to our new __future__ import node.\n        import_.prefix = root.children[0].prefix\n        root.children[0].prefix = u''\n        # End the __future__ import line with a newline and add a blank line\n        # afterwards:\n    children = [import_ , Newline()]\n    root.insert_child(idx, Node(syms.simple_stmt, children))", "response": "This function looks for a future import line and adds it to the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef future_import2(feature, node):\n    root = find_root(node)\n\n    if does_tree_import(u\"__future__\", feature, node):\n        return\n\n    insert_pos = 0\n    for idx, node in enumerate(root.children):\n        if node.type == syms.simple_stmt and node.children and \\\n           node.children[0].type == token.STRING:\n            insert_pos = idx + 1\n            break\n\n    for thing_after in root.children[insert_pos:]:\n        if thing_after.type == token.NEWLINE:\n            insert_pos += 1\n            continue\n\n        prefix = thing_after.prefix\n        thing_after.prefix = u\"\"\n        break\n    else:\n        prefix = u\"\"\n\n    import_ = FromImport(u\"__future__\", [Leaf(token.NAME, feature, prefix=u\" \")])\n\n    children = [import_, Newline()]\n    root.insert_child(insert_pos, Node(syms.simple_stmt, children, prefix=prefix))", "response": "A simple tree import that returns a new tree with the same feature and node."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwork like `does_tree_import` but adds an import statement at the top if it was not imported (but below any __future__ imports) and below any comments such as shebang lines). Based on lib2to3.fixer_util.touch_import() Calling this multiple times adds the imports in reverse order. Also adds \"standard_library.install_aliases()\" after \"from future import standard_library\". This should probably be factored into another function.", "response": "def touch_import_top(package, name_to_import, node):\n    \"\"\"Works like `does_tree_import` but adds an import statement at the\n    top if it was not imported (but below any __future__ imports) and below any\n    comments such as shebang lines).\n\n    Based on lib2to3.fixer_util.touch_import()\n\n    Calling this multiple times adds the imports in reverse order.\n\n    Also adds \"standard_library.install_aliases()\" after \"from future import\n    standard_library\".  This should probably be factored into another function.\n    \"\"\"\n\n    root = find_root(node)\n\n    if does_tree_import(package, name_to_import, root):\n        return\n\n    # Ideally, we would look for whether futurize --all-imports has been run,\n    # as indicated by the presence of ``from builtins import (ascii, ...,\n    # zip)`` -- and, if it has, we wouldn't import the name again.\n\n    # Look for __future__ imports and insert below them\n    found = False\n    for name in ['absolute_import', 'division', 'print_function',\n                 'unicode_literals']:\n        if does_tree_import('__future__', name, root):\n            found = True\n            break\n    if found:\n        # At least one __future__ import. We want to loop until we've seen them\n        # all.\n        start, end = None, None\n        for idx, node in enumerate(root.children):\n            if check_future_import(node):\n                start = idx\n                # Start looping\n                idx2 = start\n                while node:\n                    node = node.next_sibling\n                    idx2 += 1\n                    if not check_future_import(node):\n                        end = idx2\n                        break\n                break\n        assert start is not None\n        assert end is not None\n        insert_pos = end\n    else:\n        # No __future__ imports.\n        # We look for a docstring and insert the new node below that. If no docstring\n        # exists, just insert the node at the top.\n        for idx, node in enumerate(root.children):\n            if node.type != syms.simple_stmt:\n                break\n            if not is_docstring(node):\n                # This is the usual case.\n                break\n        insert_pos = idx\n\n    if package is None:\n        import_ = Node(syms.import_name, [\n            Leaf(token.NAME, u\"import\"),\n            Leaf(token.NAME, name_to_import, prefix=u\" \")\n        ])\n    else:\n        import_ = FromImport(package, [Leaf(token.NAME, name_to_import, prefix=u\" \")])\n        if name_to_import == u'standard_library':\n            # Add:\n            #     standard_library.install_aliases()\n            # after:\n            #     from future import standard_library\n            install_hooks = Node(syms.simple_stmt,\n                                 [Node(syms.power,\n                                       [Leaf(token.NAME, u'standard_library'),\n                                        Node(syms.trailer, [Leaf(token.DOT, u'.'),\n                                        Leaf(token.NAME, u'install_aliases')]),\n                                        Node(syms.trailer, [Leaf(token.LPAR, u'('),\n                                                            Leaf(token.RPAR, u')')])\n                                       ])\n                                 ]\n                                )\n            children_hooks = [install_hooks, Newline()]\n        else:\n            children_hooks = []\n\n        # FromImport(package, [Leaf(token.NAME, name_to_import, prefix=u\" \")])\n\n    children_import = [import_, Newline()]\n    old_prefix = root.children[insert_pos].prefix\n    root.children[insert_pos].prefix = u''\n    root.insert_child(insert_pos, Node(syms.simple_stmt, children_import, prefix=old_prefix))\n    if len(children_hooks) > 0:\n        root.insert_child(insert_pos + 1, Node(syms.simple_stmt, children_hooks))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_future_import(node):\n    # node should be the import statement here\n    savenode = node\n    if not (node.type == syms.simple_stmt and node.children):\n        return set()\n    node = node.children[0]\n    # now node is the import_from node\n    if not (node.type == syms.import_from and\n            # node.type == token.NAME and      # seems to break it\n            hasattr(node.children[1], 'value') and\n            node.children[1].value == u'__future__'):\n        return set()\n    if node.children[3].type == token.LPAR:\n        node = node.children[4]\n    else:\n        node = node.children[3]\n    # now node is the import_as_name[s]\n    # print(python_grammar.number2symbol[node.type])  # breaks sometimes\n    if node.type == syms.import_as_names:\n        result = set()\n        for n in node.children:\n            if n.type == token.NAME:\n                result.add(n.value)\n            elif n.type == syms.import_as_name:\n                n = n.children[0]\n                assert n.type == token.NAME\n                result.add(n.value)\n        return result\n    elif node.type == syms.import_as_name:\n        node = node.children[0]\n        assert node.type == token.NAME\n        return set([node.value])\n    elif node.type == token.NAME:\n        return set([node.value])\n    else:\n        # TODO: handle brackets like this:\n        #     from __future__ import (absolute_import, division)\n        assert False, \"strange import: %s\" % savenode", "response": "Check if this is a future import."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrap a function call in a sequence of arguments.", "response": "def wrap_in_fn_call(fn_name, args, prefix=None):\n    \"\"\"\n    Example:\n    >>> wrap_in_fn_call(\"oldstr\", (arg,))\n    oldstr(arg)\n\n    >>> wrap_in_fn_call(\"olddiv\", (arg1, arg2))\n    olddiv(arg1, arg2)\n\n    >>> wrap_in_fn_call(\"olddiv\", [arg1, comma, arg2, comma, arg3])\n    olddiv(arg1, arg2, arg3)\n    \"\"\"\n    assert len(args) > 0\n    if len(args) == 2:\n        expr1, expr2 = args\n        newargs = [expr1, Comma(), expr2]\n    else:\n        newargs = args\n    return Call(Name(fn_name), newargs, prefix=prefix)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the next item from the iterator. If default is given the iterator is exhausted and the iterator is returned instead of raising StopIteration.", "response": "def newnext(iterator, default=_SENTINEL):\n    \"\"\"\n    next(iterator[, default])\n\n    Return the next item from the iterator. If default is given and the iterator\n    is exhausted, it is returned instead of raising StopIteration.\n    \"\"\"\n\n    # args = []\n    # if default is not _SENTINEL:\n    #     args.append(default)\n    try:\n        try:\n            return iterator.__next__()\n        except AttributeError:\n            try:\n                return iterator.next()\n            except AttributeError:\n                raise TypeError(\"'{0}' object is not an iterator\".format(\n                                           iterator.__class__.__name__))\n    except StopIteration as e:\n        if default is _SENTINEL:\n            raise e\n        else:\n            return default"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new date with new values for the specified fields.", "response": "def replace(self, year=None, month=None, day=None):\n        \"\"\"Return a new date with new values for the specified fields.\"\"\"\n        if year is None:\n            year = self._year\n        if month is None:\n            month = self._month\n        if day is None:\n            day = self._day\n        _check_date_fields(year, month, day)\n        return date(year, month, day)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _tzstr(self, sep=\":\"):\n        off = self.utcoffset()\n        if off is not None:\n            if off.days < 0:\n                sign = \"-\"\n                off = -off\n            else:\n                sign = \"+\"\n            hh, mm = divmod(off, timedelta(hours=1))\n            assert not mm % timedelta(minutes=1), \"whole minute\"\n            mm //= timedelta(minutes=1)\n            assert 0 <= hh < 24\n            off = \"%s%02d%s%02d\" % (sign, hh, sep, mm)\n        return off", "response": "Return formatted timezone offset."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nformats using strftime(). The date part of the timestamp passed to underlying strftime should not be used.", "response": "def strftime(self, fmt):\n        \"\"\"Format using strftime().  The date part of the timestamp passed\n        to underlying strftime should not be used.\n        \"\"\"\n        # The year must be >= 1000 else Python's strftime implementation\n        # can raise a bogus exception.\n        timetuple = (1900, 1, 1,\n                     self._hour, self._minute, self._second,\n                     0, 1, -1)\n        return _wrap_strftime(self, fmt, timetuple)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef utcoffset(self):\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.utcoffset(None)\n        _check_utc_offset(\"utcoffset\", offset)\n        return offset", "response": "Return the timezone offset in minutes east of UTC ( negative west of\n        UTC."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef replace(self, hour=None, minute=None, second=None, microsecond=None,\n                tzinfo=True):\n        \"\"\"Return a new time with new values for the specified fields.\"\"\"\n        if hour is None:\n            hour = self.hour\n        if minute is None:\n            minute = self.minute\n        if second is None:\n            second = self.second\n        if microsecond is None:\n            microsecond = self.microsecond\n        if tzinfo is True:\n            tzinfo = self.tzinfo\n        _check_time_fields(hour, minute, second, microsecond)\n        _check_tzinfo_arg(tzinfo)\n        return time(hour, minute, second, microsecond, tzinfo)", "response": "Return a new time with new values for the specified fields."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fromtimestamp(cls, t, tz=None):\n\n        _check_tzinfo_arg(tz)\n\n        converter = _time.localtime if tz is None else _time.gmtime\n\n        t, frac = divmod(t, 1.0)\n        us = int(frac * 1e6)\n\n        # If timestamp is less than one microsecond smaller than a\n        # full second, us can be rounded up to 1000000.  In this case,\n        # roll over to seconds, otherwise, ValueError is raised\n        # by the constructor.\n        if us == 1000000:\n            t += 1\n            us = 0\n        y, m, d, hh, mm, ss, weekday, jday, dst = converter(t)\n        ss = min(ss, 59)    # clamp out leap seconds if the platform has them\n        result = cls(y, m, d, hh, mm, ss, us, tz)\n        if tz is not None:\n            result = tz.fromutc(result)\n        return result", "response": "Construct a datetime object from a POSIX timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconstructs a UTC datetime from a POSIX timestamp ( like time. time()).", "response": "def utcfromtimestamp(cls, t):\n        \"Construct a UTC datetime from a POSIX timestamp (like time.time()).\"\n        t, frac = divmod(t, 1.0)\n        us = int(frac * 1e6)\n\n        # If timestamp is less than one microsecond smaller than a\n        # full second, us can be rounded up to 1000000.  In this case,\n        # roll over to seconds, otherwise, ValueError is raised\n        # by the constructor.\n        if us == 1000000:\n            t += 1\n            us = 0\n        y, m, d, hh, mm, ss, weekday, jday, dst = _time.gmtime(t)\n        ss = min(ss, 59)    # clamp out leap seconds if the platform has them\n        return cls(y, m, d, hh, mm, ss, us)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns local time tuple compatible with time. localtime().", "response": "def timetuple(self):\n        \"Return local time tuple compatible with time.localtime().\"\n        dst = self.dst()\n        if dst is None:\n            dst = -1\n        elif dst:\n            dst = 1\n        else:\n            dst = 0\n        return _build_struct_time(self.year, self.month, self.day,\n                                  self.hour, self.minute, self.second,\n                                  dst)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef timestamp(self):\n        \"Return POSIX timestamp as float\"\n        if self._tzinfo is None:\n            return _time.mktime((self.year, self.month, self.day,\n                                 self.hour, self.minute, self.second,\n                                 -1, -1, -1)) + self.microsecond / 1e6\n        else:\n            return (self - _EPOCH).total_seconds()", "response": "Return POSIX timestamp as float"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning UTC time tuple compatible with time. gmtime().", "response": "def utctimetuple(self):\n        \"Return UTC time tuple compatible with time.gmtime().\"\n        offset = self.utcoffset()\n        if offset:\n            self -= offset\n        y, m, d = self.year, self.month, self.day\n        hh, mm, ss = self.hour, self.minute, self.second\n        return _build_struct_time(y, m, d, hh, mm, ss, 0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new datetime with new values for the specified fields.", "response": "def replace(self, year=None, month=None, day=None, hour=None,\n                minute=None, second=None, microsecond=None, tzinfo=True):\n        \"\"\"Return a new datetime with new values for the specified fields.\"\"\"\n        if year is None:\n            year = self.year\n        if month is None:\n            month = self.month\n        if day is None:\n            day = self.day\n        if hour is None:\n            hour = self.hour\n        if minute is None:\n            minute = self.minute\n        if second is None:\n            second = self.second\n        if microsecond is None:\n            microsecond = self.microsecond\n        if tzinfo is True:\n            tzinfo = self.tzinfo\n        _check_date_fields(year, month, day)\n        _check_time_fields(hour, minute, second, microsecond)\n        _check_tzinfo_arg(tzinfo)\n        return datetime(year, month, day, hour, minute, second,\n                          microsecond, tzinfo)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the time formatted according to ISO.", "response": "def isoformat(self, sep='T'):\n        \"\"\"Return the time formatted according to ISO.\n\n        This is 'YYYY-MM-DD HH:MM:SS.mmmmmm', or 'YYYY-MM-DD HH:MM:SS' if\n        self.microsecond == 0.\n\n        If self.tzinfo is not None, the UTC offset is also attached, giving\n        'YYYY-MM-DD HH:MM:SS.mmmmmm+HH:MM' or 'YYYY-MM-DD HH:MM:SS+HH:MM'.\n\n        Optional argument sep specifies the separator between date and\n        time, default 'T'.\n        \"\"\"\n        s = (\"%04d-%02d-%02d%c\" % (self._year, self._month, self._day,\n                                  sep) +\n                _format_time(self._hour, self._minute, self._second,\n                             self._microsecond))\n        off = self.utcoffset()\n        if off is not None:\n            if off.days < 0:\n                sign = \"-\"\n                off = -off\n            else:\n                sign = \"+\"\n            hh, mm = divmod(off, timedelta(hours=1))\n            assert not mm % timedelta(minutes=1), \"whole minute\"\n            mm //= timedelta(minutes=1)\n            s += \"%s%02d:%02d\" % (sign, hh, mm)\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstrings format -> new datetime parsed from a string ( like time. strftime ).", "response": "def strptime(cls, date_string, format):\n        'string, format -> new datetime parsed from a string (like time.strptime()).'\n        import _strptime\n        return _strptime._strptime_datetime(cls, date_string, format)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(self, fp, headersonly=False):\n        feedparser = FeedParser(self._class, policy=self.policy)\n        if headersonly:\n            feedparser._set_headersonly()\n        while True:\n            data = fp.read(8192)\n            if not data:\n                break\n            feedparser.feed(data)\n        return feedparser.close()", "response": "Parse a file and return a message structure."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a message structure from a string.", "response": "def parsestr(self, text, headersonly=False):\n        \"\"\"Create a message structure from a string.\n\n        Returns the root of the message structure.  Optional headersonly is a\n        flag specifying whether to stop parsing after reading the headers or\n        not.  The default is False, meaning it parses the entire contents of\n        the file.\n        \"\"\"\n        return self.parse(StringIO(text), headersonly=headersonly)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a message structure from the data in a binary file.", "response": "def parse(self, fp, headersonly=False):\n        \"\"\"Create a message structure from the data in a binary file.\n\n        Reads all the data from the file and returns the root of the message\n        structure.  Optional headersonly is a flag specifying whether to stop\n        parsing after reading the headers or not.  The default is False,\n        meaning it parses the entire contents of the file.\n        \"\"\"\n        fp = TextIOWrapper(fp, encoding='ascii', errors='surrogateescape')\n        with fp:\n            return self.parser.parse(fp, headersonly)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a message structure from a byte string.", "response": "def parsebytes(self, text, headersonly=False):\n        \"\"\"Create a message structure from a byte string.\n\n        Returns the root of the message structure.  Optional headersonly is a\n        flag specifying whether to stop parsing after reading the headers or\n        not.  The default is False, meaning it parses the entire contents of\n        the file.\n        \"\"\"\n        text = text.decode('ASCII', errors='surrogateescape')\n        return self.parser.parsestr(text, headersonly)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a string into a Message object model.", "response": "def message_from_string(s, *args, **kws):\n    \"\"\"Parse a string into a Message object model.\n\n    Optional _class and strict are passed to the Parser constructor.\n    \"\"\"\n    from future.backports.email.parser import Parser\n    return Parser(*args, **kws).parsestr(s)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a bytes string into a Message object model.", "response": "def message_from_bytes(s, *args, **kws):\n    \"\"\"Parse a bytes string into a Message object model.\n\n    Optional _class and strict are passed to the Parser constructor.\n    \"\"\"\n    from future.backports.email.parser import BytesParser\n    return BytesParser(*args, **kws).parsebytes(s)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread a file and parse its contents into a Message object.", "response": "def message_from_file(fp, *args, **kws):\n    \"\"\"Read a file and parse its contents into a Message object model.\n\n    Optional _class and strict are passed to the Parser constructor.\n    \"\"\"\n    from future.backports.email.parser import Parser\n    return Parser(*args, **kws).parse(fp)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef message_from_binary_file(fp, *args, **kws):\n    from future.backports.email.parser import BytesParser\n    return BytesParser(*args, **kws).parse(fp)", "response": "Read a binary file and parse its contents into a Message object model."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_headers(fp, _class=HTTPMessage):\n    headers = []\n    while True:\n        line = fp.readline(_MAXLINE + 1)\n        if len(line) > _MAXLINE:\n            raise LineTooLong(\"header line\")\n        headers.append(line)\n        if len(headers) > _MAXHEADERS:\n            raise HTTPException(\"got more than %d headers\" % _MAXHEADERS)\n        if line in (b'\\r\\n', b'\\n', b''):\n            break\n    hstring = bytes(b'').join(headers).decode('iso-8859-1')\n    return email_parser.Parser(_class=_class).parsestr(hstring)", "response": "Parses only RFC2822 headers from a file pointer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _safe_read(self, amt):\n        s = []\n        while amt > 0:\n            chunk = self.fp.read(min(amt, MAXAMOUNT))\n            if not chunk:\n                raise IncompleteRead(bytes(b'').join(s), amt)\n            s.append(chunk)\n            amt -= len(chunk)\n        return bytes(b\"\").join(s)", "response": "Read the number of bytes requested compensating for partial reads."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset up the host and port for the HTTP CONNECT Tunnelling.", "response": "def set_tunnel(self, host, port=None, headers=None):\n        \"\"\" Sets up the host and the port for the HTTP CONNECT Tunnelling.\n\n        The headers argument should be a mapping of extra HTTP headers\n        to send with the CONNECT request.\n        \"\"\"\n        self._tunnel_host = host\n        self._tunnel_port = port\n        if headers:\n            self._tunnel_headers = headers\n        else:\n            self._tunnel_headers.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconnect to the host and port specified in __init__.", "response": "def connect(self):\n        \"\"\"Connect to the host and port specified in __init__.\"\"\"\n        self.sock = socket_create_connection((self.host,self.port),\n                                             self.timeout, self.source_address)\n        if self._tunnel_host:\n            self._tunnel()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nclosing the connection to the HTTP server.", "response": "def close(self):\n        \"\"\"Close the connection to the HTTP server.\"\"\"\n        if self.sock:\n            self.sock.close()   # close it manually... there may be other refs\n            self.sock = None\n        if self.__response:\n            self.__response.close()\n            self.__response = None\n        self.__state = _CS_IDLE"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send(self, data):\n\n        if self.sock is None:\n            if self.auto_open:\n                self.connect()\n            else:\n                raise NotConnected()\n\n        if self.debuglevel > 0:\n            print(\"send:\", repr(data))\n        blocksize = 8192\n        # Python 2.7 array objects have a read method which is incompatible\n        # with the 2-arg calling syntax below.\n        if hasattr(data, \"read\") and not isinstance(data, array):\n            if self.debuglevel > 0:\n                print(\"sendIng a read()able\")\n            encode = False\n            try:\n                mode = data.mode\n            except AttributeError:\n                # io.BytesIO and other file-like objects don't have a `mode`\n                # attribute.\n                pass\n            else:\n                if \"b\" not in mode:\n                    encode = True\n                    if self.debuglevel > 0:\n                        print(\"encoding file using iso-8859-1\")\n            while 1:\n                datablock = data.read(blocksize)\n                if not datablock:\n                    break\n                if encode:\n                    datablock = datablock.encode(\"iso-8859-1\")\n                self.sock.sendall(datablock)\n            return\n        try:\n            self.sock.sendall(data)\n        except TypeError:\n            if isinstance(data, collections.Iterable):\n                for d in data:\n                    self.sock.sendall(d)\n            else:\n                raise TypeError(\"data should be a bytes-like object \"\n                                \"or an iterable, got %r\" % type(data))", "response": "Send data to the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend the current buffered request and clear the buffer and clear the buffer.", "response": "def _send_output(self, message_body=None):\n        \"\"\"Send the currently buffered request and clear the buffer.\n\n        Appends an extra \\\\r\\\\n to the buffer.\n        A message_body may be specified, to be appended to the request.\n        \"\"\"\n        self._buffer.extend((bytes(b\"\"), bytes(b\"\")))\n        msg = bytes(b\"\\r\\n\").join(self._buffer)\n        del self._buffer[:]\n        # If msg and message_body are sent in a single send() call,\n        # it will avoid performance problems caused by the interaction\n        # between delayed ack and the Nagle algorithm.\n        if isinstance(message_body, bytes):\n            msg += message_body\n            message_body = None\n        self.send(msg)\n        if message_body is not None:\n            # message_body was not a string (i.e. it is a file), and\n            # we must run the risk of Nagle.\n            self.send(message_body)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a request to the server.", "response": "def putrequest(self, method, url, skip_host=0, skip_accept_encoding=0):\n        \"\"\"Send a request to the server.\n\n        `method' specifies an HTTP request method, e.g. 'GET'.\n        `url' specifies the object being requested, e.g. '/index.html'.\n        `skip_host' if True does not add automatically a 'Host:' header\n        `skip_accept_encoding' if True does not add automatically an\n           'Accept-Encoding:' header\n        \"\"\"\n\n        # if a prior response has been completed, then forget about it.\n        if self.__response and self.__response.isclosed():\n            self.__response = None\n\n\n        # in certain cases, we cannot issue another request on this connection.\n        # this occurs when:\n        #   1) we are in the process of sending a request.   (_CS_REQ_STARTED)\n        #   2) a response to a previous request has signalled that it is going\n        #      to close the connection upon completion.\n        #   3) the headers for the previous response have not been read, thus\n        #      we cannot determine whether point (2) is true.   (_CS_REQ_SENT)\n        #\n        # if there is no prior response, then we can request at will.\n        #\n        # if point (2) is true, then we will have passed the socket to the\n        # response (effectively meaning, \"there is no prior response\"), and\n        # will open a new one when a new request is made.\n        #\n        # Note: if a prior response exists, then we *can* start a new request.\n        #       We are not allowed to begin fetching the response to this new\n        #       request, however, until that prior response is complete.\n        #\n        if self.__state == _CS_IDLE:\n            self.__state = _CS_REQ_STARTED\n        else:\n            raise CannotSendRequest(self.__state)\n\n        # Save the method we use, we need it later in the response phase\n        self._method = method\n        if not url:\n            url = '/'\n        request = '%s %s %s' % (method, url, self._http_vsn_str)\n\n        # Non-ASCII characters should have been eliminated earlier\n        self._output(request.encode('ascii'))\n\n        if self._http_vsn == 11:\n            # Issue some standard headers for better HTTP/1.1 compliance\n\n            if not skip_host:\n                # this header is issued *only* for HTTP/1.1\n                # connections. more specifically, this means it is\n                # only issued when the client uses the new\n                # HTTPConnection() class. backwards-compat clients\n                # will be using HTTP/1.0 and those clients may be\n                # issuing this header themselves. we should NOT issue\n                # it twice; some web servers (such as Apache) barf\n                # when they see two Host: headers\n\n                # If we need a non-standard port,include it in the\n                # header.  If the request is going through a proxy,\n                # but the host of the actual URL, not the host of the\n                # proxy.\n\n                netloc = ''\n                if url.startswith('http'):\n                    nil, netloc, nil, nil, nil = urlsplit(url)\n\n                if netloc:\n                    try:\n                        netloc_enc = netloc.encode(\"ascii\")\n                    except UnicodeEncodeError:\n                        netloc_enc = netloc.encode(\"idna\")\n                    self.putheader('Host', netloc_enc)\n                else:\n                    try:\n                        host_enc = self.host.encode(\"ascii\")\n                    except UnicodeEncodeError:\n                        host_enc = self.host.encode(\"idna\")\n\n                    # As per RFC 273, IPv6 address should be wrapped with []\n                    # when used as Host header\n\n                    if self.host.find(':') >= 0:\n                        host_enc = bytes(b'[' + host_enc + b']')\n\n                    if self.port == self.default_port:\n                        self.putheader('Host', host_enc)\n                    else:\n                        host_enc = host_enc.decode(\"ascii\")\n                        self.putheader('Host', \"%s:%s\" % (host_enc, self.port))\n\n            # note: we are assuming that clients will not attempt to set these\n            #       headers since *this* library must deal with the\n            #       consequences. this also means that when the supporting\n            #       libraries are updated to recognize other forms, then this\n            #       code should be changed (removed or updated).\n\n            # we only want a Content-Encoding of \"identity\" since we don't\n            # support encodings such as x-gzip or x-deflate.\n            if not skip_accept_encoding:\n                self.putheader('Accept-Encoding', 'identity')\n\n            # we can accept \"chunked\" Transfer-Encodings, but no others\n            # NOTE: no TE header implies *only* \"chunked\"\n            #self.putheader('TE', 'chunked')\n\n            # if TE is supplied in the header, then it must appear in a\n            # Connection header.\n            #self.putheader('Connection', 'TE')\n\n        else:\n            # For HTTP/1.0, the server will assume \"not chunked\"\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef putheader(self, header, *values):\n        if self.__state != _CS_REQ_STARTED:\n            raise CannotSendHeader()\n\n        if hasattr(header, 'encode'):\n            header = header.encode('ascii')\n        values = list(values)\n        for i, one_value in enumerate(values):\n            if hasattr(one_value, 'encode'):\n                values[i] = one_value.encode('latin-1')\n            elif isinstance(one_value, int):\n                values[i] = str(one_value).encode('ascii')\n        value = bytes(b'\\r\\n\\t').join(values)\n        header = header + bytes(b': ') + value\n        self._output(header)", "response": "Send a request header line to the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef endheaders(self, message_body=None):\n        if self.__state == _CS_REQ_STARTED:\n            self.__state = _CS_REQ_SENT\n        else:\n            raise CannotSendHeader()\n        self._send_output(message_body)", "response": "Indicate that the last header line has been sent to the server."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef request(self, method, url, body=None, headers={}):\n        self._send_request(method, url, body, headers)", "response": "Send a request to the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the response from the server.", "response": "def getresponse(self):\n        \"\"\"Get the response from the server.\n\n        If the HTTPConnection is in the correct state, returns an\n        instance of HTTPResponse or of whatever object is returned by\n        class the response_class variable.\n\n        If a request has not been sent or if a previous response has\n        not be handled, ResponseNotReady is raised.  If the HTTP\n        response indicates that the connection should be closed, then\n        it will be closed before the response is returned.  When the\n        connection is closed, the underlying socket is closed.\n        \"\"\"\n\n        # if a prior response has been completed, then forget about it.\n        if self.__response and self.__response.isclosed():\n            self.__response = None\n\n        # if a prior response exists, then it must be completed (otherwise, we\n        # cannot read this response's header to determine the connection-close\n        # behavior)\n        #\n        # note: if a prior response existed, but was connection-close, then the\n        # socket and response were made independent of this HTTPConnection\n        # object since a new request requires that we open a whole new\n        # connection\n        #\n        # this means the prior response had one of two states:\n        #   1) will_close: this connection was reset and the prior socket and\n        #                  response operate independently\n        #   2) persistent: the response was retained and we await its\n        #                  isclosed() status to become true.\n        #\n        if self.__state != _CS_REQ_SENT or self.__response:\n            raise ResponseNotReady(self.__state)\n\n        if self.debuglevel > 0:\n            response = self.response_class(self.sock, self.debuglevel,\n                                           method=self._method)\n        else:\n            response = self.response_class(self.sock, method=self._method)\n\n        response.begin()\n        assert response.will_close != _UNKNOWN\n        self.__state = _CS_IDLE\n\n        if response.will_close:\n            # this effectively passes the connection to the response\n            self.close()\n        else:\n            # remember this, so we can tell when it is complete\n            self.__response = response\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating that the input token contains ASCII non - printable characters and register a defect.", "response": "def _validate_xtext(xtext):\n    \"\"\"If input token contains ASCII non-printables, register a defect.\"\"\"\n\n    non_printables = _non_printable_finder(xtext)\n    if non_printables:\n        xtext.defects.append(errors.NonPrintableDefect(non_printables))\n    if utils._has_surrogates(xtext):\n        xtext.defects.append(errors.UndecodableBytesDefect(\n            \"Non-ASCII characters found in header token\"))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nscanning printables/quoted-pairs until endchars and return unquoted ptext. This function turns a run of qcontent, ccontent-without-comments, or dtext-with-quoted-printables into a single string by unquoting any quoted printables. It returns the string, the remaining value, and a flag that is True iff there were any quoted printables decoded.", "response": "def _get_ptext_to_endchars(value, endchars):\n    \"\"\"Scan printables/quoted-pairs until endchars and return unquoted ptext.\n\n    This function turns a run of qcontent, ccontent-without-comments, or\n    dtext-with-quoted-printables into a single string by unquoting any\n    quoted printables.  It returns the string, the remaining value, and\n    a flag that is True iff there were any quoted printables decoded.\n\n    \"\"\"\n    _3to2list = list(_wsp_splitter(value, 1))\n    fragment, remainder, = _3to2list[:1] + [_3to2list[1:]]\n    vchars = []\n    escape = False\n    had_qp = False\n    for pos in range(len(fragment)):\n        if fragment[pos] == '\\\\':\n            if escape:\n                escape = False\n                had_qp = True\n            else:\n                escape = True\n                continue\n        if escape:\n            escape = False\n        elif fragment[pos] in endchars:\n            break\n        vchars.append(fragment[pos])\n    else:\n        pos = pos + 1\n    return ''.join(vchars), ''.join([fragment[pos:]] + remainder), had_qp"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndecode a run of RFC 2047 encoded words into text.", "response": "def _decode_ew_run(value):\n    \"\"\" Decode a run of RFC2047 encoded words.\n\n        _decode_ew_run(value) -> (text, value, defects)\n\n    Scans the supplied value for a run of tokens that look like they are RFC\n    2047 encoded words, decodes those words into text according to RFC 2047\n    rules (whitespace between encoded words is discarded), and returns the text\n    and the remaining value (including any leading whitespace on the remaining\n    value), as well as a list of any defects encountered while decoding.  The\n    input value may not have any leading whitespace.\n\n    \"\"\"\n    res = []\n    defects = []\n    last_ws = ''\n    while value:\n        try:\n            tok, ws, value = _wsp_splitter(value, 1)\n        except ValueError:\n            tok, ws, value = value, '', ''\n        if not (tok.startswith('=?') and tok.endswith('?=')):\n            return ''.join(res), last_ws + tok + ws + value, defects\n        text, charset, lang, new_defects = _ew.decode(tok)\n        res.append(text)\n        defects.extend(new_defects)\n        last_ws = ws\n    return ''.join(res), last_ws, defects"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_fws(value):\n    newvalue = value.lstrip()\n    fws = WhiteSpaceTerminal(value[:len(value)-len(newvalue)], 'fws')\n    return fws, newvalue", "response": "Returns the FWS and the new value of the unfolding token."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets an encoded word from a string.", "response": "def get_encoded_word(value):\n    \"\"\" encoded-word = \"=?\" charset \"?\" encoding \"?\" encoded-text \"?=\"\n\n    \"\"\"\n    ew = EncodedWord()\n    if not value.startswith('=?'):\n        raise errors.HeaderParseError(\n            \"expected encoded word but found {}\".format(value))\n    _3to2list1 = list(value[2:].split('?=', 1))\n    tok, remainder, = _3to2list1[:1] + [_3to2list1[1:]]\n    if tok == value[2:]:\n        raise errors.HeaderParseError(\n            \"expected encoded word but found {}\".format(value))\n    remstr = ''.join(remainder)\n    if remstr[:2].isdigit():\n        _3to2list3 = list(remstr.split('?=', 1))\n        rest, remainder, = _3to2list3[:1] + [_3to2list3[1:]]\n        tok = tok + '?=' + rest\n    if len(tok.split()) > 1:\n        ew.defects.append(errors.InvalidHeaderDefect(\n            \"whitespace inside encoded word\"))\n    ew.cte = value\n    value = ''.join(remainder)\n    try:\n        text, charset, lang, defects = _ew.decode('=?' + tok + '?=')\n    except ValueError:\n        raise errors.HeaderParseError(\n            \"encoded word format invalid: '{}'\".format(ew.cte))\n    ew.charset = charset\n    ew.lang = lang\n    ew.defects.extend(defects)\n    while text:\n        if text[0] in WSP:\n            token, text = get_fws(text)\n            ew.append(token)\n            continue\n        _3to2list5 = list(_wsp_splitter(text, 1))\n        chars, remainder, = _3to2list5[:1] + [_3to2list5[1:]]\n        vtext = ValueTerminal(chars, 'vtext')\n        _validate_xtext(vtext)\n        ew.append(vtext)\n        text = ''.join(remainder)\n    return ew, value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the UnstructuredTokenList for the given value.", "response": "def get_unstructured(value):\n    \"\"\"unstructured = (*([FWS] vchar) *WSP) / obs-unstruct\n       obs-unstruct = *((*LF *CR *(obs-utext) *LF *CR)) / FWS)\n       obs-utext = %d0 / obs-NO-WS-CTL / LF / CR\n\n       obs-NO-WS-CTL is control characters except WSP/CR/LF.\n\n    So, basically, we have printable runs, plus control characters or nulls in\n    the obsolete syntax, separated by whitespace.  Since RFC 2047 uses the\n    obsolete syntax in its specification, but requires whitespace on either\n    side of the encoded words, I can see no reason to need to separate the\n    non-printable-non-whitespace from the printable runs if they occur, so we\n    parse this into xtext tokens separated by WSP tokens.\n\n    Because an 'unstructured' value must by definition constitute the entire\n    value, this 'get' routine does not return a remaining value, only the\n    parsed TokenList.\n\n    \"\"\"\n    # XXX: but what about bare CR and LF?  They might signal the start or\n    # end of an encoded word.  YAGNI for now, since out current parsers\n    # will never send us strings with bard CR or LF.\n\n    unstructured = UnstructuredTokenList()\n    while value:\n        if value[0] in WSP:\n            token, value = get_fws(value)\n            unstructured.append(token)\n            continue\n        if value.startswith('=?'):\n            try:\n                token, value = get_encoded_word(value)\n            except errors.HeaderParseError:\n                pass\n            else:\n                have_ws = True\n                if len(unstructured) > 0:\n                    if unstructured[-1].token_type != 'fws':\n                        unstructured.defects.append(errors.InvalidHeaderDefect(\n                            \"missing whitespace before encoded word\"))\n                        have_ws = False\n                if have_ws and len(unstructured) > 1:\n                    if unstructured[-2].token_type == 'encoded-word':\n                        unstructured[-1] = EWWhiteSpaceTerminal(\n                            unstructured[-1], 'fws')\n                unstructured.append(token)\n                continue\n        _3to2list7 = list(_wsp_splitter(value, 1))\n        tok, remainder, = _3to2list7[:1] + [_3to2list7[1:]]\n        vtext = ValueTerminal(tok, 'vtext')\n        _validate_xtext(vtext)\n        unstructured.append(vtext)\n        value = ''.join(remainder)\n    return unstructured"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_qp_ctext(value):\n    ptext, value, _ = _get_ptext_to_endchars(value, '()')\n    ptext = WhiteSpaceTerminal(ptext, 'ptext')\n    _validate_xtext(ptext)\n    return ptext, value", "response": "Returns the ptext and value of a ctext token."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the qcontent of a value.", "response": "def get_qcontent(value):\n    \"\"\"qcontent = qtext / quoted-pair\n\n    We allow anything except the DQUOTE character, but if we find any ASCII\n    other than the RFC defined printable ASCII an NonPrintableDefect is\n    added to the token's defects list.  Any quoted pairs are converted to their\n    unquoted values, so what is returned is a 'ptext' token.  In this case it\n    is a ValueTerminal.\n\n    \"\"\"\n    ptext, value, _ = _get_ptext_to_endchars(value, '\"')\n    ptext = ValueTerminal(ptext, 'ptext')\n    _validate_xtext(ptext)\n    return ptext, value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_atext(value):\n    m = _non_atom_end_matcher(value)\n    if not m:\n        raise errors.HeaderParseError(\n            \"expected atext but found '{}'\".format(value))\n    atext = m.group()\n    value = value[len(atext):]\n    atext = ValueTerminal(atext, 'atext')\n    _validate_xtext(atext)\n    return atext, value", "response": "Get the atext and value of a token."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_bare_quoted_string(value):\n    if value[0] != '\"':\n        raise errors.HeaderParseError(\n            \"expected '\\\"' but found '{}'\".format(value))\n    bare_quoted_string = BareQuotedString()\n    value = value[1:]\n    while value and value[0] != '\"':\n        if value[0] in WSP:\n            token, value = get_fws(value)\n        else:\n            token, value = get_qcontent(value)\n        bare_quoted_string.append(token)\n    if not value:\n        bare_quoted_string.defects.append(errors.InvalidHeaderDefect(\n            \"end of header inside quoted string\"))\n        return bare_quoted_string, value\n    return bare_quoted_string, value[1:]", "response": "Parses a bare - quoted - string and returns it and its value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a comment from a string.", "response": "def get_comment(value):\n    \"\"\"comment = \"(\" *([FWS] ccontent) [FWS] \")\"\n       ccontent = ctext / quoted-pair / comment\n\n    We handle nested comments here, and quoted-pair in our qp-ctext routine.\n    \"\"\"\n    if value and value[0] != '(':\n        raise errors.HeaderParseError(\n            \"expected '(' but found '{}'\".format(value))\n    comment = Comment()\n    value = value[1:]\n    while value and value[0] != \")\":\n        if value[0] in WSP:\n            token, value = get_fws(value)\n        elif value[0] == '(':\n            token, value = get_comment(value)\n        else:\n            token, value = get_qp_ctext(value)\n        comment.append(token)\n    if not value:\n        comment.defects.append(errors.InvalidHeaderDefect(\n            \"end of header inside comment\"))\n        return comment, value\n    return comment, value[1:]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget CFWS list and value.", "response": "def get_cfws(value):\n    \"\"\"CFWS = (1*([FWS] comment) [FWS]) / FWS\n\n    \"\"\"\n    cfws = CFWSList()\n    while value and value[0] in CFWS_LEADER:\n        if value[0] in WSP:\n            token, value = get_fws(value)\n        else:\n            token, value = get_comment(value)\n        cfws.append(token)\n    return cfws, value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a quoted - string and the value.", "response": "def get_quoted_string(value):\n    \"\"\"quoted-string = [CFWS] <bare-quoted-string> [CFWS]\n\n    'bare-quoted-string' is an intermediate class defined by this\n    parser and not by the RFC grammar.  It is the quoted string\n    without any attached CFWS.\n    \"\"\"\n    quoted_string = QuotedString()\n    if value and value[0] in CFWS_LEADER:\n        token, value = get_cfws(value)\n        quoted_string.append(token)\n    token, value = get_bare_quoted_string(value)\n    quoted_string.append(token)\n    if value and value[0] in CFWS_LEADER:\n        token, value = get_cfws(value)\n        quoted_string.append(token)\n    return quoted_string, value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the atom and the value.", "response": "def get_atom(value):\n    \"\"\"atom = [CFWS] 1*atext [CFWS]\n\n    \"\"\"\n    atom = Atom()\n    if value and value[0] in CFWS_LEADER:\n        token, value = get_cfws(value)\n        atom.append(token)\n    if value and value[0] in ATOM_ENDS:\n        raise errors.HeaderParseError(\n            \"expected atom but found '{}'\".format(value))\n    token, value = get_atext(value)\n    atom.append(token)\n    if value and value[0] in CFWS_LEADER:\n        token, value = get_cfws(value)\n        atom.append(token)\n    return atom, value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_dot_atom_text(value):\n    dot_atom_text = DotAtomText()\n    if not value or value[0] in ATOM_ENDS:\n        raise errors.HeaderParseError(\"expected atom at a start of \"\n            \"dot-atom-text but found '{}'\".format(value))\n    while value and value[0] not in ATOM_ENDS:\n        token, value = get_atext(value)\n        dot_atom_text.append(token)\n        if value and value[0] == '.':\n            dot_atom_text.append(DOT)\n            value = value[1:]\n    if dot_atom_text[-1] is DOT:\n        raise errors.HeaderParseError(\"expected atom at end of dot-atom-text \"\n            \"but found '{}'\".format('.'+value))\n    return dot_atom_text, value", "response": "Get the dot - text of the given value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_dot_atom(value):\n    dot_atom = DotAtom()\n    if value[0] in CFWS_LEADER:\n        token, value = get_cfws(value)\n        dot_atom.append(token)\n    token, value = get_dot_atom_text(value)\n    dot_atom.append(token)\n    if value and value[0] in CFWS_LEADER:\n        token, value = get_cfws(value)\n        dot_atom.append(token)\n    return dot_atom, value", "response": "Get the dot - atom and the value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the word from the parse tree.", "response": "def get_word(value):\n    \"\"\"word = atom / quoted-string\n\n    Either atom or quoted-string may start with CFWS.  We have to peel off this\n    CFWS first to determine which type of word to parse.  Afterward we splice\n    the leading CFWS, if any, into the parsed sub-token.\n\n    If neither an atom or a quoted-string is found before the next special, a\n    HeaderParseError is raised.\n\n    The token returned is either an Atom or a QuotedString, as appropriate.\n    This means the 'word' level of the formal grammar is not represented in the\n    parse tree; this is because having that extra layer when manipulating the\n    parse tree is more confusing than it is helpful.\n\n    \"\"\"\n    if value[0] in CFWS_LEADER:\n        leader, value = get_cfws(value)\n    else:\n        leader = None\n    if value[0]=='\"':\n        token, value = get_quoted_string(value)\n    elif value[0] in SPECIALS:\n        raise errors.HeaderParseError(\"Expected 'atom' or 'quoted-string' \"\n                                      \"but found '{}'\".format(value))\n    else:\n        token, value = get_atom(value)\n    if leader is not None:\n        token[:0] = [leader]\n    return token, value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_phrase(value):\n    phrase = Phrase()\n    try:\n        token, value = get_word(value)\n        phrase.append(token)\n    except errors.HeaderParseError:\n        phrase.defects.append(errors.InvalidHeaderDefect(\n            \"phrase does not start with word\"))\n    while value and value[0] not in PHRASE_ENDS:\n        if value[0]=='.':\n            phrase.append(DOT)\n            phrase.defects.append(errors.ObsoleteHeaderDefect(\n                \"period in 'phrase'\"))\n            value = value[1:]\n        else:\n            try:\n                token, value = get_word(value)\n            except errors.HeaderParseError:\n                if value[0] in CFWS_LEADER:\n                    token, value = get_cfws(value)\n                    phrase.defects.append(errors.ObsoleteHeaderDefect(\n                        \"comment found without atom\"))\n                else:\n                    raise\n            phrase.append(token)\n    return phrase, value", "response": "Returns a phrase and the value of the value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a single local - part from a value.", "response": "def get_local_part(value):\n    \"\"\" local-part = dot-atom / quoted-string / obs-local-part\n\n    \"\"\"\n    local_part = LocalPart()\n    leader = None\n    if value[0] in CFWS_LEADER:\n        leader, value = get_cfws(value)\n    if not value:\n        raise errors.HeaderParseError(\n            \"expected local-part but found '{}'\".format(value))\n    try:\n        token, value = get_dot_atom(value)\n    except errors.HeaderParseError:\n        try:\n            token, value = get_word(value)\n        except errors.HeaderParseError:\n            if value[0] != '\\\\' and value[0] in PHRASE_ENDS:\n                raise\n            token = TokenList()\n    if leader is not None:\n        token[:0] = [leader]\n    local_part.append(token)\n    if value and (value[0]=='\\\\' or value[0] not in PHRASE_ENDS):\n        obs_local_part, value = get_obs_local_part(str(local_part) + value)\n        if obs_local_part.token_type == 'invalid-obs-local-part':\n            local_part.defects.append(errors.InvalidHeaderDefect(\n                \"local-part is not dot-atom, quoted-string, or obs-local-part\"))\n        else:\n            local_part.defects.append(errors.ObsoleteHeaderDefect(\n                \"local-part is not a dot-atom (contains CFWS)\"))\n        local_part[0] = obs_local_part\n    try:\n        local_part.value.encode('ascii')\n    except UnicodeEncodeError:\n        local_part.defects.append(errors.NonASCIILocalPartDefect(\n                \"local-part contains non-ASCII characters)\"))\n    return local_part, value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_obs_local_part(value):\n    obs_local_part = ObsLocalPart()\n    last_non_ws_was_dot = False\n    while value and (value[0]=='\\\\' or value[0] not in PHRASE_ENDS):\n        if value[0] == '.':\n            if last_non_ws_was_dot:\n                obs_local_part.defects.append(errors.InvalidHeaderDefect(\n                    \"invalid repeated '.'\"))\n            obs_local_part.append(DOT)\n            last_non_ws_was_dot = True\n            value = value[1:]\n            continue\n        elif value[0]=='\\\\':\n            obs_local_part.append(ValueTerminal(value[0],\n                                                'misplaced-special'))\n            value = value[1:]\n            obs_local_part.defects.append(errors.InvalidHeaderDefect(\n                \"'\\\\' character outside of quoted-string/ccontent\"))\n            last_non_ws_was_dot = False\n            continue\n        if obs_local_part and obs_local_part[-1].token_type != 'dot':\n            obs_local_part.defects.append(errors.InvalidHeaderDefect(\n                \"missing '.' between words\"))\n        try:\n            token, value = get_word(value)\n            last_non_ws_was_dot = False\n        except errors.HeaderParseError:\n            if value[0] not in CFWS_LEADER:\n                raise\n            token, value = get_cfws(value)\n        obs_local_part.append(token)\n    if (obs_local_part[0].token_type == 'dot' or\n            obs_local_part[0].token_type=='cfws' and\n            obs_local_part[1].token_type=='dot'):\n        obs_local_part.defects.append(errors.InvalidHeaderDefect(\n            \"Invalid leading '.' in local part\"))\n    if (obs_local_part[-1].token_type == 'dot' or\n            obs_local_part[-1].token_type=='cfws' and\n            obs_local_part[-2].token_type=='dot'):\n        obs_local_part.defects.append(errors.InvalidHeaderDefect(\n            \"Invalid trailing '.' in local part\"))\n    if obs_local_part.defects:\n        obs_local_part.token_type = 'invalid-obs-local-part'\n    return obs_local_part, value", "response": "Parses the value into an ObsLocalPart object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_dtext(value):\n    ptext, value, had_qp = _get_ptext_to_endchars(value, '[]')\n    ptext = ValueTerminal(ptext, 'ptext')\n    if had_qp:\n        ptext.defects.append(errors.ObsoleteHeaderDefect(\n            \"quoted printable found in domain-literal\"))\n    _validate_xtext(ptext)\n    return ptext, value", "response": "Returns a ptext token and value for the given value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_domain_literal(value):\n    domain_literal = DomainLiteral()\n    if value[0] in CFWS_LEADER:\n        token, value = get_cfws(value)\n        domain_literal.append(token)\n    if not value:\n        raise errors.HeaderParseError(\"expected domain-literal\")\n    if value[0] != '[':\n        raise errors.HeaderParseError(\"expected '[' at start of domain-literal \"\n                \"but found '{}'\".format(value))\n    value = value[1:]\n    if _check_for_early_dl_end(value, domain_literal):\n        return domain_literal, value\n    domain_literal.append(ValueTerminal('[', 'domain-literal-start'))\n    if value[0] in WSP:\n        token, value = get_fws(value)\n        domain_literal.append(token)\n    token, value = get_dtext(value)\n    domain_literal.append(token)\n    if _check_for_early_dl_end(value, domain_literal):\n        return domain_literal, value\n    if value[0] in WSP:\n        token, value = get_fws(value)\n        domain_literal.append(token)\n    if _check_for_early_dl_end(value, domain_literal):\n        return domain_literal, value\n    if value[0] != ']':\n        raise errors.HeaderParseError(\"expected ']' at end of domain-literal \"\n                \"but found '{}'\".format(value))\n    domain_literal.append(ValueTerminal(']', 'domain-literal-end'))\n    value = value[1:]\n    if value and value[0] in CFWS_LEADER:\n        token, value = get_cfws(value)\n        domain_literal.append(token)\n    return domain_literal, value", "response": "Parses a domain - literal from a CFWS - formatted string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a domain and value for the given value.", "response": "def get_domain(value):\n    \"\"\" domain = dot-atom / domain-literal / obs-domain\n        obs-domain = atom *(\".\" atom))\n\n    \"\"\"\n    domain = Domain()\n    leader = None\n    if value[0] in CFWS_LEADER:\n        leader, value = get_cfws(value)\n    if not value:\n        raise errors.HeaderParseError(\n            \"expected domain but found '{}'\".format(value))\n    if value[0] == '[':\n        token, value = get_domain_literal(value)\n        if leader is not None:\n            token[:0] = [leader]\n        domain.append(token)\n        return domain, value\n    try:\n        token, value = get_dot_atom(value)\n    except errors.HeaderParseError:\n        token, value = get_atom(value)\n    if leader is not None:\n        token[:0] = [leader]\n    domain.append(token)\n    if value and value[0] == '.':\n        domain.defects.append(errors.ObsoleteHeaderDefect(\n            \"domain is not a dot-atom (contains CFWS)\"))\n        if domain[0].token_type == 'dot-atom':\n            domain[:] = domain[0]\n        while value and value[0] == '.':\n            domain.append(DOT)\n            token, value = get_atom(value[1:])\n            domain.append(token)\n    return domain, value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_addr_spec(value):\n    addr_spec = AddrSpec()\n    token, value = get_local_part(value)\n    addr_spec.append(token)\n    if not value or value[0] != '@':\n        addr_spec.defects.append(errors.InvalidHeaderDefect(\n            \"add-spec local part with no domain\"))\n        return addr_spec, value\n    addr_spec.append(ValueTerminal('@', 'address-at-symbol'))\n    token, value = get_domain(value[1:])\n    addr_spec.append(token)\n    return addr_spec, value", "response": "Get the addr - spec and value for a given value."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses an obs - route from a CFWS - formatted string.", "response": "def get_obs_route(value):\n    \"\"\" obs-route = obs-domain-list \":\"\n        obs-domain-list = *(CFWS / \",\") \"@\" domain *(\",\" [CFWS] [\"@\" domain])\n\n        Returns an obs-route token with the appropriate sub-tokens (that is,\n        there is no obs-domain-list in the parse tree).\n    \"\"\"\n    obs_route = ObsRoute()\n    while value and (value[0]==',' or value[0] in CFWS_LEADER):\n        if value[0] in CFWS_LEADER:\n            token, value = get_cfws(value)\n            obs_route.append(token)\n        elif value[0] == ',':\n            obs_route.append(ListSeparator)\n            value = value[1:]\n    if not value or value[0] != '@':\n        raise errors.HeaderParseError(\n            \"expected obs-route domain but found '{}'\".format(value))\n    obs_route.append(RouteComponentMarker)\n    token, value = get_domain(value[1:])\n    obs_route.append(token)\n    while value and value[0]==',':\n        obs_route.append(ListSeparator)\n        value = value[1:]\n        if not value:\n            break\n        if value[0] in CFWS_LEADER:\n            token, value = get_cfws(value)\n            obs_route.append(token)\n        if value[0] == '@':\n            obs_route.append(RouteComponentMarker)\n            token, value = get_domain(value[1:])\n            obs_route.append(token)\n    if not value:\n        raise errors.HeaderParseError(\"end of header while parsing obs-route\")\n    if value[0] != ':':\n        raise errors.HeaderParseError( \"expected ':' marking end of \"\n            \"obs-route but found '{}'\".format(value))\n    obs_route.append(ValueTerminal(':', 'end-of-obs-route-marker'))\n    return obs_route, value[1:]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_angle_addr(value):\n    angle_addr = AngleAddr()\n    if value[0] in CFWS_LEADER:\n        token, value = get_cfws(value)\n        angle_addr.append(token)\n    if not value or value[0] != '<':\n        raise errors.HeaderParseError(\n            \"expected angle-addr but found '{}'\".format(value))\n    angle_addr.append(ValueTerminal('<', 'angle-addr-start'))\n    value = value[1:]\n    # Although it is not legal per RFC5322, SMTP uses '<>' in certain\n    # circumstances.\n    if value[0] == '>':\n        angle_addr.append(ValueTerminal('>', 'angle-addr-end'))\n        angle_addr.defects.append(errors.InvalidHeaderDefect(\n            \"null addr-spec in angle-addr\"))\n        value = value[1:]\n        return angle_addr, value\n    try:\n        token, value = get_addr_spec(value)\n    except errors.HeaderParseError:\n        try:\n            token, value = get_obs_route(value)\n            angle_addr.defects.append(errors.ObsoleteHeaderDefect(\n                \"obsolete route specification in angle-addr\"))\n        except errors.HeaderParseError:\n            raise errors.HeaderParseError(\n                \"expected addr-spec or obs-route but found '{}'\".format(value))\n        angle_addr.append(token)\n        token, value = get_addr_spec(value)\n    angle_addr.append(token)\n    if value and value[0] == '>':\n        value = value[1:]\n    else:\n        angle_addr.defects.append(errors.InvalidHeaderDefect(\n            \"missing trailing '>' on angle-addr\"))\n    angle_addr.append(ValueTerminal('>', 'angle-addr-end'))\n    if value and value[0] in CFWS_LEADER:\n        token, value = get_cfws(value)\n        angle_addr.append(token)\n    return angle_addr, value", "response": "Parses an angle - addr from the given value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_display_name(value):\n    display_name = DisplayName()\n    token, value = get_phrase(value)\n    display_name.extend(token[:])\n    display_name.defects = token.defects[:]\n    return display_name, value", "response": "Return a display - name token containing the phrase and the value."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_name_addr(value):\n    name_addr = NameAddr()\n    # Both the optional display name and the angle-addr can start with cfws.\n    leader = None\n    if value[0] in CFWS_LEADER:\n        leader, value = get_cfws(value)\n        if not value:\n            raise errors.HeaderParseError(\n                \"expected name-addr but found '{}'\".format(leader))\n    if value[0] != '<':\n        if value[0] in PHRASE_ENDS:\n            raise errors.HeaderParseError(\n                \"expected name-addr but found '{}'\".format(value))\n        token, value = get_display_name(value)\n        if not value:\n            raise errors.HeaderParseError(\n                \"expected name-addr but found '{}'\".format(token))\n        if leader is not None:\n            token[0][:0] = [leader]\n            leader = None\n        name_addr.append(token)\n    token, value = get_angle_addr(value)\n    if leader is not None:\n        token[:0] = [leader]\n    name_addr.append(token)\n    return name_addr, value", "response": "Parses a string into a name - addr and value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a mailbox from a string.", "response": "def get_mailbox(value):\n    \"\"\" mailbox = name-addr / addr-spec\n\n    \"\"\"\n    # The only way to figure out if we are dealing with a name-addr or an\n    # addr-spec is to try parsing each one.\n    mailbox = Mailbox()\n    try:\n        token, value = get_name_addr(value)\n    except errors.HeaderParseError:\n        try:\n            token, value = get_addr_spec(value)\n        except errors.HeaderParseError:\n            raise errors.HeaderParseError(\n                \"expected mailbox but found '{}'\".format(value))\n    if any(isinstance(x, errors.InvalidHeaderDefect)\n                       for x in token.all_defects):\n        mailbox.token_type = 'invalid-mailbox'\n    mailbox.append(token)\n    return mailbox, value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_invalid_mailbox(value, endchars):\n    invalid_mailbox = InvalidMailbox()\n    while value and value[0] not in endchars:\n        if value[0] in PHRASE_ENDS:\n            invalid_mailbox.append(ValueTerminal(value[0],\n                                                 'misplaced-special'))\n            value = value[1:]\n        else:\n            token, value = get_phrase(value)\n            invalid_mailbox.append(token)\n    return invalid_mailbox, value", "response": "Reads the value and returns an InvalidMailbox TokenList that is not a Mailbox."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_mailbox_list(value):\n    mailbox_list = MailboxList()\n    while value and value[0] != ';':\n        try:\n            token, value = get_mailbox(value)\n            mailbox_list.append(token)\n        except errors.HeaderParseError:\n            leader = None\n            if value[0] in CFWS_LEADER:\n                leader, value = get_cfws(value)\n                if not value or value[0] in ',;':\n                    mailbox_list.append(leader)\n                    mailbox_list.defects.append(errors.ObsoleteHeaderDefect(\n                        \"empty element in mailbox-list\"))\n                else:\n                    token, value = get_invalid_mailbox(value, ',;')\n                    if leader is not None:\n                        token[:0] = [leader]\n                    mailbox_list.append(token)\n                    mailbox_list.defects.append(errors.InvalidHeaderDefect(\n                        \"invalid mailbox in mailbox-list\"))\n            elif value[0] == ',':\n                mailbox_list.defects.append(errors.ObsoleteHeaderDefect(\n                    \"empty element in mailbox-list\"))\n            else:\n                token, value = get_invalid_mailbox(value, ',;')\n                if leader is not None:\n                    token[:0] = [leader]\n                mailbox_list.append(token)\n                mailbox_list.defects.append(errors.InvalidHeaderDefect(\n                    \"invalid mailbox in mailbox-list\"))\n        if value and value[0] not in ',;':\n            # Crap after mailbox; treat it as an invalid mailbox.\n            # The mailbox info will still be available.\n            mailbox = mailbox_list[-1]\n            mailbox.token_type = 'invalid-mailbox'\n            token, value = get_invalid_mailbox(value, ',;')\n            mailbox.extend(token)\n            mailbox_list.defects.append(errors.InvalidHeaderDefect(\n                \"invalid mailbox in mailbox-list\"))\n        if value and value[0] == ',':\n            mailbox_list.append(ListSeparator)\n            value = value[1:]\n    return mailbox_list, value", "response": "Parses the mailbox list from the given value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the value into a group - list.", "response": "def get_group_list(value):\n    \"\"\" group-list = mailbox-list / CFWS / obs-group-list\n        obs-group-list = 1*([CFWS] \",\") [CFWS]\n\n    \"\"\"\n    group_list = GroupList()\n    if not value:\n        group_list.defects.append(errors.InvalidHeaderDefect(\n            \"end of header before group-list\"))\n        return group_list, value\n    leader = None\n    if value and value[0] in CFWS_LEADER:\n        leader, value = get_cfws(value)\n        if not value:\n            # This should never happen in email parsing, since CFWS-only is a\n            # legal alternative to group-list in a group, which is the only\n            # place group-list appears.\n            group_list.defects.append(errors.InvalidHeaderDefect(\n                \"end of header in group-list\"))\n            group_list.append(leader)\n            return group_list, value\n        if value[0] == ';':\n            group_list.append(leader)\n            return group_list, value\n    token, value = get_mailbox_list(value)\n    if len(token.all_mailboxes)==0:\n        if leader is not None:\n            group_list.append(leader)\n        group_list.extend(token)\n        group_list.defects.append(errors.ObsoleteHeaderDefect(\n            \"group-list with empty entries\"))\n        return group_list, value\n    if leader is not None:\n        token[:0] = [leader]\n    group_list.append(token)\n    return group_list, value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a header group and returns it as a tuple.", "response": "def get_group(value):\n    \"\"\" group = display-name \":\" [group-list] \";\" [CFWS]\n\n    \"\"\"\n    group = Group()\n    token, value = get_display_name(value)\n    if not value or value[0] != ':':\n        raise errors.HeaderParseError(\"expected ':' at end of group \"\n            \"display name but found '{}'\".format(value))\n    group.append(token)\n    group.append(ValueTerminal(':', 'group-display-name-terminator'))\n    value = value[1:]\n    if value and value[0] == ';':\n        group.append(ValueTerminal(';', 'group-terminator'))\n        return group, value[1:]\n    token, value = get_group_list(value)\n    group.append(token)\n    if not value:\n        group.defects.append(errors.InvalidHeaderDefect(\n            \"end of header in group\"))\n    if value[0] != ';':\n        raise errors.HeaderParseError(\n            \"expected ';' at end of group but found {}\".format(value))\n    group.append(ValueTerminal(';', 'group-terminator'))\n    value = value[1:]\n    if value and value[0] in CFWS_LEADER:\n        token, value = get_cfws(value)\n        group.append(token)\n    return group, value"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse an address from the mailbox or group token.", "response": "def get_address(value):\n    \"\"\" address = mailbox / group\n\n    Note that counter-intuitively, an address can be either a single address or\n    a list of addresses (a group).  This is why the returned Address object has\n    a 'mailboxes' attribute which treats a single address as a list of length\n    one.  When you need to differentiate between to two cases, extract the single\n    element, which is either a mailbox or a group token.\n\n    \"\"\"\n    # The formal grammar isn't very helpful when parsing an address.  mailbox\n    # and group, especially when allowing for obsolete forms, start off very\n    # similarly.  It is only when you reach one of @, <, or : that you know\n    # what you've got.  So, we try each one in turn, starting with the more\n    # likely of the two.  We could perhaps make this more efficient by looking\n    # for a phrase and then branching based on the next character, but that\n    # would be a premature optimization.\n    address = Address()\n    try:\n        token, value = get_group(value)\n    except errors.HeaderParseError:\n        try:\n            token, value = get_mailbox(value)\n        except errors.HeaderParseError:\n            raise errors.HeaderParseError(\n                \"expected address but found '{}'\".format(value))\n    address.append(token)\n    return address, value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the input string into a list of addresses.", "response": "def get_address_list(value):\n    \"\"\" address_list = (address *(\",\" address)) / obs-addr-list\n        obs-addr-list = *([CFWS] \",\") address *(\",\" [address / CFWS])\n\n    We depart from the formal grammar here by continuing to parse until the end\n    of the input, assuming the input to be entirely composed of an\n    address-list.  This is always true in email parsing, and allows us\n    to skip invalid addresses to parse additional valid ones.\n\n    \"\"\"\n    address_list = AddressList()\n    while value:\n        try:\n            token, value = get_address(value)\n            address_list.append(token)\n        except errors.HeaderParseError as err:\n            leader = None\n            if value[0] in CFWS_LEADER:\n                leader, value = get_cfws(value)\n                if not value or value[0] == ',':\n                    address_list.append(leader)\n                    address_list.defects.append(errors.ObsoleteHeaderDefect(\n                        \"address-list entry with no content\"))\n                else:\n                    token, value = get_invalid_mailbox(value, ',')\n                    if leader is not None:\n                        token[:0] = [leader]\n                    address_list.append(Address([token]))\n                    address_list.defects.append(errors.InvalidHeaderDefect(\n                        \"invalid address in address-list\"))\n            elif value[0] == ',':\n                address_list.defects.append(errors.ObsoleteHeaderDefect(\n                    \"empty element in address-list\"))\n            else:\n                token, value = get_invalid_mailbox(value, ',')\n                if leader is not None:\n                    token[:0] = [leader]\n                address_list.append(Address([token]))\n                address_list.defects.append(errors.InvalidHeaderDefect(\n                    \"invalid address in address-list\"))\n        if value and value[0] != ',':\n            # Crap after address; treat it as an invalid mailbox.\n            # The mailbox info will still be available.\n            mailbox = address_list[-1][0]\n            mailbox.token_type = 'invalid-mailbox'\n            token, value = get_invalid_mailbox(value, ',')\n            mailbox.extend(token)\n            address_list.defects.append(errors.InvalidHeaderDefect(\n                \"invalid address in address-list\"))\n        if value:  # Must be a , at this point.\n            address_list.append(ValueTerminal(',', 'list-separator'))\n            value = value[1:]\n    return address_list, value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_mime_version(value):\n    # The [CFWS] is implicit in the RFC 2045 BNF.\n    # XXX: This routine is a bit verbose, should factor out a get_int method.\n    mime_version = MIMEVersion()\n    if not value:\n        mime_version.defects.append(errors.HeaderMissingRequiredValue(\n            \"Missing MIME version number (eg: 1.0)\"))\n        return mime_version\n    if value[0] in CFWS_LEADER:\n        token, value = get_cfws(value)\n        mime_version.append(token)\n        if not value:\n            mime_version.defects.append(errors.HeaderMissingRequiredValue(\n                \"Expected MIME version number but found only CFWS\"))\n    digits = ''\n    while value and value[0] != '.' and value[0] not in CFWS_LEADER:\n        digits += value[0]\n        value = value[1:]\n    if not digits.isdigit():\n        mime_version.defects.append(errors.InvalidHeaderDefect(\n            \"Expected MIME major version number but found {!r}\".format(digits)))\n        mime_version.append(ValueTerminal(digits, 'xtext'))\n    else:\n        mime_version.major = int(digits)\n        mime_version.append(ValueTerminal(digits, 'digits'))\n    if value and value[0] in CFWS_LEADER:\n        token, value = get_cfws(value)\n        mime_version.append(token)\n    if not value or value[0] != '.':\n        if mime_version.major is not None:\n            mime_version.defects.append(errors.InvalidHeaderDefect(\n                \"Incomplete MIME version; found only major number\"))\n        if value:\n            mime_version.append(ValueTerminal(value, 'xtext'))\n        return mime_version\n    mime_version.append(ValueTerminal('.', 'version-separator'))\n    value = value[1:]\n    if value and value[0] in CFWS_LEADER:\n        token, value = get_cfws(value)\n        mime_version.append(token)\n    if not value:\n        if mime_version.major is not None:\n            mime_version.defects.append(errors.InvalidHeaderDefect(\n                \"Incomplete MIME version; found only major number\"))\n        return mime_version\n    digits = ''\n    while value and value[0] not in CFWS_LEADER:\n        digits += value[0]\n        value = value[1:]\n    if not digits.isdigit():\n        mime_version.defects.append(errors.InvalidHeaderDefect(\n            \"Expected MIME minor version number but found {!r}\".format(digits)))\n        mime_version.append(ValueTerminal(digits, 'xtext'))\n    else:\n        mime_version.minor = int(digits)\n        mime_version.append(ValueTerminal(digits, 'digits'))\n    if value and value[0] in CFWS_LEADER:\n        token, value = get_cfws(value)\n        mime_version.append(token)\n    if value:\n        mime_version.defects.append(errors.InvalidHeaderDefect(\n            \"Excess non-CFWS text after MIME version\"))\n        mime_version.append(ValueTerminal(value, 'xtext'))\n    return mime_version", "response": "Parses a MIME version header."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_invalid_parameter(value):\n    invalid_parameter = InvalidParameter()\n    while value and value[0] != ';':\n        if value[0] in PHRASE_ENDS:\n            invalid_parameter.append(ValueTerminal(value[0],\n                                                   'misplaced-special'))\n            value = value[1:]\n        else:\n            token, value = get_phrase(value)\n            invalid_parameter.append(token)\n    return invalid_parameter, value", "response": "Reads the next ; and returns a InvalidParameter TokenList that is not a Parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_ttext(value):\n    m = _non_token_end_matcher(value)\n    if not m:\n        raise errors.HeaderParseError(\n            \"expected ttext but found '{}'\".format(value))\n    ttext = m.group()\n    value = value[len(ttext):]\n    ttext = ValueTerminal(ttext, 'ttext')\n    _validate_xtext(ttext)\n    return ttext, value", "response": "Get the ttext and value from a value."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a Token object and the value.", "response": "def get_token(value):\n    \"\"\"token = [CFWS] 1*ttext [CFWS]\n\n    The RFC equivalent of ttext is any US-ASCII chars except space, ctls, or\n    tspecials.  We also exclude tabs even though the RFC doesn't.\n\n    The RFC implies the CFWS but is not explicit about it in the BNF.\n\n    \"\"\"\n    mtoken = Token()\n    if value and value[0] in CFWS_LEADER:\n        token, value = get_cfws(value)\n        mtoken.append(token)\n    if value and value[0] in TOKEN_ENDS:\n        raise errors.HeaderParseError(\n            \"expected token but found '{}'\".format(value))\n    token, value = get_ttext(value)\n    mtoken.append(token)\n    if value and value[0] in CFWS_LEADER:\n        token, value = get_cfws(value)\n        mtoken.append(token)\n    return mtoken, value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_attrtext(value):\n    m = _non_attribute_end_matcher(value)\n    if not m:\n        raise errors.HeaderParseError(\n            \"expected attrtext but found {!r}\".format(value))\n    attrtext = m.group()\n    value = value[len(attrtext):]\n    attrtext = ValueTerminal(attrtext, 'attrtext')\n    _validate_xtext(attrtext)\n    return attrtext, value", "response": "Get the attrtext and value from the value."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_attribute(value):\n    attribute = Attribute()\n    if value and value[0] in CFWS_LEADER:\n        token, value = get_cfws(value)\n        attribute.append(token)\n    if value and value[0] in ATTRIBUTE_ENDS:\n        raise errors.HeaderParseError(\n            \"expected token but found '{}'\".format(value))\n    token, value = get_attrtext(value)\n    attribute.append(token)\n    if value and value[0] in CFWS_LEADER:\n        token, value = get_cfws(value)\n        attribute.append(token)\n    return attribute, value", "response": "Get the attribute from the given value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_extended_attrtext(value):\n    m = _non_extended_attribute_end_matcher(value)\n    if not m:\n        raise errors.HeaderParseError(\n            \"expected extended attrtext but found {!r}\".format(value))\n    attrtext = m.group()\n    value = value[len(attrtext):]\n    attrtext = ValueTerminal(attrtext, 'extended-attrtext')\n    _validate_xtext(attrtext)\n    return attrtext, value", "response": "Parses the extended - attrtext value."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the extended attribute from the given value.", "response": "def get_extended_attribute(value):\n    \"\"\" [CFWS] 1*extended_attrtext [CFWS]\n\n    This is like the non-extended version except we allow % characters, so that\n    we can pick up an encoded value as a single string.\n\n    \"\"\"\n    # XXX: should we have an ExtendedAttribute TokenList?\n    attribute = Attribute()\n    if value and value[0] in CFWS_LEADER:\n        token, value = get_cfws(value)\n        attribute.append(token)\n    if value and value[0] in EXTENDED_ATTRIBUTE_ENDS:\n        raise errors.HeaderParseError(\n            \"expected token but found '{}'\".format(value))\n    token, value = get_extended_attrtext(value)\n    attribute.append(token)\n    if value and value[0] in CFWS_LEADER:\n        token, value = get_cfws(value)\n        attribute.append(token)\n    return attribute, value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_section(value):\n    section = Section()\n    if not value or value[0] != '*':\n        raise errors.HeaderParseError(\"Expected section but found {}\".format(\n                                        value))\n    section.append(ValueTerminal('*', 'section-marker'))\n    value = value[1:]\n    if not value or not value[0].isdigit():\n        raise errors.HeaderParseError(\"Expected section number but \"\n                                      \"found {}\".format(value))\n    digits = ''\n    while value and value[0].isdigit():\n        digits += value[0]\n        value = value[1:]\n    if digits[0] == '0' and digits != '0':\n        section.defects.append(errors.InvalidHeaderError(\"section number\"\n            \"has an invalid leading 0\"))\n    section.number = int(digits)\n    section.append(ValueTerminal(digits, 'digits'))\n    return section, value", "response": "Returns a section and a value."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the value of a resource header.", "response": "def get_value(value):\n    \"\"\" quoted-string / attribute\n\n    \"\"\"\n    v = Value()\n    if not value:\n        raise errors.HeaderParseError(\"Expected value but found end of string\")\n    leader = None\n    if value[0] in CFWS_LEADER:\n        leader, value = get_cfws(value)\n    if not value:\n        raise errors.HeaderParseError(\"Expected value but found \"\n                                      \"only {}\".format(leader))\n    if value[0] == '\"':\n        token, value = get_quoted_string(value)\n    else:\n        token, value = get_extended_attribute(value)\n    if leader is not None:\n        token[:0] = [leader]\n    v.append(token)\n    return v, value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_parameter(value):\n    # It is possible CFWS would also be implicitly allowed between the section\n    # and the 'extended-attribute' marker (the '*') , but we've never seen that\n    # in the wild and we will therefore ignore the possibility.\n    param = Parameter()\n    token, value = get_attribute(value)\n    param.append(token)\n    if not value or value[0] == ';':\n        param.defects.append(errors.InvalidHeaderDefect(\"Parameter contains \"\n            \"name ({}) but no value\".format(token)))\n        return param, value\n    if value[0] == '*':\n        try:\n            token, value = get_section(value)\n            param.sectioned = True\n            param.append(token)\n        except errors.HeaderParseError:\n            pass\n        if not value:\n            raise errors.HeaderParseError(\"Incomplete parameter\")\n        if value[0] == '*':\n            param.append(ValueTerminal('*', 'extended-parameter-marker'))\n            value = value[1:]\n            param.extended = True\n    if value[0] != '=':\n        raise errors.HeaderParseError(\"Parameter not followed by '='\")\n    param.append(ValueTerminal('=', 'parameter-separator'))\n    value = value[1:]\n    leader = None\n    if value and value[0] in CFWS_LEADER:\n        token, value = get_cfws(value)\n        param.append(token)\n    remainder = None\n    appendto = param\n    if param.extended and value and value[0] == '\"':\n        # Now for some serious hackery to handle the common invalid case of\n        # double quotes around an extended value.  We also accept (with defect)\n        # a value marked as encoded that isn't really.\n        qstring, remainder = get_quoted_string(value)\n        inner_value = qstring.stripped_value\n        semi_valid = False\n        if param.section_number == 0:\n            if inner_value and inner_value[0] == \"'\":\n                semi_valid = True\n            else:\n                token, rest = get_attrtext(inner_value)\n                if rest and rest[0] == \"'\":\n                    semi_valid = True\n        else:\n            try:\n                token, rest = get_extended_attrtext(inner_value)\n            except:\n                pass\n            else:\n                if not rest:\n                    semi_valid = True\n        if semi_valid:\n            param.defects.append(errors.InvalidHeaderDefect(\n                \"Quoted string value for extended parameter is invalid\"))\n            param.append(qstring)\n            for t in qstring:\n                if t.token_type == 'bare-quoted-string':\n                    t[:] = []\n                    appendto = t\n                    break\n            value = inner_value\n        else:\n            remainder = None\n            param.defects.append(errors.InvalidHeaderDefect(\n                \"Parameter marked as extended but appears to have a \"\n                \"quoted string value that is non-encoded\"))\n    if value and value[0] == \"'\":\n        token = None\n    else:\n        token, value = get_value(value)\n    if not param.extended or param.section_number > 0:\n        if not value or value[0] != \"'\":\n            appendto.append(token)\n            if remainder is not None:\n                assert not value, value\n                value = remainder\n            return param, value\n        param.defects.append(errors.InvalidHeaderDefect(\n            \"Apparent initial-extended-value but attribute \"\n            \"was not marked as extended or was not initial section\"))\n    if not value:\n        # Assume the charset/lang is missing and the token is the value.\n        param.defects.append(errors.InvalidHeaderDefect(\n            \"Missing required charset/lang delimiters\"))\n        appendto.append(token)\n        if remainder is None:\n            return param, value\n    else:\n        if token is not None:\n            for t in token:\n                if t.token_type == 'extended-attrtext':\n                    break\n            t.token_type == 'attrtext'\n            appendto.append(t)\n            param.charset = t.value\n        if value[0] != \"'\":\n            raise errors.HeaderParseError(\"Expected RFC2231 char/lang encoding \"\n                                          \"delimiter, but found {!r}\".format(value))\n        appendto.append(ValueTerminal(\"'\", 'RFC2231 delimiter'))\n        value = value[1:]\n        if value and value[0] != \"'\":\n            token, value = get_attrtext(value)\n            appendto.append(token)\n            param.lang = token.value\n            if not value or value[0] != \"'\":\n                raise errors.HeaderParseError(\"Expected RFC2231 char/lang encoding \"\n                                  \"delimiter, but found {}\".format(value))\n        appendto.append(ValueTerminal(\"'\", 'RFC2231 delimiter'))\n        value = value[1:]\n    if remainder is not None:\n        # Treat the rest of value as bare quoted string content.\n        v = Value()\n        while value:\n            if value[0] in WSP:\n                token, value = get_fws(value)\n            else:\n                token, value = get_qcontent(value)\n            v.append(token)\n        token = v\n    else:\n        token, value = get_value(value)\n    appendto.append(token)\n    if remainder is not None:\n        assert not value, value\n        value = remainder\n    return param, value", "response": "Returns a Parameter object and the value of the value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the MIME header value into a TokenList.", "response": "def parse_mime_parameters(value):\n    \"\"\" parameter *( \";\" parameter )\n\n    That BNF is meant to indicate this routine should only be called after\n    finding and handling the leading ';'.  There is no corresponding rule in\n    the formal RFC grammar, but it is more convenient for us for the set of\n    parameters to be treated as its own TokenList.\n\n    This is 'parse' routine because it consumes the reminaing value, but it\n    would never be called to parse a full header.  Instead it is called to\n    parse everything after the non-parameter value of a specific MIME header.\n\n    \"\"\"\n    mime_parameters = MimeParameters()\n    while value:\n        try:\n            token, value = get_parameter(value)\n            mime_parameters.append(token)\n        except errors.HeaderParseError as err:\n            leader = None\n            if value[0] in CFWS_LEADER:\n                leader, value = get_cfws(value)\n            if not value:\n                mime_parameters.append(leader)\n                return mime_parameters\n            if value[0] == ';':\n                if leader is not None:\n                    mime_parameters.append(leader)\n                mime_parameters.defects.append(errors.InvalidHeaderDefect(\n                    \"parameter entry with no content\"))\n            else:\n                token, value = get_invalid_parameter(value)\n                if leader:\n                    token[:0] = [leader]\n                mime_parameters.append(token)\n                mime_parameters.defects.append(errors.InvalidHeaderDefect(\n                    \"invalid parameter {!r}\".format(token)))\n        if value and value[0] != ';':\n            # Junk after the otherwise valid parameter.  Mark it as\n            # invalid, but it will have a value.\n            param = mime_parameters[-1]\n            param.token_type = 'invalid-parameter'\n            token, value = get_invalid_parameter(value)\n            param.extend(token)\n            mime_parameters.defects.append(errors.InvalidHeaderDefect(\n                \"parameter with invalid trailing text {!r}\".format(token)))\n        if value:\n            # Must be a ';' at this point.\n            mime_parameters.append(ValueTerminal(';', 'parameter-separator'))\n            value = value[1:]\n    return mime_parameters"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the parameters in an invalid MIME header.", "response": "def _find_mime_parameters(tokenlist, value):\n    \"\"\"Do our best to find the parameters in an invalid MIME header\n\n    \"\"\"\n    while value and value[0] != ';':\n        if value[0] in PHRASE_ENDS:\n            tokenlist.append(ValueTerminal(value[0], 'misplaced-special'))\n            value = value[1:]\n        else:\n            token, value = get_phrase(value)\n            tokenlist.append(token)\n    if not value:\n        return\n    tokenlist.append(ValueTerminal(';', 'parameter-separator'))\n    tokenlist.append(parse_mime_parameters(value[1:]))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_content_type_header(value):\n    ctype = ContentType()\n    recover = False\n    if not value:\n        ctype.defects.append(errors.HeaderMissingRequiredValue(\n            \"Missing content type specification\"))\n        return ctype\n    try:\n        token, value = get_token(value)\n    except errors.HeaderParseError:\n        ctype.defects.append(errors.InvalidHeaderDefect(\n            \"Expected content maintype but found {!r}\".format(value)))\n        _find_mime_parameters(ctype, value)\n        return ctype\n    ctype.append(token)\n    # XXX: If we really want to follow the formal grammer we should make\n    # mantype and subtype specialized TokenLists here.  Probably not worth it.\n    if not value or value[0] != '/':\n        ctype.defects.append(errors.InvalidHeaderDefect(\n            \"Invalid content type\"))\n        if value:\n            _find_mime_parameters(ctype, value)\n        return ctype\n    ctype.maintype = token.value.strip().lower()\n    ctype.append(ValueTerminal('/', 'content-type-separator'))\n    value = value[1:]\n    try:\n        token, value = get_token(value)\n    except errors.HeaderParseError:\n        ctype.defects.append(errors.InvalidHeaderDefect(\n            \"Expected content subtype but found {!r}\".format(value)))\n        _find_mime_parameters(ctype, value)\n        return ctype\n    ctype.append(token)\n    ctype.subtype = token.value.strip().lower()\n    if not value:\n        return ctype\n    if value[0] != ';':\n        ctype.defects.append(errors.InvalidHeaderDefect(\n            \"Only parameters are valid after content type, but \"\n            \"found {!r}\".format(value)))\n        # The RFC requires that a syntactically invalid content-type be treated\n        # as text/plain.  Perhaps we should postel this, but we should probably\n        # only do that if we were checking the subtype value against IANA.\n        del ctype.maintype, ctype.subtype\n        _find_mime_parameters(ctype, value)\n        return ctype\n    ctype.append(ValueTerminal(';', 'parameter-separator'))\n    ctype.append(parse_mime_parameters(value[1:]))\n    return ctype", "response": "Parses the content - type header."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_content_disposition_header(value):\n    disp_header = ContentDisposition()\n    if not value:\n        disp_header.defects.append(errors.HeaderMissingRequiredValue(\n            \"Missing content disposition\"))\n        return disp_header\n    try:\n        token, value = get_token(value)\n    except errors.HeaderParseError:\n        ctype.defects.append(errors.InvalidHeaderDefect(\n            \"Expected content disposition but found {!r}\".format(value)))\n        _find_mime_parameters(disp_header, value)\n        return disp_header\n    disp_header.append(token)\n    disp_header.content_disposition = token.value.strip().lower()\n    if not value:\n        return disp_header\n    if value[0] != ';':\n        disp_header.defects.append(errors.InvalidHeaderDefect(\n            \"Only parameters are valid after content disposition, but \"\n            \"found {!r}\".format(value)))\n        _find_mime_parameters(disp_header, value)\n        return disp_header\n    disp_header.append(ValueTerminal(';', 'parameter-separator'))\n    disp_header.append(parse_mime_parameters(value[1:]))\n    return disp_header", "response": "Parses a Content - Disposition header."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_content_transfer_encoding_header(value):\n    # We should probably validate the values, since the list is fixed.\n    cte_header = ContentTransferEncoding()\n    if not value:\n        cte_header.defects.append(errors.HeaderMissingRequiredValue(\n            \"Missing content transfer encoding\"))\n        return cte_header\n    try:\n        token, value = get_token(value)\n    except errors.HeaderParseError:\n        ctype.defects.append(errors.InvalidHeaderDefect(\n            \"Expected content trnasfer encoding but found {!r}\".format(value)))\n    else:\n        cte_header.append(token)\n        cte_header.cte = token.value.strip().lower()\n    if not value:\n        return cte_header\n    while value:\n        cte_header.defects.append(errors.InvalidHeaderDefect(\n            \"Extra text after content transfer encoding\"))\n        if value[0] in PHRASE_ENDS:\n            cte_header.append(ValueTerminal(value[0], 'misplaced-special'))\n            value = value[1:]\n        else:\n            token, value = get_phrase(value)\n            cte_header.append(token)\n    return cte_header", "response": "Parses a content transfer encoding header."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the length of s when it is encoded with base64.", "response": "def header_length(bytearray):\n    \"\"\"Return the length of s when it is encoded with base64.\"\"\"\n    groups_of_3, leftover = divmod(len(bytearray), 3)\n    # 4 bytes out for each 3 bytes (or nonzero fraction thereof) in.\n    n = groups_of_3 * 4\n    if leftover:\n        n += 4\n    return n"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef header_encode(header_bytes, charset='iso-8859-1'):\n    if not header_bytes:\n        return \"\"\n    if isinstance(header_bytes, str):\n        header_bytes = header_bytes.encode(charset)\n    encoded = b64encode(header_bytes).decode(\"ascii\")\n    return '=?%s?b?%s?=' % (charset, encoded)", "response": "Encode a single header line with Base64 encoding in a given charset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef body_encode(s, maxlinelen=76, eol=NL):\n    if not s:\n        return s\n\n    encvec = []\n    max_unencoded = maxlinelen * 3 // 4\n    for i in range(0, len(s), max_unencoded):\n        # BAW: should encode() inherit b2a_base64()'s dubious behavior in\n        # adding a newline to the encoded string?\n        enc = b2a_base64(s[i:i + max_unencoded]).decode(\"ascii\")\n        if enc.endswith(NL) and eol != NL:\n            enc = enc[:-1] + eol\n        encvec.append(enc)\n    return EMPTYSTRING.join(encvec)", "response": "r Encode a string with base64."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndecode a raw base64 string returning a bytes object.", "response": "def decode(string):\n    \"\"\"Decode a raw base64 string, returning a bytes object.\n\n    This function does not parse a full MIME header value encoded with\n    base64 (like =?iso-8895-1?b?bmloISBuaWgh?=) -- please use the high\n    level email.header class for that functionality.\n    \"\"\"\n    if not string:\n        return bytes()\n    elif isinstance(string, str):\n        return a2b_base64(string.encode('raw-unicode-escape'))\n    else:\n        return a2b_base64(string)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef urlparse(url, scheme='', allow_fragments=True):\n    url, scheme, _coerce_result = _coerce_args(url, scheme)\n    splitresult = urlsplit(url, scheme, allow_fragments)\n    scheme, netloc, url, query, fragment = splitresult\n    if scheme in uses_params and ';' in url:\n        url, params = _splitparams(url)\n    else:\n        params = ''\n    result = ParseResult(scheme, netloc, url, params, query, fragment)\n    return _coerce_result(result)", "response": "Parse a URL into 6 components."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef urlunparse(components):\n    scheme, netloc, url, params, query, fragment, _coerce_result = (\n                                                  _coerce_args(*components))\n    if params:\n        url = \"%s;%s\" % (url, params)\n    return _coerce_result(urlunsplit((scheme, netloc, url, query, fragment)))", "response": "Put a parsed URL back together."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef urlunsplit(components):\n    scheme, netloc, url, query, fragment, _coerce_result = (\n                                          _coerce_args(*components))\n    if netloc or (scheme and scheme in uses_netloc and url[:2] != '//'):\n        if url and url[:1] != '/': url = '/' + url\n        url = '//' + (netloc or '') + url\n    if scheme:\n        url = scheme + ':' + url\n    if query:\n        url = url + '?' + query\n    if fragment:\n        url = url + '#' + fragment\n    return _coerce_result(url)", "response": "Combine the elements of a tuple as returned by urlsplit into a complete URL as a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove any existing fragment from the URL.", "response": "def urldefrag(url):\n    \"\"\"Removes any existing fragment from URL.\n\n    Returns a tuple of the defragmented URL and the fragment.  If\n    the URL contained no fragments, the second element is the\n    empty string.\n    \"\"\"\n    url, _coerce_result = _coerce_args(url)\n    if '#' in url:\n        s, n, p, a, q, frag = urlparse(url)\n        defrag = urlunparse((s, n, p, a, q, ''))\n    else:\n        frag = ''\n        defrag = url\n    return _coerce_result(DefragResult(defrag, frag))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unquote(string, encoding='utf-8', errors='replace'):\n    if '%' not in string:\n        string.split\n        return string\n    if encoding is None:\n        encoding = 'utf-8'\n    if errors is None:\n        errors = 'replace'\n    bits = _asciire.split(string)\n    res = [bits[0]]\n    append = res.append\n    for i in range(1, len(bits), 2):\n        append(unquote_to_bytes(bits[i]).decode(encoding, errors))\n        append(bits[i + 1])\n    return ''.join(res)", "response": "Unquote a string with percent - encoded characters."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_qs(qs, keep_blank_values=False, strict_parsing=False,\n             encoding='utf-8', errors='replace'):\n    \"\"\"Parse a query given as a string argument.\n\n        Arguments:\n\n        qs: percent-encoded query string to be parsed\n\n        keep_blank_values: flag indicating whether blank values in\n            percent-encoded queries should be treated as blank strings.\n            A true value indicates that blanks should be retained as\n            blank strings.  The default false value indicates that\n            blank values are to be ignored and treated as if they were\n            not included.\n\n        strict_parsing: flag indicating what to do with parsing errors.\n            If false (the default), errors are silently ignored.\n            If true, errors raise a ValueError exception.\n\n        encoding and errors: specify how to decode percent-encoded sequences\n            into Unicode characters, as accepted by the bytes.decode() method.\n    \"\"\"\n    parsed_result = {}\n    pairs = parse_qsl(qs, keep_blank_values, strict_parsing,\n                      encoding=encoding, errors=errors)\n    for name, value in pairs:\n        if name in parsed_result:\n            parsed_result[name].append(value)\n        else:\n            parsed_result[name] = [value]\n    return parsed_result", "response": "Parse a query string given as a string argument."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a percent - encoded query string into a list of tuples.", "response": "def parse_qsl(qs, keep_blank_values=False, strict_parsing=False,\n              encoding='utf-8', errors='replace'):\n    \"\"\"Parse a query given as a string argument.\n\n    Arguments:\n\n    qs: percent-encoded query string to be parsed\n\n    keep_blank_values: flag indicating whether blank values in\n        percent-encoded queries should be treated as blank strings.  A\n        true value indicates that blanks should be retained as blank\n        strings.  The default false value indicates that blank values\n        are to be ignored and treated as if they were  not included.\n\n    strict_parsing: flag indicating what to do with parsing errors. If\n        false (the default), errors are silently ignored. If true,\n        errors raise a ValueError exception.\n\n    encoding and errors: specify how to decode percent-encoded sequences\n        into Unicode characters, as accepted by the bytes.decode() method.\n\n    Returns a list, as G-d intended.\n    \"\"\"\n    qs, _coerce_result = _coerce_args(qs)\n    pairs = [s2 for s1 in qs.split('&') for s2 in s1.split(';')]\n    r = []\n    for name_value in pairs:\n        if not name_value and not strict_parsing:\n            continue\n        nv = name_value.split('=', 1)\n        if len(nv) != 2:\n            if strict_parsing:\n                raise ValueError(\"bad query field: %r\" % (name_value,))\n            # Handle case of a control-name with no equal sign\n            if keep_blank_values:\n                nv.append('')\n            else:\n                continue\n        if len(nv[1]) or keep_blank_values:\n            name = nv[0].replace('+', ' ')\n            name = unquote(name, encoding=encoding, errors=errors)\n            name = _coerce_result(name)\n            value = nv[1].replace('+', ' ')\n            value = unquote(value, encoding=encoding, errors=errors)\n            value = _coerce_result(value)\n            r.append((name, value))\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlikes unquote but also replaces plus signs by spaces.", "response": "def unquote_plus(string, encoding='utf-8', errors='replace'):\n    \"\"\"Like unquote(), but also replace plus signs by spaces, as required for\n    unquoting HTML form values.\n\n    unquote_plus('%7e/abc+def') -> '~/abc def'\n    \"\"\"\n    string = string.replace('+', ' ')\n    return unquote(string, encoding, errors)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef quote(string, safe='/', encoding=None, errors=None):\n    if isinstance(string, str):\n        if not string:\n            return string\n        if encoding is None:\n            encoding = 'utf-8'\n        if errors is None:\n            errors = 'strict'\n        string = string.encode(encoding, errors)\n    else:\n        if encoding is not None:\n            raise TypeError(\"quote() doesn't support 'encoding' for bytes\")\n        if errors is not None:\n            raise TypeError(\"quote() doesn't support 'errors' for bytes\")\n    return quote_from_bytes(string, safe)", "response": "Quotes a string into a reserved character in a resource definition section."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nliking quote but also replaces spaces with +.", "response": "def quote_plus(string, safe='', encoding=None, errors=None):\n    \"\"\"Like quote(), but also replace ' ' with '+', as required for quoting\n    HTML form values. Plus signs in the original string are escaped unless\n    they are included in safe. It also does not have safe default to '/'.\n    \"\"\"\n    # Check if ' ' in string, where string may either be a str or bytes.  If\n    # there are no spaces, the regular quote will produce the right answer.\n    if ((isinstance(string, str) and ' ' not in string) or\n        (isinstance(string, bytes) and b' ' not in string)):\n        return quote(string, safe, encoding, errors)\n    if isinstance(safe, str):\n        space = str(' ')\n    else:\n        space = bytes(b' ')\n    string = quote(string, safe + space, encoding, errors)\n    return string.replace(' ', '+')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef quote_from_bytes(bs, safe='/'):\n    if not isinstance(bs, (bytes, bytearray)):\n        raise TypeError(\"quote_from_bytes() expected bytes\")\n    if not bs:\n        return str('')\n    ### For Python-Future:\n    bs = bytes(bs)\n    ###\n    if isinstance(safe, str):\n        # Normalize 'safe' by converting to bytes and removing non-ASCII chars\n        safe = str(safe).encode('ascii', 'ignore')\n    else:\n        ### For Python-Future:\n        safe = bytes(safe)\n        ###\n        safe = bytes([c for c in safe if c < 128])\n    if not bs.rstrip(_ALWAYS_SAFE_BYTES + safe):\n        return bs.decode()\n    try:\n        quoter = _safe_quoters[safe]\n    except KeyError:\n        _safe_quoters[safe] = quoter = Quoter(safe).__getitem__\n    return str('').join([quoter(char) for char in bs])", "response": "Like quote but accepts a bytes object rather than a str and does\n    not perform string - to - bytes encoding."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nencoding a sequence of two - element tuples or dictionary into a URL query string.", "response": "def urlencode(query, doseq=False, safe='', encoding=None, errors=None):\n    \"\"\"Encode a sequence of two-element tuples or dictionary into a URL query string.\n\n    If any values in the query arg are sequences and doseq is true, each\n    sequence element is converted to a separate parameter.\n\n    If the query arg is a sequence of two-element tuples, the order of the\n    parameters in the output will match the order of parameters in the\n    input.\n\n    The query arg may be either a string or a bytes type. When query arg is a\n    string, the safe, encoding and error parameters are sent the quote_plus for\n    encoding.\n    \"\"\"\n\n    if hasattr(query, \"items\"):\n        query = query.items()\n    else:\n        # It's a bother at times that strings and string-like objects are\n        # sequences.\n        try:\n            # non-sequence items should not work with len()\n            # non-empty strings will fail this\n            if len(query) and not isinstance(query[0], tuple):\n                raise TypeError\n            # Zero-length sequences of all types will get here and succeed,\n            # but that's a minor nit.  Since the original implementation\n            # allowed empty dicts that type of behavior probably should be\n            # preserved for consistency\n        except TypeError:\n            ty, va, tb = sys.exc_info()\n            raise_with_traceback(TypeError(\"not a valid non-string sequence \"\n                                           \"or mapping object\"), tb)\n\n    l = []\n    if not doseq:\n        for k, v in query:\n            if isinstance(k, bytes):\n                k = quote_plus(k, safe)\n            else:\n                k = quote_plus(str(k), safe, encoding, errors)\n\n            if isinstance(v, bytes):\n                v = quote_plus(v, safe)\n            else:\n                v = quote_plus(str(v), safe, encoding, errors)\n            l.append(k + '=' + v)\n    else:\n        for k, v in query:\n            if isinstance(k, bytes):\n                k = quote_plus(k, safe)\n            else:\n                k = quote_plus(str(k), safe, encoding, errors)\n\n            if isinstance(v, bytes):\n                v = quote_plus(v, safe)\n                l.append(k + '=' + v)\n            elif isinstance(v, str):\n                v = quote_plus(v, safe, encoding, errors)\n                l.append(k + '=' + v)\n            else:\n                try:\n                    # Is this a sufficient test for sequence-ness?\n                    x = len(v)\n                except TypeError:\n                    # not a sequence\n                    v = quote_plus(str(v), safe, encoding, errors)\n                    l.append(k + '=' + v)\n                else:\n                    # loop over the sequence\n                    for elt in v:\n                        if isinstance(elt, bytes):\n                            elt = quote_plus(elt, safe)\n                        else:\n                            elt = quote_plus(str(elt), safe, encoding, errors)\n                        l.append(k + '=' + elt)\n    return str('&').join(l)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts URL to bytes.", "response": "def to_bytes(url):\n    \"\"\"to_bytes(u\"URL\") --> 'URL'.\"\"\"\n    # Most URL schemes require ASCII. If that changes, the conversion\n    # can be relaxed.\n    # XXX get rid of to_bytes()\n    if isinstance(url, str):\n        try:\n            url = url.encode(\"ASCII\").decode()\n        except UnicodeError:\n            raise UnicodeError(\"URL \" + repr(url) +\n                               \" contains non-ASCII characters\")\n    return url"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nunwraps a URL into a single element.", "response": "def unwrap(url):\n    \"\"\"unwrap('<URL:type://host/path>') --> 'type://host/path'.\"\"\"\n    url = str(url).strip()\n    if url[:1] == '<' and url[-1:] == '>':\n        url = url[1:-1].strip()\n    if url[:4] == 'URL:': url = url[4:].strip()\n    return url"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef splittype(url):\n    global _typeprog\n    if _typeprog is None:\n        import re\n        _typeprog = re.compile('^([^/:]+):')\n\n    match = _typeprog.match(url)\n    if match:\n        scheme = match.group(1)\n        return scheme.lower(), url[len(scheme) + 1:]\n    return None, url", "response": "splittype - Split type into type and url"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef splithost(url):\n    global _hostprog\n    if _hostprog is None:\n        import re\n        _hostprog = re.compile('^//([^/?]*)(.*)$')\n\n    match = _hostprog.match(url)\n    if match:\n        host_port = match.group(1)\n        path = match.group(2)\n        if path and not path.startswith('/'):\n            path = '/' + path\n        return host_port, path\n    return None, url", "response": "splithost - returns host port and path"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsplitting host and port into numeric and numerical ports.", "response": "def splitnport(host, defport=-1):\n    \"\"\"Split host and port, returning numeric port.\n    Return given default port if no ':' found; defaults to -1.\n    Return numerical port if a valid number are found after ':'.\n    Return None if ':' but not a valid number.\"\"\"\n    global _nportprog\n    if _nportprog is None:\n        import re\n        _nportprog = re.compile('^(.*):(.*)$')\n\n    match = _nportprog.match(host)\n    if match:\n        host, port = match.group(1, 2)\n        try:\n            if not port: raise ValueError(\"no digits\")\n            nport = int(port)\n        except ValueError:\n            nport = None\n        return host, nport\n    return host, defport"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef splitquery(url):\n    global _queryprog\n    if _queryprog is None:\n        import re\n        _queryprog = re.compile('^(.*)\\?([^?]*)$')\n\n    match = _queryprog.match(url)\n    if match: return match.group(1, 2)\n    return url, None", "response": "splitquery - Split url into path query"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef splitvalue(attr):\n    global _valueprog\n    if _valueprog is None:\n        import re\n        _valueprog = re.compile('^([^=]*)=(.*)$')\n\n    match = _valueprog.match(attr)\n    if match: return match.group(1, 2)\n    return attr, None", "response": "splitvalue - > attr = value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef header_encode(header_bytes, charset='iso-8859-1'):\n    # Return empty headers as an empty string.\n    if not header_bytes:\n        return ''\n    # Iterate over every byte, encoding if necessary.\n    encoded = []\n    for octet in header_bytes:\n        encoded.append(_QUOPRI_HEADER_MAP[octet])\n    # Now add the RFC chrome to each encoded chunk and glue the chunks\n    # together.\n    return '=?%s?q?%s?=' % (charset, EMPTYSTRING.join(encoded))", "response": "Encode a single header line with quoted - printable encoding."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef body_encode(body, maxlinelen=76, eol=NL):\n\n    if maxlinelen < 4:\n        raise ValueError(\"maxlinelen must be at least 4\")\n    if not body:\n        return body\n\n    # The last line may or may not end in eol, but all other lines do.\n    last_has_eol = (body[-1] in '\\r\\n')\n\n    # This accumulator will make it easier to build the encoded body.\n    encoded_body = _body_accumulator(maxlinelen, eol)\n\n    lines = body.splitlines()\n    last_line_no = len(lines) - 1\n    for line_no, line in enumerate(lines):\n        last_char_index = len(line) - 1\n        for i, c in enumerate(line):\n            if body_check(ord(c)):\n                c = quote(c)\n            encoded_body.write_char(c, i==last_char_index)\n        # Add an eol if input line had eol.  All input lines have eol except\n        # possibly the last one.\n        if line_no < last_line_no or last_has_eol:\n            encoded_body.newline()\n\n    return encoded_body.getvalue()", "response": "Encode a string of text with maxlinelen characters."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndecoding a quoted - printable string.", "response": "def decode(encoded, eol=NL):\n    \"\"\"Decode a quoted-printable string.\n\n    Lines are separated with eol, which defaults to \\\\n.\n    \"\"\"\n    if not encoded:\n        return encoded\n    # BAW: see comment in encode() above.  Again, we're building up the\n    # decoded string with string concatenation, which could be done much more\n    # efficiently.\n    decoded = ''\n\n    for line in encoded.splitlines():\n        line = line.rstrip()\n        if not line:\n            decoded += eol\n            continue\n\n        i = 0\n        n = len(line)\n        while i < n:\n            c = line[i]\n            if c != '=':\n                decoded += c\n                i += 1\n            # Otherwise, c == \"=\".  Are we at the end of the line?  If so, add\n            # a soft line break.\n            elif i+1 == n:\n                i += 1\n                continue\n            # Decode if in form =AB\n            elif i+2 < n and line[i+1] in hexdigits and line[i+2] in hexdigits:\n                decoded += unquote(line[i:i+3])\n                i += 3\n            # Otherwise, not in form =AB, pass literally\n            else:\n                decoded += c\n                i += 1\n\n            if i == n:\n                decoded += eol\n    # Special case if original string did not end with eol\n    if encoded[-1] not in '\\r\\n' and decoded.endswith(eol):\n        decoded = decoded[:-1]\n    return decoded"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndecode a string encoded with RFC 2045 MIME header Q encoding.", "response": "def header_decode(s):\n    \"\"\"Decode a string encoded with RFC 2045 MIME header `Q' encoding.\n\n    This function does not parse a full MIME header value encoded with\n    quoted-printable (like =?iso-8895-1?q?Hello_World?=) -- please use\n    the high level email.header class for that functionality.\n    \"\"\"\n    s = s.replace('_', ' ')\n    return re.sub(r'=[a-fA-F0-9]{2}', _unquote_match, s, re.ASCII)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_str(self, s):\n        self.write(s)\n        self.room -= len(s)", "response": "Add string s to the accumulated body."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef newline(self):\n        self.write_str(self.eol)\n        self.room = self.maxlinelen", "response": "Write eol then start new line."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_wrapped(self, s, extra_room=0):\n        if self.room < len(s) + extra_room:\n            self.write_soft_break()\n        self.write_str(s)", "response": "Write a string to the underlying output."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reset(self):\n        self.rawdata = ''\n        self.lasttag = '???'\n        self.interesting = interesting_normal\n        self.cdata_elem = None\n        _markupbase.ParserBase.reset(self)", "response": "Reset this instance. Loses all unprocessed data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef feed(self, data):\n        self.rawdata = self.rawdata + data\n        self.goahead(0)", "response": "Feed data to the parser."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nencode the message s payload in Base64.", "response": "def encode_base64(msg):\n    \"\"\"Encode the message's payload in Base64.\n\n    Also, add an appropriate Content-Transfer-Encoding header.\n    \"\"\"\n    orig = msg.get_payload()\n    encdata = str(_bencode(orig), 'ascii')\n    msg.set_payload(encdata)\n    msg['Content-Transfer-Encoding'] = 'base64'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nencoding the message s payload in quoted - printable.", "response": "def encode_quopri(msg):\n    \"\"\"Encode the message's payload in quoted-printable.\n\n    Also, add an appropriate Content-Transfer-Encoding header.\n    \"\"\"\n    orig = msg.get_payload()\n    encdata = _qencode(orig)\n    msg.set_payload(encdata)\n    msg['Content-Transfer-Encoding'] = 'quoted-printable'"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the Content - Transfer - Encoding header to 7bit or 8bit.", "response": "def encode_7or8bit(msg):\n    \"\"\"Set the Content-Transfer-Encoding header to 7bit or 8bit.\"\"\"\n    orig = msg.get_payload()\n    if orig is None:\n        # There's no payload.  For backwards compatibility we use 7bit\n        msg['Content-Transfer-Encoding'] = '7bit'\n        return\n    # We play a trick to make this go fast.  If encoding/decode to ASCII\n    # succeeds, we know the data must be 7bit, otherwise treat it as 8bit.\n    try:\n        if isinstance(orig, str):\n            orig.encode('ascii')\n        else:\n            orig.decode('ascii')\n    except UnicodeError:\n        charset = msg.get_charset()\n        output_cset = charset and charset.output_charset\n        # iso-2022-* is non-ASCII but encodes to a 7-bit representation\n        if output_cset and output_cset.lower().startswith('iso-2022-'):\n            msg['Content-Transfer-Encoding'] = '7bit'\n        else:\n            msg['Content-Transfer-Encoding'] = '8bit'\n    else:\n        msg['Content-Transfer-Encoding'] = '7bit'\n    if not isinstance(orig, str):\n        msg.set_payload(orig.decode('ascii', 'surrogateescape'))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines whether the module m is from the Python 2 standard library.", "response": "def is_py2_stdlib_module(m):\n    \"\"\"\n    Tries to infer whether the module m is from the Python 2 standard library.\n    This may not be reliable on all systems.\n    \"\"\"\n    if PY3:\n        return False\n    if not 'stdlib_path' in is_py2_stdlib_module.__dict__:\n        stdlib_files = [contextlib.__file__, os.__file__, copy.__file__]\n        stdlib_paths = [os.path.split(f)[0] for f in stdlib_files]\n        if not len(set(stdlib_paths)) == 1:\n            # This seems to happen on travis-ci.org. Very strange. We'll try to\n            # ignore it.\n            flog.warn('Multiple locations found for the Python standard '\n                         'library: %s' % stdlib_paths)\n        # Choose the first one arbitrarily\n        is_py2_stdlib_module.stdlib_path = stdlib_paths[0]\n\n    if m.__name__ in sys.builtin_module_names:\n        return True\n\n    if hasattr(m, '__file__'):\n        modpath = os.path.split(m.__file__)\n        if (modpath[0].startswith(is_py2_stdlib_module.stdlib_path) and\n            'site-packages' not in modpath[0]):\n            return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef scrub_py2_sys_modules():\n    if PY3:\n        return {}\n    scrubbed = {}\n    for modulename in REPLACED_MODULES & set(RENAMES.keys()):\n        if not modulename in sys.modules:\n            continue\n\n        module = sys.modules[modulename]\n\n        if is_py2_stdlib_module(module):\n            flog.debug('Deleting (Py2) {} from sys.modules'.format(modulename))\n            scrubbed[modulename] = sys.modules[modulename]\n            del sys.modules[modulename]\n    return scrubbed", "response": "Scrubs sys. modules to remove any Python 2 standard library modules from sys. modules that are not in sys. modules."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef restore_sys_modules(scrubbed):\n    clash = set(sys.modules) & set(scrubbed)\n    if len(clash) != 0:\n        # If several, choose one arbitrarily to raise an exception about\n        first = list(clash)[0]\n        raise ImportError('future module {} clashes with Py2 module'\n                          .format(first))\n    sys.modules.update(scrubbed)", "response": "Restore sys. modules from scrubbed modules."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef install_aliases():\n    if PY3:\n        return\n    # if hasattr(install_aliases, 'run_already'):\n    #     return\n    for (newmodname, newobjname, oldmodname, oldobjname) in MOVES:\n        __import__(newmodname)\n        # We look up the module in sys.modules because __import__ just returns the\n        # top-level package:\n        newmod = sys.modules[newmodname]\n        # newmod.__future_module__ = True\n\n        __import__(oldmodname)\n        oldmod = sys.modules[oldmodname]\n\n        obj = getattr(oldmod, oldobjname)\n        setattr(newmod, newobjname, obj)\n\n    # Hack for urllib so it appears to have the same structure on Py2 as on Py3\n    import urllib\n    from future.backports.urllib import request\n    from future.backports.urllib import response\n    from future.backports.urllib import parse\n    from future.backports.urllib import error\n    from future.backports.urllib import robotparser\n    urllib.request = request\n    urllib.response = response\n    urllib.parse = parse\n    urllib.error = error\n    urllib.robotparser = robotparser\n    sys.modules['urllib.request'] = request\n    sys.modules['urllib.response'] = response\n    sys.modules['urllib.parse'] = parse\n    sys.modules['urllib.error'] = error\n    sys.modules['urllib.robotparser'] = robotparser\n\n    # Patch the test module so it appears to have the same structure on Py2 as on Py3\n    try:\n        import test\n    except ImportError:\n        pass\n    try:\n        from future.moves.test import support\n    except ImportError:\n        pass\n    else:\n        test.support = support\n        sys.modules['test.support'] = support\n\n    # Patch the dbm module so it appears to have the same structure on Py2 as on Py3\n    try:\n        import dbm\n    except ImportError:\n        pass\n    else:\n        from future.moves.dbm import dumb\n        dbm.dumb = dumb\n        sys.modules['dbm.dumb'] = dumb\n        try:\n            from future.moves.dbm import gnu\n        except ImportError:\n            pass\n        else:\n            dbm.gnu = gnu\n            sys.modules['dbm.gnu'] = gnu\n        try:\n            from future.moves.dbm import ndbm\n        except ImportError:\n            pass\n        else:\n            dbm.ndbm = ndbm\n            sys.modules['dbm.ndbm'] = ndbm", "response": "Installs the aliases for the current base class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove the import hooks from sys. meta_path.", "response": "def remove_hooks(scrub_sys_modules=False):\n    \"\"\"\n    This function removes the import hook from sys.meta_path.\n    \"\"\"\n    if PY3:\n        return\n    flog.debug('Uninstalling hooks ...')\n    # Loop backwards, so deleting items keeps the ordering:\n    for i, hook in list(enumerate(sys.meta_path))[::-1]:\n        if hasattr(hook, 'RENAMER'):\n            del sys.meta_path[i]\n\n    # Explicit is better than implicit. In the future the interface should\n    # probably change so that scrubbing the import hooks requires a separate\n    # function call. Left as is for now for backward compatibility with\n    # v0.11.x.\n    if scrub_sys_modules:\n        scrub_future_sys_modules()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef detect_hooks():\n    flog.debug('Detecting hooks ...')\n    present = any([hasattr(hook, 'RENAMER') for hook in sys.meta_path])\n    if present:\n        flog.debug('Detected.')\n    else:\n        flog.debug('Not detected.')\n    return present", "response": "Detects if the import hooks are installed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncache sys. py2_modules for later use.", "response": "def cache_py2_modules():\n    \"\"\"\n    Currently this function is unneeded, as we are not attempting to provide import hooks\n    for modules with ambiguous names: email, urllib, pickle.\n    \"\"\"\n    if len(sys.py2_modules) != 0:\n        return\n    assert not detect_hooks()\n    import urllib\n    sys.py2_modules['urllib'] = urllib\n\n    import email\n    sys.py2_modules['email'] = email\n\n    import pickle\n    sys.py2_modules['pickle'] = pickle"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef import_(module_name, backport=False):\n    # Python 2.6 doesn't have importlib in the stdlib, so it requires\n    # the backported ``importlib`` package from PyPI as a dependency to use\n    # this function:\n    import importlib\n\n    if PY3:\n        return __import__(module_name)\n    else:\n        # client.blah = blah\n        # Then http.client = client\n        # etc.\n        if backport:\n            prefix = 'future.backports'\n        else:\n            prefix = 'future.moves'\n        parts = prefix.split('.') + module_name.split('.')\n\n        modules = []\n        for i, part in enumerate(parts):\n            sofar = '.'.join(parts[:i+1])\n            modules.append(importlib.import_module(sofar))\n        for i, part in reversed(list(enumerate(parts))):\n            if i == 0:\n                break\n            setattr(modules[i-1], part, modules[i])\n\n        # Return the next-most top-level module after future.backports / future.moves:\n        return modules[2]", "response": "Import a module from a Python 3 standard library and return the top - level module."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nimports a module and returns a list of attributes.", "response": "def from_import(module_name, *symbol_names, **kwargs):\n    \"\"\"\n    Example use:\n        >>> HTTPConnection = from_import('http.client', 'HTTPConnection')\n        >>> HTTPServer = from_import('http.server', 'HTTPServer')\n        >>> urlopen, urlparse = from_import('urllib.request', 'urlopen', 'urlparse')\n\n    Equivalent to this on Py3:\n\n        >>> from module_name import symbol_names[0], symbol_names[1], ...\n\n    and this on Py2:\n\n        >>> from future.moves.module_name import symbol_names[0], ...\n\n    or:\n\n        >>> from future.backports.module_name import symbol_names[0], ...\n\n    except that it also handles dotted module names such as ``http.client``.\n    \"\"\"\n\n    if PY3:\n        return __import__(module_name)\n    else:\n        if 'backport' in kwargs and bool(kwargs['backport']):\n            prefix = 'future.backports'\n        else:\n            prefix = 'future.moves'\n        parts = prefix.split('.') + module_name.split('.')\n        module = importlib.import_module(prefix + '.' + module_name)\n        output = [getattr(module, name) for name in symbol_names]\n        if len(output) == 1:\n            return output[0]\n        else:\n            return output"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds and loads a module in sys. modules.", "response": "def _find_and_load_module(self, name, path=None):\n        \"\"\"\n        Finds and loads it. But if there's a . in the name, handles it\n        properly.\n        \"\"\"\n        bits = name.split('.')\n        while len(bits) > 1:\n            # Treat the first bit as a package\n            packagename = bits.pop(0)\n            package = self._find_and_load_module(packagename, path)\n            try:\n                path = package.__path__\n            except AttributeError:\n                # This could be e.g. moves.\n                flog.debug('Package {0} has no __path__.'.format(package))\n                if name in sys.modules:\n                    return sys.modules[name]\n                flog.debug('What to do here?')\n\n        name = bits[0]\n        module_info = imp.find_module(name, path)\n        return imp.load_module(name, *module_info)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nencoding the unicode string into a byte string.", "response": "def encode(self, encoding='utf-8', errors='strict'):\n        \"\"\"\n        Returns bytes\n\n        Encode S using the codec registered for encoding. Default encoding\n        is 'utf-8'. errors may be given to set a different error\n        handling scheme. Default is 'strict' meaning that encoding errors raise\n        a UnicodeEncodeError. Other possible values are 'ignore', 'replace' and\n        'xmlcharrefreplace' as well as any other name registered with\n        codecs.register_error that can handle UnicodeEncodeErrors.\n        \"\"\"\n        from future.types.newbytes import newbytes\n        # Py2 unicode.encode() takes encoding and errors as optional parameter,\n        # not keyword arguments as in Python 3 str.\n\n        # For the surrogateescape error handling mechanism, the\n        # codecs.register_error() function seems to be inadequate for an\n        # implementation of it when encoding. (Decoding seems fine, however.)\n        # For example, in the case of\n        #     u'\\udcc3'.encode('ascii', 'surrogateescape_handler')\n        # after registering the ``surrogateescape_handler`` function in\n        # future.utils.surrogateescape, both Python 2.x and 3.x raise an\n        # exception anyway after the function is called because the unicode\n        # string it has to return isn't encodable strictly as ASCII.\n\n        if errors == 'surrogateescape':\n            if encoding == 'utf-16':\n                # Known to fail here. See test_encoding_works_normally()\n                raise NotImplementedError('FIXME: surrogateescape handling is '\n                                          'not yet implemented properly')\n            # Encode char by char, building up list of byte-strings\n            mybytes = []\n            for c in self:\n                code = ord(c)\n                if 0xD800 <= code <= 0xDCFF:\n                    mybytes.append(newbytes([code - 0xDC00]))\n                else:\n                    mybytes.append(c.encode(encoding=encoding))\n            return newbytes(b'').join(mybytes)\n        return newbytes(super(newstr, self).encode(encoding, errors))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlike newstr. find but raise ValueError when the substring is not found.", "response": "def index(self, sub, *args):\n        \"\"\"\n        Like newstr.find() but raise ValueError when the substring is not\n        found.\n        \"\"\"\n        pos = self.find(sub, *args)\n        if pos == -1:\n            raise ValueError('substring not found')\n        return pos"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef splitlines(self, keepends=False):\n        # Py2 unicode.splitlines() takes keepends as an optional parameter,\n        # not as a keyword argument as in Python 3 str.\n        parts = super(newstr, self).splitlines(keepends)\n        return [newstr(part) for part in parts]", "response": "Return a list of lines in the string S."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a translation table usable for str. translate.", "response": "def maketrans(x, y=None, z=None):\n        \"\"\"\n        Return a translation table usable for str.translate().\n\n        If there is only one argument, it must be a dictionary mapping Unicode\n        ordinals (integers) or characters to Unicode ordinals, strings or None.\n        Character keys will be then converted to ordinals.\n        If there are two arguments, they must be strings of equal length, and\n        in the resulting dictionary, each character in x will be mapped to the\n        character at the same position in y. If there is a third argument, it\n        must be a string, whose characters will be mapped to None in the result.\n        \"\"\"\n\n        if y is None:\n            assert z is None\n            if not isinstance(x, dict):\n                raise TypeError('if you give only one argument to maketrans it must be a dict')\n            result = {}\n            for (key, value) in x.items():\n                if len(key) > 1:\n                    raise ValueError('keys in translate table must be strings or integers')\n                result[ord(key)] = value\n        else:\n            if not isinstance(x, unicode) and isinstance(y, unicode):\n                raise TypeError('x and y must be unicode strings')\n            if not len(x) == len(y):\n                raise ValueError('the first two maketrans arguments must have equal length')\n            result = {}\n            for (xi, yi) in zip(x, y):\n                if len(xi) > 1:\n                    raise ValueError('keys in translate table must be strings or integers')\n                result[ord(xi)] = ord(yi)\n\n        if z is not None:\n            for char in z:\n                result[ord(char)] = None\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef translate(self, table):\n        l = []\n        for c in self:\n            if ord(c) in table:\n                val = table[ord(c)]\n                if val is None:\n                    continue\n                elif isinstance(val, unicode):\n                    l.append(val)\n                else:\n                    l.append(chr(val))\n            else:\n                l.append(c)\n        return ''.join(l)", "response": "Returns a copy of the string S where all characters have been mapped to the given translation table."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of ( REALNAME EMAIL ) for each fieldvalue.", "response": "def getaddresses(fieldvalues):\n    \"\"\"Return a list of (REALNAME, EMAIL) for each fieldvalue.\"\"\"\n    all = COMMASPACE.join(fieldvalues)\n    a = _AddressList(all)\n    return a.addresslist"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a date string as specified by RFC 2822.", "response": "def formatdate(timeval=None, localtime=False, usegmt=False):\n    \"\"\"Returns a date string as specified by RFC 2822, e.g.:\n\n    Fri, 09 Nov 2001 01:08:47 -0000\n\n    Optional timeval if given is a floating point time value as accepted by\n    gmtime() and localtime(), otherwise the current time is used.\n\n    Optional localtime is a flag that when True, interprets timeval, and\n    returns a date relative to the local timezone instead of UTC, properly\n    taking daylight savings time into account.\n\n    Optional argument usegmt means that the timezone is written out as\n    an ascii string, not numeric one (so \"GMT\" instead of \"+0000\"). This\n    is needed for HTTP, and is only used when localtime==False.\n    \"\"\"\n    # Note: we cannot use strftime() because that honors the locale and RFC\n    # 2822 requires that day and month names be the English abbreviations.\n    if timeval is None:\n        timeval = time.time()\n    if localtime:\n        now = time.localtime(timeval)\n        # Calculate timezone offset, based on whether the local zone has\n        # daylight savings time, and whether DST is in effect.\n        if time.daylight and now[-1]:\n            offset = time.altzone\n        else:\n            offset = time.timezone\n        hours, minutes = divmod(abs(offset), 3600)\n        # Remember offset is in seconds west of UTC, but the timezone is in\n        # minutes east of UTC, so the signs differ.\n        if offset > 0:\n            sign = '-'\n        else:\n            sign = '+'\n        zone = '%s%02d%02d' % (sign, hours, minutes // 60)\n    else:\n        now = time.gmtime(timeval)\n        # Timezone offset is always -0000\n        if usegmt:\n            zone = 'GMT'\n        else:\n            zone = '-0000'\n    return _format_timetuple_and_zone(now, zone)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef format_datetime(dt, usegmt=False):\n    now = dt.timetuple()\n    if usegmt:\n        if dt.tzinfo is None or dt.tzinfo != datetime.timezone.utc:\n            raise ValueError(\"usegmt option requires a UTC datetime\")\n        zone = 'GMT'\n    elif dt.tzinfo is None:\n        zone = '-0000'\n    else:\n        zone = dt.strftime(\"%z\")\n    return _format_timetuple_and_zone(now, zone)", "response": "Turn a datetime into a date string as specified in RFC 2822."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a string suitable for RFC 2822 compliant Message - ID e. g. 20020201195627. 366539. 96671.", "response": "def make_msgid(idstring=None, domain=None):\n    \"\"\"Returns a string suitable for RFC 2822 compliant Message-ID, e.g:\n\n    <20020201195627.33539.96671@nightshade.la.mastaler.com>\n\n    Optional idstring if given is a string used to strengthen the\n    uniqueness of the message id.  Optional domain if given provides the\n    portion of the message id after the '@'.  It defaults to the locally\n    defined hostname.\n    \"\"\"\n    timeval = time.time()\n    utcdate = time.strftime('%Y%m%d%H%M%S', time.gmtime(timeval))\n    pid = os.getpid()\n    randint = random.randrange(100000)\n    if idstring is None:\n        idstring = ''\n    else:\n        idstring = '.' + idstring\n    if domain is None:\n        domain = socket.getfqdn()\n    msgid = '<%s.%s.%s%s@%s>' % (utcdate, pid, randint, idstring, domain)\n    return msgid"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves quotes from a string.", "response": "def unquote(str):\n    \"\"\"Remove quotes from a string.\"\"\"\n    if len(str) > 1:\n        if str.startswith('\"') and str.endswith('\"'):\n            return str[1:-1].replace('\\\\\\\\', '\\\\').replace('\\\\\"', '\"')\n        if str.startswith('<') and str.endswith('>'):\n            return str[1:-1]\n    return str"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndecode string according to RFC 2231", "response": "def decode_rfc2231(s):\n    \"\"\"Decode string according to RFC 2231\"\"\"\n    parts = s.split(TICK, 2)\n    if len(parts) <= 2:\n        return None, None, s\n    return parts"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nencodes a string according to RFC 2231.", "response": "def encode_rfc2231(s, charset=None, language=None):\n    \"\"\"Encode string according to RFC 2231.\n\n    If neither charset nor language is given, then s is returned as-is.  If\n    charset is given but not language, the string is encoded using the empty\n    string for language.\n    \"\"\"\n    s = url_quote(s, safe='', encoding=charset or 'ascii')\n    if charset is None and language is None:\n        return s\n    if language is None:\n        language = ''\n    return \"%s'%s'%s\" % (charset, language, s)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decode_params(params):\n    # Copy params so we don't mess with the original\n    params = params[:]\n    new_params = []\n    # Map parameter's name to a list of continuations.  The values are a\n    # 3-tuple of the continuation number, the string value, and a flag\n    # specifying whether a particular segment is %-encoded.\n    rfc2231_params = {}\n    name, value = params.pop(0)\n    new_params.append((name, value))\n    while params:\n        name, value = params.pop(0)\n        if name.endswith('*'):\n            encoded = True\n        else:\n            encoded = False\n        value = unquote(value)\n        mo = rfc2231_continuation.match(name)\n        if mo:\n            name, num = mo.group('name', 'num')\n            if num is not None:\n                num = int(num)\n            rfc2231_params.setdefault(name, []).append((num, value, encoded))\n        else:\n            new_params.append((name, '\"%s\"' % quote(value)))\n    if rfc2231_params:\n        for name, continuations in rfc2231_params.items():\n            value = []\n            extended = False\n            # Sort by number\n            continuations.sort()\n            # And now append all values in numerical order, converting\n            # %-encodings for the encoded segments.  If any of the\n            # continuation names ends in a *, then the entire string, after\n            # decoding segments and concatenating, must have the charset and\n            # language specifiers at the beginning of the string.\n            for num, s, encoded in continuations:\n                if encoded:\n                    # Decode as \"latin-1\", so the characters in s directly\n                    # represent the percent-encoded octet values.\n                    # collapse_rfc2231_value treats this as an octet sequence.\n                    s = url_unquote(s, encoding=\"latin-1\")\n                    extended = True\n                value.append(s)\n            value = quote(EMPTYSTRING.join(value))\n            if extended:\n                charset, language, value = decode_rfc2231(value)\n                new_params.append((name, (charset, language, '\"%s\"' % value)))\n            else:\n                new_params.append((name, '\"%s\"' % value))\n    return new_params", "response": "Decodes the parameters list according to RFC 2231."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a naive datetime object.", "response": "def localtime(dt=None, isdst=-1):\n    \"\"\"Return local time as an aware datetime object.\n\n    If called without arguments, return current time.  Otherwise *dt*\n    argument should be a datetime instance, and it is converted to the\n    local time zone according to the system time zone database.  If *dt* is\n    naive (that is, dt.tzinfo is None), it is assumed to be in local time.\n    In this case, a positive or zero value for *isdst* causes localtime to\n    presume initially that summer time (for example, Daylight Saving Time)\n    is or is not (respectively) in effect for the specified time.  A\n    negative value for *isdst* causes the localtime() function to attempt\n    to divine whether summer time is in effect for the specified time.\n\n    \"\"\"\n    if dt is None:\n        return datetime.datetime.now(datetime.timezone.utc).astimezone()\n    if dt.tzinfo is not None:\n        return dt.astimezone()\n    # We have a naive datetime.  Convert to a (localtime) timetuple and pass to\n    # system mktime together with the isdst hint.  System mktime will return\n    # seconds since epoch.\n    tm = dt.timetuple()[:-1] + (isdst,)\n    seconds = time.mktime(tm)\n    localtm = time.localtime(seconds)\n    try:\n        delta = datetime.timedelta(seconds=localtm.tm_gmtoff)\n        tz = datetime.timezone(delta, localtm.tm_zone)\n    except AttributeError:\n        # Compute UTC offset and compare with the value implied by tm_isdst.\n        # If the values match, use the zone name implied by tm_isdst.\n        delta = dt - datetime.datetime(*time.gmtime(seconds)[:6])\n        dst = time.daylight and localtm.tm_isdst > 0\n        gmtoff = -(time.altzone if dst else time.timezone)\n        if delta == datetime.timedelta(seconds=gmtoff):\n            tz = datetime.timezone(delta, time.tzname[dst])\n        else:\n            tz = datetime.timezone(delta)\n    return dt.replace(tzinfo=tz)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef smart_text(s, encoding='utf-8', strings_only=False, errors='strict'):\n    if isinstance(s, Promise):\n        # The input is the result of a gettext_lazy() call.\n        return s\n    return force_text(s, encoding, strings_only, errors)", "response": "Returns a unicode object representing s using the encoding codec."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining if the object is of a protected type.", "response": "def is_protected_type(obj):\n    \"\"\"Determine if the object instance is of a protected type.\n\n    Objects of protected types are preserved as-is when passed to\n    force_text(strings_only=True).\n    \"\"\"\n    return isinstance(obj, six.integer_types + (type(None), float, Decimal,\n        datetime.datetime, datetime.date, datetime.time))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a bytestring version of s encoded as specified in encoding.", "response": "def smart_bytes(s, encoding='utf-8', strings_only=False, errors='strict'):\n    \"\"\"\n    Returns a bytestring version of 's', encoded as specified in 'encoding'.\n\n    If strings_only is True, don't convert (some) non-string-like objects.\n    \"\"\"\n    if isinstance(s, Promise):\n        # The input is the result of a gettext_lazy() call.\n        return s\n    return force_bytes(s, encoding, strings_only, errors)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a file system path to a URI portion that is suitable for inclusion in a URL.", "response": "def filepath_to_uri(path):\n    \"\"\"Convert a file system path to a URI portion that is suitable for\n    inclusion in a URL.\n\n    We are assuming input is either UTF-8 or unicode already.\n\n    This method will encode certain chars that would normally be recognized as\n    special chars for URIs.  Note that this method does not encode the '\n    character, as it is a valid character within URIs.  See\n    encodeURIComponent() JavaScript function for more details.\n\n    Returns an ASCII string containing the encoded result.\n    \"\"\"\n    if path is None:\n        return path\n    # I know about `os.sep` and `os.altsep` but I want to leave\n    # some flexibility for hardcoding separators.\n    return quote(force_bytes(path).replace(b\"\\\\\", b\"/\"), safe=b\"/~!*()'\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the encoding of the default system locale but falls back to the given fallback encoding if the encoding is unsupported by python.", "response": "def get_system_encoding():\n    \"\"\"\n    The encoding of the default system locale but falls back to the given\n    fallback encoding if the encoding is unsupported by python or could\n    not be determined.  See tickets #10335 and #5846\n    \"\"\"\n    try:\n        encoding = locale.getdefaultlocale()[1] or 'ascii'\n        codecs.lookup(encoding)\n    except Exception:\n        encoding = 'ascii'\n    return encoding"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndecoding an encoded word and return a tuple of the string charset lang and defects list.", "response": "def decode(ew):\n    \"\"\"Decode encoded word and return (string, charset, lang, defects) tuple.\n\n    An RFC 2047/2243 encoded word has the form:\n\n        =?charset*lang?cte?encoded_string?=\n\n    where '*lang' may be omitted but the other parts may not be.\n\n    This function expects exactly such a string (that is, it does not check the\n    syntax and may raise errors if the string is not well formed), and returns\n    the encoded_string decoded first from its Content Transfer Encoding and\n    then from the resulting bytes into unicode using the specified charset.  If\n    the cte-decoded string does not successfully decode using the specified\n    character set, a defect is added to the defects list and the unknown octets\n    are replaced by the unicode 'unknown' character \\uFDFF.\n\n    The specified charset and language are returned.  The default for language,\n    which is rarely if ever encountered, is the empty string.\n\n    \"\"\"\n    _, charset, cte, cte_string, _ = str(ew).split('?')\n    charset, _, lang = charset.partition('*')\n    cte = cte.lower()\n    # Recover the original bytes and do CTE decoding.\n    bstring = cte_string.encode('ascii', 'surrogateescape')\n    bstring, defects = _cte_decoders[cte](bstring)\n    # Turn the CTE decoded bytes into unicode.\n    try:\n        string = bstring.decode(charset)\n    except UnicodeError:\n        defects.append(errors.UndecodableBytesDefect(\"Encoded word \"\n            \"contains bytes not decodable using {} charset\".format(charset)))\n        string = bstring.decode(charset, 'surrogateescape')\n    except LookupError:\n        string = bstring.decode('ascii', 'surrogateescape')\n        if charset.lower() != 'unknown-8bit':\n            defects.append(errors.CharsetError(\"Unknown charset {} \"\n                \"in encoded word; decoded as unknown bytes\".format(charset)))\n    return string, charset, lang, defects"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef encode(string, charset='utf-8', encoding=None, lang=''):\n    string = str(string)\n    if charset == 'unknown-8bit':\n        bstring = string.encode('ascii', 'surrogateescape')\n    else:\n        bstring = string.encode(charset)\n    if encoding is None:\n        qlen = _cte_encode_length['q'](bstring)\n        blen = _cte_encode_length['b'](bstring)\n        # Bias toward q.  5 is arbitrary.\n        encoding = 'q' if qlen - blen < 5 else 'b'\n    encoded = _cte_encoders[encoding](bstring)\n    if lang:\n        lang = '*' + lang\n    return \"=?{0}{1}?{2}?{3}?=\".format(charset, lang, encoding, encoded)", "response": "Encode a string using the CTE encoding that produces the shorter result."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbind a method to class python 2 and python 3 compatible.", "response": "def bind_method(cls, name, func):\n    \"\"\"Bind a method to class, python 2 and python 3 compatible.\n\n    Parameters\n    ----------\n\n    cls : type\n        class to receive bound method\n    name : basestring\n        name of method on class instance\n    func : function\n        function to be bound as method\n\n\n    Returns\n    -------\n    None\n    \"\"\"\n    # only python 2 has bound/unbound method issue\n    if not PY3:\n        setattr(cls, name, types.MethodType(func, None, cls))\n    else:\n        setattr(cls, name, func)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef most_common(self, n=None):\n        '''List the n most common elements and their counts from the most\n        common to the least.  If n is None, then list all element counts.\n\n        >>> Counter('abracadabra').most_common(3)\n        [('a', 5), ('r', 2), ('b', 2)]\n\n        '''\n        if n is None:\n            return sorted(iteritems(self), key=itemgetter(1), reverse=True)\n        return nlargest(n, iteritems(self), key=itemgetter(1))", "response": "List the n most common elements and their counts from the most common to the least."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clone(self, **kw):\n        newpolicy = self.__class__.__new__(self.__class__)\n        for attr, value in self.__dict__.items():\n            object.__setattr__(newpolicy, attr, value)\n        for attr, value in kw.items():\n            if not hasattr(self, attr):\n                raise TypeError(\n                    \"{!r} is an invalid keyword argument for {}\".format(\n                        attr, self.__class__.__name__))\n            object.__setattr__(newpolicy, attr, value)\n        return newpolicy", "response": "Return a new instance with specified attributes changed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle_defect(self, obj, defect):\n        if self.raise_on_defect:\n            raise defect\n        self.register_defect(obj, defect)", "response": "This method is called by the parser when an object is raised by the parser."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _quote(str, LegalChars=_LegalChars):\n    if all(c in LegalChars for c in str):\n        return str\n    else:\n        return '\"' + _nulljoin(_Translator.get(s, s) for s in str) + '\"'", "response": "r Quote a string for use in a cookie header."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __set(self, key, real_value, coded_value):\n        M = self.get(key, Morsel())\n        M.set(key, real_value, coded_value)\n        dict.__setitem__(self, key, M)", "response": "Private method for setting a cookie s value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a string suitable for HTTP.", "response": "def output(self, attrs=None, header=\"Set-Cookie:\", sep=\"\\015\\012\"):\n        \"\"\"Return a string suitable for HTTP.\"\"\"\n        result = []\n        items = sorted(self.items())\n        for key, value in items:\n            result.append(value.output(attrs, header))\n        return sep.join(result)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a string suitable for JavaScript.", "response": "def js_output(self, attrs=None):\n        \"\"\"Return a string suitable for JavaScript.\"\"\"\n        result = []\n        items = sorted(self.items())\n        for key, value in items:\n            result.append(value.js_output(attrs))\n        return _nulljoin(result)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load(self, rawdata):\n        if isinstance(rawdata, str):\n            self.__parse_string(rawdata)\n        else:\n            # self.update() wouldn't call our custom __setitem__\n            for key, value in rawdata.items():\n                self[key] = value\n        return", "response": "Load cookies from a string or dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef items(self):\n        if ver == (2, 7):\n            return self.viewitems()\n        elif ver == (2, 6):\n            return self.iteritems()\n        elif ver >= (3, 0):\n            return self.items()", "response": "A generator that returns a set - like object providing a view on the items of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef values(self):\n        if ver == (2, 7):\n            return self.viewvalues()\n        elif ver == (2, 6):\n            return self.itervalues()\n        elif ver >= (3, 0):\n            return self.values()", "response": "A set - like object providing a view on the D s values\n            On Python 2. 7 + and Python 2. 6 + the values\n            is an iterator over the D s values\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _whatsnd(data):\n    hdr = data[:512]\n    fakefile = BytesIO(hdr)\n    for testfn in sndhdr.tests:\n        res = testfn(hdr, fakefile)\n        if res is not None:\n            return _sndhdr_MIMEmap.get(res[0])\n    return None", "response": "Try to identify a sound file type."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the parent has metaclass", "response": "def has_metaclass(parent):\n    \"\"\" we have to check the cls_node without changing it.\n        There are two possiblities:\n          1)  clsdef => suite => simple_stmt => expr_stmt => Leaf('__meta')\n          2)  clsdef => simple_stmt => expr_stmt => Leaf('__meta')\n    \"\"\"\n    for node in parent.children:\n        if node.type == syms.suite:\n            return has_metaclass(node)\n        elif node.type == syms.simple_stmt and node.children:\n            expr_node = node.children[0]\n            if expr_node.type == syms.expr_stmt and expr_node.children:\n                left_side = expr_node.children[0]\n                if isinstance(left_side, Leaf) and \\\n                        left_side.value == '__metaclass__':\n                    return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fixup_simple_stmt(parent, i, stmt_node):\n    for semi_ind, node in enumerate(stmt_node.children):\n        if node.type == token.SEMI: # *sigh*\n            break\n    else:\n        return\n\n    node.remove() # kill the semicolon\n    new_expr = Node(syms.expr_stmt, [])\n    new_stmt = Node(syms.simple_stmt, [new_expr])\n    while stmt_node.children[semi_ind:]:\n        move_node = stmt_node.children[semi_ind]\n        new_expr.append_child(move_node.clone())\n        move_node.remove()\n    parent.insert_child(i, new_stmt)\n    new_leaf1 = new_stmt.children[0].children[0]\n    old_leaf1 = stmt_node.children[0].children[0]\n    new_leaf1.prefix = old_leaf1.prefix", "response": "Fixup the simple_stmt node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fixup_indent(suite):\n    kids = suite.children[::-1]\n    # find the first indent\n    while kids:\n        node = kids.pop()\n        if node.type == token.INDENT:\n            break\n\n    # find the first Leaf\n    while kids:\n        node = kids.pop()\n        if isinstance(node, Leaf) and node.type != token.DEDENT:\n            if node.prefix:\n                node.prefix = u''\n            return\n        else:\n            kids.extend(node.children[::-1])", "response": "Fixup the indent of the hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a newstr with the bytes decoded using the codec registered for encoding.", "response": "def decode(self, encoding='utf-8', errors='strict'):\n        \"\"\"\n        Returns a newstr (i.e. unicode subclass)\n\n        Decode B using the codec registered for encoding. Default encoding\n        is 'utf-8'. errors may be given to set a different error\n        handling scheme.  Default is 'strict' meaning that encoding errors raise\n        a UnicodeDecodeError.  Other possible values are 'ignore' and 'replace'\n        as well as any other name registered with codecs.register_error that is\n        able to handle UnicodeDecodeErrors.\n        \"\"\"\n        # Py2 str.encode() takes encoding and errors as optional parameter,\n        # not keyword arguments as in Python 3 str.\n\n        from future.types.newstr import newstr\n\n        if errors == 'surrogateescape':\n            from future.utils.surrogateescape import register_surrogateescape\n            register_surrogateescape()\n\n        return newstr(super(newbytes, self).decode(encoding, errors))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef splitlines(self, keepends=False):\n        # Py2 str.splitlines() takes keepends as an optional parameter,\n        # not as a keyword argument as in Python 3 bytes.\n        parts = super(newbytes, self).splitlines(keepends)\n        return [newbytes(part) for part in parts]", "response": "A version of str. splitlines that uses newbytes to split the lines in a new bytes object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rindex(self, sub, *args):\n        '''\n        S.rindex(sub [,start [,end]]) -> int\n\n        Like S.rfind() but raise ValueError when the substring is not found.\n        '''\n        pos = self.rfind(sub, *args)\n        if pos == -1:\n            raise ValueError('substring not found')", "response": "rindex - Find the index of a substring in the array"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef index(self, sub, *args):\n        '''\n        Returns index of sub in bytes.\n        Raises ValueError if byte is not in bytes and TypeError if can't\n        be converted bytes or its length is not 1.\n        '''\n        if isinstance(sub, int):\n            if len(args) == 0:\n                start, end = 0, len(self)\n            elif len(args) == 1:\n                start = args[0]\n            elif len(args) == 2:\n                start, end = args\n            else:\n                raise TypeError('takes at most 3 arguments')\n            return list(self)[start:end].index(sub)\n        if not isinstance(sub, bytes):\n            try:\n                sub = self.__class__(sub)\n            except (TypeError, ValueError):\n                raise TypeError(\"can't convert sub to bytes\")\n        try:\n            return super(newbytes, self).index(sub, *args)\n        except ValueError:\n            raise ValueError('substring not found')", "response": "Returns index of sub in bytes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef python_2_unicode_compatible(cls):\n    if not PY3:\n        cls.__unicode__ = cls.__str__\n        cls.__str__ = lambda self: self.__unicode__().encode('utf-8')\n    return cls", "response": "A class decorator that returns a class that is Python 2 and 3 with unicode text."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef isidentifier(s, dotted=False):\n    '''\n    A function equivalent to the str.isidentifier method on Py3\n    '''\n    if dotted:\n        return all(isidentifier(a) for a in s.split('.'))\n    if PY3:\n        return s.isidentifier()\n    else:\n        import re\n        _name_re = re.compile(r\"[a-zA-Z_][a-zA-Z0-9_]*$\")\n        return bool(_name_re.match(s))", "response": "A function equivalent to the str. isidentifier method on Py3\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfunction for iterating over dictionary items with the same set - like behaviour on Py2. 7 as on Py3. Passes kwargs to method.", "response": "def viewitems(obj, **kwargs):\n    \"\"\"\n    Function for iterating over dictionary items with the same set-like\n    behaviour on Py2.7 as on Py3.\n\n    Passes kwargs to method.\"\"\"\n    func = getattr(obj, \"viewitems\", None)\n    if not func:\n        func = obj.items\n    return func(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfunctions for iterating over dictionary keys with the same set - like behaviour on Py2. 7 as on Py3. Passes kwargs to method.", "response": "def viewkeys(obj, **kwargs):\n    \"\"\"\n    Function for iterating over dictionary keys with the same set-like\n    behaviour on Py2.7 as on Py3.\n\n    Passes kwargs to method.\"\"\"\n    func = getattr(obj, \"viewkeys\", None)\n    if not func:\n        func = obj.keys\n    return func(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfunctioning for iterating over dictionary values with the same set - like behaviour on Py2. 7 as on Py3. Passes kwargs to method.", "response": "def viewvalues(obj, **kwargs):\n    \"\"\"\n    Function for iterating over dictionary values with the same set-like\n    behaviour on Py2.7 as on Py3.\n\n    Passes kwargs to method.\"\"\"\n    func = getattr(obj, \"viewvalues\", None)\n    if not func:\n        func = obj.values\n    return func(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the globals and locals of the calling frame.", "response": "def _get_caller_globals_and_locals():\n    \"\"\"\n    Returns the globals and locals of the calling frame.\n\n    Is there an alternative to frame hacking here?\n    \"\"\"\n    caller_frame = inspect.stack()[2]\n    myglobals = caller_frame[0].f_globals\n    mylocals = caller_frame[0].f_locals\n    return myglobals, mylocals"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the string without any initial or final quotes.", "response": "def _repr_strip(mystring):\n    \"\"\"\n    Returns the string without any initial or final quotes.\n    \"\"\"\n    r = repr(mystring)\n    if r.startswith(\"'\") and r.endswith(\"'\"):\n        return r[1:-1]\n    else:\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nclassing decorator that returns an iterator that returns the items of the given class.", "response": "def implements_iterator(cls):\n    '''\n    From jinja2/_compat.py. License: BSD.\n\n    Use as a decorator like this::\n\n        @implements_iterator\n        class UppercasingIterator(object):\n            def __init__(self, iterable):\n                self._iter = iter(iterable)\n            def __iter__(self):\n                return self\n            def __next__(self):\n                return next(self._iter).upper()\n\n    '''\n    if PY3:\n        return cls\n    else:\n        cls.next = cls.__next__\n        del cls.__next__\n        return cls"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef as_native_str(encoding='utf-8'):\n    '''\n    A decorator to turn a function or method call that returns text, i.e.\n    unicode, into one that returns a native platform str.\n\n    Use it as a decorator like this::\n\n        from __future__ import unicode_literals\n\n        class MyClass(object):\n            @as_native_str(encoding='ascii')\n            def __repr__(self):\n                return next(self._iter).upper()\n    '''\n    if PY3:\n        return lambda f: f\n    else:\n        def encoder(f):\n            @functools.wraps(f)\n            def wrapper(*args, **kwargs):\n                return f(*args, **kwargs).encode(encoding=encoding)\n            return wrapper\n        return encoder", "response": "A function that returns a native platform str."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconnects to the USMA API and create an authenticated session", "response": "def _connect(self):\n        \"\"\"\n        Retrieve token from USMA API and create an authenticated session\n        :returns OAuth2Session: authenticated client session\n        \"\"\"\n        oauth_client = BackendApplicationClient(client_id=self.client_id)\n        oauth_session = OAuth2Session(client=oauth_client)\n        token = oauth_session.fetch_token(token_url=self.url + \"oauth/token\",\n                                            client_id=self.client_id,\n                                            client_secret=self.client_secret)\n                                            #verify=False)\n        return OAuth2Session(client_id=self.client_id,\n                                    token=token)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _retrieve_items(self, num_items, params, item_type=\"events\", page=0):\n        try:\n            page = page\n            retries = 0\n            item_list = []\n            remaining = num_items\n            list_name = \"eventResourceList\" if item_type == \"events\" else \"alarms\"\n            if not params:\n                params = {}\n            # Loop if num_items > MAX_EVENTS as that is the max number of items in one page\n            while remaining and page < (num_items//MAX_EVENTS+1) and retries < MAX_RETRIES:\n                params[\"page\"] = page\n                params[\"size\"] = min(remaining, MAX_EVENTS)\n                #response = self.client.get(self.url+item_type, params=params, verify=False)\n                response = self.client.get(self.url+item_type, params=params)\n                if response.status_code == 200:\n                    self.log.warning(\"Got 200. Page info: {}.\".format(response.json()['page']))\n                    page += 1\n                    retries = 0\n                    items = response.json()['_embedded'][list_name]\n                    item_list.extend(items)\n                    remaining = min(remaining, response.json()['page']['totalElements'])\n                    remaining -= len(items)\n                else:\n                    retries += 1\n                    self.log.error(\"Got response {response.status_code} for page {params['page']} \" + \\\n                                   \"and size {params['size']}. Attempt {retries}.\")\n                    time.sleep(1)\n            return item_list\n\n        except TokenExpiredError as e:\n            self.log.error(\"ERROR: {e}. Token Expired.\")\n            self.client = self._connect()\n            return self._retrieve_items(remaining, params, item_type, page)\n\n        except KeyError as e:\n            self.log.error(\"ERROR: {e}. {response.json()['_links']}\")\n            return []", "response": "Retrieve num_items from item_type\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget events as list of dictionaries", "response": "def get_events_list(self, num_items=100, params=None):\n        \"\"\"\n        Get events as list of dictionaries\n        :param int num_items: Max items to retrieve\n        :param dict params: Additional params dictionary according to:\n            https://www.alienvault.com/documentation/api/usm-anywhere-api.htm#/events\n        :returns list: list of events\n        \"\"\"\n        if params and set(params.keys()) - VALID_EVENT_PARAMS:\n            self.log.error(\"Invalid alarm query parameters: {set(params.keys()) - VALID_ALARM_PARAMS}\")\n            return None\n        return self._retrieve_items(item_type=\"events\", num_items=num_items, params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget events as pandas DataFrame", "response": "def get_events_df(self, num_items=100, params=None, columns=None, drop_columns=None, convert_ips=True):\n        \"\"\"\n        Get events as pandas DataFrame\n        :param int num_items: Max items to retrieve\n        :param dict params: Additional params dictionary according to:\n            https://www.alienvault.com/documentation/api/usm-anywhere-api.htm#/events\n        :parama list columns: list of columns to include in DataFrame\n        :returns pandas.DataFrame: dataframe of events\n        \"\"\"\n        if params and set(params.keys()) - VALID_EVENT_PARAMS:\n            self.log.error(\"Invalid alarm query parameters: {set(params.keys()) - VALID_ALARM_PARAMS}\")\n            return None\n        events = self._retrieve_items(item_type=\"events\", num_items=num_items, params=params)\n        df = self._to_df(events, columns, drop_columns, convert_ips)\n        return df"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting alarms as list of dictionaries", "response": "def get_alarms_list(self, num_items=100, params=None):\n        \"\"\"\n        Get alarms as list of dictionaries\n        :param int num_items: Max items to retrieve\n        :param dict params: Additional params dictionary according to:\n            https://www.alienvault.com/documentation/api/usm-anywhere-api.htm#/alarms\n        :returns list: list of alarms\n        \"\"\"\n        if params and set(params.keys()) - VALID_ALARM_PARAMS:\n            self.log.error(\"Invalid alarm query parameters: {set(params.keys()) - VALID_ALARM_PARAMS}\")\n            return None\n        return self._retrieve_items(item_type=\"alarms\", num_items=num_items, params=params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrelates data attributes to graph structure and visual representation.", "response": "def bind(self, source=None, destination=None, node=None,\n             edge_title=None, edge_label=None, edge_color=None, edge_weight=None,\n             point_title=None, point_label=None, point_color=None, point_size=None):\n        \"\"\"Relate data attributes to graph structure and visual representation.\n\n        To facilitate reuse and replayable notebooks, the binding call is chainable. Invocation does not effect the old binding: it instead returns a new Plotter instance with the new bindings added to the existing ones. Both the old and new bindings can then be used for different graphs.\n\n\n        :param source: Attribute containing an edge's source ID\n        :type source: String.\n\n        :param destination: Attribute containing an edge's destination ID\n        :type destination: String.\n\n        :param node: Attribute containing a node's ID\n        :type node: String.\n\n        :param edge_title: Attribute overriding edge's minimized label text. By default, the edge source and destination is used.\n        :type edge_title: HtmlString.\n\n        :param edge_label: Attribute overriding edge's expanded label text. By default, scrollable list of attribute/value mappings.\n        :type edge_label: HtmlString.\n\n        :param edge_color: Attribute overriding edge's color. `See palette definitions <https://graphistry.github.io/docs/legacy/api/0.9.2/api.html#extendedpalette>`_ for values. Based on Color Brewer.\n        :type edge_color: String.\n\n        :param edge_weight: Attribute overriding edge weight. Default is 1. Advanced layout controls will relayout edges based on this value.\n        :type edge_weight: String.\n\n        :param point_title: Attribute overriding node's minimized label text. By default, the node ID is used.\n        :type point_title: HtmlString.\n\n        :param point_label: Attribute overriding node's expanded label text. By default, scrollable list of attribute/value mappings.\n        :type point_label: HtmlString.\n\n        :param point_color: Attribute overriding node's color. `See palette definitions <https://graphistry.github.io/docs/legacy/api/0.9.2/api.html#extendedpalette>`_ for values. Based on Color Brewer.\n        :type point_color: Integer.\n\n        :param point_size: Attribute overriding node's size. By default, uses the node degree. The visualization will normalize point sizes and adjust dynamically using semantic zoom.\n        :type point_size: HtmlString.\n\n        :returns: Plotter.\n        :rtype: Plotter.\n\n        **Example: Minimal**\n            ::\n\n                import graphistry\n                g = graphistry.bind()\n                g = g.bind(source='src', destination='dst')\n\n        **Example: Node colors**\n            ::\n\n                import graphistry\n                g = graphistry.bind()\n                g = g.bind(source='src', destination='dst',\n                           node='id', point_color='color')\n\n        **Example: Chaining**\n            ::\n\n                import graphistry\n                g = graphistry.bind(source='src', destination='dst', node='id')\n\n                g1 = g.bind(point_color='color1', point_size='size1')\n\n                g.bind(point_color='color1b')\n\n                g2a = g1.bind(point_color='color2a')\n                g2b = g1.bind(point_color='color2b', point_size='size2b')\n\n                g3a = g2a.bind(point_size='size3a')\n                g3b = g2b.bind(point_size='size3b')\n\n        In the above **Chaining** example, all bindings use src/dst/id. Colors and sizes bind to:\n            ::\n\n                g: default/default\n                g1: color1/size1\n                g2a: color2a/size1\n                g2b: color2b/size2b\n                g3a: color2a/size3a\n                g3b: color2b/size3b\n\n\n        \"\"\"\n        res = copy.copy(self)\n        res._source = source or self._source\n        res._destination = destination or self._destination\n        res._node = node or self._node\n\n        res._edge_title = edge_title or self._edge_title\n        res._edge_label = edge_label or self._edge_label\n        res._edge_color = edge_color or self._edge_color\n        res._edge_weight = edge_weight or self._edge_weight\n\n        res._point_title = point_title or self._point_title\n        res._point_label = point_label or self._point_label\n        res._point_color = point_color or self._point_color\n        res._point_size = point_size or self._point_size\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef nodes(self, nodes):\n\n\n        res = copy.copy(self)\n        res._nodes = nodes\n        return res", "response": "Specify the set of nodes and associated data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef edges(self, edges):\n\n        res = copy.copy(self)\n        res._edges = edges\n        return res", "response": "Specify edge list data and associated edge attribute values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef graph(self, ig):\n\n        res = copy.copy(self)\n        res._edges = ig\n        res._nodes = None\n        return res", "response": "Specify the node and edge data."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nspecifies iframe height and add URL parameter dictionary.", "response": "def settings(self, height=None, url_params={}, render=None):\n        \"\"\"Specify iframe height and add URL parameter dictionary.\n\n        The library takes care of URI component encoding for the dictionary.\n\n        :param height: Height in pixels.\n        :type height: Integer.\n\n        :param url_params: Dictionary of querystring parameters to append to the URL.\n        :type url_params: Dictionary\n\n        :param render: Whether to render the visualization using the native notebook environment (default True), or return the visualization URL\n        :type render: Boolean\n\n        \"\"\"\n\n        res = copy.copy(self)\n        res._height = height or self._height\n        res._url_params = dict(self._url_params, **url_params)\n        res._render = self._render if render == None else render\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot(self, graph=None, nodes=None, name=None, render=None, skip_upload=False):\n\n        if graph is None:\n            if self._edges is None:\n                util.error('Graph/edges must be specified.')\n            g = self._edges\n        else:\n            g = graph\n        n = self._nodes if nodes is None else nodes\n        name = name or util.random_string(10)\n\n        self._check_mandatory_bindings(not isinstance(n, type(None)))\n\n        api_version = PyGraphistry.api_version()\n        if (api_version == 1):\n            dataset = self._plot_dispatch(g, n, name, 'json')\n            if skip_upload:\n                return dataset\n            info = PyGraphistry._etl1(dataset)\n        elif (api_version == 2):\n            dataset = self._plot_dispatch(g, n, name, 'vgraph')\n            if skip_upload:\n                return dataset\n            info = PyGraphistry._etl2(dataset)\n\n        viz_url = PyGraphistry._viz_url(info, self._url_params)\n        full_url = '%s:%s' % (PyGraphistry._config['protocol'], viz_url)\n\n        if render == False or (render == None and not self._render):\n            return full_url\n        elif util.in_ipython():\n            from IPython.core.display import HTML\n            return HTML(util.make_iframe(viz_url, self._height, PyGraphistry._config['protocol']))\n        else:\n            import webbrowser\n            webbrowser.open(full_url)\n            return full_url", "response": "Upload data to the Graphistry server and show an iframe of it."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pandas2igraph(self, edges, directed=True):\n\n\n        import igraph\n        self._check_mandatory_bindings(False)\n        self._check_bound_attribs(edges, ['source', 'destination'], 'Edge')\n        \n        self._node = self._node or Plotter._defaultNodeId\n        eattribs = edges.columns.values.tolist()\n        eattribs.remove(self._source)\n        eattribs.remove(self._destination)\n        cols = [self._source, self._destination] + eattribs\n        etuples = [tuple(x) for x in edges[cols].values]\n        return igraph.Graph.TupleList(etuples, directed=directed, edge_attrs=eattribs,\n                                      vertex_name_attr=self._node)", "response": "Convert a pandas edge dataframe to an igraph graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef igraph2pandas(self, ig):\n\n        def get_edgelist(ig):\n            idmap = dict(enumerate(ig.vs[self._node]))\n            for e in ig.es:\n                t = e.tuple\n                yield dict({self._source: idmap[t[0]], self._destination: idmap[t[1]]},\n                            **e.attributes())\n\n        self._check_mandatory_bindings(False)\n        if self._node is None:\n            ig.vs[Plotter._defaultNodeId] = [v.index for v in ig.vs]\n            self._node = Plotter._defaultNodeId\n        elif self._node not in ig.vs.attributes():\n            util.error('Vertex attribute \"%s\" bound to \"node\" does not exist.' % self._node)\n\n        edata = get_edgelist(ig)\n        ndata = [v.attributes() for v in ig.vs]\n        nodes = pandas.DataFrame(ndata, columns=ig.vs.attributes())\n        cols = [self._source, self._destination] + ig.es.attributes()\n        edges = pandas.DataFrame(edata, columns=cols)\n        return (edges, nodes)", "response": "Under current bindings transform an IGraph into a pandas edges dataframe and a nodes dataframe."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef authenticate():\n        key = PyGraphistry.api_key()\n        #Mocks may set to True, so bypass in that case\n        if (key is None) and PyGraphistry._is_authenticated == False:\n            util.error('API key not set explicitly in `register()` or available at ' + EnvVarNames['api_key'])\n        if not PyGraphistry._is_authenticated:\n            PyGraphistry._check_key_and_version()\n            PyGraphistry._is_authenticated = True", "response": "Authenticate via already provided configuration."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the hostname of the server or set it using aliases.", "response": "def server(value=None):\n        \"\"\"Get the hostname of the server or set the server using hostname or aliases.\n        Supported aliases: 'localhost', 'staging', 'labs'.\n        Also set via environment variable GRAPHISTRY_HOSTNAME.\"\"\"\n        if value is None:\n            return PyGraphistry._config['hostname']\n\n        # setter\n        shortcuts = {'dev': 'localhost:3000',\n                     'staging': 'staging.graphistry.com',\n                     'labs': 'labs.graphistry.com'}\n        if value in shortcuts:\n            resolved = shortcuts[value]\n            PyGraphistry._config['hostname'] = resolved\n            util.warn('Resolving alias %s to %s' % (value, resolved))\n        else:\n            PyGraphistry._config['hostname'] = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef api_key(value=None):\n\n        if value is None:\n            return PyGraphistry._config['api_key']\n\n        # setter\n        if value is not PyGraphistry._config['api_key']:\n            PyGraphistry._config['api_key'] = value.strip()\n            PyGraphistry._is_authenticated = False", "response": "Set or get the API key."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef certificate_validation(value=None):\n        if value is None:\n            return PyGraphistry._config['certificate_validation']\n\n        # setter\n        v = bool(strtobool(value)) if isinstance(value, basestring) else value\n        if v == False:\n            requests.packages.urllib3.disable_warnings()\n        PyGraphistry._config['certificate_validation'] = v", "response": "Enable or disable SSL certificate validation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nregister an API key with the Graphistry server and set the relevant parameters.", "response": "def register(key=None, server=None, protocol=None, api=None, certificate_validation=None, bolt=None):\n        \"\"\"API key registration and server selection\n\n        Changing the key effects all derived Plotter instances.\n\n        :param key: API key.\n        :type key: String.\n        :param server: URL of the visualization server.\n        :type server: Optional string.\n        :param protocol: Protocol used to contact visualization server\n        :type protocol: Optional string.\n        :returns: None.\n        :rtype: None.\n\n\n        **Example: Standard**\n                ::\n\n                    import graphistry\n                    graphistry.register(key=\"my api key\")\n\n        **Example: Developer**\n                ::\n\n                    import graphistry\n                    graphistry.register('my api key', server='staging', protocol='https')\n\n\n        **Example: Through environment variable**\n                ::\n                    export GRAPHISTRY_API_KEY = 'my api key'\n                ::\n                    import graphistry\n                    graphistry.register()\n        \"\"\"\n        PyGraphistry.api_key(key)\n        PyGraphistry.server(server)\n        PyGraphistry.api_version(api)\n        PyGraphistry.protocol(protocol)\n        PyGraphistry.certificate_validation(certificate_validation)\n        PyGraphistry.authenticate()\n        PyGraphistry.set_bolt_driver(bolt)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntransforming a dataframe into a hypergraph.", "response": "def hypergraph(raw_events, entity_types=None, opts={}, drop_na=True, drop_edge_attrs=False, verbose=True, direct=False):\n        \"\"\"Transform a dataframe into a hypergraph.\n\n        :param Dataframe raw_events: Dataframe to transform\n        :param List entity_types: Optional list of columns (strings) to turn into nodes, None signifies all\n        :param Dict opts: See below\n        :param bool drop_edge_attrs: Whether to include each row's attributes on its edges, defaults to False (include)\n        :param bool verbose: Whether to print size information\n        :param bool direct: Omit hypernode and instead strongly connect nodes in an event\n\n        Create a graph out of the dataframe, and return the graph components as dataframes, \n        and the renderable result Plotter. It reveals relationships between the rows and between column values.\n        This transform is useful for lists of events, samples, relationships, and other structured high-dimensional data.\n\n        The transform creates a node for every row, and turns a row's column entries into node attributes. \n        If direct=False (default), every unique value within a column is also turned into a node. \n        Edges are added to connect a row's nodes to each of its column nodes, or if direct=True, to one another.\n        Nodes are given the attribute 'type' corresponding to the originating column name, or in the case of a row, 'EventID'.\n\n\n        Consider a list of events. Each row represents a distinct event, and each column some metadata about an event. \n        If multiple events have common metadata, they will be transitively connected through those metadata values. \n        The layout algorithm will try to cluster the events together. \n        Conversely, if an event has unique metadata, the unique metadata will turn into nodes that only have connections to the event node, and the clustering algorithm will cause them to form a ring around the event node.\n\n        Best practice is to set EVENTID to a row's unique ID,\n        SKIP to all non-categorical columns (or entity_types to all categorical columns),\n        and CATEGORY to group columns with the same kinds of values.\n\n\n        The optional ``opts={...}`` configuration options are:\n\n        * 'EVENTID': Column name to inspect for a row ID. By default, uses the row index.\n        * 'CATEGORIES': Dictionary mapping a category name to inhabiting columns. E.g., {'IP': ['srcAddress', 'dstAddress']}.  If the same IP appears in both columns, this makes the transform generate one node for it, instead of one for each column.\n        * 'DELIM': When creating node IDs, defines the separator used between the column name and node value\n        * 'SKIP': List of column names to not turn into nodes. For example, dates and numbers are often skipped.\n        * 'EDGES': For direct=True, instead of making all edges, pick column pairs. E.g., {'a': ['b', 'd'], 'd': ['d']} creates edges between columns a->b and a->d, and self-edges d->d.\n\n\n        :returns: {'entities': DF, 'events': DF, 'edges': DF, 'nodes': DF, 'graph': Plotter}\n        :rtype: Dictionary\n\n        **Example**\n\n            ::\n\n                import graphistry\n                h = graphistry.hypergraph(my_df)\n                g = h['graph'].plot()\n\n        \"\"\"\n        from . import hyper\n        return hyper.Hypergraph().hypergraph(PyGraphistry, raw_events, entity_types, opts, drop_na, drop_edge_attrs, verbose, direct)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a base plotter. This function is called at start of a program.", "response": "def bind(node=None, source=None, destination=None,\n             edge_title=None, edge_label=None, edge_color=None, edge_weight=None,\n             point_title=None, point_label=None, point_color=None, point_size=None):\n        \"\"\"Create a base plotter.\n\n        Typically called at start of a program. For parameters, see ``plotter.bind()`` .\n\n        :returns: Plotter.\n        :rtype: Plotter.\n\n        **Example**\n\n                ::\n\n                    import graphistry\n                    g = graphistry.bind()\n\n        \"\"\"\n\n\n\n        from . import plotter\n        return plotter.Plotter().bind(source, destination, node, \\\n                              edge_title, edge_label, edge_color, edge_weight, \\\n                              point_title, point_label, point_color, point_size)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_GET(self):\n        self.do_HEAD()\n        template = pkg_resources.resource_string(__name__, \"static/splash.html\")\n        page = template.decode('utf-8').format(self.headers.get(\"User-Agent\"), self.cache)\n        self.wfile.write(page.encode('utf-8'))", "response": "Serve a GET request."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nserves a HEAD request.", "response": "def do_HEAD(self):\n        \"\"\"Serve a HEAD request.\"\"\"\n        self.queue.put(self.headers.get(\"User-Agent\"))\n        self.send_response(six.moves.BaseHTTPServer.HTTPStatus.OK)\n        self.send_header(\"Location\", self.path)\n        self.end_headers()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pages(self):\n        # type: () -> ProfileIterator\n        \"\"\"Obtain an iterator over Instagram post pages.\n\n        Returns:\n            PageIterator: an iterator over the instagram post pages.\n\n        Raises:\n            ValueError: when the requested user does not exist.\n            RuntimeError: when the user is a private account\n                and there is no logged user (or the logged user\n                does not follow that account).\n\n        \"\"\"\n        if self._owner_id is None:\n            it = ProfileIterator.from_username(self._username, self.session)\n            self._owner_id = it.owner_id\n            return it\n        return ProfileIterator(self._owner_id, self.session, self.rhx)", "response": "Obtain an iterator over the instagram post pages."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a generator that yields only the refered post.", "response": "def medias(self, timeframe=None):\n        \"\"\"Return a generator that yields only the refered post.\n\n        Yields:\n            dict: a media dictionary obtained from the given post.\n\n        Raises:\n            StopIteration: if the post does not fit the timeframe.\n\n        \"\"\"\n        info = self.info\n        if timeframe is not None:\n            start, end = TimedMediasIterator.get_times(timeframe)\n            timestamp = info.get(\"taken_at_timestamp\") or info[\"media\"]\n            if not (start >= timestamp >= end):\n                raise StopIteration\n        yield info"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndownload the refered post to the destination.", "response": "def download(self,\n                 destination,       # type: Union[str, fs.base.FS]\n                 condition=None,    # type: Optional[Callable[[dict], bool]]\n                 media_count=None,  # type: Optional[int]\n                 timeframe=None,    # type: Optional[_Timeframe]\n                 new_only=False,    # type: bool\n                 pgpbar_cls=None,   # type: Optional[Type[ProgressBar]]\n                 dlpbar_cls=None,   # type: Optional[Type[ProgressBar]]\n                 ):\n        # type: (...) -> int\n        \"\"\"Download the refered post to the destination.\n\n        See `InstaLooter.download` for argument reference.\n\n        Note:\n            This function, opposed to other *looter* implementations, will\n            not spawn new threads, but simply use the main thread to download\n            the files.\n\n            Since a worker is in charge of downloading a *media* at a time\n            (and not a *file*), there would be no point in spawning more.\n\n        \"\"\"\n        destination, close_destination = self._init_destfs(destination)\n\n        queue = Queue()                           # type: Queue[Dict]\n        medias_queued = self._fill_media_queue(\n            queue, destination, iter(self.medias()), media_count,\n            new_only, condition)\n        queue.put(None)\n\n        worker = InstaDownloader(\n            queue=queue,\n            destination=destination,\n            namegen=self.namegen,\n            add_metadata=self.add_metadata,\n            dump_json=self.dump_json,\n            dump_only=self.dump_only,\n            pbar=None,\n            session=self.session)\n        worker.run()\n\n        return medias_queued"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef warn_logging(logger):\n    # type: (logging.Logger) -> Callable\n    \"\"\"Create a `showwarning` function that uses the given logger.\n\n    Arguments:\n        logger (~logging.Logger): the logger to use.\n\n    Returns:\n        function: a function that can be used as the `warnings.showwarning`\n            callback.\n\n    \"\"\"\n    def showwarning(message, category, filename, lineno, file=None, line=None):\n        logger.warning(message)\n    return showwarning", "response": "Create a showwarning function that uses the given logger."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wrap_warnings(logger):\n    def decorator(func):\n        @functools.wraps(func)\n        def new_func(*args, **kwargs):\n            showwarning = warnings.showwarning\n            warnings.showwarning = warn_logging(logger)\n            try:\n                return func(*args, **kwargs)\n            finally:\n                warnings.showwarning = showwarning\n        return new_func\n    return decorator", "response": "Decorator to wrap warnings. showwarning with the given logger."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef date_from_isoformat(isoformat_date):\n    year, month, day = isoformat_date.split('-')\n    return datetime.date(int(year), int(month), int(day))", "response": "Convert an ISO - 8601 date into a datetime. date object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a CLI token to a datetime tuple.", "response": "def get_times_from_cli(cli_token):\n    \"\"\"Convert a CLI token to a datetime tuple.\n\n    Argument:\n        cli_token (str): an isoformat datetime token ([ISO date]:[ISO date])\n            or a special value among:\n                * thisday\n                * thisweek\n                * thismonth\n                * thisyear\n\n    Returns:\n        tuple: a datetime.date objects couple, where the first item is\n            the start of a time frame and the second item the end of the\n            time frame. Both elements can also be None, if no date was\n            provided.\n\n    Raises:\n        ValueError: when the CLI token is not in the right format\n            (no colon in the token, not one of the special values, dates\n            are not in proper ISO-8601 format.)\n\n    See Also:\n        `ISO-8601 specification <https://en.wikipedia.org/wiki/ISO_8601>`_.\n\n    \"\"\"\n    today = datetime.date.today()\n\n    if cli_token==\"thisday\":\n        return today, today\n    elif cli_token==\"thisweek\":\n        return today, today - dateutil.relativedelta.relativedelta(days=7)\n    elif cli_token==\"thismonth\":\n        return today, today - dateutil.relativedelta.relativedelta(months=1)\n    elif cli_token==\"thisyear\":\n        return today, today - dateutil.relativedelta.relativedelta(years=1)\n    else:\n        try:\n            start_date, stop_date = cli_token.split(':')\n        except ValueError:\n            raise ValueError(\"--time parameter must contain a colon (:)\")\n        if not start_date and not stop_date: # ':', no start date, no stop date\n            return None, None\n        try:\n            start_date = date_from_isoformat(start_date) if start_date else None\n            stop_date = date_from_isoformat(stop_date) if stop_date else None\n        except ValueError:\n            raise ValueError(\"--time parameter was not provided ISO formatted dates\")\n        return start_date, stop_date"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns from the command line interface. Arguments: argv (list): The positional arguments to read. Defaults to `sys.argv` to use CLI arguments. stream (~io.IOBase): A file where to write error messages. Leave to `None` to use the `~coloredlogs.StandardErrorHandler` for logs, and `sys.stderr` for error messages. Returns: int: An error code, or 0 if the program executed successfully.", "response": "def main(argv=None, stream=None):\n    \"\"\"Run from the command line interface.\n\n    Arguments:\n        argv (list): The positional arguments to read. Defaults to\n            `sys.argv` to use CLI arguments.\n        stream (~io.IOBase): A file where to write error messages.\n            Leave to `None` to use the `~coloredlogs.StandardErrorHandler`\n            for logs, and `sys.stderr` for error messages.\n\n    Returns:\n        int: An error code, or 0 if the program executed successfully.\n    \"\"\"\n\n    _print = functools.partial(print, file=stream or sys.stderr)\n\n    # Parse command line arguments\n    try:\n        args = docopt.docopt(\n            HELP, argv, version='instalooter {}'.format(__version__))\n    except docopt.DocoptExit as de:\n        _print(de)\n        return 1\n\n    # Print usage and exit if required (docopt does not do this !)\n    if args['--usage']:\n        _print(USAGE)\n        return 0\n\n    # Set the loggers up with the requested logging level\n    level = \"ERROR\" if args['--quiet'] else args.get(\"--loglevel\", \"INFO\")\n    for logger_ in (logger, login_logger, batch_logger):\n        coloredlogs.install(\n            level=int(level) if level.isdigit() else level,\n            stream=stream,\n            logger=logger_)\n\n    # Check the requested logging level\n    if args['-W'] not in WARNING_ACTIONS:\n        _print(\"Unknown warning action:\", args['-W'])\n        _print(\"    available actions:\", ', '.join(WARNING_ACTIONS))\n        return 1\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(args['-W'])\n\n        try:\n            # Run in batch mode\n            if args['batch']:\n                # Load the batch configuration from the given file\n                with open(args['<batch_file>']) as batch_file:\n                    batch_runner = BatchRunner(batch_file, args)\n                # Run the batch\n                batch_runner.run_all()\n                return 0\n\n            # Login if requested\n            if args['login']:\n                try:\n                    if not args['--username']:\n                        args['--username'] = six.moves.input('Username: ')\n                    login(args)\n                    return 0\n                except ValueError as ve:\n                    logger.error(\"%s\", ve)\n                    if args[\"--traceback\"]:\n                       traceback.print_exc()\n                    return 1\n\n            # Logout if requested\n            if args['logout']:\n                if InstaLooter._cachefs.exists(InstaLooter._COOKIE_FILE):\n                    InstaLooter._logout()\n                    logger.success('Logged out.')\n                else:\n                    warnings.warn('Cookie file not found.')\n                return 0\n\n            # Normal download mode:\n            if args['user']:\n                looter_cls = ProfileLooter\n                target = args['<profile>']\n            elif args['hashtag']:\n                looter_cls = HashtagLooter\n                target = args['<hashtag>']\n            elif args['post']:\n                looter_cls = PostLooter\n                target = args['<post_token>']\n            else:\n                raise NotImplementedError(\"TODO\")\n\n            # Instantiate the looter\n            looter = looter_cls(\n                target,\n                add_metadata=args['--add-metadata'],\n                get_videos=args['--get-videos'],\n                videos_only=args['--videos-only'],\n                jobs=int(args['--jobs']) if args['--jobs'] is not None else 16,\n                template=args['--template'],\n                dump_json=args['--dump-json'],\n                dump_only=args['--dump-only'],\n                extended_dump=args['--extended-dump']\n            )\n\n            # Attempt to login and extract the timeframe\n            if args['--username']:\n                login(args)\n            if args['--num-to-dl']:\n                args['--num-to-dl'] = int(args['--num-to-dl'])\n            try:\n                if args['--time'] is not None:\n                    args['--time'] = get_times_from_cli(args['--time'])\n            except ValueError as ve:\n                _print(\"invalid format for --time parameter:\", args[\"--time\"])\n                _print(\"    (format is [D]:[D] where D is an ISO 8601 date)\")\n                return 1\n\n            logger.debug(\"Opening destination filesystem\")\n            dest_url = args.get('<directory>') or os.getcwd()\n            dest_fs = fs.open_fs(dest_url, create=True)\n\n            logger.notice(\"Starting download of `%s`\", target)\n            n = looter.download(\n                destination=dest_fs,\n                media_count=args['--num-to-dl'],\n                timeframe=args['--time'],\n                new_only=args['--new'],\n                pgpbar_cls=None if args['--quiet'] else TqdmProgressBar,\n                dlpbar_cls=None if args['--quiet'] else TqdmProgressBar)\n            if n > 1:\n                logger.success(\"Downloaded %i posts.\", n)\n            elif n == 1:\n                logger.success(\"Downloaded %i post.\", n)\n\n        except (Exception, KeyboardInterrupt) as e:\n            from .threadutils import threads_force_join, threads_count\n            # Show error traceback if any\n            if not isinstance(e, KeyboardInterrupt):\n                logger.critical(\"%s\", e)\n                if args[\"--traceback\"]:\n                    traceback.print_exc()\n            else:\n                logger.critical(\"Interrupted\")\n            # Close remaining threads spawned by InstaLooter.download\n            count = threads_count()\n            if count:\n                logger.notice(\"Terminating %i remaining workers...\", count)\n                threads_force_join()\n            # Return the error number if any\n            errno = e.errno if hasattr(e, \"errno\") else None\n            return errno if errno is not None else 1\n\n        else:\n            return 0\n\n        finally:\n            logger.debug(\"Closing destination filesystem\")\n            try:\n                dest_fs.close()\n            except Exception:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning all the jobs in the configuration file.", "response": "def run_all(self):\n        # type: () -> None\n        \"\"\"Run all the jobs specified in the configuration file.\n        \"\"\"\n        logger.debug(\"Creating batch session\")\n        session = Session()\n\n        for section_id in self.parser.sections():\n            self.run_job(section_id, session=session)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_job(self, section_id, session=None):\n        # type: (Text, Optional[Session]) -> None\n        \"\"\"Run a job as described in the section named ``section_id``.\n\n        Raises:\n            KeyError: when the section could not be found.\n\n        \"\"\"\n        if not self.parser.has_section(section_id):\n            raise KeyError('section not found: {}'.format(section_id))\n\n        session = session or Session()\n\n        for name, looter_cls in six.iteritems(self._CLS_MAP):\n\n                targets = self.get_targets(self._get(section_id, name))\n                quiet = self._getboolean(\n                    section_id, \"quiet\", self.args.get(\"--quiet\", False))\n\n                if targets:\n                    logger.info(\"Launching {} job for section {}\".format(name, section_id))\n\n                for target, directory in six.iteritems(targets):\n                    try:\n                        logger.info(\"Downloading {} to {}\".format(target, directory))\n                        looter = looter_cls(\n                            target,\n                            add_metadata=self._getboolean(section_id, 'add-metadata', False),\n                            get_videos=self._getboolean(section_id, 'get-videos', False),\n                            videos_only=self._getboolean(section_id, 'videos-only', False),\n                            jobs=self._getint(section_id, 'jobs', 16),\n                            template=self._get(section_id, 'template', '{id}'),\n                            dump_json=self._getboolean(section_id, 'dump-json', False),\n                            dump_only=self._getboolean(section_id, 'dump-only', False),\n                            extended_dump=self._getboolean(section_id, 'extended-dump', False),\n                            session=session)\n\n                        if self.parser.has_option(section_id, 'username'):\n                            looter.logout()\n                            username = self._get(section_id, 'username')\n                            password = self._get(section_id, 'password') or \\\n                                getpass.getpass('Password for \"{}\": '.format(username))\n                            looter.login(username, password)\n\n                        n = looter.download(\n                            directory,\n                            media_count=self._getint(section_id, 'num-to-dl'),\n                            # FIXME: timeframe=self._get(section_id, 'timeframe'),\n                            new_only=self._getboolean(section_id, 'new', False),\n                            pgpbar_cls=None if quiet else TqdmProgressBar,\n                            dlpbar_cls=None if quiet else TqdmProgressBar)\n\n                        logger.success(\"Downloaded %i medias !\", n)\n\n                    except Exception as exception:\n                        logger.error(six.text_type(exception))", "response": "Runs a job in the section named section_id."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_targets(self, raw_string):\n        # type: (Optional[Text]) -> Dict[Text, Text]\n        \"\"\"Extract targets from a string in 'key: value' format.\n        \"\"\"\n        targets = {}\n        if raw_string is not None:\n            for line in raw_string.splitlines():\n                if line:\n                    target, directory = line.split(':', 1)\n                    targets[target.strip()] = directory.strip()\n        return targets", "response": "Extract targets from a string in key : value format."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef html_page_context(app, pagename, templatename, context, doctree):\n    rendered_toc = get_rendered_toctree(app.builder, pagename)\n    context['toc'] = rendered_toc\n    context['display_toc'] = True  # force toctree to display\n\n    if \"toctree\" not in context:\n        # json builder doesn't use toctree func, so nothing to replace\n        return\n\n    def make_toctree(collapse=True):\n        return get_rendered_toctree(app.builder,\n                                    pagename,\n                                    prune=False,\n                                    collapse=collapse,\n                                    )\n\n    context['toctree'] = make_toctree", "response": "Event handler for the html - page - context signal."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_rendered_toctree(builder, docname, prune=False, collapse=True):\n    fulltoc = build_full_toctree(builder,\n                                 docname,\n                                 prune=prune,\n                                 collapse=collapse,\n                                 )\n    rendered_toc = builder.render_partial(fulltoc)['fragment']\n    return rendered_toc", "response": "Build the toctree relative to the named document and then return the rendered HTML fragment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_full_toctree(builder, docname, prune, collapse):\n    env = builder.env\n    doctree = env.get_doctree(env.config.master_doc)\n    toctrees = []\n    for toctreenode in doctree.traverse(addnodes.toctree):\n        toctree = env.resolve_toctree(docname, builder, toctreenode,\n                                      collapse=collapse,\n                                      prune=prune,\n                                      )\n        toctrees.append(toctree)\n    if not toctrees:\n        return None\n    result = toctrees[0]\n    for toctree in toctrees[1:]:\n        if toctree:\n            result.extend(toctree.children)\n    env.resolve_references(result, docname, builder)\n    return result", "response": "Build a full toctree starting from docname containing all\n    sub - documents."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the cluster settings from the instance metadata.", "response": "def _readClusterSettings(self):\n        \"\"\"\n        Reads the cluster settings from the instance metadata, which assumes the instance\n        is the leader.\n        \"\"\"\n        instanceMetaData = get_instance_metadata()\n        region = zoneToRegion(self._zone)\n        conn = boto.ec2.connect_to_region(region)\n        instance = conn.get_all_instances(instance_ids=[instanceMetaData[\"instance-id\"]])[0].instances[0]\n        self.clusterName = str(instance.tags[\"Name\"])\n        self._buildContext()\n        self._subnetID = instance.subnet_id\n        self._leaderPrivateIP = instanceMetaData['local-ipv4']  # this is PRIVATE IP\n        self._keyName = list(instanceMetaData['public-keys'].keys())[0]\n        self._tags = self.getLeader().tags\n        self._masterPublicKey = self._setSSH()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlaunches a new cluster.", "response": "def launchCluster(self, leaderNodeType, leaderStorage, owner, **kwargs):\n        \"\"\"\n        In addition to the parameters inherited from the abstractProvisioner,\n        the AWS launchCluster takes the following parameters:\n        keyName: The key used to communicate with instances\n        vpcSubnet: A subnet (optional).\n        \"\"\"\n        if 'keyName' not in kwargs:\n            raise RuntimeError(\"A keyPairName is required for the AWS provisioner.\")\n        self._keyName = kwargs['keyName']\n        self._vpcSubnet = kwargs['vpcSubnet'] if 'vpcSubnet' in kwargs else None\n\n        profileARN = self._getProfileARN()\n        # the security group name is used as the cluster identifier\n        sgs = self._createSecurityGroup()\n        bdm = self._getBlockDeviceMapping(E2Instances[leaderNodeType], rootVolSize=leaderStorage)\n\n        self._masterPublicKey = 'AAAAB3NzaC1yc2Enoauthorizedkeyneeded' # dummy key\n        userData =  self._getCloudConfigUserData('leader', self._masterPublicKey)\n        specKwargs = {'key_name': self._keyName, 'security_group_ids': [sg.id for sg in sgs],\n                  'instance_type': leaderNodeType,\n                  'user_data': userData, 'block_device_map': bdm,\n                  'instance_profile_arn': profileARN,\n                  'placement': self._zone}\n        if self._vpcSubnet:\n            specKwargs[\"subnet_id\"] = self._vpcSubnet\n        instances = create_ondemand_instances(self._ctx.ec2, image_id=self._discoverAMI(),\n                                                  spec=specKwargs, num_instances=1)\n\n        # wait for the leader to finish setting up\n        leader = instances[0]\n        wait_instances_running(self._ctx.ec2, [leader])\n        self._waitForIP(leader)\n        leaderNode = Node(publicIP=leader.ip_address, privateIP=leader.private_ip_address,\n                          name=leader.id, launchTime=leader.launch_time, nodeType=leaderNodeType,\n                          preemptable=False, tags=leader.tags)\n        leaderNode.waitForNode('toil_leader')\n\n        defaultTags = {'Name': self.clusterName, 'Owner': owner}\n        if kwargs['userTags']:\n            defaultTags.update(kwargs['userTags'])\n\n        # if we running launch cluster we need to save this data as it won't be generated\n        # from the metadata. This data is needed to launch worker nodes.\n        self._leaderPrivateIP = leader.private_ip_address\n        self._addTags([leader], defaultTags)\n        self._tags = leader.tags\n        self._subnetID = leader.subnet_id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nterminating instances and delete the profile and security group.", "response": "def destroyCluster(self):\n        \"\"\"\n        Terminate instances and delete the profile and security group.\n        \"\"\"\n        assert self._ctx\n        def expectedShutdownErrors(e):\n            return e.status == 400 and 'dependent object' in e.body\n\n        instances = self._getNodesInCluster(nodeType=None, both=True)\n        spotIDs = self._getSpotRequestIDs()\n        if spotIDs:\n            self._ctx.ec2.cancel_spot_instance_requests(request_ids=spotIDs)\n        instancesToTerminate = awsFilterImpairedNodes(instances, self._ctx.ec2)\n        vpcId = None\n        if instancesToTerminate:\n            vpcId = instancesToTerminate[0].vpc_id\n            self._deleteIAMProfiles(instances=instancesToTerminate)\n            self._terminateInstances(instances=instancesToTerminate)\n        if len(instances) == len(instancesToTerminate):\n            logger.debug('Deleting security group...')\n            removed = False\n            for attempt in retry(timeout=300, predicate=expectedShutdownErrors):\n                with attempt:\n                    for sg in self._ctx.ec2.get_all_security_groups():\n                        if sg.name == self.clusterName and vpcId and sg.vpc_id == vpcId:\n                            try:\n                                self._ctx.ec2.delete_security_group(group_id=sg.id)\n                                removed = True\n                            except BotoServerError as e:\n                                if e.error_code == 'InvalidGroup.NotFound':\n                                    pass\n                                else:\n                                    raise\n            if removed:\n                logger.debug('... Succesfully deleted security group')\n        else:\n            assert len(instances) > len(instancesToTerminate)\n            # the security group can't be deleted until all nodes are terminated\n            logger.warning('The TOIL_AWS_NODE_DEBUG environment variable is set and some nodes '\n                           'have failed health checks. As a result, the security group & IAM '\n                           'roles will not be deleted.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwaits until the instance has a public IP address assigned to it.", "response": "def _waitForIP(cls, instance):\n        \"\"\"\n        Wait until the instances has a public IP address assigned to it.\n\n        :type instance: boto.ec2.instance.Instance\n        \"\"\"\n        logger.debug('Waiting for ip...')\n        while True:\n            time.sleep(a_short_time)\n            instance.update()\n            if instance.ip_address or instance.public_dns_name or instance.private_ip_address:\n                logger.debug('...got ip')\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwaiting until the specified EC2 resource transitions from one of the given states to another.", "response": "def wait_transition(resource, from_states, to_state,\n                    state_getter=attrgetter('state')):\n    \"\"\"\n    Wait until the specified EC2 resource (instance, image, volume, ...) transitions from any\n    of the given 'from' states to the specified 'to' state. If the instance is found in a state\n    other that the to state or any of the from states, an exception will be thrown.\n\n    :param resource: the resource to monitor\n    :param from_states:\n        a set of states that the resource is expected to be in before the  transition occurs\n    :param to_state: the state of the resource when this method returns\n    \"\"\"\n    state = state_getter(resource)\n    while state in from_states:\n        time.sleep(a_short_time)\n        for attempt in retry_ec2():\n            with attempt:\n                resource.update(validate=True)\n        state = state_getter(resource)\n    if state != to_state:\n        raise UnexpectedResourceState(resource, to_state, state)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nyields every instance that is in the running state. Yields every instance that is in the pending state and every instance in the given iterable is running as soon as it does.", "response": "def wait_instances_running(ec2, instances):\n    \"\"\"\n    Wait until no instance in the given iterable is 'pending'. Yield every instance that\n    entered the running state as soon as it does.\n\n    :param boto.ec2.connection.EC2Connection ec2: the EC2 connection to use for making requests\n    :param Iterator[Instance] instances: the instances to wait on\n    :rtype: Iterator[Instance]\n    \"\"\"\n    running_ids = set()\n    other_ids = set()\n    while True:\n        pending_ids = set()\n        for i in instances:\n            if i.state == 'pending':\n                pending_ids.add(i.id)\n            elif i.state == 'running':\n                assert i.id not in running_ids\n                running_ids.add(i.id)\n                yield i\n            else:\n                assert i.id not in other_ids\n                other_ids.add(i.id)\n                yield i\n        log.info('%i instance(s) pending, %i running, %i other.',\n                 *map(len, (pending_ids, running_ids, other_ids)))\n        if not pending_ids:\n            break\n        seconds = max(a_short_time, min(len(pending_ids), 10 * a_short_time))\n        log.info('Sleeping for %is', seconds)\n        time.sleep(seconds)\n        for attempt in retry_ec2():\n            with attempt:\n                instances = ec2.get_only_instances(list(pending_ids))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nyielding spot requests as soon as they leave the open state or a timeout occurs.", "response": "def wait_spot_requests_active(ec2, requests, timeout=None, tentative=False):\n    \"\"\"\n    Wait until no spot request in the given iterator is in the 'open' state or, optionally,\n    a timeout occurs. Yield spot requests as soon as they leave the 'open' state.\n\n    :param Iterator[SpotInstanceRequest] requests:\n\n    :param float timeout: Maximum time in seconds to spend waiting or None to wait forever. If a\n    timeout occurs, the remaining open requests will be cancelled.\n\n    :param bool tentative: if True, give up on a spot request at the earliest indication of it\n    not being fulfilled immediately\n\n    :rtype: Iterator[list[SpotInstanceRequest]]\n    \"\"\"\n\n    if timeout is not None:\n        timeout = time.time() + timeout\n    active_ids = set()\n    other_ids = set()\n    open_ids = None\n\n    def cancel():\n        log.warn('Cancelling remaining %i spot requests.', len(open_ids))\n        ec2.cancel_spot_instance_requests(list(open_ids))\n\n    def spot_request_not_found(e):\n        error_code = 'InvalidSpotInstanceRequestID.NotFound'\n        return isinstance(e, EC2ResponseError) and e.error_code == error_code\n\n    try:\n        while True:\n            open_ids, eval_ids, fulfill_ids = set(), set(), set()\n            batch = []\n            for r in requests:\n                if r.state == 'open':\n                    open_ids.add(r.id)\n                    if r.status.code == 'pending-evaluation':\n                        eval_ids.add(r.id)\n                    elif r.status.code == 'pending-fulfillment':\n                        fulfill_ids.add(r.id)\n                    else:\n                        log.info(\n                            'Request %s entered status %s indicating that it will not be '\n                            'fulfilled anytime soon.', r.id, r.status.code)\n                elif r.state == 'active':\n                    assert r.id not in active_ids\n                    active_ids.add(r.id)\n                    batch.append(r)\n                else:\n                    assert r.id not in other_ids\n                    other_ids.add(r.id)\n                    batch.append(r)\n            if batch:\n                yield batch\n            log.info('%i spot requests(s) are open (%i of which are pending evaluation and %i '\n                     'are pending fulfillment), %i are active and %i are in another state.',\n                     *map(len, (open_ids, eval_ids, fulfill_ids, active_ids, other_ids)))\n            if not open_ids or tentative and not eval_ids and not fulfill_ids:\n                break\n            sleep_time = 2 * a_short_time\n            if timeout is not None and time.time() + sleep_time >= timeout:\n                log.warn('Timed out waiting for spot requests.')\n                break\n            log.info('Sleeping for %is', sleep_time)\n            time.sleep(sleep_time)\n            for attempt in retry_ec2(retry_while=spot_request_not_found):\n                with attempt:\n                    requests = ec2.get_all_spot_instance_requests(\n                        list(open_ids))\n    except BaseException:\n        if open_ids:\n            with panic(log):\n                cancel()\n        raise\n    else:\n        if open_ids:\n            cancel()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_spot_instances(ec2, price, image_id, spec, num_instances=1, timeout=None, tentative=False, tags=None):\n    def spotRequestNotFound(e):\n        return e.error_code == \"InvalidSpotInstanceRequestID.NotFound\"\n\n    for attempt in retry_ec2(retry_for=a_long_time,\n                             retry_while=inconsistencies_detected):\n        with attempt:\n            requests = ec2.request_spot_instances(\n                price, image_id, count=num_instances, **spec)\n\n    if tags is not None:\n        for requestID in (request.id for request in requests):\n            for attempt in retry_ec2(retry_while=spotRequestNotFound):\n                with attempt:\n                    ec2.create_tags([requestID], tags)\n\n    num_active, num_other = 0, 0\n    # noinspection PyUnboundLocalVariable,PyTypeChecker\n    # request_spot_instances's type annotation is wrong\n    for batch in wait_spot_requests_active(ec2,\n                                           requests,\n                                           timeout=timeout,\n                                           tentative=tentative):\n        instance_ids = []\n        for request in batch:\n            if request.state == 'active':\n                instance_ids.append(request.instance_id)\n                num_active += 1\n            else:\n                log.info(\n                    'Request %s in unexpected state %s.',\n                    request.id,\n                    request.state)\n                num_other += 1\n        if instance_ids:\n            # This next line is the reason we batch. It's so we can get multiple instances in\n            # a single request.\n            yield ec2.get_only_instances(instance_ids)\n    if not num_active:\n        message = 'None of the spot requests entered the active state'\n        if tentative:\n            log.warn(message + '.')\n        else:\n            raise RuntimeError(message)\n    if num_other:\n        log.warn('%i request(s) entered a state other than active.', num_other)", "response": "Yields the list of instances that are in the active state and the other instances in the active state."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates num_instances on demand instances for the specified image.", "response": "def create_ondemand_instances(ec2, image_id, spec, num_instances=1):\n    \"\"\"\n    Requests the RunInstances EC2 API call but accounts for the race between recently created\n    instance profiles, IAM roles and an instance creation that refers to them.\n\n    :rtype: list[Instance]\n    \"\"\"\n    instance_type = spec['instance_type']\n    log.info('Creating %s instance(s) ... ', instance_type)\n    for attempt in retry_ec2(retry_for=a_long_time,\n                             retry_while=inconsistencies_detected):\n        with attempt:\n            return ec2.run_instances(image_id,\n                                     min_count=num_instances,\n                                     max_count=num_instances,\n                                     **spec).instances"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef distVersion():\n    from pkg_resources import parse_version\n    build_number = buildNumber()\n    parsedBaseVersion = parse_version(baseVersion)\n    if isinstance(parsedBaseVersion, tuple):\n        raise RuntimeError(\"Setuptools version 8.0 or newer required. Update by running \"\n                           \"'pip install setuptools --upgrade'\")\n\n    if build_number is not None and parsedBaseVersion.is_prerelease:\n        return baseVersion + '.dev' + build_number\n    else:\n        return baseVersion", "response": "Returns the distribution version identifying a published release on PyPI."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns a parasol command and returns the exit code and the output of the command.", "response": "def _runParasol(self, command, autoRetry=True):\n        \"\"\"\n        Issues a parasol command using popen to capture the output. If the command fails then it\n        will try pinging parasol until it gets a response. When it gets a response it will\n        recursively call the issue parasol command, repeating this pattern for a maximum of N\n        times. The final exit value will reflect this.\n        \"\"\"\n        command = list(concat(self.parasolCommand, command))\n        while True:\n            logger.debug('Running %r', command)\n            process = subprocess.Popen(command,\n                                       stdout=subprocess.PIPE,\n                                       stderr=subprocess.PIPE,\n                                       bufsize=-1)\n            stdout, stderr = process.communicate()\n            status = process.wait()\n            for line in stderr.decode('utf-8').split('\\n'):\n                if line: logger.warn(line)\n            if status == 0:\n                return 0, stdout.decode('utf-8').split('\\n')\n            message = 'Command %r failed with exit status %i' % (command, status)\n            if autoRetry:\n                logger.warn(message)\n            else:\n                logger.error(message)\n                return status, None\n            logger.warn('Waiting for a 10s, before trying again')\n            time.sleep(10)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nissuing a batch job.", "response": "def issueBatchJob(self, jobNode):\n        \"\"\"\n        Issues parasol with job commands.\n        \"\"\"\n        self.checkResourceRequest(jobNode.memory, jobNode.cores, jobNode.disk)\n\n        MiB = 1 << 20\n        truncatedMemory = (old_div(jobNode.memory, MiB)) * MiB\n        # Look for a batch for jobs with these resource requirements, with\n        # the memory rounded down to the nearest megabyte. Rounding down\n        # meams the new job can't ever decrease the memory requirements\n        # of jobs already in the batch.\n        if len(self.resultsFiles) >= self.maxBatches:\n            raise RuntimeError( 'Number of batches reached limit of %i' % self.maxBatches)\n        try:\n            results = self.resultsFiles[(truncatedMemory, jobNode.cores)]\n        except KeyError:\n            results = getTempFile(rootDir=self.parasolResultsDir)\n            self.resultsFiles[(truncatedMemory, jobNode.cores)] = results\n\n        # Prefix the command with environment overrides, optionally looking them up from the\n        # current environment if the value is None\n        command = ' '.join(concat('env', self.__environment(), jobNode.command))\n        parasolCommand = ['-verbose',\n                          '-ram=%i' % jobNode.memory,\n                          '-cpu=%i' % jobNode.cores,\n                          '-results=' + results,\n                          'add', 'job', command]\n        # Deal with the cpus\n        self.usedCpus += jobNode.cores\n        while True:  # Process finished results with no wait\n            try:\n                jobID = self.cpuUsageQueue.get_nowait()\n            except Empty:\n                break\n            if jobID in list(self.jobIDsToCpu.keys()):\n                self.usedCpus -= self.jobIDsToCpu.pop(jobID)\n            assert self.usedCpus >= 0\n        while self.usedCpus > self.maxCores:  # If we are still waiting\n            jobID = self.cpuUsageQueue.get()\n            if jobID in list(self.jobIDsToCpu.keys()):\n                self.usedCpus -= self.jobIDsToCpu.pop(jobID)\n            assert self.usedCpus >= 0\n        # Now keep going\n        while True:\n            line = self._runParasol(parasolCommand)[1][0]\n            match = self.parasolOutputPattern.match(line)\n            if match is None:\n                # This is because parasol add job will return success, even if the job was not\n                # properly issued!\n                logger.debug('We failed to properly add the job, we will try again after a 5s.')\n                time.sleep(5)\n            else:\n                jobID = int(match.group(1))\n                self.jobIDsToCpu[jobID] = jobNode.cores\n                self.runningJobs.add(jobID)\n                logger.debug(\"Got the parasol job id: %s from line: %s\" % (jobID, line))\n                return jobID"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nkills the given jobs represented as Job ids then checks them are not in the list of issued jobs.", "response": "def killBatchJobs(self, jobIDs):\n        \"\"\"Kills the given jobs, represented as Job ids, then checks they are dead by checking\n        they are not in the list of issued jobs.\n        \"\"\"\n        while True:\n            for jobID in jobIDs:\n                if jobID in self.runningJobs:\n                    self.runningJobs.remove(jobID)\n                exitValue = self._runParasol(['remove', 'job', str(jobID)],\n                                             autoRetry=False)[0]\n                logger.debug(\"Tried to remove jobID: %i, with exit value: %i\" % (jobID, exitValue))\n            runningJobs = self.getIssuedBatchJobIDs()\n            if set(jobIDs).difference(set(runningJobs)) == set(jobIDs):\n                break\n            logger.warn( 'Tried to kill some jobs, but something happened and they are still '\n                         'going, will try againin 5s.')\n            time.sleep(5)\n        # Update the CPU usage, because killed jobs aren't written to the results file.\n        for jobID in jobIDs:\n            if jobID in list(self.jobIDsToCpu.keys()):\n                self.usedCpus -= self.jobIDsToCpu.pop(jobID)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting all queued and running jobs for a results file.", "response": "def getJobIDsForResultsFile(self, resultsFile):\n        \"\"\"\n        Get all queued and running jobs for a results file.\n        \"\"\"\n        jobIDs = []\n        for line in self._runParasol(['-extended', 'list', 'jobs'])[1]:\n            fields = line.strip().split()\n            if len(fields) == 0 or fields[-1] != resultsFile:\n                continue\n            jobID = fields[0]\n            jobIDs.append(int(jobID))\n        return set(jobIDs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the list of jobs issued to parasol in all results files but not including jobs that were created by other users.", "response": "def getIssuedBatchJobIDs(self):\n        \"\"\"\n        Gets the list of jobs issued to parasol in all results files, but not including jobs\n        created by other users.\n        \"\"\"\n        issuedJobs = set()\n        for resultsFile in itervalues(self.resultsFiles):\n            issuedJobs.update(self.getJobIDsForResultsFile(resultsFile))\n\n        return list(issuedJobs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getRunningBatchJobIDs(self):\n        # Example lines..\n        # r 5410186 benedictpaten worker 1247029663 localhost\n        # r 5410324 benedictpaten worker 1247030076 localhost\n        runningJobs = {}\n        issuedJobs = self.getIssuedBatchJobIDs()\n        for line in self._runParasol(['pstat2'])[1]:\n            if line != '':\n                match = self.runningPattern.match(line)\n                if match is not None:\n                    jobID = int(match.group(1))\n                    startTime = int(match.group(2))\n                    if jobID in issuedJobs:  # It's one of our jobs\n                        runningJobs[jobID] = time.time() - startTime\n        return runningJobs", "response": "Returns a map of running jobIDs and the time they have been running."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef updatedJobWorker(self):\n        resultsFiles = set()\n        resultsFileHandles = []\n        try:\n            while self.running:\n                # Look for any new results files that have been created, and open them\n                newResultsFiles = set(os.listdir(self.parasolResultsDir)).difference(resultsFiles)\n                for newFile in newResultsFiles:\n                    newFilePath = os.path.join(self.parasolResultsDir, newFile)\n                    resultsFileHandles.append(open(newFilePath, 'r'))\n                    resultsFiles.add(newFile)\n                for fileHandle in resultsFileHandles:\n                    while self.running:\n                        line = fileHandle.readline()\n                        if not line:\n                            break\n                        assert line[-1] == '\\n'\n                        (status, host, jobId, exe, usrTicks, sysTicks, submitTime, startTime,\n                         endTime, user, errFile, command) = line[:-1].split(None, 11)\n                        status = int(status)\n                        jobId = int(jobId)\n                        if os.WIFEXITED(status):\n                            status = os.WEXITSTATUS(status)\n                        else:\n                            status = -status\n                        self.cpuUsageQueue.put(jobId)\n                        startTime = int(startTime)\n                        endTime = int(endTime)\n                        if endTime == startTime:\n                            # Both, start and end time is an integer so to get sub-second\n                            # accuracy we use the ticks reported by Parasol as an approximation.\n                            # This isn't documented but what Parasol calls \"ticks\" is actually a\n                            # hundredth of a second. Parasol does the unit conversion early on\n                            # after a job finished. Search paraNode.c for ticksToHundreths. We\n                            # also cheat a little by always reporting at least one hundredth of a\n                            # second.\n                            usrTicks = int(usrTicks)\n                            sysTicks = int(sysTicks)\n                            wallTime = float( max( 1, usrTicks + sysTicks) ) * 0.01\n                        else:\n                            wallTime = float(endTime - startTime)\n                        self.updatedJobsQueue.put((jobId, status, wallTime))\n                time.sleep(1)\n        except:\n            logger.warn(\"Error occurred while parsing parasol results files.\")\n            raise\n        finally:\n            for fileHandle in resultsFileHandles:\n                fileHandle.close()", "response": "This function is called by the job worker thread to update the status of jobs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clusterFactory(provisioner, clusterName=None, zone=None, nodeStorage=50, sseKey=None):\n    if provisioner == 'aws':\n        try:\n            from toil.provisioners.aws.awsProvisioner import AWSProvisioner\n        except ImportError:\n            logger.error('The aws extra must be installed to use this provisioner')\n            raise\n        return AWSProvisioner(clusterName, zone, nodeStorage, sseKey)\n    elif provisioner == 'gce':\n        try:\n            from toil.provisioners.gceProvisioner import GCEProvisioner\n        except ImportError:\n            logger.error('The google extra must be installed to use this provisioner')\n            raise\n        return GCEProvisioner(clusterName, zone, nodeStorage, sseKey)\n    elif provisioner == 'azure':\n        try:\n            from toil.provisioners.azure.azureProvisioner import AzureProvisioner\n        except ImportError:\n            logger.error('The azure extra must be installed to use this provisioner')\n            raise\n        return AzureProvisioner(clusterName, zone, nodeStorage)\n    else:\n        raise RuntimeError(\"Invalid provisioner '%s'\" % provisioner)", "response": "Returns a new cluster object for the cloud type of the cluster."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef glob(glob_pattern, directoryname):\n    '''\n    Walks through a directory and its subdirectories looking for files matching\n    the glob_pattern and returns a list=[].\n\n    :param directoryname: Any accessible folder name on the filesystem.\n    :param glob_pattern: A string like \"*.txt\", which would find all text files.\n    :return: A list=[] of absolute filepaths matching the glob pattern.\n    '''\n    matches = []\n    for root, dirnames, filenames in os.walk(directoryname):\n        for filename in fnmatch.filter(filenames, glob_pattern):\n            absolute_filepath = os.path.join(root, filename)\n            matches.append(absolute_filepath)\n    return matches", "response": "Walks through a directory and returns a list of absolute filepaths matching the glob_pattern and returns a list of absolute filepaths matching the glob_pattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_docker_bashscript_file(temp_dir, docker_dir, globs, cmd, job_name):\n    '''\n    Creates a bashscript to inject into a docker container for the job.\n\n    This script wraps the job command(s) given in a bash script, hard links the\n    outputs and returns an \"rc\" file containing the exit code.  All of this is\n    done in an effort to parallel the Broad's cromwell engine, which is the\n    native WDL runner.  As they've chosen to write and then run a bashscript for\n    every command, so shall we.\n\n    :param temp_dir: The current directory outside of docker to deposit the\n                     bashscript into, which will be the bind mount that docker\n                     loads files from into its own containerized filesystem.\n                     This is usually the tempDir created by this individual job\n                     using 'tempDir = job.fileStore.getLocalTempDir()'.\n    :param docker_dir: The working directory inside of the docker container\n                       which is bind mounted to 'temp_dir'.  By default this is\n                       'data'.\n    :param globs: A list of expected output files to retrieve as glob patterns\n                  that will be returned as hard links to the current working\n                  directory.\n    :param cmd: A bash command to be written into the bash script and run.\n    :param job_name: The job's name, only used to write in a file name\n                     identifying the script as written for that job.\n                     Will be used to call the script later.\n    :return: Nothing, but it writes and deposits a bash script in temp_dir\n             intended to be run inside of a docker container for this job.\n    '''\n    wdl_copyright = heredoc_wdl('''        \\n\n        # Borrowed/rewritten from the Broad's Cromwell implementation.  As \n        # that is under a BSD-ish license, I include here the license off \n        # of their GitHub repo.  Thank you Broadies!\n\n        # Copyright (c) 2015, Broad Institute, Inc.\n        # All rights reserved.\n\n        # Redistribution and use in source and binary forms, with or without\n        # modification, are permitted provided that the following conditions are met:\n\n        # * Redistributions of source code must retain the above copyright notice, this\n        #   list of conditions and the following disclaimer.\n\n        # * Redistributions in binary form must reproduce the above copyright notice,\n        #   this list of conditions and the following disclaimer in the documentation\n        #   and/or other materials provided with the distribution.\n\n        # * Neither the name Broad Institute, Inc. nor the names of its\n        #   contributors may be used to endorse or promote products derived from\n        #   this software without specific prior written permission.\n\n        # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n        # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n        # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n        # DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n        # FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n        # DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n        # SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n        # CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n        # OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n        # OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE\n\n        # make a temp directory w/identifier\n        ''')\n    prefix_dict = {\"docker_dir\": docker_dir,\n                   \"cmd\": cmd}\n    bashfile_prefix = heredoc_wdl('''\n        tmpDir=$(mktemp -d /{docker_dir}/execution/tmp.XXXXXX)\n        chmod 777 $tmpDir\n        # set destination for java to deposit all of its files\n        export _JAVA_OPTIONS=-Djava.io.tmpdir=$tmpDir\n        export TMPDIR=$tmpDir\n\n        (\n        cd /{docker_dir}/execution\n        {cmd}\n        )\n\n        # gather the input command return code\n        echo $? > \"$tmpDir/rc.tmp\"\n\n        ''', prefix_dict)\n\n    bashfile_string = '#!/bin/bash' + wdl_copyright + bashfile_prefix\n\n    begin_globbing_string = heredoc_wdl('''\n        (\n        mkdir \"$tmpDir/globs\"\n        ''')\n\n    bashfile_string = bashfile_string + begin_globbing_string\n\n    for glob_input in globs:\n        add_this_glob = \\\n            '( ln -L ' + glob_input + \\\n            ' \"$tmpDir/globs\" 2> /dev/null ) || ( ln ' + glob_input + \\\n            ' \"$tmpDir/globs\" )\\n'\n        bashfile_string = bashfile_string + add_this_glob\n\n    bashfile_suffix = heredoc_wdl('''\n        )\n\n        # flush RAM to disk\n        sync\n\n        mv \"$tmpDir/rc.tmp\" \"$tmpDir/rc\"\n        chmod -R 777 $tmpDir\n        ''')\n\n    bashfile_string = bashfile_string + bashfile_suffix\n\n    with open(os.path.join(temp_dir, job_name + '_script.sh'), 'w') as bashfile:\n        bashfile.write(bashfile_string)", "response": "This function generates a bashscript file for a job in a container."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_infile(f, fileStore):\n    # check if this has already been processed\n    if isinstance(f, tuple):\n        return f\n    elif isinstance(f, list):\n        return process_array_infile(f, fileStore)\n    elif isinstance(f, basestring):\n        return process_single_infile(f, fileStore)\n    else:\n        raise RuntimeError('Error processing file: '.format(str(f)))", "response": "Process a single file or array of files and returns a tuple or an array of tuples replacing all previous path\n    strings with the original filename."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a string representing memory and returns an integer of bytes.", "response": "def parse_memory(memory):\n    \"\"\"\n    Parses a string representing memory and returns\n    an integer # of bytes.\n\n    :param memory:\n    :return:\n    \"\"\"\n    memory = str(memory)\n    if 'None' in memory:\n        return 2147483648 # toil's default\n    try:\n        import re\n        raw_mem_split = re.split('([a-zA-Z]+)', memory)\n        mem_split = []\n\n        for r in raw_mem_split:\n            if r:\n                mem_split.append(r.replace(' ', ''))\n\n        if len(mem_split) == 1:\n            return int(memory)\n\n        if len(mem_split) == 2:\n            num = mem_split[0]\n            unit = mem_split[1]\n            return int(float(num) * return_bytes(unit))\n        else:\n            raise RuntimeError('Memory parsing failed: {}'.format(memory))\n    except:\n        return 2147483648"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef size(f, unit='B', fileStore=None):\n    divisor = return_bytes(unit)\n    fileID = process_infile(f, fileStore)[0]\n    return fileID.size / divisor", "response": "Returns the size of a file in bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_tsv(f, delimiter=\"\\t\"):\n    '''\n    Take a tsv filepath and return an array; e.g. [[],[],[]].\n\n    For example, a file containing:\n\n    1   2   3\n    4   5   6\n    7   8   9\n\n    would return the array: [['1','2','3'], ['4','5','6'], ['7','8','9']]\n\n    :param tsv_filepath:\n    :return: tsv_array\n    '''\n    tsv_array = []\n    with open(f, \"r\") as f:\n        data_file = csv.reader(f, delimiter=delimiter)\n        for line in data_file:\n            tsv_array.append(line)\n    return (tsv_array)", "response": "Read a tsv file and return an array of items."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _readClusterSettings(self):\n        metadata_server = \"http://metadata/computeMetadata/v1/instance/\"\n        metadata_flavor = {'Metadata-Flavor': 'Google'}\n        zone = requests.get(metadata_server + 'zone', headers = metadata_flavor).text\n        self._zone = zone.split('/')[-1]\n\n        project_metadata_server = \"http://metadata/computeMetadata/v1/project/\"\n        self._projectId = requests.get(project_metadata_server + 'project-id', headers = metadata_flavor).text\n\n        # From a GCE instance, these values can be blank. Only the projectId is needed\n        self._googleJson = ''\n        self._clientEmail = ''\n\n        self._tags = requests.get(metadata_server + 'description', headers = metadata_flavor).text\n        tags = json.loads(self._tags)\n        self.clusterName = tags['clusterName']\n        self._gceDriver = self._getDriver()\n        self._instanceGroup = self._gceDriver.ex_get_instancegroup(self.clusterName, zone=self._zone)\n\n        leader = self.getLeader()\n        self._leaderPrivateIP = leader.privateIP\n\n        # generate a public key for the leader, which is used to talk to workers\n        self._masterPublicKey = self._setSSH()\n\n        # The location of the Google credentials file on instances.\n        self._credentialsPath = GoogleJobStore.nodeServiceAccountJson\n        self._keyName = 'core' # key name leader users to communicate with works\n        self._botoPath = self.NODE_BOTO_PATH", "response": "Read the cluster settings from the instance."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _readCredentials(self):\n        self._googleJson = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')\n        if not self._googleJson:\n            raise RuntimeError('GOOGLE_APPLICATION_CREDENTIALS not set.')\n        try:\n            with open(self._googleJson) as jsonFile:\n                self.googleConnectionParams = json.loads(jsonFile.read())\n        except:\n             raise RuntimeError('GCEProvisioner: Could not parse the Google service account json file %s'\n                                % self._googleJson)\n\n        self._projectId = self.googleConnectionParams['project_id']\n        self._clientEmail = self.googleConnectionParams['client_email']\n        self._credentialsPath = self._googleJson\n        self._masterPublicKey = None\n        self._gceDriver = self._getDriver()", "response": "Read the credentials from the file specified by GOOGLE_APPLICATION_CREDENTIALS."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef launchCluster(self, leaderNodeType, leaderStorage, owner, **kwargs):\n        if 'keyName' not in kwargs:\n            raise RuntimeError(\"A keyPairName is required for the GCE provisioner.\")\n        self._keyName = kwargs['keyName']\n        if 'botoPath' in kwargs:\n            self._botoPath = kwargs['botoPath']\n        self._vpcSubnet = kwargs['vpcSubnet'] if 'vpcSubnet' in kwargs else None\n\n        # Throws an error if cluster exists\n        self._instanceGroup = self._gceDriver.ex_create_instancegroup(self.clusterName, self._zone)\n        logger.debug('Launching leader')\n\n        # GCE doesn't have a dictionary tags field. The tags field is just a string list.\n        # Therefore, dumping tags into the description.\n        tags = {'Owner': self._keyName, 'clusterName': self.clusterName}\n        if 'userTags' in kwargs:\n            tags.update(kwargs['userTags'])\n        self._tags = json.dumps(tags)\n\n        userData =  self._getCloudConfigUserData('leader')\n        metadata = {'items': [{'key': 'user-data', 'value': userData}]}\n        imageType = 'coreos-stable'\n        sa_scopes = [{'scopes': ['compute', 'storage-full']}]\n        disk = {}\n        disk['initializeParams'] = {\n            'sourceImage': self.SOURCE_IMAGE,\n            'diskSizeGb' : leaderStorage }\n        disk.update({'boot': True,\n             'autoDelete': True })\n        name= 'l' + str(uuid.uuid4())\n        leader = self._gceDriver.create_node(name, leaderNodeType, imageType,\n                                            location=self._zone,\n                                            ex_service_accounts=sa_scopes,\n                                            ex_metadata=metadata,\n                                            ex_subnetwork=self._vpcSubnet,\n                                            ex_disks_gce_struct = [disk],\n                                            description=self._tags,\n                                            ex_preemptible=False)\n\n        self._instanceGroup.add_instances([leader])\n        self._leaderPrivateIP = leader.private_ips[0] # needed if adding workers\n        #self.subnetID = leader.subnet_id #TODO: get subnetID\n\n        # Wait for the appliance to start and inject credentials.\n        leaderNode = Node(publicIP=leader.public_ips[0], privateIP=leader.private_ips[0],\n                          name=leader.name, launchTime=leader.created_at, nodeType=leader.size,\n                          preemptable=False, tags=self._tags)\n        leaderNode.waitForNode('toil_leader', keyName=self._keyName)\n        leaderNode.copySshKeys(self._keyName)\n        leaderNode.injectFile(self._credentialsPath, GoogleJobStore.nodeServiceAccountJson, 'toil_leader')\n        if self._botoPath:\n            leaderNode.injectFile(self._botoPath, self.NODE_BOTO_PATH, 'toil_leader')\n        logger.debug('Launched leader')", "response": "Launches a new cluster."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef destroyCluster(self):\n        logger.debug(\"Destroying cluster %s\" % self.clusterName)\n        instancesToTerminate = self._getNodesInCluster()\n        attempts = 0\n        while instancesToTerminate and attempts < 3:\n            self._terminateInstances(instances=instancesToTerminate)\n            instancesToTerminate = self._getNodesInCluster()\n            attempts += 1\n\n        # remove group\n        instanceGroup = self._gceDriver.ex_get_instancegroup(self.clusterName, zone=self._zone)\n        instanceGroup.destroy()", "response": "Terminate all instances in the cluster and remove the group."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninject the worker files into the node.", "response": "def _injectWorkerFiles(self, node, botoExists):\n        \"\"\"\n        Set up the credentials on the worker.\n        \"\"\"\n        node.waitForNode('toil_worker', keyName=self._keyName)\n        node.copySshKeys(self._keyName)\n        node.injectFile(self._credentialsPath, GoogleJobStore.nodeServiceAccountJson, 'toil_worker')\n        if self._sseKey:\n            node.injectFile(self._sseKey, self._sseKey, 'toil_worker')\n        if botoExists:\n            node.injectFile(self._botoPath, self.NODE_BOTO_PATH, 'toil_worker')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a Cloudant driver instance.", "response": "def _getDriver(self):\n        \"\"\"  Connect to GCE \"\"\"\n        driverCls = get_driver(Provider.GCE)\n        return driverCls(self._clientEmail,\n                         self._googleJson,\n                         project=self._projectId,\n                         datacenter=self._zone)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ex_create_multiple_nodes(\n            self, base_name, size, image, number, location=None,\n            ex_network='default', ex_subnetwork=None, ex_tags=None,\n            ex_metadata=None, ignore_errors=True, use_existing_disk=True,\n            poll_interval=2, external_ip='ephemeral',\n            ex_disk_type='pd-standard', ex_disk_auto_delete=True,\n            ex_service_accounts=None, timeout=DEFAULT_TASK_COMPLETION_TIMEOUT,\n            description=None, ex_can_ip_forward=None, ex_disks_gce_struct=None,\n            ex_nic_gce_struct=None, ex_on_host_maintenance=None,\n            ex_automatic_restart=None, ex_image_family=None,\n            ex_preemptible=None):\n        \"\"\"\n         Monkey patch to gce.py in libcloud to allow disk and images to be specified.\n         Also changed name to a uuid below.\n         The prefix 'wp' identifies preemptable nodes and 'wn' non-preemptable nodes.\n        \"\"\"\n        # if image and ex_disks_gce_struct:\n        #    raise ValueError(\"Cannot specify both 'image' and \"\n        #                     \"'ex_disks_gce_struct'.\")\n\n        driver = self._getDriver()\n        if image and ex_image_family:\n            raise ValueError(\"Cannot specify both 'image' and \"\n                             \"'ex_image_family'\")\n\n        location = location or driver.zone\n        if not hasattr(location, 'name'):\n            location = driver.ex_get_zone(location)\n        if not hasattr(size, 'name'):\n            size = driver.ex_get_size(size, location)\n        if not hasattr(ex_network, 'name'):\n            ex_network = driver.ex_get_network(ex_network)\n        if ex_subnetwork and not hasattr(ex_subnetwork, 'name'):\n            ex_subnetwork = \\\n                driver.ex_get_subnetwork(ex_subnetwork,\n                                       region=driver._get_region_from_zone(\n                                           location))\n        if ex_image_family:\n            image = driver.ex_get_image_from_family(ex_image_family)\n        if image and not hasattr(image, 'name'):\n            image = driver.ex_get_image(image)\n        if not hasattr(ex_disk_type, 'name'):\n            ex_disk_type = driver.ex_get_disktype(ex_disk_type, zone=location)\n\n        node_attrs = {'size': size,\n                      'image': image,\n                      'location': location,\n                      'network': ex_network,\n                      'subnetwork': ex_subnetwork,\n                      'tags': ex_tags,\n                      'metadata': ex_metadata,\n                      'ignore_errors': ignore_errors,\n                      'use_existing_disk': use_existing_disk,\n                      'external_ip': external_ip,\n                      'ex_disk_type': ex_disk_type,\n                      'ex_disk_auto_delete': ex_disk_auto_delete,\n                      'ex_service_accounts': ex_service_accounts,\n                      'description': description,\n                      'ex_can_ip_forward': ex_can_ip_forward,\n                      'ex_disks_gce_struct': ex_disks_gce_struct,\n                      'ex_nic_gce_struct': ex_nic_gce_struct,\n                      'ex_on_host_maintenance': ex_on_host_maintenance,\n                      'ex_automatic_restart': ex_automatic_restart,\n                      'ex_preemptible': ex_preemptible}\n        # List for holding the status information for disk/node creation.\n        status_list = []\n\n        for i in range(number):\n            name = 'wp' if ex_preemptible else 'wn'\n            name += str(uuid.uuid4()) #'%s-%03d' % (base_name, i)\n            status = {'name': name, 'node_response': None, 'node': None}\n            status_list.append(status)\n\n        start_time = time.time()\n        complete = False\n        while not complete:\n            if (time.time() - start_time >= timeout):\n                raise Exception(\"Timeout (%s sec) while waiting for multiple \"\n                                \"instances\")\n            complete = True\n            time.sleep(poll_interval)\n            for status in status_list:\n                # Create the node or check status if already in progress.\n                if not status['node']:\n                    if not status['node_response']:\n                        driver._multi_create_node(status, node_attrs)\n                    else:\n                        driver._multi_check_node(status, node_attrs)\n                # If any of the nodes have not been created (or failed) we are\n                # not done yet.\n                if not status['node']:\n                    complete = False\n\n        # Return list of nodes\n        node_list = []\n        for status in status_list:\n            node_list.append(status['node'])\n        return node_list", "response": "Monkey patch to gce.py in libcloud to allow disk and images to be specified.\n         Also changed name to a uuid below.\n         The prefix 'wp' identifies preemptable nodes and 'wn' non-preemptable nodes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a parent directory n places above the input filepath.", "response": "def fetch_parent_dir(filepath, n=1):\n    '''Returns a parent directory, n places above the input filepath.\n\n    Equivalent to something like: '/home/user/dir'.split('/')[-2] if n=2.\n    '''\n    filepath = os.path.realpath(filepath)\n    for i in range(n):\n        filepath = os.path.dirname(filepath)\n    return os.path.basename(filepath)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef runSetup():\n    boto = 'boto==2.48.0'\n    boto3 = 'boto3>=1.7.50, <2.0'\n    futures = 'futures==3.1.1'\n    pycryptodome = 'pycryptodome==3.5.1'\n    pymesos = 'pymesos==0.3.7'\n    psutil = 'psutil==3.0.1'\n    azureCosmosdbTable = 'azure-cosmosdb-table==0.37.1'\n    azureAnsible = 'ansible[azure]==2.5.0a1'\n    azureStorage = 'azure-storage==0.35.1'\n    secretstorage = 'secretstorage<3'\n    pynacl = 'pynacl==1.1.2'\n    gcs = 'google-cloud-storage==1.6.0'\n    gcs_oauth2_boto_plugin = 'gcs_oauth2_boto_plugin==1.14'\n    apacheLibcloud = 'apache-libcloud==2.2.1'\n    cwltool = 'cwltool==1.0.20190228155703'\n    schemaSalad = 'schema-salad<5,>=4.1'\n    galaxyLib = 'galaxy-lib==18.9.2'\n    htcondor = 'htcondor>=8.6.0'\n    dill = 'dill==0.2.7.1'\n    six = 'six>=1.10.0'\n    future = 'future'\n    requests = 'requests>=2, <3'\n    docker = 'docker==2.5.1'\n    subprocess32 = 'subprocess32<=3.5.2'\n    dateutil = 'python-dateutil'\n    addict = 'addict<=2.2.0'\n    sphinx = 'sphinx==1.7.5'\n    pathlib2 = 'pathlib2==2.3.2'\n\n    core_reqs = [\n        dill,\n        six,\n        future,\n        requests,\n        docker,\n        dateutil,\n        psutil,\n        subprocess32,\n        addict,\n        sphinx,\n        pathlib2]\n\n    mesos_reqs = [\n        pymesos,\n        psutil]\n    aws_reqs = [\n        boto,\n        boto3,\n        futures,\n        pycryptodome]\n    azure_reqs = [\n        azureCosmosdbTable,\n        secretstorage,\n        azureAnsible,\n        azureStorage]\n    encryption_reqs = [\n        pynacl]\n    google_reqs = [\n        gcs_oauth2_boto_plugin,  # is this being used??\n        apacheLibcloud,\n        gcs]\n    cwl_reqs = [\n        cwltool,\n        schemaSalad,\n        galaxyLib]\n    wdl_reqs = []\n    htcondor_reqs = [\n        htcondor]\n\n    # htcondor is not supported by apple\n    # this is tricky to conditionally support in 'all' due\n    # to how wheels work, so it is not included in all and\n    # must be explicitly installed as an extra\n    all_reqs = \\\n        mesos_reqs + \\\n        aws_reqs + \\\n        azure_reqs + \\\n        encryption_reqs + \\\n        google_reqs + \\\n        cwl_reqs\n\n    # remove the subprocess32 backport if not python2\n    if not sys.version_info[0] == 2:\n        core_reqs.remove(subprocess32)\n\n    setup(\n        name='toil',\n        version=version.distVersion,\n        description='Pipeline management software for clusters.',\n        author='Benedict Paten',\n        author_email='benedict@soe.usc.edu',\n        url=\"https://github.com/BD2KGenomics/toil\",\n        classifiers=[\n          'Development Status :: 5 - Production/Stable',\n          'Environment :: Console',\n          'Intended Audience :: Developers',\n          'Intended Audience :: Science/Research',\n          'Intended Audience :: Healthcare Industry',\n          'License :: OSI Approved :: Apache Software License',\n          'Natural Language :: English',\n          'Operating System :: MacOS :: MacOS X',\n          'Operating System :: POSIX',\n          'Operating System :: POSIX :: Linux',\n          'Programming Language :: Python :: 2.7',\n          'Topic :: Scientific/Engineering',\n          'Topic :: Scientific/Engineering :: Bio-Informatics',\n          'Topic :: Scientific/Engineering :: Astronomy',\n          'Topic :: Scientific/Engineering :: Atmospheric Science',\n          'Topic :: Scientific/Engineering :: Information Analysis',\n          'Topic :: Scientific/Engineering :: Medical Science Apps.',\n          'Topic :: System :: Distributed Computing',\n          'Topic :: Utilities'],\n        license=\"Apache License v2.0\",\n        install_requires=core_reqs,\n        extras_require={\n            'mesos': mesos_reqs,\n            'aws': aws_reqs,\n            'azure': azure_reqs,\n            'encryption': encryption_reqs,\n            'google': google_reqs,\n            'cwl': cwl_reqs,\n            'wdl': wdl_reqs,\n            'htcondor:sys_platform!=\"darwin\"': htcondor_reqs,\n            'all': all_reqs},\n        package_dir={'': 'src'},\n        packages=find_packages(where='src',\n                               # Note that we intentionally include the top-level `test` package for\n                               # functionality like the @experimental and @integrative decoratorss:\n                               exclude=['*.test.*']),\n        package_data = {\n            '': ['*.yml', 'contrib/azure_rm.py', 'cloud-config'],\n        },\n        # Unfortunately, the names of the entry points are hard-coded elsewhere in the code base so\n        # you can't just change them here. Luckily, most of them are pretty unique strings, and thus\n        # easy to search for.\n        entry_points={\n            'console_scripts': [\n                'toil = toil.utils.toilMain:main',\n                '_toil_worker = toil.worker:main',\n                'cwltoil = toil.cwl.cwltoil:main [cwl]',\n                'toil-cwl-runner = toil.cwl.cwltoil:main [cwl]',\n                'toil-wdl-runner = toil.wdl.toilwdl:main',\n                '_toil_mesos_executor = toil.batchSystems.mesos.executor:main [mesos]']})", "response": "This function is used to run the setup function."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload and return the module object for src / toil. version. py generating it from the template if required.", "response": "def importVersion():\n    \"\"\"\n    Load and return the module object for src/toil/version.py, generating it from the template if\n    required.\n    \"\"\"\n    import imp\n    try:\n        # Attempt to load the template first. It only exists in a working copy cloned via git.\n        import version_template\n    except ImportError:\n        # If loading the template fails we must be in a unpacked source distribution and\n        # src/toil/version.py will already exist.\n        pass\n    else:\n        # Use the template to generate src/toil/version.py\n        import os\n        import errno\n        from tempfile import NamedTemporaryFile\n\n        new = version_template.expand_()\n        try:\n            with open('src/toil/version.py') as f:\n                old = f.read()\n        except IOError as e:\n            if e.errno == errno.ENOENT:\n                old = None\n            else:\n                raise\n\n        if old != new:\n            with NamedTemporaryFile(mode='w',dir='src/toil', prefix='version.py.', delete=False) as f:\n                f.write(new)\n            os.rename(f.name, 'src/toil/version.py')\n    # Unfortunately, we can't use a straight import here because that would also load the stuff\n    # defined in src/toil/__init__.py which imports modules from external dependencies that may\n    # yet to be installed when setup.py is invoked.\n    return imp.load_source('toil.version', 'src/toil/version.py')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreporting the state of a Toil workflow.", "response": "def main():\n    \"\"\"Reports the state of a Toil workflow.\"\"\"\n    parser = getBasicOptionParser()\n\n    parser.add_argument(\"jobStore\", type=str,\n                        help=\"The location of a job store that holds the information about the \"\n                             \"workflow whose status is to be reported on.\" + jobStoreLocatorHelp)\n\n    parser.add_argument(\"--failIfNotComplete\", action=\"store_true\",\n                        help=\"Return exit value of 1 if toil jobs not all completed. default=%(default)s\",\n                        default=False)\n\n    parser.add_argument(\"--noAggStats\", dest=\"stats\", action=\"store_false\",\n                        help=\"Do not print overall, aggregate status of workflow.\",\n                        default=True)\n\n    parser.add_argument(\"--printDot\", action=\"store_true\",\n                        help=\"Print dot formatted description of the graph. If using --jobs will \"\n                             \"restrict to subgraph including only those jobs. default=%(default)s\",\n                        default=False)\n\n    parser.add_argument(\"--jobs\", nargs='+',\n                        help=\"Restrict reporting to the following jobs (allows subsetting of the report).\",\n                        default=None)\n\n    parser.add_argument(\"--printPerJobStats\", action=\"store_true\",\n                        help=\"Print info about each job. default=%(default)s\",\n                        default=False)\n\n    parser.add_argument(\"--printLogs\", action=\"store_true\",\n                        help=\"Print the log files of jobs (if they exist). default=%(default)s\",\n                        default=False)\n\n    parser.add_argument(\"--printChildren\", action=\"store_true\",\n                        help=\"Print children of each job. default=%(default)s\",\n                        default=False)\n\n    parser.add_argument(\"--version\", action='version', version=version)\n\n    options = parseBasicOptions(parser)\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(0)\n\n    config = Config()\n    config.setOptions(options)\n\n    try:\n        status = ToilStatus(config.jobStore, options.jobs)\n    except NoSuchJobStoreException:\n        print('No job store found.')\n        return\n    except JobException:  # Workflow likely complete, user informed in ToilStatus()\n        return\n\n    jobStats = status.report_on_jobs()\n\n    # Info to be reported.\n    hasChildren = jobStats['hasChildren']\n    readyToRun = jobStats['readyToRun']\n    zombies = jobStats['zombies']\n    hasServices = jobStats['hasServices']\n    services = jobStats['services']\n    hasLogFile = jobStats['hasLogFile']\n    properties = jobStats['properties']\n    childNumber = jobStats['childNumber']\n\n    if options.printPerJobStats:\n        status.printAggregateJobStats(properties, childNumber)\n    if options.printLogs:\n        status.printJobLog()\n    if options.printChildren:\n        status.printJobChildren()\n    if options.printDot:\n        status.print_dot_chart()\n    if options.stats:\n        print('Of the %i jobs considered, '\n           'there are %i jobs with children, '\n           '%i jobs ready to run, '\n           '%i zombie jobs, '\n           '%i jobs with services, '\n           '%i services, '\n           'and %i jobs with log files currently in %s.' %\n            (len(status.jobsToReport), len(hasChildren), len(readyToRun), len(zombies),\n             len(hasServices), len(services), len(hasLogFile), status.jobStore))\n\n    if len(status.jobsToReport) > 0 and options.failIfNotComplete:\n        # Upon workflow completion, all jobs will have been removed from job store\n        exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprinting a dot output graph representing the workflow.", "response": "def print_dot_chart(self):\n        \"\"\"Print a dot output graph representing the workflow.\"\"\"\n        print(\"digraph toil_graph {\")\n        print(\"# This graph was created from job-store: %s\" % self.jobStoreName)\n\n        # Make job IDs to node names map\n        jobsToNodeNames = dict(enumerate(map(lambda job: job.jobStoreID, self.jobsToReport)))\n\n        # Print the nodes\n        for job in set(self.jobsToReport):\n            print('%s [label=\"%s %s\"];' % (\n                jobsToNodeNames[job.jobStoreID], job.jobName, job.jobStoreID))\n\n        # Print the edges\n        for job in set(self.jobsToReport):\n            for level, jobList  in enumerate(job.stack):\n                for childJob in jobList:\n                    # Check, b/c successor may be finished / not in the set of jobs\n                    if childJob.jobStoreID in jobsToNodeNames:\n                        print('%s -> %s [label=\"%i\"];' % (\n                            jobsToNodeNames[job.jobStoreID],\n                            jobsToNodeNames[childJob.jobStoreID], level))\n        print(\"}\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef printJobLog(self):\n        for job in self.jobsToReport:\n            if job.logJobStoreFileID is not None:\n                msg = \"LOG_FILE_OF_JOB:%s LOG: =======>\\n\" % job\n                with job.getLogFileHandle(self.jobStore) as fH:\n                    msg += fH.read()\n                msg += \"<=========\"\n            else:\n                msg = \"LOG_FILE_OF_JOB:%s LOG: Job has no log file\" % job\n            print(msg)", "response": "Takes a list of jobs finds their log files and prints them to the terminal."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef printJobChildren(self):\n        for job in self.jobsToReport:\n            children = \"CHILDREN_OF_JOB:%s \" % job\n            for level, jobList in enumerate(job.stack):\n                for childJob in jobList:\n                    children += \"\\t(CHILD_JOB:%s,PRECEDENCE:%i)\" % (childJob, level)\n            print(children)", "response": "Takes a list of jobs and prints their successors."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints a job s ID log file remaining tries and other properties.", "response": "def printAggregateJobStats(self, properties, childNumber):\n        \"\"\"Prints a job's ID, log file, remaining tries, and other properties.\"\"\"\n        for job in self.jobsToReport:\n            lf = lambda x: \"%s:%s\" % (x, str(x in properties))\n            print(\"\\t\".join((\"JOB:%s\" % job,\n                             \"LOG_FILE:%s\" % job.logJobStoreFileID,\n                             \"TRYS_REMAINING:%i\" % job.remainingRetryCount,\n                             \"CHILD_NUMBER:%s\" % childNumber,\n                             lf(\"READY_TO_RUN\"), lf(\"IS_ZOMBIE\"),\n                             lf(\"HAS_SERVICES\"), lf(\"IS_SERVICE\"))))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef report_on_jobs(self):\n        hasChildren = []\n        readyToRun = []\n        zombies = []\n        hasLogFile = []\n        hasServices = []\n        services = []\n        properties = set()\n\n        for job in self.jobsToReport:\n            if job.logJobStoreFileID is not None:\n                hasLogFile.append(job)\n\n            childNumber = reduce(lambda x, y: x + y, map(len, job.stack) + [0])\n            if childNumber > 0:  # Total number of successors > 0\n                hasChildren.append(job)\n                properties.add(\"HAS_CHILDREN\")\n            elif job.command is not None:\n                # Job has no children and a command to run. Indicates job could be run.\n                readyToRun.append(job)\n                properties.add(\"READY_TO_RUN\")\n            else:\n                # Job has no successors and no command, so is a zombie job.\n                zombies.append(job)\n                properties.add(\"IS_ZOMBIE\")\n            if job.services:\n                hasServices.append(job)\n                properties.add(\"HAS_SERVICES\")\n            if job.startJobStoreID or job.terminateJobStoreID or job.errorJobStoreID:\n                # These attributes are only set in service jobs\n                services.append(job)\n                properties.add(\"IS_SERVICE\")\n\n        jobStats = {'hasChildren': hasChildren,\n                    'readyToRun': readyToRun,\n                    'zombies': zombies,\n                    'hasServices': hasServices,\n                    'services': services,\n                    'hasLogFile': hasLogFile,\n                    'properties': properties,\n                    'childNumber': childNumber}\n        return jobStats", "response": "Returns a dictionary of information about the jobs in the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the status of a process with a particular PID.", "response": "def getPIDStatus(jobStoreName):\n        \"\"\"\n        Determine the status of a process with a particular pid.\n\n        Checks to see if a process exists or not.\n\n        :return: A string indicating the status of the PID of the workflow as stored in the jobstore.\n        :rtype: str\n        \"\"\"\n        try:\n            jobstore = Toil.resumeJobStore(jobStoreName)\n        except NoSuchJobStoreException:\n            return 'QUEUED'\n        except NoSuchFileException:\n            return 'QUEUED'\n\n        try:\n            with jobstore.readSharedFileStream('pid.log') as pidFile:\n                pid = int(pidFile.read())\n                try:\n                    os.kill(pid, 0)  # Does not kill process when 0 is passed.\n                except OSError:  # Process not found, must be done.\n                    return 'COMPLETED'\n                else:\n                    return 'RUNNING'\n        except NoSuchFileException:\n            pass\n        return 'QUEUED'"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getStatus(jobStoreName):\n        try:\n            jobstore = Toil.resumeJobStore(jobStoreName)\n        except NoSuchJobStoreException:\n            return 'QUEUED'\n        except NoSuchFileException:\n            return 'QUEUED'\n\n        try:\n            with jobstore.readSharedFileStream('succeeded.log') as successful:\n                pass\n            return 'COMPLETED'\n        except NoSuchFileException:\n            try:\n                with jobstore.readSharedFileStream('failed.log') as failed:\n                    pass\n                return 'ERROR'\n            except NoSuchFileException:\n                pass\n        return 'RUNNING'", "response": "Returns the status of a workflow."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fetchRootJob(self):\n        try:\n            return self.jobStore.loadRootJob()\n        except JobException:\n            print('Root job is absent. The workflow may have completed successfully.', file=sys.stderr)\n            raise", "response": "Fetches the root job from the jobStore that provides context for all other jobs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fetchUserJobs(self, jobs):\n        jobsToReport = []\n        for jobID in jobs:\n            try:\n                jobsToReport.append(self.jobStore.load(jobID))\n            except JobException:\n                print('The job %s could not be found.' % jobID, file=sys.stderr)\n                raise\n        return jobsToReport", "response": "Takes a user input array of jobs verifies that they are in the jobStore and returns the array of jobsToReport."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntraverse the job graph and return the list of jobs to report.", "response": "def traverseJobGraph(self, rootJob, jobsToReport=None, foundJobStoreIDs=None):\n        \"\"\"\n        Find all current jobs in the jobStore and return them as an Array.\n\n        :param jobNode rootJob: The root job of the workflow.\n        :param list jobsToReport: A list of jobNodes to be added to and returned.\n        :param set foundJobStoreIDs: A set of jobStoreIDs used to keep track of jobStoreIDs encountered in traversal.\n        :returns jobsToReport: The list of jobs currently in the job graph.\n        \"\"\"\n        if jobsToReport is None:\n            jobsToReport = []\n\n        if foundJobStoreIDs is None:\n            foundJobStoreIDs = set()\n\n        if rootJob.jobStoreID in foundJobStoreIDs:\n            return jobsToReport\n\n        foundJobStoreIDs.add(rootJob.jobStoreID)\n        jobsToReport.append(rootJob)\n        # Traverse jobs in stack\n        for jobs in rootJob.stack:\n            for successorJobStoreID in [x.jobStoreID for x in jobs]:\n                if successorJobStoreID not in foundJobStoreIDs and self.jobStore.exists(successorJobStoreID):\n                    self.traverseJobGraph(self.jobStore.load(successorJobStoreID), jobsToReport, foundJobStoreIDs)\n\n        # Traverse service jobs\n        for jobs in rootJob.services:\n            for serviceJobStoreID in [x.jobStoreID for x in jobs]:\n                if self.jobStore.exists(serviceJobStoreID):\n                    if serviceJobStoreID in foundJobStoreIDs:\n                        raise RuntimeError('Service job was unexpectedly found while traversing ')\n                    foundJobStoreIDs.add(serviceJobStoreID)\n                    jobsToReport.append(self.jobStore.load(serviceJobStoreID))\n\n        return jobsToReport"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef toilPackageDirPath():\n    result = os.path.dirname(os.path.realpath(__file__))\n    assert result.endswith('/toil')\n    return result", "response": "Returns the absolute path of the directory that corresponds to the top - level toil package."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nresolve the given entry point to a path to the Toil entry point.", "response": "def resolveEntryPoint(entryPoint):\n    \"\"\"\n    Returns the path to the given entry point (see setup.py) that *should* work on a worker. The\n    return value may be an absolute or a relative path.\n    \"\"\"\n    if inVirtualEnv():\n        path = os.path.join(os.path.dirname(sys.executable), entryPoint)\n        # Inside a virtualenv we try to use absolute paths to the entrypoints.\n        if os.path.isfile(path):\n            # If the entrypoint is present, Toil must have been installed into the virtualenv (as\n            # opposed to being included via --system-site-packages). For clusters this means that\n            # if Toil is installed in a virtualenv on the leader, it must be installed in\n            # a virtualenv located at the same path on each worker as well.\n            assert os.access(path, os.X_OK)\n            return path\n        else:\n            # For virtualenv's that have the toil package directory on their sys.path but whose\n            # bin directory lacks the Toil entrypoints, i.e. where Toil is included via\n            # --system-site-packages, we rely on PATH just as if we weren't in a virtualenv.\n            return entryPoint\n    else:\n        # Outside a virtualenv it is hard to predict where the entry points got installed. It is\n        # the responsibility of the user to ensure that they are present on PATH and point to the\n        # correct version of Toil. This is still better than an absolute path because it gives\n        # the user control over Toil's location on both leader and workers.\n        return entryPoint"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the number of physical pages of the system memory.", "response": "def physicalMemory():\n    \"\"\"\n    >>> n = physicalMemory()\n    >>> n > 0\n    True\n    >>> n == physicalMemory()\n    True\n    \"\"\"\n    try:\n        return os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')\n    except ValueError:\n        return int(subprocess.check_output(['sysctl', '-n', 'hw.memsize']).decode('utf-8').strip())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef applianceSelf(forceDockerAppliance=False):\n    import toil.version\n    registry = lookupEnvVar(name='docker registry',\n                            envName='TOIL_DOCKER_REGISTRY',\n                            defaultValue=toil.version.dockerRegistry)\n    name = lookupEnvVar(name='docker name',\n                        envName='TOIL_DOCKER_NAME',\n                        defaultValue=toil.version.dockerName)\n    appliance = lookupEnvVar(name='docker appliance',\n                             envName='TOIL_APPLIANCE_SELF',\n                             defaultValue=registry + '/' + name + ':' + toil.version.dockerTag)\n\n    checkDockerSchema(appliance)\n\n    if forceDockerAppliance:\n        return appliance\n    else:\n        return checkDockerImageExists(appliance=appliance)", "response": "Returns the fully qualified name of the Docker image to start Toil appliance containers from."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lookupEnvVar(name, envName, defaultValue):\n    try:\n        value = os.environ[envName]\n    except KeyError:\n        log.info('Using default %s of %s as %s is not set.', name, defaultValue, envName)\n        return defaultValue\n    else:\n        log.info('Overriding %s of %s with %s from %s.', name, defaultValue, value, envName)\n        return value", "response": "Lookup the value of an environment variable in the toil environment variable."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef checkDockerImageExists(appliance):\n    if currentCommit in appliance:\n        return appliance\n    registryName, imageName, tag = parseDockerAppliance(appliance)\n\n    if registryName == 'docker.io':\n        return requestCheckDockerIo(origAppliance=appliance, imageName=imageName, tag=tag)\n    else:\n        return requestCheckRegularDocker(origAppliance=appliance, registryName=registryName, imageName=imageName, tag=tag)", "response": "Checks a url registryName for the existence of a docker image with a given tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parseDockerAppliance(appliance):\n    appliance = appliance.lower()\n\n    # get the tag\n    if ':' in appliance:\n        tag = appliance.split(':')[-1]\n        appliance = appliance[:-(len(':' + tag))] # remove only the tag\n    else:\n        # default to 'latest' if no tag is specified\n        tag = 'latest'\n\n    # get the registry and image\n    registryName = 'docker.io' # default if not specified\n    imageName = appliance # will be true if not specified\n    if '/' in appliance and '.' in appliance.split('/')[0]:\n        registryName = appliance.split('/')[0]\n        imageName = appliance[len(registryName):]\n    registryName = registryName.strip('/')\n    imageName = imageName.strip('/')\n\n    return registryName, imageName, tag", "response": "Takes a string describing a docker image and returns the parsed registry image reference and tag for that image."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef requestCheckRegularDocker(origAppliance, registryName, imageName, tag):\n    ioURL = 'https://{webhost}/v2/{pathName}/manifests/{tag}' \\\n              ''.format(webhost=registryName, pathName=imageName, tag=tag)\n    response = requests.head(ioURL)\n    if not response.ok:\n        raise ApplianceImageNotFound(origAppliance, ioURL, response.status_code)\n    else:\n        return origAppliance", "response": "Checks to see if an image exists using the requests library."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef requestCheckDockerIo(origAppliance, imageName, tag):\n    # only official images like 'busybox' or 'ubuntu'\n    if '/' not in imageName:\n        imageName = 'library/' + imageName\n\n    token_url = 'https://auth.docker.io/token?service=registry.docker.io&scope=repository:{repo}:pull'.format(repo=imageName)\n    requests_url = 'https://registry-1.docker.io/v2/{repo}/manifests/{tag}'.format(repo=imageName, tag=tag)\n\n    token = requests.get(token_url)\n    jsonToken = token.json()\n    bearer = jsonToken[\"token\"]\n    response = requests.head(requests_url, headers={'Authorization': 'Bearer {}'.format(bearer)})\n    if not response.ok:\n        raise ApplianceImageNotFound(origAppliance, requests_url, response.status_code)\n    else:\n        return origAppliance", "response": "Checks docker. io to see if an image exists using requests library."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _monkey_patch_boto():\n    \n    from boto import provider\n    from botocore.session import Session\n    from botocore.credentials import create_credential_resolver, RefreshableCredentials\n\n    # We cache the final credentials so that we don't send multiple processes to\n    # simultaneously bang on the EC2 metadata server or ask for MFA pins from the\n    # user.\n    cache_path = '~/.cache/aws/cached_temporary_credentials'\n    datetime_format = \"%Y-%m-%dT%H:%M:%SZ\"  # incidentally the same as the format used by AWS\n    log = logging.getLogger(__name__)\n\n    def datetime_to_str(dt):\n        \"\"\"\n        Convert a naive (implicitly UTC) datetime object into a string, explicitly UTC.\n\n        >>> datetime_to_str(datetime(1970, 1, 1, 0, 0, 0))\n        '1970-01-01T00:00:00Z'\n        \"\"\"\n        return dt.strftime(datetime_format)\n\n\n    def str_to_datetime(s):\n        \"\"\"\n        Convert a string, explicitly UTC into a naive (implicitly UTC) datetime object.\n\n        >>> str_to_datetime( '1970-01-01T00:00:00Z' )\n        datetime.datetime(1970, 1, 1, 0, 0)\n\n        Just to show that the constructor args for seconds and microseconds are optional:\n        >>> datetime(1970, 1, 1, 0, 0, 0)\n        datetime.datetime(1970, 1, 1, 0, 0)\n        \"\"\"\n        return datetime.strptime(s, datetime_format)\n\n    class BotoCredentialAdapter(provider.Provider):\n        \"\"\"\n        Adapter to allow Boto 2 to use AWS credentials obtained via Boto 3's\n        credential finding logic. This allows for automatic role assumption\n        respecting the Boto 3 config files, even when parts of the app still use\n        Boto 2.\n        \n        This class also handles cacheing credentials in multi-process environments\n        to avoid loads of processes swamping the EC2 metadata service.\n        \"\"\"\n        \n        def __init__(self, name, access_key=None, secret_key=None,\n            security_token=None, profile_name=None, **kwargs):\n            \"\"\"\n            Create a new BotoCredentialAdapter.\n            \"\"\"\n            # TODO: We take kwargs because new boto2 versions have an 'anon'\n            # argument and we want to be future proof\n            \n            if (name == 'aws' or name is None) and access_key is None and not kwargs.get('anon', False):\n                # We are on AWS and we don't have credentials passed along and we aren't anonymous.\n                # We will backend into a boto3 resolver for getting credentials.\n                self._boto3_resolver = create_credential_resolver(Session(profile=profile_name))\n            else:\n                # We will use the normal flow\n                self._boto3_resolver = None\n            \n            # Pass along all the arguments\n            super(BotoCredentialAdapter, self).__init__(name, access_key=access_key,\n                secret_key=secret_key, security_token=security_token,\n                profile_name=profile_name, **kwargs)\n            \n        def get_credentials(self, access_key=None, secret_key=None, security_token=None, profile_name=None):\n            \"\"\"\n            Make sure our credential fields are populated. Called by the base class\n            constructor.\n            \"\"\"\n            \n            if self._boto3_resolver is not None:\n                # Go get the credentials from the cache, or from boto3 if not cached.\n                # We need to be eager here; having the default None\n                # _credential_expiry_time makes the accessors never try to refresh.\n                self._obtain_credentials_from_cache_or_boto3()\n            else:\n                # We're not on AWS, or they passed a key, or we're anonymous.\n                # Use the normal route; our credentials shouldn't expire.\n                super(BotoCredentialAdapter, self).get_credentials(access_key=access_key,\n                    secret_key=secret_key, security_token=security_token, profile_name=profile_name)\n            \n        def _populate_keys_from_metadata_server(self):\n            \"\"\"\n            This override is misnamed; it's actually the only hook we have to catch\n            _credential_expiry_time being too soon and refresh the credentials. We\n            actually just go back and poke the cache to see if it feels like\n            getting us new credentials.\n            \n            Boto 2 hardcodes a refresh within 5 minutes of expiry:\n            https://github.com/boto/boto/blob/591911db1029f2fbb8ba1842bfcc514159b37b32/boto/provider.py#L247\n            \n            Boto 3 wants to refresh 15 or 10 minutes before expiry:\n            https://github.com/boto/botocore/blob/8d3ea0e61473fba43774eb3c74e1b22995ee7370/botocore/credentials.py#L279\n            \n            So if we ever want to refresh, Boto 3 wants to refresh too.\n            \"\"\"\n            \n            # This should only happen if we have expiring credentials, which we should only get from boto3\n            assert(self._boto3_resolver is not None)\n            \n            self._obtain_credentials_from_cache_or_boto3()\n        \n        def _obtain_credentials_from_boto3(self):\n            \"\"\"\n            We know the current cached credentials are not good, and that we\n            need to get them from Boto 3. Fill in our credential fields\n            (_access_key, _secret_key, _security_token,\n            _credential_expiry_time) from Boto 3.\n            \"\"\"\n            \n            # We get a Credentials object\n            # <https://github.com/boto/botocore/blob/8d3ea0e61473fba43774eb3c74e1b22995ee7370/botocore/credentials.py#L227>\n            # or a RefreshableCredentials, or None on failure.\n            creds = None\n            for attempt in retry(timeout=10, predicate=lambda _: True):\n                with attempt:\n                    creds = self._boto3_resolver.load_credentials()\n                    \n                    if creds is None:\n                        try:\n                            resolvers = str(self._boto3_resolver.providers)\n                        except:\n                            resolvers = \"(Resolvers unavailable)\"\n                        raise RuntimeError(\"Could not obtain AWS credentials from Boto3. Resolvers tried: \" + resolvers)\n            \n            # Make sure the credentials actually has some credentials if it is lazy\n            creds.get_frozen_credentials()\n            \n            # Get when the credentials will expire, if ever\n            if isinstance(creds, RefreshableCredentials):\n                # Credentials may expire.\n                # Get a naive UTC datetime like boto 2 uses from the boto 3 time.\n                self._credential_expiry_time = creds._expiry_time.astimezone(timezone('UTC')).replace(tzinfo=None)\n            else:\n                # Credentials never expire\n                self._credential_expiry_time = None\n            \n            # Then, atomically get all the credentials bits. They may be newer than we think they are, but never older.\n            frozen = creds.get_frozen_credentials()\n            \n            # Copy them into us\n            self._access_key = frozen.access_key\n            self._secret_key = frozen.secret_key\n            self._security_token = frozen.token\n        \n        def _obtain_credentials_from_cache_or_boto3(self):\n            \"\"\"\n            Get the cached credentials, or retrieve them from Boto 3 and cache them\n            (or wait for another cooperating process to do so) if they are missing\n            or not fresh enough.\n            \"\"\"\n            path = os.path.expanduser(cache_path)\n            tmp_path = path + '.tmp'\n            while True:\n                log.debug('Attempting to read cached credentials from %s.', path)\n                try:\n                    with open(path, 'r') as f:\n                        content = f.read()\n                        if content:\n                            record = content.split('\\n')\n                            assert len(record) == 4\n                            self._access_key = record[0]\n                            self._secret_key = record[1]\n                            self._security_token = record[2]\n                            self._credential_expiry_time = str_to_datetime(record[3])\n                        else:\n                            log.debug('%s is empty. Credentials are not temporary.', path)\n                            self._obtain_credentials_from_boto3()\n                            return\n                except IOError as e:\n                    if e.errno == errno.ENOENT:\n                        log.debug('Cached credentials are missing.')\n                        dir_path = os.path.dirname(path)\n                        if not os.path.exists(dir_path):\n                            log.debug('Creating parent directory %s', dir_path)\n                            # A race would be ok at this point\n                            mkdir_p(dir_path)\n                    else:\n                        raise\n                else:\n                    if self._credentials_need_refresh():\n                        log.debug('Cached credentials are expired.')\n                    else:\n                        log.debug('Cached credentials exist and are still fresh.')\n                        return\n                # We get here if credentials are missing or expired\n                log.debug('Racing to create %s.', tmp_path)\n                # Only one process, the winner, will succeed\n                try:\n                    fd = os.open(tmp_path, os.O_CREAT | os.O_EXCL | os.O_WRONLY, 0o600)\n                except OSError as e:\n                    if e.errno == errno.EEXIST:\n                        log.debug('Lost the race to create %s. Waiting on winner to remove it.', tmp_path)\n                        while os.path.exists(tmp_path):\n                            time.sleep(0.1)\n                        log.debug('Winner removed %s. Trying from the top.', tmp_path)\n                    else:\n                        raise\n                else:\n                    try:\n                        log.debug('Won the race to create %s.  Requesting credentials from backend.', tmp_path)\n                        self._obtain_credentials_from_boto3()\n                    except:\n                        os.close(fd)\n                        fd = None\n                        log.debug('Failed to obtain credentials, removing %s.', tmp_path)\n                        # This unblocks the loosers.\n                        os.unlink(tmp_path)\n                        # Bail out. It's too likely to happen repeatedly\n                        raise\n                    else:\n                        if self._credential_expiry_time is None:\n                            os.close(fd)\n                            fd = None\n                            log.debug('Credentials are not temporary.  Leaving %s empty and renaming it to %s.', tmp_path, path)\n                            # No need to actually cache permanent credentials,\n                            # because we hnow we aren't getting them from the\n                            # metadata server or by assuming a role. Those both\n                            # give temporary credentials.\n                        else:\n                            log.debug('Writing credentials to %s.', tmp_path)\n                            with os.fdopen(fd, 'w') as fh:\n                                fd = None\n                                fh.write('\\n'.join([\n                                    self._access_key,\n                                    self._secret_key,\n                                    self._security_token,\n                                    datetime_to_str(self._credential_expiry_time)]))\n                            log.debug('Wrote credentials to %s. Renaming to %s.', tmp_path, path)\n                        os.rename(tmp_path, path)\n                        return\n                    finally:\n                        if fd is not None:\n                            os.close(fd)\n                            \n    \n    # Now we have defined the adapter class. Patch the Boto module so it replaces the default Provider when Boto makes Providers.\n    provider.Provider = BotoCredentialAdapter", "response": "Monkey patch boto to use the same credentials as the Boto 3 configuration."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the remainingRetryCount to greater than zero and set the memory to be at least as big as the default memory.", "response": "def setupJobAfterFailure(self, config):\n        \"\"\"\n        Reduce the remainingRetryCount if greater than zero and set the memory\n        to be at least as big as the default memory (in case of exhaustion of memory,\n        which is common).\n        \"\"\"\n        self.remainingRetryCount = max(0, self.remainingRetryCount - 1)\n        logger.warn(\"Due to failure we are reducing the remaining retry count of job %s with ID %s to %s\",\n                    self, self.jobStoreID, self.remainingRetryCount)\n        # Set the default memory to be at least as large as the default, in\n        # case this was a malloc failure (we do this because of the combined\n        # batch system)\n        if self.memory < config.defaultMemory:\n            self._memory = config.defaultMemory\n            logger.warn(\"We have increased the default memory of the failed job %s to %s bytes\",\n                        self, self.memory)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrestarting the checkpoint after the total failure of jobs in its subtree.", "response": "def restartCheckpoint(self, jobStore):\n        \"\"\"Restart a checkpoint after the total failure of jobs in its subtree.\n\n        Writes the changes to the jobStore immediately. All the\n        checkpoint's successors will be deleted, but its retry count\n        will *not* be decreased.\n\n        Returns a list with the IDs of any successors deleted.\n        \"\"\"\n        assert self.checkpoint is not None\n        successorsDeleted = []\n        if self.stack or self.services or self.command != None:\n            if self.command != None:\n                assert self.command == self.checkpoint\n                logger.debug(\"Checkpoint job already has command set to run\")\n            else:\n                self.command = self.checkpoint\n\n            jobStore.update(self) # Update immediately to ensure that checkpoint\n            # is made before deleting any remaining successors\n\n            if self.stack or self.services:\n                # If the subtree of successors is not complete restart everything\n                logger.debug(\"Checkpoint job has unfinished successor jobs, deleting the jobs on the stack: %s, services: %s \" %\n                             (self.stack, self.services))\n                # Delete everything on the stack, as these represent successors to clean\n                # up as we restart the queue\n                def recursiveDelete(jobGraph2):\n                    # Recursive walk the stack to delete all remaining jobs\n                    for jobs in jobGraph2.stack + jobGraph2.services:\n                        for jobNode in jobs:\n                            if jobStore.exists(jobNode.jobStoreID):\n                                recursiveDelete(jobStore.load(jobNode.jobStoreID))\n                            else:\n                                logger.debug(\"Job %s has already been deleted\", jobNode)\n                    if jobGraph2 != self:\n                        logger.debug(\"Checkpoint is deleting old successor job: %s\", jobGraph2.jobStoreID)\n                        jobStore.delete(jobGraph2.jobStoreID)\n                        successorsDeleted.append(jobGraph2.jobStoreID)\n                recursiveDelete(self)\n\n                self.stack = [ [], [] ] # Initialise the job to mimic the state of a job\n                # that has been previously serialised but which as yet has no successors\n\n                self.services = [] # Empty the services\n\n                # Update the jobStore to avoid doing this twice on failure and make this clean.\n                jobStore.update(self)\n        return successorsDeleted"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fromJobNode(cls, jobNode, jobStoreID, tryCount):\n        return cls(command=jobNode.command,\n                   jobStoreID=jobStoreID,\n                   remainingRetryCount=tryCount,\n                   predecessorNumber=jobNode.predecessorNumber,\n                   unitName=jobNode.unitName, jobName=jobNode.jobName,\n                   **jobNode._requirements)", "response": "Builds a job graph object from a given job node."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sns(self):\n        if self.__sns is None:\n            self.__sns = self.__aws_connect(sns)\n        return self.__sns", "response": "returns a SNSConnection object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sqs(self):\n        if self.__sqs is None:\n            self.__sqs = self.__aws_connect(sqs)\n        return self.__sqs", "response": "returns a new SQSConnection object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef absolute_name(self, name):\n        if self.is_absolute_name(name):\n            result = name\n        else:\n            result = self.namespace + name\n        if not self.name_re.match(result):\n            raise self.InvalidPathError(result)\n        return result", "response": "Returns the absolute form of the specified resource name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a name that can be used to be used in AWS resources.", "response": "def to_aws_name(self, name):\n        \"\"\"\n        Returns a transliteration of the name that safe to use for resource names on AWS. If the\n        given name is relative, it converted to its absolute form before the transliteration.\n\n        The transliteration uses two consequitive '_' to encode a single '_' and a single '_' to\n        separate the name components. AWS-safe names are by definition absolute such that the\n        leading separator can be removed. This leads to fairly readable AWS-safe names,\n        especially for names in the root namespace, where the transliteration is the identity\n        function if the input does not contain any '_'.\n\n        This scheme only works if name components don't start with '_'. Without that condition,\n        '/_' would become '___' the inverse of which is '_/'.\n\n        >>> ctx = Context( 'us-west-1b', namespace='/' )\n\n        >>> ctx.to_aws_name( 'foo' )\n        'foo'\n        >>> ctx.from_aws_name( 'foo' )\n        'foo'\n\n        Illegal paths that would introduce ambiguity need to raise an exception\n        >>> ctx.to_aws_name('/_') # doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last): \n        ...\n        InvalidPathError: Invalid path '/_'\n        >>> ctx.to_aws_name('/_/') # doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last):\n        ...\n        InvalidPathError: Invalid path '/_/'\n        >>> ctx.from_aws_name('___') # doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last): \n        ...\n        InvalidPathError: Invalid path '/_/'\n\n        >>> ctx.to_aws_name( 'foo_bar')\n        'foo__bar'\n        >>> ctx.from_aws_name( 'foo__bar')\n        'foo_bar'\n\n        >>> ctx.to_aws_name( '/sub_ns/foo_bar')\n        'sub__ns_foo__bar'\n        >>> ctx.to_aws_name( 'sub_ns/foo_bar')\n        'sub__ns_foo__bar'\n        >>> ctx.from_aws_name( 'sub__ns_foo__bar' )\n        'sub_ns/foo_bar'\n\n        >>> ctx.to_aws_name( 'g_/' )\n        'g___'\n        >>> ctx.from_aws_name( 'g___' )\n        'g_/'\n\n        >>> ctx = Context( 'us-west-1b', namespace='/this_ns/' )\n\n        >>> ctx.to_aws_name( 'foo' )\n        'this__ns_foo'\n        >>> ctx.from_aws_name( 'this__ns_foo' )\n        'foo'\n\n        >>> ctx.to_aws_name( 'foo_bar')\n        'this__ns_foo__bar'\n        >>> ctx.from_aws_name( 'this__ns_foo__bar')\n        'foo_bar'\n\n        >>> ctx.to_aws_name( '/other_ns/foo_bar' )\n        'other__ns_foo__bar'\n        >>> ctx.from_aws_name( 'other__ns_foo__bar' )\n        '/other_ns/foo_bar'\n\n        >>> ctx.to_aws_name( 'other_ns/foo_bar' )\n        'this__ns_other__ns_foo__bar'\n        >>> ctx.from_aws_name( 'this__ns_other__ns_foo__bar' )\n        'other_ns/foo_bar'\n\n        >>> ctx.to_aws_name( '/this_ns/foo_bar' )\n        'this__ns_foo__bar'\n        >>> ctx.from_aws_name( 'this__ns_foo__bar' )\n        'foo_bar'\n        \"\"\"\n        name = self.absolute_name(name)\n        assert name.startswith('/')\n        return name[1:].replace('_', '__').replace('/', '_')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_aws_name(self, name):\n        name = '_'.join(s.replace('_', '/') for s in name.split('__'))\n        name = '/' + name\n        if not self.name_re.match(name):\n            raise self.InvalidPathError(name)\n        if name.startswith(self.namespace):\n            name = name[len(self.namespace):]\n        return name", "response": "Convert a aws name to a context name."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new resource object for the given job store and the given path.", "response": "def create(cls, jobStore, leaderPath):\n        \"\"\"\n        Saves the content of the file or directory at the given path to the given job store\n        and returns a resource object representing that content for the purpose of obtaining it\n        again at a generic, public URL. This method should be invoked on the leader node.\n\n        :param toil.jobStores.abstractJobStore.AbstractJobStore jobStore:\n\n        :param str leaderPath:\n\n        :rtype: Resource\n        \"\"\"\n        pathHash = cls._pathHash(leaderPath)\n        contentHash = hashlib.md5()\n        # noinspection PyProtectedMember\n        with cls._load(leaderPath) as src:\n            with jobStore.writeSharedFileStream(sharedFileName=pathHash, isProtected=False) as dst:\n                userScript = src.read()\n                contentHash.update(userScript)\n                dst.write(userScript)\n        return cls(name=os.path.basename(leaderPath),\n                   pathHash=pathHash,\n                   url=jobStore.getSharedPublicUrl(sharedFileName=pathHash),\n                   contentHash=contentHash.hexdigest())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npreparing this system for downloading and lookup of resources.", "response": "def prepareSystem(cls):\n        \"\"\"\n        Prepares this system for the downloading and lookup of resources. This method should only\n        be invoked on a worker node. It is idempotent but not thread-safe.\n        \"\"\"\n        try:\n            resourceRootDirPath = os.environ[cls.rootDirPathEnvName]\n        except KeyError:\n            # Create directory holding local copies of requested resources ...\n            resourceRootDirPath = mkdtemp()\n            # .. and register its location in an environment variable such that child processes\n            # can find it.\n            os.environ[cls.rootDirPathEnvName] = resourceRootDirPath\n        assert os.path.isdir(resourceRootDirPath)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove all downloaded localized resources from the system.", "response": "def cleanSystem(cls):\n        \"\"\"\n        Removes all downloaded, localized resources\n        \"\"\"\n        resourceRootDirPath = os.environ[cls.rootDirPathEnvName]\n        os.environ.pop(cls.rootDirPathEnvName)\n        shutil.rmtree(resourceRootDirPath)\n        for k, v in list(os.environ.items()):\n            if k.startswith(cls.resourceEnvNamePrefix):\n                os.environ.pop(k)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nregisters this resource for later retrieval via lookup.", "response": "def register(self):\n        \"\"\"\n        Register this resource for later retrieval via lookup(), possibly in a child process.\n        \"\"\"\n        os.environ[self.resourceEnvNamePrefix + self.pathHash] = self.pickle()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a resource object representing a resource created from a file or directory at the given path on the leader.", "response": "def lookup(cls, leaderPath):\n        \"\"\"\n        Returns a resource object representing a resource created from a file or directory at the\n        given path on the leader. This method should be invoked on the worker. The given path\n        does not need to refer to an existing file or directory on the worker, it only identifies\n        the resource within an instance of toil. This method returns None if no resource for the\n        given path exists.\n\n        :rtype: Resource\n        \"\"\"\n        pathHash = cls._pathHash(leaderPath)\n        try:\n            path_key = cls.resourceEnvNamePrefix + pathHash\n            s = os.environ[path_key]\n        except KeyError:\n            log.warn(\"'%s' may exist, but is not yet referenced by the worker (KeyError from os.environ[]).\", str(path_key))\n            return None\n        else:\n            self = cls.unpickle(s)\n            assert self.pathHash == pathHash\n            return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndownloading this resource from its URL to a file on the local system.", "response": "def download(self, callback=None):\n        \"\"\"\n        Downloads this resource from its URL to a file on the local system. This method should\n        only be invoked on a worker node after the node was setup for accessing resources via\n        prepareSystem().\n        \"\"\"\n        dirPath = self.localDirPath\n        if not os.path.exists(dirPath):\n            tempDirPath = mkdtemp(dir=os.path.dirname(dirPath), prefix=self.contentHash + \"-\")\n            self._save(tempDirPath)\n            if callback is not None:\n                callback(tempDirPath)\n            try:\n                os.rename(tempDirPath, dirPath)\n            except OSError as e:\n                # If dirPath already exists & is non-empty either ENOTEMPTY or EEXIST will be raised\n                if e.errno == errno.ENOTEMPTY or e.errno == errno.EEXIST:\n                    # Another process beat us to it.\n                    # TODO: This is correct but inefficient since multiple processes download the resource redundantly\n                    pass\n                else:\n                    raise"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef localDirPath(self):\n        rootDirPath = os.environ[self.rootDirPathEnvName]\n        return os.path.join(rootDirPath, self.contentHash)", "response": "Returns the path to the local directory where the resource is stored."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _download(self, dstFile):\n        for attempt in retry(predicate=lambda e: isinstance(e, HTTPError) and e.code == 400):\n            with attempt:\n                with closing(urlopen(self.url)) as content:\n                    buf = content.read()\n        contentHash = hashlib.md5(buf)\n        assert contentHash.hexdigest() == self.contentHash\n        dstFile.write(buf)", "response": "Download this resource from its URL to the given file object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload the cache file from a file.", "response": "def _load(cls, path):\n        \"\"\"\n        :type path: str\n        \"\"\"\n        bytesIO = BytesIO()\n        initfile = os.path.join(path, '__init__.py')\n        if os.path.isfile(initfile):\n            # This is a package directory. To emulate\n            # PyZipFile.writepy's behavior, we need to keep everything\n            # relative to this path's parent directory.\n            rootDir = os.path.dirname(path)\n        else:\n            # This is a simple user script (with possibly a few helper files)\n            rootDir = path\n        with ZipFile(file=bytesIO, mode='w') as zipFile:\n            for dirName, _, fileList in os.walk(path):\n                for fileName in fileList:\n                    fullPath = os.path.join(dirName, fileName)\n                    zipFile.write(fullPath, os.path.relpath(fullPath, rootDir))\n        bytesIO.seek(0)\n        return bytesIO"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _load(cls, path):\n        assert os.path.basename(path) == 'site-packages'\n        bytesIO = BytesIO()\n        with ZipFile(file=bytesIO, mode='w') as zipFile:\n            for dirName, _, fileList in os.walk(path):\n                zipFile.write(dirName)\n                for fileName in fileList:\n                    fullPath = os.path.join(dirName, fileName)\n                    zipFile.write(fullPath, os.path.relpath(fullPath, path))\n        bytesIO.seek(0)\n        return bytesIO", "response": "Load a single entry from a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef forModule(cls, name):\n        module = sys.modules[name]\n        filePath = os.path.abspath(module.__file__)\n        filePath = filePath.split(os.path.sep)\n        filePath[-1], extension = os.path.splitext(filePath[-1])\n        if not extension in ('.py', '.pyc'):\n            raise Exception('The name of a user script/module must end in .py or .pyc.')\n        if name == '__main__':\n            log.debug(\"Discovering real name of module\")\n            # User script/module was invoked as the main program\n            if module.__package__:\n                # Invoked as a module via python -m foo.bar\n                log.debug(\"Script was invoked as a module\")\n                name = [filePath.pop()]\n                for package in reversed(module.__package__.split('.')):\n                    dirPathTail = filePath.pop()\n                    assert dirPathTail == package\n                    name.append(dirPathTail)\n                name = '.'.join(reversed(name))\n                dirPath = os.path.sep.join(filePath)\n            else:\n                # Invoked as a script via python foo/bar.py\n                name = filePath.pop()\n                dirPath = os.path.sep.join(filePath)\n                cls._check_conflict(dirPath, name)\n        else:\n            # User module was imported. Determine the directory containing the top-level package\n            if filePath[-1] == '__init__':\n                # module is a subpackage\n                filePath.pop()\n\n            for package in reversed(name.split('.')):\n                dirPathTail = filePath.pop()\n                assert dirPathTail == package\n            dirPath = os.path.sep.join(filePath)\n        log.debug(\"Module dir is %s\", dirPath)\n        if not os.path.isdir(dirPath):\n            raise Exception('Bad directory path %s for module %s. Note that hot-deployment does not support .egg-link files yet, or scripts located in the root directory.' % (dirPath, name))\n        fromVirtualEnv = inVirtualEnv() and dirPath.startswith(sys.prefix)\n        return cls(dirPath=dirPath, name=name, fromVirtualEnv=fromVirtualEnv)", "response": "Returns an instance of the class cls representing the module with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking whether the module of the given name conflicts with another module on the sys. path.", "response": "def _check_conflict(cls, dirPath, name):\n        \"\"\"\n        Check whether the module of the given name conflicts with another module on the sys.path.\n\n        :param dirPath: the directory from which the module was originally loaded\n        :param name: the mpdule name\n        \"\"\"\n        old_sys_path = sys.path\n        try:\n            sys.path = [d for d in old_sys_path if os.path.realpath(d) != os.path.realpath(dirPath)]\n            try:\n                colliding_module = importlib.import_module(name)\n            except ImportError:\n                pass\n            else:\n                raise ResourceException(\n                    \"The user module '%s' collides with module '%s from '%s'.\" % (\n                        name, colliding_module.__name__, colliding_module.__file__))\n        finally:\n            sys.path = old_sys_path"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the concrete subclass of Resource that s appropriate for auto - deploying this module.", "response": "def _getResourceClass(self):\n        \"\"\"\n        Return the concrete subclass of Resource that's appropriate for auto-deploying this module.\n        \"\"\"\n        if self.fromVirtualEnv:\n            subcls = VirtualEnvResource\n        elif os.path.isdir(self._resourcePath):\n            subcls = DirectoryResource\n        elif os.path.isfile(self._resourcePath):\n            subcls = FileResource\n        elif os.path.exists(self._resourcePath):\n            raise AssertionError(\"Neither a file or a directory: '%s'\" % self._resourcePath)\n        else:\n            raise AssertionError(\"No such file or directory: '%s'\" % self._resourcePath)\n        return subcls"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a new resource descriptor that points to a local copy of this module.", "response": "def localize(self):\n        \"\"\"\n        Check if this module was saved as a resource. If it was, return a new module descriptor\n        that points to a local copy of that resource. Should only be called on a worker node. On\n        the leader, this method returns this resource, i.e. self.\n\n        :rtype: toil.resource.Resource\n        \"\"\"\n        if not self._runningOnWorker():\n            log.warn('The localize() method should only be invoked on a worker.')\n        resource = Resource.lookup(self._resourcePath)\n        if resource is None:\n            return self\n        else:\n            def stash(tmpDirPath):\n                # Save the original dirPath such that we can restore it in globalize()\n                with open(os.path.join(tmpDirPath, '.stash'), 'w') as f:\n                    f.write('1' if self.fromVirtualEnv else '0')\n                    f.write(self.dirPath)\n\n            resource.download(callback=stash)\n            return self.__class__(dirPath=resource.localDirPath,\n                                  name=self.name,\n                                  fromVirtualEnv=self.fromVirtualEnv)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef globalize(self):\n        try:\n            with open(os.path.join(self.dirPath, '.stash')) as f:\n                fromVirtualEnv = [False, True][int(f.read(1))]\n                dirPath = f.read()\n        except IOError as e:\n            if e.errno == errno.ENOENT:\n                if self._runningOnWorker():\n                    log.warn(\"Can't globalize module %r.\", self)\n                return self\n            else:\n                raise\n        else:\n            return self.__class__(dirPath=dirPath,\n                                  name=self.name,\n                                  fromVirtualEnv=fromVirtualEnv)", "response": "Returns a new instance of the class with the same name and dirPath."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the path to the directory where the user script should be loaded.", "response": "def _resourcePath(self):\n        \"\"\"\n        The path to the directory that should be used when shipping this module and its siblings\n        around as a resource.\n        \"\"\"\n        if self.fromVirtualEnv:\n            return self.dirPath\n        elif '.' in self.name:\n            return os.path.join(self.dirPath, self._rootPackage())\n        else:\n            initName = self._initModuleName(self.dirPath)\n            if initName:\n                raise ResourceException(\n                    \"Toil does not support loading a user script from a package directory. You \"\n                    \"may want to remove %s from %s or invoke the user script as a module via \"\n                    \"'PYTHONPATH=\\\"%s\\\" python -m %s.%s'.\" %\n                    tuple(concat(initName, self.dirPath, os.path.split(self.dirPath), self.name)))\n            return self.dirPath"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fetchJobStoreFiles(jobStore, options):\n    for jobStoreFile in options.fetch:\n        jobStoreHits = recursiveGlob(directoryname=options.jobStore,\n                                     glob_pattern=jobStoreFile)\n        for jobStoreFileID in jobStoreHits:\n            logger.debug(\"Copying job store file: %s to %s\",\n                        jobStoreFileID,\n                        options.localFilePath[0])\n            jobStore.readFile(jobStoreFileID,\n                              os.path.join(options.localFilePath[0],\n                              os.path.basename(jobStoreFileID)),\n                              symlink=options.useSymlinks)", "response": "Fetches all files from the given directory and copies them into the given jobStore."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfetching a list of all files contained in the jobStore directory input if nameOfJob is not declared, otherwise it only prints out the names of files for that specific job for which it can find a match. Also creates a logFile containing this same record of job files in the working directory. :param jobStorePath: Directory path to recursively look for files. :param nameOfJob: Default is None, which prints out all files in the jobStore. If specified, it will print all jobStore files that have been written to the jobStore by that job.", "response": "def printContentsOfJobStore(jobStorePath, nameOfJob=None):\n    \"\"\"\n    Fetch a list of all files contained in the jobStore directory input if\n    nameOfJob is not declared, otherwise it only prints out the names of files\n    for that specific job for which it can find a match.  Also creates a logFile\n    containing this same record of job files in the working directory.\n\n    :param jobStorePath: Directory path to recursively look for files.\n    :param nameOfJob: Default is None, which prints out all files in the jobStore.\n    If specified, it will print all jobStore files that have been written to the\n    jobStore by that job.\n    \"\"\"\n\n    if nameOfJob:\n        glob = \"*\" + nameOfJob + \"*\"\n        logFile = nameOfJob + \"_fileset.txt\"\n    else:\n        glob = \"*\"\n        logFile = \"jobstore_files.txt\"\n        nameOfJob = \"\"\n\n    list_of_files = recursiveGlob(directoryname=jobStorePath, glob_pattern=glob)\n    if os.path.exists(logFile):\n        os.remove(logFile)\n    for gfile in sorted(list_of_files):\n        if not gfile.endswith('.new'):\n            logger.debug(nameOfJob + \"File: %s\", os.path.basename(gfile))\n            with open(logFile, \"a+\") as f:\n                    f.write(os.path.basename(gfile))\n                    f.write(\"\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the maximum number of bytes of disk the job will require to run.", "response": "def disk(self):\n        \"\"\"\n        The maximum number of bytes of disk the job will require to run.\n        \"\"\"\n        if self._disk is not None:\n            return self._disk\n        elif self._config is not None:\n            return self._config.defaultDisk\n        else:\n            raise AttributeError(\"Default value for 'disk' cannot be determined\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef memory(self):\n        if self._memory is not None:\n            return self._memory\n        elif self._config is not None:\n            return self._config.defaultMemory\n        else:\n            raise AttributeError(\"Default value for 'memory' cannot be determined\")", "response": "Returns the maximum number of bytes of memory the job will require to run."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cores(self):\n        if self._cores is not None:\n            return self._cores\n        elif self._config is not None:\n            return self._config.defaultCores\n        else:\n            raise AttributeError(\"Default value for 'cores' cannot be determined\")", "response": "Returns the number of CPU cores required."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef preemptable(self):\n        if self._preemptable is not None:\n            return self._preemptable\n        elif self._config is not None:\n            return self._config.defaultPreemptable\n        else:\n            raise AttributeError(\"Default value for 'preemptable' cannot be determined\")", "response": "Returns a boolean indicating whether the job can be run on a preemptable node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _requirements(self):\n        return {'memory': getattr(self, 'memory', None),\n                'cores': getattr(self, 'cores', None),\n                'disk': getattr(self, 'disk', None),\n                'preemptable': getattr(self, 'preemptable', None)}", "response": "Returns a dictionary of all the object s resource requirements. Unset values are defaulted to None\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a Toil job s resource requirement value and apply type checks.", "response": "def _parseResource(name, value):\n        \"\"\"\n        Parse a Toil job's resource requirement value and apply resource-specific type checks. If the\n        value is a string, a binary or metric unit prefix in it will be evaluated and the\n        corresponding integral value will be returned.\n\n        :param str name: The name of the resource\n        :param None|str|float|int value: The resource value\n        :rtype: int|float|None\n\n        >>> Job._parseResource('cores', None)\n        >>> Job._parseResource('cores', 1), Job._parseResource('disk', 1), \\\n        Job._parseResource('memory', 1)\n        (1, 1, 1)\n        >>> Job._parseResource('cores', '1G'), Job._parseResource('disk', '1G'), \\\n        Job._parseResource('memory', '1G')\n        (1073741824, 1073741824, 1073741824)\n        >>> Job._parseResource('cores', 1.1)\n        1.1\n        >>> Job._parseResource('disk', 1.1) # doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last):\n        ...\n        TypeError: The 'disk' requirement does not accept values that are of <type 'float'>\n        >>> Job._parseResource('memory', object()) # doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last):\n        ...\n        TypeError: The 'memory' requirement does not accept values that are of ...\n        \"\"\"\n        assert name in ('memory', 'disk', 'cores')\n        if value is None:\n            return value\n        elif isinstance(value, (str, bytes)):\n            value = human2bytes(value)\n        if isinstance(value, int):\n            return value\n        elif isinstance(value, float) and name == 'cores':\n            return value\n        else:\n            raise TypeError(\"The '%s' requirement does not accept values that are of %s\"\n                            % (name, type(value)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new object from a job graph object.", "response": "def fromJobGraph(cls, jobGraph):\n        \"\"\"\n        Takes a job graph object and returns a job node object\n        :param toil.jobGraph.JobGraph jobGraph: A job graph object to be transformed into a job node\n        :return: A job node object\n        :rtype: toil.job.JobNode\n        \"\"\"\n        return cls(jobStoreID=jobGraph.jobStoreID,\n                   requirements=jobGraph._requirements,\n                   command=jobGraph.command,\n                   jobName=jobGraph.jobName,\n                   unitName=jobGraph.unitName,\n                   displayName=jobGraph.displayName,\n                   predecessorNumber=jobGraph.predecessorNumber)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fromJob(cls, job, command, predecessorNumber):\n        return cls(jobStoreID=None,\n                   requirements=job._requirements,\n                   command=command,\n                   jobName=job.jobName,\n                   unitName=job.unitName,\n                   displayName=job.displayName,\n                   predecessorNumber=predecessorNumber)", "response": "Build a JobNode object from a job object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef addChild(self, childJob):\n        self._children.append(childJob)\n        childJob._addPredecessor(self)\n        return childJob", "response": "Adds a child job to be run as child of this job."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef addFollowOn(self, followOnJob):\n        self._followOns.append(followOnJob)\n        followOnJob._addPredecessor(self)\n        return followOnJob", "response": "Adds a follow - on job to the list of follow - on jobs."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a service to the job.", "response": "def addService(self, service, parentService=None):\n        \"\"\"\n        Add a service.\n\n        The :func:`toil.job.Job.Service.start` method of the service will be called\n        after the run method has completed but before any successors are run.\n        The service's :func:`toil.job.Job.Service.stop` method will be called once\n        the successors of the job have been run.\n\n        Services allow things like databases and servers to be started and accessed\n        by jobs in a workflow.\n\n        :raises toil.job.JobException: If service has already been made the child of a job or another service.\n        :param toil.job.Job.Service service: Service to add.\n        :param toil.job.Job.Service parentService: Service that will be started before 'service' is\n            started. Allows trees of services to be established. parentService must be a service\n            of this job.\n        :return: a promise that will be replaced with the return value from\n            :func:`toil.job.Job.Service.start` of service in any successor of the job.\n        :rtype: toil.job.Promise\n        \"\"\"\n        if parentService is not None:\n            # Do check to ensure that parentService is a service of this job\n            def check(services):\n                for jS in services:\n                    if jS.service == parentService or check(jS.service._childServices):\n                        return True\n                return False\n            if not check(self._services):\n                raise JobException(\"Parent service is not a service of the given job\")\n            return parentService._addChild(service)\n        else:\n            if service._hasParent:\n                raise JobException(\"The service already has a parent service\")\n            service._hasParent = True\n            jobService = ServiceJob(service)\n            self._services.append(jobService)\n            return jobService.rv()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a function as a child job.", "response": "def addChildFn(self, fn, *args, **kwargs):\n        \"\"\"\n        Adds a function as a child job.\n\n        :param fn: Function to be run as a child job with ``*args`` and ``**kwargs`` as \\\n        arguments to this function. See toil.job.FunctionWrappingJob for reserved \\\n        keyword arguments used to specify resource requirements.\n        :return: The new child job that wraps fn.\n        :rtype: toil.job.FunctionWrappingJob\n        \"\"\"\n        if PromisedRequirement.convertPromises(kwargs):\n            return self.addChild(PromisedRequirementFunctionWrappingJob.create(fn, *args, **kwargs))\n        else:\n            return self.addChild(FunctionWrappingJob(fn, *args, **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef addFollowOnFn(self, fn, *args, **kwargs):\n        if PromisedRequirement.convertPromises(kwargs):\n            return self.addFollowOn(PromisedRequirementFunctionWrappingJob.create(fn, *args, **kwargs))\n        else:\n            return self.addFollowOn(FunctionWrappingJob(fn, *args, **kwargs))", "response": "Adds a function as a follow - on job."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a job function as a child job.", "response": "def addChildJobFn(self, fn, *args, **kwargs):\n        \"\"\"\n        Adds a job function as a child job. See :class:`toil.job.JobFunctionWrappingJob`\n        for a definition of a job function.\n\n        :param fn: Job function to be run as a child job with ``*args`` and ``**kwargs`` as \\\n        arguments to this function. See toil.job.JobFunctionWrappingJob for reserved \\\n        keyword arguments used to specify resource requirements.\n        :return: The new child job that wraps fn.\n        :rtype: toil.job.JobFunctionWrappingJob\n        \"\"\"\n        if PromisedRequirement.convertPromises(kwargs):\n            return self.addChild(PromisedRequirementJobFunctionWrappingJob.create(fn, *args, **kwargs))\n        else:\n            return self.addChild(JobFunctionWrappingJob(fn, *args, **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a function to be run as a follow - on job.", "response": "def addFollowOnJobFn(self, fn, *args, **kwargs):\n        \"\"\"\n        Add a follow-on job function. See :class:`toil.job.JobFunctionWrappingJob`\n        for a definition of a job function.\n\n        :param fn: Job function to be run as a follow-on job with ``*args`` and ``**kwargs`` as \\\n        arguments to this function. See toil.job.JobFunctionWrappingJob for reserved \\\n        keyword arguments used to specify resource requirements.\n        :return: The new follow-on job that wraps fn.\n        :rtype: toil.job.JobFunctionWrappingJob\n        \"\"\"\n        if PromisedRequirement.convertPromises(kwargs):\n            return self.addFollowOn(PromisedRequirementJobFunctionWrappingJob.create(fn, *args, **kwargs))\n        else:\n            return self.addFollowOn(JobFunctionWrappingJob(fn, *args, **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the path to a temporary directory for the given xsd object.", "response": "def tempDir(self):\n        \"\"\"\n        Shortcut to calling :func:`job.fileStore.getLocalTempDir`. Temp dir is created on first call\n        and will be returned for first and future calls\n        :return: Path to tempDir. See `job.fileStore.getLocalTempDir`\n        :rtype: str\n        \"\"\"\n        if self._tempDir is None:\n            self._tempDir = self._fileStore.getLocalTempDir()\n        return self._tempDir"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef log(self, text, level=logging.INFO):\n        self._fileStore.logToMaster(text, level)", "response": "convenience wrapper for :func:`fileStore.logToMaster`"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wrapFn(fn, *args, **kwargs):\n        if PromisedRequirement.convertPromises(kwargs):\n            return PromisedRequirementFunctionWrappingJob.create(fn, *args, **kwargs)\n        else:\n            return FunctionWrappingJob(fn, *args, **kwargs)", "response": "This function wraps a function in a Job."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a Job out of a job function. \\ Convenience function for constructor of :class:`toil.job.JobFunctionWrappingJob`. :param fn: Job function to be run with ``*args`` and ``**kwargs`` as arguments. \\ See toil.job.JobFunctionWrappingJob for reserved keyword arguments used \\ to specify resource requirements. :return: The new job function that wraps fn. :rtype: toil.job.JobFunctionWrappingJob", "response": "def wrapJobFn(fn, *args, **kwargs):\n        \"\"\"\n        Makes a Job out of a job function. \\\n        Convenience function for constructor of :class:`toil.job.JobFunctionWrappingJob`.\n\n        :param fn: Job function to be run with ``*args`` and ``**kwargs`` as arguments. \\\n        See toil.job.JobFunctionWrappingJob for reserved keyword arguments used \\\n        to specify resource requirements.\n        :return: The new job function that wraps fn.\n        :rtype: toil.job.JobFunctionWrappingJob\n        \"\"\"\n        if PromisedRequirement.convertPromises(kwargs):\n            return PromisedRequirementJobFunctionWrappingJob.create(fn, *args, **kwargs)\n        else:\n            return JobFunctionWrappingJob(fn, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getRootJobs(self):\n        roots = set()\n        visited = set()\n        #Function to get the roots of a job\n        def getRoots(job):\n            if job not in visited:\n                visited.add(job)\n                if len(job._directPredecessors) > 0:\n                    list(map(lambda p : getRoots(p), job._directPredecessors))\n                else:\n                    roots.add(job)\n                #The following call ensures we explore all successor edges.\n                list(map(lambda c : getRoots(c), job._children +\n                    job._followOns))\n        getRoots(self)\n        return roots", "response": "Returns the set of jobs that contain this job."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if the augmented job graph contains cycles.", "response": "def checkJobGraphAcylic(self):\n        \"\"\"\n        :raises toil.job.JobGraphDeadlockException: if the connected component \\\n        of jobs containing this job contains any cycles of child/followOn dependencies \\\n        in the *augmented job graph* (see below). Such cycles are not allowed \\\n        in valid job graphs.\n\n        A follow-on edge (A, B) between two jobs A and B is equivalent \\\n        to adding a child edge to B from (1) A, (2) from each child of A, \\\n        and (3) from the successors of each child of A. We call each such edge \\\n        an edge an \"implied\" edge. The augmented job graph is a job graph including \\\n        all the implied edges.\n\n        For a job graph G = (V, E) the algorithm is ``O(|V|^2)``. It is ``O(|V| + |E|)`` for \\\n        a graph with no follow-ons. The former follow-on case could be improved!\n        \"\"\"\n        #Get the root jobs\n        roots = self.getRootJobs()\n        if len(roots) == 0:\n            raise JobGraphDeadlockException(\"Graph contains no root jobs due to cycles\")\n\n        #Get implied edges\n        extraEdges = self._getImpliedEdges(roots)\n\n        #Check for directed cycles in the augmented graph\n        visited = set()\n        for root in roots:\n            root._checkJobGraphAcylicDFS([], visited, extraEdges)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef checkNewCheckpointsAreLeafVertices(self):\n        roots = self.getRootJobs() # Roots jobs of component, these are preexisting jobs in the graph\n\n        # All jobs in the component of the job graph containing self\n        jobs = set()\n        list(map(lambda x : x._dfs(jobs), roots))\n\n        # Check for each job for which checkpoint is true that it is a cut vertex or leaf\n        for y in [x for x in jobs if x.checkpoint]:\n            if y not in roots: # The roots are the prexisting jobs\n                if not Job._isLeafVertex(y):\n                    raise JobGraphDeadlockException(\"New checkpoint job %s is not a leaf in the job graph\" % y)", "response": "Checks if new checkpoints are leaf vertices."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nregisters a function that will be called after the current job conclusion at the given time.", "response": "def defer(self, function, *args, **kwargs):\n        \"\"\"\n        Register a deferred function, i.e. a callable that will be invoked after the current\n        attempt at running this job concludes. A job attempt is said to conclude when the job\n        function (or the :meth:`toil.job.Job.run` method for class-based jobs) returns, raises an\n        exception or after the process running it terminates abnormally. A deferred function will\n        be called on the node that attempted to run the job, even if a subsequent attempt is made\n        on another node. A deferred function should be idempotent because it may be called\n        multiple times on the same node or even in the same process. More than one deferred\n        function may be registered per job attempt by calling this method repeatedly with\n        different arguments. If the same function is registered twice with the same or different\n        arguments, it will be called twice per job attempt.\n\n        Examples for deferred functions are ones that handle cleanup of resources external to\n        Toil, like Docker containers, files outside the work directory, etc.\n\n        :param callable function: The function to be called after this job concludes.\n\n        :param list args: The arguments to the function\n\n        :param dict kwargs: The keyword arguments to the function\n        \"\"\"\n        if self._fileStore is None:\n            raise Exception('A deferred function may only be registered with a job while that job is running.')\n        self._fileStore._registerDeferredFunction(DeferredFunction.create(function, *args, **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _addPredecessor(self, predecessorJob):\n        if predecessorJob in self._directPredecessors:\n            raise RuntimeError(\"The given job is already a predecessor of this job\")\n        self._directPredecessors.add(predecessorJob)", "response": "Adds a predecessor job to the set of predecessor jobs. Raises a \\ RuntimeError if the job is already a predecessor."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _loadJob(cls, command, jobStore):\n        commandTokens = command.split()\n        assert \"_toil\" == commandTokens[0]\n        userModule = ModuleDescriptor.fromCommand(commandTokens[2:])\n        logger.debug('Loading user module %s.', userModule)\n        userModule = cls._loadUserModule(userModule)\n        pickleFile = commandTokens[1]\n\n        # Get a directory to download the job in\n        directory = tempfile.mkdtemp()\n        # Initialize a blank filename so the finally below can't fail due to a\n        # missing variable\n        filename = ''\n\n        try:\n            # Get a filename to download the job to.\n            # Don't use mkstemp because we would need to delete and replace the\n            # file.\n            # Don't use a NamedTemporaryFile context manager because its\n            # context manager exit will crash if we deleted it.\n            filename = os.path.join(directory, 'job')\n                \n            # Download the job\n            if pickleFile == \"firstJob\":\n                jobStore.readSharedFile(pickleFile, filename)\n            else:\n                jobStore.readFile(pickleFile, filename)\n\n            # Open and unpickle\n            with open(filename, 'rb') as fileHandle:\n                return cls._unpickle(userModule, fileHandle, jobStore.config)\n\n            # TODO: We ought to just unpickle straight from a streaming read\n        finally:\n            # Clean up the file\n            if os.path.exists(filename):\n                os.unlink(filename)\n            # Clean up the directory we put it in\n            # TODO: we assume nobody else put anything in the directory\n            if os.path.exists(directory):\n                os.rmdir(directory)", "response": "Loads a job from the given command and returns the Job instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _unpickle(cls, userModule, fileHandle, config):\n        \n        def filter_main(module_name, class_name):\n            try:\n                if module_name == '__main__':\n                    return getattr(userModule, class_name)\n                else:\n                    return getattr(importlib.import_module(module_name), class_name)\n            except:\n                if module_name == '__main__':\n                    logger.debug('Failed getting %s from module %s.', class_name, userModule)\n                else:\n                    logger.debug('Failed getting %s from module %s.', class_name, module_name)\n                raise\n\n        try:\n            unpickler = pickle.Unpickler(fileHandle)\n            # In Python 2 with cPickle we set \"find_global\"\n            unpickler.find_global = filter_main\n        except AttributeError:\n            # In Python 3 find_global isn't real and we are supposed to\n            # subclass unpickler and override find_class. We can't just replace\n            # it. But with cPickle in Pyhton 2 we can't subclass Unpickler.\n            \n            class FilteredUnpickler(pickle.Unpickler):\n                def find_class(self, module, name):\n                    return filter_main(module, name)\n                    \n            unpickler = FilteredUnpickler(fileHandle)\n            \n        runnable = unpickler.load()\n        assert isinstance(runnable, BaseJob)\n        runnable._config = config\n        return runnable", "response": "Unpickles an object graph from the given file handle while loading symbols \\ __main__ module from the given userModule."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the values for promises using the return values from this job s run function.", "response": "def _fulfillPromises(self, returnValues, jobStore):\n        \"\"\"\n        Sets the values for promises using the return values from this job's run() function.\n        \"\"\"\n        for path, promiseFileStoreIDs in iteritems(self._rvs):\n            if not path:\n                # Note that its possible for returnValues to be a promise, not an actual return\n                # value. This is the case if the job returns a promise from another job. In\n                # either case, we just pass it on.\n                promisedValue = returnValues\n            else:\n                # If there is an path ...\n                if isinstance(returnValues, Promise):\n                    # ... and the value itself is a Promise, we need to created a new, narrower\n                    # promise and pass it on.\n                    promisedValue = Promise(returnValues.job, path)\n                else:\n                    # Otherwise, we just select the desired component of the return value.\n                    promisedValue = returnValues\n                    for index in path:\n                        promisedValue = promisedValue[index]\n            for promiseFileStoreID in promiseFileStoreIDs:\n                # File may be gone if the job is a service being re-run and the accessing job is\n                # already complete.\n                if jobStore.fileExists(promiseFileStoreID):\n                    with jobStore.updateFileStream(promiseFileStoreID) as fileHandle:\n                        pickle.dump(promisedValue, fileHandle, pickle.HIGHEST_PROTOCOL)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _dfs(self, visited):\n        if self not in visited:\n            visited.add(self)\n            for successor in self._children + self._followOns:\n                successor._dfs(visited)", "response": "Recursively adds the job and all jobs reachable on a directed path from current node to the given set."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _checkJobGraphAcylicDFS(self, stack, visited, extraEdges):\n        if self not in visited:\n            visited.add(self)\n            stack.append(self)\n            for successor in self._children + self._followOns + extraEdges[self]:\n                successor._checkJobGraphAcylicDFS(stack, visited, extraEdges)\n            assert stack.pop() == self\n        if self in stack:\n            stack.append(self)\n            raise JobGraphDeadlockException(\"A cycle of job dependencies has been detected '%s'\" % stack)", "response": "Acyclic DFS traversal of the job graph."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the set of implied edges. See Job. checkJobGraphAcylic", "response": "def _getImpliedEdges(roots):\n        \"\"\"\n        Gets the set of implied edges. See Job.checkJobGraphAcylic\n        \"\"\"\n        #Get nodes in job graph\n        nodes = set()\n        for root in roots:\n            root._dfs(nodes)\n\n        ##For each follow-on edge calculate the extra implied edges\n        #Adjacency list of implied edges, i.e. map of jobs to lists of jobs\n        #connected by an implied edge\n        extraEdges = dict([(n, []) for n in nodes])\n        for job in nodes:\n            if len(job._followOns) > 0:\n                #Get set of jobs connected by a directed path to job, starting\n                #with a child edge\n                reacheable = set()\n                for child in job._children:\n                    child._dfs(reacheable)\n                #Now add extra edges\n                for descendant in reacheable:\n                    extraEdges[descendant] += job._followOns[:]\n        return extraEdges"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an empty job graph for the job.", "response": "def _createEmptyJobGraphForJob(self, jobStore, command=None, predecessorNumber=0):\n        \"\"\"\n        Create an empty job for the job.\n        \"\"\"\n        # set _config to determine user determined default values for resource requirements\n        self._config = jobStore.config\n        return jobStore.create(JobNode.fromJob(self, command=command,\n                                               predecessorNumber=predecessorNumber))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _makeJobGraphs(self, jobGraph, jobStore):\n        jobsToJobGraphs = {self:jobGraph}\n        for successors in (self._followOns, self._children):\n            jobs = [successor._makeJobGraphs2(jobStore, jobsToJobGraphs) for successor in successors]\n            jobGraph.stack.append(jobs)\n        return jobsToJobGraphs", "response": "Creates a jobGraph for each job in the job graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of jobs that can be run before the job at the given index.", "response": "def getTopologicalOrderingOfJobs(self):\n        \"\"\"\n        :returns: a list of jobs such that for all pairs of indices i, j for which i < j, \\\n        the job at index i can be run before the job at index j.\n        :rtype: list\n        \"\"\"\n        ordering = []\n        visited = set()\n        def getRunOrder(job):\n            #Do not add the job to the ordering until all its predecessors have been\n            #added to the ordering\n            for p in job._directPredecessors:\n                if p not in visited:\n                    return\n            if job not in visited:\n                visited.add(job)\n                ordering.append(job)\n                list(map(getRunOrder, job._children + job._followOns))\n        getRunOrder(self)\n        return ordering"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nserialise a job into the jobStore.", "response": "def _serialiseJob(self, jobStore, jobsToJobGraphs, rootJobGraph):\n        \"\"\"\n        Pickle a job and its jobGraph to disk.\n        \"\"\"\n        # Pickle the job so that its run method can be run at a later time.\n        # Drop out the children/followOns/predecessors/services - which are\n        # all recorded within the jobStore and do not need to be stored within\n        # the job\n        self._children, self._followOns, self._services = [], [], []\n        self._directPredecessors, self._promiseJobStore = set(), None\n        # The pickled job is \"run\" as the command of the job, see worker\n        # for the mechanism which unpickles the job and executes the Job.run\n        # method.\n        with jobStore.writeFileStream(rootJobGraph.jobStoreID) as (fileHandle, fileStoreID):\n            pickle.dump(self, fileHandle, pickle.HIGHEST_PROTOCOL)\n        # Note that getUserScript() may have been overridden. This is intended. If we used\n        # self.userModule directly, we'd be getting a reference to job.py if the job was\n        # specified as a function (as opposed to a class) since that is where FunctionWrappingJob\n        #  is defined. What we really want is the module that was loaded as __main__,\n        # and FunctionWrappingJob overrides getUserScript() to give us just that. Only then can\n        # filter_main() in _unpickle( ) do its job of resolving any user-defined type or function.\n        userScript = self.getUserScript().globalize()\n        jobsToJobGraphs[self].command = ' '.join(('_toil', fileStoreID) + userScript.toCommand())\n        #Update the status of the jobGraph on disk\n        jobStore.update(jobsToJobGraphs[self])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _serialiseServices(self, jobStore, jobGraph, rootJobGraph):\n        def processService(serviceJob, depth):\n            # Extend the depth of the services if necessary\n            if depth == len(jobGraph.services):\n                jobGraph.services.append([])\n\n            # Recursively call to process child services\n            for childServiceJob in serviceJob.service._childServices:\n                processService(childServiceJob, depth+1)\n\n            # Make a job wrapper\n            serviceJobGraph = serviceJob._createEmptyJobGraphForJob(jobStore, predecessorNumber=1)\n\n            # Create the start and terminate flags\n            serviceJobGraph.startJobStoreID = jobStore.getEmptyFileStoreID()\n            serviceJobGraph.terminateJobStoreID = jobStore.getEmptyFileStoreID()\n            serviceJobGraph.errorJobStoreID = jobStore.getEmptyFileStoreID()\n            assert jobStore.fileExists(serviceJobGraph.startJobStoreID)\n            assert jobStore.fileExists(serviceJobGraph.terminateJobStoreID)\n            assert jobStore.fileExists(serviceJobGraph.errorJobStoreID)\n\n            # Create the service job tuple\n            j = ServiceJobNode(jobStoreID=serviceJobGraph.jobStoreID,\n                               memory=serviceJobGraph.memory, cores=serviceJobGraph.cores,\n                               disk=serviceJobGraph.disk, preemptable=serviceJobGraph.preemptable,\n                               startJobStoreID=serviceJobGraph.startJobStoreID,\n                               terminateJobStoreID=serviceJobGraph.terminateJobStoreID,\n                               errorJobStoreID=serviceJobGraph.errorJobStoreID,\n                               jobName=serviceJobGraph.jobName, unitName=serviceJobGraph.unitName,\n                               command=serviceJobGraph.command,\n                               predecessorNumber=serviceJobGraph.predecessorNumber)\n\n            # Add the service job tuple to the list of services to run\n            jobGraph.services[depth].append(j)\n\n            # Break the links between the services to stop them being serialised together\n            #childServices = serviceJob.service._childServices\n            serviceJob.service._childServices = None\n            assert serviceJob._services == []\n            #service = serviceJob.service\n\n            # Pickle the job\n            serviceJob.pickledService = pickle.dumps(serviceJob.service, protocol=pickle.HIGHEST_PROTOCOL)\n            serviceJob.service = None\n\n            # Serialise the service job and job wrapper\n            serviceJob._serialiseJob(jobStore, { serviceJob:serviceJobGraph }, rootJobGraph)\n\n            # Restore values\n            #serviceJob.service = service\n            #serviceJob.service._childServices = childServices\n\n        for serviceJob in self._services:\n            processService(serviceJob, 0)\n\n        self._services = []", "response": "Serialises the services for a job."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _serialiseJobGraph(self, jobGraph, jobStore, returnValues, firstJob):\n        #Check if the job graph has created\n        #any cycles of dependencies or has multiple roots\n        self.checkJobGraphForDeadlocks()\n\n        #Create the jobGraphs for followOns/children\n        with jobStore.batch():\n            jobsToJobGraphs = self._makeJobGraphs(jobGraph, jobStore)\n        #Get an ordering on the jobs which we use for pickling the jobs in the\n        #correct order to ensure the promises are properly established\n        ordering = self.getTopologicalOrderingOfJobs()\n        assert len(ordering) == len(jobsToJobGraphs)\n\n        with jobStore.batch():\n            # Temporarily set the jobStore locators for the promise call back functions\n            for job in ordering:\n                job.prepareForPromiseRegistration(jobStore)\n                def setForServices(serviceJob):\n                    serviceJob.prepareForPromiseRegistration(jobStore)\n                    for childServiceJob in serviceJob.service._childServices:\n                        setForServices(childServiceJob)\n                for serviceJob in job._services:\n                    setForServices(serviceJob)\n\n            ordering.reverse()\n            assert self == ordering[-1]\n            if firstJob:\n                #If the first job we serialise all the jobs, including the root job\n                for job in ordering:\n                    # Pickle the services for the job\n                    job._serialiseServices(jobStore, jobsToJobGraphs[job], jobGraph)\n                    # Now pickle the job\n                    job._serialiseJob(jobStore, jobsToJobGraphs, jobGraph)\n            else:\n                #We store the return values at this point, because if a return value\n                #is a promise from another job, we need to register the promise\n                #before we serialise the other jobs\n                self._fulfillPromises(returnValues, jobStore)\n                #Pickle the non-root jobs\n                for job in ordering[:-1]:\n                    # Pickle the services for the job\n                    job._serialiseServices(jobStore, jobsToJobGraphs[job], jobGraph)\n                    # Pickle the job itself\n                    job._serialiseJob(jobStore, jobsToJobGraphs, jobGraph)\n                # Pickle any services for the job\n                self._serialiseServices(jobStore, jobGraph, jobGraph)", "response": "Serialise the jobGraph into the jobStore."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _serialiseFirstJob(self, jobStore):\n\n        # Check if the workflow root is a checkpoint but not a leaf vertex.\n        # All other job vertices in the graph are checked by checkNewCheckpointsAreLeafVertices\n        if self.checkpoint and not Job._isLeafVertex(self):\n            raise JobGraphDeadlockException(\n                'New checkpoint job %s is not a leaf in the job graph' % self)\n\n        # Create first jobGraph\n        jobGraph = self._createEmptyJobGraphForJob(jobStore=jobStore, predecessorNumber=0)\n        # Write the graph of jobs to disk\n        self._serialiseJobGraph(jobGraph, jobStore, None, True)\n        jobStore.update(jobGraph)\n        # Store the name of the first job in a file in case of restart. Up to this point the\n        # root job is not recoverable. FIXME: \"root job\" or \"first job\", which one is it?\n        jobStore.setRootJob(jobGraph.jobStoreID)\n        return jobGraph", "response": "Serialises the first job in the job store. Returns the wrapping job."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nserialise an existing job.", "response": "def _serialiseExistingJob(self, jobGraph, jobStore, returnValues):\n        \"\"\"\n        Serialise an existing job.\n        \"\"\"\n        self._serialiseJobGraph(jobGraph, jobStore, returnValues, False)\n        #Drop the completed command, if not dropped already\n        jobGraph.command = None\n        #Merge any children (follow-ons) created in the initial serialisation\n        #with children (follow-ons) created in the subsequent scale-up.\n        assert len(jobGraph.stack) >= 4\n        combinedChildren = jobGraph.stack[-1] + jobGraph.stack[-3]\n        combinedFollowOns = jobGraph.stack[-2] + jobGraph.stack[-4]\n        jobGraph.stack = jobGraph.stack[:-4]\n        if len(combinedFollowOns) > 0:\n            jobGraph.stack.append(combinedFollowOns)\n        if len(combinedChildren) > 0:\n            jobGraph.stack.append(combinedChildren)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate an encapsulated Toil job function with unfulfilled promised resource requirements.", "response": "def create(cls, userFunction, *args, **kwargs):\n        \"\"\"\n        Creates an encapsulated Toil job function with unfulfilled promised resource\n        requirements. After the promises are fulfilled, a child job function is created\n        using updated resource values. The subgraph is encapsulated to ensure that this\n        child job function is run before other children in the workflow. Otherwise, a\n        different child may try to use an unresolved promise return value from the parent.\n        \"\"\"\n        return EncapsulatedJob(cls(userFunction, *args, **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the value of the key.", "response": "def getValue(self):\n        \"\"\"\n        Returns PromisedRequirement value\n        \"\"\"\n        func = dill.loads(self._func)\n        return func(*self._args)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if reserved resource keyword is a Promise or PromisedRequirement instance. Converts Promise instance to PromisedRequirement instance.", "response": "def convertPromises(kwargs):\n        \"\"\"\n        Returns True if reserved resource keyword is a Promise or\n        PromisedRequirement instance. Converts Promise instance\n        to PromisedRequirement.\n\n        :param kwargs: function keyword arguments\n        :return: bool\n        \"\"\"\n        for r in [\"disk\", \"memory\", \"cores\"]:\n            if isinstance(kwargs.get(r), Promise):\n                kwargs[r] = PromisedRequirement(kwargs[r])\n                return True\n            elif isinstance(kwargs.get(r), PromisedRequirement):\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getPublicIP():\n    try:\n        # Try to get the internet-facing IP by attempting a connection\n        # to a non-existent server and reading what IP was used.\n        with closing(socket.socket(socket.AF_INET, socket.SOCK_DGRAM)) as sock:\n            # 203.0.113.0/24 is reserved as TEST-NET-3 by RFC 5737, so\n            # there is guaranteed to be no one listening on the other\n            # end (and we won't accidentally DOS anyone).\n            sock.connect(('203.0.113.1', 1))\n            ip = sock.getsockname()[0]\n        return ip\n    except:\n        # Something went terribly wrong. Just give loopback rather\n        # than killing everything, because this is often called just\n        # to provide a default argument\n        return '127.0.0.1'", "response": "Get the IP that this computer uses to contact the internet."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setDefaultOptions(config):\n    config.batchSystem = \"singleMachine\"\n    config.disableAutoDeployment = False\n    config.environment = {}\n    config.statePollingWait = None  # if not set, will default to seconds in getWaitDuration()\n    config.maxLocalJobs = multiprocessing.cpu_count()\n    config.manualMemArgs = False\n\n    # single machine\n    config.scale = 1\n    config.linkImports = False\n\n    # mesos\n    config.mesosMasterAddress = '%s:5050' % getPublicIP()\n\n    # parasol\n    config.parasolCommand = 'parasol'\n    config.parasolMaxBatches = 10000", "response": "Sets the default options for builtin batch systems."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef googleRetryPredicate(e):\n    if isinstance(e, GoogleAPICallError) and e.code == 429:\n        return True\n    if isinstance(e, InternalServerError) or isinstance(e, ServiceUnavailable):\n        return True\n    return False", "response": "Returns True if the Google API call error is too long."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef googleRetry(f):\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        for attempt in retry(delays=truncExpBackoff(),\n                             timeout=300,\n                             predicate=googleRetryPredicate):\n            with attempt:\n                return f(*args, **kwargs)\n    return wrapper", "response": "A decorator that retries the given function if google throws any angry service\n    errors."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the blob specified by the url.", "response": "def _getBlobFromURL(cls, url, exists=False):\n        \"\"\"\n        Gets the blob specified by the url.\n\n        caution: makes no api request. blob may not ACTUALLY exist\n\n        :param urlparse.ParseResult url: the URL\n\n        :param bool exists: if True, then syncs local blob object with cloud\n        and raises exceptions if it doesn't exist remotely\n\n        :return: the blob requested\n        :rtype: :class:`~google.cloud.storage.blob.Blob`\n        \"\"\"\n        bucketName = url.netloc\n        fileName = url.path\n\n        # remove leading '/', which can cause problems if fileName is a path\n        if fileName.startswith('/'):\n            fileName = fileName[1:]\n\n        storageClient = storage.Client()\n        bucket = storageClient.get_bucket(bucketName)\n        blob = bucket.blob(bytes(fileName))\n\n        if exists:\n            if not blob.exists():\n                raise NoSuchFileException\n            # sync with cloud so info like size is available\n            blob.reload()\n        return blob"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _readContents(self, jobStoreID):\n        job = self.bucket.get_blob(bytes(jobStoreID), encryption_key=self.sseKey)\n        if job is None:\n            raise NoSuchJobException(jobStoreID)\n        return job.download_as_string()", "response": "Reads the contents of the file containing the job store."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nyields a context manager that can be used to upload a file to the bucket.", "response": "def _uploadStream(self, fileName, update=False, encrypt=True):\n        \"\"\"\n        Yields a context manager that can be used to write to the bucket\n        with a stream. See :class:`~toil.jobStores.utils.WritablePipe` for an example.\n\n        Will throw assertion error if the file shouldn't be updated\n        and yet exists.\n\n        :param fileName: name of file to be inserted into bucket\n        :type fileName: str\n        :param update: whether or not the file is to be updated\n        :type update: bool\n        :param encrypt: whether or not the file is encrypted\n        :type encrypt: bool\n        :return: an instance of WritablePipe.\n        :rtype: :class:`~toil.jobStores.utils.writablePipe`\n        \"\"\"\n        blob = self.bucket.blob(bytes(fileName), encryption_key=self.sseKey if encrypt else None)\n\n        class UploadPipe(WritablePipe):\n            def readFrom(self, readable):\n                if not update:\n                    assert not blob.exists()\n                blob.upload_from_file(readable)\n\n        with UploadPipe() as writable:\n            yield writable"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _downloadStream(self, fileName, encrypt=True):\n        blob = self.bucket.get_blob(bytes(fileName), encryption_key=self.sseKey if encrypt else None)\n        if blob is None:\n            raise NoSuchFileException(fileName)\n\n        class DownloadPipe(ReadablePipe):\n            def writeTo(self, writable):\n                try:\n                    blob.download_to_file(writable)\n                finally:\n                    writable.close()\n\n        with DownloadPipe() as readable:\n            yield readable", "response": "Yields a context manager that can be used to read from the bucket with a stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the mean value of a sequence of values.", "response": "def mean(xs):\n    \"\"\"\n    Return the mean value of a sequence of values.\n\n    >>> mean([2,4,4,4,5,5,7,9])\n    5.0\n    >>> mean([9,10,11,7,13])\n    10.0\n    >>> mean([1,1,10,19,19])\n    10.0\n    >>> mean([10,10,10,10,10])\n    10.0\n    >>> mean([1,\"b\"])\n    Traceback (most recent call last):\n      ...\n    ValueError: Input can't have non-numeric elements\n    >>> mean([])\n    Traceback (most recent call last):\n      ...\n    ValueError: Input can't be empty\n    \"\"\"\n    try:\n        return sum(xs) / float(len(xs))\n    except TypeError:\n        raise ValueError(\"Input can't have non-numeric elements\")\n    except ZeroDivisionError:\n        raise ValueError(\"Input can't be empty\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef std_dev(xs):\n    m = mean(xs)  # this checks our pre-conditions, too\n    return sqrt(sum((x - m) ** 2 for x in xs) / float(len(xs)))", "response": "Returns the standard deviation of the given iterable of numbers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsplit a sequence into a list of subsequences of the given size.", "response": "def partition_seq(seq, size):\n    \"\"\"\n    Splits a sequence into an iterable of subsequences. All subsequences are of the given size,\n    except the last one, which may be smaller. If the input list is modified while the returned\n    list is processed, the behavior of the program is undefined.\n\n    :param seq: the list to split\n    :param size: the desired size of the sublists, must be > 0\n    :type size: int\n    :return: an iterable of sublists\n\n    >>> list(partition_seq(\"\",1))\n    []\n    >>> list(partition_seq(\"abcde\",2))\n    ['ab', 'cd', 'e']\n    >>> list(partition_seq(\"abcd\",2))\n    ['ab', 'cd']\n    >>> list(partition_seq(\"abcde\",1))\n    ['a', 'b', 'c', 'd', 'e']\n    >>> list(partition_seq(\"abcde\",0))\n    Traceback (most recent call last):\n    ...\n    ValueError: Size must be greater than 0\n    >>> l=[1,2,3,4]\n    >>> i = iter( partition_seq(l,2) )\n    >>> l.pop(0)\n    1\n    >>> next(i)\n    [2, 3]\n    \"\"\"\n    if size < 1:\n        raise ValueError('Size must be greater than 0')\n    return (seq[pos:pos + size] for pos in xrange(0, len(seq), size))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntests whether FILENAME matches PATTERN.", "response": "def fnmatch( name, pat ):\n    \"\"\"Test whether FILENAME matches PATTERN.\n\n    Patterns are Unix shell style:\n\n    *       matches everything\n    ?       matches any single character\n    [seq]   matches any character in seq\n    [!seq]  matches any char not in seq\n\n    An initial period in FILENAME is not special.\n    Both FILENAME and PATTERN are first case-normalized\n    if the operating system requires it.\n    If you don't want this, use fnmatchcase(FILENAME, PATTERN).\n\n    >>> fnmatch('bar', '*' )\n    True\n    >>> fnmatch('foo/bar', '*' )\n    False\n    >>> fnmatch('foo/bar', '**' )\n    True\n    >>> fnmatch('foo/bar', '*/*' )\n    True\n    >>> fnmatch('foo/bar', '**/*' )\n    True\n    >>> fnmatch('/bar', '**/*' )\n    True\n    >>> fnmatch('/', '**' )\n    True\n    >>> fnmatch('/', '*' )\n    False\n    \"\"\"\n\n    import os\n\n    name = os.path.normcase( name )\n    pat = os.path.normcase( pat )\n    return fnmatchcase( name, pat )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the subset of the list NAMES that match PAT", "response": "def filter( names, pat ):\n    \"\"\"Return the subset of the list NAMES that match PAT\"\"\"\n    import os, posixpath\n\n    result = [ ]\n    pat = os.path.normcase( pat )\n    if not pat in _cache:\n        res = translate( pat )\n        if len( _cache ) >= _MAXCACHE:\n            _cache.clear( )\n        _cache[ pat ] = re.compile( res )\n    match = _cache[ pat ].match\n    if os.path is posixpath:\n        # normcase on posix is NOP. Optimize it away from the loop.\n        for name in names:\n            if match( name ):\n                result.append( name )\n    else:\n        for name in names:\n            if match( os.path.normcase( name ) ):\n                result.append( name )\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the current instance s meta - data to get the cluster settings.", "response": "def _readClusterSettings(self):\n        \"\"\"\n        Read the current instance's meta-data to get the cluster settings.\n        \"\"\"\n        # get the leader metadata\n        mdUrl = \"http://169.254.169.254/metadata/instance?api-version=2017-08-01\"\n        header = {'Metadata': 'True'}\n        request = urllib.request.Request(url=mdUrl, headers=header)\n        response = urllib.request.urlopen(request)\n        data = response.read()\n        dataStr = data.decode(\"utf-8\")\n        metadata = json.loads(dataStr)\n\n        # set values from the leader meta-data\n        self._zone = metadata['compute']['location']\n        self.clusterName = metadata['compute']['resourceGroupName']\n        tagsStr = metadata['compute']['tags']\n        tags = dict(item.split(\":\") for item in tagsStr.split(\";\"))\n        self._owner = tags.get('owner', 'no-owner')\n        leader = self.getLeader()\n        self._leaderPrivateIP = leader.privateIP\n        self._setSSH()  # create id_rsa.pub file on the leader if it is not there\n        self._masterPublicKeyFile =  self.LEADER_HOME_DIR + '.ssh/id_rsa.pub'\n\n        # Add static nodes to /etc/hosts since Azure sometimes fails to find them with DNS\n        map(lambda x: self._addToHosts(x), self.getProvisionedWorkers(None))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlaunches an Azure cluster using Ansible.", "response": "def launchCluster(self, leaderNodeType, leaderStorage, owner, **kwargs):\n        \"\"\"\n        Launches an Azure cluster using Ansible.\n        A resource group is created for the cluster. All the virtual machines are created within this\n        resource group.\n\n        Cloud-config is called during vm creation to create directories and launch the appliance.\n\n        The azureStorageCredentials must be passed in kwargs. These credentials allow access to\n        Azure jobStores.\n        \"\"\"\n        self._owner = owner\n        self._masterPublicKeyFile = kwargs['publicKeyFile']\n        if not self._masterPublicKeyFile:\n            raise RuntimeError(\"The Azure provisioner requires a public key file.\")\n        storageCredentials = kwargs['azureStorageCredentials']\n        if not storageCredentials:\n            raise RuntimeError(\"azureStorageCredentials must be given.\")\n\n        self._checkValidClusterName()\n        self._checkIfClusterExists()\n\n        # Create the cluster.\n        clusterArgs = {\n            'resgrp': self.clusterName,  # The resource group, which represents the cluster.\n            'region': self._zone\n        }\n        self.callPlaybook(self.playbook['create-cluster'], clusterArgs, wait=True)\n\n        ansibleArgs = {\n            'vmsize': leaderNodeType,\n            'resgrp': self.clusterName,  # The resource group, which represents the cluster.\n            'region': self._zone,\n            'role': \"leader\",\n            'owner': self._owner,  # Just a tag.\n            'diskSize': str(leaderStorage),  # TODO: not implemented\n            'publickeyfile': self._masterPublicKeyFile   # The users public key to be added to authorized_keys\n        }\n        # Ansible reads the cloud-config script from a file.\n        with tempfile.NamedTemporaryFile(delete=False) as t:\n            userData =  self._getCloudConfigUserData('leader')\n            t.write(userData)\n            ansibleArgs['cloudconfig'] = t.name\n\n        # Launch the leader VM.\n        retries = 0\n        while True:\n            instanceName = 'l' + str(uuid.uuid4())\n            ansibleArgs['vmname'] = instanceName\n            # Azure limits the name to 24 characters, no dashes.\n            ansibleArgs['storagename'] = instanceName.replace('-', '')[:24]\n\n            self.callPlaybook(self.playbook['create'], ansibleArgs, wait=True)\n            try:\n                leaderNode = self.getLeader()\n            except IndexError:\n                raise RuntimeError(\"Failed to launcher leader\")\n            self._leaderPrivateIP = leaderNode.privateIP # IP available as soon as the playbook finishes\n\n            try:\n                # Fix for DNS failure.\n                self._addToHosts(leaderNode, leaderNode.publicIP)\n\n                leaderNode.waitForNode('toil_leader') # Make sure leader appliance is up.\n\n                # Transfer credentials\n                if storageCredentials is not None:\n                    fullPathCredentials = os.path.expanduser(storageCredentials)\n                    if os.path.isfile(fullPathCredentials):\n                        leaderNode.injectFile(fullPathCredentials, self.LEADER_HOME_DIR, 'toil_leader')\n                ansibleCredentials = '.azure/credentials'\n                fullPathAnsibleCredentials = os.path.expanduser('~/' + ansibleCredentials)\n                if os.path.isfile(fullPathAnsibleCredentials):\n                    leaderNode.sshAppliance('mkdir', '-p', self.LEADER_HOME_DIR + '.azure')\n                    leaderNode.injectFile(fullPathAnsibleCredentials,\n                                          self.LEADER_HOME_DIR + ansibleCredentials,\n                                          'toil_leader')\n                break #success!\n            except RuntimeError as e:\n                self._terminateNode(instanceName, False) # remove failed leader\n                retries += 1\n                if retries == 3:\n                    logger.debug(\"Leader appliance failed to start. Giving up.\")\n                    raise e\n                logger.debug(\"Leader appliance failed to start, retrying. (Error %s)\" % e)\n\n\n        logger.debug('Launched leader')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if the resource group exists and delete it if it exists.", "response": "def _checkIfClusterExists(self):\n        \"\"\"\n        Try deleting the resource group. This will fail if it exists and raise an exception.\n        \"\"\"\n        ansibleArgs = {\n            'resgrp': self.clusterName,\n            'region': self._zone\n        }\n        try:\n            self.callPlaybook(self.playbook['check-cluster'], ansibleArgs, wait=True)\n        except RuntimeError:\n            logger.info(\"The cluster could not be created. Try deleting the cluster if it already exits.\")\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds an privateIP hostname line to the hosts file.", "response": "def _addToHosts(self, node, destinationIP=None):\n        \"\"\"\n        Add an \"privateIP hostname\" line to the /etc/hosts file. If destinationIP is given,\n        do this on the remote machine.\n\n        Azure VMs sometimes fail to initialize, causing the appliance to fail.\n        This error is given:\n           Failed to obtain the IP address for 'l7d41a19b-15a6-442c-8ba1-9678a951d824';\n           the DNS service may not be able to resolve it: Name or service not known.\n        This method is a fix.\n\n        :param node: Node to add to /etc/hosts.\n        :param destinationIP: A remote host's address\n        \"\"\"\n        cmd = \"echo %s %s | sudo tee --append /etc/hosts > /dev/null\" % (node.privateIP, node.name)\n        logger.debug(\"Running command %s on %s\" % (cmd, destinationIP))\n        if destinationIP:\n            subprocess.Popen([\"ssh\", \"-oStrictHostKeyChecking=no\", \"core@%s\" % destinationIP, cmd])\n        else:\n            subprocess.Popen(cmd, shell=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _getNodes(self, role=None, nodeType=None):\n        allNodes = self._azureComputeClient.virtual_machines.list(self.clusterName)\n        rv = []\n        allNodeNames = []\n        for node in allNodes:\n            allNodeNames.append(node.name)\n            nodeRole = node.tags.get('role', None)\n            if node.provisioning_state != 'Succeeded' or (role is not None and nodeRole != role):\n                continue\n            if nodeType and node.hardware_profile.vm_size != nodeType:\n                continue\n\n            network_interface = self._azureNetworkClient.network_interfaces.get(self.clusterName, node.name)\n            if not network_interface.ip_configurations:\n                continue # no networks assigned to this node\n            publicIP = self._azureNetworkClient.public_ip_addresses.get(self.clusterName, node.name)\n            rv.append(Node(\n                publicIP=publicIP.ip_address,\n                privateIP=network_interface.ip_configurations[0].private_ip_address,\n                name=node.name,\n                launchTime=None,  # Not used with Azure.\n                nodeType=node.hardware_profile.vm_size,\n                preemptable=False) # Azure doesn't have preemptable nodes\n            )\n        logger.debug('All nodes in cluster: ' + ', '.join(allNodeNames))\n        return rv", "response": "Returns a list of Node objects representing the instances in the cluster with the given role and nodeType."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _checkValidClusterName(self):\n        p = re.compile('^[a-zA-Z0-9][a-zA-Z0-9_.\\-]*[a-zA-Z0-9]$')\n        if len(self.clusterName) < 2 or len(self.clusterName) > 64 or not p.match(self.clusterName):\n            raise RuntimeError(\"Invalid cluster name (%s).\"\n                                \" It must be between 2 and 64 characters and contain only alpha-numeric\"\n                                \" characters, hyphens, underscores, and periods. It must start and\"\n                                \" end only with alpha-numeric characters.\" % self.clusterName)", "response": "Check that the cluster name is valid."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encrypt(message, keyPath):\n    with open(keyPath, 'rb') as f:\n        key = f.read()\n    if len(key) != SecretBox.KEY_SIZE:\n        raise ValueError(\"Key is %d bytes, but must be exactly %d bytes\" % (len(key),\n                                                                            SecretBox.KEY_SIZE))\n    sb = SecretBox(key)\n    # We generate the nonce using secure random bits. For long enough\n    # nonce size, the chance of a random nonce collision becomes\n    # *much* smaller than the chance of a subtle coding error causing\n    # a nonce reuse. Currently the nonce size is 192 bits--the chance\n    # of a collision is astronomically low. (This approach is\n    # recommended in the libsodium documentation.)\n    nonce = nacl.utils.random(SecretBox.NONCE_SIZE)\n    assert len(nonce) == SecretBox.NONCE_SIZE\n    return bytes(sb.encrypt(message, nonce))", "response": "Encrypts a message given a path to a local file containing a 256 - bit key."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlikes memoize, but guarantees that decorated function is only called once, even when multiple threads are calling the decorating function with multiple parameters.", "response": "def sync_memoize(f):\n    \"\"\"\n    Like memoize, but guarantees that decorated function is only called once, even when multiple\n    threads are calling the decorating function with multiple parameters.\n    \"\"\"\n    # TODO: Think about an f that is recursive\n    memory = {}\n    lock = Lock()\n\n    @wraps(f)\n    def new_f(*args):\n        try:\n            return memory[args]\n        except KeyError:\n            # on cache misses, retry with lock held\n            with lock:\n                try:\n                    return memory[args]\n                except KeyError:\n                    r = f(*args)\n                    memory[args] = r\n                    return r\n    return new_f"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef properties(obj):\n    return dict((attr, getattr(obj, attr)) for attr in dir(obj) if not attr.startswith('__'))", "response": "Returns a dictionary with one entry per attribute of the given object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses an ISO - formatted datetime in UTC.", "response": "def parse_iso_utc(s):\n    \"\"\"\n    Parses an ISO time with a hard-coded Z for zulu-time (UTC) at the end. Other timezones are\n    not supported.\n\n    :param str s: the ISO-formatted time\n\n    :rtype: datetime.datetime\n\n    :return: an timezone-naive datetime object\n\n    >>> parse_iso_utc('2016-04-27T00:28:04.000Z')\n    datetime.datetime(2016, 4, 27, 0, 28, 4)\n    >>> parse_iso_utc('2016-04-27T00:28:04Z')\n    datetime.datetime(2016, 4, 27, 0, 28, 4)\n    >>> parse_iso_utc('2016-04-27T00:28:04X')\n    Traceback (most recent call last):\n    ...\n    ValueError: Not a valid ISO datetime in UTC: 2016-04-27T00:28:04X\n    \"\"\"\n    m = rfc3339_datetime_re().match(s)\n    if not m:\n        raise ValueError( 'Not a valid ISO datetime in UTC: ' + s )\n    else:\n        fmt = '%Y-%m-%dT%H:%M:%S' + ('.%f' if m.group(7) else '') + 'Z'\n        return datetime.datetime.strptime(s, fmt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef less_strict_bool(x):\n    if x is None:\n        return False\n    elif x is True or x is False:\n        return x\n    else:\n        return strict_bool(x)", "response": "Idempotent and None - safe version of strict_bool."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting up the sort.", "response": "def setup(job, input_file_id, n, down_checkpoints):\n    \"\"\"Sets up the sort.\n    Returns the FileID of the sorted file\n    \"\"\"\n    # Write the input file to the file store\n    job.fileStore.logToMaster(\"Starting the merge sort\")\n    return job.addChildJobFn(down,\n                             input_file_id, n,\n                             down_checkpoints=down_checkpoints,\n                             memory='600M').rv()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninputting is a file and a range into that file to sort and an output location in which to write the sorted file. If the range is larger than a threshold N the range is divided recursively and a follow on job is then created which merges back the results. Otherwise, the file is sorted and placed in the output.", "response": "def down(job, input_file_id, n, down_checkpoints):\n    \"\"\"Input is a file and a range into that file to sort and an output location in which\n    to write the sorted file.\n    If the range is larger than a threshold N the range is divided recursively and\n    a follow on job is then created which merges back the results. Otherwise,\n    the file is sorted and placed in the output.\n    \"\"\"\n    # Read the file\n    input_file = job.fileStore.readGlobalFile(input_file_id, cache=False)\n    length = os.path.getsize(input_file)\n    if length > n:\n        # We will subdivide the file\n        job.fileStore.logToMaster(\"Splitting file: %s of size: %s\"\n                                  % (input_file_id, length), level=logging.CRITICAL)\n        # Split the file into two copies\n        mid_point = get_midpoint(input_file, 0, length)\n        t1 = job.fileStore.getLocalTempFile()\n        with open(t1, 'w') as fH:\n            copy_subrange_of_file(input_file, 0, mid_point + 1, fH)\n        t2 = job.fileStore.getLocalTempFile()\n        with open(t2, 'w') as fH:\n            copy_subrange_of_file(input_file, mid_point + 1, length, fH)\n\n        # Call the down function recursively\n        return job.addFollowOnJobFn(up, job.addChildJobFn(down, job.fileStore.writeGlobalFile(t1), n,\n                                    down_checkpoints=down_checkpoints, memory='600M').rv(),\n                                    job.addChildJobFn(down, job.fileStore.writeGlobalFile(t2), n,\n                                                      down_checkpoints=down_checkpoints,\n                                                      memory='600M').rv()).rv()\n    else:\n        # We can sort this bit of the file\n        job.fileStore.logToMaster(\"Sorting file: %s of size: %s\"\n                                  % (input_file_id, length), level=logging.CRITICAL)\n        # Sort the copy and write back to the fileStore\n        output_file = job.fileStore.getLocalTempFile()\n        sort(input_file, output_file)\n        return job.fileStore.writeGlobalFile(output_file)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef up(job, input_file_id_1, input_file_id_2):\n    with job.fileStore.writeGlobalFileStream() as (fileHandle, output_id):\n        with job.fileStore.readGlobalFileStream(input_file_id_1) as inputFileHandle1:\n            with job.fileStore.readGlobalFileStream(input_file_id_2) as inputFileHandle2:\n                job.fileStore.logToMaster(\"Merging %s and %s to %s\"\n                                          % (input_file_id_1, input_file_id_2, output_id))\n                merge(inputFileHandle1, inputFileHandle2, fileHandle)\n\n        # Cleanup up the input files - these deletes will occur after the completion is successful.\n        job.fileStore.deleteGlobalFile(input_file_id_1)\n        job.fileStore.deleteGlobalFile(input_file_id_2)\n        return output_id", "response": "Merges two files and places them in the output."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsorting the given file.", "response": "def sort(in_file, out_file):\n    \"\"\"Sorts the given file.\n    \"\"\"\n    filehandle = open(in_file, 'r')\n    lines = filehandle.readlines()\n    filehandle.close()\n    lines.sort()\n    filehandle = open(out_file, 'w')\n    for line in lines:\n        filehandle.write(line)\n    filehandle.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmerging together two files maintaining sorted order.", "response": "def merge(filehandle_1, filehandle_2, output_filehandle):\n    \"\"\"Merges together two files maintaining sorted order.\n    \"\"\"\n    line2 = filehandle_2.readline()\n    for line1 in filehandle_1.readlines():\n        while line2 != '' and line2 <= line1:\n            output_filehandle.write(line2)\n            line2 = filehandle_2.readline()\n        output_filehandle.write(line1)\n    while line2 != '':\n        output_filehandle.write(line2)\n        line2 = filehandle_2.readline()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncopies the range between fileStart and fileEnd to the given output_filehandle.", "response": "def copy_subrange_of_file(input_file, file_start, file_end, output_filehandle):\n    \"\"\"Copies the range (in bytes) between fileStart and fileEnd to the given\n    output file handle.\n    \"\"\"\n    with open(input_file, 'r') as fileHandle:\n        fileHandle.seek(file_start)\n        data = fileHandle.read(file_end - file_start)\n        assert len(data) == file_end - file_start\n        output_filehandle.write(data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind the point in the file to split.", "response": "def get_midpoint(file, file_start, file_end):\n    \"\"\"Finds the point in the file to split.\n    Returns an int i such that fileStart <= i < fileEnd\n    \"\"\"\n    filehandle = open(file, 'r')\n    mid_point = (file_start + file_end) / 2\n    assert mid_point >= file_start\n    filehandle.seek(mid_point)\n    line = filehandle.readline()\n    assert len(line) >= 1\n    if len(line) + mid_point < file_end:\n        return mid_point + len(line) - 1\n    filehandle.seek(file_start)\n    line = filehandle.readline()\n    assert len(line) >= 1\n    assert len(line) + file_start <= file_end\n    return len(line) + file_start - 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nspins - wait and block for a file to appear before returning False if it does not exist.", "response": "def waitForExists(self, jobStoreID, maxTries=35, sleepTime=1):\n        \"\"\"Spin-wait and block for a file to appear before returning False if it does not.\n        \n        The total max wait time is maxTries * sleepTime. The current default is\n        tuned to match Linux NFS defaults where the client's cache of the directory\n        listing on the server is supposed to become coherent within 30 sec.\n        Delayes beyond that would probably indicate a pathologically slow file system\n        that just should not be used for the jobStore.\n        \n        The warning will be sent to the log only on the first retry.\n        \n        In practice, the need for retries happens rarely, but it does happen\n        over the course of large workflows with a jobStore on a busy NFS.\"\"\"\n        for iTry in range(1,maxTries+1):\n            jobFile = self._getJobFileName(jobStoreID)\n            if os.path.exists(jobFile):\n                return True\n            if iTry >= maxTries:\n                return False\n            elif iTry == 1:\n                logger.warning((\"Job file `{}` for job `{}` does not exist (yet). We will try #{} more times with {}s \"\n                        \"intervals.\").format(jobFile, jobStoreID, maxTries - iTry, sleepTime))\n            time.sleep(sleepTime)\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _readFromUrl(cls, url, writable):\n        # we use a ~10Mb buffer to improve speed\n        with open(cls._extractPathFromUrl(url), 'rb') as readable:\n            shutil.copyfileobj(readable, writable, length=cls.BUFFER_SIZE)", "response": "Reads the contents of a file from a given url and writes it to writable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract the local file path of the given URL.", "response": "def _extractPathFromUrl(url):\n        \"\"\"\n        :return: local file path of file pointed at by the given URL\n        \"\"\"\n        if url.netloc != '' and url.netloc != 'localhost':\n            raise RuntimeError(\"The URL '%s' is invalid\" % url.geturl())\n        return url.netloc + url.path"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the name of the function that is used by the user code.", "response": "def _getUserCodeFunctionName(self):\n        \"\"\"\n        Get the name of the function 4 levels up the stack (above this\n        function, our caller, and whatever Toil code delegated to the JobStore\n        implementation). Returns a string usable in a filename, and returns a\n        placeholder string if the function name is unsuitable or can't be\n        gotten.\n        \"\"\"\n        \n        # Record the name of the job/function writing the file in the file name\n        try:\n            # It ought to be fourth-to-last on the stack, above us, the write\n            # function, and the FileStore or context manager. Probably.\n            sourceFunctionName = traceback.extract_stack()[-4][2]\n        except:\n            sourceFunctionName = \"UNKNOWNJOB\"\n            # make sure the function name fetched has no spaces or oddities\n        if not re.match(\"^[A-Za-z0-9_-]*$\", sourceFunctionName):\n            sourceFunctionName = \"ODDLYNAMEDJOB\"\n            \n        return sourceFunctionName"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _getTempSharedDir(self):\n        tempDir = self.tempFilesDir\n        for i in range(self.levels):\n            tempDir = os.path.join(tempDir, random.choice(self.validDirs))\n            if not os.path.exists(tempDir):\n                try:\n                    os.mkdir(tempDir)\n                except os.error:\n                    if not os.path.exists(tempDir): # In the case that a collision occurs and\n                        # it is created while we wait then we ignore\n                        raise\n        return tempDir", "response": "Gets a temporary directory in the hierarchy of directories in self. tempFilesDir."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a unique file name within a jobStore directory or a tmp directory.", "response": "def _getUniqueName(self, fileName, jobStoreID=None, sourceFunctionName=\"x\"):\n        \"\"\"\n        Create unique file name within a jobStore directory or tmp directory.\n\n        :param fileName: A file name, which can be a full path as only the\n        basename will be used.\n        :param jobStoreID: If given, the path returned will be in the jobStore directory.\n        Otherwise, the tmp directory will be used.\n        :param sourceFunctionName: This name is the name of the function that\n            generated this file.  Defaults to x if that name was not a normal\n            name.  Used for tracking files.\n        :return: The full path with a unique file name.\n        \"\"\"\n        fd, absPath = self._getTempFile(jobStoreID)\n        os.close(fd)\n        os.unlink(absPath)\n        # remove the .tmp extension and add the file name\n        (noExt,ext) = os.path.splitext(absPath)\n        uniquePath = noExt + '-' + sourceFunctionName + '-' + os.path.basename(fileName)\n        if os.path.exists(absPath):\n            return absPath  # give up, just return temp name to avoid conflicts\n        return uniquePath"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _getTempFile(self, jobStoreID=None):\n        if jobStoreID != None:\n            # Make a temporary file within the job's directory\n            self._checkJobStoreId(jobStoreID)\n            return tempfile.mkstemp(suffix=\".tmp\",\n                                dir=os.path.join(self._getAbsPath(jobStoreID), \"g\"))\n        else:\n            # Make a temporary file within the temporary file structure\n            return tempfile.mkstemp(prefix=\"tmp\", suffix=\".tmp\", dir=self._getTempSharedDir())", "response": "Returns a temporary file for the given job store ID."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _fetchAzureAccountKey(accountName):\n    try:\n        return os.environ['AZURE_ACCOUNT_KEY_' + accountName]\n    except KeyError:\n        try:\n            return os.environ['AZURE_ACCOUNT_KEY']\n        except KeyError:\n            configParser = RawConfigParser()\n            configParser.read(os.path.expanduser(credential_file_path))\n            try:\n                return configParser.get('AzureStorageCredentials', accountName)\n            except NoOptionError:\n                raise RuntimeError(\"No account key found for '%s', please provide it in '%s'\" %\n                                   (accountName, credential_file_path))", "response": "Fetch the account key for a given Azure storage account."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndefaults retry predicate for AzureExceptions.", "response": "def defaultRetryPredicate(exception):\n    \"\"\"\n    >>> defaultRetryPredicate(socket.error())\n    True\n    >>> defaultRetryPredicate(socket.gaierror())\n    True\n    >>> defaultRetryPredicate(HTTPException())\n    True\n    >>> defaultRetryPredicate(requests.ConnectionError())\n    True\n    >>> defaultRetryPredicate(AzureException('x could not be completed within the specified time'))\n    True\n    >>> defaultRetryPredicate(AzureException('x service unavailable'))\n    True\n    >>> defaultRetryPredicate(AzureException('x server is busy'))\n    True\n    >>> defaultRetryPredicate(AzureException('x'))\n    False\n    >>> defaultRetryPredicate(RuntimeError())\n    False\n    \"\"\"\n    return (isinstance(exception, (socket.error,\n                                   socket.gaierror,\n                                   HTTPException,\n                                   requests.ConnectionError,\n                                   requests.Timeout))\n            or isinstance(exception, AzureException) and\n            any(message in str(exception).lower() for message in (\n                \"could not be completed within the specified time\",\n                \"service unavailable\",\n                \"server is busy\")))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if the job store exists by querying the existence of the statsFileIDs table.", "response": "def _jobStoreExists(self):\n        \"\"\"\n        Checks if job store exists by querying the existence of the statsFileIDs table. Note that\n        this is the last component that is deleted in :meth:`.destroy`.\n        \"\"\"\n        for attempt in retry_azure():\n            with attempt:\n                try:\n                    exists = self.tableService.exists(table_name=self._qualify('statsFileIDs'))\n                except AzureMissingResourceHttpError as e:\n                    if e.status_code == 404:\n                        return False\n                    else:\n                        raise\n                else:\n                    return exists"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parseWasbUrl(cls, url):\n        assert url.scheme in ('wasb', 'wasbs')\n        try:\n            container, account = url.netloc.split('@')\n        except ValueError:\n            raise InvalidImportExportUrlException(url)\n        suffix = '.blob.core.windows.net'\n        if account.endswith(suffix):\n            account = account[:-len(suffix)]\n        else:\n            raise InvalidImportExportUrlException(url)\n        assert url.path[0] == '/'\n        return cls.BlobInfo(account=account, container=container, name=url.path[1:])", "response": "Parses the URL and returns a AzureJobStore. BlobInfo object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _uploadStream(self, jobStoreFileID, container, checkForModification=False, encrypted=None):\n        if checkForModification:\n            try:\n                expectedVersion = container.get_blob_properties(blob_name=str(jobStoreFileID)).properties.etag\n            except AzureMissingResourceHttpError:\n                expectedVersion = None\n\n        if encrypted is None:\n            encrypted = self.keyPath is not None\n        elif encrypted:\n            if self.keyPath is None:\n                raise RuntimeError('Encryption requested but no key was provided')\n\n        maxBlockSize = self._maxAzureBlockBytes\n        if encrypted:\n            # There is a small overhead for encrypted data.\n            maxBlockSize -= encryption.overhead\n\n        store = self\n\n        class UploadPipe(WritablePipe):\n\n            def readFrom(self, readable):\n                blocks = []\n                try:\n                    while True:\n                        buf = readable.read(maxBlockSize)\n                        if len(buf) == 0:\n                            # We're safe to break here even if we never read anything, since\n                            # putting an empty block list creates an empty blob.\n                            break\n                        if encrypted:\n                            buf = encryption.encrypt(buf, store.keyPath)\n                        blockID = store._newFileID()\n                        container.put_block(blob_name=str(jobStoreFileID),\n                                            block=buf,\n                                            block_id=blockID)\n                        blocks.append(BlobBlock(blockID))\n                except:\n                    with panic(log=logger):\n                        # This is guaranteed to delete any uncommitted blocks.\n                        container.delete_blob(blob_name=str(jobStoreFileID))\n\n                if checkForModification and expectedVersion is not None:\n                    # Acquire a (60-second) write lock,\n                    leaseID = container.acquire_blob_lease(blob_name=str(jobStoreFileID),\n                                                           lease_duration=60)\n                    # check for modification,\n                    blob = container.get_blob_properties(blob_name=str(jobStoreFileID))\n                    if blob.properties.etag != expectedVersion:\n                        container.release_blob_lease(blob_name=str(jobStoreFileID), lease_id=leaseID)\n                        raise ConcurrentFileModificationException(jobStoreFileID)\n                    # commit the file,\n                    container.put_block_list(blob_name=str(jobStoreFileID),\n                                             block_list=blocks,\n                                             lease_id=leaseID,\n                                             metadata=dict(encrypted=str(encrypted)))\n                    # then release the lock.\n                    container.release_blob_lease(blob_name=str(jobStoreFileID), lease_id=leaseID)\n                else:\n                    # No need to check for modification, just blindly write over whatever\n                    # was there.\n                    container.put_block_list(blob_name=str(jobStoreFileID),\n                                             block_list=blocks,\n                                             metadata=dict(encrypted=str(encrypted)))\n\n        with UploadPipe() as writable:\n            yield writable", "response": "Upload a stream of data to the specified container."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating an AzureJob from an entity.", "response": "def fromEntity(cls, jobEntity):\n        \"\"\"\n        :type jobEntity: Entity\n        :rtype: AzureJob\n        \"\"\"\n        for attr in cls.defaultAttrs:\n            del jobEntity[attr]\n        return cls.fromItem(jobEntity)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fromItem(cls, item):\n        chunkedJob = list(item.items())\n        chunkedJob.sort()\n        if len(chunkedJob) == 1:\n            # First element of list = tuple, second element of tuple = serialized job\n            wholeJobString = chunkedJob[0][1].value\n        else:\n            wholeJobString = ''.join(item[1].value for item in chunkedJob)\n        return pickle.loads(bz2.decompress(wholeJobString))", "response": "Create an AzureJob from a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef toEntity(self, chunkSize=maxAzureTablePropertySize):\n        assert chunkSize <= maxAzureTablePropertySize\n        item = {}\n        serializedAndEncodedJob = bz2.compress(pickle.dumps(self, protocol=pickle.HIGHEST_PROTOCOL))\n        jobChunks = [serializedAndEncodedJob[i:i + chunkSize]\n                     for i in range(0, len(serializedAndEncodedJob), chunkSize)]\n        for attributeOrder, chunk in enumerate(jobChunks):\n            item['_' + str(attributeOrder).zfill(3)] = EntityProperty('Edm.Binary', chunk)\n        item['RowKey'] = str(self.jobStoreID)\n        item['PartitionKey'] = str(AzureTable.defaultPartition)\n        return Entity(item)", "response": "Converts the job into an entity."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dockerPredicate(e):\n    if not isinstance(e, subprocess.CalledProcessError):\n        return False\n    if e.returncode == 125:\n        return True", "response": "Returns True if the exception is retriable False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalling Docker with check_output.", "response": "def subprocessDockerCall(job,\n                         tool,\n                         parameters=None,\n                         workDir=None,\n                         dockerParameters=None,\n                         checkOutput=True,\n                         outfile=None,\n                         errfile=None,\n                         defer=None):\n    \"\"\"\n    Deprecated.  Calls Docker using subprocess.check_output().\n\n    Assumes `docker` is on the PATH.  Uses Toil's defer functionality to ensure\n    containers are shutdown even in case of job or pipeline failure\n\n    Example of using dockerCall in toil to index a FASTA file with SAMtools:\n        def toil_job(job):\n            work_dir = job.fileStore.getLocalTempDir()\n            path = job.fileStore.readGlobalFile(ref_id, os.path.join(\n                                                          work_dir, 'ref.fasta')\n            parameters = ['faidx', path]\n            dockerCall(job, tool='quay.io/ucgc_cgl/samtools:latest',\n                       work_dir=work_dir, parameters=parameters)\n\n    :param toil.Job.job job: The Job instance for the calling function.\n    :param str tool: Name of the Docker image to be used\n                     (e.g. quay.io/ucsc_cgl/samtools).\n    :param list[str] parameters: Command line arguments to be passed.\n           If list of lists: list[list[str]], then treat as successive commands\n           chained with pipe.\n    :param str workDir: Directory to mount into the container via `-v`.\n                        Destination convention is '/data'.\n    :param list[str] dockerParameters: Parameters to pass to Docker.\n             Default parameters are `--rm`, `--log-driver none`, and the\n             mountpoint `-v work_dir:/data` where /data is the destination\n             convention.\n             These defaults are removed if docker_parmaters is passed,\n             so be sure to pass them if they are desired.\n    :param file outfile: Pipe output of Docker call to file handle\n    :param file errfile: Pipe standard error of Docker call to file handle\n    :param int defer: What action should be taken on the container upon job\n                      completion?\n           FORGO (0) will leave the container untouched.\n           STOP (1) will attempt to stop the container with `docker stop`\n           (useful for debugging).\n           RM (2) will stop the container and then forcefully remove it from the\n           system using `docker rm -f`. This is the default behavior if defer is\n           set to None.\n    \"\"\"\n    if parameters is None:\n        parameters = []\n    if workDir is None:\n        workDir = os.getcwd()\n\n    # Setup the outgoing subprocess call for docker\n    baseDockerCall = ['docker', 'run']\n    if dockerParameters:\n        baseDockerCall += dockerParameters\n    else:\n        baseDockerCall += ['--rm', '--log-driver', 'none', '-v',\n                           os.path.abspath(workDir) + ':/data']\n\n    # Ensure the user has passed a valid value for defer\n    assert defer in (None, FORGO, STOP, RM)\n\n    # Get container name which is needed for _dockerKill\n    try:\n        if any('--name' in x for x in baseDockerCall):\n            if any('--name=' in x for x in baseDockerCall):\n                containerName = [x.split('=')[1]\n                                 for x in baseDockerCall if '--name' in x][0]\n            else:\n                containerName = baseDockerCall[\n                    baseDockerCall.index('--name') + 1]\n        else:\n            containerName = getContainerName(job)\n            baseDockerCall.extend(['--name', containerName])\n    except ValueError:\n        containerName = getContainerName(job)\n        baseDockerCall.extend(['--name', containerName])\n    except IndexError:\n        raise RuntimeError(\n            \"Couldn't parse Docker's `--name=` option, check parameters: \" +\n            str(dockerParameters))\n\n    # Defer the container on-exit action\n    if '--rm' in baseDockerCall and defer is None:\n        defer = RM\n    if '--rm' in baseDockerCall and defer is not RM:\n        logger.warn('--rm being passed to docker call but defer not set to '\n                    'dockerCall.RM, defer set to: ' + str(defer))\n    job.defer(_dockerKill, containerName, action=defer)\n    # Defer the permission fixing function which will run after this job.\n    # We call this explicitly later on in this function,\n    # but we defer it as well to handle unexpected job failure.\n    job.defer(_fixPermissions, tool, workDir)\n\n    # Make subprocess call\n\n    # If parameters is list of lists, treat each list as separate command and chain with pipes\n    if len(parameters) > 0 and type(parameters[0]) is list:\n        # When piping, all arguments now get merged into a single string to bash -c.\n        # We try to support spaces in paths by wrapping them all in quotes first.\n        chain_params = [' '.join(p) for p in [list(map(quote, q)) for q in parameters]]\n        # Use bash's set -eo pipefail to detect and abort on a failure in any command in the chain\n        call = baseDockerCall + ['--entrypoint', '/bin/bash',  tool, '-c',\n                                 'set -eo pipefail && {}'.format(' | '.join(chain_params))]\n    else:\n        call = baseDockerCall + [tool] + parameters\n    logger.info(\"Calling docker with \" + repr(call))\n\n    params = {}\n    if outfile:\n        params['stdout'] = outfile\n    if errfile:\n        params['stderr'] = errfile\n    if checkOutput:\n        callMethod = subprocess.check_output\n    else:\n        callMethod = subprocess.check_call\n\n    for attempt in retry(predicate=dockerPredicate):\n        with attempt:\n            out = callMethod(call, **params)\n\n    _fixPermissions(tool=tool, workDir=workDir)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef apiDockerCall(job,\n                  image,\n                  parameters=None,\n                  deferParam=None,\n                  volumes=None,\n                  working_dir=None,\n                  containerName=None,\n                  entrypoint=None,\n                  detach=False,\n                  log_config=None,\n                  auto_remove=None,\n                  remove=False,\n                  user=None,\n                  environment=None,\n                  stdout=None,\n                  stderr=False,\n                  streamfile=None,\n                  **kwargs):\n    \"\"\"\n    A toil wrapper for the python docker API.\n\n    Docker API Docs: https://docker-py.readthedocs.io/en/stable/index.html\n    Docker API Code: https://github.com/docker/docker-py\n\n    This implements docker's python API within toil so that calls are run as\n    jobs, with the intention that failed/orphaned docker jobs be handled\n    appropriately.\n\n    Example of using dockerCall in toil to index a FASTA file with SAMtools:\n    def toil_job(job):\n        working_dir = job.fileStore.getLocalTempDir()\n        path = job.fileStore.readGlobalFile(ref_id,\n                                          os.path.join(working_dir, 'ref.fasta')\n        parameters = ['faidx', path]\n        apiDockerCall(job,\n                      image='quay.io/ucgc_cgl/samtools:latest',\n                      working_dir=working_dir,\n                      parameters=parameters)\n                      \n    Note that when run with detatch=False, or with detatch=True and stdout=True\n    or stderr=True, this is a blocking call. When run with detatch=True and\n    without output capture, the container is started and returned without\n    waiting for it to finish.\n\n    :param toil.Job.job job: The Job instance for the calling function.\n    :param str image: Name of the Docker image to be used.\n                     (e.g. 'quay.io/ucsc_cgl/samtools:latest')\n    :param list[str] parameters: A list of string elements.  If there are\n                                 multiple elements, these will be joined with\n                                 spaces.  This handling of multiple elements\n                                 provides backwards compatibility with previous\n                                 versions which called docker using\n                                 subprocess.check_call().\n                                 **If list of lists: list[list[str]], then treat\n                                 as successive commands chained with pipe.\n    :param str working_dir: The working directory.\n    :param int deferParam: Action to take on the container upon job completion.\n           FORGO (0) leaves the container untouched and running.\n           STOP (1) Sends SIGTERM, then SIGKILL if necessary to the container.\n           RM (2) Immediately send SIGKILL to the container. This is the default\n           behavior if defer is set to None.\n    :param str name: The name/ID of the container.\n    :param str entrypoint: Prepends commands sent to the container.  See:\n                      https://docker-py.readthedocs.io/en/stable/containers.html\n    :param bool detach: Run the container in detached mode. (equivalent to '-d')\n    :param bool stdout: Return logs from STDOUT when detach=False (default: True).\n                        Block and capture stdout to a file when detach=True\n                        (default: False). Output capture defaults to output.log,\n                        and can be specified with the \"streamfile\" kwarg.\n    :param bool stderr: Return logs from STDERR when detach=False (default: False).\n                        Block and capture stderr to a file when detach=True\n                        (default: False). Output capture defaults to output.log,\n                        and can be specified with the \"streamfile\" kwarg.\n    :param str streamfile: Collect container output to this file if detach=True and \n                        stderr and/or stdout are True. Defaults to \"output.log\".\n    :param dict log_config: Specify the logs to return from the container.  See:\n                      https://docker-py.readthedocs.io/en/stable/containers.html\n    :param bool remove: Remove the container on exit or not.\n    :param str user: The container will be run with the privileges of\n                     the user specified.  Can be an actual name, such\n                     as 'root' or 'lifeisaboutfishtacos', or it can be\n                     the uid or gid of the user ('0' is root; '1000' is\n                     an example of a less privileged uid or gid), or a\n                     complement of the uid:gid (RECOMMENDED), such as\n                     '0:0' (root user : root group) or '1000:1000'\n                     (some other user : some other user group).\n    :param environment: Allows one to set environment variables inside of the\n                        container, such as:\n    :param kwargs: Additional keyword arguments supplied to the docker API's\n                   run command.  The list is 75 keywords total, for examples\n                   and full documentation see:\n                   https://docker-py.readthedocs.io/en/stable/containers.html\n                   \n    :returns: Returns the standard output/standard error text, as requested, when \n              detatch=False. Returns the underlying\n              docker.models.containers.Container object from the Docker API when\n              detatch=True.\n    \"\"\"\n\n    # make certain that files have the correct permissions\n    thisUser = os.getuid()\n    thisGroup = os.getgid()\n    if user is None:\n        user = str(thisUser) + \":\" + str(thisGroup)\n\n    if containerName is None:\n        containerName = getContainerName(job)\n\n    if working_dir is None:\n        working_dir = os.getcwd()\n\n    if volumes is None:\n        volumes = {working_dir: {'bind': '/data', 'mode': 'rw'}}\n\n    if parameters is None:\n        parameters = []\n\n    # If 'parameters' is a list of lists, treat each list as a separate command\n    # and chain with pipes.\n    if len(parameters) > 0 and type(parameters[0]) is list:\n        if entrypoint is None:\n            entrypoint = ['/bin/bash', '-c']\n        chain_params = \\\n            [' '.join((quote(arg) for arg in command)) \\\n             for command in parameters]\n        command = ' | '.join(chain_params)\n        pipe_prefix = \"set -eo pipefail && \"\n        command = [pipe_prefix + command]\n        logger.debug(\"Calling docker with: \" + repr(command))\n\n    # If 'parameters' is a normal list, join all elements into a single string\n    # element, quoting and escaping each element.\n    # Example: ['echo','the Oread'] becomes: [\"echo 'the Oread'\"]\n    # Note that this is still a list, and the docker API prefers this as best\n    # practice:\n    # http://docker-py.readthedocs.io/en/stable/containers.html\n    elif len(parameters) > 0 and type(parameters) is list:\n        command = ' '.join((quote(arg) for arg in parameters))\n        logger.debug(\"Calling docker with: \" + repr(command))\n\n    # If the 'parameters' lists are empty, they are respecified as None, which\n    # tells the API to simply create and run the container\n    else:\n        entrypoint = None\n        command = None\n\n    working_dir = os.path.abspath(working_dir)\n\n    # Ensure the user has passed a valid value for deferParam\n    assert deferParam in (None, FORGO, STOP, RM), \\\n        'Please provide a valid value for deferParam.'\n\n    client = docker.from_env(version='auto')\n\n    if deferParam == STOP:\n        job.defer(dockerStop, containerName, client)\n\n    if deferParam == FORGO:\n        remove = False\n    elif deferParam == RM:\n        remove = True\n        job.defer(dockerKill, containerName, client)\n    elif remove is True:\n        job.defer(dockerKill, containerName, client)\n\n    if auto_remove is None:\n        auto_remove = remove\n\n    try:\n        if detach is False:\n            # When detach is False, this returns stdout normally:\n            # >>> client.containers.run(\"ubuntu:latest\", \"echo hello world\")\n            # 'hello world\\n'\n            if stdout is None:\n                stdout = True\n            out = client.containers.run(image=image,\n                                        command=command,\n                                        working_dir=working_dir,\n                                        entrypoint=entrypoint,\n                                        name=containerName,\n                                        detach=False,\n                                        volumes=volumes,\n                                        auto_remove=auto_remove,\n                                        stdout=stdout,\n                                        stderr=stderr,\n                                        remove=remove,\n                                        log_config=log_config,\n                                        user=user,\n                                        environment=environment,\n                                        **kwargs)\n\n            return out\n        else:\n            if (stdout or stderr) and log_config is None:\n                logger.warn('stdout or stderr specified, but log_config is not set.  '\n                            'Defaulting to \"journald\".')\n                log_config = dict(type='journald')\n\n            if stdout is None:\n                stdout = False\n\n            # When detach is True, this returns a container object:\n            # >>> client.containers.run(\"bfirsh/reticulate-splines\", detach=True)\n            # <Container '45e6d2de7c54'>\n            container = client.containers.run(image=image,\n                                              command=command,\n                                              working_dir=working_dir,\n                                              entrypoint=entrypoint,\n                                              name=containerName,\n                                              detach=True,\n                                              volumes=volumes,\n                                              auto_remove=auto_remove,\n                                              stdout=stdout,\n                                              stderr=stderr,\n                                              remove=remove,\n                                              log_config=log_config,\n                                              user=user,\n                                              environment=environment,\n                                              **kwargs)\n            if stdout or stderr:\n                if streamfile is None:\n                    streamfile = 'output.log'\n                for line in container.logs(stdout=stdout, stderr=stderr, stream=True):\n                    # stream=True makes this loop blocking; we will loop until\n                    # the container stops and there is no more output.\n                    with open(streamfile, 'w') as f:\n                        f.write(line)\n                        \n            # If we didn't capture output, the caller will need to .wait() on\n            # the container to know when it is done. Even if we did capture\n            # output, the caller needs the container to get at the exit code.\n            return container\n            \n    except ContainerError:\n        logger.error(\"Docker had non-zero exit.  Check your command: \" + repr(command))\n        raise\n    except ImageNotFound:\n        logger.error(\"Docker image not found.\")\n        raise\n    except requests.exceptions.HTTPError as e:\n        logger.error(\"The server returned an error.\")\n        raise create_api_error_from_http_exception(e)", "response": "This function is a wrapper for the Docker API call that returns a toil job object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks whether the container is running or not.", "response": "def containerIsRunning(container_name):\n    \"\"\"\n    Checks whether the container is running or not.\n    :param container_name: Name of the container being checked.\n    :returns: True if status is 'running', False if status is anything else,\n    and None if the container does not exist.\n    \"\"\"\n    client = docker.from_env(version='auto')\n    try:\n        this_container = client.containers.get(container_name)\n        if this_container.status == 'running':\n            return True\n        else:\n            # this_container.status == 'exited', 'restarting', or 'paused'\n            return False\n    except NotFound:\n        return None\n    except requests.exceptions.HTTPError as e:\n        logger.debug(\"Server error attempting to call container: \",\n                      container_name)\n        raise create_api_error_from_http_exception(e)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getContainerName(job):\n    return '--'.join([str(job),\n                      base64.b64encode(os.urandom(9), b'-_').decode('utf-8')])\\\n                      .replace(\"'\", '').replace('\"', '').replace('_', '')", "response": "Create a random string including the job name and return it."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _fixPermissions(tool, workDir):\n    if os.geteuid() == 0:\n        # we're running as root so this chown is redundant\n        return\n\n    baseDockerCall = ['docker', 'run', '--log-driver=none',\n                      '-v', os.path.abspath(workDir) + ':/data', '--rm',\n                      '--entrypoint=chown']\n    stat = os.stat(workDir)\n    command = baseDockerCall + [tool] + ['-R', '{}:{}'.format(stat.st_uid,\n                                                              stat.st_gid),\n                                         '/data']\n    for attempt in retry(predicate=dockerPredicate):\n        with attempt:\n            subprocess.check_call(command)", "response": "This function is used to fix permissions of a mounted Docker directory by reusing the tool to change ownership of the mounted Docker directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef callPlaybook(self, playbook, ansibleArgs, wait=True, tags=[\"all\"]):\n        playbook = os.path.join(self.playbooks, playbook)  # Path to playbook being executed\n        verbosity = \"-vvvvv\" if logger.isEnabledFor(logging.DEBUG) else \"-v\"\n        command = [\"ansible-playbook\", verbosity, \"--tags\", \",\".join(tags), \"--extra-vars\"]\n        command.append(\" \".join([\"=\".join(i) for i in ansibleArgs.items()]))  # Arguments being passed to playbook\n        command.append(playbook)\n\n        logger.debug(\"Executing Ansible call `%s`\", \" \".join(command))\n        p = subprocess.Popen(command)\n        if wait:\n            p.communicate()\n            if p.returncode != 0:\n                # FIXME: parse error codes\n                raise RuntimeError(\"Ansible reported an error when executing playbook %s\" % playbook)", "response": "Runs an Ansible playbook and returns a dictionary of the result."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self):\n        # Start the stats/logging aggregation thread\n        self.statsAndLogging.start()\n        if self.config.metrics:\n            self.toilMetrics = ToilMetrics(provisioner=self.provisioner)\n\n        try:\n\n            # Start service manager thread\n            self.serviceManager.start()\n            try:\n\n                # Create cluster scaling processes if not None\n                if self.clusterScaler is not None:\n                    self.clusterScaler.start()\n\n                try:\n                    # Run the main loop\n                    self.innerLoop()\n                finally:\n                    if self.clusterScaler is not None:\n                        logger.debug('Waiting for workers to shutdown.')\n                        startTime = time.time()\n                        self.clusterScaler.shutdown()\n                        logger.debug('Worker shutdown complete in %s seconds.', time.time() - startTime)\n\n            finally:\n                # Ensure service manager thread is properly shutdown\n                self.serviceManager.shutdown()\n\n        finally:\n            # Ensure the stats and logging thread is properly shutdown\n            self.statsAndLogging.shutdown()\n            if self.toilMetrics:\n                self.toilMetrics.shutdown()\n\n        # Filter the failed jobs\n        self.toilState.totalFailedJobs = [j for j in self.toilState.totalFailedJobs if self.jobStore.exists(j.jobStoreID)]\n\n        try:\n            self.create_status_sentinel_file(self.toilState.totalFailedJobs)\n        except IOError as e:\n            logger.debug('Error from importFile with hardlink=True: {}'.format(e))\n\n        logger.info(\"Finished toil run %s\" %\n                     (\"successfully.\" if not self.toilState.totalFailedJobs \\\n                else (\"with %s failed jobs.\" % len(self.toilState.totalFailedJobs))))\n\n        if len(self.toilState.totalFailedJobs):\n            logger.info(\"Failed jobs at end of the run: %s\", ' '.join(str(job) for job in self.toilState.totalFailedJobs))\n        # Cleanup\n        if len(self.toilState.totalFailedJobs) > 0:\n            raise FailedJobsException(self.config.jobStore, self.toilState.totalFailedJobs, self.jobStore)\n\n        return self.jobStore.getRootJobReturnValue()", "response": "This method runs the root job s main loop and creates status sentinel files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a file in the jobstore indicating failure or success.", "response": "def create_status_sentinel_file(self, fail):\n        \"\"\"Create a file in the jobstore indicating failure or success.\"\"\"\n        logName = 'failed.log' if fail else 'succeeded.log'\n        localLog = os.path.join(os.getcwd(), logName)\n        open(localLog, 'w').close()\n        self.jobStore.importFile('file://' + localLog, logName, hardlink=True)\n\n        if os.path.exists(localLog):  # Bandaid for Jenkins tests failing stochastically and unexplainably.\n            os.remove(localLog)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if a successor job is ready to run when there are multiple predecessors.", "response": "def _checkSuccessorReadyToRunMultiplePredecessors(self, jobGraph, jobNode, successorJobStoreID):\n        \"\"\"Handle the special cases of checking if a successor job is\n        ready to run when there are multiple predecessors\"\"\"\n        # See implementation note at the top of this file for discussion of multiple predecessors\n        logger.debug(\"Successor job: %s of job: %s has multiple \"\n                     \"predecessors\", jobNode, jobGraph)\n\n        # Get the successor job graph, which is caches\n        if successorJobStoreID not in self.toilState.jobsToBeScheduledWithMultiplePredecessors:\n            self.toilState.jobsToBeScheduledWithMultiplePredecessors[successorJobStoreID] = self.jobStore.load(successorJobStoreID)\n        successorJobGraph = self.toilState.jobsToBeScheduledWithMultiplePredecessors[successorJobStoreID]\n\n        # Add the jobGraph as a finished predecessor to the successor\n        successorJobGraph.predecessorsFinished.add(jobGraph.jobStoreID)\n\n        # If the successor is in the set of successors of failed jobs\n        if successorJobStoreID in self.toilState.failedSuccessors:\n            if not self._handledFailedSuccessor(jobNode, jobGraph, successorJobStoreID):\n                return False\n\n        # If the successor job's predecessors have all not all completed then\n        # ignore the jobGraph as is not yet ready to run\n        assert len(successorJobGraph.predecessorsFinished) <= successorJobGraph.predecessorNumber\n        if len(successorJobGraph.predecessorsFinished) < successorJobGraph.predecessorNumber:\n            return False\n        else:\n            # Remove the successor job from the cache\n            self.toilState.jobsToBeScheduledWithMultiplePredecessors.pop(successorJobStoreID)\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes a successor job ready to run returning False if they should not yet be run", "response": "def _makeJobSuccessorReadyToRun(self, jobGraph, jobNode):\n        \"\"\"make a successor job ready to run, returning False if they should\n        not yet be run\"\"\"\n        successorJobStoreID = jobNode.jobStoreID\n        #Build map from successor to predecessors.\n        if successorJobStoreID not in self.toilState.successorJobStoreIDToPredecessorJobs:\n            self.toilState.successorJobStoreIDToPredecessorJobs[successorJobStoreID] = []\n        self.toilState.successorJobStoreIDToPredecessorJobs[successorJobStoreID].append(jobGraph)\n\n        if jobNode.predecessorNumber > 1:\n            return self._checkSuccessorReadyToRunMultiplePredecessors(jobGraph, jobNode, successorJobStoreID)\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _processReadyJobs(self):\n        logger.debug('Built the jobs list, currently have %i jobs to update and %i jobs issued',\n                     len(self.toilState.updatedJobs), self.getNumberOfJobsIssued())\n\n        updatedJobs = self.toilState.updatedJobs # The updated jobs to consider below\n        self.toilState.updatedJobs = set() # Resetting the list for the next set\n\n        for jobGraph, resultStatus in updatedJobs:\n            self._processReadyJob(jobGraph, resultStatus)", "response": "Process jobs that are ready to be scheduled and have successors to schedule"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting any available service jobs from the service manager.", "response": "def _startServiceJobs(self):\n        \"\"\"Start any service jobs available from the service manager\"\"\"\n        self.issueQueingServiceJobs()\n        while True:\n            serviceJob = self.serviceManager.getServiceJobsToStart(0)\n            # Stop trying to get jobs when function returns None\n            if serviceJob is None:\n                break\n            logger.debug('Launching service job: %s', serviceJob)\n            self.issueServiceJob(serviceJob)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _processJobsWithRunningServices(self):\n        while True:\n            jobGraph = self.serviceManager.getJobGraphWhoseServicesAreRunning(0)\n            if jobGraph is None: # Stop trying to get jobs when function returns None\n                break\n            logger.debug('Job: %s has established its services.', jobGraph.jobStoreID)\n            jobGraph.services = []\n            self.toilState.updatedJobs.add((jobGraph, 0))", "response": "Get jobs whose services have started"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _gatherUpdatedJobs(self, updatedJobTuple):\n        jobID, result, wallTime = updatedJobTuple\n        # easy, track different state\n        try:\n            updatedJob = self.jobBatchSystemIDToIssuedJob[jobID]\n        except KeyError:\n            logger.warn(\"A result seems to already have been processed \"\n                        \"for job %s\", jobID)\n        else:\n            if result == 0:\n                cur_logger = (logger.debug if str(updatedJob.jobName).startswith(CWL_INTERNAL_JOBS)\n                              else logger.info)\n                cur_logger('Job ended successfully: %s', updatedJob)\n                if self.toilMetrics:\n                    self.toilMetrics.logCompletedJob(updatedJob)\n            else:\n                logger.warn('Job failed with exit value %i: %s',\n                            result, updatedJob)\n            self.processFinishedJob(jobID, result, wallTime=wallTime)", "response": "Gather any new updated jobGraph from the batch system"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing jobs that have gone awry and have not yet been processed", "response": "def _processLostJobs(self):\n        \"\"\"Process jobs that have gone awry\"\"\"\n        # In the case that there is nothing happening (no updated jobs to\n        # gather for rescueJobsFrequency seconds) check if there are any jobs\n        # that have run too long (see self.reissueOverLongJobs) or which have\n        # gone missing from the batch system (see self.reissueMissingJobs)\n        if ((time.time() - self.timeSinceJobsLastRescued) >= self.config.rescueJobsFrequency):\n            # We only rescue jobs every N seconds, and when we have apparently\n            # exhausted the current jobGraph supply\n            self.reissueOverLongJobs()\n            logger.info(\"Reissued any over long jobs\")\n\n            hasNoMissingJobs = self.reissueMissingJobs()\n            if hasNoMissingJobs:\n                self.timeSinceJobsLastRescued = time.time()\n            else:\n                # This means we'll try again in a minute, providing things are quiet\n                self.timeSinceJobsLastRescued += 60\n            logger.debug(\"Rescued any (long) missing jobs\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef innerLoop(self):\n        self.timeSinceJobsLastRescued = time.time()\n\n        while self.toilState.updatedJobs or \\\n              self.getNumberOfJobsIssued() or \\\n              self.serviceManager.jobsIssuedToServiceManager:\n\n            if self.toilState.updatedJobs:\n                self._processReadyJobs()\n\n            # deal with service-related jobs\n            self._startServiceJobs()\n            self._processJobsWithRunningServices()\n\n            # check in with the batch system\n            updatedJobTuple = self.batchSystem.getUpdatedBatchJob(maxWait=2)\n            if updatedJobTuple is not None:\n                self._gatherUpdatedJobs(updatedJobTuple)\n            else:\n                self._processLostJobs()\n\n            # Check on the associated threads and exit if a failure is detected\n            self.statsAndLogging.check()\n            self.serviceManager.check()\n            # the cluster scaler object will only be instantiated if autoscaling is enabled\n            if self.clusterScaler is not None:\n                self.clusterScaler.check()\n\n            if len(self.toilState.updatedJobs) == 0 and self.deadlockThrottler.throttle(wait=False):\n                # Nothing happened this round and it's been long\n                # enough since we last checked. Check for deadlocks.\n                self.checkForDeadlocks()\n\n        logger.debug(\"Finished the main loop: no jobs left to run.\")\n\n        # Consistency check the toil state\n        assert self.toilState.updatedJobs == set()\n        assert self.toilState.successorCounts == {}\n        assert self.toilState.successorJobStoreIDToPredecessorJobs == {}\n        assert self.toilState.serviceJobStoreIDToPredecessorJob == {}\n        assert self.toilState.servicesIssued == {}", "response": "The main loop for processing jobs by the leader."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the system is deadlocked running service jobs.", "response": "def checkForDeadlocks(self):\n        \"\"\"\n        Checks if the system is deadlocked running service jobs.\n        \"\"\"\n        totalRunningJobs = len(self.batchSystem.getRunningBatchJobIDs())\n        totalServicesIssued = self.serviceJobsIssued + self.preemptableServiceJobsIssued\n        # If there are no updated jobs and at least some jobs running\n        if totalServicesIssued >= totalRunningJobs and totalRunningJobs > 0:\n            serviceJobs = [x for x in list(self.jobBatchSystemIDToIssuedJob.keys()) if isinstance(self.jobBatchSystemIDToIssuedJob[x], ServiceJobNode)]\n            runningServiceJobs = set([x for x in serviceJobs if self.serviceManager.isRunning(self.jobBatchSystemIDToIssuedJob[x])])\n            assert len(runningServiceJobs) <= totalRunningJobs\n\n            # If all the running jobs are active services then we have a potential deadlock\n            if len(runningServiceJobs) == totalRunningJobs:\n                # We wait self.config.deadlockWait seconds before declaring the system deadlocked\n                if self.potentialDeadlockedJobs != runningServiceJobs:\n                    self.potentialDeadlockedJobs = runningServiceJobs\n                    self.potentialDeadlockTime = time.time()\n                elif time.time() - self.potentialDeadlockTime >= self.config.deadlockWait:\n                    raise DeadlockException(\"The system is service deadlocked - all %d running jobs are active services\" % totalRunningJobs)\n            else:\n                # We have observed non-service jobs running, so reset the potential deadlock\n                self.potentialDeadlockedJobs = set()\n                self.potentialDeadlockTime = 0\n        else:\n            # We have observed non-service jobs running, so reset the potential deadlock\n            self.potentialDeadlockedJobs = set()\n            self.potentialDeadlockTime = 0"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nissue a job to the queue of jobs.", "response": "def issueJob(self, jobNode):\n        \"\"\"Add a job to the queue of jobs.\"\"\"\n        jobNode.command = ' '.join((resolveEntryPoint('_toil_worker'),\n                                    jobNode.jobName,\n                                    self.jobStoreLocator,\n                                    jobNode.jobStoreID))\n        # jobBatchSystemID is an int that is an incremented counter for each job\n        jobBatchSystemID = self.batchSystem.issueBatchJob(jobNode)\n        self.jobBatchSystemIDToIssuedJob[jobBatchSystemID] = jobNode\n        if jobNode.preemptable:\n            # len(jobBatchSystemIDToIssuedJob) should always be greater than or equal to preemptableJobsIssued,\n            # so increment this value after the job is added to the issuedJob dict\n            self.preemptableJobsIssued += 1\n        cur_logger = logger.debug if jobNode.jobName.startswith(CWL_INTERNAL_JOBS) else logger.info\n        cur_logger(\"Issued job %s with job batch system ID: \"\n                   \"%s and cores: %s, disk: %s, and memory: %s\",\n                   jobNode, str(jobBatchSystemID), int(jobNode.cores),\n                   bytes2human(jobNode.disk), bytes2human(jobNode.memory))\n        if self.toilMetrics:\n            self.toilMetrics.logIssuedJob(jobNode)\n            self.toilMetrics.logQueueSize(self.getNumberOfJobsIssued())"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nissues a service job.", "response": "def issueServiceJob(self, jobNode):\n        \"\"\"\n        Issue a service job, putting it on a queue if the maximum number of service\n        jobs to be scheduled has been reached.\n        \"\"\"\n        if jobNode.preemptable:\n            self.preemptableServiceJobsToBeIssued.append(jobNode)\n        else:\n            self.serviceJobsToBeIssued.append(jobNode)\n        self.issueQueingServiceJobs()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef issueQueingServiceJobs(self):\n        while len(self.serviceJobsToBeIssued) > 0 and self.serviceJobsIssued < self.config.maxServiceJobs:\n            self.issueJob(self.serviceJobsToBeIssued.pop())\n            self.serviceJobsIssued += 1\n        while len(self.preemptableServiceJobsToBeIssued) > 0 and self.preemptableServiceJobsIssued < self.config.maxPreemptableServiceJobs:\n            self.issueJob(self.preemptableServiceJobsToBeIssued.pop())\n            self.preemptableServiceJobsIssued += 1", "response": "Issues any queuing service jobs up to the limit of the maximum allowed."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the number of jobs that have been added by issueJob and notJob.", "response": "def getNumberOfJobsIssued(self, preemptable=None):\n        \"\"\"\n        Gets number of jobs that have been added by issueJob(s) and not\n        removed by removeJob\n\n        :param None or boolean preemptable: If none, return all types of jobs.\n          If true, return just the number of preemptable jobs. If false, return\n          just the number of non-preemptable jobs.\n        \"\"\"\n        if preemptable is None:\n            return len(self.jobBatchSystemIDToIssuedJob)\n        elif preemptable:\n            return self.preemptableJobsIssued\n        else:\n            assert len(self.jobBatchSystemIDToIssuedJob) >= self.preemptableJobsIssued\n            return len(self.jobBatchSystemIDToIssuedJob) - self.preemptableJobsIssued"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves a job from the system.", "response": "def removeJob(self, jobBatchSystemID):\n        \"\"\"Removes a job from the system.\"\"\"\n        assert jobBatchSystemID in self.jobBatchSystemIDToIssuedJob\n        jobNode = self.jobBatchSystemIDToIssuedJob[jobBatchSystemID]\n        if jobNode.preemptable:\n            # len(jobBatchSystemIDToIssuedJob) should always be greater than or equal to preemptableJobsIssued,\n            # so decrement this value before removing the job from the issuedJob map\n            assert self.preemptableJobsIssued > 0\n            self.preemptableJobsIssued -= 1\n        del self.jobBatchSystemIDToIssuedJob[jobBatchSystemID]\n        # If service job\n        if jobNode.jobStoreID in self.toilState.serviceJobStoreIDToPredecessorJob:\n            # Decrement the number of services\n            if jobNode.preemptable:\n                self.preemptableServiceJobsIssued -= 1\n            else:\n                self.serviceJobsIssued -= 1\n\n        return jobNode"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef killJobs(self, jobsToKill):\n        if len(jobsToKill) > 0:\n            self.batchSystem.killBatchJobs(jobsToKill)\n            for jobBatchSystemID in jobsToKill:\n                self.processFinishedJob(jobBatchSystemID, 1)", "response": "Kills the given set of jobs and sends them for processing."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck each issued job - if it is running for longer than maxJobDuration issue a kill instruction.", "response": "def reissueOverLongJobs(self):\n        \"\"\"\n        Check each issued job - if it is running for longer than desirable\n        issue a kill instruction.\n        Wait for the job to die then we pass the job to processFinishedJob.\n        \"\"\"\n        maxJobDuration = self.config.maxJobDuration\n        jobsToKill = []\n        if maxJobDuration < 10000000:  # We won't bother doing anything if rescue time > 16 weeks.\n            runningJobs = self.batchSystem.getRunningBatchJobIDs()\n            for jobBatchSystemID in list(runningJobs.keys()):\n                if runningJobs[jobBatchSystemID] > maxJobDuration:\n                    logger.warn(\"The job: %s has been running for: %s seconds, more than the \"\n                                \"max job duration: %s, we'll kill it\",\n                                str(self.jobBatchSystemIDToIssuedJob[jobBatchSystemID].jobStoreID),\n                                str(runningJobs[jobBatchSystemID]),\n                                str(maxJobDuration))\n                    jobsToKill.append(jobBatchSystemID)\n            self.killJobs(jobsToKill)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck all the current job ids are in the list of currently running batch system jobs. If a job is missing, we mark it as so, if it is missing for a number of runs of this function (say 10).. then we try deleting the job (though its probably lost), we wait then we pass the job to processFinishedJob.", "response": "def reissueMissingJobs(self, killAfterNTimesMissing=3):\n        \"\"\"\n        Check all the current job ids are in the list of currently running batch system jobs.\n        If a job is missing, we mark it as so, if it is missing for a number of runs of\n        this function (say 10).. then we try deleting the job (though its probably lost), we wait\n        then we pass the job to processFinishedJob.\n        \"\"\"\n        runningJobs = set(self.batchSystem.getIssuedBatchJobIDs())\n        jobBatchSystemIDsSet = set(list(self.jobBatchSystemIDToIssuedJob.keys()))\n        #Clean up the reissueMissingJobs_missingHash hash, getting rid of jobs that have turned up\n        missingJobIDsSet = set(list(self.reissueMissingJobs_missingHash.keys()))\n        for jobBatchSystemID in missingJobIDsSet.difference(jobBatchSystemIDsSet):\n            self.reissueMissingJobs_missingHash.pop(jobBatchSystemID)\n            logger.warn(\"Batch system id: %s is no longer missing\", str(jobBatchSystemID))\n        assert runningJobs.issubset(jobBatchSystemIDsSet) #Assert checks we have\n        #no unexpected jobs running\n        jobsToKill = []\n        for jobBatchSystemID in set(jobBatchSystemIDsSet.difference(runningJobs)):\n            jobStoreID = self.jobBatchSystemIDToIssuedJob[jobBatchSystemID].jobStoreID\n            if jobBatchSystemID in self.reissueMissingJobs_missingHash:\n                self.reissueMissingJobs_missingHash[jobBatchSystemID] += 1\n            else:\n                self.reissueMissingJobs_missingHash[jobBatchSystemID] = 1\n            timesMissing = self.reissueMissingJobs_missingHash[jobBatchSystemID]\n            logger.warn(\"Job store ID %s with batch system id %s is missing for the %i time\",\n                        jobStoreID, str(jobBatchSystemID), timesMissing)\n            if self.toilMetrics:\n                self.toilMetrics.logMissingJob()\n            if timesMissing == killAfterNTimesMissing:\n                self.reissueMissingJobs_missingHash.pop(jobBatchSystemID)\n                jobsToKill.append(jobBatchSystemID)\n        self.killJobs(jobsToKill)\n        return len( self.reissueMissingJobs_missingHash ) == 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfunctions processes a finished job and updates its state.", "response": "def processFinishedJob(self, batchSystemID, resultStatus, wallTime=None):\n        \"\"\"\n        Function reads a processed jobGraph file and updates its state.\n        \"\"\"\n        jobNode = self.removeJob(batchSystemID)\n        jobStoreID = jobNode.jobStoreID\n        if wallTime is not None and self.clusterScaler is not None:\n            self.clusterScaler.addCompletedJob(jobNode, wallTime)\n        if self.jobStore.exists(jobStoreID):\n            logger.debug(\"Job %s continues to exist (i.e. has more to do)\", jobNode)\n            try:\n                jobGraph = self.jobStore.load(jobStoreID)\n            except NoSuchJobException:\n                # Avoid importing AWSJobStore as the corresponding extra might be missing\n                if self.jobStore.__class__.__name__ == 'AWSJobStore':\n                    # We have a ghost job - the job has been deleted but a stale read from\n                    # SDB gave us a false positive when we checked for its existence.\n                    # Process the job from here as any other job removed from the job store.\n                    # This is a temporary work around until https://github.com/BD2KGenomics/toil/issues/1091\n                    # is completed\n                    logger.warn('Got a stale read from SDB for job %s', jobNode)\n                    self.processRemovedJob(jobNode, resultStatus)\n                    return\n                else:\n                    raise\n            if jobGraph.logJobStoreFileID is not None:\n                with jobGraph.getLogFileHandle(self.jobStore) as logFileStream:\n                    # more memory efficient than read().striplines() while leaving off the\n                    # trailing \\n left when using readlines()\n                    # http://stackoverflow.com/a/15233739\n                    StatsAndLogging.logWithFormatting(jobStoreID, logFileStream, method=logger.warn,\n                                                      message='The job seems to have left a log file, indicating failure: %s' % jobGraph)\n                if self.config.writeLogs or self.config.writeLogsGzip:\n                    with jobGraph.getLogFileHandle(self.jobStore) as logFileStream:\n                        StatsAndLogging.writeLogFiles(jobGraph.chainedJobs, logFileStream, self.config)\n            if resultStatus != 0:\n                # If the batch system returned a non-zero exit code then the worker\n                # is assumed not to have captured the failure of the job, so we\n                # reduce the retry count here.\n                if jobGraph.logJobStoreFileID is None:\n                    logger.warn(\"No log file is present, despite job failing: %s\", jobNode)\n                jobGraph.setupJobAfterFailure(self.config)\n                self.jobStore.update(jobGraph)\n            elif jobStoreID in self.toilState.hasFailedSuccessors:\n                # If the job has completed okay, we can remove it from the list of jobs with failed successors\n                self.toilState.hasFailedSuccessors.remove(jobStoreID)\n\n            self.toilState.updatedJobs.add((jobGraph, resultStatus)) #Now we know the\n            #jobGraph is done we can add it to the list of updated jobGraph files\n            logger.debug(\"Added job: %s to active jobs\", jobGraph)\n        else:  #The jobGraph is done\n            self.processRemovedJob(jobNode, resultStatus)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getSuccessors(jobGraph, alreadySeenSuccessors, jobStore):\n        successors = set()\n        def successorRecursion(jobGraph):\n            # For lists of successors\n            for successorList in jobGraph.stack:\n\n                # For each successor in list of successors\n                for successorJobNode in successorList:\n\n                    # If successor not already visited\n                    if successorJobNode.jobStoreID not in alreadySeenSuccessors:\n\n                        # Add to set of successors\n                        successors.add(successorJobNode.jobStoreID)\n                        alreadySeenSuccessors.add(successorJobNode.jobStoreID)\n\n                        # Recurse if job exists\n                        # (job may not exist if already completed)\n                        if jobStore.exists(successorJobNode.jobStoreID):\n                            successorRecursion(jobStore.load(successorJobNode.jobStoreID))\n\n        successorRecursion(jobGraph)  # Recurse from jobGraph\n\n        return successors", "response": "Get successors of the given job by walking the job graph recursively and adding them to alreadySeenSuccessors."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses a totally failed job.", "response": "def processTotallyFailedJob(self, jobGraph):\n        \"\"\"\n        Processes a totally failed job.\n        \"\"\"\n        # Mark job as a totally failed job\n        self.toilState.totalFailedJobs.add(JobNode.fromJobGraph(jobGraph))\n        if self.toilMetrics:\n            self.toilMetrics.logFailedJob(jobGraph)\n\n        if jobGraph.jobStoreID in self.toilState.serviceJobStoreIDToPredecessorJob: # Is\n            # a service job\n            logger.debug(\"Service job is being processed as a totally failed job: %s\", jobGraph)\n\n            predecesssorJobGraph = self.toilState.serviceJobStoreIDToPredecessorJob[jobGraph.jobStoreID]\n\n            # This removes the service job as a service of the predecessor\n            # and potentially makes the predecessor active\n            self._updatePredecessorStatus(jobGraph.jobStoreID)\n\n            # Remove the start flag, if it still exists. This indicates\n            # to the service manager that the job has \"started\", this prevents\n            # the service manager from deadlocking while waiting\n            self.jobStore.deleteFile(jobGraph.startJobStoreID)\n\n            # Signal to any other services in the group that they should\n            # terminate. We do this to prevent other services in the set\n            # of services from deadlocking waiting for this service to start properly\n            if predecesssorJobGraph.jobStoreID in self.toilState.servicesIssued:\n                self.serviceManager.killServices(self.toilState.servicesIssued[predecesssorJobGraph.jobStoreID], error=True)\n                logger.debug(\"Job: %s is instructing all the services of its parent job to quit\", jobGraph)\n\n            self.toilState.hasFailedSuccessors.add(predecesssorJobGraph.jobStoreID) # This ensures that the\n            # job will not attempt to run any of it's successors on the stack\n        else:\n            # Is a non-service job\n            assert jobGraph.jobStoreID not in self.toilState.servicesIssued\n\n            # Traverse failed job's successor graph and get the jobStoreID of new successors.\n            # Any successor already in toilState.failedSuccessors will not be traversed\n            # All successors traversed will be added to toilState.failedSuccessors and returned\n            # as a set (unseenSuccessors).\n            unseenSuccessors = self.getSuccessors(jobGraph, self.toilState.failedSuccessors,\n                                                  self.jobStore)\n            logger.debug(\"Found new failed successors: %s of job: %s\", \" \".join(\n                         unseenSuccessors), jobGraph)\n\n            # For each newly found successor\n            for successorJobStoreID in unseenSuccessors:\n\n                # If the successor is a successor of other jobs that have already tried to schedule it\n                if successorJobStoreID in self.toilState.successorJobStoreIDToPredecessorJobs:\n\n                    # For each such predecessor job\n                    # (we remove the successor from toilState.successorJobStoreIDToPredecessorJobs to avoid doing\n                    # this multiple times for each failed predecessor)\n                    for predecessorJob in self.toilState.successorJobStoreIDToPredecessorJobs.pop(successorJobStoreID):\n\n                        # Reduce the predecessor job's successor count.\n                        self.toilState.successorCounts[predecessorJob.jobStoreID] -= 1\n\n                        # Indicate that it has failed jobs.\n                        self.toilState.hasFailedSuccessors.add(predecessorJob.jobStoreID)\n                        logger.debug(\"Marking job: %s as having failed successors (found by \"\n                                     \"reading successors failed job)\", predecessorJob)\n\n                        # If the predecessor has no remaining successors, add to list of active jobs\n                        assert self.toilState.successorCounts[predecessorJob.jobStoreID] >= 0\n                        if self.toilState.successorCounts[predecessorJob.jobStoreID] == 0:\n                            self.toilState.updatedJobs.add((predecessorJob, 0))\n\n                            # Remove the predecessor job from the set of jobs with successors.\n                            self.toilState.successorCounts.pop(predecessorJob.jobStoreID)\n\n            # If the job has predecessor(s)\n            if jobGraph.jobStoreID in self.toilState.successorJobStoreIDToPredecessorJobs:\n\n                # For each predecessor of the job\n                for predecessorJobGraph in self.toilState.successorJobStoreIDToPredecessorJobs[jobGraph.jobStoreID]:\n\n                    # Mark the predecessor as failed\n                    self.toilState.hasFailedSuccessors.add(predecessorJobGraph.jobStoreID)\n                    logger.debug(\"Totally failed job: %s is marking direct predecessor: %s \"\n                                 \"as having failed jobs\", jobGraph, predecessorJobGraph)\n\n                self._updatePredecessorStatus(jobGraph.jobStoreID)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _updatePredecessorStatus(self, jobStoreID):\n        if jobStoreID in self.toilState.serviceJobStoreIDToPredecessorJob:\n            # Is a service job\n            predecessorJob = self.toilState.serviceJobStoreIDToPredecessorJob.pop(jobStoreID)\n            self.toilState.servicesIssued[predecessorJob.jobStoreID].pop(jobStoreID)\n            if len(self.toilState.servicesIssued[predecessorJob.jobStoreID]) == 0: # Predecessor job has\n                # all its services terminated\n                self.toilState.servicesIssued.pop(predecessorJob.jobStoreID) # The job has no running services\n                self.toilState.updatedJobs.add((predecessorJob, 0)) # Now we know\n                # the job is done we can add it to the list of updated job files\n\n        elif jobStoreID not in self.toilState.successorJobStoreIDToPredecessorJobs:\n            #We have reach the root job\n            assert len(self.toilState.updatedJobs) == 0\n            assert len(self.toilState.successorJobStoreIDToPredecessorJobs) == 0\n            assert len(self.toilState.successorCounts) == 0\n            logger.debug(\"Reached root job %s so no predecessors to clean up\" % jobStoreID)\n\n        else:\n            # Is a non-root, non-service job\n            logger.debug(\"Cleaning the predecessors of %s\" % jobStoreID)\n\n            # For each predecessor\n            for predecessorJob in self.toilState.successorJobStoreIDToPredecessorJobs.pop(jobStoreID):\n\n                # Reduce the predecessor's number of successors by one to indicate the\n                # completion of the jobStoreID job\n                self.toilState.successorCounts[predecessorJob.jobStoreID] -= 1\n\n                # If the predecessor job is done and all the successors are complete\n                if self.toilState.successorCounts[predecessorJob.jobStoreID] == 0:\n\n                    # Remove it from the set of jobs with active successors\n                    self.toilState.successorCounts.pop(predecessorJob.jobStoreID)\n\n                    if predecessorJob.jobStoreID not in self.toilState.hasFailedSuccessors:\n                        # Pop stack at this point, as we can get rid of its successors\n                        predecessorJob.stack.pop()\n\n                    # Now we know the job is done we can add it to the list of updated job files\n                    assert predecessorJob not in self.toilState.updatedJobs\n                    self.toilState.updatedJobs.add((predecessorJob, 0))", "response": "Update the status of predecessors for finished successor job."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _buildToilState(self, jobGraph, jobStore, jobCache=None):\n\n        def getJob(jobId):\n            if jobCache is not None:\n                if jobId in jobCache:\n                    return jobCache[jobId]\n            return jobStore.load(jobId)\n\n        # If the jobGraph has a command, is a checkpoint, has services or is ready to be\n        # deleted it is ready to be processed\n        if jobGraph.command is not None or jobGraph.checkpoint is not None or jobGraph.services or not jobGraph.stack:\n            logger.debug('Found job to run: %s, with command: %s, with checkpoint: %s, '\n                         'with  services: %s, with stack: %s', jobGraph.jobStoreID,\n                         jobGraph.command is not None, jobGraph.checkpoint is not None,\n                         len(jobGraph.services) > 0, len(jobGraph.stack) == 0)\n            self.updatedJobs.add((jobGraph, 0))\n\n            if jobGraph.checkpoint is not None:\n                jobGraph.command = jobGraph.checkpoint\n\n        else: # There exist successors\n            logger.debug(\"Adding job: %s to the state with %s successors\" % (jobGraph.jobStoreID, len(jobGraph.stack[-1])))\n            \n            # Record the number of successors\n            self.successorCounts[jobGraph.jobStoreID] = len(jobGraph.stack[-1])\n\n            def processSuccessorWithMultiplePredecessors(successorJobGraph):\n                # If jobGraph is not reported as complete by the successor\n                if jobGraph.jobStoreID not in successorJobGraph.predecessorsFinished:\n\n                    # Update the successor's status to mark the predecessor complete\n                    successorJobGraph.predecessorsFinished.add(jobGraph.jobStoreID)\n\n                # If the successor has no predecessors to finish\n                assert len(successorJobGraph.predecessorsFinished) <= successorJobGraph.predecessorNumber\n                if len(successorJobGraph.predecessorsFinished) == successorJobGraph.predecessorNumber:\n                    \n                    # It is ready to be run, so remove it from the cache\n                    self.jobsToBeScheduledWithMultiplePredecessors.pop(successorJobStoreID)\n                    \n                    # Recursively consider the successor\n                    self._buildToilState(successorJobGraph, jobStore, jobCache=jobCache)\n            \n            # For each successor\n            for successorJobNode in jobGraph.stack[-1]:\n                successorJobStoreID = successorJobNode.jobStoreID\n                \n                # If the successor jobGraph does not yet point back at a\n                # predecessor we have not yet considered it\n                if successorJobStoreID not in self.successorJobStoreIDToPredecessorJobs:\n\n                    # Add the job as a predecessor\n                    self.successorJobStoreIDToPredecessorJobs[successorJobStoreID] = [jobGraph]\n                    \n                    # If predecessor number > 1 then the successor has multiple predecessors\n                    if successorJobNode.predecessorNumber > 1:\n                        \n                        # We load the successor job\n                        successorJobGraph = getJob(successorJobStoreID)\n                        \n                        # We put the successor job in the cache of successor jobs with multiple predecessors\n                        assert successorJobStoreID not in self.jobsToBeScheduledWithMultiplePredecessors\n                        self.jobsToBeScheduledWithMultiplePredecessors[successorJobStoreID] = successorJobGraph\n                        \n                        # Process successor\n                        processSuccessorWithMultiplePredecessors(successorJobGraph)\n                            \n                    else:\n                        # The successor has only the jobGraph as a predecessor so\n                        # recursively consider the successor\n                        self._buildToilState(getJob(successorJobStoreID), jobStore, jobCache=jobCache)\n                \n                else:\n                    # We've already seen the successor\n                    \n                    # Add the job as a predecessor\n                    assert jobGraph not in self.successorJobStoreIDToPredecessorJobs[successorJobStoreID]\n                    self.successorJobStoreIDToPredecessorJobs[successorJobStoreID].append(jobGraph)\n                    \n                    # If the successor has multiple predecessors\n                    if successorJobStoreID in self.jobsToBeScheduledWithMultiplePredecessors:\n                        \n                        # Get the successor from cache\n                        successorJobGraph = self.jobsToBeScheduledWithMultiplePredecessors[successorJobStoreID]\n                        \n                        # Process successor\n                        processSuccessorWithMultiplePredecessors(successorJobGraph)", "response": "Builds the ToilState object for the given jobGraph."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwait for the stats and logging collator thread to finish", "response": "def shutdown(self):\n        \"\"\"\n        Finish up the stats/logging aggregation thread\n        \"\"\"\n        logger.debug('Waiting for stats and logging collator thread to finish ...')\n        startTime = time.time()\n        self._stop.set()\n        self._worker.join()\n        logger.debug('... finished collating stats and logs. Took %s seconds', time.time() - startTime)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nresolving the contents an indirect dictionary to produce a dictionary actual values including merging multiple sources into a single input.", "response": "def _resolve_indirect_inner(maybe_idict):\n    \"\"\"Resolve the contents an indirect dictionary (containing promises) to produce\n    a dictionary actual values, including merging multiple sources into a\n    single input.\n    \"\"\"\n\n    if isinstance(maybe_idict, IndirectDict):\n        result = {}\n        for key, value in list(maybe_idict.items()):\n            if isinstance(value, (MergeInputs, DefaultWithSource)):\n                result[key] = value.resolve()\n            else:\n                result[key] = value[1].get(value[0])\n        return result\n    return maybe_idict"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resolve_indirect(pdict):\n\n    inner = IndirectDict() if isinstance(pdict, IndirectDict) else {}\n    needs_eval = False\n    for k, value in iteritems(pdict):\n        if isinstance(value, StepValueFrom):\n            inner[k] = value.inner\n            needs_eval = True\n        else:\n            inner[k] = value\n    res = _resolve_indirect_inner(inner)\n    if needs_eval:\n        ev = {}\n        for k, value in iteritems(pdict):\n            if isinstance(value, StepValueFrom):\n                ev[k] = value.do_eval(res, res[k])\n            else:\n                ev[k] = res[k]\n        return ev\n    return res", "response": "Resolve the contents an indirect dictionary and evaluate expressions to produce the dictionary of actual values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef simplify_list(maybe_list):\n    if isinstance(maybe_list, MutableSequence):\n        is_list = aslist(maybe_list)\n        if len(is_list) == 1:\n            return is_list[0]\n    return maybe_list", "response": "Turn a length one list loaded by cwltool into a scalar.\n    Anything else is passed as - is by reference."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget path to input file from Toil jobstore.", "response": "def toil_get_file(file_store, index, existing, file_store_id):\n    \"\"\"Get path to input file from Toil jobstore.\"\"\"\n\n    if not file_store_id.startswith(\"toilfs:\"):\n        return file_store.jobStore.getPublicUrl(file_store.jobStore.importFile(file_store_id))\n    src_path = file_store.readGlobalFile(file_store_id[7:])\n    index[src_path] = file_store_id\n    existing[file_store_id] = src_path\n    return schema_salad.ref_resolver.file_uri(src_path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite a file into the Toil jobstore.", "response": "def write_file(writeFunc, index, existing, x):\n    \"\"\"Write a file into the Toil jobstore.\n\n    'existing' is a set of files retrieved as inputs from toil_get_file. This\n    ensures they are mapped back as the same name if passed through.\n\n    \"\"\"\n    # Toil fileStore reference\n    if x.startswith(\"toilfs:\"):\n        return x\n    # File literal outputs with no path, we don't write these and will fail\n    # with unsupportedRequirement when retrieving later with getFile\n    elif x.startswith(\"_:\"):\n        return x\n    else:\n        x = existing.get(x, x)\n        if x not in index:\n            if not urlparse.urlparse(x).scheme:\n                rp = os.path.realpath(x)\n            else:\n                rp = x\n            try:\n                index[x] = \"toilfs:\" + writeFunc(rp)\n                existing[index[x]] = x\n            except Exception as e:\n                cwllogger.error(\"Got exception '%s' while copying '%s'\", e, x)\n                raise\n        return index[x]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef uploadFile(uploadfunc, fileindex, existing, uf, skip_broken=False):\n\n    if uf[\"location\"].startswith(\"toilfs:\") or uf[\"location\"].startswith(\"_:\"):\n        return\n    if uf[\"location\"] in fileindex:\n        uf[\"location\"] = fileindex[uf[\"location\"]]\n        return\n    if not uf[\"location\"] and uf[\"path\"]:\n        uf[\"location\"] = schema_salad.ref_resolver.file_uri(uf[\"path\"])\n    if uf[\"location\"].startswith(\"file://\") and not os.path.isfile(uf[\"location\"][7:]):\n        if skip_broken:\n            return\n        else:\n            raise cwltool.errors.WorkflowException(\n                \"File is missing: %s\" % uf[\"location\"])\n    uf[\"location\"] = write_file(\n        uploadfunc, fileindex, existing, uf[\"location\"])", "response": "Update a file object so that the location is a reference to the file store if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef toilStageFiles(file_store, cwljob, outdir, index, existing, export,\n                   destBucket=None):\n    \"\"\"Copy input files out of the global file store and update location and\n    path.\"\"\"\n\n    def _collectDirEntries(obj):\n    # type: (Union[Dict[Text, Any], List[Dict[Text, Any]]]) -> Iterator[Dict[Text, Any]]\n        if isinstance(obj, dict):\n            if obj.get(\"class\") in (\"File\", \"Directory\"):\n                yield obj\n            else:\n                for sub_obj in obj.values():\n                    for dir_entry in _collectDirEntries(sub_obj):\n                        yield dir_entry\n        elif isinstance(obj, list):\n            for sub_obj in obj:\n                for dir_entry in _collectDirEntries(sub_obj):\n                    yield dir_entry\n\n    jobfiles = list(_collectDirEntries(cwljob))\n    pm = ToilPathMapper(\n        jobfiles, \"\", outdir, separateDirs=False, stage_listing=True)\n    for f, p in pm.items():\n        if not p.staged:\n            continue\n\n        # Deal with bucket exports\n        if destBucket:\n            # Directories don't need to be created if we're exporting to\n            # a bucket\n            if p.type == \"File\":\n                # Remove the staging directory from the filepath and\n                # form the destination URL\n                unstageTargetPath = p.target[len(outdir):]\n                destUrl = '/'.join(s.strip('/')\n                                   for s in [destBucket, unstageTargetPath])\n\n                file_store.exportFile(p.resolved[7:], destUrl)\n\n            continue\n\n        if not os.path.exists(os.path.dirname(p.target)):\n            os.makedirs(os.path.dirname(p.target), 0o0755)\n        if p.type == \"File\":\n            file_store.exportFile(p.resolved[7:], \"file://\" + p.target)\n        elif p.type == \"Directory\" and not os.path.exists(p.target):\n            os.makedirs(p.target, 0o0755)\n        elif p.type == \"CreateFile\":\n            with open(p.target, \"wb\") as n:\n                n.write(p.resolved.encode(\"utf-8\"))\n\n    def _check_adjust(f):\n        f[\"location\"] = schema_salad.ref_resolver.file_uri(\n            pm.mapper(f[\"location\"])[1])\n        if \"contents\" in f:\n            del f[\"contents\"]\n        return f\n\n    visit_class(cwljob, (\"File\", \"Directory\"), _check_adjust)", "response": "Copy input files out of the global file store and update location and path."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a temporary directory in the hierarchy of directories under a given top directory.", "response": "def _makeNestedTempDir(top, seed, levels=2):\n    \"\"\"\n    Gets a temporary directory in the hierarchy of directories under a given\n    top directory.\n\n    This exists to avoid placing too many temporary directories under a single\n    top in a flat structure, which can slow down metadata updates such as\n    deletes on the local file system.\n\n    The seed parameter allows for deterministic placement of the created\n    directory. The seed is hashed into hex digest and the directory structure\n    is created from the initial letters of the digest.\n\n    :param top : string, top directory for the hierarchy\n    :param seed : string, the hierarchy will be generated from this seed string\n    :rtype : string, path to temporary directory - will be created when\n        necessary.\n    \"\"\"\n    # Valid chars for the creation of temporary directories\n    validDirs = hashlib.md5(six.b(str(seed))).hexdigest()\n    tempDir = top\n    for i in range(max(min(levels, len(validDirs)), 1)):\n        tempDir = os.path.join(tempDir, validDirs[i])\n        if not os.path.exists(tempDir):\n            try:\n                os.makedirs(tempDir)\n            except os.error:\n                if not os.path.exists(tempDir):\n                    # In the case that a collision occurs and\n                    # it is created while we wait then we ignore\n                    raise\n    return tempDir"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef makeJob(tool, jobobj, step_inputs, runtime_context):\n\n    if tool.tool[\"class\"] == \"Workflow\":\n        wfjob = CWLWorkflow(tool, jobobj, runtime_context)\n        followOn = ResolveIndirect(wfjob.rv())\n        wfjob.addFollowOn(followOn)\n        return (wfjob, followOn)\n    else:\n        # get_requirement\n        resourceReq, _ = tool.get_requirement(\"ResourceRequirement\")\n        if resourceReq:\n            for req in (\"coresMin\", \"coresMax\", \"ramMin\", \"ramMax\",\n                        \"tmpdirMin\", \"tmpdirMax\", \"outdirMin\", \"outdirMax\"):\n                r = resourceReq.get(req)\n                if isinstance(r, string_types) and (\"$(\" in r or \"${\" in r):\n                    # Found a dynamic resource requirement so use a job wrapper\n                    job = CWLJobWrapper(tool, jobobj, runtime_context)\n                    return (job, job)\n        job = CWLJob(tool, jobobj, runtime_context)\n        return (job, job)", "response": "Create a correct Toil Job object for the given tool."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves pickle problems from the object.", "response": "def remove_pickle_problems(obj):\n    \"\"\"doc_loader does not pickle correctly, causing Toil errors, remove from\n       objects.\n    \"\"\"\n    if hasattr(obj, \"doc_loader\"):\n        obj.doc_loader = None\n    if hasattr(obj, \"embedded_tool\"):\n        obj.embedded_tool = remove_pickle_problems(obj.embedded_tool)\n    if hasattr(obj, \"steps\"):\n        obj.steps = [remove_pickle_problems(s) for s in obj.steps]\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove temporarly created directories.", "response": "def cleanTempDirs(job):\n    \"\"\"Remove temporarly created directories.\"\"\"\n    if job is CWLJob and job._succeeded:  # Only CWLJobs have this attribute.\n        for tempDir in job.openTempDirs:\n            if os.path.exists(tempDir):\n                shutil.rmtree(tempDir)\n        job.openTempDirs = []"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main(args=None, stdout=sys.stdout):\n    cwllogger.removeHandler(defaultStreamHandler)\n    config = Config()\n    config.cwl = True\n    parser = argparse.ArgumentParser()\n    addOptions(parser, config)\n    parser.add_argument(\"cwltool\", type=str)\n    parser.add_argument(\"cwljob\", nargs=argparse.REMAINDER)\n\n    # Will override the \"jobStore\" positional argument, enables\n    # user to select jobStore or get a default from logic one below.\n    parser.add_argument(\"--jobStore\", type=str)\n    parser.add_argument(\"--not-strict\", action=\"store_true\")\n    parser.add_argument(\"--quiet\", dest=\"logLevel\", action=\"store_const\",\n                        const=\"ERROR\")\n    parser.add_argument(\"--basedir\", type=str)\n    parser.add_argument(\"--outdir\", type=str, default=os.getcwd())\n    parser.add_argument(\"--version\", action='version', version=baseVersion)\n\n    dockergroup = parser.add_mutually_exclusive_group()\n    dockergroup.add_argument(\n        \"--user-space-docker-cmd\",\n        help=\"(Linux/OS X only) Specify a user space docker command (like \"\n        \"udocker or dx-docker) that will be used to call 'pull' and 'run'\")\n    dockergroup.add_argument(\n        \"--singularity\", action=\"store_true\", default=False,\n        help=\"[experimental] Use Singularity runtime for running containers. \"\n        \"Requires Singularity v2.3.2+ and Linux with kernel version v3.18+ or \"\n        \"with overlayfs support backported.\")\n    dockergroup.add_argument(\n        \"--no-container\", action=\"store_true\", help=\"Do not execute jobs in a \"\n        \"Docker container, even when `DockerRequirement` \"\n        \"is specified under `hints`.\")\n\n    parser.add_argument(\n        \"--preserve-environment\", type=str, nargs='+',\n        help=\"Preserve specified environment variables when running\"\n        \" CommandLineTools\", metavar=(\"VAR1 VAR2\"), default=(\"PATH\",),\n        dest=\"preserve_environment\")\n    parser.add_argument(\n        \"--preserve-entire-environment\", action=\"store_true\",\n        help=\"Preserve all environment variable when running \"\n             \"CommandLineTools.\",\n        default=False, dest=\"preserve_entire_environment\")\n    parser.add_argument(\n        \"--destBucket\", type=str,\n        help=\"Specify a cloud bucket endpoint for output files.\")\n    parser.add_argument(\n        \"--beta-dependency-resolvers-configuration\", default=None)\n    parser.add_argument(\n        \"--beta-dependencies-directory\", default=None)\n    parser.add_argument(\n        \"--beta-use-biocontainers\", default=None, action=\"store_true\")\n    parser.add_argument(\n        \"--beta-conda-dependencies\", default=None, action=\"store_true\")\n    parser.add_argument(\n        \"--tmpdir-prefix\", type=Text,\n        help=\"Path prefix for temporary directories\",\n        default=\"tmp\")\n    parser.add_argument(\n        \"--tmp-outdir-prefix\", type=Text,\n        help=\"Path prefix for intermediate output directories\",\n        default=\"tmp\")\n    parser.add_argument(\n        \"--force-docker-pull\", action=\"store_true\", default=False,\n        dest=\"force_docker_pull\",\n        help=\"Pull latest docker image even if it is locally present\")\n    parser.add_argument(\n        \"--no-match-user\", action=\"store_true\", default=False,\n        help=\"Disable passing the current uid to `docker run --user`\")\n    parser.add_argument(\n        \"--no-read-only\", action=\"store_true\", default=False,\n        help=\"Do not set root directory in the container as read-only\")\n\n    # mkdtemp actually creates the directory, but\n    # toil requires that the directory not exist,\n    # so make it and delete it and allow\n    # toil to create it again (!)\n    workdir = tempfile.mkdtemp()\n    os.rmdir(workdir)\n\n    if args is None:\n        args = sys.argv[1:]\n\n    # we use workdir as jobStore:\n    options = parser.parse_args([workdir] + args)\n\n    # if tmpdir_prefix is not the default value, set workDir too\n    if options.tmpdir_prefix != 'tmp':\n        options.workDir = options.tmpdir_prefix\n\n    if options.provisioner and not options.jobStore:\n        raise NoSuchJobStoreException(\n            'Please specify a jobstore with the --jobStore option when specifying a provisioner.')\n\n    use_container = not options.no_container\n\n    if options.logLevel:\n        cwllogger.setLevel(options.logLevel)\n\n    outdir = os.path.abspath(options.outdir)\n    tmp_outdir_prefix = os.path.abspath(options.tmp_outdir_prefix)\n    tmpdir_prefix = os.path.abspath(options.tmpdir_prefix)\n\n    fileindex = {}\n    existing = {}\n    conf_file = getattr(options,\n                        \"beta_dependency_resolvers_configuration\", None)\n    use_conda_dependencies = getattr(options, \"beta_conda_dependencies\", None)\n    job_script_provider = None\n    if conf_file or use_conda_dependencies:\n        dependencies_configuration = DependenciesConfiguration(options)\n        job_script_provider = dependencies_configuration\n\n    options.default_container = None\n    runtime_context = cwltool.context.RuntimeContext(vars(options))\n    runtime_context.find_default_container = functools.partial(\n        find_default_container, options)\n    runtime_context.workdir = workdir\n    runtime_context.move_outputs = \"leave\"\n    runtime_context.rm_tmpdir = False\n    loading_context = cwltool.context.LoadingContext(vars(options))\n\n    with Toil(options) as toil:\n        if options.restart:\n            outobj = toil.restart()\n        else:\n            loading_context.hints = [{\n                \"class\": \"ResourceRequirement\",\n                \"coresMin\": toil.config.defaultCores,\n                \"ramMin\": toil.config.defaultMemory / (2**20),\n                \"outdirMin\": toil.config.defaultDisk / (2**20),\n                \"tmpdirMin\": 0\n            }]\n            loading_context.construct_tool_object = toil_make_tool\n            loading_context.resolver = cwltool.resolver.tool_resolver\n            loading_context.strict = not options.not_strict\n            options.workflow = options.cwltool\n            options.job_order = options.cwljob\n            uri, tool_file_uri = cwltool.load_tool.resolve_tool_uri(\n                options.cwltool, loading_context.resolver,\n                loading_context.fetcher_constructor)\n            options.tool_help = None\n            options.debug = options.logLevel == \"DEBUG\"\n            job_order_object, options.basedir, jobloader = \\\n                cwltool.main.load_job_order(\n                    options, sys.stdin, loading_context.fetcher_constructor,\n                    loading_context.overrides_list, tool_file_uri)\n            loading_context, workflowobj, uri = cwltool.load_tool.fetch_document(uri, loading_context)\n            loading_context, uri = cwltool.load_tool.resolve_and_validate_document(loading_context, workflowobj, uri)\n            loading_context.overrides_list.extend(loading_context.metadata.get(\"cwltool:overrides\", []))\n            try:\n                tool = cwltool.load_tool.make_tool(uri, loading_context)\n            except cwltool.process.UnsupportedRequirement as err:\n                logging.error(err)\n                return 33\n            runtime_context.secret_store = SecretStore()\n            initialized_job_order = cwltool.main.init_job_order(\n                job_order_object, options, tool, jobloader, sys.stdout,\n                secret_store=runtime_context.secret_store)\n            fs_access = cwltool.stdfsaccess.StdFsAccess(options.basedir)\n            fill_in_defaults(\n                tool.tool[\"inputs\"], initialized_job_order, fs_access)\n\n            def path_to_loc(obj):\n                if \"location\" not in obj and \"path\" in obj:\n                    obj[\"location\"] = obj[\"path\"]\n                    del obj[\"path\"]\n\n            def import_files(tool):\n                visit_class(tool, (\"File\", \"Directory\"), path_to_loc)\n                visit_class(tool, (\"File\", ), functools.partial(\n                    add_sizes, fs_access))\n                normalizeFilesDirs(tool)\n                adjustDirObjs(tool, functools.partial(\n                    get_listing, fs_access, recursive=True))\n                adjustFileObjs(tool, functools.partial(\n                    uploadFile, toil.importFile, fileindex, existing,\n                    skip_broken=True))\n\n            tool.visit(import_files)\n\n            for inp in tool.tool[\"inputs\"]:\n                def set_secondary(fileobj):\n                    if isinstance(fileobj, Mapping) and fileobj.get(\"class\") == \"File\":\n                        if \"secondaryFiles\" not in fileobj:\n                            fileobj[\"secondaryFiles\"] = [{\"location\": cwltool.builder.substitute(fileobj[\"location\"],\n                                                         sf[\"pattern\"]), \"class\": \"File\"}\n                                                         for sf in inp[\"secondaryFiles\"]]\n\n                    if isinstance(fileobj, MutableSequence):\n                        for entry in fileobj:\n                            set_secondary(entry)\n\n                if shortname(inp[\"id\"]) in initialized_job_order and inp.get(\"secondaryFiles\"):\n                    set_secondary(initialized_job_order[shortname(inp[\"id\"])])\n\n            import_files(initialized_job_order)\n            visitSteps(tool, import_files)\n\n            try:\n                runtime_context.use_container = use_container\n                runtime_context.tmpdir = os.path.realpath(tmpdir_prefix)\n                runtime_context.tmp_outdir_prefix = os.path.realpath(\n                    tmp_outdir_prefix)\n                runtime_context.job_script_provider = job_script_provider\n                runtime_context.force_docker_pull = options.force_docker_pull\n                runtime_context.no_match_user = options.no_match_user\n                runtime_context.no_read_only = options.no_read_only\n                (wf1, _) = makeJob(tool, {}, None, runtime_context)\n            except cwltool.process.UnsupportedRequirement as err:\n                logging.error(err)\n                return 33\n\n            wf1.cwljob = initialized_job_order\n            if wf1 is CWLJob:  # Clean up temporary directories only created with CWLJobs.\n                wf1.addFollowOnFn(cleanTempDirs, wf1)\n            outobj = toil.start(wf1)\n\n        outobj = resolve_indirect(outobj)\n\n        # Stage files. Specify destination bucket if specified in CLI\n        # options. If destination bucket not passed in,\n        # options.destBucket's value will be None.\n        toilStageFiles(\n            toil,\n            outobj,\n            outdir,\n            fileindex,\n            existing,\n            export=True,\n            destBucket=options.destBucket)\n\n        if not options.destBucket:\n            visit_class(outobj, (\"File\",), functools.partial(\n                compute_checksums, cwltool.stdfsaccess.StdFsAccess(\"\")))\n\n        visit_class(outobj, (\"File\", ), MutationManager().unset_generation)\n        stdout.write(json.dumps(outobj, indent=4))\n\n    return 0", "response": "Main entry point for toil - cwl - runner."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nevaluate the current object.", "response": "def do_eval(self, inputs, ctx):\n        \"\"\"Evalute ourselves.\"\"\"\n        return cwltool.expression.do_eval(\n            self.expr, inputs, self.req, None, None, {}, context=ctx)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines the final input value.", "response": "def resolve(self):\n        \"\"\"Determine the final input value.\"\"\"\n        if self.source:\n            result = self.source[1][self.source[0]]\n            if result:\n                return result\n        return self.default"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwalks basedir and return all files matching string", "response": "def find(basedir, string):\n    \"\"\"\n    walk basedir and return all files matching string\n    \"\"\"\n    matches = []\n    for root, dirnames, filenames in os.walk(basedir):\n        for filename in fnmatch.filter(filenames, string):\n            matches.append(os.path.join(root, filename))\n    return matches"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind the first file that matches string starting from basedir", "response": "def find_first_match(basedir, string):\n    \"\"\"\n    return the first file that matches string starting from basedir\n    \"\"\"\n    matches = find(basedir, string)\n    return matches[0] if matches else matches"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting the key=val pairs in a LSF config stream to tuples of tokens", "response": "def tokenize_conf_stream(conf_handle):\n    \"\"\"\n    convert the key=val pairs in a LSF config stream to tuples of tokens\n    \"\"\"\n    for line in conf_handle:\n        if line.startswith(\"#\"):\n            continue\n        tokens = line.split(\"=\")\n        if len(tokens) != 2:\n            continue\n        yield (tokens[0].strip(), tokens[1].strip())"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\napplies fn to each line of bparams returning the result", "response": "def apply_bparams(fn):\n    \"\"\"\n    apply fn to each line of bparams, returning the result\n    \"\"\"\n    cmd = [\"bparams\", \"-a\"]\n    try:\n        output = subprocess.check_output(cmd).decode('utf-8')\n    except:\n        return None\n    return fn(output.split(\"\\n\"))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\napplying fn to each line of lsadmin returning the result", "response": "def apply_lsadmin(fn):\n    \"\"\"\n    apply fn to each line of lsadmin, returning the result\n    \"\"\"\n    cmd = [\"lsadmin\", \"showconf\", \"lim\"]\n    try:\n        output = subprocess.check_output(cmd).decode('utf-8')\n    except:\n        return None\n    return fn(output.split(\"\\n\"))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_lsf_units(resource=False):\n    lsf_units = apply_bparams(get_lsf_units_from_stream)\n    if lsf_units:\n        return lsf_units\n\n    lsf_units = apply_lsadmin(get_lsf_units_from_stream)\n    if lsf_units:\n        return lsf_units\n\n    lsf_units = apply_conf_file(get_lsf_units_from_stream, LSF_CONF_FILENAME)\n    if lsf_units:\n        return lsf_units\n\n    # -R usage units are in MB, not KB by default\n    if resource:\n        return DEFAULT_RESOURCE_UNITS\n    else:\n        return DEFAULT_LSF_UNITS", "response": "Get lsf units from lsadmin and lsf. conf files"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if the cluster is configured for reservations to be per core False if it is per job", "response": "def per_core_reservation():\n    \"\"\"\n    returns True if the cluster is configured for reservations to be per core,\n    False if it is per job\n    \"\"\"\n    per_core = apply_bparams(per_core_reserve_from_stream)\n    if per_core:\n        if per_core.upper() == \"Y\":\n            return True\n        else:\n            return False\n\n    per_core = apply_lsadmin(per_core_reserve_from_stream)\n    if per_core:\n        if per_core.upper() == \"Y\":\n            return True\n        else:\n            return False\n\n    per_core = apply_conf_file(per_core_reserve_from_stream, LSB_PARAMS_FILENAME)\n    if per_core and per_core.upper() == \"Y\":\n        return True\n    else:\n        return False\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef addOptions(parser, config=Config()):\n    # Wrapper function that allows toil to be used with both the optparse and\n    # argparse option parsing modules\n    addLoggingOptions(parser)  # This adds the logging stuff.\n    if isinstance(parser, ArgumentParser):\n        def addGroup(headingString, bodyString):\n            return parser.add_argument_group(headingString, bodyString).add_argument\n\n        parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")  # Custom type for arg=True/False.\n        _addOptions(addGroup, config)\n    else:\n        raise RuntimeError(\"Unanticipated class passed to addOptions(), %s. Expecting \"\n                           \"argparse.ArgumentParser\" % parser.__class__)", "response": "Adds toil options to a parser object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn unique ID of the current node.", "response": "def getNodeID():\n    \"\"\"\n    Return unique ID of the current node (host).\n\n    Tries several methods until success. The returned ID should be identical across calls from different processes on\n    the same node at least until the next OS reboot.\n\n    The last resort method is uuid.getnode() that in some rare OS configurations may return a random ID each time it is\n    called. However, this method should never be reached on a Linux system, because reading from\n    /proc/sys/kernel/random/boot_id will be tried prior to that. If uuid.getnode() is reached, it will be called twice,\n    and exception raised if the values are not identical.\n    \"\"\"\n    for idSourceFile in [\"/var/lib/dbus/machine-id\", \"/proc/sys/kernel/random/boot_id\"]:\n        if os.path.exists(idSourceFile):\n            try:\n                with open(idSourceFile, \"r\") as inp:\n                    nodeID = inp.readline().strip()\n            except EnvironmentError:\n                logger.warning((\"Exception when trying to read ID file {}. Will try next method to get node ID.\").format(idSourceFile), exc_info=True)\n            else:\n                if len(nodeID.split()) == 1:\n                    logger.debug(\"Obtained node ID {} from file {}\".format(nodeID, idSourceFile))\n                    break\n                else:\n                    logger.warning((\"Node ID {} from file {} contains spaces. Will try next method to get node ID.\").format(nodeID, idSourceFile))\n    else:\n        nodeIDs = []\n        for i_call in range(2):\n            nodeID = str(uuid.getnode()).strip()\n            if len(nodeID.split()) == 1:\n                nodeIDs.append(nodeID)\n            else:\n                logger.warning(\"Node ID {} from uuid.getnode() contains spaces\".format(nodeID))\n        nodeID = \"\"\n        if len(nodeIDs) == 2:\n            if nodeIDs[0] == nodeIDs[1]:\n                nodeID = nodeIDs[0]\n            else:\n                logger.warning(\n                    \"Different node IDs {} received from repeated calls to uuid.getnode(). You should use \" \\\n                    \"another method to generate node ID.\".format(nodeIDs))\n\n            logger.debug(\"Obtained node ID {} from uuid.getnode()\".format(nodeID))\n    if not nodeID:\n        logger.warning(\n            \"Failed to generate stable node ID, returning empty string. If you see this message with a \" \\\n            \"work dir on a shared file system when using workers running on multiple nodes, you might experience \" \\\n            \"cryptic job failures\")\n    return nodeID"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a list of strings of the form NAME = VALUE or just NAME into a dictionary.", "response": "def parseSetEnv(l):\n    \"\"\"\n    Parses a list of strings of the form \"NAME=VALUE\" or just \"NAME\" into a dictionary. Strings\n    of the latter from will result in dictionary entries whose value is None.\n\n    :type l: list[str]\n    :rtype: dict[str,str]\n\n    >>> parseSetEnv([])\n    {}\n    >>> parseSetEnv(['a'])\n    {'a': None}\n    >>> parseSetEnv(['a='])\n    {'a': ''}\n    >>> parseSetEnv(['a=b'])\n    {'a': 'b'}\n    >>> parseSetEnv(['a=a', 'a=b'])\n    {'a': 'b'}\n    >>> parseSetEnv(['a=b', 'c=d'])\n    {'a': 'b', 'c': 'd'}\n    >>> parseSetEnv(['a=b=c'])\n    {'a': 'b=c'}\n    >>> parseSetEnv([''])\n    Traceback (most recent call last):\n    ...\n    ValueError: Empty name\n    >>> parseSetEnv(['=1'])\n    Traceback (most recent call last):\n    ...\n    ValueError: Empty name\n    \"\"\"\n    d = dict()\n    for i in l:\n        try:\n            k, v = i.split('=', 1)\n        except ValueError:\n            k, v = i, None\n        if not k:\n            raise ValueError('Empty name')\n        d[k] = v\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getFileSystemSize(dirPath):\n    assert os.path.exists(dirPath)\n    diskStats = os.statvfs(dirPath)\n    freeSpace = diskStats.f_frsize * diskStats.f_bavail\n    diskSize = diskStats.f_frsize * diskStats.f_blocks\n    return freeSpace, diskSize", "response": "Returns the free space and total size of the file system hosting dirPath."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setOptions(self, options):\n        from toil.lib.humanize import human2bytes  # This import is used to convert\n        # from human readable quantites to integers\n        def setOption(varName, parsingFn=None, checkFn=None, default=None):\n            # If options object has the option \"varName\" specified\n            # then set the \"varName\" attrib to this value in the config object\n            x = getattr(options, varName, None)\n            if x is None:\n                x = default\n\n            if x is not None:\n                if parsingFn is not None:\n                    x = parsingFn(x)\n                if checkFn is not None:\n                    try:\n                        checkFn(x)\n                    except AssertionError:\n                        raise RuntimeError(\"The %s option has an invalid value: %s\"\n                                           % (varName, x))\n                setattr(self, varName, x)\n\n        # Function to parse integer from string expressed in different formats\n        h2b = lambda x: human2bytes(str(x))\n\n        def parseJobStore(s):\n            name, rest = Toil.parseLocator(s)\n            if name == 'file':\n                # We need to resolve relative paths early, on the leader, because the worker process\n                # may have a different working directory than the leader, e.g. under Mesos.\n                return Toil.buildLocator(name, os.path.abspath(rest))\n            else:\n                return s\n\n        def parseStrList(s):\n            s = s.split(\",\")\n            s = [str(x) for x in s]\n            return s\n\n        def parseIntList(s):\n            s = s.split(\",\")\n            s = [int(x) for x in s]\n            return s\n\n        # Core options\n        setOption(\"jobStore\", parsingFn=parseJobStore)\n        # TODO: LOG LEVEL STRING\n        setOption(\"workDir\")\n        if self.workDir is not None:\n            self.workDir = os.path.abspath(self.workDir)\n            if not os.path.exists(self.workDir):\n                raise RuntimeError(\"The path provided to --workDir (%s) does not exist.\"\n                                   % self.workDir)\n        setOption(\"stats\")\n        setOption(\"cleanWorkDir\")\n        setOption(\"clean\")\n        if self.stats:\n            if self.clean != \"never\" and self.clean is not None:\n                raise RuntimeError(\"Contradicting options passed: Clean flag is set to %s \"\n                                   \"despite the stats flag requiring \"\n                                   \"the jobStore to be intact at the end of the run. \"\n                                   \"Set clean to \\'never\\'\" % self.clean)\n            self.clean = \"never\"\n        elif self.clean is None:\n            self.clean = \"onSuccess\"\n        setOption('clusterStats')\n        setOption(\"restart\")\n\n        # Batch system options\n        setOption(\"batchSystem\")\n        setBatchOptions(self, setOption)\n        setOption(\"disableAutoDeployment\")\n        setOption(\"scale\", float, fC(0.0))\n        setOption(\"mesosMasterAddress\")\n        setOption(\"parasolCommand\")\n        setOption(\"parasolMaxBatches\", int, iC(1))\n        setOption(\"linkImports\")\n        setOption(\"environment\", parseSetEnv)\n\n        # Autoscaling options\n        setOption(\"provisioner\")\n        setOption(\"nodeTypes\", parseStrList)\n        setOption(\"nodeOptions\")\n        setOption(\"minNodes\", parseIntList)\n        setOption(\"maxNodes\", parseIntList)\n        setOption(\"targetTime\", int)\n        if self.targetTime <= 0:\n            raise RuntimeError('targetTime (%s) must be a positive integer!'\n                               '' % self.targetTime)\n        setOption(\"betaInertia\", float)\n        if not 0.0 <= self.betaInertia <= 0.9:\n            raise RuntimeError('betaInertia (%f) must be between 0.0 and 0.9!'\n                               '' % self.betaInertia)\n        setOption(\"scaleInterval\", float)\n        setOption(\"metrics\")\n        setOption(\"preemptableCompensation\", float)\n        if not 0.0 <= self.preemptableCompensation <= 1.0:\n            raise RuntimeError('preemptableCompensation (%f) must be between 0.0 and 1.0!'\n                               '' % self.preemptableCompensation)\n        setOption(\"nodeStorage\", int)\n\n        # Parameters to limit service jobs / detect deadlocks\n        setOption(\"maxServiceJobs\", int)\n        setOption(\"maxPreemptableServiceJobs\", int)\n        setOption(\"deadlockWait\", int)\n        setOption(\"statePollingWait\", int)\n\n        # Resource requirements\n        setOption(\"defaultMemory\", h2b, iC(1))\n        setOption(\"defaultCores\", float, fC(1.0))\n        setOption(\"defaultDisk\", h2b, iC(1))\n        setOption(\"readGlobalFileMutableByDefault\")\n        setOption(\"maxCores\", int, iC(1))\n        setOption(\"maxMemory\", h2b, iC(1))\n        setOption(\"maxDisk\", h2b, iC(1))\n        setOption(\"defaultPreemptable\")\n\n        # Retrying/rescuing jobs\n        setOption(\"retryCount\", int, iC(1))\n        setOption(\"maxJobDuration\", int, iC(1))\n        setOption(\"rescueJobsFrequency\", int, iC(1))\n\n        # Misc\n        setOption(\"maxLocalJobs\", int)\n        setOption(\"disableCaching\")\n        setOption(\"disableChaining\")\n        setOption(\"maxLogFileSize\", h2b, iC(1))\n        setOption(\"writeLogs\")\n        setOption(\"writeLogsGzip\")\n        setOption(\"runCwlInternalJobsOnWorkers\")\n\n        def checkSse(sseKey):\n            with open(sseKey) as f:\n                assert (len(f.readline().rstrip()) == 32)\n\n        setOption(\"sseKey\", checkFn=checkSse)\n        setOption(\"cseKey\", checkFn=checkSse)\n        setOption(\"servicePollingInterval\", float, fC(0.0))\n        setOption(\"forceDockerAppliance\")\n\n        # Debug options\n        setOption(\"debugWorker\")\n        setOption(\"badWorker\", float, fC(0.0, 1.0))\n        setOption(\"badWorkerFailInterval\", float, fC(0.0))", "response": "Sets the options of the current instance to the values in the options object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstarting a new workflow with the given root job.", "response": "def start(self, rootJob):\n        \"\"\"\n        Invoke a Toil workflow with the given job as the root for an initial run. This method\n        must be called in the body of a ``with Toil(...) as toil:`` statement. This method should\n        not be called more than once for a workflow that has not finished.\n\n        :param toil.job.Job rootJob: The root job of the workflow\n        :return: The root job's return value\n        \"\"\"\n        self._assertContextManagerUsed()\n        self.writePIDFile()\n        if self.config.restart:\n            raise ToilRestartException('A Toil workflow can only be started once. Use '\n                                       'Toil.restart() to resume it.')\n\n        self._batchSystem = self.createBatchSystem(self.config)\n        self._setupAutoDeployment(rootJob.getUserScript())\n        try:\n            self._setBatchSystemEnvVars()\n            self._serialiseEnv()\n            self._cacheAllJobs()\n\n            # Pickle the promised return value of the root job, then write the pickled promise to\n            # a shared file, where we can find and unpickle it at the end of the workflow.\n            # Unpickling the promise will automatically substitute the promise for the actual\n            # return value.\n            with self._jobStore.writeSharedFileStream('rootJobReturnValue') as fH:\n                rootJob.prepareForPromiseRegistration(self._jobStore)\n                promise = rootJob.rv()\n                pickle.dump(promise, fH, protocol=pickle.HIGHEST_PROTOCOL)\n\n            # Setup the first wrapper and cache it\n            rootJobGraph = rootJob._serialiseFirstJob(self._jobStore)\n            self._cacheJob(rootJobGraph)\n\n            self._setProvisioner()\n            return self._runMainLoop(rootJobGraph)\n        finally:\n            self._shutdownBatchSystem()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef restart(self):\n        self._assertContextManagerUsed()\n        self.writePIDFile()\n        if not self.config.restart:\n            raise ToilRestartException('A Toil workflow must be initiated with Toil.start(), '\n                                       'not restart().')\n\n        from toil.job import JobException\n        try:\n            self._jobStore.loadRootJob()\n        except JobException:\n            logger.warning(\n                'Requested restart but the workflow has already been completed; allowing exports to rerun.')\n            return self._jobStore.getRootJobReturnValue()\n\n        self._batchSystem = self.createBatchSystem(self.config)\n        self._setupAutoDeployment()\n        try:\n            self._setBatchSystemEnvVars()\n            self._serialiseEnv()\n            self._cacheAllJobs()\n            self._setProvisioner()\n            rootJobGraph = self._jobStore.clean(jobCache=self._jobCache)\n            return self._runMainLoop(rootJobGraph)\n        finally:\n            self._shutdownBatchSystem()", "response": "Restarts a root job."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getJobStore(cls, locator):\n        name, rest = cls.parseLocator(locator)\n        if name == 'file':\n            from toil.jobStores.fileJobStore import FileJobStore\n            return FileJobStore(rest)\n        elif name == 'aws':\n            from toil.jobStores.aws.jobStore import AWSJobStore\n            return AWSJobStore(rest)\n        elif name == 'azure':\n            from toil.jobStores.azureJobStore import AzureJobStore\n            return AzureJobStore(rest)\n        elif name == 'google':\n            from toil.jobStores.googleJobStore import GoogleJobStore\n            return GoogleJobStore(rest)\n        else:\n            raise RuntimeError(\"Unknown job store implementation '%s'\" % name)", "response": "Returns an instance of the concrete job store implementation that matches the given locator."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate an instance of the batch system specified in the given config.", "response": "def createBatchSystem(config):\n        \"\"\"\n        Creates an instance of the batch system specified in the given config.\n\n        :param toil.common.Config config: the current configuration\n\n        :rtype: batchSystems.abstractBatchSystem.AbstractBatchSystem\n\n        :return: an instance of a concrete subclass of AbstractBatchSystem\n        \"\"\"\n        kwargs = dict(config=config,\n                      maxCores=config.maxCores,\n                      maxMemory=config.maxMemory,\n                      maxDisk=config.maxDisk)\n\n        from toil.batchSystems.registry import batchSystemFactoryFor\n\n        try:\n            factory = batchSystemFactoryFor(config.batchSystem)\n            batchSystemClass = factory()\n        except:\n            raise RuntimeError('Unrecognised batch system: %s' % config.batchSystem)\n\n        if not config.disableCaching and not batchSystemClass.supportsWorkerCleanup():\n            raise RuntimeError('%s currently does not support shared caching.  Set the '\n                               '--disableCaching flag if you want to '\n                               'use this batch system.' % config.batchSystem)\n        logger.debug('Using the %s' %\n                    re.sub(\"([a-z])([A-Z])\", \"\\g<1> \\g<2>\", batchSystemClass.__name__).lower())\n\n        return batchSystemClass(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine the user script, save it to the job store and inject a reference to the saved copy into the batch system such that it can auto-deploy the resource on the worker nodes. :param toil.resource.ModuleDescriptor userScript: the module descriptor referencing the user script. If None, it will be looked up in the job store.", "response": "def _setupAutoDeployment(self, userScript=None):\n        \"\"\"\n        Determine the user script, save it to the job store and inject a reference to the saved\n        copy into the batch system such that it can auto-deploy the resource on the worker\n        nodes.\n\n        :param toil.resource.ModuleDescriptor userScript: the module descriptor referencing the\n               user script. If None, it will be looked up in the job store.\n        \"\"\"\n        if userScript is not None:\n            # This branch is hit when a workflow is being started\n            if userScript.belongsToToil:\n                logger.debug('User script %s belongs to Toil. No need to auto-deploy it.', userScript)\n                userScript = None\n            else:\n                if (self._batchSystem.supportsAutoDeployment() and\n                        not self.config.disableAutoDeployment):\n                    # Note that by saving the ModuleDescriptor, and not the Resource we allow for\n                    # redeploying a potentially modified user script on workflow restarts.\n                    with self._jobStore.writeSharedFileStream('userScript') as f:\n                        pickle.dump(userScript, f, protocol=pickle.HIGHEST_PROTOCOL)\n                else:\n                    from toil.batchSystems.singleMachine import SingleMachineBatchSystem\n                    if not isinstance(self._batchSystem, SingleMachineBatchSystem):\n                        logger.warn('Batch system does not support auto-deployment. The user '\n                                    'script %s will have to be present at the same location on '\n                                    'every worker.', userScript)\n                    userScript = None\n        else:\n            # This branch is hit on restarts\n            from toil.jobStores.abstractJobStore import NoSuchFileException\n            try:\n                with self._jobStore.readSharedFileStream('userScript') as f:\n                    userScript = safeUnpickleFromStream(f)\n            except NoSuchFileException:\n                logger.debug('User script neither set explicitly nor present in the job store.')\n                userScript = None\n        if userScript is None:\n            logger.debug('No user script to auto-deploy.')\n        else:\n            logger.debug('Saving user script %s as a resource', userScript)\n            userScriptResource = userScript.saveAsResourceTo(self._jobStore)\n            logger.debug('Injecting user script %s into batch system.', userScriptResource)\n            self._batchSystem.setUserScript(userScriptResource)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nimporting the file at the given URL into the job store.", "response": "def importFile(self, srcUrl, sharedFileName=None):\n        \"\"\"\n        Imports the file at the given URL into job store.\n\n        See :func:`toil.jobStores.abstractJobStore.AbstractJobStore.importFile` for a\n        full description\n        \"\"\"\n        self._assertContextManagerUsed()\n        return self._jobStore.importFile(srcUrl, sharedFileName=sharedFileName)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexports file to destination pointed at by the destination URL.", "response": "def exportFile(self, jobStoreFileID, dstUrl):\n        \"\"\"\n        Exports file to destination pointed at by the destination URL.\n\n        See :func:`toil.jobStores.abstractJobStore.AbstractJobStore.exportFile` for a\n        full description\n        \"\"\"\n        self._assertContextManagerUsed()\n        self._jobStore.exportFile(jobStoreFileID, dstUrl)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _setBatchSystemEnvVars(self):\n        for envDict in (self._jobStore.getEnv(), self.config.environment):\n            for k, v in iteritems(envDict):\n                self._batchSystem.setEnv(k, v)", "response": "Sets the environment variables required by the batch system."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nserialises the environment of this process into a globally accessible pickle file.", "response": "def _serialiseEnv(self):\n        \"\"\"\n        Puts the environment in a globally accessible pickle file.\n        \"\"\"\n        # Dump out the environment of this process in the environment pickle file.\n        with self._jobStore.writeSharedFileStream(\"environment.pickle\") as fileHandle:\n            pickle.dump(dict(os.environ), fileHandle, pickle.HIGHEST_PROTOCOL)\n        logger.debug(\"Written the environment for the jobs to the environment file\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndownload all jobs in the current job store into self. jobCache.", "response": "def _cacheAllJobs(self):\n        \"\"\"\n        Downloads all jobs in the current job store into self.jobCache.\n        \"\"\"\n        logger.debug('Caching all jobs in job store')\n        self._jobCache = {jobGraph.jobStoreID: jobGraph for jobGraph in self._jobStore.jobs()}\n        logger.debug('{} jobs downloaded.'.format(len(self._jobCache)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getWorkflowDir(workflowID, configWorkDir=None):\n        workDir = configWorkDir or os.getenv('TOIL_WORKDIR') or tempfile.gettempdir()\n        if not os.path.exists(workDir):\n            raise RuntimeError(\"The directory specified by --workDir or TOIL_WORKDIR (%s) does not \"\n                               \"exist.\" % workDir)\n        # Create the workflow dir, make it unique to each host in case workDir is on a shared FS.\n        # This prevents workers on different nodes from erasing each other's directories.\n        workflowDir = os.path.join(workDir, 'toil-%s-%s' % (workflowID, getNodeID()))\n        try:\n            # Directory creation is atomic\n            os.mkdir(workflowDir)\n        except OSError as err:\n            if err.errno != 17:\n                # The directory exists if a previous worker set it up.\n                raise\n        else:\n            logger.debug('Created the workflow directory at %s' % workflowDir)\n        return workflowDir", "response": "Returns a path to the directory where the worker directories and the cache will be located."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun the main loop with the given root job.", "response": "def _runMainLoop(self, rootJob):\n        \"\"\"\n        Runs the main loop with the given job.\n        :param toil.job.Job rootJob: The root job for the workflow.\n        :rtype: Any\n        \"\"\"\n        logProcessContext(self.config)\n\n        with RealtimeLogger(self._batchSystem,\n                            level=self.options.logLevel if self.options.realTimeLogging else None):\n            # FIXME: common should not import from leader\n            from toil.leader import Leader\n            return Leader(config=self.config,\n                          batchSystem=self._batchSystem,\n                          provisioner=self._provisioner,\n                          jobStore=self._jobStore,\n                          rootJob=rootJob,\n                          jobCache=self._jobCache).run()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nshut down the current batch system if it has been created.", "response": "def _shutdownBatchSystem(self):\n        \"\"\"\n        Shuts down current batch system if it has been created.\n        \"\"\"\n        assert self._batchSystem is not None\n\n        startTime = time.time()\n        logger.debug('Shutting down batch system ...')\n        self._batchSystem.shutdown()\n        logger.debug('... finished shutting down the batch system in %s seconds.'\n                     % (time.time() - startTime))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting a pid file to the jobstore.", "response": "def writePIDFile(self):\n        \"\"\"\n        Write a the pid of this process to a file in the jobstore.\n\n        Overwriting the current contents of pid.log is a feature, not a bug of this method.\n        Other methods will rely on always having the most current pid available.\n        So far there is no reason to store any old pids.\n        \"\"\"\n        with self._jobStore.writeSharedFileStream('pid.log') as f:\n            f.write(str(os.getpid()).encode('utf-8'))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_asts(self, ast_root, name):\n        '''\n        Finds an AST node with the given name and the entire subtree under it.\n        A function borrowed from scottfrazer.  Thank you Scott Frazer!\n\n        :param ast_root: The WDL AST.  The whole thing generally, but really\n                         any portion that you wish to search.\n        :param name: The name of the subtree you're looking for, like \"Task\".\n        :return: nodes representing the AST subtrees matching the \"name\" given.\n        '''\n        nodes = []\n        if isinstance(ast_root, wdl_parser.AstList):\n            for node in ast_root:\n                nodes.extend(self.find_asts(node, name))\n        elif isinstance(ast_root, wdl_parser.Ast):\n            if ast_root.name == name:\n                nodes.append(ast_root)\n            for attr_name, attr in ast_root.attributes.items():\n                nodes.extend(self.find_asts(attr, name))\n        return nodes", "response": "Finds an AST node with the given name and the entire subtree under it."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dict_from_JSON(self, JSON_file):\n        '''\n        Takes a WDL-mapped json file and creates a dict containing the bindings.\n        The 'return' value is only used for unittests.\n\n        :param JSON_file: A required JSON file containing WDL variable bindings.\n        :return: Returns the self.json_dict purely for unittests.\n        '''\n\n        # TODO: Add context support for variables within multiple wdl files\n        with open(JSON_file) as data_file:\n            data = json.load(data_file)\n        for d in data:\n            if isinstance(data[d], basestring):\n                self.json_dict[d] = '\"' + data[d] + '\"'\n            else:\n                self.json_dict[d] = data[d]\n        return self.json_dict", "response": "Takes a WDL - mapped json file and creates a dict containing the bindings."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_tasks_dict(self, ast):\n        '''\n        Parse each \"Task\" in the AST.  This will create self.tasks_dictionary,\n        where each task name is a key.\n\n        :return: Creates the self.tasks_dictionary necessary for much of the\n        parser.  Returning it is only necessary for unittests.\n        '''\n        tasks = self.find_asts(ast, 'Task')\n        for task in tasks:\n            self.parse_task(task)\n        return self.tasks_dictionary", "response": "Parse each Task in the AST and create a self. tasks_dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_task(self, task):\n        '''\n        Parses a WDL task AST subtree.\n\n        Currently looks at and parses 4 sections:\n        1. Declarations (e.g. string x = 'helloworld')\n        2. Commandline (a bash command with dynamic variables inserted)\n        3. Runtime (docker image; disk; CPU; RAM; etc.)\n        4. Outputs (expected return values/files)\n\n        :param task: An AST subtree of a WDL \"Task\".\n        :return: Returns nothing but adds a task to the self.tasks_dictionary\n        necessary for much of the parser.\n        '''\n\n        task_name = task.attributes[\"name\"].source_string\n\n        # task declarations\n        declaration_array = []\n        for declaration_subAST in task.attr(\"declarations\"):\n            declaration_array.append(self.parse_task_declaration(declaration_subAST))\n            self.tasks_dictionary.setdefault(task_name, OrderedDict())['inputs'] = declaration_array\n\n        for section in task.attr(\"sections\"):\n\n            # task commandline entries section [command(s) to run]\n            if section.name == \"RawCommand\":\n                command_array = self.parse_task_rawcommand(section)\n                self.tasks_dictionary.setdefault(task_name, OrderedDict())['raw_commandline'] = command_array\n\n            # task runtime section (docker image; disk; CPU; RAM; etc.)\n            if section.name == \"Runtime\":\n                runtime_dict = self.parse_task_runtime(section.attr(\"map\"))\n                self.tasks_dictionary.setdefault(task_name, OrderedDict())['runtime'] = runtime_dict\n\n            # task output filenames section (expected return values/files)\n            if section.name == \"Outputs\":\n                output_array = self.parse_task_outputs(section)\n                self.tasks_dictionary.setdefault(task_name, OrderedDict())['outputs'] = output_array", "response": "Parses a WDL task AST subtree and adds it to the self. tasks_dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the declaration section of the WDL task AST subtree.", "response": "def parse_task_declaration(self, declaration_subAST):\n        '''\n        Parses the declaration section of the WDL task AST subtree.\n\n        Examples:\n\n        String my_name\n        String your_name\n        Int two_chains_i_mean_names = 0\n\n        :param declaration_subAST: Some subAST representing a task declaration\n                                   like: 'String file_name'\n        :return: var_name, var_type, var_value\n            Example:\n                Input subAST representing:   'String file_name'\n                Output:  var_name='file_name', var_type='String', var_value=None\n        '''\n        var_name = self.parse_declaration_name(declaration_subAST.attr(\"name\"))\n        var_type = self.parse_declaration_type(declaration_subAST.attr(\"type\"))\n        var_expressn = self.parse_declaration_expressn(declaration_subAST.attr(\"expression\"), es='')\n\n        return (var_name, var_type, var_expressn)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_task_rawcommand(self, rawcommand_subAST):\n        '''\n        Parses the rawcommand section of the WDL task AST subtree.\n\n        Task \"rawcommands\" are divided into many parts.  There are 2 types of\n        parts: normal strings, & variables that can serve as changeable inputs.\n\n        The following example command:\n            'echo ${variable1} ${variable2} > output_file.txt'\n\n        Has 5 parts:\n                     Normal  String: 'echo '\n                     Variable Input: variable1\n                     Normal  String: ' '\n                     Variable Input: variable2\n                     Normal  String: ' > output_file.txt'\n\n        Variables can also have additional conditions, like 'sep', which is like\n        the python ''.join() function and in WDL looks like: ${sep=\" -V \" GVCFs}\n        and would be translated as: ' -V '.join(GVCFs).\n\n        :param rawcommand_subAST: A subAST representing some bash command.\n        :return: A list=[] of tuples=() representing the parts of the command:\n             e.g. [(command_var, command_type, additional_conditions_list), ...]\n                  Where: command_var = 'GVCFs'\n                         command_type = 'variable'\n                         command_actions = {'sep': ' -V '}\n        '''\n        command_array = []\n        for code_snippet in rawcommand_subAST.attributes[\"parts\"]:\n\n            # normal string\n            if isinstance(code_snippet, wdl_parser.Terminal):\n                command_var = \"r'''\" + code_snippet.source_string + \"'''\"\n\n            # a variable like ${dinosaurDNA}\n            if isinstance(code_snippet, wdl_parser.Ast):\n                if code_snippet.name == 'CommandParameter':\n                    # change in the future?  seems to be a different parameter but works for all cases it seems?\n                    code_expr = self.parse_declaration_expressn(code_snippet.attr('expr'), es='')\n                    code_attributes = self.parse_task_rawcommand_attributes(code_snippet.attr('attributes'))\n                    command_var = self.modify_cmd_expr_w_attributes(code_expr, code_attributes)\n\n            if isinstance(code_snippet, wdl_parser.AstList):\n                raise NotImplementedError\n            command_array.append(command_var)\n\n        return command_array", "response": "Parses the rawcommand section of the WDL task AST subtree."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_task_runtime(self, runtime_subAST):\n        '''\n        Parses the runtime section of the WDL task AST subtree.\n\n        The task \"runtime\" section currently supports context fields for a\n        docker container, CPU resources, RAM resources, and disk resources.\n\n        :param runtime_subAST: A subAST representing runtime parameters.\n        :return: A list=[] of runtime attributes, for example:\n                 runtime_attributes = [('docker','quay.io/encode-dcc/map:v1.0'),\n                                       ('cpu','2'),\n                                       ('memory','17.1 GB'),\n                                       ('disks','local-disk 420 HDD')]\n        '''\n        runtime_attributes = OrderedDict()\n        if isinstance(runtime_subAST, wdl_parser.Terminal):\n            raise NotImplementedError\n        elif isinstance(runtime_subAST, wdl_parser.Ast):\n            raise NotImplementedError\n        elif isinstance(runtime_subAST, wdl_parser.AstList):\n            for ast in runtime_subAST:\n                key = self.parse_task_runtime_key(ast.attr('key'))\n                value = self.parse_declaration_expressn(ast.attr('value'), es='')\n                if value.startswith('\"'):\n                    value = self.translate_wdl_string_to_python_string(value[1:-1])\n                runtime_attributes[key] = value\n        return runtime_attributes", "response": "Parses the task runtime section of the WDL task AST subtree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_task_outputs(self, i):\n        '''\n        Parse the WDL output section.\n\n        Outputs are like declarations, with a type, name, and value.  Examples:\n\n        ------------\n        Simple Cases\n        ------------\n\n        'Int num = 7'\n            var_name: 'num'\n            var_type: 'Int'\n            var_value: 7\n\n        String idea = 'Lab grown golden eagle burgers.'\n            var_name: 'idea'\n            var_type: 'String'\n            var_value: 'Lab grown golden eagle burgers.'\n\n        File ideaFile = 'goldenEagleStemCellStartUpDisrupt.txt'\n            var_name: 'ideaFile'\n            var_type: 'File'\n            var_value: 'goldenEagleStemCellStartUpDisrupt.txt'\n\n        -------------------\n        More Abstract Cases\n        -------------------\n\n        Array[File] allOfMyTerribleIdeas = glob(*.txt)[0]\n            var_name:      'allOfMyTerribleIdeas'\n            var_type**:    'File'\n            var_value:     [*.txt]\n            var_actions:   {'index_lookup': '0', 'glob': 'None'}\n\n        **toilwdl.py converts 'Array[File]' to 'ArrayFile'\n\n        :return: output_array representing outputs generated by the job/task:\n                e.g. x = [(var_name, var_type, var_value, var_actions), ...]\n        '''\n        output_array = []\n        for j in i.attributes['attributes']:\n            if j.name == 'Output':\n                var_name = self.parse_declaration_name(j.attr(\"name\"))\n                var_type = self.parse_declaration_type(j.attr(\"type\"))\n                var_expressn = self.parse_declaration_expressn(j.attr(\"expression\"), es='', output_expressn=True)\n                if not (var_expressn.startswith('(') and var_expressn.endswith(')')):\n                    var_expressn = self.translate_wdl_string_to_python_string(var_expressn)\n                output_array.append((var_name, var_type, var_expressn))\n            else:\n                raise NotImplementedError\n        return output_array", "response": "Parses the WDL output section of the WDL file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntranslate a WDL string into a string that can be read into a python file.", "response": "def translate_wdl_string_to_python_string(self, some_string):\n        '''\n        Parses a string representing a given job's output filename into something\n        python can read.  Replaces ${string}'s with normal variables and the rest\n        with normal strings all concatenated with ' + '.\n\n        Will not work with additional parameters, such as:\n        ${default=\"foo\" bar}\n        or\n        ${true=\"foo\" false=\"bar\" Boolean baz}\n\n        This method expects to be passed only strings with some combination of\n        \"${abc}\" and \"abc\" blocks.\n\n        :param job: A list such that:\n                        (job priority #, job ID #, Job Skeleton Name, Job Alias)\n        :param some_string: e.g. '${sampleName}.vcf'\n        :return: output_string, e.g. 'sampleName + \".vcf\"'\n        '''\n\n        try:\n            # add support for 'sep'\n            output_string = ''\n            edited_string = some_string.strip()\n\n            if edited_string.find('${') != -1:\n                continue_loop = True\n                while (continue_loop):\n                    index_start = edited_string.find('${')\n                    index_end = edited_string.find('}', index_start)\n\n                    stringword = edited_string[:index_start]\n\n                    if index_start != 0:\n                        output_string = output_string + \"'\" + stringword + \"' + \"\n\n                    keyword = edited_string[index_start + 2:index_end]\n                    output_string = output_string + \"str(\" + keyword + \") + \"\n\n                    edited_string = edited_string[index_end + 1:]\n                    if edited_string.find('${') == -1:\n                        continue_loop = False\n                        if edited_string:\n                            output_string = output_string + \"'\" + edited_string + \"' + \"\n            else:\n                output_string = \"'\" + edited_string + \"'\"\n\n            if output_string.endswith(' + '):\n                output_string = output_string[:-3]\n\n            return output_string\n        except:\n            return ''"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse each Workflow in the AST and create the self. workflows_dictionary.", "response": "def create_workflows_dict(self, ast):\n        '''\n        Parse each \"Workflow\" in the AST.  This will create self.workflows_dictionary,\n        where each called job is a tuple key of the form: (priority#, job#, name, alias).\n\n        :return: Creates the self.workflows_dictionary necessary for much of the\n        parser.  Returning it is only necessary for unittests.\n        '''\n        workflows = self.find_asts(ast, 'Workflow')\n        for workflow in workflows:\n            self.parse_workflow(workflow)\n        return self.workflows_dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a WDL workflow AST subtree and creates the self. workflows_dictionary.", "response": "def parse_workflow(self, workflow):\n        '''\n        Parses a WDL workflow AST subtree.\n\n        Currently looks at and parses 3 sections:\n        1. Declarations (e.g. string x = 'helloworld')\n        2. Calls (similar to a python def)\n        3. Scatter (which expects to map to a Call or multiple Calls)\n\n        Returns nothing but creates the self.workflows_dictionary necessary for much\n        of the parser.\n\n        :param workflow: An AST subtree of a WDL \"Workflow\".\n        :return: Returns nothing but adds a workflow to the\n                 self.workflows_dictionary necessary for much of the parser.\n        '''\n        workflow_name = workflow.attr('name').source_string\n\n        wf_declared_dict = OrderedDict()\n        for section in workflow.attr(\"body\"):\n\n            if section.name == \"Declaration\":\n                var_name, var_map = self.parse_workflow_declaration(section)\n                wf_declared_dict[var_name] = var_map\n            self.workflows_dictionary.setdefault(workflow_name, OrderedDict())['wf_declarations'] = wf_declared_dict\n\n            if section.name == \"Scatter\":\n                scattertask = self.parse_workflow_scatter(section)\n                self.workflows_dictionary.setdefault(workflow_name, OrderedDict())['scatter' + str(self.scatter_number)] = scattertask\n                self.scatter_number += 1\n\n            if section.name == \"Call\":\n                task = self.parse_workflow_call(section)\n                self.workflows_dictionary.setdefault(workflow_name, OrderedDict())['call' + str(self.call_number)] = task\n                self.call_number += 1\n\n            if section.name == \"If\":\n                task = self.parse_workflow_if(section)\n                self.workflows_dictionary.setdefault(workflow_name, OrderedDict())['if' + str(self.if_number)] = task\n                self.if_number += 1"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the declaration name of the workflow function.", "response": "def parse_declaration_name(self, nameAST):\n        \"\"\"\n        Required.\n\n        Nothing fancy here.  Just the name of the workflow\n        function.  For example: \"rnaseqexample\" would be the following\n        wdl workflow's name:\n\n        workflow rnaseqexample {File y; call a {inputs: y}; call b;}\n        task a {File y}\n        task b {command{\"echo 'ATCG'\"}}\n\n        :param nameAST:\n        :return:\n        \"\"\"\n        if isinstance(nameAST, wdl_parser.Terminal):\n            return nameAST.source_string\n        elif isinstance(nameAST, wdl_parser.Ast):\n            return nameAST.source_string\n        elif isinstance(nameAST, wdl_parser.AstList):\n            raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_declaration_type(self, typeAST):\n        if isinstance(typeAST, wdl_parser.Terminal):\n            return typeAST.source_string\n        elif isinstance(typeAST, wdl_parser.Ast):\n            if typeAST.name == 'Type':\n                return self.parse_declaration_type(typeAST.attr('subtype'))\n            elif typeAST.name == 'OptionalType':\n                return self.parse_declaration_type(typeAST.attr('innerType'))\n            else:\n                raise NotImplementedError\n        elif isinstance(typeAST, wdl_parser.AstList):\n            for ast in typeAST:\n                # TODO only ever seen one element lists.\n                return self.parse_declaration_type(ast)", "response": "Parses the type AST node and returns the corresponding value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_declaration_expressn(self, expressionAST, es, output_expressn=False):\n        if not expressionAST:\n            return None\n        else:\n            if isinstance(expressionAST, wdl_parser.Terminal):\n                if expressionAST.str == 'boolean':\n                    if expressionAST.source_string == 'false':\n                        return 'False'\n                    elif expressionAST.source_string == 'true':\n                        return 'True'\n                    else:\n                        raise TypeError('Parsed boolean ({}) must be expressed as \"true\" or \"false\".'\n                                        ''.format(expressionAST.source_string))\n                elif expressionAST.str == 'string' and not output_expressn:\n                    parsed_string = self.translate_wdl_string_to_python_string(expressionAST.source_string)\n                    return '{string}'.format(string=parsed_string)\n                else:\n                    return '{string}'.format(string=expressionAST.source_string)\n            elif isinstance(expressionAST, wdl_parser.Ast):\n                if expressionAST.name == 'Add':\n                    es = es + self.parse_declaration_expressn_operator(expressionAST.attr('lhs'),\n                                                                       expressionAST.attr('rhs'),\n                                                                       es,\n                                                                       operator=' + ')\n                elif expressionAST.name == 'Subtract':\n                    es = es + self.parse_declaration_expressn_operator(expressionAST.attr('lhs'),\n                                                                       expressionAST.attr('rhs'),\n                                                                       es,\n                                                                       operator=' - ')\n                elif expressionAST.name == 'Multiply':\n                    es = es + self.parse_declaration_expressn_operator(expressionAST.attr('lhs'),\n                                                                       expressionAST.attr('rhs'),\n                                                                       es,\n                                                                       operator=' * ')\n                elif expressionAST.name == 'Divide':\n                    es = es + self.parse_declaration_expressn_operator(expressionAST.attr('lhs'),\n                                                                       expressionAST.attr('rhs'),\n                                                                       es,\n                                                                       operator=' / ')\n                elif expressionAST.name == 'GreaterThan':\n                    es = es + self.parse_declaration_expressn_operator(expressionAST.attr('lhs'),\n                                                                       expressionAST.attr('rhs'),\n                                                                       es,\n                                                                       operator=' > ')\n                elif expressionAST.name == 'LessThan':\n                    es = es + self.parse_declaration_expressn_operator(expressionAST.attr('lhs'),\n                                                                       expressionAST.attr('rhs'),\n                                                                       es,\n                                                                       operator=' < ')\n                elif expressionAST.name == 'FunctionCall':\n                    es = es + self.parse_declaration_expressn_fncall(expressionAST.attr('name'),\n                                                                     expressionAST.attr('params'),\n                                                                     es)\n                elif expressionAST.name == 'TernaryIf':\n                    es = es + self.parse_declaration_expressn_ternaryif(expressionAST.attr('cond'),\n                                                                        expressionAST.attr('iftrue'),\n                                                                        expressionAST.attr('iffalse'),\n                                                                        es)\n                elif expressionAST.name == 'MemberAccess':\n                    es = es + self.parse_declaration_expressn_memberaccess(expressionAST.attr('lhs'),\n                                                                           expressionAST.attr('rhs'),\n                                                                           es)\n                elif expressionAST.name == 'ArrayLiteral':\n                    es = es + self.parse_declaration_expressn_arrayliteral(expressionAST.attr('values'),\n                                                                           es)\n                elif expressionAST.name == 'TupleLiteral':\n                    es = es + self.parse_declaration_expressn_tupleliteral(expressionAST.attr('values'),\n                                                                           es)\n                elif expressionAST.name == 'ArrayOrMapLookup':\n                    es = es + self.parse_declaration_expressn_arraymaplookup(expressionAST.attr('lhs'),\n                                                                             expressionAST.attr('rhs'),\n                                                                             es)\n                elif expressionAST.name == 'LogicalNot':\n                    es = es + self.parse_declaration_expressn_logicalnot(expressionAST.attr('expression'),\n                                                                         es)\n                else:\n                    raise NotImplementedError\n            elif isinstance(expressionAST, wdl_parser.AstList):\n                raise NotImplementedError\n            return '(' + es + ')'", "response": "Parses the expressionAST and returns the corresponding expression in the workflow."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the declaration of a memberaccess expression.", "response": "def parse_declaration_expressn_memberaccess(self, lhsAST, rhsAST, es):\n        \"\"\"\n        Instead of \"Class.variablename\", use \"Class.rv('variablename')\".\n\n        :param lhsAST:\n        :param rhsAST:\n        :param es:\n        :return:\n        \"\"\"\n        if isinstance(lhsAST, wdl_parser.Terminal):\n            es = es + lhsAST.source_string\n        elif isinstance(lhsAST, wdl_parser.Ast):\n            raise NotImplementedError\n        elif isinstance(lhsAST, wdl_parser.AstList):\n            raise NotImplementedError\n\n        es = es + '_'\n\n        if isinstance(rhsAST, wdl_parser.Terminal):\n            es = es + rhsAST.source_string\n        elif isinstance(rhsAST, wdl_parser.Ast):\n            raise NotImplementedError\n        elif isinstance(rhsAST, wdl_parser.AstList):\n            raise NotImplementedError\n\n        return es"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_declaration_expressn_ternaryif(self, cond, iftrue, iffalse, es):\n        es = es + self.parse_declaration_expressn(iftrue, es='')\n        es = es + ' if ' + self.parse_declaration_expressn(cond, es='')\n        es = es + ' else ' + self.parse_declaration_expressn(iffalse, es='')\n        return es", "response": "This function parses the declaration of an external if statement."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_declaration_expressn_tupleliteral(self, values, es):\n        es = es + '('\n        for ast in values:\n            es = es + self.parse_declaration_expressn(ast, es='') + ', '\n        if es.endswith(', '):\n            es = es[:-2]\n        return es + ')'", "response": "Same in python. Just a parenthesis enclosed tuple."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the declaration of a logical entry point.", "response": "def parse_declaration_expressn_operator(self, lhsAST, rhsAST, es, operator):\n        \"\"\"\n        Simply joins the left and right hand arguments lhs and rhs with an operator.\n\n        :param lhsAST:\n        :param rhsAST:\n        :param es:\n        :param operator:\n        :return:\n        \"\"\"\n        if isinstance(lhsAST, wdl_parser.Terminal):\n            if lhsAST.str == 'string':\n                es = es + '\"{string}\"'.format(string=lhsAST.source_string)\n            else:\n                es = es + '{string}'.format(string=lhsAST.source_string)\n        elif isinstance(lhsAST, wdl_parser.Ast):\n            es = es + self.parse_declaration_expressn(lhsAST, es='')\n        elif isinstance(lhsAST, wdl_parser.AstList):\n            raise NotImplementedError\n\n        es = es + operator\n\n        if isinstance(rhsAST, wdl_parser.Terminal):\n            if rhsAST.str == 'string':\n                es = es + '\"{string}\"'.format(string=rhsAST.source_string)\n            else:\n                es = es + '{string}'.format(string=rhsAST.source_string)\n        elif isinstance(rhsAST, wdl_parser.Ast):\n            es = es + self.parse_declaration_expressn(rhsAST, es='')\n        elif isinstance(rhsAST, wdl_parser.AstList):\n            raise NotImplementedError\n        return es"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse out a cromwell s built - in function call.", "response": "def parse_declaration_expressn_fncall(self, name, params, es):\n        \"\"\"\n        Parses out cromwell's built-in function calls.\n\n        Some of these are special\n        and need minor adjustments, for example length(), which is equivalent to\n        python's len() function.  Or sub, which is equivalent to re.sub(), but\n        needs a rearrangement of input variables.\n\n        Known to be supported: sub, size, read_tsv, length, select_first.\n\n        :param name:\n        :param params:\n        :param es:\n        :return:\n        \"\"\"\n        # name of the function\n        if isinstance(name, wdl_parser.Terminal):\n            if name.str:\n                # use python's built-in for length()\n                if name.source_string == 'length':\n                    es = es + 'len('\n                elif name.source_string == 'stdout':\n                    return es + 'stdout'\n                else:\n                    es = es + name.source_string + '('\n            else:\n                raise NotImplementedError\n        elif isinstance(name, wdl_parser.Ast):\n            raise NotImplementedError\n        elif isinstance(name, wdl_parser.AstList):\n            raise NotImplementedError\n\n        # use python's re.sub() for sub()\n        if name.source_string == 'sub':\n            es_params = self.parse_declaration_expressn_fncall_SUBparams(params)\n        else:\n            es_params = self.parse_declaration_expressn_fncall_normalparams(params)\n\n        if name.source_string == 'glob':\n            return es + es_params + ', tempDir)'\n        elif name.source_string == 'size':\n            return es + es_params + ', fileStore=fileStore)'\n        else:\n            return es + es_params + ')'"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the arguments passed to the sub function.", "response": "def parse_declaration_expressn_fncall_SUBparams(self, params):\n        \"\"\"\n        Needs rearrangement:\n\n        0 1 2\n        WDL native params: sub(input, pattern, replace)\n\n        1 2 0\n        Python's re.sub() params: sub(pattern, replace, input)\n\n        :param params:\n        :param es:\n        :return:\n        \"\"\"\n        # arguments passed to the function\n        if isinstance(params, wdl_parser.Terminal):\n            raise NotImplementedError\n        elif isinstance(params, wdl_parser.Ast):\n            raise NotImplementedError\n        elif isinstance(params, wdl_parser.AstList):\n            assert len(params) == 3, ('sub() function requires exactly 3 arguments.')\n            es_params0 = self.parse_declaration_expressn(params[0], es='')\n            es_params1 = self.parse_declaration_expressn(params[1], es='')\n            es_params2 = self.parse_declaration_expressn(params[2], es='')\n            return es_params1 + ', ' + es_params2 + ', ' + es_params0"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a WDL declaration AST subtree into a string and a python dictionary containing its type and value.", "response": "def parse_workflow_declaration(self, wf_declaration_subAST):\n        '''\n        Parses a WDL declaration AST subtree into a string and a python\n        dictionary containing its 'type' and 'value'.\n\n        For example:\n        var_name = refIndex\n        var_map = {'type': File,\n                   'value': bamIndex}\n\n        :param wf_declaration_subAST: An AST subtree of a workflow declaration.\n        :return: var_name, which is the name of the declared variable\n        :return: var_map, a dictionary with keys for type and value.\n                          e.g. {'type': File, 'value': bamIndex}\n        '''\n        var_map = OrderedDict()\n        var_name = self.parse_declaration_name(wf_declaration_subAST.attr(\"name\"))\n        var_type = self.parse_declaration_type(wf_declaration_subAST.attr(\"type\"))\n        var_expressn = self.parse_declaration_expressn(wf_declaration_subAST.attr(\"expression\"), es='')\n\n        var_map['name'] = var_name\n        var_map['type'] = var_type\n        var_map['value'] = var_expressn\n\n        return var_name, var_map"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_workflow_call_body_declarations(self, i):\n        declaration_array = []\n        if isinstance(i, wdl_parser.Terminal):\n            declaration_array = [i.source_string]\n        elif isinstance(i, wdl_parser.Ast):\n            raise NotImplementedError\n        elif isinstance(i, wdl_parser.AstList):\n            for ast in i:\n                declaration_array.append(self.parse_task_declaration(ast))\n\n        # have not seen this used so raise to check\n        if declaration_array:\n            raise NotImplementedError\n\n        return declaration_array", "response": "Parses the workflow call body declarations."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_workflow_call_body_io(self, i):\n        if isinstance(i, wdl_parser.Terminal):\n            raise NotImplementedError\n        elif isinstance(i, wdl_parser.Ast):\n            raise NotImplementedError\n        elif isinstance(i, wdl_parser.AstList):\n            for ast in i:\n                assert len(i) == 1\n                if ast.name == 'Inputs':\n                    return self.parse_workflow_call_body_io_map(ast.attr('map'))\n                else:\n                    raise NotImplementedError", "response": "Parse the workflow call body IO."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the IOMapping element in the workflow call body.", "response": "def parse_workflow_call_body_io_map(self, i):\n        \"\"\"\n        Required.\n\n        :param i:\n        :return:\n        \"\"\"\n        io_map = OrderedDict()\n        if isinstance(i, wdl_parser.Terminal):\n            raise NotImplementedError\n        elif isinstance(i, wdl_parser.Ast):\n            raise NotImplementedError\n        elif isinstance(i, wdl_parser.AstList):\n            for ast in i:\n                if ast.name == 'IOMapping':\n                    key = self.parse_declaration_expressn(ast.attr(\"key\"), es='')\n                    value = self.parse_declaration_expressn(ast.attr(\"value\"), es='')\n                    io_map[key] = value\n                else:\n                    raise NotImplementedError\n        return io_map"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the call body of a workflow call.", "response": "def parse_workflow_call_body(self, i):\n        \"\"\"\n        Required.\n\n        :param i:\n        :return:\n        \"\"\"\n        io_map = OrderedDict()\n\n        if isinstance(i, wdl_parser.Terminal):\n            return i.source_string # no io mappings; represents just a blank call\n        elif isinstance(i, wdl_parser.Ast):\n            if i.name == 'CallBody':\n                declarations = self.parse_workflow_call_body_declarations(i.attr(\"declarations\")) # have not seen this used\n                io_map = self.parse_workflow_call_body_io(i.attr('io'))\n            else:\n                raise NotImplementedError\n        elif isinstance(i, wdl_parser.AstList):\n            raise NotImplementedError\n\n        return io_map"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a WDL workflow call AST subtree to give the variable mappings for that particular job and task call.", "response": "def parse_workflow_call(self, i):\n        '''\n        Parses a WDL workflow call AST subtree to give the variable mappings for\n        that particular job/task \"call\".\n\n        :param i: WDL workflow job object\n        :return: python dictionary of io mappings for that job call\n        '''\n        task_being_called = self.parse_workflow_call_taskname(i.attr(\"task\"))\n        task_alias = self.parse_workflow_call_taskalias(i.attr(\"alias\"))\n        io_map = self.parse_workflow_call_body(i.attr(\"body\"))\n\n        if not task_alias:\n            task_alias = task_being_called\n\n        return {'task': task_being_called, 'alias': task_alias, 'io': io_map}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _registered(self):\n        # The weird mapping of the SDB item attribute value to the property value is due to\n        # backwards compatibility. 'True' becomes True, that's easy. Toil < 3.3.0 writes this at\n        # the end of job store creation. Absence of either the registry, the item or the\n        # attribute becomes False, representing a truly absent, non-existing job store. An\n        # attribute value of 'False', which is what Toil < 3.3.0 writes at the *beginning* of job\n        # store destruction, indicates a job store in transition, reflecting the fact that 3.3.0\n        # may leak buckets or domains even though the registry reports 'False' for them. We\n        # can't handle job stores that were partially created by 3.3.0, though.\n        registry_domain = self._bindDomain(domain_name='toil-registry',\n                                           create=False,\n                                           block=False)\n        if registry_domain is None:\n            return False\n        else:\n            for attempt in retry_sdb():\n                with attempt:\n                    attributes = registry_domain.get_attributes(item_name=self.namePrefix,\n                                                                attribute_name='exists',\n                                                                consistent_read=True)\n                    try:\n                        exists = attributes['exists']\n                    except KeyError:\n                        return False\n                    else:\n                        if exists == 'True':\n                            return True\n                        elif exists == 'False':\n                            return None\n                        else:\n                            assert False", "response": "Returns True if the job store is registered False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _getKeyForUrl(url, existing=None):\n        # Get the bucket's region to avoid a redirect per request\n        try:\n            with closing(boto.connect_s3()) as s3:\n                location = s3.get_bucket(url.netloc).get_location()\n                region = bucket_location_to_region(location)\n        except S3ResponseError as e:\n            if e.error_code == 'AccessDenied':\n                s3 = boto.connect_s3()\n            else:\n                raise\n        else:\n            # Note that caller is responsible for closing the connection\n            s3 = boto.s3.connect_to_region(region)\n\n        try:\n            keyName = url.path[1:]\n            bucketName = url.netloc\n            bucket = s3.get_bucket(bucketName)\n            key = bucket.get_key(keyName.encode('utf-8'))\n            if existing is True:\n                if key is None:\n                    raise RuntimeError(\"Key '%s' does not exist in bucket '%s'.\" %\n                                       (keyName, bucketName))\n            elif existing is False:\n                if key is not None:\n                    raise RuntimeError(\"Key '%s' exists in bucket '%s'.\" %\n                                       (keyName, bucketName))\n            elif existing is None:\n                pass\n            else:\n                assert False\n            if key is None:\n                key = bucket.new_key(keyName)\n        except:\n            with panic():\n                s3.close()\n        else:\n            return key", "response": "This method extracts a key from a given s3 url."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _connectSimpleDB(self):\n        db = boto.sdb.connect_to_region(self.region)\n        if db is None:\n            raise ValueError(\"Could not connect to SimpleDB. Make sure '%s' is a valid SimpleDB \"\n                             \"region.\" % self.region)\n        assert db is not None\n        monkeyPatchSdbConnection(db)\n        return db", "response": "Connect to the SimpleDB and return a connection object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _connectS3(self):\n        s3 = boto.s3.connect_to_region(self.region)\n        if s3 is None:\n            raise ValueError(\"Could not connect to S3. Make sure '%s' is a valid S3 region.\" %\n                             self.region)\n        return s3", "response": "Connect to S3 and return a connection object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _bindBucket(self, bucket_name, create=False, block=True, versioning=False):\n        assert self.minBucketNameLen <= len(bucket_name) <= self.maxBucketNameLen\n        assert self.bucketNameRe.match(bucket_name)\n        log.debug(\"Binding to job store bucket '%s'.\", bucket_name)\n\n        def bucket_creation_pending(e):\n            # https://github.com/BD2KGenomics/toil/issues/955\n            # https://github.com/BD2KGenomics/toil/issues/995\n            # https://github.com/BD2KGenomics/toil/issues/1093\n            return (isinstance(e, (S3CreateError, S3ResponseError))\n                    and e.error_code in ('BucketAlreadyOwnedByYou', 'OperationAborted'))\n\n        bucketExisted = True\n        for attempt in retry_s3(predicate=bucket_creation_pending):\n            with attempt:\n                try:\n                    bucket = self.s3.get_bucket(bucket_name, validate=True)\n                except S3ResponseError as e:\n                    if e.error_code == 'NoSuchBucket':\n                        bucketExisted = False\n                        log.debug(\"Bucket '%s' does not exist.\", bucket_name)\n                        if create:\n                            log.debug(\"Creating bucket '%s'.\", bucket_name)\n                            location = region_to_bucket_location(self.region)\n                            bucket = self.s3.create_bucket(bucket_name, location=location)\n                            assert self.__getBucketRegion(bucket) == self.region\n                        elif block:\n                            raise\n                        else:\n                            return None\n                    elif e.status == 301:\n                        # This is raised if the user attempts to get a bucket in a region outside\n                        # the specified one, if the specified one is not `us-east-1`.  The us-east-1\n                        # server allows a user to use buckets from any region.\n                        bucket = self.s3.get_bucket(bucket_name, validate=False)\n                        raise BucketLocationConflictException(self.__getBucketRegion(bucket))\n                    else:\n                        raise\n                else:\n                    if self.__getBucketRegion(bucket) != self.region:\n                        raise BucketLocationConflictException(self.__getBucketRegion(bucket))\n                if versioning and not bucketExisted:\n                    # only call this method on bucket creation\n                    bucket.configure_versioning(True)\n                else:\n                    # now test for versioning consistency\n                    # we should never see any of these errors since 'versioning' should always be true\n                    bucket_versioning = self.__getBucketVersioning(bucket)\n                    if bucket_versioning != versioning:\n                        assert False, 'Cannot modify versioning on existing bucket'\n                    elif bucket_versioning is None:\n                        assert False, 'Cannot use a bucket with versioning suspended'\n                if bucketExisted:\n                    log.debug(\"Using pre-existing job store bucket '%s'.\", bucket_name)\n                else:\n                    log.debug(\"Created new job store bucket '%s'.\", bucket_name)\n\n                return bucket", "response": "Bind a bucket to the job store."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbinding a domain to the job store.", "response": "def _bindDomain(self, domain_name, create=False, block=True):\n        \"\"\"\n        Return the Boto Domain object representing the SDB domain of the given name. If the\n        domain does not exist and `create` is True, it will be created.\n\n        :param str domain_name: the name of the domain to bind to\n\n        :param bool create: True if domain should be created if it doesn't exist\n\n        :param bool block: If False, return None if the domain doesn't exist. If True, wait until\n               domain appears. This parameter is ignored if create is True.\n\n        :rtype: Domain|None\n        :raises SDBResponseError: If `block` is True and the domain still doesn't exist after the\n                retry timeout expires.\n        \"\"\"\n        log.debug(\"Binding to job store domain '%s'.\", domain_name)\n        retryargs = dict(predicate=lambda e: no_such_sdb_domain(e) or sdb_unavailable(e))\n        if not block:\n            retryargs['timeout'] = 15\n        for attempt in retry_sdb(**retryargs):\n            with attempt:\n                try:\n                    return self.db.get_domain(domain_name)\n                except SDBResponseError as e:\n                    if no_such_sdb_domain(e):\n                        if create:\n                            return self.db.create_domain(domain_name)\n                        elif block:\n                            raise\n                        else:\n                            return None\n                    else:\n                        raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the versioning status of a bucket.", "response": "def __getBucketVersioning(self, bucket):\n        \"\"\"\n        For newly created buckets get_versioning_status returns an empty dict. In the past we've\n        seen None in this case. We map both to a return value of False.\n\n        Otherwise, the 'Versioning' entry in the dictionary returned by get_versioning_status can\n        be 'Enabled', 'Suspended' or 'Disabled' which we map to True, None and False\n        respectively. Note that we've never seen a versioning status of 'Disabled', only the\n        empty dictionary. Calling configure_versioning with False on a bucket will cause\n        get_versioning_status to then return 'Suspended' even on a new bucket that never had\n        versioning enabled.\n        \"\"\"\n        for attempt in retry_s3():\n            with attempt:\n                status = bucket.get_versioning_status()\n                return self.versionings[status['Versioning']] if status else False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nflatten an iterable except for string elements.", "response": "def flatten( iterables ):\n    \"\"\" Flatten an iterable, except for string elements. \"\"\"\n    for it in iterables:\n        if isinstance(it, str):\n            yield it\n        else:\n            for element in it:\n                yield element"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef checkResourceRequest(self, memory, cores, disk):\n        assert memory is not None\n        assert disk is not None\n        assert cores is not None\n        if cores > self.maxCores:\n            raise InsufficientSystemResources('cores', cores, self.maxCores)\n        if memory > self.maxMemory:\n            raise InsufficientSystemResources('memory', memory, self.maxMemory)\n        if disk > self.maxDisk:\n            raise InsufficientSystemResources('disk', disk, self.maxDisk)", "response": "Check if a resource request is not greater than that available or allowed."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting an environment variable for the worker process before it is launched.", "response": "def setEnv(self, name, value=None):\n        \"\"\"\n        Set an environment variable for the worker process before it is launched. The worker\n        process will typically inherit the environment of the machine it is running on but this\n        method makes it possible to override specific variables in that inherited environment\n        before the worker is launched. Note that this mechanism is different to the one used by\n        the worker internally to set up the environment of a job. A call to this method affects\n        all jobs issued after this method returns. Note to implementors: This means that you\n        would typically need to copy the variables before enqueuing a job.\n\n        If no value is provided it will be looked up from the current environment.\n\n        NB: Only the Mesos and single-machine batch systems support passing environment\n        variables. On other batch systems, this method has no effect. See\n        https://github.com/BD2KGenomics/toil/issues/547.\n\n        :param str name: the environment variable to be set on the worker.\n\n        :param str value: if given, the environment variable given by name will be set to this value.\n               if None, the variable's current value will be used as the value on the worker\n\n        :raise RuntimeError: if value is None and the name cannot be found in the environment\n        \"\"\"\n        if value is None:\n            try:\n                value = os.environ[name]\n            except KeyError:\n                raise RuntimeError(\"%s does not exist in current environment\", name)\n        self.environment[name] = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef workerCleanup(info):\n        assert isinstance(info, WorkerCleanupInfo)\n        workflowDir = Toil.getWorkflowDir(info.workflowID, info.workDir)\n        workflowDirContents = os.listdir(workflowDir)\n        shutdownFileStore(workflowDir, info.workflowID)\n        if (info.cleanWorkDir == 'always'\n            or info.cleanWorkDir in ('onSuccess', 'onError')\n            and workflowDirContents in ([], [cacheDirName(info.workflowID)])):\n            shutil.rmtree(workflowDir)", "response": "Cleans up the worker node on batch system shutdown."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nhandle the local job.", "response": "def handleLocalJob(self, jobNode):  # type: (JobNode) -> Optional[int]\n        \"\"\"\n        To be called by issueBatchJobs.\n\n        Returns the jobID if the jobNode has been submitted to the local queue,\n        otherwise returns None\n        \"\"\"\n        if (not self.config.runCwlInternalJobsOnWorkers\n                and jobNode.jobName.startswith(CWL_INTERNAL_JOBS)):\n            return self.localBatch.issueBatchJob(jobNode)\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getNextJobID(self):  # type: () -> int\n        with self.localBatch.jobIndexLock:\n            jobID = self.localBatch.jobIndex\n            self.localBatch.jobIndex += 1\n        return jobID", "response": "Returns the next available job ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_sqs(cls, sqs_message):\n        sns_message = json.loads(sqs_message.get_body())\n        return Message.from_sns(sns_message['Message'])", "response": "Creates a Message object from a SQS message"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef shutdownFileStore(workflowDir, workflowID):\n    cacheDir = os.path.join(workflowDir, cacheDirName(workflowID))\n    if os.path.exists(cacheDir):\n        # The presence of the cacheDir suggests this was a cached run. We don't need the cache lock\n        # for any of this since this is the final cleanup of a job and there should be  no other\n        # conflicting processes using the cache.\n        CachingFileStore.shutdown(cacheDir)\n    else:\n        # This absence of cacheDir suggests otherwise.\n        NonCachingFileStore.shutdown(workflowDir)", "response": "Shuts down the file store for the given workflow."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new instance of this class with the given callable and arguments.", "response": "def create(cls, function, *args, **kwargs):\n        \"\"\"\n        Capture the given callable and arguments as an instance of this class.\n\n        :param callable function: The deferred action to take in the form of a function\n        :param tuple args: Non-keyword arguments to the function\n        :param dict kwargs: Keyword arguments to the function\n        \"\"\"\n        # The general principle is to deserialize as late as possible, i.e. when the function is\n        # to be invoked, as that will avoid redundantly deserializing deferred functions for\n        # concurrently running jobs when the cache state is loaded from disk. By implication we\n        # should serialize as early as possible. We need to serialize the function as well as its\n        # arguments.\n        return cls(*list(map(dill.dumps, (function, args, kwargs))),\n                   name=function.__name__,\n                   module=ModuleDescriptor.forModule(function.__module__).globalize())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef invoke(self):\n        logger.debug('Running deferred function %s.', self)\n        self.module.makeLoadable()\n        function, args, kwargs = list(map(dill.loads, (self.function, self.args, self.kwargs)))\n        return function(*args, **kwargs)", "response": "Invoke the captured function with the captured arguments."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getLocalTempDir(self):\n        return os.path.abspath(tempfile.mkdtemp(prefix=\"t\", dir=self.localTempDir))", "response": "Returns the absolute path to a new local temporary directory that will be used to write files that persist for the duration of\n clf."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getLocalTempFile(self):\n        handle, tmpFile = tempfile.mkstemp(prefix=\"tmp\", suffix=\".tmp\", dir=self.localTempDir)\n        os.close(handle)\n        return os.path.abspath(tmpFile)", "response": "Returns a new local temporary file that will persist for the duration of the job."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef readGlobalFile(self, fileStoreID, userPath=None, cache=True, mutable=False, symlink=False):\n        raise NotImplementedError()", "response": "Reads a global file from the file store."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nresolve the absolute path to the given file.", "response": "def _resolveAbsoluteLocalPath(self, filePath):\n        \"\"\"\n        Return the absolute path to filePath.  This is a wrapper for os.path.abspath because mac OS\n        symlinks /tmp and /var (the most common places for a default tempdir) to /private/tmp and\n        /private/var respectively.\n\n        :param str filePath: The absolute or relative path to the file. If relative, it must be\n               relative to the local temp working dir\n        :return: Absolute path to key\n        :rtype: str\n        \"\"\"\n        if os.path.isabs(filePath):\n            return os.path.abspath(filePath)\n        else:\n            return os.path.join(self.localTempDir, filePath)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninvokes the specified deferred functions and return a list of names of functions that raised an exception while being invoked.", "response": "def _runDeferredFunctions(deferredFunctions):\n        \"\"\"\n        Invoke the specified deferred functions and return a list of names of functions that\n        raised an exception while being invoked.\n\n        :param list[DeferredFunction] deferredFunctions: the DeferredFunctions to run\n        :rtype: list[str]\n        \"\"\"\n        failures = []\n        for deferredFunction in deferredFunctions:\n            try:\n                deferredFunction.invoke()\n            except:\n                failures.append(deferredFunction.name)\n                logger.exception('%s failed.', deferredFunction)\n        return failures"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending a logging message to the leader.", "response": "def logToMaster(self, text, level=logging.INFO):\n        \"\"\"\n        Send a logging message to the leader. The message will also be \\\n        logged by the worker at the same level.\n\n        :param text: The string to log.\n        :param int level: The logging level.\n        \"\"\"\n        logger.log(level=level, msg=(\"LOG-TO-MASTER: \" + text))\n        self.loggingMessages.append(dict(text=text, level=level))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _pidExists(pid):\n        assert pid > 0\n        try:\n            os.kill(pid, 0)\n        except OSError as err:\n            if err.errno == errno.ESRCH:\n                # ESRCH == No such process\n                return False\n            else:\n                raise\n        else:\n            return True", "response": "This will return True if the process associated with pid is still running on the machine False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting a file to the global file store.", "response": "def writeGlobalFile(self, localFileName, cleanup=False):\n        \"\"\"\n        Takes a file (as a path) and uploads it to the job store.  Depending on the jobstore\n        used, carry out the appropriate cache functions.\n        \"\"\"\n        absLocalFileName = self._resolveAbsoluteLocalPath(localFileName)\n        # What does this do?\n        cleanupID = None if not cleanup else self.jobGraph.jobStoreID\n        # If the file is from the scope of local temp dir\n        if absLocalFileName.startswith(self.localTempDir):\n            # If the job store is of type FileJobStore and the job store and the local temp dir\n            # are on the same file system, then we want to hard link the files instead of copying\n            # barring the case where the file being written was one that was previously read\n            # from the file store. In that case, you want to copy to the file store so that\n            # the two have distinct nlink counts.\n            # Can read without a lock because we're only reading job-specific info.\n            jobSpecificFiles = list(self._CacheState._load(self.cacheStateFile).jobState[\n                self.jobID]['filesToFSIDs'].keys())\n            # Saying nlink is 2 implicitly means we are using the job file store, and it is on\n            # the same device as the work dir.\n            if self.nlinkThreshold == 2 and absLocalFileName not in jobSpecificFiles:\n                jobStoreFileID = self.jobStore.getEmptyFileStoreID(cleanupID)\n                # getEmptyFileStoreID creates the file in the scope of the job store hence we\n                # need to delete it before linking.\n                os.remove(self.jobStore._getAbsPath(jobStoreFileID))\n                os.link(absLocalFileName, self.jobStore._getAbsPath(jobStoreFileID))\n            # If they're not on the file system, or if the file is already linked with an\n            # existing file, we need to copy to the job store.\n            # Check if the user allows asynchronous file writes\n            elif self.jobStore.config.useAsync:\n                jobStoreFileID = self.jobStore.getEmptyFileStoreID(cleanupID)\n                # Before we can start the async process, we should also create a dummy harbinger\n                # file in the cache such that any subsequent jobs asking for this file will not\n                # attempt to download it from the job store till the write is complete.  We do\n                # this now instead of in the writing thread because there is an edge case where\n                # readGlobalFile in a subsequent job is called before the writing thread has\n                # received the message to write the file and has created the dummy harbinger\n                # (and the file was unable to be cached/was evicted from the cache).\n                harbingerFile = self.HarbingerFile(self, fileStoreID=jobStoreFileID)\n                harbingerFile.write()\n                fileHandle = open(absLocalFileName, 'rb')\n                with self._pendingFileWritesLock:\n                    self._pendingFileWrites.add(jobStoreFileID)\n                # A file handle added to the queue allows the asyncWrite threads to remove their\n                # jobID from _pendingFileWrites. Therefore, a file should only be added after\n                # its fileID is added to _pendingFileWrites\n                self.queue.put((fileHandle, jobStoreFileID))\n            # Else write directly to the job store.\n            else:\n                jobStoreFileID = self.jobStore.writeFile(absLocalFileName, cleanupID)\n            # Local files are cached by default, unless they were written from previously read\n            # files.\n            if absLocalFileName not in jobSpecificFiles:\n                self.addToCache(absLocalFileName, jobStoreFileID, 'write')\n            else:\n                self._JobState.updateJobSpecificFiles(self, jobStoreFileID, absLocalFileName,\n                                                      0.0, False)\n        # Else write directly to the job store.\n        else:\n            jobStoreFileID = self.jobStore.writeFile(absLocalFileName, cleanupID)\n            # Non local files are NOT cached by default, but they are tracked as local files.\n            self._JobState.updateJobSpecificFiles(self, jobStoreFileID, None,\n                                                  0.0, False)\n        return FileID.forPath(jobStoreFileID, absLocalFileName)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading a file from the file store and stores it in the local temp directory.", "response": "def readGlobalFile(self, fileStoreID, userPath=None, cache=True, mutable=False, symlink=False):\n        \"\"\"\n        Downloads a file described by fileStoreID from the file store to the local directory.\n        The function first looks for the file in the cache and if found, it hardlinks to the\n        cached copy instead of downloading.\n\n        The cache parameter will be used only if the file isn't already in the cache, and\n        provided user path (if specified) is in the scope of local temp dir.\n\n        :param bool cache: If True, a copy of the file will be saved into a cache that can be\n               used by other workers. caching supports multiple concurrent workers requesting the\n               same file by allowing only one to download the file while the others wait for it to\n               complete.\n        :param bool mutable: If True, the file path returned points to a file that is\n               modifiable by the user. Using False is recommended as it saves disk by making\n               multiple workers share a file via hard links. The default is False.\n        \"\"\"\n        # Check that the file hasn't been deleted by the user\n        if fileStoreID in self.filesToDelete:\n            raise RuntimeError('Trying to access a file in the jobStore you\\'ve deleted: ' + \\\n                               '%s' % fileStoreID)\n        # Get the name of the file as it would be in the cache\n        cachedFileName = self.encodedFileID(fileStoreID)\n        # setup the harbinger variable for the file.  This is an identifier that the file is\n        # currently being downloaded by another job and will be in the cache shortly. It is used\n        # to prevent multiple jobs from simultaneously downloading the same file from the file\n        # store.\n        harbingerFile = self.HarbingerFile(self, cachedFileName=cachedFileName)\n        # setup the output filename.  If a name is provided, use it - This makes it a Named\n        # Local File. If a name isn't provided, use the base64 encoded name such that we can\n        # easily identify the files later on.\n        if userPath is not None:\n            localFilePath = self._resolveAbsoluteLocalPath(userPath)\n            if os.path.exists(localFilePath):\n                # yes, this is illegal now.\n                raise RuntimeError(' File %s ' % localFilePath + ' exists. Cannot Overwrite.')\n            fileIsLocal = True if localFilePath.startswith(self.localTempDir) else False\n        else:\n            localFilePath = self.getLocalTempFileName()\n            fileIsLocal = True\n        # First check whether the file is in cache.  If it is, then hardlink the file to\n        # userPath. Cache operations can only occur on local files.\n        with self.cacheLock() as lockFileHandle:\n            if fileIsLocal and self._fileIsCached(fileStoreID):\n                logger.debug('CACHE: Cache hit on file with ID \\'%s\\'.' % fileStoreID)\n                assert not os.path.exists(localFilePath)\n                if mutable:\n                    shutil.copyfile(cachedFileName, localFilePath)\n                    cacheInfo = self._CacheState._load(self.cacheStateFile)\n                    jobState = self._JobState(cacheInfo.jobState[self.jobID])\n                    jobState.addToJobSpecFiles(fileStoreID, localFilePath, -1, None)\n                    cacheInfo.jobState[self.jobID] = jobState.__dict__\n                    cacheInfo.write(self.cacheStateFile)\n                else:\n                    os.link(cachedFileName, localFilePath)\n                    self.returnFileSize(fileStoreID, localFilePath, lockFileHandle,\n                                        fileAlreadyCached=True)\n            # If the file is not in cache, check whether the .harbinger file for the given\n            # FileStoreID exists.  If it does, the wait and periodically check for the removal\n            # of the file and the addition of the completed download into cache of the file by\n            # the other job. Then we link to it.\n            elif fileIsLocal and harbingerFile.exists():\n                harbingerFile.waitOnDownload(lockFileHandle)\n                # If the code reaches here, the harbinger file has been removed. This means\n                # either the file was successfully downloaded and added to cache, or something\n                # failed. To prevent code duplication, we recursively call readGlobalFile.\n                flock(lockFileHandle, LOCK_UN)\n                return self.readGlobalFile(fileStoreID, userPath=userPath, cache=cache,\n                                           mutable=mutable)\n            # If the file is not in cache, then download it to the userPath and then add to\n            # cache if specified.\n            else:\n                logger.debug('CACHE: Cache miss on file with ID \\'%s\\'.' % fileStoreID)\n                if fileIsLocal and cache:\n                    # If caching of the downloaded file is desired, First create the harbinger\n                    # file so other jobs know not to redundantly download the same file.  Write\n                    # the PID of this process into the file so other jobs know who is carrying\n                    # out the download.\n                    harbingerFile.write()\n                    # Now release the file lock while the file is downloaded as download could\n                    # take a while.\n                    flock(lockFileHandle, LOCK_UN)\n                    # Use try:finally: so that the .harbinger file is removed whether the\n                    # download succeeds or not.\n                    try:\n                        self.jobStore.readFile(fileStoreID,\n                                               '/.'.join(os.path.split(cachedFileName)),\n                                               symlink=False)\n                    except:\n                        if os.path.exists('/.'.join(os.path.split(cachedFileName))):\n                            os.remove('/.'.join(os.path.split(cachedFileName)))\n                        raise\n                    else:\n                        # If the download succeded, officially add the file to cache (by\n                        # recording it in the cache lock file) if possible.\n                        if os.path.exists('/.'.join(os.path.split(cachedFileName))):\n                            os.rename('/.'.join(os.path.split(cachedFileName)), cachedFileName)\n                            # If this is not true we get into trouble in our internal reference counting.\n                            assert(os.stat(cachedFileName).st_nlink == self.nlinkThreshold)\n                            self.addToCache(localFilePath, fileStoreID, 'read', mutable)\n                            # We don't need to return the file size here because addToCache\n                            # already does it for us\n                    finally:\n                        # In any case, delete the harbinger file.\n                        harbingerFile.delete()\n                else:\n                    # Release the cache lock since the remaining stuff is not cache related.\n                    flock(lockFileHandle, LOCK_UN)\n                    self.jobStore.readFile(fileStoreID, localFilePath, symlink=False)\n                    # Make sure we got a file with the number of links we expect.\n                    # If this is not true we get into trouble in our internal reference counting.\n                    assert(os.stat(localFilePath).st_nlink == self.nlinkThreshold)\n                    os.chmod(localFilePath, stat.S_IRUSR | stat.S_IRGRP | stat.S_IROTH)\n                    # Now that we have the file, we have 2 options. It's modifiable or not.\n                    # Either way, we need to account for FileJobStore making links instead of\n                    # copies.\n                    if mutable:\n                        if self.nlinkThreshold == 2:\n                            # nlinkThreshold can only be 1 or 2 and it can only be 2 iff the\n                            # job store is FilejobStore, and the job store and local temp dir\n                            # are on the same device. An atomic rename removes the nlink on the\n                            # file handle linked from the job store.\n                            shutil.copyfile(localFilePath, localFilePath + '.tmp')\n                            os.rename(localFilePath + '.tmp', localFilePath)\n                        self._JobState.updateJobSpecificFiles(self, fileStoreID, localFilePath,\n                                                              -1, False)\n                    # If it was immutable\n                    else:\n                        if self.nlinkThreshold == 2:\n                            self._accountForNlinkEquals2(localFilePath)\n                        self._JobState.updateJobSpecificFiles(self, fileStoreID, localFilePath,\n                                                              0.0, False)\n        return localFilePath"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cacheLock(self):\n        cacheLockFile = open(self.cacheLockFile, 'w')\n        try:\n            flock(cacheLockFile, LOCK_EX)\n            logger.debug(\"CACHE: Obtained lock on file %s\" % self.cacheLockFile)\n            yield cacheLockFile\n        except IOError:\n            logger.critical('CACHE: Unable to acquire lock on %s' % self.cacheLockFile)\n            raise\n        finally:\n            cacheLockFile.close()\n            logger.debug(\"CACHE: Released lock\")", "response": "A context manager that acquires a lock on the Lock file that will be used to prevent synchronous cache operations between workers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate the cache lock file for the cache on the node.", "response": "def _createCacheLockFile(self, tempCacheDir):\n        \"\"\"\n        Create the cache lock file file to contain the state of the cache on the node.\n\n        :param str tempCacheDir: Temporary directory to use for setting up a cache lock file the\n               first time.\n        \"\"\"\n        # The nlink threshold is setup along with the first instance of the cache class on the\n        # node. It needs the cache dir to sniff link count for files form the job store.\n        self.setNlinkThreshold(tempCacheDir)\n        # Get the free space on the device\n        freeSpace, _ = getFileSystemSize(tempCacheDir)\n        # Create the cache lock file.\n        open(os.path.join(tempCacheDir, os.path.basename(self.cacheLockFile)), 'w').close()\n        # Setup the cache state file\n        personalCacheStateFile = os.path.join(tempCacheDir,\n                                              os.path.basename(self.cacheStateFile))\n        # Setup the initial values for the cache state file in a dict\n        cacheInfo = self._CacheState({\n            'nlink': self.nlinkThreshold,\n            'attemptNumber': self.workflowAttemptNumber,\n            'total': freeSpace,\n            'cached': 0,\n            'sigmaJob': 0,\n            'cacheDir': self.localCacheDir,\n            'jobState': {}})\n        cacheInfo.write(personalCacheStateFile)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef encodedFileID(self, jobStoreFileID):\n        \n        base64Text = base64.urlsafe_b64encode(jobStoreFileID.encode('utf-8')).decode('utf-8')\n        \n        outCachedFile = os.path.join(self.localCacheDir, base64Text)\n        return outCachedFile", "response": "This method encodes the jobStoreFileID into a base64 encoded file name and returns the path to the hashed file in the local cache folder."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndecode a cached fileName back to a job store file ID.", "response": "def decodedFileID(self, cachedFilePath):\n        \"\"\"\n        Decode a cached fileName back to a job store file ID.\n\n        :param str cachedFilePath: Path to the cached file\n        :return: The jobstore file ID associated with the file\n        :rtype: str\n        \"\"\"\n        fileDir, fileName = os.path.split(cachedFilePath)\n        assert fileDir == self.localCacheDir, 'Can\\'t decode uncached file names'\n        # We encode and decode here because base64 can't work with unencoded text\n        # Its probably worth, later, converting all file name variables to bytes\n        # and not text.\n        return base64.urlsafe_b64decode(fileName.encode('utf-8')).decode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef addToCache(self, localFilePath, jobStoreFileID, callingFunc, mutable=False):\n        assert callingFunc in ('read', 'write')\n        with self.cacheLock() as lockFileHandle:\n            cachedFile = self.encodedFileID(jobStoreFileID)\n            # The file to be cached MUST originate in the environment of the TOIL temp directory\n            if (os.stat(self.localCacheDir).st_dev !=\n                    os.stat(os.path.dirname(localFilePath)).st_dev):\n                raise InvalidSourceCacheError('Attempting to cache a file across file systems '\n                                              'cachedir = %s, file = %s.' % (self.localCacheDir,\n                                                                             localFilePath))\n            if not localFilePath.startswith(self.localTempDir):\n                raise InvalidSourceCacheError('Attempting a cache operation on a non-local file '\n                                              '%s.' % localFilePath)\n            if callingFunc == 'read' and mutable:\n                shutil.copyfile(cachedFile, localFilePath)\n                fileSize = os.stat(cachedFile).st_size\n                cacheInfo = self._CacheState._load(self.cacheStateFile)\n                cacheInfo.cached += fileSize if cacheInfo.nlink != 2 else 0\n                if not cacheInfo.isBalanced():\n                    os.remove(cachedFile)\n                    cacheInfo.cached -= fileSize if cacheInfo.nlink != 2 else 0\n                    logger.debug('Could not download both download ' +\n                                 '%s as mutable and add to ' % os.path.basename(localFilePath) +\n                                 'cache. Hence only mutable copy retained.')\n                else:\n                    logger.debug('CACHE: Added file with ID \\'%s\\' to the cache.' %\n                                jobStoreFileID)\n                jobState = self._JobState(cacheInfo.jobState[self.jobID])\n                jobState.addToJobSpecFiles(jobStoreFileID, localFilePath, -1, False)\n                cacheInfo.jobState[self.jobID] = jobState.__dict__\n                cacheInfo.write(self.cacheStateFile)\n            else:\n                # There are two possibilities, read and immutable, and write. both cases do\n                # almost the same thing except for the direction of the os.link hence we're\n                # writing them together.\n                if callingFunc == 'read':  # and mutable is inherently False\n                    src = cachedFile\n                    dest = localFilePath\n                    # To mirror behaviour of shutil.copyfile\n                    if os.path.exists(dest):\n                        os.remove(dest)\n                else:  # write\n                    src = localFilePath\n                    dest = cachedFile\n                try:\n                    os.link(src, dest)\n                except OSError as err:\n                    if err.errno != errno.EEXIST:\n                        raise\n                    # If we get the EEXIST error, it can only be from write since in read we are\n                    # explicitly deleting the file.  This shouldn't happen with the .partial\n                    # logic hence we raise a cache error.\n                    raise CacheError('Attempting to recache a file %s.' % src)\n                else:\n                    # Chmod the cached file. Cached files can never be modified.\n                    os.chmod(cachedFile, stat.S_IRUSR | stat.S_IRGRP | stat.S_IROTH)\n                    # Return the filesize of cachedFile to the job and increase the cached size\n                    # The values passed here don't matter since rFS looks at the file only for\n                    # the stat\n                    self.returnFileSize(jobStoreFileID, localFilePath, lockFileHandle,\n                                        fileAlreadyCached=False)\n                if callingFunc == 'read':\n                    logger.debug('CACHE: Read file with ID \\'%s\\' from the cache.' %\n                                 jobStoreFileID)\n                else:\n                    logger.debug('CACHE: Added file with ID \\'%s\\' to the cache.' %\n                                 jobStoreFileID)", "response": "Add a file to the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncleans up the cache for a given number of files allowed in the current job.", "response": "def cleanCache(self, newJobReqs):\n        \"\"\"\n        Cleanup all files in the cache directory to ensure that at lead newJobReqs are available\n        for use.\n\n        :param float newJobReqs: the total number of bytes of files allowed in the cache.\n        \"\"\"\n        with self._CacheState.open(self) as cacheInfo:\n            # Add the new job's disk requirements to the sigmaJobDisk variable\n            cacheInfo.sigmaJob += newJobReqs\n            # Initialize the job state here. we use a partial in the jobSpecificFiles call so\n            # that this entire thing is pickleable. Based on answer by user Nathaniel Gentile at\n            # http://stackoverflow.com/questions/2600790\n            assert self.jobID not in cacheInfo.jobState\n            cacheInfo.jobState[self.jobID] = {\n                'jobName': self.jobName,\n                'jobReqs': newJobReqs,\n                'jobDir': self.localTempDir,\n                'jobSpecificFiles': defaultdict(partial(defaultdict,int)),\n                'filesToFSIDs': defaultdict(set),\n                'pid': os.getpid(),\n                'deferredFunctions': []}\n            # If the caching equation is balanced, do nothing.\n            if cacheInfo.isBalanced():\n                return None\n\n            # List of deletable cached files.  A deletable cache file is one\n            #  that is not in use by any other worker (identified by the number of symlinks to\n            # the file)\n            allCacheFiles = [os.path.join(self.localCacheDir, x)\n                             for x in os.listdir(self.localCacheDir)\n                             if not self._isHidden(x)]\n            allCacheFiles = [(path, os.stat(path)) for path in allCacheFiles]\n            # TODO mtime vs ctime\n            deletableCacheFiles = {(path, inode.st_mtime, inode.st_size)\n                                   for path, inode in allCacheFiles\n                                   if inode.st_nlink == self.nlinkThreshold}\n\n            # Sort in descending order of mtime so the first items to be popped from the list\n            # are the least recently created.\n            deletableCacheFiles = sorted(deletableCacheFiles, key=lambda x: (-x[1], -x[2]))\n            logger.debug('CACHE: Need %s bytes for new job. Detecting an estimated %s (out of a '\n                         'total %s) bytes available for running the new job. The size of the cache '\n                         'is %s bytes.', newJobReqs,\n                         (cacheInfo.total - (cacheInfo.cached + cacheInfo.sigmaJob - newJobReqs)),\n                         cacheInfo.total, cacheInfo.cached)\n            logger.debug('CACHE: Evicting files to make room for the new job.')\n\n            # Now do the actual file removal\n            totalEvicted = 0\n            while not cacheInfo.isBalanced() and len(deletableCacheFiles) > 0:\n                cachedFile, fileCreateTime, cachedFileSize = deletableCacheFiles.pop()\n                os.remove(cachedFile)\n                cacheInfo.cached -= cachedFileSize if self.nlinkThreshold != 2 else 0\n                totalEvicted += cachedFileSize\n                assert cacheInfo.cached >= 0\n                logger.debug('CACHE: Evicted  file with ID \\'%s\\' (%s bytes)' %\n                             (self.decodedFileID(cachedFile), cachedFileSize))\n            logger.debug('CACHE: Evicted a total of %s bytes. Available space is now %s bytes.',\n                         totalEvicted,\n                         (cacheInfo.total - (cacheInfo.cached + cacheInfo.sigmaJob - newJobReqs)))\n            if not cacheInfo.isBalanced():\n                raise CacheUnbalancedError()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef removeSingleCachedFile(self, fileStoreID):\n        with self._CacheState.open(self) as cacheInfo:\n            cachedFile = self.encodedFileID(fileStoreID)\n            cachedFileStats = os.stat(cachedFile)\n            # We know the file exists because this function was called in the if block.  So we\n            # have to ensure nothing has changed since then.\n            assert cachedFileStats.st_nlink <= self.nlinkThreshold, \\\n                   'Attempting to delete a global file that is in use by another job.'\n\n            assert cachedFileStats.st_nlink >= self.nlinkThreshold, \\\n                   'A global file has too FEW links at deletion time. Our link threshold is incorrect!'\n            # Remove the file size from the cached file size if the jobstore is not fileJobStore\n            # and then delete the file\n            os.remove(cachedFile)\n            if self.nlinkThreshold != 2:\n                cacheInfo.cached -= cachedFileStats.st_size\n            if not cacheInfo.isBalanced():\n                self.logToMaster('CACHE: The cache was not balanced on removing single file',\n                                 logging.WARN)\n            self.logToMaster('CACHE: Successfully removed file with ID \\'%s\\'.' % fileStoreID)\n        return None", "response": "Removes a single file from the cache forcibly."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _accountForNlinkEquals2(self, localFilePath):\n        fileStats = os.stat(localFilePath)\n        assert fileStats.st_nlink >= self.nlinkThreshold\n        with self._CacheState.open(self) as cacheInfo:\n            cacheInfo.sigmaJob -= fileStats.st_size\n            jobState = self._JobState(cacheInfo.jobState[self.jobID])\n            jobState.updateJobReqs(fileStats.st_size, 'remove')", "response": "This function accounts for the fact that the nlinkThreshold == 2 size of the file is accounted for by the file store copy of the file that was linked to the local file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the status of the job on the disk.", "response": "def _updateJobWhenDone(self):\n        \"\"\"\n        Asynchronously update the status of the job on the disk, first waiting \\\n        until the writing threads have finished and the input blockFn has stopped \\\n        blocking.\n        \"\"\"\n\n        def asyncUpdate():\n            try:\n                # Wait till all file writes have completed\n                for i in range(len(self.workers)):\n                    self.queue.put(None)\n\n                for thread in self.workers:\n                    thread.join()\n\n                # Wait till input block-fn returns - in the event of an exception\n                # this will eventually terminate\n                self.inputBlockFn()\n\n                # Check the terminate event, if set we can not guarantee\n                # that the workers ended correctly, therefore we exit without\n                # completing the update\n                if self._terminateEvent.isSet():\n                    raise RuntimeError(\"The termination flag is set, exiting before update\")\n\n                # Indicate any files that should be deleted once the update of\n                # the job wrapper is completed.\n                self.jobGraph.filesToDelete = list(self.filesToDelete)\n\n                # Complete the job\n                self.jobStore.update(self.jobGraph)\n\n                # Delete any remnant jobs\n                list(map(self.jobStore.delete, self.jobsToDelete))\n\n                # Delete any remnant files\n                list(map(self.jobStore.deleteFile, self.filesToDelete))\n\n                # Remove the files to delete list, having successfully removed the files\n                if len(self.filesToDelete) > 0:\n                    self.jobGraph.filesToDelete = []\n                    # Update, removing emptying files to delete\n                    self.jobStore.update(self.jobGraph)\n            except:\n                self._terminateEvent.set()\n                raise\n            finally:\n                # Indicate that _blockFn can return\n                # This code will always run\n                self.updateSemaphore.release()\n\n        # The update semaphore is held while the job is written to the job store\n        try:\n            self.updateSemaphore.acquire()\n            t = Thread(target=asyncUpdate)\n            t.start()\n        except:\n            # This is to ensure that the semaphore is released in a crash to stop a deadlock\n            # scenario\n            self.updateSemaphore.release()\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef findAndHandleDeadJobs(cls, nodeInfo, batchSystemShutdown=False):\n        # A list of tuples of (job name, pid or process running job, registered defer functions)\n        for jobState in cls._getAllJobStates(nodeInfo):\n            if not cls._pidExists(jobState['jobPID']):\n                # using same logic to prevent races as CachingFileStore._setupCache\n                myPID = str(os.getpid())\n                cleanupFile = os.path.join(jobState['jobDir'], '.cleanup')\n                with open(os.path.join(jobState['jobDir'], '.' + myPID), 'w') as f:\n                    f.write(myPID)\n                while True:\n                    try:\n                        os.rename(f.name, cleanupFile)\n                    except OSError as err:\n                        if err.errno == errno.ENOTEMPTY:\n                            with open(cleanupFile, 'r') as f:\n                                cleanupPID = f.read()\n                            if cls._pidExists(int(cleanupPID)):\n                                # Cleanup your own mess.  It's only polite.\n                                os.remove(f.name)\n                                break\n                            else:\n                                os.remove(cleanupFile)\n                                continue\n                        else:\n                            raise\n                    else:\n                        logger.warning('Detected that job (%s) prematurely terminated.  Fixing the '\n                                       'state of the job on disk.', jobState['jobName'])\n                        if not batchSystemShutdown:\n                            logger.debug(\"Deleting the stale working directory.\")\n                            # Delete the old work directory if it still exists.  Do this only during\n                            # the life of the program and dont' do it during the batch system\n                            # cleanup.  Leave that to the batch system cleanup code.\n                            shutil.rmtree(jobState['jobDir'])\n                        # Run any deferred functions associated with the job\n                        logger.debug('Running user-defined deferred functions.')\n                        cls._runDeferredFunctions(jobState['deferredFunctions'])\n                        break", "response": "Find the state of all jobs that are still running and handle them."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _getAllJobStates(workflowDir):\n        jobStateFiles = []\n        for root, dirs, files in os.walk(workflowDir):\n            for filename in files:\n                if filename == '.jobState':\n                    jobStateFiles.append(os.path.join(root, filename))\n        for filename in jobStateFiles:\n            try:\n                yield NonCachingFileStore._readJobState(filename)\n            except IOError as e:\n                if e.errno == 2:\n                    # job finished & deleted its jobState file since the jobState files were discovered\n                    continue\n                else:\n                    raise", "response": "Generator function that deserializes and yields the job state for every job on the node."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _createJobStateFile(self):\n        jobStateFile = os.path.join(self.localTempDir, '.jobState')\n        jobState = {'jobPID': os.getpid(),\n                    'jobName': self.jobName,\n                    'jobDir': self.localTempDir,\n                    'deferredFunctions': []}\n        with open(jobStateFile + '.tmp', 'wb') as fH:\n            dill.dump(jobState, fH)\n        os.rename(jobStateFile + '.tmp', jobStateFile)\n        return jobStateFile", "response": "Create the job state file for the current job and fill in the required\n        values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write(self, data):\n        \n        # Do the write\n        self.backingStream.write(data)\n        \n        for listener in self.writeListeners:\n            # Send out notifications\n            listener(len(data))", "response": "Write the given data to the file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nscheduling the services of a job asynchronously.", "response": "def scheduleServices(self, jobGraph):\n        \"\"\"\n        Schedule the services of a job asynchronously.\n        When the job's services are running the jobGraph for the job will\n        be returned by toil.leader.ServiceManager.getJobGraphsWhoseServicesAreRunning.\n\n        :param toil.jobGraph.JobGraph jobGraph: wrapper of job with services to schedule.\n        \"\"\"\n        # Add jobGraph to set being processed by the service manager\n        self.jobGraphsWithServicesBeingStarted.add(jobGraph)\n\n        # Add number of jobs managed by ServiceManager\n        self.jobsIssuedToServiceManager += sum(map(len, jobGraph.services)) + 1 # The plus one accounts for the root job\n\n        # Asynchronously schedule the services\n        self._jobGraphsWithServicesToStart.put(jobGraph)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getJobGraphWhoseServicesAreRunning(self, maxWait):\n        try:\n            jobGraph = self._jobGraphsWithServicesThatHaveStarted.get(timeout=maxWait)\n            self.jobGraphsWithServicesBeingStarted.remove(jobGraph)\n            assert self.jobsIssuedToServiceManager >= 0\n            self.jobsIssuedToServiceManager -= 1\n            return jobGraph\n        except Empty:\n            return None", "response": "Returns a jobGraph added to scheduleServices whose services are running."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getServiceJobsToStart(self, maxWait):\n        try:\n            serviceJob = self._serviceJobGraphsToStart.get(timeout=maxWait)\n            assert self.jobsIssuedToServiceManager >= 0\n            self.jobsIssuedToServiceManager -= 1\n            return serviceJob\n        except Empty:\n            return None", "response": "Returns a tuple of the service job store ID memory cores disk and.. of the service job to start."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nkills all the communication jobs for the given services.", "response": "def killServices(self, services, error=False):\n        \"\"\"\n        :param dict services: Maps service jobStoreIDs to the communication flags for the service\n        \"\"\"\n        for serviceJobStoreID in services:\n            serviceJob = services[serviceJobStoreID]\n            if error:\n                self.jobStore.deleteFile(serviceJob.errorJobStoreID)\n            self.jobStore.deleteFile(serviceJob.terminateJobStoreID)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef isRunning(self, serviceJobNode):\n        return (not self.jobStore.fileExists(serviceJobNode.startJobStoreID)) and self.isActive(serviceJobNode)", "response": "Returns true if the service job has started and is active otherwise returns false."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nthreads used to schedule services.", "response": "def _startServices(jobGraphsWithServicesToStart,\n                       jobGraphsWithServicesThatHaveStarted,\n                       serviceJobsToStart,\n                       terminate, jobStore):\n        \"\"\"\n        Thread used to schedule services.\n        \"\"\"\n        servicesThatAreStarting = set()\n        servicesRemainingToStartForJob = {}\n        serviceToJobGraph = {}\n        while True:\n            with throttle(1.0):\n                if terminate.is_set():\n                    logger.debug('Received signal to quit starting services.')\n                    break\n                try:\n                    jobGraph = jobGraphsWithServicesToStart.get_nowait()\n                    if len(jobGraph.services) > 1:\n                        # Have to fall back to the old blocking behavior to\n                        # ensure entire service \"groups\" are issued as a whole.\n                        blockUntilServiceGroupIsStarted(jobGraph,\n                                                        jobGraphsWithServicesThatHaveStarted,\n                                                        serviceJobsToStart, terminate, jobStore)\n                        continue\n                    # Found a new job that needs to schedule its services.\n                    for serviceJob in jobGraph.services[0]:\n                        serviceToJobGraph[serviceJob] = jobGraph\n                    servicesRemainingToStartForJob[jobGraph] = len(jobGraph.services[0])\n                    # Issue the service jobs all at once.\n                    for serviceJob in jobGraph.services[0]:\n                        logger.debug(\"Service manager is starting service job: %s, start ID: %s\", serviceJob, serviceJob.startJobStoreID)\n                        serviceJobsToStart.put(serviceJob)\n                    # We should now start to monitor these services to see if\n                    # they've started yet.\n                    servicesThatAreStarting.update(jobGraph.services[0])\n                except Empty:\n                    # No new jobs that need services scheduled.\n                    pass\n\n                for serviceJob in list(servicesThatAreStarting):\n                    if not jobStore.fileExists(serviceJob.startJobStoreID):\n                        # Service has started!\n                        servicesThatAreStarting.remove(serviceJob)\n                        parentJob = serviceToJobGraph[serviceJob]\n                        servicesRemainingToStartForJob[parentJob] -= 1\n                        assert servicesRemainingToStartForJob[parentJob] >= 0\n                        del serviceToJobGraph[serviceJob]\n\n                # Find if any jobGraphs have had *all* their services started.\n                jobGraphsToRemove = set()\n                for jobGraph, remainingServices in servicesRemainingToStartForJob.items():\n                    if remainingServices == 0:\n                        jobGraphsWithServicesThatHaveStarted.put(jobGraph)\n                        jobGraphsToRemove.add(jobGraph)\n                for jobGraph in jobGraphsToRemove:\n                    del servicesRemainingToStartForJob[jobGraph]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef zoneToRegion(zone):\n    from toil.lib.context import Context\n    return Context.availability_zone_re.match(zone).group(1)", "response": "Get a region from a zone name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchoosing the spot zone based on the bid and the spot_history.", "response": "def choose_spot_zone(zones, bid, spot_history):\n    \"\"\"\n    Returns the zone to put the spot request based on, in order of priority:\n\n       1) zones with prices currently under the bid\n\n       2) zones with the most stable price\n\n    :param list[boto.ec2.zone.Zone] zones:\n    :param float bid:\n    :param list[boto.ec2.spotpricehistory.SpotPriceHistory] spot_history:\n\n    :rtype: str\n    :return: the name of the selected zone\n\n    >>> from collections import namedtuple\n    >>> FauxHistory = namedtuple('FauxHistory', ['price', 'availability_zone'])\n    >>> ZoneTuple = namedtuple('ZoneTuple', ['name'])\n    >>> zones = [ZoneTuple('us-west-2a'), ZoneTuple('us-west-2b')]\n    >>> spot_history = [FauxHistory(0.1, 'us-west-2a'), \\\n                        FauxHistory(0.2, 'us-west-2a'), \\\n                        FauxHistory(0.3, 'us-west-2b'), \\\n                        FauxHistory(0.6, 'us-west-2b')]\n    >>> choose_spot_zone(zones, 0.15, spot_history)\n    'us-west-2a'\n\n    >>> spot_history=[FauxHistory(0.3, 'us-west-2a'), \\\n                      FauxHistory(0.2, 'us-west-2a'), \\\n                      FauxHistory(0.1, 'us-west-2b'), \\\n                      FauxHistory(0.6, 'us-west-2b')]\n    >>> choose_spot_zone(zones, 0.15, spot_history)\n    'us-west-2b'\n\n    >>> spot_history=[FauxHistory(0.1, 'us-west-2a'), \\\n                      FauxHistory(0.7, 'us-west-2a'), \\\n                      FauxHistory(0.1, 'us-west-2b'), \\\n                      FauxHistory(0.6, 'us-west-2b')]\n    >>> choose_spot_zone(zones, 0.15, spot_history)\n    'us-west-2b'\n    \"\"\"\n    # Create two lists of tuples of form: [(zone.name, std_deviation), ...] one for zones\n    # over the bid price and one for zones under bid price. Each are sorted by increasing\n    # standard deviation values.\n    markets_under_bid, markets_over_bid = [], []\n    for zone in zones:\n        zone_histories = [zone_history for zone_history in spot_history if zone_history.availability_zone == zone.name]\n        if zone_histories:\n            price_deviation = std_dev([history.price for history in zone_histories])\n            recent_price = zone_histories[0].price\n        else:\n            price_deviation, recent_price = 0.0, bid\n        zone_tuple = ZoneTuple(name=zone.name, price_deviation=price_deviation)\n        (markets_over_bid, markets_under_bid)[recent_price < bid].append(zone_tuple)\n\n    return min(markets_under_bid or markets_over_bid, key=attrgetter('price_deviation')).name"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef optimize_spot_bid(ctx, instance_type, spot_bid):\n    spot_history = _get_spot_history(ctx, instance_type)\n    if spot_history:\n        _check_spot_bid(spot_bid, spot_history)\n    zones = ctx.ec2.get_all_zones()\n    most_stable_zone = choose_spot_zone(zones, spot_bid, spot_history)\n    logger.debug(\"Placing spot instances in zone %s.\", most_stable_zone)\n    return most_stable_zone", "response": "Optimize the bid for a spot instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck that the bid is less than the average of the instances of the current instance type.", "response": "def _check_spot_bid(spot_bid, spot_history):\n    \"\"\"\n    Prevents users from potentially over-paying for instances\n\n    Note: this checks over the whole region, not a particular zone\n\n    :param spot_bid: float\n\n    :type spot_history: list[SpotPriceHistory]\n\n    :raises UserError: if bid is > 2X the spot price's average\n\n    >>> from collections import namedtuple\n    >>> FauxHistory = namedtuple( \"FauxHistory\", [ \"price\", \"availability_zone\" ] )\n    >>> spot_data = [ FauxHistory( 0.1, \"us-west-2a\" ), \\\n                      FauxHistory( 0.2, \"us-west-2a\" ), \\\n                      FauxHistory( 0.3, \"us-west-2b\" ), \\\n                      FauxHistory( 0.6, \"us-west-2b\" ) ]\n    >>> # noinspection PyProtectedMember\n    >>> _check_spot_bid( 0.1, spot_data )\n    >>> # noinspection PyProtectedMember\n\n    # >>> Box._check_spot_bid( 2, spot_data )\n    Traceback (most recent call last):\n    ...\n    UserError: Your bid $ 2.000000 is more than double this instance type's average spot price ($ 0.300000) over the last week\n    \"\"\"\n    average = mean([datum.price for datum in spot_history])\n    if spot_bid > average * 2:\n        logger.warn(\"Your bid $ %f is more than double this instance type's average \"\n                 \"spot price ($ %f) over the last week\", spot_bid, average)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns list of 1 000 most recent spot market data points represented as SpotPriceHistory objects.", "response": "def _get_spot_history(ctx, instance_type):\n    \"\"\"\n    Returns list of 1,000 most recent spot market data points represented as SpotPriceHistory\n    objects. Note: The most recent object/data point will be first in the list.\n\n    :rtype: list[SpotPriceHistory]\n    \"\"\"\n\n    one_week_ago = datetime.datetime.now() - datetime.timedelta(days=7)\n    spot_data = ctx.ec2.get_spot_price_history(start_time=one_week_ago.isoformat(),\n                                               instance_type=instance_type,\n                                               product_description=\"Linux/UNIX\")\n    spot_data.sort(key=attrgetter(\"timestamp\"), reverse=True)\n    return spot_data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef checkValidNodeTypes(provisioner, nodeTypes):\n    if not nodeTypes:\n        return\n    if not isinstance(nodeTypes, list):\n        nodeTypes = [nodeTypes]\n    if not isinstance(nodeTypes[0], string_types):\n        return\n    # check if a valid node type for aws\n    from toil.lib.generatedEC2Lists import E2Instances, regionDict\n    if provisioner == 'aws':\n        from toil.provisioners.aws import getCurrentAWSZone\n        currentZone = getCurrentAWSZone()\n        if not currentZone:\n            currentZone = 'us-west-2'\n        else:\n            currentZone = currentZone[:-1] # adds something like 'a' or 'b' to the end\n        # check if instance type exists in this region\n        for nodeType in nodeTypes:\n            if nodeType and ':' in nodeType:\n                nodeType = nodeType.split(':')[0]\n            if nodeType not in regionDict[currentZone]:\n                # They probably misspelled it and can't tell.\n                close = get_close_matches(nodeType, regionDict[currentZone], 1)\n                if len(close) > 0:\n                    helpText = ' Did you mean ' + close[0] + '?'\n                else:\n                    helpText = ''\n                raise RuntimeError('Invalid nodeType (%s) specified for AWS in region: %s.%s'\n                                   '' % (nodeType, currentZone, helpText))\n    # Only checks if aws nodeType specified for gce/azure atm.\n    if provisioner == 'gce' or provisioner == 'azure':\n        for nodeType in nodeTypes:\n            if nodeType and ':' in nodeType:\n                nodeType = nodeType.split(':')[0]\n            try:\n                E2Instances[nodeType]\n                raise RuntimeError(\"It looks like you've specified an AWS nodeType with the \"\n                                   \"{} provisioner.  Please specify an {} nodeType.\"\n                                   \"\".format(provisioner, provisioner))\n            except KeyError:\n                pass", "response": "Checks if a valid nodeType is specified for AWS or Azure."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks options and throw error if something goes wrong", "response": "def checkOptions(options, parser):\n    \"\"\" Check options, throw parser.error() if something goes wrong\n    \"\"\"\n\n    if options.jobStore == None:\n        parser.error(\"Specify --jobStore\")\n    defaultCategories = [\"time\", \"clock\", \"wait\", \"memory\"]\n    if options.categories is None:\n        options.categories = defaultCategories\n    else:\n        options.categories = [x.lower() for x in options.categories.split(\",\")]\n    for c in options.categories:\n        if c not in defaultCategories:\n            parser.error(\"Unknown category %s. Must be from %s\"\n                         % (c, str(defaultCategories)))\n    extraSort = [\"count\", \"alpha\"]\n    if options.sortCategory is not None:\n        if (options.sortCategory not in defaultCategories and\n            options.sortCategory not in extraSort):\n            parser.error(\"Unknown --sortCategory %s. Must be from %s\"\n                         % (options.sortCategory,\n                            str(defaultCategories + extraSort)))\n    sortFields = [\"min\", \"med\", \"ave\", \"max\", \"total\"]\n    if options.sortField is not None:\n        if (options.sortField not in sortFields):\n            parser.error(\"Unknown --sortField %s. Must be from %s\"\n                         % (options.sortField, str(sortFields)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npad the beginning of a string with spaces if necessary.", "response": "def padStr(s, field=None):\n    \"\"\" Pad the begining of a string with spaces, if necessary.\n    \"\"\"\n    if field is None:\n        return s\n    else:\n      if len(s) >= field:\n          return s\n      else:\n          return \" \" * (field - len(s)) + s"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prettyMemory(k, field=None, isBytes=False):\n    if isBytes:\n        k /= 1024\n    if k < 1024:\n        return padStr(\"%gK\" % k, field)\n    if k < (1024 * 1024):\n        return padStr(\"%.1fM\" % (old_div(k, 1024.0)), field)\n    if k < (1024 * 1024 * 1024):\n        return padStr(\"%.1fG\" % (k / 1024.0 / 1024.0), field)\n    if k < (1024 * 1024 * 1024 * 1024):\n        return padStr(\"%.1fT\" % (k / 1024.0 / 1024.0 / 1024.0), field)\n    if k < (1024 * 1024 * 1024 * 1024 * 1024):\n        return padStr(\"%.1fP\" % (k / 1024.0 / 1024.0 / 1024.0 / 1024.0), field)", "response": "Given input k as kilobytes return a nicely formatted string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef prettyTime(t, field=None):\n    from math import floor\n    pluralDict = {True: \"s\", False: \"\"}\n    if t < 120:\n        return padStr(\"%ds\" % t, field)\n    if t < 120 * 60:\n        m = floor(old_div(t, 60.))\n        s = t % 60\n        return padStr(\"%dm%ds\" % (m, s), field)\n    if t < 25 * 60 * 60:\n        h = floor(t / 60. / 60.)\n        m = floor(old_div((t - (h * 60. * 60.)), 60.))\n        s = t % 60\n        return padStr(\"%dh%gm%ds\" % (h, m, s), field)\n    if t < 7 * 24 * 60 * 60:\n        d = floor(t / 24. / 60. / 60.)\n        h = floor((t - (d * 24. * 60. * 60.)) / 60. / 60.)\n        m = floor(old_div((t\n                   - (d * 24. * 60. * 60.)\n                   - (h * 60. * 60.)), 60.))\n        s = t % 60\n        dPlural = pluralDict[d > 1]\n        return padStr(\"%dday%s%dh%dm%ds\" % (d, dPlural, h, m, s), field)\n    w = floor(t / 7. / 24. / 60. / 60.)\n    d = floor((t - (w * 7 * 24 * 60 * 60)) / 24. / 60. / 60.)\n    h = floor((t\n                 - (w * 7. * 24. * 60. * 60.)\n                 - (d * 24. * 60. * 60.))\n                / 60. / 60.)\n    m = floor(old_div((t\n                 - (w * 7. * 24. * 60. * 60.)\n                 - (d * 24. * 60. * 60.)\n                 - (h * 60. * 60.)), 60.))\n    s = t % 60\n    wPlural = pluralDict[w > 1]\n    dPlural = pluralDict[d > 1]\n    return padStr(\"%dweek%s%dday%s%dh%dm%ds\" % (w, wPlural, d,\n                                                dPlural, h, m, s), field)", "response": "Given input t as seconds return a nicely formatted string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reportTime(t, options, field=None):\n    if options.pretty:\n        return prettyTime(t, field=field)\n    else:\n        if field is not None:\n            return \"%*.2f\" % (field, t)\n        else:\n            return \"%.2f\" % t", "response": "Given t seconds report back the correct format as string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive k kilobytes report back the correct format as string.", "response": "def reportMemory(k, options, field=None, isBytes=False):\n    \"\"\" Given k kilobytes, report back the correct format as string.\n    \"\"\"\n    if options.pretty:\n        return prettyMemory(int(k), field=field, isBytes=isBytes)\n    else:\n        if isBytes:\n            k /= 1024.\n        if field is not None:\n            return \"%*dK\" % (field - 1, k)  # -1 for the \"K\"\n        else:\n            return \"%dK\" % int(k)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef refineData(root, options):\n    worker = root.worker\n    job = root.jobs\n    jobTypesTree = root.job_types\n    jobTypes = []\n    for childName in jobTypesTree:\n        jobTypes.append(jobTypesTree[childName])\n    return root, worker, job, jobTypes", "response": "walk down from the root and gather up the important bits."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sprintTag(key, tag, options, columnWidths=None):\n    if columnWidths is None:\n        columnWidths = ColumnWidths()\n    header = \"  %7s \" % decorateTitle(\"Count\", options)\n    sub_header = \"  %7s \" % \"n\"\n    tag_str = \"  %s\" % reportNumber(tag.total_number, options, field=7)\n    out_str = \"\"\n    if key == \"job\":\n        out_str += \" %-12s | %7s%7s%7s%7s\\n\" % (\"Worker Jobs\", \"min\",\n                                           \"med\", \"ave\", \"max\")\n        worker_str = \"%s| \" % (\" \" * 14)\n        for t in [tag.min_number_per_worker, tag.median_number_per_worker,\n                  tag.average_number_per_worker, tag.max_number_per_worker]:\n            worker_str += reportNumber(t, options, field=7)\n        out_str += worker_str + \"\\n\"\n    if \"time\" in options.categories:\n        header += \"| %*s \" % (columnWidths.title(\"time\"),\n                              decorateTitle(\"Time\", options))\n        sub_header += decorateSubHeader(\"Time\", columnWidths, options)\n        tag_str += \" | \"\n        for t, width in [\n            (tag.min_time, columnWidths.getWidth(\"time\", \"min\")),\n            (tag.median_time, columnWidths.getWidth(\"time\", \"med\")),\n            (tag.average_time, columnWidths.getWidth(\"time\", \"ave\")),\n            (tag.max_time, columnWidths.getWidth(\"time\", \"max\")),\n            (tag.total_time, columnWidths.getWidth(\"time\", \"total\")),\n            ]:\n            tag_str += reportTime(t, options, field=width)\n    if \"clock\" in options.categories:\n        header += \"| %*s \" % (columnWidths.title(\"clock\"),\n                              decorateTitle(\"Clock\", options))\n        sub_header += decorateSubHeader(\"Clock\", columnWidths, options)\n        tag_str += \" | \"\n        for t, width in [\n            (tag.min_clock, columnWidths.getWidth(\"clock\", \"min\")),\n            (tag.median_clock, columnWidths.getWidth(\"clock\", \"med\")),\n            (tag.average_clock, columnWidths.getWidth(\"clock\", \"ave\")),\n            (tag.max_clock, columnWidths.getWidth(\"clock\", \"max\")),\n            (tag.total_clock, columnWidths.getWidth(\"clock\", \"total\")),\n            ]:\n            tag_str += reportTime(t, options, field=width)\n    if \"wait\" in options.categories:\n        header += \"| %*s \" % (columnWidths.title(\"wait\"),\n                              decorateTitle(\"Wait\", options))\n        sub_header += decorateSubHeader(\"Wait\", columnWidths, options)\n        tag_str += \" | \"\n        for t, width in [\n            (tag.min_wait, columnWidths.getWidth(\"wait\", \"min\")),\n            (tag.median_wait, columnWidths.getWidth(\"wait\", \"med\")),\n            (tag.average_wait, columnWidths.getWidth(\"wait\", \"ave\")),\n            (tag.max_wait, columnWidths.getWidth(\"wait\", \"max\")),\n            (tag.total_wait, columnWidths.getWidth(\"wait\", \"total\")),\n            ]:\n            tag_str += reportTime(t, options, field=width)\n    if \"memory\" in options.categories:\n        header += \"| %*s \" % (columnWidths.title(\"memory\"),\n                              decorateTitle(\"Memory\", options))\n        sub_header += decorateSubHeader(\"Memory\", columnWidths, options)\n        tag_str += \" | \"\n        for t, width in [\n            (tag.min_memory, columnWidths.getWidth(\"memory\", \"min\")),\n            (tag.median_memory, columnWidths.getWidth(\"memory\", \"med\")),\n            (tag.average_memory, columnWidths.getWidth(\"memory\", \"ave\")),\n            (tag.max_memory, columnWidths.getWidth(\"memory\", \"max\")),\n            (tag.total_memory, columnWidths.getWidth(\"memory\", \"total\")),\n            ]:\n            tag_str += reportMemory(t, options, field=width)\n    out_str += header + \"\\n\"\n    out_str += sub_header + \"\\n\"\n    out_str += tag_str + \"\\n\"\n    return out_str", "response": "Generate a pretty - print ready string from a JTTag."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecorate the subheader of a new item in the order they appear in the new item.", "response": "def decorateSubHeader(title, columnWidths, options):\n    \"\"\" Add a marker to the correct field if the TITLE is sorted on.\n    \"\"\"\n    title = title.lower()\n    if title != options.sortCategory:\n        s = \"| %*s%*s%*s%*s%*s \" % (\n            columnWidths.getWidth(title, \"min\"), \"min\",\n            columnWidths.getWidth(title, \"med\"), \"med\",\n            columnWidths.getWidth(title, \"ave\"), \"ave\",\n            columnWidths.getWidth(title, \"max\"), \"max\",\n            columnWidths.getWidth(title, \"total\"), \"total\")\n        return s\n    else:\n        s = \"| \"\n        for field, width in [(\"min\", columnWidths.getWidth(title, \"min\")),\n                             (\"med\", columnWidths.getWidth(title, \"med\")),\n                             (\"ave\", columnWidths.getWidth(title, \"ave\")),\n                             (\"max\", columnWidths.getWidth(title, \"max\")),\n                             (\"total\", columnWidths.getWidth(title, \"total\"))]:\n            if options.sortField == field:\n                s += \"%*s*\" % (width - 1, field)\n            else:\n                s += \"%*s\" % (width, field)\n        s += \" \"\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a float value attribute NAME from TREE.", "response": "def get(tree, name):\n    \"\"\" Return a float value attribute NAME from TREE.\n    \"\"\"\n    if name in tree:\n        value = tree[name]\n    else:\n        return float(\"nan\")\n    try:\n        a = float(value)\n    except ValueError:\n        a = float(\"nan\")\n    return a"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of jobTypes sorted by the given options.", "response": "def sortJobs(jobTypes, options):\n    \"\"\" Return a jobTypes all sorted.\n    \"\"\"\n    longforms = {\"med\": \"median\",\n                 \"ave\": \"average\",\n                 \"min\": \"min\",\n                 \"total\": \"total\",\n                 \"max\": \"max\",}\n    sortField = longforms[options.sortField]\n    if (options.sortCategory == \"time\" or\n        options.sortCategory == \"clock\" or\n        options.sortCategory == \"wait\" or\n        options.sortCategory == \"memory\"\n        ):\n        return sorted(\n            jobTypes,\n            key=lambda tag: getattr(tag, \"%s_%s\"\n                                    % (sortField, options.sortCategory)),\n            reverse=options.sortReverse)\n    elif options.sortCategory == \"alpha\":\n        return sorted(\n            jobTypes, key=lambda tag: tag.name,\n            reverse=options.sortReverse)\n    elif options.sortCategory == \"count\":\n        return sorted(jobTypes, key=lambda tag: tag.total_number,\n                      reverse=options.sortReverse)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint the important bits out.", "response": "def reportPrettyData(root, worker, job, job_types, options):\n    \"\"\" print the important bits out.\n    \"\"\"\n    out_str = \"Batch System: %s\\n\" % root.batch_system\n    out_str += (\"Default Cores: %s  Default Memory: %s\\n\"\n                \"Max Cores: %s\\n\" % (\n        reportNumber(get(root, \"default_cores\"), options),\n        reportMemory(get(root, \"default_memory\"), options, isBytes=True),\n        reportNumber(get(root, \"max_cores\"), options),\n        ))\n    out_str += (\"Total Clock: %s  Total Runtime: %s\\n\" % (\n        reportTime(get(root, \"total_clock\"), options),\n        reportTime(get(root, \"total_run_time\"), options),\n        ))\n    job_types = sortJobs(job_types, options)\n    columnWidths = computeColumnWidths(job_types, worker, job, options)\n    out_str += \"Worker\\n\"\n    out_str += sprintTag(\"worker\", worker, options, columnWidths=columnWidths)\n    out_str += \"Job\\n\"\n    out_str += sprintTag(\"job\", job, options, columnWidths=columnWidths)\n    for t in job_types:\n        out_str += \" %s\\n\" % t.name\n        out_str += sprintTag(t.name, t, options, columnWidths=columnWidths)\n    return out_str"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef computeColumnWidths(job_types, worker, job, options):\n    cw = ColumnWidths()\n    for t in job_types:\n        updateColumnWidths(t, cw, options)\n    updateColumnWidths(worker, cw, options)\n    updateColumnWidths(job, cw, options)\n    return cw", "response": "Compute the correct column widths for the given job types."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the column width attributes for this tag.", "response": "def updateColumnWidths(tag, cw, options):\n    \"\"\" Update the column width attributes for this tag's fields.\n    \"\"\"\n    longforms = {\"med\": \"median\",\n                 \"ave\": \"average\",\n                 \"min\": \"min\",\n                 \"total\": \"total\",\n                 \"max\": \"max\",}\n    for category in [\"time\", \"clock\", \"wait\", \"memory\"]:\n        if category in options.categories:\n            for field in [\"min\", \"med\", \"ave\", \"max\", \"total\"]:\n                t = getattr(tag, \"%s_%s\" % (longforms[field], category))\n                if category in [\"time\", \"clock\", \"wait\"]:\n                    s = reportTime(t, options,\n                                   field=cw.getWidth(category, field)).strip()\n                else:\n                    s = reportMemory(t, options,\n                                     field=cw.getWidth(category, field), isBytes=True).strip()\n                if len(s) >= cw.getWidth(category, field):\n                    # this string is larger than max, width must be increased\n                    cw.setWidth(category, field, len(s) + 1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds an element for output.", "response": "def buildElement(element, items, itemName):\n    \"\"\" Create an element for output.\n    \"\"\"\n    def assertNonnegative(i,name):\n        if i < 0:\n            raise RuntimeError(\"Negative value %s reported for %s\" %(i,name) )\n        else:\n            return float(i)\n\n    itemTimes = []\n    itemClocks = []\n    itemMemory = []\n    for item in items:\n        itemTimes.append(assertNonnegative(float(item[\"time\"]), \"time\"))\n        itemClocks.append(assertNonnegative(float(item[\"clock\"]), \"clock\"))\n        itemMemory.append(assertNonnegative(float(item[\"memory\"]), \"memory\"))\n    assert len(itemClocks) == len(itemTimes) == len(itemMemory)\n\n    itemWaits=[]\n    for index in range(0,len(itemTimes)):\n        itemWaits.append(itemTimes[index] - itemClocks[index])\n\n    itemWaits.sort()\n    itemTimes.sort()\n    itemClocks.sort()\n    itemMemory.sort()\n\n    if len(itemTimes) == 0:\n        itemTimes.append(0)\n        itemClocks.append(0)\n        itemWaits.append(0)\n        itemMemory.append(0)\n\n    element[itemName]=Expando(\n        total_number=float(len(items)),\n        total_time=float(sum(itemTimes)),\n        median_time=float(itemTimes[old_div(len(itemTimes),2)]),\n        average_time=float(old_div(sum(itemTimes),len(itemTimes))),\n        min_time=float(min(itemTimes)),\n        max_time=float(max(itemTimes)),\n        total_clock=float(sum(itemClocks)),\n        median_clock=float(itemClocks[old_div(len(itemClocks),2)]),\n        average_clock=float(old_div(sum(itemClocks),len(itemClocks))),\n        min_clock=float(min(itemClocks)),\n        max_clock=float(max(itemClocks)),\n        total_wait=float(sum(itemWaits)),\n        median_wait=float(itemWaits[old_div(len(itemWaits),2)]),\n        average_wait=float(old_div(sum(itemWaits),len(itemWaits))),\n        min_wait=float(min(itemWaits)),\n        max_wait=float(max(itemWaits)),\n        total_memory=float(sum(itemMemory)),\n        median_memory=float(itemMemory[old_div(len(itemMemory),2)]),\n        average_memory=float(old_div(sum(itemMemory),len(itemMemory))),\n        min_memory=float(min(itemMemory)),\n        max_memory=float(max(itemMemory)),\n        name=itemName\n    )\n    return element[itemName]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncollecting and return the stats and config data.", "response": "def getStats(jobStore):\n    \"\"\" Collect and return the stats and config data.\n    \"\"\"\n    def aggregateStats(fileHandle,aggregateObject):\n        try:\n            stats = json.load(fileHandle, object_hook=Expando)\n            for key in list(stats.keys()):\n                if key in aggregateObject:\n                    aggregateObject[key].append(stats[key])\n                else:\n                    aggregateObject[key]=[stats[key]]\n        except ValueError:\n            logger.critical(\"File %s contains corrupted json. Skipping file.\" % fileHandle)\n            pass  # The file is corrupted.\n\n    aggregateObject = Expando()\n    callBack = partial(aggregateStats, aggregateObject=aggregateObject)\n    jobStore.readStatsAndLogging(callBack, readAll=True)\n    return aggregateObject"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncollate the stats and report by creating a new stats tag.", "response": "def processData(config, stats):\n    \"\"\"\n    Collate the stats and report\n    \"\"\"\n    if 'total_time' not in stats or 'total_clock' not in stats:\n        # toil job not finished yet\n        stats.total_time = [0.0]\n        stats.total_clock = [0.0]\n\n    stats.total_time = sum([float(number) for number in stats.total_time])\n    stats.total_clock = sum([float(number) for number in stats.total_clock])\n\n    collatedStatsTag = Expando(total_run_time=stats.total_time,\n                               total_clock=stats.total_clock,\n                               batch_system=config.batchSystem,\n                               default_memory=str(config.defaultMemory),\n                               default_cores=str(config.defaultCores),\n                               max_cores=str(config.maxCores)\n                               )\n\n    # Add worker info\n    worker = [_f for _f in getattr(stats, 'workers', []) if _f]\n    jobs = [_f for _f in getattr(stats, 'jobs', []) if _f]\n    jobs = [item for sublist in jobs for item in sublist]\n\n    def fn4(job):\n        try:\n            return list(jobs)\n        except TypeError:\n            return []\n\n    buildElement(collatedStatsTag, worker, \"worker\")\n    createSummary(buildElement(collatedStatsTag, jobs, \"jobs\"),\n                  getattr(stats, 'workers', []), \"worker\", fn4)\n    # Get info for each job\n    jobNames = set()\n    for job in jobs:\n        jobNames.add(job.class_name)\n    jobTypesTag = Expando()\n    collatedStatsTag.job_types = jobTypesTag\n    for jobName in jobNames:\n        jobTypes = [ job for job in jobs if job.class_name == jobName ]\n        buildElement(jobTypesTag, jobTypes, jobName)\n    collatedStatsTag.name = \"collatedStatsTag\"\n    return collatedStatsTag"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main():\n    parser = getBasicOptionParser()\n    initializeOptions(parser)\n    options = parseBasicOptions(parser)\n    checkOptions(options, parser)\n    config = Config()\n    config.setOptions(options)\n    jobStore = Toil.resumeJobStore(config.jobStore)\n    stats = getStats(jobStore)\n    collatedStatsTag = processData(jobStore.config, stats)\n    reportData(collatedStatsTag, options)", "response": "Main function for the main workflow."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef title(self, category):\n        return sum(\n            [self.getWidth(category, x) for x in self.fields])", "response": "Return the total printed length of this category item."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef nextChainableJobGraph(jobGraph, jobStore):\n    #If no more jobs to run or services not finished, quit\n    if len(jobGraph.stack) == 0 or len(jobGraph.services) > 0 or jobGraph.checkpoint != None:\n        logger.debug(\"Stopping running chain of jobs: length of stack: %s, services: %s, checkpoint: %s\",\n                     len(jobGraph.stack), len(jobGraph.services), jobGraph.checkpoint != None)\n        return None\n\n    #Get the next set of jobs to run\n    jobs = jobGraph.stack[-1]\n    assert len(jobs) > 0\n\n    #If there are 2 or more jobs to run in parallel we quit\n    if len(jobs) >= 2:\n        logger.debug(\"No more jobs can run in series by this worker,\"\n                    \" it's got %i children\", len(jobs)-1)\n        return None\n\n    #We check the requirements of the jobGraph to see if we can run it\n    #within the current worker\n    successorJobNode = jobs[0]\n    if successorJobNode.memory > jobGraph.memory:\n        logger.debug(\"We need more memory for the next job, so finishing\")\n        return None\n    if successorJobNode.cores > jobGraph.cores:\n        logger.debug(\"We need more cores for the next job, so finishing\")\n        return None\n    if successorJobNode.disk > jobGraph.disk:\n        logger.debug(\"We need more disk for the next job, so finishing\")\n        return None\n    if successorJobNode.preemptable != jobGraph.preemptable:\n        logger.debug(\"Preemptability is different for the next job, returning to the leader\")\n        return None\n    if successorJobNode.predecessorNumber > 1:\n        logger.debug(\"The jobGraph has multiple predecessors, we must return to the leader.\")\n        return None\n\n    # Load the successor jobGraph\n    successorJobGraph = jobStore.load(successorJobNode.jobStoreID)\n\n    # Somewhat ugly, but check if job is a checkpoint job and quit if\n    # so\n    if successorJobGraph.command.startswith(\"_toil \"):\n        #Load the job\n        successorJob = Job._loadJob(successorJobGraph.command, jobStore)\n\n        # Check it is not a checkpoint\n        if successorJob.checkpoint:\n            logger.debug(\"Next job is checkpoint, so finishing\")\n            return None\n\n    # Made it through! This job is chainable.\n    return successorJobGraph", "response": "Returns the next chainable jobGraph after this jobGraph or None if the chain must terminate."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling a single message from the socket server.", "response": "def handle(self):\n        \"\"\"\n        Handle a single message. SocketServer takes care of splitting out the messages.\n        \n        Messages are JSON-encoded logging module records.\n        \"\"\"\n        # Unpack the data from the request\n        data, socket = self.request\n        \n        try:\n            # Parse it as JSON\n            message_attrs = json.loads(data.decode('utf-8'))\n            # Fluff it up into a proper logging record\n            record = logging.makeLogRecord(message_attrs)\n        except:\n            # Complain someone is sending us bad logging data\n            logging.error(\"Malformed log message from {}\".format(self.client_address[0]))\n        else:\n            # Log level filtering should have been done on the remote end. The handle() method\n            # skips it on this end.\n            log.handle(record)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstopping the server on the leader.", "response": "def _stopLeader(cls):\n        \"\"\"\n        Stop the server on the leader.\n        \"\"\"\n        with cls.lock:\n            assert cls.initialized > 0\n            cls.initialized -= 1\n            if cls.initialized == 0:\n                if cls.loggingServer:\n                    log.info('Stopping real-time logging server.')\n                    cls.loggingServer.shutdown()\n                    cls.loggingServer = None\n                if cls.serverThread:\n                    log.info('Joining real-time logging server thread.')\n                    cls.serverThread.join()\n                    cls.serverThread = None\n                for k in list(os.environ.keys()):\n                    if k.startswith(cls.envPrefix):\n                        os.environ.pop(k)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the logger that logs real - time to the leader.", "response": "def getLogger(cls):\n        \"\"\"\n        Get the logger that logs real-time to the leader.\n        \n        Note that if the returned logger is used on the leader, you will see the message twice,\n        since it still goes to the normal log handlers, too.\n        \"\"\"\n        # Only do the setup once, so we don't add a handler every time we log. Use a lock to do\n        # so safely even if we're being called in different threads. Use double-checked locking\n        # to reduce the overhead introduced by the lock.\n        if cls.logger is None:\n            with cls.lock:\n                if cls.logger is None:\n                    cls.logger = logging.getLogger('toil-rt')\n                    try:\n                        level = os.environ[cls.envPrefix + 'LEVEL']\n                    except KeyError:\n                        # There is no server running on the leader, so suppress most log messages\n                        # and skip the UDP stuff.\n                        cls.logger.setLevel(logging.CRITICAL)\n                    else:\n                        # Adopt the logging level set on the leader.\n                        toil.lib.bioio.setLogLevel(level, cls.logger)\n                        try:\n                            address = os.environ[cls.envPrefix + 'ADDRESS']\n                        except KeyError:\n                            pass\n                        else:\n                            # We know where to send messages to, so send them.\n                            host, port = address.split(':')\n                            cls.logger.addHandler(JSONDatagramHandler(host, int(port)))\n        return cls.logger"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuploads a file to s3 using multipart uploading if applicable", "response": "def uploadFromPath(localFilePath, partSize, bucket, fileID, headers):\n    \"\"\"\n    Uploads a file to s3, using multipart uploading if applicable\n\n    :param str localFilePath: Path of the file to upload to s3\n    :param int partSize: max size of each part in the multipart upload, in bytes\n    :param boto.s3.Bucket bucket: the s3 bucket to upload to\n    :param str fileID: the name of the file to upload to\n    :param headers: http headers to use when uploading - generally used for encryption purposes\n    :return: version of the newly uploaded file\n    \"\"\"\n    file_size, file_time = fileSizeAndTime(localFilePath)\n    if file_size <= partSize:\n        key = bucket.new_key(key_name=bytes(fileID))\n        key.name = fileID\n        for attempt in retry_s3():\n            with attempt:\n                key.set_contents_from_filename(localFilePath, headers=headers)\n        version = key.version_id\n    else:\n        with open(localFilePath, 'rb') as f:\n            version = chunkedFileUpload(f, bucket, fileID, file_size, headers, partSize)\n    for attempt in retry_s3():\n        with attempt:\n            key = bucket.get_key(bytes(fileID),\n                                 headers=headers,\n                                 version_id=version)\n    assert key.size == file_size\n    # Make reasonably sure that the file wasn't touched during the upload\n    assert fileSizeAndTime(localFilePath) == (file_size, file_time)\n    return version"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncopying a key from a source key to a destination key in multiple parts.", "response": "def copyKeyMultipart(srcBucketName, srcKeyName, srcKeyVersion, dstBucketName, dstKeyName, sseAlgorithm=None, sseKey=None,\n                     copySourceSseAlgorithm=None, copySourceSseKey=None):\n    \"\"\"\n    Copies a key from a source key to a destination key in multiple parts. Note that if the\n    destination key exists it will be overwritten implicitly, and if it does not exist a new\n    key will be created. If the destination bucket does not exist an error will be raised.\n\n    :param str srcBucketName: The name of the bucket to be copied from.\n    :param str srcKeyName: The name of the key to be copied from.\n    :param str srcKeyVersion: The version of the key to be copied from.\n    :param str dstBucketName: The name of the destination bucket for the copy.\n    :param str dstKeyName: The name of the destination key that will be created or overwritten.\n    :param str sseAlgorithm: Server-side encryption algorithm for the destination.\n    :param str sseKey: Server-side encryption key for the destination.\n    :param str copySourceSseAlgorithm: Server-side encryption algorithm for the source.\n    :param str copySourceSseKey: Server-side encryption key for the source.\n\n    :rtype: str\n    :return: The version of the copied file (or None if versioning is not enabled for dstBucket).\n    \"\"\"\n    s3 = boto3.resource('s3')\n    dstBucket = s3.Bucket(oldstr(dstBucketName))\n    dstObject = dstBucket.Object(oldstr(dstKeyName))\n    copySource = {'Bucket': oldstr(srcBucketName), 'Key': oldstr(srcKeyName)}\n    if srcKeyVersion is not None:\n        copySource['VersionId'] = oldstr(srcKeyVersion)\n\n    # The boto3 functions don't allow passing parameters as None to\n    # indicate they weren't provided. So we have to do a bit of work\n    # to ensure we only provide the parameters when they are actually\n    # required.\n    destEncryptionArgs = {}\n    if sseKey is not None:\n        destEncryptionArgs.update({'SSECustomerAlgorithm': sseAlgorithm,\n                                   'SSECustomerKey': sseKey})\n    copyEncryptionArgs = {}\n    if copySourceSseKey is not None:\n        copyEncryptionArgs.update({'CopySourceSSECustomerAlgorithm': copySourceSseAlgorithm,\n                                   'CopySourceSSECustomerKey': copySourceSseKey})\n    copyEncryptionArgs.update(destEncryptionArgs)\n\n    dstObject.copy(copySource, ExtraArgs=copyEncryptionArgs)\n\n    # Unfortunately, boto3's managed copy doesn't return the version\n    # that it actually copied to. So we have to check immediately\n    # after, leaving open the possibility that it may have been\n    # modified again in the few seconds since the copy finished. There\n    # isn't much we can do about it.\n    info = boto3.client('s3').head_object(Bucket=dstObject.bucket_name, Key=dstObject.key,\n                                          **destEncryptionArgs)\n    return info.get('VersionId', None)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmonkey - patched version of SDBConnection. put_attributes that uses POST instead of GET", "response": "def _put_attributes_using_post(self, domain_or_name, item_name, attributes,\n                               replace=True, expected_value=None):\n    \"\"\"\n    Monkey-patched version of SDBConnection.put_attributes that uses POST instead of GET\n\n    The GET version is subject to the URL length limit which kicks in before the 256 x 1024 limit\n    for attribute values. Using POST prevents that.\n\n    https://github.com/BD2KGenomics/toil/issues/502\n    \"\"\"\n    domain, domain_name = self.get_domain_and_name(domain_or_name)\n    params = {'DomainName': domain_name,\n              'ItemName': item_name}\n    self._build_name_value_list(params, attributes, replace)\n    if expected_value:\n        self._build_expected_value(params, expected_value)\n    # The addition of the verb keyword argument is the only difference to put_attributes (Hannes)\n    return self.get_status('PutAttributes', params, verb='POST')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef attributesToBinary(cls, attributes):\n        chunks = [(int(k), v) for k, v in iteritems(attributes) if cls._isValidChunkName(k)]\n        chunks.sort()\n        numChunks = int(attributes[u'numChunks'])\n        if numChunks:\n            serializedJob = b''.join(v for k, v in chunks)\n            compressed = base64.b64decode(serializedJob)\n            if compressed[0] == b'C'[0]:\n                binary = bz2.decompress(compressed[1:])\n            elif compressed[0] == b'U'[0]:\n                binary = compressed[1:]\n            else:\n                raise RuntimeError('Unexpected prefix {}'.format(compressed[0]))\n        else:\n            binary = None\n        return binary, numChunks", "response": "Converts the dictionary of attributes to binary data and the number of chunks it was composed from\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setLogLevel(level, logger=None):\n    if logger is None:\n        logger = rootLogger\n    level = level.upper()\n    if level == \"OFF\": level = \"CRITICAL\"\n    # Note that getLevelName works in both directions, numeric to textual and textual to numeric\n    numericLevel = logging.getLevelName(level)\n    assert logging.getLevelName(numericLevel) == level\n    logger.setLevel(numericLevel)\n    # There are quite a few cases where we expect AWS requests to fail, but it seems\n    # that boto handles these by logging the error *and* raising an exception. We\n    # don't want to confuse the user with those error messages.\n    logging.getLogger( 'boto' ).setLevel( logging.CRITICAL )", "response": "Sets the log level to a given string level."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef logFile(fileName, printFunction=logger.info):\n    printFunction(\"Reporting file: %s\" % fileName)\n    shortName = fileName.split(\"/\")[-1]\n    fileHandle = open(fileName, 'r')\n    line = fileHandle.readline()\n    while line != '':\n        if line[-1] == '\\n':\n            line = line[:-1]\n        printFunction(\"%s:\\t%s\" % (shortName, line))\n        line = fileHandle.readline()\n    fileHandle.close()", "response": "Writes out a formatted version of the given log file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef logStream(fileHandle, shortName, printFunction=logger.info):\n    printFunction(\"Reporting file: %s\" % shortName)\n    line = fileHandle.readline()\n    while line != '':\n        if line[-1] == '\\n':\n            line = line[:-1]\n        printFunction(\"%s:\\t%s\" % (shortName, line))\n        line = fileHandle.readline()\n    fileHandle.close()", "response": "Writes out a formatted version of the given log stream."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _addLoggingOptions(addOptionFn):\n    # BEFORE YOU ADD OR REMOVE OPTIONS TO THIS FUNCTION, KNOW THAT YOU MAY ONLY USE VARIABLES ACCEPTED BY BOTH\n    # optparse AND argparse FOR EXAMPLE, YOU MAY NOT USE default=%default OR default=%(default)s\n    defaultLogLevelName = logging.getLevelName( defaultLogLevel )\n    addOptionFn(\"--logOff\", dest=\"logLevel\",\n                default=defaultLogLevelName,\n                action=\"store_const\", const=\"CRITICAL\",\n                help=\"Same as --logCritical\")\n    for level in supportedLogLevels:\n        levelName = logging.getLevelName(level)\n        levelNameCapitalized = levelName.capitalize()\n        addOptionFn(\"--log\" + levelNameCapitalized, dest=\"logLevel\",\n                    default=defaultLogLevelName,\n                    action=\"store_const\", const=levelName,\n                    help=\"Turn on logging at level %s and above. (default is %s)\" % (levelName, defaultLogLevelName))\n    addOptionFn(\"--logLevel\", dest=\"logLevel\", default=defaultLogLevelName,\n                help=(\"Log at given level (may be either OFF (or CRITICAL), ERROR, WARN (or WARNING), INFO or DEBUG). \"\n                      \"(default is %s)\" % defaultLogLevelName))\n    addOptionFn(\"--logFile\", dest=\"logFile\", help=\"File to log in\")\n    addOptionFn(\"--rotatingLogging\", dest=\"logRotating\", action=\"store_true\", default=False,\n                help=\"Turn on rotating logging, which prevents log files getting too big.\")", "response": "Adds logging options to the Grammar."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setLoggingFromOptions(options):\n    formatStr = ' '.join([socket.gethostname(), '%(asctime)s', '%(threadName)s',\n                          '%(levelname)s', '%(name)s:', '%(message)s'])\n    logging.basicConfig(format=formatStr)\n    rootLogger.setLevel(defaultLogLevel)\n    if options.logLevel is not None:\n        setLogLevel(options.logLevel)\n    else:\n        # Ensure that any other log level overrides are in effect even if no log level is explicitly set\n        setLogLevel(getLogLevelString())\n    logger.debug(\"Root logger is at level '%s', 'toil' logger at level '%s'.\",\n                getLogLevelString(logger=rootLogger), getLogLevelString(logger=toilLogger))\n    if options.logFile is not None:\n        addLoggingFileHandler(options.logFile, rotatingLogging=options.logRotating)\n        logger.debug(\"Logging to file '%s'.\" % options.logFile)", "response": "Sets the logging from a dictionary of name value options."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef system(command):\n    logger.debug('Running: %r', command)\n    subprocess.check_call(command, shell=isinstance(command, string_types), bufsize=-1)", "response": "A convenience wrapper around subprocess. check_call that logs the command before passing iton to iton."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nliking os. path. abspath except it doesn t dereference symlinks", "response": "def absSymPath(path):\n    \"\"\"like os.path.abspath except it doesn't dereference symlinks\n    \"\"\"\n    curr_path = os.getcwd()\n    return os.path.normpath(os.path.join(curr_path, path))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef makePublicDir(dirName):\n    if not os.path.exists(dirName):\n        os.mkdir(dirName)\n        os.chmod(dirName, 0o777)\n    return dirName", "response": "Makes a given subdirectory if it doesn t already exist making sure it is public."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_AST(wdl_file, outdir=None):\n    '''\n    Writes a file with the AST for a wdl file in the outdir.\n    '''\n    if outdir is None:\n        outdir = os.getcwd()\n    with open(os.path.join(outdir, 'AST.out'), 'w') as f:\n        with open(wdl_file, 'r') as wdl:\n            wdl_string = wdl.read()\n            ast = wdl_parser.parse(wdl_string).ast()\n            f.write(ast.dumps(indent=2))", "response": "Writes a file with the AST for a wdl file in the outdir."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting out the main section of the compiled toil script.", "response": "def write_main(self):\n        '''\n        Writes out a huge string representing the main section of the python\n        compiled toil script.\n\n        Currently looks at and writes 5 sections:\n        1. JSON Variables (includes importing and preparing files as tuples)\n        2. TSV Variables (includes importing and preparing files as tuples)\n        3. CSV Variables (includes importing and preparing files as tuples)\n        4. Wrapping each WDL \"task\" function as a toil job\n        5. List out children and encapsulated jobs by priority, then start job0.\n\n        This should create variable declarations necessary for function calls.\n        Map file paths appropriately and store them in the toil fileStore so\n        that they are persistent from job to job.  Create job wrappers for toil.\n        And finally write out, and run the jobs in order of priority using the\n        addChild and encapsulate commands provided by toil.\n\n        :return: giant string containing the main def for the toil script.\n        '''\n\n        main_section = ''\n\n        # write out the main header\n        main_header = self.write_main_header()\n        main_section = main_section + main_header\n\n        # write out the workflow declarations\n        main_section = main_section + '        # WF Declarations\\n'\n        wf_declarations_to_write = self.write_main_wfdeclarations()\n        main_section = main_section + wf_declarations_to_write\n\n        # write toil job wrappers with input vars\n        jobs_to_write = self.write_main_jobwrappers()\n        main_section = main_section + jobs_to_write\n\n        # loop to export all outputs to a cloud bucket\n        if self.destBucket:\n            main_destbucket = self.write_main_destbucket()\n            main_section = main_section + main_destbucket\n\n        return main_section"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting out jobs as wrapped in preparation for calling.", "response": "def write_main_jobwrappers(self):\n        '''\n        Writes out 'jobs' as wrapped toil objects in preparation for calling.\n\n        :return: A string representing this.\n        '''\n        main_section = ''\n\n        # toil cannot technically start with multiple jobs, so an empty\n        # 'initialize_jobs' function is always called first to get around this\n        main_section = main_section + '\\n        job0 = Job.wrapJobFn(initialize_jobs)\\n'\n\n        # declare each job in main as a wrapped toil function in order of priority\n        for wf in self.workflows_dictionary:\n            for assignment in self.workflows_dictionary[wf]:\n                if assignment.startswith('call'):\n                    main_section += '        job0 = job0.encapsulate()\\n'\n                    main_section += self.write_main_jobwrappers_call(self.workflows_dictionary[wf][assignment])\n                if assignment.startswith('scatter'):\n                    main_section += '        job0 = job0.encapsulate()\\n'\n                    main_section += self.write_main_jobwrappers_scatter(self.workflows_dictionary[wf][assignment],\n                                                                        assignment)\n                if assignment.startswith('if'):\n                    main_section += '        if {}:\\n'.format(self.workflows_dictionary[wf][assignment]['expression'])\n                    main_section += self.write_main_jobwrappers_if(self.workflows_dictionary[wf][assignment]['body'])\n\n        main_section += '\\n        fileStore.start(job0)\\n'\n\n        return main_section"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_functions(self):\n        '''\n        Writes out a python function for each WDL \"task\" object.\n\n        :return: a giant string containing the meat of the job defs.\n        '''\n\n        # toil cannot technically start with multiple jobs, so an empty\n        # 'initialize_jobs' function is always called first to get around this\n        fn_section = 'def initialize_jobs(job):\\n' + \\\n                     '    job.fileStore.logToMaster(\"initialize_jobs\")\\n'\n\n        for job in self.tasks_dictionary:\n            fn_section += self.write_function(job)\n\n        for wf in self.workflows_dictionary:\n            for assignment in self.workflows_dictionary[wf]:\n                if assignment.startswith('scatter'):\n                    fn_section += self.write_scatterfunction(self.workflows_dictionary[wf][assignment], assignment)\n                if assignment.startswith('if'):\n                    fn_section += self.write_scatterfunctions_within_if(self.workflows_dictionary[wf][assignment]['body'])\n\n        return fn_section", "response": "Writes out a python function for each WDL task object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite out a python function for each WDL scatter object.", "response": "def write_scatterfunction(self, job, scattername):\n        '''\n        Writes out a python function for each WDL \"scatter\" object.\n        '''\n\n        scatter_outputs = self.fetch_scatter_outputs(job)\n\n        # write the function header\n        fn_section = self.write_scatterfunction_header(scattername)\n\n        # write the scatter definitions\n        fn_section += self.write_scatterfunction_lists(scatter_outputs)\n\n        # write\n        fn_section += self.write_scatterfunction_loop(job, scatter_outputs)\n\n        # write the outputs for the task to return\n        fn_section += self.write_scatterfunction_outputreturn(scatter_outputs)\n\n        return fn_section"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_function(self, job):\n        '''\n        Writes out a python function for each WDL \"task\" object.\n\n        Each python function is a unit of work written out as a string in\n        preparation to being written out to a file.  In WDL, each \"job\" is\n        called a \"task\".  Each WDL task is written out in multiple steps:\n\n        1: Header and inputs (e.g. 'def mapping(self, input1, input2)')\n        2: Log job name (e.g. 'job.fileStore.logToMaster('initialize_jobs')')\n        3: Create temp dir (e.g. 'tempDir = fileStore.getLocalTempDir()')\n        4: import filenames and use readGlobalFile() to get files from the\n           jobStore\n        5: Reformat commandline variables (like converting to ' '.join(files)).\n        6: Commandline call using subprocess.Popen().\n        7: Write the section returning the outputs.  Also logs stats.\n\n        :return: a giant string containing the meat of the job defs for the toil script.\n        '''\n\n        # write the function header\n        fn_section = self.write_function_header(job)\n\n        # write out commandline keywords\n        fn_section += self.write_function_cmdline(job)\n\n        if self.needsdocker(job):\n            # write a bash script to inject into the docker\n            fn_section += self.write_function_bashscriptline(job)\n            # write a call to the docker API\n            fn_section += self.write_function_dockercall(job)\n        else:\n            # write a subprocess call\n            fn_section += self.write_function_subprocesspopen()\n\n        # write the outputs for the definition to return\n        fn_section += self.write_function_outputreturn(job, docker=self.needsdocker(job))\n\n        return fn_section", "response": "Writes out a python function for each WDL task object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting the header that starts each function for example this function can write and return.", "response": "def write_function_header(self, job):\n        '''\n        Writes the header that starts each function, for example, this function\n        can write and return:\n\n        'def write_function_header(self, job, job_declaration_array):'\n\n        :param job: A list such that:\n                        (job priority #, job ID #, Job Skeleton Name, Job Alias)\n        :param job_declaration_array: A list of all inputs that job requires.\n        :return: A string representing this.\n        '''\n        fn_section = '\\n\\nclass {jobname}Cls(Job):\\n'.format(jobname=job)\n        fn_section += '    def __init__(self, '\n        if 'inputs' in self.tasks_dictionary[job]:\n            for i in self.tasks_dictionary[job]['inputs']:\n                var = i[0]\n                vartype = i[1]\n                if vartype == 'String':\n                    fn_section += '{input}=\"\", '.format(input=var)\n                else:\n                    fn_section += '{input}=None, '.format(input=var)\n        fn_section += '*args, **kwargs):\\n'\n        fn_section += '        super({jobname}Cls, self).__init__(*args, **kwargs)\\n'.format(jobname=job)\n\n        # TODO: Resolve inherent problems resolving resource requirements\n        # In WDL, \"local-disk \" + 500 + \" HDD\" cannot be directly converted to python.\n        # This needs a special handler.\n        if 'runtime' in self.tasks_dictionary[job]:\n            runtime_resources = []\n            if 'memory' in self.tasks_dictionary[job]['runtime']:\n                runtime_resources.append('memory=memory')\n                memory = self.tasks_dictionary[job]['runtime']['memory']\n                fn_section += '        memory=parse_memory({})\\n'.format(memory)\n            if 'cpu' in self.tasks_dictionary[job]['runtime']:\n                runtime_resources.append('cores=cores')\n                cores = self.tasks_dictionary[job]['runtime']['cpu']\n                fn_section += '        cores=parse_cores({})\\n'.format(cores)\n            if 'disks' in self.tasks_dictionary[job]['runtime']:\n                runtime_resources.append('disk=disk')\n                disk = self.tasks_dictionary[job]['runtime']['disks']\n                fn_section += '        disk=parse_disk({})\\n'.format(disk)\n            runtime_resources = ['self'] + runtime_resources\n            fn_section += '        Job.__init__({})\\n\\n'.format(', '.join(runtime_resources))\n\n        if 'inputs' in self.tasks_dictionary[job]:\n            for i in self.tasks_dictionary[job]['inputs']:\n                var = i[0]\n                var_expressn = i[2]\n                json_expressn = self.json_var(task=job, var=var)\n\n                # json declarations have priority and can overwrite\n                # whatever is in the wdl file\n                if json_expressn:\n                    var_expressn = json_expressn\n                if not var_expressn:\n                    var_expressn = var\n\n                fn_section += '        self.id_{} = {}\\n'.format(var, var_expressn)\n\n        fn_section += heredoc_wdl('''\n\n                             def run(self, fileStore):\n                                 fileStore.logToMaster(\"{jobname}\")\n                                 tempDir = fileStore.getLocalTempDir()\n                                 \n                                 try:\n                                     os.makedirs(os.path.join(tempDir, 'execution'))\n                                 except OSError as e:\n                                     if e.errno != errno.EEXIST:\n                                         raise\n                                 ''', {'jobname': job}, indent='    ')[1:]\n        if 'inputs' in self.tasks_dictionary[job]:\n            for i in self.tasks_dictionary[job]['inputs']:\n                var = i[0]\n                var_type = i[1]\n                docker_bool = str(self.needsdocker(job))\n                if var_type == 'File':\n                    fn_section += '        {} = process_and_read_file(abspath_file(self.id_{}, asldijoiu23r8u34q89fho934t8u34fcurrentworkingdir), tempDir, fileStore, docker={})\\n'.format(var, var, docker_bool)\n                else:\n                    fn_section += '        {} = self.id_{}\\n'.format(var, var)\n\n        return fn_section"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_function_bashscriptline(self, job):\n        '''\n        Writes a function to create a bashscript for injection into the docker\n        container.\n\n        :param job_task_reference: The job referenced in WDL's Task section.\n        :param job_alias: The actual job name to be written.\n        :return: A string writing all of this.\n        '''\n        fn_section = \"        generate_docker_bashscript_file(temp_dir=tempDir, docker_dir=tempDir, globs=[\"\n        # TODO: Add glob\n        # if 'outputs' in self.tasks_dictionary[job]:\n        #     for output in self.tasks_dictionary[job]['outputs']:\n        #         fn_section += '({}), '.format(output[2])\n        if fn_section.endswith(', '):\n            fn_section = fn_section[:-2]\n        fn_section += \"], cmd=cmd, job_name='{}')\\n\\n\".format(str(job))\n\n        return fn_section", "response": "Writes a function to create a bashscript for injection into the docker\n        container."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_function_dockercall(self, job):\n        '''\n        Writes a string containing the apiDockerCall() that will run the job.\n\n        :param job_task_reference: The name of the job calling docker.\n        :param docker_image: The corresponding name of the docker image.\n                                                            e.g. \"ubuntu:latest\"\n        :return: A string containing the apiDockerCall() that will run the job.\n        '''\n        docker_dict = {\"docker_image\": self.tasks_dictionary[job]['runtime']['docker'],\n                       \"job_task_reference\": job,\n                       \"docker_user\": str(self.docker_user)}\n        docker_template = heredoc_wdl('''\n        stdout = apiDockerCall(self, \n                               image={docker_image}, \n                               working_dir=tempDir, \n                               parameters=[os.path.join(tempDir, \"{job_task_reference}_script.sh\")], \n                               entrypoint=\"/bin/bash\", \n                               user={docker_user}, \n                               stderr=True, \n                               volumes={{tempDir: {{\"bind\": tempDir}}}})\n        writetype = 'wb' if isinstance(stdout, bytes) else 'w'\n        with open(os.path.join(asldijoiu23r8u34q89fho934t8u34fcurrentworkingdir, '{job_task_reference}.log'), writetype) as f:\n            f.write(stdout)\n            ''', docker_dict, indent='        ')[1:]\n\n        return docker_template", "response": "Writes a docker call to the log file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite a series of commandline variables to be concatenated together with apiDockerCall or withMacroCommand.", "response": "def write_function_cmdline(self, job):\n        '''\n        Write a series of commandline variables to be concatenated together\n        eventually and either called with subprocess.Popen() or with\n        apiDockerCall() if a docker image is called for.\n\n        :param job: A list such that:\n                        (job priority #, job ID #, Job Skeleton Name, Job Alias)\n        :return: A string representing this.\n        '''\n\n        fn_section = '\\n'\n        cmd_array = []\n        if 'raw_commandline' in self.tasks_dictionary[job]:\n            for cmd in self.tasks_dictionary[job]['raw_commandline']:\n                if not cmd.startswith(\"r'''\"):\n                    cmd = 'str({i} if not isinstance({i}, tuple) else process_and_read_file({i}, tempDir, fileStore)).strip(\"{nl}\")'.format(i=cmd, nl=r\"\\n\")\n                fn_section = fn_section + heredoc_wdl('''\n                        try:\n                            # Intended to deal with \"optional\" inputs that may not exist\n                            # TODO: handle this better\n                            command{num} = {cmd}\n                        except:\n                            command{num} = ''\\n''', {'cmd': cmd, 'num': self.cmd_num}, indent='        ')\n                cmd_array.append('command' + str(self.cmd_num))\n                self.cmd_num = self.cmd_num + 1\n\n        if cmd_array:\n            fn_section += '\\n        cmd = '\n            for command in cmd_array:\n                fn_section += '{command} + '.format(command=command)\n            if fn_section.endswith(' + '):\n                fn_section = fn_section[:-3]\n            fn_section += '\\n        cmd = textwrap.dedent(cmd.strip(\"{nl}\"))\\n'.format(nl=r\"\\n\")\n\n        return fn_section"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds the output values that this function needs and write them out as a string. :param job: A list such that: (job priority #, job ID #, Job Skeleton Name, Job Alias) :param job_task_reference: The name of the job to look up values for. :return: A string representing this.", "response": "def write_function_outputreturn(self, job, docker=False):\n        '''\n        Find the output values that this function needs and write them out as a\n        string.\n\n        :param job: A list such that:\n                        (job priority #, job ID #, Job Skeleton Name, Job Alias)\n        :param job_task_reference: The name of the job to look up values for.\n        :return: A string representing this.\n        '''\n\n        fn_section = ''\n        if 'outputs' in self.tasks_dictionary[job]:\n            return_values = []\n            for output in self.tasks_dictionary[job]['outputs']:\n                output_name = output[0]\n                output_type = output[1]\n                output_value = output[2]\n\n                if output_type == 'File':\n                    nonglob_dict = {\n                        \"output_name\": output_name,\n                        \"expression\": output_value,\n                        \"out_dir\": self.output_directory,\n                        \"output_type\": output_type}\n\n                    nonglob_template = heredoc_wdl('''\n                        # output-type: {output_type}\n                        output_filename = {expression}\n                        {output_name} = process_outfile(output_filename, fileStore, tempDir, '{out_dir}')\n                    ''', nonglob_dict, indent='        ')[1:]\n                    fn_section += nonglob_template\n                    return_values.append(output_name)\n                else:\n                    fn_section += '        {} = {}\\n'.format(output_name, output_value)\n                    return_values.append(output_name)\n\n            if return_values:\n                fn_section += '        rvDict = {'\n            for return_value in return_values:\n                fn_section += '\"{rv}\": {rv}, '.format(rv=return_value)\n            if fn_section.endswith(', '):\n                fn_section = fn_section[:-2]\n            if return_values:\n                fn_section = fn_section + '}\\n'\n\n            if return_values:\n                fn_section += '        return rvDict\\n\\n'\n\n        return fn_section"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_python_file(self,\n                          module_section,\n                          fn_section,\n                          main_section,\n                          output_file):\n        '''\n        Just takes three strings and writes them to output_file.\n\n        :param module_section: A string of 'import modules'.\n        :param fn_section: A string of python 'def functions()'.\n        :param main_section: A string declaring toil options and main's header.\n        :param job_section: A string import files into toil and declaring jobs.\n        :param output_file: The file to write the compiled toil script to.\n        '''\n        with open(output_file, 'w') as file:\n            file.write(module_section)\n            file.write(fn_section)\n            file.write(main_section)", "response": "This function writes the python file to output_file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_mappings(self, i):\n        '''\n        Intended to take a ToilWDL_instance (i) and prints the final task dict,\n        and workflow dict.\n\n        Does not work by default with toil since Toil actively suppresses stdout\n        during the run.\n\n        :param i: A class object instance with the following dict variables:\n                                self.tasks_dictionary\n                                self.workflows_dictionary\n        '''\n        from collections import OrderedDict\n\n        class Formatter(object):\n            def __init__(self):\n                self.types = {}\n                self.htchar = '\\t'\n                self.lfchar = '\\n'\n                self.indent = 0\n                self.set_formater(object, self.__class__.format_object)\n                self.set_formater(dict, self.__class__.format_dict)\n                self.set_formater(list, self.__class__.format_list)\n                self.set_formater(tuple, self.__class__.format_tuple)\n\n            def set_formater(self, obj, callback):\n                self.types[obj] = callback\n\n            def __call__(self, value, **args):\n                for key in args:\n                    setattr(self, key, args[key])\n                formater = self.types[type(value) if type(value) in self.types else object]\n                return formater(self, value, self.indent)\n\n            def format_object(self, value, indent):\n                return repr(value)\n\n            def format_dict(self, value, indent):\n                items = [\n                    self.lfchar + self.htchar * (indent + 1) + repr(key) + ': ' +\n                    (self.types[type(value[key]) if type(value[key]) in self.types else object])(self, value[key], indent + 1)\n                    for key in value]\n                return '{%s}' % (','.join(items) + self.lfchar + self.htchar * indent)\n\n            def format_list(self, value, indent):\n                items = [\n                    self.lfchar + self.htchar * (indent + 1) + (\n                    self.types[type(item) if type(item) in self.types else object])(self, item, indent + 1)\n                    for item in value]\n                return '[%s]' % (','.join(items) + self.lfchar + self.htchar * indent)\n\n            def format_tuple(self, value, indent):\n                items = [\n                    self.lfchar + self.htchar * (indent + 1) + (\n                    self.types[type(item) if type(item) in self.types else object])(self, item, indent + 1)\n                    for item in value]\n                return '(%s)' % (','.join(items) + self.lfchar + self.htchar * indent)\n\n        pretty = Formatter()\n\n        def format_ordereddict(self, value, indent):\n            items = [\n                self.lfchar + self.htchar * (indent + 1) +\n                \"(\" + repr(key) + ', ' + (self.types[\n                    type(value[key]) if type(value[key]) in self.types else object\n                ])(self, value[key], indent + 1) + \")\"\n                for key in value\n            ]\n            return 'OrderedDict([%s])' % (','.join(items) +\n                                          self.lfchar + self.htchar * indent)\n\n        pretty.set_formater(OrderedDict, format_ordereddict)\n\n        with open('mappings.out', 'w') as f:\n            f.write(pretty(i.tasks_dictionary))\n            f.write('\\n\\n\\n\\n\\n\\n')\n            f.write(pretty(i.workflows_dictionary))", "response": "Writes the final dict of tasks and workflows to stdout."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setAutoscaledNodeTypes(self, nodeTypes):\n        self._spotBidsMap = {}\n        self.nodeShapes = []\n        self.nodeTypes = []\n        for nodeTypeStr in nodeTypes:\n            nodeBidTuple = nodeTypeStr.split(\":\")\n            if len(nodeBidTuple) == 2:\n                # This is a preemptable node type, with a spot bid\n                nodeType, bid = nodeBidTuple\n                self.nodeTypes.append(nodeType)\n                self.nodeShapes.append(self.getNodeShape(nodeType, preemptable=True))\n                self._spotBidsMap[nodeType] = bid\n            else:\n                self.nodeTypes.append(nodeTypeStr)\n                self.nodeShapes.append(self.getNodeShape(nodeTypeStr, preemptable=False))", "response": "Set the node types shapes and spot bids."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _setSSH(self):\n        if not os.path.exists('/root/.sshSuccess'):\n            subprocess.check_call(['ssh-keygen', '-f', '/root/.ssh/id_rsa', '-t', 'rsa', '-N', ''])\n            with open('/root/.sshSuccess', 'w') as f:\n                f.write('written here because of restrictive permissions on .ssh dir')\n        os.chmod('/root/.ssh', 0o700)\n        subprocess.check_call(['bash', '-c', 'eval $(ssh-agent) && ssh-add -k'])\n        with open('/root/.ssh/id_rsa.pub') as f:\n            masterPublicKey = f.read()\n        masterPublicKey = masterPublicKey.split(' ')[1]  # take 'body' of key\n        # confirm it really is an RSA public key\n        assert masterPublicKey.startswith('AAAAB3NzaC1yc2E'), masterPublicKey\n        return masterPublicKey", "response": "Generate a key pair save it in / root. ssh and return the public key."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nissue a batch job for the given jobNode. Returns the unique jobID.", "response": "def issueBatchJob(self, jobNode):\n        \"\"\"\n        Issues the following command returning a unique jobID. Command is the string to run, memory\n        is an int giving the number of bytes the job needs to run in and cores is the number of cpus\n        needed for the job and error-file is the path of the file to place any std-err/std-out in.\n        \"\"\"\n        localID = self.handleLocalJob(jobNode)\n        if localID:\n            return localID\n        self.checkResourceRequest(jobNode.memory, jobNode.cores, jobNode.disk)\n        jobID = self.getNextJobID()\n        job = ToilJob(jobID=jobID,\n                      name=str(jobNode),\n                      resources=MesosShape(wallTime=0, **jobNode._requirements),\n                      command=jobNode.command,\n                      userScript=self.userScript,\n                      environment=self.environment.copy(),\n                      workerCleanupInfo=self.workerCleanupInfo)\n        jobType = job.resources\n        log.debug(\"Queueing the job command: %s with job id: %s ...\", jobNode.command, str(jobID))\n\n        # TODO: round all elements of resources\n\n        self.taskResources[jobID] = job.resources\n        self.jobQueues.insertJob(job, jobType)\n        log.debug(\"... queued\")\n        return jobID"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates and returns an ExecutorInfo - shaped object representing our executor implementation.", "response": "def _buildExecutor(self):\n        \"\"\"\n        Creates and returns an ExecutorInfo-shaped object representing our executor implementation.\n        \"\"\"\n        # The executor program is installed as a setuptools entry point by setup.py\n        info = addict.Dict()\n        info.name = \"toil\"\n        info.command.value = resolveEntryPoint('_toil_mesos_executor')\n        info.executor_id.value = \"toil-%i\" % os.getpid()\n        info.source = pwd.getpwuid(os.getuid()).pw_name\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting the Mesos driver thread.", "response": "def _startDriver(self):\n        \"\"\"\n        The Mesos driver thread which handles the scheduler's communication with the Mesos master\n        \"\"\"\n        framework = addict.Dict()\n        framework.user = getpass.getuser() # We must determine the user name ourselves with pymesos\n        framework.name = \"toil\"\n        framework.principal = framework.name\n        # Make the driver which implements most of the scheduler logic and calls back to us for the user-defined parts.\n        # Make sure it will call us with nice namespace-y addicts\n        self.driver = MesosSchedulerDriver(self, framework,\n                                           self._resolveAddress(self.mesosMasterAddress),\n                                           use_addict=True, implicit_acknowledgements=True)\n        self.driver.start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _resolveAddress(address):\n        address = address.split(':')\n        assert len(address) in (1, 2)\n        address[0] = socket.gethostbyname(address[0])\n        return ':'.join(address)", "response": "Resolves the host in the given string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninvoke when the scheduler successfully registers with a masterInfo.", "response": "def registered(self, driver, frameworkId, masterInfo):\n        \"\"\"\n        Invoked when the scheduler successfully registers with a Mesos master\n        \"\"\"\n        log.debug(\"Registered with framework ID %s\", frameworkId.value)\n        # Save the framework ID\n        self.frameworkId = frameworkId.value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeclining Mesos offers and return a list of all the jobs that can be run.", "response": "def resourceOffers(self, driver, offers):\n        \"\"\"\n        Invoked when resources have been offered to this framework.\n        \"\"\"\n        self._trackOfferedNodes(offers)\n\n        jobTypes = self.jobQueues.sortedTypes\n\n        if not jobTypes:\n            log.debug('There are no queued tasks. Declining Mesos offers.')\n            # Without jobs, we can get stuck with no jobs and no new offers until we decline it.\n            self._declineAllOffers(driver, offers)\n            return\n\n        unableToRun = True\n        # Right now, gives priority to largest jobs\n        for offer in offers:\n            if offer.hostname in self.ignoredNodes:\n                log.debug(\"Declining offer %s because node %s is designated for termination\" %\n                        (offer.id.value, offer.hostname))\n                driver.declineOffer(offer.id)\n                continue\n            runnableTasks = []\n            # TODO: In an offer, can there ever be more than one resource with the same name?\n            offerCores, offerMemory, offerDisk, offerPreemptable = self._parseOffer(offer)\n            log.debug('Got offer %s for a %spreemptable agent with %.2f MiB memory, %.2f core(s) '\n                      'and %.2f MiB of disk.', offer.id.value, '' if offerPreemptable else 'non-',\n                      offerMemory, offerCores, offerDisk)\n            remainingCores = offerCores\n            remainingMemory = offerMemory\n            remainingDisk = offerDisk\n\n            for jobType in jobTypes:\n                runnableTasksOfType = []\n                # Because we are not removing from the list until outside of the while loop, we\n                # must decrement the number of jobs left to run ourselves to avoid an infinite\n                # loop.\n                nextToLaunchIndex = 0\n                # Toil specifies disk and memory in bytes but Mesos uses MiB\n                while ( not self.jobQueues.typeEmpty(jobType)\n                       # On a non-preemptable node we can run any job, on a preemptable node we\n                       # can only run preemptable jobs:\n                       and (not offerPreemptable or jobType.preemptable)\n                       and remainingCores >= jobType.cores\n                       and remainingDisk >= toMiB(jobType.disk)\n                       and remainingMemory >= toMiB(jobType.memory)):\n                    task = self._prepareToRun(jobType, offer)\n                    # TODO: this used to be a conditional but Hannes wanted it changed to an assert\n                    # TODO: ... so we can understand why it exists.\n                    assert int(task.task_id.value) not in self.runningJobMap\n                    runnableTasksOfType.append(task)\n                    log.debug(\"Preparing to launch Mesos task %s with %.2f cores, %.2f MiB memory, and %.2f MiB disk using offer %s ...\",\n                              task.task_id.value, jobType.cores, toMiB(jobType.memory), toMiB(jobType.disk), offer.id.value)\n                    remainingCores -= jobType.cores\n                    remainingMemory -= toMiB(jobType.memory)\n                    remainingDisk -= toMiB(jobType.disk)\n                    nextToLaunchIndex += 1\n                if not self.jobQueues.typeEmpty(jobType):\n                    # report that remaining jobs cannot be run with the current resourcesq:\n                    log.debug('Offer %(offer)s not suitable to run the tasks with requirements '\n                              '%(requirements)r. Mesos offered %(memory)s memory, %(cores)s cores '\n                              'and %(disk)s of disk on a %(non)spreemptable agent.',\n                              dict(offer=offer.id.value,\n                                   requirements=jobType.__dict__,\n                                   non='' if offerPreemptable else 'non-',\n                                   memory=fromMiB(offerMemory),\n                                   cores=offerCores,\n                                   disk=fromMiB(offerDisk)))\n                runnableTasks.extend(runnableTasksOfType)\n            # Launch all runnable tasks together so we only call launchTasks once per offer\n            if runnableTasks:\n                unableToRun = False\n                driver.launchTasks(offer.id, runnableTasks)\n                self._updateStateToRunning(offer, runnableTasks)\n            else:\n                log.debug('Although there are queued jobs, none of them could be run with offer %s '\n                          'extended to the framework.', offer.id)\n                driver.declineOffer(offer.id)\n\n        if unableToRun and time.time() > (self.lastTimeOfferLogged + self.logPeriod):\n            self.lastTimeOfferLogged = time.time()\n            log.debug('Although there are queued jobs, none of them were able to run in '\n                     'any of the offers extended to the framework. There are currently '\n                     '%i jobs running. Enable debug level logging to see more details about '\n                     'job types and offers received.', len(self.runningJobMap))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds the Mesos task object for a given Toil job and Mesos offer.", "response": "def _newMesosTask(self, job, offer):\n        \"\"\"\n        Build the Mesos task object for a given the Toil job and Mesos offer\n        \"\"\"\n        task = addict.Dict()\n        task.task_id.value = str(job.jobID)\n        task.agent_id.value = offer.agent_id.value\n        task.name = job.name\n        task.data = encode_data(pickle.dumps(job))\n        task.executor = addict.Dict(self.executor)\n\n        task.resources = []\n\n        task.resources.append(addict.Dict())\n        cpus = task.resources[-1]\n        cpus.name = 'cpus'\n        cpus.type = 'SCALAR'\n        cpus.scalar.value = job.resources.cores\n\n        task.resources.append(addict.Dict())\n        disk = task.resources[-1]\n        disk.name = 'disk'\n        disk.type = 'SCALAR'\n        if toMiB(job.resources.disk) > 1:\n            disk.scalar.value = toMiB(job.resources.disk)\n        else:\n            log.warning(\"Job %s uses less disk than Mesos requires. Rounding %s up to 1 MiB.\",\n                        job.jobID, job.resources.disk)\n            disk.scalar.value = 1\n        \n        task.resources.append(addict.Dict())\n        mem = task.resources[-1]\n        mem.name = 'mem'\n        mem.type = 'SCALAR'\n        if toMiB(job.resources.memory) > 1:\n            mem.scalar.value = toMiB(job.resources.memory)\n        else:\n            log.warning(\"Job %s uses less memory than Mesos requires. Rounding %s up to 1 MiB.\",\n                        job.jobID, job.resources.memory)\n            mem.scalar.value = 1\n        return task"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef statusUpdate(self, driver, update):\n        jobID = int(update.task_id.value)\n        log.debug(\"Job %i is in state '%s' due to reason '%s'.\", jobID, update.state, update.reason)\n        \n        def jobEnded(_exitStatus, wallTime=None):\n            \"\"\"\n            Notify external observers of the job ending.\n            \"\"\"\n            self.updatedJobsQueue.put((jobID, _exitStatus, wallTime))\n            agentIP = None\n            try:\n                agentIP = self.runningJobMap[jobID].agentIP\n            except KeyError:\n                log.warning(\"Job %i returned exit code %i but isn't tracked as running.\",\n                            jobID, _exitStatus)\n            else:\n                # Mark the job as no longer running. We MUST do this BEFORE\n                # saying we killed the job, or it will be possible for another\n                # thread to kill a job and then see it as running.\n                del self.runningJobMap[jobID]\n\n            try:\n                self.hostToJobIDs[agentIP].remove(jobID)\n            except KeyError:\n                log.warning(\"Job %i returned exit code %i from unknown host.\",\n                            jobID, _exitStatus)\n                            \n            try:\n                self.killJobIds.remove(jobID)\n            except KeyError:\n                pass\n            else:\n                # We were asked to kill this job, so say that we have done so.\n                # We do this LAST, after all status updates for the job have\n                # been handled, to ensure a consistent view of the scheduler\n                # state from other threads.\n                self.killedJobIds.add(jobID)\n\n        if update.state == 'TASK_FINISHED':\n            # We get the running time of the job via the timestamp, which is in job-local time in seconds\n            assert(update.has_key('timestamp'))\n            jobEnded(0, wallTime=update.timestamp)\n        elif update.state == 'TASK_FAILED':\n            try:\n                exitStatus = int(update.message)\n            except ValueError:\n                exitStatus = 255\n                log.warning(\"Job %i failed with message '%s' due to reason '%s' on executor '%s' on agent '%s'.\",\n                            jobID, update.message, update.reason,\n                            update.executor_id, update.agent_id)\n            else:\n                log.warning(\"Job %i failed with exit status %i and message '%s' due to reason '%s' on executor '%s' on agent '%s'.\",\n                            jobID, exitStatus,\n                            update.message, update.reason,\n                            update.executor_id, update.agent_id)\n            \n            jobEnded(exitStatus)\n        elif update.state in ('TASK_LOST', 'TASK_KILLED', 'TASK_ERROR'):\n            log.warning(\"Job %i is in unexpected state %s with message '%s' due to reason '%s'.\",\n                        jobID, update.state, update.message, update.reason)\n            jobEnded(255)\n            \n        if update.has_key('limitation'):\n            log.warning(\"Job limit info: %s\" % update.limitation)", "response": "Called by the scheduler when a status update is received from the scheduler."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninvoking when an executor sends a message.", "response": "def frameworkMessage(self, driver, executorId, agentId, message):\n        \"\"\"\n        Invoked when an executor sends a message.\n        \"\"\"\n        \n        # Take it out of base 64 encoding from Protobuf\n        message = decode_data(message)\n        \n        log.debug('Got framework message from executor %s running on agent %s: %s',\n                  executorId.value, agentId.value, message)\n        message = ast.literal_eval(message)\n        assert isinstance(message, dict)\n        # Handle the mandatory fields of a message\n        nodeAddress = message.pop('address')\n        executor = self._registerNode(nodeAddress, agentId.value)\n        # Handle optional message fields\n        for k, v in iteritems(message):\n            if k == 'nodeInfo':\n                assert isinstance(v, dict)\n                resources = [taskData for taskData in itervalues(self.runningJobMap)\n                             if taskData.executorID == executorId.value]\n                requestedCores = sum(taskData.cores for taskData in resources)\n                requestedMemory = sum(taskData.memory for taskData in resources)\n                executor.nodeInfo = NodeInfo(requestedCores=requestedCores, requestedMemory=requestedMemory, **v)\n                self.executors[nodeAddress] = executor\n            else:\n                raise RuntimeError(\"Unknown message field '%s'.\" % k)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _registerNode(self, nodeAddress, agentId, nodePort=5051):\n        executor = self.executors.get(nodeAddress)\n        if executor is None or executor.agentId != agentId:\n            executor = self.ExecutorInfo(nodeAddress=nodeAddress,\n                                         agentId=agentId,\n                                         nodeInfo=None,\n                                         lastSeen=time.time())\n            self.executors[nodeAddress] = executor\n        else:\n            executor.lastSeen = time.time()\n            \n        # Record the IP under the agent id\n        self.agentsByID[agentId] = nodeAddress\n            \n        return executor", "response": "Called when we get communication from an agent. Registers the agent s IP under the agent s id and returns the executor object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall when an executor has exited or terminated abnormally.", "response": "def executorLost(self, driver, executorId, agentId, status):\n        \"\"\"\n        Invoked when an executor has exited/terminated abnormally.\n        \"\"\"\n        \n        failedId = executorId.get('value', None)\n        \n        log.warning(\"Executor '%s' reported lost with status '%s'.\", failedId, status)\n        \n        self._handleFailedExecutor(agentId.value, failedId)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts n bytes into a human readable string based on format.", "response": "def bytes2human(n, fmt='%(value).1f %(symbol)s', symbols='customary'):\n    \"\"\"\n    Convert n bytes into a human readable string based on format.\n    symbols can be either \"customary\", \"customary_ext\", \"iec\" or \"iec_ext\",\n    see: http://goo.gl/kTQMs\n    \"\"\"\n    n = int(n)\n    if n < 0:\n        raise ValueError(\"n < 0\")\n    symbols = SYMBOLS[symbols]\n    prefix = {}\n    for i, s in enumerate(symbols[1:]):\n        prefix[s] = 1 << (i+1)*10\n    for symbol in reversed(symbols[1:]):\n        if n >= prefix[symbol]:\n            value = old_div(float(n), prefix[symbol])\n            return fmt % locals()\n    return fmt % dict(symbol=symbols[0], value=n)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a floating point value representing the percentage of pre - paid time left in cycle for this node.", "response": "def remainingBillingInterval(self):\n        \"\"\"\n        If the node has a launch time, this function returns a floating point value\n        between 0 and 1.0 representing how far we are into the\n        current billing cycle for the given instance. If the return value is .25, we are one\n        quarter into the billing cycle, with three quarters remaining before we will be charged\n        again for that instance.\n\n        Assumes a billing cycle of one hour.\n\n        :return: Float from 0 -> 1.0 representing percentage of pre-paid time left in cycle.\n        \"\"\"\n        if self.launchTime:\n            now = datetime.datetime.utcnow()\n            delta = now - parse_iso_utc(self.launchTime)\n            return 1 - delta.total_seconds() / 3600.0 % 1.0\n        else:\n            return 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncopying authorized_keys file to the core user from the keyName user.", "response": "def copySshKeys(self, keyName):\n        \"\"\" Copy authorized_keys file to the core user from the keyName user.\"\"\"\n        if keyName == 'core':\n            return # No point.\n\n        # Make sure that keys are there.\n        self._waitForSSHKeys(keyName=keyName)\n\n        # copy keys to core user so that the ssh calls will work\n        # - normal mechanism failed unless public key was in the google-ssh format\n        # - even so, the key wasn't copied correctly to the core account\n        keyFile = '/home/%s/.ssh/authorized_keys' % keyName\n        self.sshInstance('/usr/bin/sudo', '/usr/bin/cp', keyFile, '/home/core/.ssh', user=keyName)\n        self.sshInstance('/usr/bin/sudo', '/usr/bin/chown', 'core',\n                         '/home/core/.ssh/authorized_keys', user=keyName)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef injectFile(self, fromFile, toFile, role):\n        maxRetries = 10\n        for retry in range(maxRetries):\n            try:\n                self.coreRsync([fromFile, \":\" + toFile], applianceName=role)\n                return True\n            except Exception as e:\n                logger.debug(\"Rsync to new node failed, trying again. Error message: %s\" % e)\n                time.sleep(10*retry)\n        raise RuntimeError(\"Failed to inject file %s to %s with ip %s\" % (fromFile, role, self.effectiveIP) )", "response": "rsync a file to the vm with the given role"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwaits until the instance represented by this box is accessible via SSH.", "response": "def _waitForSSHPort(self):\n        \"\"\"\n        Wait until the instance represented by this box is accessible via SSH.\n\n        :return: the number of unsuccessful attempts to connect to the port before a the first\n        success\n        \"\"\"\n        logger.debug('Waiting for ssh port to open...')\n        for i in count():\n            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            try:\n                s.settimeout(a_short_time)\n                s.connect((self.effectiveIP, 22))\n                logger.debug('...ssh port open')\n                return i\n            except socket.error:\n                pass\n            finally:\n                s.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sshAppliance(self, *args, **kwargs):\n        kwargs['appliance'] = True\n        return self.coreSSH(*args, **kwargs)", "response": "This method is used to create a TTY shell for the appliance with the specified tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns a command on the instance and return the binary output of the command.", "response": "def sshInstance(self, *args, **kwargs):\n        \"\"\"\n        Run a command on the instance.\n        Returns the binary output of the command.\n        \"\"\"\n        kwargs['collectStdout'] = True\n        return self.coreSSH(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun a core command on the local machine.", "response": "def coreSSH(self, *args, **kwargs):\n        \"\"\"\n        If strict=False, strict host key checking will be temporarily disabled.\n        This is provided as a convenience for internal/automated functions and\n        ought to be set to True whenever feasible, or whenever the user is directly\n        interacting with a resource (e.g. rsync-cluster or ssh-cluster). Assumed\n        to be False by default.\n\n        kwargs: input, tty, appliance, collectStdout, sshOptions, strict\n        \"\"\"\n        commandTokens = ['ssh', '-t']\n        strict = kwargs.pop('strict', False)\n        if not strict:\n            kwargs['sshOptions'] = ['-oUserKnownHostsFile=/dev/null', '-oStrictHostKeyChecking=no'] \\\n                                 + kwargs.get('sshOptions', [])\n        sshOptions = kwargs.pop('sshOptions', None)\n        # Forward ports:\n        # 3000 for Grafana dashboard\n        # 9090 for Prometheus dashboard\n        # 5050 for Mesos dashboard (although to talk to agents you will need a proxy)\n        commandTokens.extend(['-L', '3000:localhost:3000', \\\n                              '-L', '9090:localhost:9090', \\\n                              '-L', '5050:localhost:5050'])\n        if sshOptions:\n            # add specified options to ssh command\n            assert isinstance(sshOptions, list)\n            commandTokens.extend(sshOptions)\n        # specify host\n        user = kwargs.pop('user', 'core')   # CHANGED: Is this needed?\n        commandTokens.append('%s@%s' % (user,str(self.effectiveIP)))\n        appliance = kwargs.pop('appliance', None)\n        if appliance:\n            # run the args in the appliance\n            tty = kwargs.pop('tty', None)\n            ttyFlag = '-t' if tty else ''\n            commandTokens += ['docker', 'exec', '-i', ttyFlag, 'toil_leader']\n\n        inputString = kwargs.pop('input', None)\n        if inputString is not None:\n            kwargs['stdin'] = subprocess.PIPE\n        collectStdout = kwargs.pop('collectStdout', None)\n        if collectStdout:\n            kwargs['stdout'] = subprocess.PIPE\n        kwargs['stderr'] = subprocess.PIPE\n\n        logger.debug('Node %s: %s', self.effectiveIP, ' '.join(args))\n        args = list(map(pipes.quote, args))\n        commandTokens += args\n        logger.debug('Full command %s', ' '.join(commandTokens))\n        popen = subprocess.Popen(commandTokens, **kwargs)\n        stdout, stderr = popen.communicate(input=inputString)\n        # at this point the process has already exited, no need for a timeout\n        resultValue = popen.wait()\n        # ssh has been throwing random 255 errors - why?\n        if resultValue != 0:\n            logger.debug('SSH Error (%s) %s' % (resultValue, stderr))\n            raise RuntimeError('Executing the command \"%s\" on the appliance returned a non-zero '\n                               'exit code %s with stdout %s and stderr %s'\n                               % (' '.join(args), resultValue, stdout, stderr))\n        return stdout"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadjusting the ending reservation for a job.", "response": "def adjustEndingReservationForJob(reservation, jobShape, wallTime):\n    \"\"\"\n    Add a job to an ending reservation that ends at wallTime, splitting\n    the reservation if the job doesn't fill the entire timeslice.\n    \"\"\"\n    if jobShape.wallTime - wallTime < reservation.shape.wallTime:\n        # This job only partially fills one of the slices. Create a new slice.\n        reservation.shape, nS = split(reservation.shape, jobShape, jobShape.wallTime - wallTime)\n        nS.nReservation = reservation.nReservation\n        reservation.nReservation = nS\n    else:\n        # This job perfectly fits within the boundaries of the slices.\n        reservation.subtract(jobShape)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsplit a node allocation into two nodes and a new node reservation for that node.", "response": "def split(nodeShape, jobShape, wallTime):\n    \"\"\"\n    Partition a node allocation into two to fit the job, returning the\n    modified shape of the node and a new node reservation for\n    the extra time that the job didn't fill.\n    \"\"\"\n    return (Shape(wallTime,\n                  nodeShape.memory - jobShape.memory,\n                  nodeShape.cores - jobShape.cores,\n                  nodeShape.disk - jobShape.disk,\n                  nodeShape.preemptable),\n            NodeReservation(Shape(nodeShape.wallTime - wallTime,\n                                  nodeShape.memory,\n                                  nodeShape.cores,\n                                  nodeShape.disk,\n                                  nodeShape.preemptable)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npacking a list of jobShapes into the fewest nodes reasonable. Can be run multiple times.", "response": "def binPack(self, jobShapes):\n        \"\"\"Pack a list of jobShapes into the fewest nodes reasonable. Can be run multiple times.\"\"\"\n        # TODO: Check for redundancy with batchsystems.mesos.JobQueue() sorting\n        logger.debug('Running bin packing for node shapes %s and %s job(s).',\n                     self.nodeShapes, len(jobShapes))\n        # Sort in descending order from largest to smallest. The FFD like-strategy will pack the\n        # jobs in order from longest to shortest.\n        jobShapes.sort()\n        jobShapes.reverse()\n        assert len(jobShapes) == 0 or jobShapes[0] >= jobShapes[-1]\n        for jS in jobShapes:\n            self.addJobShape(jS)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfunctioning adds the job to the first node that fits this jobShape.", "response": "def addJobShape(self, jobShape):\n        \"\"\"\n        Function adds the job to the first node reservation in which it will fit (this is the\n        bin-packing aspect).\n        \"\"\"\n        chosenNodeShape = None\n        for nodeShape in self.nodeShapes:\n            if NodeReservation(nodeShape).fits(jobShape):\n                # This node shape is the first that fits this jobShape\n                chosenNodeShape = nodeShape\n                break\n\n        if chosenNodeShape is None:\n            logger.warning(\"Couldn't fit job with requirements %r into any nodes in the nodeTypes \"\n                           \"list.\" % jobShape)\n            return\n\n        # grab current list of job objects appended to this nodeType\n        nodeReservations = self.nodeReservations[chosenNodeShape]\n        for nodeReservation in nodeReservations:\n            if nodeReservation.attemptToAddJob(jobShape, chosenNodeShape, self.targetTime):\n                # We succeeded adding the job to this node reservation. Now we're done.\n                return\n\n        reservation = NodeReservation(chosenNodeShape)\n        currentTimeAllocated = chosenNodeShape.wallTime\n        adjustEndingReservationForJob(reservation, jobShape, 0)\n        self.nodeReservations[chosenNodeShape].append(reservation)\n\n        # Extend the reservation if necessary to cover the job's entire runtime.\n        while currentTimeAllocated < jobShape.wallTime:\n            extendThisReservation = NodeReservation(reservation.shape)\n            currentTimeAllocated += chosenNodeShape.wallTime\n            reservation.nReservation = extendThisReservation\n            reservation = extendThisReservation"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dict from node shape to number of nodes required to run the packed jobs.", "response": "def getRequiredNodes(self):\n        \"\"\"\n        Returns a dict from node shape to number of nodes required to run the packed jobs.\n        \"\"\"\n        return {nodeShape:len(self.nodeReservations[nodeShape]) for nodeShape in self.nodeShapes}"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if a job shape s resource requirements will fit within this allocation.", "response": "def fits(self, jobShape):\n        \"\"\"Check if a job shape's resource requirements will fit within this allocation.\"\"\"\n        return jobShape.memory <= self.shape.memory and \\\n               jobShape.cores <= self.shape.cores and \\\n               jobShape.disk <= self.shape.disk and \\\n               (jobShape.preemptable or not self.shape.preemptable)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets all time - slice shapes in order from this reservation on.", "response": "def shapes(self):\n        \"\"\"Get all time-slice shapes, in order, from this reservation on.\"\"\"\n        shapes = []\n        curRes = self\n        while curRes is not None:\n            shapes.append(curRes.shape)\n            curRes = curRes.nReservation\n        return shapes"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef subtract(self, jobShape):\n        self.shape = Shape(self.shape.wallTime,\n                           self.shape.memory - jobShape.memory,\n                           self.shape.cores - jobShape.cores,\n                           self.shape.disk - jobShape.disk,\n                           self.shape.preemptable)", "response": "Subtracts the resources needed to run a jobShape from the reservation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef attemptToAddJob(self, jobShape, nodeShape, targetTime):\n        # starting slice of time that we can fit in so far\n        startingReservation = self\n        # current end of the slices we can fit in so far\n        endingReservation = startingReservation\n        # the amount of runtime of the job currently covered by slices\n        availableTime = 0\n        # total time from when the instance started up to startingReservation\n        startingReservationTime = 0\n\n        while True:\n            # True == can run the job (resources & preemptable only; NO time)\n            if endingReservation.fits(jobShape):\n                # add the time left available on the reservation\n                availableTime += endingReservation.shape.wallTime\n                # does the job time fit in the reservation's remaining time?\n                if availableTime >= jobShape.wallTime:\n                    timeSlice = 0\n                    while startingReservation != endingReservation:\n                        # removes resources only (NO time) from startingReservation\n                        startingReservation.subtract(jobShape)\n                        # set aside the timeSlice\n                        timeSlice += startingReservation.shape.wallTime\n                        startingReservation = startingReservation.nReservation\n                    assert jobShape.wallTime - timeSlice <= startingReservation.shape.wallTime\n                    adjustEndingReservationForJob(endingReservation, jobShape, timeSlice)\n                    # Packed the job.\n                    return True\n\n                # If the job would fit, but is longer than the total node allocation\n                # extend the node allocation\n                elif endingReservation.nReservation == None and startingReservation == self:\n                    # Extend the node reservation to accommodate jobShape\n                    endingReservation.nReservation = NodeReservation(nodeShape)\n            # can't run the job with the current resources\n            else:\n                if startingReservationTime + availableTime + endingReservation.shape.wallTime <= targetTime:\n                    startingReservation = endingReservation.nReservation\n                    startingReservationTime += availableTime + endingReservation.shape.wallTime\n                    availableTime = 0\n                else:\n                    break\n\n            endingReservation = endingReservation.nReservation\n            if endingReservation is None:\n                # Reached the end of the reservation without success so stop trying to\n                # add to reservation\n                break\n        # Couldn't pack the job.\n        return False", "response": "Attempts to add a job into this reservation timeslice and the reservations after it. Returns True if the job was packed False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrounding a number to the nearest integer to the number of odds.", "response": "def _round(self, number):\n        \"\"\"\n        Helper function for rounding-as-taught-in-school (X.5 rounds to X+1 if positive).\n        Python 3 now rounds 0.5 to whichever side is even (i.e. 2.5 rounds to 2).\n        \n        :param int number: a float to round.\n        :return: closest integer to number, rounding ties away from 0.\n        \"\"\"\n        \n        sign = 1 if number >= 0 else -1\n        \n        rounded = int(round(number))\n        nextRounded = int(round(number + 1 * sign))\n        \n        if nextRounded == rounded:\n            # We rounded X.5 to even, and it was also away from 0.\n            return rounded\n        elif nextRounded == rounded + 1 * sign:\n            # We rounded normally (we are in Python 2)\n            return rounded\n        elif nextRounded == rounded + 2 * sign:\n            # We rounded X.5 to even, but it was towards 0.\n            # Go away from 0 instead.\n            return rounded + 1 * sign\n        else:\n            # If we get here, something has gone wrong.\n            raise RuntimeError(\"Could not round {}\".format(number))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef addCompletedJob(self, job, wallTime):\n\n        #Adjust average runtimes to include this job.\n        if job.jobName in self.jobNameToAvgRuntime:\n            prevAvg = self.jobNameToAvgRuntime[job.jobName]\n            prevNum = self.jobNameToNumCompleted[job.jobName]\n            self.jobNameToAvgRuntime[job.jobName] = float(prevAvg*prevNum + wallTime)/(prevNum + 1)\n            self.jobNameToNumCompleted[job.jobName] += 1\n        else:\n            self.jobNameToAvgRuntime[job.jobName] = wallTime\n            self.jobNameToNumCompleted[job.jobName] = 1\n\n        self.totalJobsCompleted += 1\n        self.totalAvgRuntime = float(self.totalAvgRuntime * (self.totalJobsCompleted - 1) + \\\n                                     wallTime)/self.totalJobsCompleted", "response": "Adds the shape of a completed job to the queue allowing the scalar to use the last N\n        completed jobs in factoring how many nodes required in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setStaticNodes(self, nodes, preemptable):\n        prefix = 'non-' if not preemptable else ''\n        logger.debug(\"Adding %s to %spreemptable static nodes\", nodes, prefix)\n        if nodes is not None:\n            self.static[preemptable] = {node.privateIP : node for node in nodes}", "response": "Sets the static nodes for the given list of nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef smoothEstimate(self, nodeShape, estimatedNodeCount):\n        weightedEstimate = (1 - self.betaInertia) * estimatedNodeCount + \\\n                           self.betaInertia * self.previousWeightedEstimate[nodeShape]\n        self.previousWeightedEstimate[nodeShape] = weightedEstimate\n        return self._round(weightedEstimate)", "response": "Smooth out fluctuations in the estimate for this node compared to\n        previous runs. Returns an integer."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives the resource requirements of queued jobs and the current size of the cluster, returns a dict mapping from nodeShape to the number of nodes we want in the cluster right now.", "response": "def getEstimatedNodeCounts(self, queuedJobShapes, currentNodeCounts):\n        \"\"\"\n        Given the resource requirements of queued jobs and the current size of the cluster, returns\n        a dict mapping from nodeShape to the number of nodes we want in the cluster right now.\n        \"\"\"\n        nodesToRunQueuedJobs = binPacking(jobShapes=queuedJobShapes,\n                                          nodeShapes=self.nodeShapes,\n                                          goalTime=self.targetTime)\n        estimatedNodeCounts = {}\n        for nodeShape in self.nodeShapes:\n            nodeType = self.nodeShapeToType[nodeShape]\n\n            logger.debug(\"Nodes of type %s to run queued jobs = \"\n                        \"%s\" % (nodeType, nodesToRunQueuedJobs[nodeShape]))\n            # Actual calculation of the estimated number of nodes required\n            estimatedNodeCount = 0 if nodesToRunQueuedJobs[nodeShape] == 0 \\\n                else max(1, self._round(nodesToRunQueuedJobs[nodeShape]))\n            logger.debug(\"Estimating %i nodes of shape %s\" % (estimatedNodeCount, nodeShape))\n\n            # Use inertia parameter to smooth out fluctuations according to an exponentially\n            # weighted moving average.\n            estimatedNodeCount = self.smoothEstimate(nodeShape, estimatedNodeCount)\n\n            # If we're scaling a non-preemptable node type, we need to see if we have a \n            # deficit of preemptable nodes of this type that we should compensate for.\n            if not nodeShape.preemptable:\n                compensation = self.config.preemptableCompensation\n                assert 0.0 <= compensation <= 1.0\n                # The number of nodes we provision as compensation for missing preemptable\n                # nodes is the product of the deficit (the number of preemptable nodes we did\n                # _not_ allocate) and configuration preference.\n                compensationNodes = self._round(self.preemptableNodeDeficit[nodeType] * compensation)\n                if compensationNodes > 0:\n                    logger.debug('Adding %d non-preemptable nodes of type %s to compensate for a '\n                                'deficit of %d preemptable ones.', compensationNodes,\n                                nodeType,\n                                self.preemptableNodeDeficit[nodeType])\n                estimatedNodeCount += compensationNodes\n\n            logger.debug(\"Currently %i nodes of type %s in cluster\" % (currentNodeCounts[nodeShape],\n                                                                      nodeType))\n            if self.leader.toilMetrics:\n                self.leader.toilMetrics.logClusterSize(nodeType=nodeType,\n                                                       currentSize=currentNodeCounts[nodeShape],\n                                                       desiredSize=estimatedNodeCount)\n\n            # Bound number using the max and min node parameters\n            if estimatedNodeCount > self.maxNodes[nodeShape]:\n                logger.debug('Limiting the estimated number of necessary %s (%s) to the '\n                             'configured maximum (%s).', nodeType,\n                             estimatedNodeCount,\n                             self.maxNodes[nodeShape])\n                estimatedNodeCount = self.maxNodes[nodeShape]\n            elif estimatedNodeCount < self.minNodes[nodeShape]:\n                logger.debug('Raising the estimated number of necessary %s (%s) to the '\n                            'configured minimum (%s).', nodeType,\n                            estimatedNodeCount,\n                            self.minNodes[nodeShape])\n                estimatedNodeCount = self.minNodes[nodeShape]\n            estimatedNodeCounts[nodeShape] = estimatedNodeCount\n        return estimatedNodeCounts"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef updateClusterSize(self, estimatedNodeCounts):\n        newNodeCounts = defaultdict(int)\n        for nodeShape, estimatedNodeCount in estimatedNodeCounts.items():\n            nodeType = self.nodeShapeToType[nodeShape]\n\n            newNodeCount = self.setNodeCount(nodeType=nodeType, numNodes=estimatedNodeCount, preemptable=nodeShape.preemptable)\n            # If we were scaling up a preemptable node type and failed to meet\n            # our target, we will attempt to compensate for the deficit while scaling\n            # non-preemptable nodes of this type.\n            if nodeShape.preemptable:\n                if newNodeCount < estimatedNodeCount:\n                    deficit = estimatedNodeCount - newNodeCount\n                    logger.debug('Preemptable scaler detected deficit of %d nodes of type %s.' % (deficit, nodeType))\n                    self.preemptableNodeDeficit[nodeType] = deficit\n                else:\n                    self.preemptableNodeDeficit[nodeType] = 0\n            newNodeCounts[nodeShape] = newNodeCount\n\n        #Attempt to terminate any nodes that we previously designated for \n        #termination, but which still had workers running.\n        self._terminateIgnoredNodes()\n        return newNodeCounts", "response": "Updates the size of the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nattempts to grow or shrink the number of preemptable or non-preemptable worker nodes in the cluster to the given value, or as close a value as possible, and, after performing the necessary additions or removals of worker nodes, return the resulting number of preemptable or non-preemptable nodes currently in the cluster. :param str nodeType: The node type to add or remove. :param int numNodes: Desired size of the cluster :param bool preemptable: whether the added nodes will be preemptable, i.e. whether they may be removed spontaneously by the underlying platform at any time. :param bool force: If False, the provisioner is allowed to deviate from the given number of nodes. For example, when downsizing a cluster, a provisioner might leave nodes running if they have active jobs running on them. :rtype: int :return: the number of worker nodes in the cluster after making the necessary adjustments. This value should be, but is not guaranteed to be, close or equal to the `numNodes` argument. It represents the closest possible approximation of the actual cluster size at the time this method returns.", "response": "def setNodeCount(self, nodeType, numNodes, preemptable=False, force=False):\n        \"\"\"\n        Attempt to grow or shrink the number of preemptable or non-preemptable worker nodes in\n        the cluster to the given value, or as close a value as possible, and, after performing\n        the necessary additions or removals of worker nodes, return the resulting number of\n        preemptable or non-preemptable nodes currently in the cluster.\n\n        :param str nodeType: The node type to add or remove.\n\n        :param int numNodes: Desired size of the cluster\n\n        :param bool preemptable: whether the added nodes will be preemptable, i.e. whether they\n               may be removed spontaneously by the underlying platform at any time.\n\n        :param bool force: If False, the provisioner is allowed to deviate from the given number\n               of nodes. For example, when downsizing a cluster, a provisioner might leave nodes\n               running if they have active jobs running on them.\n\n        :rtype: int :return: the number of worker nodes in the cluster after making the necessary\n                adjustments. This value should be, but is not guaranteed to be, close or equal to\n                the `numNodes` argument. It represents the closest possible approximation of the\n                actual cluster size at the time this method returns.\n        \"\"\"\n        for attempt in retry(predicate=self.provisioner.retryPredicate):\n            with attempt:\n                workerInstances = self.getNodes(preemptable=preemptable)\n                logger.debug(\"Cluster contains %i instances\" % len(workerInstances))\n                # Reduce to nodes of the correct type\n                workerInstances = {node:workerInstances[node] for node in workerInstances if node.nodeType == nodeType}\n                ignoredNodes = [node for node in workerInstances if node.privateIP in self.ignoredNodes]\n                numIgnoredNodes = len(ignoredNodes)\n                numCurrentNodes = len(workerInstances)\n                logger.debug(\"Cluster contains %i instances of type %s (%i ignored and draining jobs until \"\n                            \"they can be safely terminated)\" % (numCurrentNodes, nodeType, numIgnoredNodes))\n                if not force:\n                    delta = numNodes - (numCurrentNodes - numIgnoredNodes)\n                else:\n                    delta = numNodes - numCurrentNodes\n                if delta > 0 and numIgnoredNodes > 0:\n                        # We can un-ignore a few nodes to compensate for the additional nodes we want.\n                        numNodesToUnignore = min(delta, numIgnoredNodes)\n                        logger.debug('Unignoring %i nodes because we want to scale back up again.' % numNodesToUnignore)\n                        delta -= numNodesToUnignore\n                        for node in ignoredNodes[:numNodesToUnignore]:\n                            self.ignoredNodes.remove(node.privateIP)\n                            self.leader.batchSystem.unignoreNode(node.privateIP)\n                if delta > 0:\n                    logger.info('Adding %i %s nodes to get to desired cluster size of %i.',\n                                delta,\n                                'preemptable' if preemptable else 'non-preemptable',\n                                numNodes)\n                    numNodes = numCurrentNodes + self._addNodes(nodeType, numNodes=delta,\n                                                                preemptable=preemptable)\n                elif delta < 0:\n                    logger.info('Removing %i %s nodes to get to desired cluster size of %i.', -delta, 'preemptable' if preemptable else 'non-preemptable', numNodes)\n                    numNodes = numCurrentNodes - self._removeNodes(workerInstances,\n                                                                   nodeType = nodeType,\n                                                                   numNodes=-delta,\n                                                                   preemptable=preemptable,\n                                                                   force=force)\n                else:\n                    if not force:\n                        logger.debug('Cluster (minus ignored nodes) already at desired size of %i. Nothing to do.', numNodes)\n                    else:\n                        logger.debug('Cluster already at desired size of %i. Nothing to do.', numNodes)\n        return numNodes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dictionary mapping node identifiers of preemptable or non - preemptable nodes to NodeInfo objects.", "response": "def getNodes(self, preemptable):\n        \"\"\"\n        Returns a dictionary mapping node identifiers of preemptable or non-preemptable nodes to\n        NodeInfo objects, one for each node.\n\n        This method is the definitive source on nodes in cluster, & is responsible for consolidating\n        cluster state between the provisioner & batch system.\n\n        :param bool preemptable: If True (False) only (non-)preemptable nodes will be returned.\n               If None, all nodes will be returned.\n\n        :rtype: dict[Node, NodeInfo]\n        \"\"\"\n        def _getInfo(allMesosNodes, ip):\n            info = None\n            try:\n                info = allMesosNodes[ip]\n            except KeyError:\n                # never seen by mesos - 1 of 3 possibilities:\n                # 1) node is still launching mesos & will come online soon\n                # 2) no jobs have been assigned to this worker. This means the executor was never\n                #    launched, so we don't even get an executorInfo back indicating 0 workers running\n                # 3) mesos crashed before launching, worker will never come online\n                # In all 3 situations it's safe to fake executor info with 0 workers, since in all\n                # cases there are no workers running.\n                info = NodeInfo(coresTotal=1, coresUsed=0, requestedCores=0,\n                                memoryTotal=1, memoryUsed=0, requestedMemory=0,\n                                workers=0)\n            else:\n                # Node was tracked but we haven't seen this in the last 10 minutes\n                inUse = self.leader.batchSystem.nodeInUse(ip)\n                if not inUse and info:\n                    # The node hasn't reported in the last 10 minutes & last we know\n                    # there weren't any tasks running. We will fake executorInfo with no\n                    # worker to reflect this, since otherwise this node will never\n                    # be considered for termination\n                    info.workers = 0\n                else:\n                    pass\n                    # despite the node not reporting to mesos jobs may still be running\n                    # so we can't terminate the node\n            return info\n\n        allMesosNodes = self.leader.batchSystem.getNodes(preemptable, timeout=None)\n        recentMesosNodes = self.leader.batchSystem.getNodes(preemptable)\n        provisionerNodes = self.provisioner.getProvisionedWorkers(nodeType=None, preemptable=preemptable)\n\n        if len(recentMesosNodes) != len(provisionerNodes):\n            logger.debug(\"Consolidating state between mesos and provisioner\")\n        nodeToInfo = {}\n        # fixme: what happens if awsFilterImpairedNodes is used?\n        # if this assertion is false it means that user-managed nodes are being\n        # used that are outside the provisioner's control\n        # this would violate many basic assumptions in autoscaling so it currently not allowed\n        for node, ip in ((node, node.privateIP) for node in provisionerNodes):\n            info = None\n            if ip not in recentMesosNodes:\n                logger.debug(\"Worker node at %s is not reporting executor information\", ip)\n                # we don't have up to date information about the node\n                info = _getInfo(allMesosNodes, ip)\n            else:\n                # mesos knows about the ip & we have up to date information - easy!\n                info = recentMesosNodes[ip]\n            # add info to dict to return\n            nodeToInfo[node] = info\n        return nodeToInfo"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check(self):\n        try:\n            self.join(timeout=0)\n        except Exception as e:\n            logger.exception(e)\n            raise", "response": "Checks if the scaler thread is still running."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef shutdown(self):\n        self.stop = True\n        if self.stats:\n            self.stats.shutDownStats()\n        self.join()", "response": "Shut down the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef enable_absolute_imports(script, script_name):\n    tree = ast.parse(script, filename=script_name)\n    lines = script.split('\\n')\n    while lines and lines[-1] == \"\":\n        lines.pop()\n    node = None\n    for child in ast.iter_child_nodes(tree):\n        if isinstance(child, ast.Import):\n            node = child\n            break\n        elif isinstance(child, ast.ImportFrom):\n            assert child.level == 0  # don't know what this means\n            if child.module == '__future__':\n                if any(alias.name == 'absolute_import' for alias in child.names):\n                    return None\n                else:\n                    if node is None: node = child\n            else:\n                node = child\n                break\n    if node is None:\n        if len(tree.body) == 0:\n            node = ast.stmt()\n            node.lineno = len(lines) + 1\n        else:\n            node = tree.body[0]\n            # This crazy heuristic tries to handle top-level docstrings with newlines in them\n            # for which lineno is the line where the docstring ends\n            if isinstance(node, ast.Expr) and isinstance(node.value, ast.Str):\n                node.lineno -= node.value.s.count('\\n')\n\n    line = 'from __future__ import absolute_import'\n    lines.insert(node.lineno - 1, line)\n    lines.append(\"\")\n    return '\\n'.join(lines)", "response": "Enable absolute imports in the given script."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing the job store with the given configuration.", "response": "def initialize(self, config):\n        \"\"\"\n        Create the physical storage for this job store, allocate a workflow ID and persist the\n        given Toil configuration to the store.\n\n        :param toil.common.Config config: the Toil configuration to initialize this job store\n               with. The given configuration will be updated with the newly allocated workflow ID.\n\n        :raises JobStoreExistsException: if the physical storage for this job store already exists\n        \"\"\"\n        assert config.workflowID is None\n        config.workflowID = str(uuid4())\n        logger.debug(\"The workflow ID is: '%s'\" % config.workflowID)\n        self.__config = config\n        self.writeConfig()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites the current configuration to the job store.", "response": "def writeConfig(self):\n        \"\"\"\n        Persists the value of the :attr:`AbstractJobStore.config` attribute to the\n        job store, so that it can be retrieved later by other instances of this class.\n        \"\"\"\n        with self.writeSharedFileStream('config.pickle', isProtected=False) as fileHandle:\n            pickle.dump(self.__config, fileHandle, pickle.HIGHEST_PROTOCOL)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresumes the job store.", "response": "def resume(self):\n        \"\"\"\n        Connect this instance to the physical storage it represents and load the Toil configuration\n        into the :attr:`AbstractJobStore.config` attribute.\n\n        :raises NoSuchJobStoreException: if the physical storage for this job store doesn't exist\n        \"\"\"\n        with self.readSharedFileStream('config.pickle') as fileHandle:\n            config = safeUnpickleFromStream(fileHandle)\n            assert config.workflowID is not None\n            self.__config = config"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setRootJob(self, rootJobStoreID):\n        with self.writeSharedFileStream(self.rootJobStoreIDFileName) as f:\n            f.write(rootJobStoreID.encode('utf-8'))", "response": "Sets the root job of the workflow backed by this job store as the root job of the workflow backed by this job store."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef loadRootJob(self):\n        try:\n            with self.readSharedFileStream(self.rootJobStoreIDFileName) as f:\n                rootJobStoreID = f.read().decode('utf-8')\n        except NoSuchFileException:\n            raise JobException('No job has been set as the root in this job store')\n        if not self.exists(rootJobStoreID):\n            raise JobException(\"The root job '%s' doesn't exist. Either the Toil workflow \"\n                               \"is finished or has never been started\" % rootJobStoreID)\n        return self.load(rootJobStoreID)", "response": "Loads the root job in the current job store."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new job and set it as the root job in this job store", "response": "def createRootJob(self, *args, **kwargs):\n        \"\"\"\n        Create a new job and set it as the root job in this job store\n\n        :rtype: toil.jobGraph.JobGraph\n        \"\"\"\n        rootJob = self.create(*args, **kwargs)\n        self.setRootJob(rootJob.jobStoreID)\n        return rootJob"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _jobStoreClasses(self):\n        jobStoreClassNames = (\n            \"toil.jobStores.azureJobStore.AzureJobStore\",\n            \"toil.jobStores.fileJobStore.FileJobStore\",\n            \"toil.jobStores.googleJobStore.GoogleJobStore\",\n            \"toil.jobStores.aws.jobStore.AWSJobStore\",\n            \"toil.jobStores.abstractJobStore.JobStoreSupport\")\n        jobStoreClasses = []\n        for className in jobStoreClassNames:\n            moduleName, className = className.rsplit('.', 1)\n            from importlib import import_module\n            try:\n                module = import_module(moduleName)\n            except ImportError:\n                logger.debug(\"Unable to import '%s' as is expected if the corresponding extra was \"\n                             \"omitted at installation time.\", moduleName)\n            else:\n                jobStoreClass = getattr(module, className)\n                jobStoreClasses.append(jobStoreClass)\n        return jobStoreClasses", "response": "Returns a list of concrete AbstractJobStore implementations whose dependencies are installed."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _findJobStoreForUrl(self, url, export=False):\n        for jobStoreCls in self._jobStoreClasses:\n            if jobStoreCls._supportsUrl(url, export):\n                return jobStoreCls\n        raise RuntimeError(\"No job store implementation supports %sporting for URL '%s'\" %\n                           ('ex' if export else 'im', url.geturl()))", "response": "Returns the AbstractJobStore subclass that supports the given URL."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nimport the file at the given URL into the job store.", "response": "def importFile(self, srcUrl, sharedFileName=None, hardlink=False):\n        \"\"\"\n        Imports the file at the given URL into job store. The ID of the newly imported file is\n        returned. If the name of a shared file name is provided, the file will be imported as\n        such and None is returned.\n\n        Currently supported schemes are:\n\n            - 's3' for objects in Amazon S3\n                e.g. s3://bucket/key\n\n            - 'wasb' for blobs in Azure Blob Storage\n                e.g. wasb://container/blob\n\n            - 'file' for local files\n                e.g. file:///local/file/path\n\n            - 'http'\n                e.g. http://someurl.com/path\n\n            - 'gs'\n                e.g. gs://bucket/file\n\n        :param str srcUrl: URL that points to a file or object in the storage mechanism of a\n                supported URL scheme e.g. a blob in an Azure Blob Storage container.\n\n        :param str sharedFileName: Optional name to assign to the imported file within the job store\n\n        :return: The jobStoreFileId of the imported file or None if sharedFileName was given\n        :rtype: toil.fileStore.FileID or None\n        \"\"\"\n        # Note that the helper method _importFile is used to read from the source and write to\n        # destination (which is the current job store in this case). To implement any\n        # optimizations that circumvent this, the _importFile method should be overridden by\n        # subclasses of AbstractJobStore.\n        srcUrl = urlparse.urlparse(srcUrl)\n        otherCls = self._findJobStoreForUrl(srcUrl)\n        return self._importFile(otherCls, srcUrl, sharedFileName=sharedFileName, hardlink=hardlink)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _importFile(self, otherCls, url, sharedFileName=None, hardlink=False):\n        if sharedFileName is None:\n            with self.writeFileStream() as (writable, jobStoreFileID):\n                otherCls._readFromUrl(url, writable)\n                return FileID(jobStoreFileID, otherCls.getSize(url))\n        else:\n            self._requireValidSharedFileName(sharedFileName)\n            with self.writeSharedFileStream(sharedFileName) as writable:\n                otherCls._readFromUrl(url, writable)\n                return None", "response": "Imports the file at the given URL using the given job store class."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef exportFile(self, jobStoreFileID, dstUrl):\n        dstUrl = urlparse.urlparse(dstUrl)\n        otherCls = self._findJobStoreForUrl(dstUrl, export=True)\n        self._exportFile(otherCls, jobStoreFileID, dstUrl)", "response": "Exports file to destination pointed at by the destination URL."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexport the file with the given id to the given URL.", "response": "def _exportFile(self, otherCls, jobStoreFileID, url):\n        \"\"\"\n        Refer to exportFile docstring for information about this method.\n\n        :param AbstractJobStore otherCls: The concrete subclass of AbstractJobStore that supports\n               exporting to the given URL. Note that the type annotation here is not completely\n               accurate. This is not an instance, it's a class, but there is no way to reflect\n               that in :pep:`484` type hints.\n\n        :param str jobStoreFileID: The id of the file that will be exported.\n\n        :param urlparse.ParseResult url: The parsed URL of the file to export to.\n        \"\"\"\n        with self.readFileStream(jobStoreFileID) as readable:\n            otherCls._writeToUrl(readable, url)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfunctioning to clean up the state of a job store.", "response": "def clean(self, jobCache=None):\n        \"\"\"\n        Function to cleanup the state of a job store after a restart.\n        Fixes jobs that might have been partially updated. Resets the try counts and removes jobs\n        that are not successors of the current root job.\n\n        :param dict[str,toil.jobGraph.JobGraph] jobCache: if a value it must be a dict\n               from job ID keys to JobGraph object values. Jobs will be loaded from the cache\n               (which can be downloaded from the job store in a batch) instead of piecemeal when\n               recursed into.\n        \"\"\"\n        if jobCache is None:\n            logger.warning(\"Cleaning jobStore recursively. This may be slow.\")\n\n        # Functions to get and check the existence of jobs, using the jobCache\n        # if present\n        def getJob(jobId):\n            if jobCache is not None:\n                try:\n                    return jobCache[jobId]\n                except KeyError:\n                    self.load(jobId)\n            else:\n                return self.load(jobId)\n\n        def haveJob(jobId):\n            if jobCache is not None:\n                if jobId in jobCache:\n                    return True\n                else:\n                    return self.exists(jobId)\n            else:\n                return self.exists(jobId)\n\n        def getJobs():\n            if jobCache is not None:\n                return itervalues(jobCache)\n            else:\n                return self.jobs()\n\n        # Iterate from the root jobGraph and collate all jobs that are reachable from it\n        # All other jobs returned by self.jobs() are orphaned and can be removed\n        reachableFromRoot = set()\n\n        def getConnectedJobs(jobGraph):\n            if jobGraph.jobStoreID in reachableFromRoot:\n                return\n            reachableFromRoot.add(jobGraph.jobStoreID)\n            # Traverse jobs in stack\n            for jobs in jobGraph.stack:\n                for successorJobStoreID in [x.jobStoreID for x in jobs]:\n                    if (successorJobStoreID not in reachableFromRoot\n                        and haveJob(successorJobStoreID)):\n                        getConnectedJobs(getJob(successorJobStoreID))\n            # Traverse service jobs\n            for jobs in jobGraph.services:\n                for serviceJobStoreID in [x.jobStoreID for x in jobs]:\n                    if haveJob(serviceJobStoreID):\n                        assert serviceJobStoreID not in reachableFromRoot\n                        reachableFromRoot.add(serviceJobStoreID)\n\n        logger.debug(\"Checking job graph connectivity...\")\n        getConnectedJobs(self.loadRootJob())\n        logger.debug(\"%d jobs reachable from root.\" % len(reachableFromRoot))\n\n        # Cleanup jobs that are not reachable from the root, and therefore orphaned\n        jobsToDelete = [x for x in getJobs() if x.jobStoreID not in reachableFromRoot]\n        for jobGraph in jobsToDelete:\n            # clean up any associated files before deletion\n            for fileID in jobGraph.filesToDelete:\n                # Delete any files that should already be deleted\n                logger.warn(\"Deleting file '%s'. It is marked for deletion but has not yet been \"\n                            \"removed.\", fileID)\n                self.deleteFile(fileID)\n            # Delete the job\n            self.delete(jobGraph.jobStoreID)\n\n        jobGraphsReachableFromRoot = {id: getJob(id) for id in reachableFromRoot}\n\n        # Clean up any checkpoint jobs -- delete any successors it\n        # may have launched, and restore the job to a pristine\n        # state\n        jobsDeletedByCheckpoints = set()\n        for jobGraph in [jG for jG in jobGraphsReachableFromRoot.values() if jG.checkpoint is not None]:\n            if jobGraph.jobStoreID in jobsDeletedByCheckpoints:\n                # This is a checkpoint that was nested within an\n                # earlier checkpoint, so it and all its successors are\n                # already gone.\n                continue\n            logger.debug(\"Restarting checkpointed job %s\" % jobGraph)\n            deletedThisRound = jobGraph.restartCheckpoint(self)\n            jobsDeletedByCheckpoints |= set(deletedThisRound)\n        for jobID in jobsDeletedByCheckpoints:\n            del jobGraphsReachableFromRoot[jobID]\n\n        # Clean up jobs that are in reachable from the root\n        for jobGraph in jobGraphsReachableFromRoot.values():\n            # jobGraphs here are necessarily in reachable from root.\n\n            changed = [False]  # This is a flag to indicate the jobGraph state has\n            # changed\n\n            # If the job has files to delete delete them.\n            if len(jobGraph.filesToDelete) != 0:\n                # Delete any files that should already be deleted\n                for fileID in jobGraph.filesToDelete:\n                    logger.critical(\"Removing file in job store: %s that was \"\n                                    \"marked for deletion but not previously removed\" % fileID)\n                    self.deleteFile(fileID)\n                jobGraph.filesToDelete = []\n                changed[0] = True\n\n            # For a job whose command is already executed, remove jobs from the stack that are\n            # already deleted. This cleans up the case that the jobGraph had successors to run,\n            # but had not been updated to reflect this.\n            if jobGraph.command is None:\n                stackSizeFn = lambda: sum(map(len, jobGraph.stack))\n                startStackSize = stackSizeFn()\n                # Remove deleted jobs\n                jobGraph.stack = [[y for y in x if self.exists(y.jobStoreID)] for x in jobGraph.stack]\n                # Remove empty stuff from the stack\n                jobGraph.stack = [x for x in jobGraph.stack if len(x) > 0]\n                # Check if anything got removed\n                if stackSizeFn() != startStackSize:\n                    changed[0] = True\n\n            # Cleanup any services that have already been finished.\n            # Filter out deleted services and update the flags for services that exist\n            # If there are services then renew\n            # the start and terminate flags if they have been removed\n            def subFlagFile(jobStoreID, jobStoreFileID, flag):\n                if self.fileExists(jobStoreFileID):\n                    return jobStoreFileID\n\n                # Make a new flag\n                newFlag = self.getEmptyFileStoreID()\n\n                # Load the jobGraph for the service and initialise the link\n                serviceJobGraph = getJob(jobStoreID)\n\n                if flag == 1:\n                    logger.debug(\"Recreating a start service flag for job: %s, flag: %s\",\n                                 jobStoreID, newFlag)\n                    serviceJobGraph.startJobStoreID = newFlag\n                elif flag == 2:\n                    logger.debug(\"Recreating a terminate service flag for job: %s, flag: %s\",\n                                 jobStoreID, newFlag)\n                    serviceJobGraph.terminateJobStoreID = newFlag\n                else:\n                    logger.debug(\"Recreating a error service flag for job: %s, flag: %s\",\n                                 jobStoreID, newFlag)\n                    assert flag == 3\n                    serviceJobGraph.errorJobStoreID = newFlag\n\n                # Update the service job on disk\n                self.update(serviceJobGraph)\n\n                changed[0] = True\n\n                return newFlag\n\n            servicesSizeFn = lambda: sum(map(len, jobGraph.services))\n            startServicesSize = servicesSizeFn()\n\n            def replaceFlagsIfNeeded(serviceJobNode):\n                serviceJobNode.startJobStoreID = subFlagFile(serviceJobNode.jobStoreID, serviceJobNode.startJobStoreID, 1)\n                serviceJobNode.terminateJobStoreID = subFlagFile(serviceJobNode.jobStoreID, serviceJobNode.terminateJobStoreID, 2)\n                serviceJobNode.errorJobStoreID = subFlagFile(serviceJobNode.jobStoreID, serviceJobNode.errorJobStoreID, 3)\n\n            # jobGraph.services is a list of lists containing serviceNodes\n            # remove all services that no longer exist\n            services = jobGraph.services\n            jobGraph.services = []\n            for serviceList in services:\n                existingServices = [service for service in serviceList if self.exists(service.jobStoreID)]\n                if existingServices:\n                    jobGraph.services.append(existingServices)\n\n            list(map(lambda serviceList: list(map(replaceFlagsIfNeeded, serviceList)), jobGraph.services))\n\n            if servicesSizeFn() != startServicesSize:\n                changed[0] = True\n\n            # Reset the retry count of the jobGraph\n            if jobGraph.remainingRetryCount != self._defaultTryCount():\n                jobGraph.remainingRetryCount = self._defaultTryCount()\n                changed[0] = True\n\n            # This cleans the old log file which may\n            # have been left if the jobGraph is being retried after a jobGraph failure.\n            if jobGraph.logJobStoreFileID != None:\n                self.deleteFile(jobGraph.logJobStoreFileID)\n                jobGraph.logJobStoreFileID = None\n                changed[0] = True\n\n            if changed[0]:  # Update, but only if a change has occurred\n                logger.critical(\"Repairing job: %s\" % jobGraph.jobStoreID)\n                self.update(jobGraph)\n\n        # Remove any crufty stats/logging files from the previous run\n        logger.debug(\"Discarding old statistics and logs...\")\n        # We have to manually discard the stream to avoid getting\n        # stuck on a blocking write from the job store.\n        def discardStream(stream):\n            \"\"\"Read the stream 4K at a time until EOF, discarding all input.\"\"\"\n            while len(stream.read(4096)) != 0:\n                pass\n        self.readStatsAndLogging(discardStream)\n\n        logger.debug(\"Job store is clean\")\n        # TODO: reloading of the rootJob may be redundant here\n        return self.loadRootJob()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef isNumber(s):\n    s = s.replace(',', '')\n    try:\n        float(s)\n        return True\n    except ValueError:\n        pass\n    try:\n        import unicodedata\n        unicodedata.numeric(s)\n        return True\n    except (TypeError, ValueError) as e:\n        pass\n    return False", "response": "Determines if a unicode string s represents a number."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parseStorage(storageData):\n    if storageData == \"EBS only\":\n        return [0, 0]\n    else:\n        specs = storageData.strip().split()\n        if isNumber(specs[0]) and specs[1] == 'x' and isNumber(specs[2]):\n            return float(specs[0].replace(',', '')), float(specs[2].replace(',', ''))\n        else:\n            raise RuntimeError('EC2 JSON format has likely changed.  Error parsing disk specs.')", "response": "Parses EC2 JSON storage param string into a number."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parseMemory(memAttribute):\n    mem = memAttribute.replace(',', '').split()\n    if mem[1] == 'GiB':\n        return float(mem[0])\n    else:\n        raise RuntimeError('EC2 JSON format has likely changed.  Error parsing memory.')", "response": "Parses the EC2 memory attribute into a float."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fetchEC2InstanceDict(regionNickname=None):\n    ec2Source = 'https://pricing.us-east-1.amazonaws.com/offers/v1.0/aws/AmazonEC2/current/index.json'\n    if regionNickname is None:\n        regionNickname = 'us-west-2'\n    region = EC2Regions[regionNickname]  # JSON uses verbose region names as keys\n\n    ec2InstanceList = []\n\n    # summon the API to grab the latest instance types/prices/specs\n    response = requests.get(ec2Source)\n    if response.ok:\n        ec2InstanceList = parseEC2Json2List(jsontext=response.text, region=region)\n\n    if ec2InstanceList:\n        return dict((_.name, _) for _ in ec2InstanceList)\n    else:\n        from toil.lib import generatedEC2Lists as defaultEC2\n        return dict((_.name, _) for _ in defaultEC2.ec2InstancesByRegion[regionNickname])", "response": "Fetches the latest EC2 instances types by region programmatically using the AWS pricing API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntake a JSON and returns a list of InstanceType objects representing EC2 instance params.", "response": "def parseEC2Json2List(jsontext, region):\n    \"\"\"\n    Takes a JSON and returns a list of InstanceType objects representing EC2 instance params.\n\n    :param jsontext:\n    :param region:\n    :return:\n    \"\"\"\n    currentList = json.loads(jsontext)\n    ec2InstanceList = []\n    for k, v in iteritems(currentList[\"products\"]):\n        if \"location\" in v[\"attributes\"] and v[\"attributes\"][\"location\"] == region:\n            # 3 tenant types: 'Host' (always $0.00; just a template?)\n            #                 'Dedicated' (toil does not support; these are pricier)\n            #                 'Shared' (AWS default and what toil uses)\n            if \"tenancy\" in v[\"attributes\"] and v[\"attributes\"][\"tenancy\"] == \"Shared\":\n                if v[\"attributes\"][\"operatingSystem\"] == \"Linux\":\n                    # The same instance can appear with multiple \"operation\"\n                    # values; \"RunInstances\" is normal, and\n                    # \"RunInstances:<code>\" is e.g. Linux with MS SQL Server\n                    # installed.\n                    if v[\"attributes\"][\"operation\"] == \"RunInstances\":\n                        disks, disk_capacity = parseStorage(v[\"attributes\"][\"storage\"])\n                        memory = parseMemory(v[\"attributes\"][\"memory\"])\n                        instance = InstanceType(name=v[\"attributes\"][\"instanceType\"],\n                                                cores=v[\"attributes\"][\"vcpu\"],\n                                                memory=memory,\n                                                disks=disks,\n                                                disk_capacity=disk_capacity)\n                        if instance not in ec2InstanceList:\n                            ec2InstanceList.append(instance)\n                        else:\n                            raise RuntimeError('EC2 JSON format has likely changed.  '\n                                               'Duplicate instance {} found.'.format(instance))\n    return ec2InstanceList"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the static EC2 Instances file with the latest version of the AWS Bulk API.", "response": "def updateStaticEC2Instances():\n    \"\"\"\n    Generates a new python file of fetchable EC2 Instances by region with current prices and specs.\n\n    Takes a few (~3+) minutes to run (you'll need decent internet).\n\n    :return: Nothing.  Writes a new 'generatedEC2Lists.py' file.\n    \"\"\"\n    logger.info(\"Updating Toil's EC2 lists to the most current version from AWS's bulk API.  \"\n                \"This may take a while, depending on your internet connection.\")\n\n    dirname = os.path.dirname(__file__)\n    # the file Toil uses to get info about EC2 instance types\n    origFile = os.path.join(dirname, 'generatedEC2Lists.py')\n    assert os.path.exists(origFile)\n    # use a temporary file until all info is fetched\n    genFile = os.path.join(dirname, 'generatedEC2Lists_tmp.py')\n    assert not os.path.exists(genFile)\n    # will be used to save a copy of the original when finished\n    oldFile = os.path.join(dirname, 'generatedEC2Lists_old.py')\n\n    # provenance note, copyright and imports\n    with open(genFile, 'w') as f:\n        f.write(textwrap.dedent('''\n        # !!! AUTOGENERATED FILE !!!\n        # Update with: src/toil/utils/toilUpdateEC2Instances.py\n        #\n        # Copyright (C) 2015-{year} UCSC Computational Genomics Lab\n        #\n        # Licensed under the Apache License, Version 2.0 (the \"License\");\n        # you may not use this file except in compliance with the License.\n        # You may obtain a copy of the License at\n        #\n        #     http://www.apache.org/licenses/LICENSE-2.0\n        #\n        # Unless required by applicable law or agreed to in writing, software\n        # distributed under the License is distributed on an \"AS IS\" BASIS,\n        # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n        # See the License for the specific language governing permissions and\n        # limitations under the License.\n        from six import iteritems\n        from toil.lib.ec2nodes import InstanceType\\n\\n\\n''').format(year=datetime.date.today().strftime(\"%Y\"))[1:])\n\n    currentEC2List = []\n    instancesByRegion = {}\n    for regionNickname, _ in iteritems(EC2Regions):\n        currentEC2Dict = fetchEC2InstanceDict(regionNickname=regionNickname)\n        for instanceName, instanceTypeObj in iteritems(currentEC2Dict):\n            if instanceTypeObj not in currentEC2List:\n                currentEC2List.append(instanceTypeObj)\n            instancesByRegion.setdefault(regionNickname, []).append(instanceName)\n\n    # write header of total EC2 instance type list\n    genString = \"# {num} Instance Types.  Generated {date}.\\n\".format(\n                num=str(len(currentEC2List)), date=str(datetime.datetime.now()))\n    genString = genString + \"E2Instances = {\\n\"\n    sortedCurrentEC2List = sorted(currentEC2List, key=lambda x: x.name)\n\n    # write the list of all instances types\n    for i in sortedCurrentEC2List:\n        z = \"    '{name}': InstanceType(name='{name}', cores={cores}, memory={memory}, disks={disks}, disk_capacity={disk_capacity}),\" \\\n            \"\\n\".format(name=i.name, cores=i.cores, memory=i.memory, disks=i.disks, disk_capacity=i.disk_capacity)\n        genString = genString + z\n    genString = genString + '}\\n\\n'\n\n    genString = genString + 'regionDict = {\\n'\n    for regionName, instanceList in iteritems(instancesByRegion):\n        genString = genString + \"              '{regionName}': [\".format(regionName=regionName)\n        for instance in sorted(instanceList):\n            genString = genString + \"'{instance}', \".format(instance=instance)\n        if genString.endswith(', '):\n            genString = genString[:-2]\n        genString = genString + '],\\n'\n    if genString.endswith(',\\n'):\n        genString = genString[:-len(',\\n')]\n    genString = genString + '}\\n'\n    with open(genFile, 'a+') as f:\n        f.write(genString)\n\n    # append key for fetching at the end\n    regionKey = '\\nec2InstancesByRegion = dict((region, [E2Instances[i] for i in instances]) for region, instances in iteritems(regionDict))\\n'\n\n    with open(genFile, 'a+') as f:\n        f.write(regionKey)\n    # preserve the original file unless it already exists\n    if not os.path.exists(oldFile):\n        os.rename(origFile, oldFile)\n    # delete the original file if it's still there\n    if os.path.exists(origFile):\n        os.remove(origFile)\n    # replace the instance list with a current list\n    os.rename(genFile, origFile)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _runWorker(self, jobCommand, jobID, environment):\n        startTime = time.time()  # Time job is started\n        if self.debugWorker and \"_toil_worker\" in jobCommand:\n            # Run the worker without forking\n            jobName, jobStoreLocator, jobStoreID = jobCommand.split()[1:] # Parse command\n            jobStore = Toil.resumeJobStore(jobStoreLocator)\n            # TODO: The following does not yet properly populate self.runningJobs so it is not possible to kill\n            # running jobs in forkless mode - see the \"None\" value in place of popen\n            info = Info(time.time(), None, killIntended=False)\n            try:\n                self.runningJobs[jobID] = info\n                try:\n                    toil_worker.workerScript(jobStore, jobStore.config, jobName, jobStoreID, \n                                             redirectOutputToLogFile=not self.debugWorker) # Call the worker\n                finally:\n                    self.runningJobs.pop(jobID)\n            finally:\n                if not info.killIntended:\n                    self.outputQueue.put((jobID, 0, time.time() - startTime))\n        else:\n            with self.popenLock:\n                popen = subprocess.Popen(jobCommand,\n                                         shell=True,\n                                         env=dict(os.environ, **environment))\n            info = Info(time.time(), popen, killIntended=False)\n            try:\n                self.runningJobs[jobID] = info\n                try:\n                    statusCode = popen.wait()\n                    if statusCode != 0 and not info.killIntended:\n                        log.error(\"Got exit code %i (indicating failure) \"\n                                  \"from job %s.\", statusCode, self.jobs[jobID])\n                finally:\n                    self.runningJobs.pop(jobID)\n            finally:\n                if not info.killIntended:\n                    self.outputQueue.put((jobID, statusCode, time.time() - startTime))", "response": "Run the jobCommand using the worker and wait for it to finish."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd the command and resources to a queue to be run.", "response": "def issueBatchJob(self, jobNode):\n        \"\"\"Adds the command and resources to a queue to be run.\"\"\"\n        # Round cores to minCores and apply scale\n        cores = math.ceil(jobNode.cores * self.scale / self.minCores) * self.minCores\n        assert cores <= self.maxCores, ('The job {} is requesting {} cores, more than the maximum of '\n                                        '{} cores this batch system was configured with. Scale is '\n                                        'set to {}.'.format(jobNode.jobName, cores, self.maxCores, self.scale))\n        assert cores >= self.minCores\n        assert jobNode.memory <= self.maxMemory, ('The job {} is requesting {} bytes of memory, more than '\n                                          'the maximum of {} this batch system was configured '\n                                          'with.'.format(jobNode.jobName, jobNode.memory, self.maxMemory))\n\n        self.checkResourceRequest(jobNode.memory, cores, jobNode.disk)\n        log.debug(\"Issuing the command: %s with memory: %i, cores: %i, disk: %i\" % (\n            jobNode.command, jobNode.memory, cores, jobNode.disk))\n        with self.jobIndexLock:\n            jobID = self.jobIndex\n            self.jobIndex += 1\n        self.jobs[jobID] = jobNode.command\n        self.inputQueue.put((jobNode.command, jobID, cores, jobNode.memory,\n                             jobNode.disk, self.environment.copy()))\n        if self.debugWorker:  # then run immediately, blocking for return\n            self.worker(self.inputQueue)\n        return jobID"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef killBatchJobs(self, jobIDs):\n        log.debug('Killing jobs: {}'.format(jobIDs))\n        for jobID in jobIDs:\n            if jobID in self.runningJobs:\n                info = self.runningJobs[jobID]\n                info.killIntended = True\n                if info.popen != None:\n                    os.kill(info.popen.pid, 9)\n                else:\n                    # No popen if running in forkless mode currently \n                    assert self.debugWorker\n                    log.critical(\"Can't kill job: %s in debug mode\" % jobID)\n                while jobID in self.runningJobs:\n                    pass", "response": "Kills jobs by ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef shutdown(self):\n        # Remove reference to inputQueue (raises exception if inputQueue is used after method call)\n        inputQueue = self.inputQueue\n        self.inputQueue = None\n        for i in range(self.numWorkers):\n            inputQueue.put(None)\n        for thread in self.workerThreads:\n            thread.join()\n        BatchSystemSupport.workerCleanup(self.workerCleanupInfo)", "response": "Cleanly terminate all worker threads. Join all worker threads."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a map of the run jobs and the return value of their processes.", "response": "def getUpdatedBatchJob(self, maxWait):\n        \"\"\"Returns a map of the run jobs and the return value of their processes.\"\"\"\n        try:\n            item = self.outputQueue.get(timeout=maxWait)\n        except Empty:\n            return None\n        jobID, exitValue, wallTime = item\n        jobCommand = self.jobs.pop(jobID)\n        log.debug(\"Ran jobID: %s with exit value: %i\", jobID, exitValue)\n        return jobID, exitValue, wallTime"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef registered(self, driver, executorInfo, frameworkInfo, agentInfo):\n        \n        # Get the ID we have been assigned, if we have it\n        self.id = executorInfo.executor_id.get('value', None)\n        \n        log.debug(\"Registered executor %s with framework\", self.id)\n        self.address = socket.gethostbyname(agentInfo.hostname)\n        nodeInfoThread = threading.Thread(target=self._sendFrameworkMessage, args=[driver])\n        nodeInfoThread.daemon = True\n        nodeInfoThread.start()", "response": "Called when executor driver has successfully connected with Mesos."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef killTask(self, driver, taskId):\n        try:\n            pid = self.runningTasks[taskId]\n            pgid = os.getpgid(pid)\n        except KeyError:\n            pass\n        else:\n            os.killpg(pgid, signal.SIGKILL)", "response": "Kill the task with the given id."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef launchTask(self, driver, task):\n        \n        log.debug(\"Asked to launch task %s\", repr(task))\n        \n        def runTask():\n\n            log.debug(\"Running task %s\", task.task_id.value)\n            startTime = time.time()\n            sendUpdate(task, 'TASK_RUNNING', wallTime=0)\n\n            # try to unpickle the task\n            try:\n                taskData = pickle.loads(decode_data(task.data))\n            except:\n                exc_info = sys.exc_info()\n                log.error('Exception while unpickling task: ', exc_info=exc_info)\n                exc_type, exc_value, exc_trace = exc_info\n                sendUpdate(task, 'TASK_FAILED', wallTime=0, msg=''.join(traceback.format_exception_only(exc_type, exc_value)))\n                return\n\n            # This is where task.data is first invoked. Using this position to setup cleanupInfo\n            if self.workerCleanupInfo is not None:\n                assert self.workerCleanupInfo == taskData.workerCleanupInfo\n            else:\n                self.workerCleanupInfo = taskData.workerCleanupInfo\n\n            # try to invoke a run on the unpickled task\n            try:\n                process = runJob(taskData)\n                self.runningTasks[task.task_id.value] = process.pid\n                try:\n                    exitStatus = process.wait()\n                    wallTime = time.time() - startTime\n                    if 0 == exitStatus:\n                        sendUpdate(task, 'TASK_FINISHED', wallTime)\n                    elif -9 == exitStatus:\n                        sendUpdate(task, 'TASK_KILLED', wallTime)\n                    else:\n                        sendUpdate(task, 'TASK_FAILED', wallTime, msg=str(exitStatus))\n                finally:\n                    del self.runningTasks[task.task_id.value]\n            except:\n                wallTime = time.time() - startTime\n                exc_info = sys.exc_info()\n                log.error('Exception while running task:', exc_info=exc_info)\n                exc_type, exc_value, exc_trace = exc_info\n                sendUpdate(task, 'TASK_FAILED', wallTime=wallTime, msg=''.join(traceback.format_exception_only(exc_type, exc_value)))\n\n            wallTime = time.time() - startTime\n            sendUpdate(task, 'TASK_FINISHED', wallTime)\n\n\n        def runJob(job):\n            \"\"\"\n            :type job: toil.batchSystems.mesos.ToilJob\n\n            :rtype: subprocess.Popen\n            \"\"\"\n            if job.userScript:\n                job.userScript.register()\n            log.debug(\"Invoking command: '%s'\", job.command)\n            # Construct the job's environment\n            jobEnv = dict(os.environ, **job.environment)\n            log.debug('Using environment variables: %s', jobEnv.keys())\n            with self.popenLock:\n                return subprocess.Popen(job.command,\n                                        preexec_fn=lambda: os.setpgrp(),\n                                        shell=True, env=jobEnv)\n\n        def sendUpdate(task, taskState, wallTime=None, msg=''):\n            update = addict.Dict()\n            update.task_id.value = task.task_id.value\n            if self.id is not None:\n                # Sign our messages as from us, since the driver doesn't do it.\n                update.executor_id.value = self.id\n            update.state = taskState\n            update.timestamp = wallTime\n            update.message = msg\n            driver.sendStatusUpdate(update)\n\n        thread = threading.Thread(target=runTask)\n        thread.start()", "response": "This method is called by the executor when a Mesos task is launched."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretries an operation while the failure matches a given predicate and until a given timeout expires.", "response": "def retry( delays=(0, 1, 1, 4, 16, 64), timeout=300, predicate=never ):\n    \"\"\"\n    Retry an operation while the failure matches a given predicate and until a given timeout\n    expires, waiting a given amount of time in between attempts. This function is a generator\n    that yields contextmanagers. See doctests below for example usage.\n\n    :param Iterable[float] delays: an interable yielding the time in seconds to wait before each\n           retried attempt, the last element of the iterable will be repeated.\n\n    :param float timeout: a overall timeout that should not be exceeded for all attempts together.\n           This is a best-effort mechanism only and it won't abort an ongoing attempt, even if the\n           timeout expires during that attempt.\n\n    :param Callable[[Exception],bool] predicate: a unary callable returning True if another\n           attempt should be made to recover from the given exception. The default value for this\n           parameter will prevent any retries!\n\n    :return: a generator yielding context managers, one per attempt\n    :rtype: Iterator\n\n    Retry for a limited amount of time:\n\n    >>> true = lambda _:True\n    >>> false = lambda _:False\n    >>> i = 0\n    >>> for attempt in retry( delays=[0], timeout=.1, predicate=true ):\n    ...     with attempt:\n    ...         i += 1\n    ...         raise RuntimeError('foo')\n    Traceback (most recent call last):\n    ...\n    RuntimeError: foo\n    >>> i > 1\n    True\n\n    If timeout is 0, do exactly one attempt:\n\n    >>> i = 0\n    >>> for attempt in retry( timeout=0 ):\n    ...     with attempt:\n    ...         i += 1\n    ...         raise RuntimeError( 'foo' )\n    Traceback (most recent call last):\n    ...\n    RuntimeError: foo\n    >>> i\n    1\n\n    Don't retry on success:\n\n    >>> i = 0\n    >>> for attempt in retry( delays=[0], timeout=.1, predicate=true ):\n    ...     with attempt:\n    ...         i += 1\n    >>> i\n    1\n\n    Don't retry on unless predicate returns True:\n\n    >>> i = 0\n    >>> for attempt in retry( delays=[0], timeout=.1, predicate=false):\n    ...     with attempt:\n    ...         i += 1\n    ...         raise RuntimeError( 'foo' )\n    Traceback (most recent call last):\n    ...\n    RuntimeError: foo\n    >>> i\n    1\n    \"\"\"\n    if timeout > 0:\n        go = [ None ]\n\n        @contextmanager\n        def repeated_attempt( delay ):\n            try:\n                yield\n            except Exception as e:\n                if time.time( ) + delay < expiration and predicate( e ):\n                    log.info( 'Got %s, trying again in %is.', e, delay )\n                    time.sleep( delay )\n                else:\n                    raise\n            else:\n                go.pop( )\n\n        delays = iter( delays )\n        expiration = time.time( ) + timeout\n        delay = next( delays )\n        while go:\n            yield repeated_attempt( delay )\n            delay = next( delays, delay )\n    else:\n        @contextmanager\n        def single_attempt( ):\n            yield\n\n        yield single_attempt( )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef retryable_http_error( e ):\n    if isinstance( e, urllib.error.HTTPError ) and e.code in ('503', '408', '500'):\n        # The server returned one of:\n        # 503 Service Unavailable\n        # 408 Request Timeout\n        # 500 Internal Server Error\n        return True\n    if isinstance( e, BadStatusLine ):\n        # The server didn't return a valid response at all\n        return True\n    return False", "response": "Determine if an error occurred during an HTTP download."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef retry_http( delays=default_delays, timeout=default_timeout, predicate=retryable_http_error ):\n    return retry( delays=delays, timeout=timeout, predicate=predicate )", "response": "Retry a single HTTP request."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef killBatchJobs(self, jobIDs):\n        self.killLocalJobs(jobIDs)\n        jobIDs = set(jobIDs)\n        logger.debug('Jobs to be killed: %r', jobIDs)\n        for jobID in jobIDs:\n            self.killQueue.put(jobID)\n        while jobIDs:\n            killedJobId = self.killedJobsQueue.get()\n            if killedJobId is None:\n                break\n            jobIDs.remove(killedJobId)\n            if killedJobId in self.currentJobs:\n                self.currentJobs.remove(killedJobId)\n            if jobIDs:\n                logger.debug('Some kills (%s) still pending, sleeping %is', len(jobIDs),\n                             self.sleepSeconds())", "response": "Kills the given jobs in the batch by checking them are not in the list of issued jobs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve running job IDs from local and batch scheduler.", "response": "def getRunningBatchJobIDs(self):\n        \"\"\"\n        Retrieve running job IDs from local and batch scheduler.\n\n        Respects statePollingWait and will return cached results if not within\n        time period to talk with the scheduler.\n        \"\"\"\n        if (self._getRunningBatchJobIDsTimestamp and (\n                datetime.now() -\n                self._getRunningBatchJobIDsTimestamp).total_seconds() <\n                self.config.statePollingWait):\n            batchIds = self._getRunningBatchJobIDsCache\n        else:\n            batchIds = with_retries(self.worker.getRunningJobIDs)\n            self._getRunningBatchJobIDsCache = batchIds\n            self._getRunningBatchJobIDsTimestamp = datetime.now()\n        batchIds.update(self.getRunningLocalJobIDs())\n        return batchIds"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nshutting down the thread and join it", "response": "def shutdown(self):\n        \"\"\"\n        Signals worker to shutdown (via sentinel) then cleanly joins the thread\n        \"\"\"\n        self.shutdownLocal()\n        newJobsQueue = self.newJobsQueue\n        self.newJobsQueue = None\n\n        newJobsQueue.put(None)\n        self.worker.join()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_lower_bound(self):\n        lower_bounds = []\n\n        for distribution in self.distribs.values():\n            lower_bound = distribution.percent_point(distribution.mean / 10000)\n            if not pd.isnull(lower_bound):\n                lower_bounds.append(lower_bound)\n\n        return min(lower_bounds)", "response": "Compute the lower bound for cumulative density integral."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_column_names(self, X):\n        if isinstance(X, pd.DataFrame):\n            return X.columns\n\n        return range(X.shape[1])", "response": "Return iterable containing columns for the given array X."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_column(self, X, column):\n        if isinstance(X, pd.DataFrame):\n            return X[column].values\n\n        return X[:, column]", "response": "Return a column of the given matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets a column on the matrix X with the given value.", "response": "def set_column(self, X, column, value):\n        \"\"\"Sets a column on the matrix X with the given value.\n\n        Args:\n            X: `numpy.ndarray` or `pandas.DataFrame`.\n            column: `int` or `str`.\n            value: `np.ndarray` with shape (1,)\n\n        Returns:\n            `np.ndarray` or `pandas.DataFrame` with the inserted column.\n\n        \"\"\"\n\n        if isinstance(X, pd.DataFrame):\n            X.loc[:, column] = value\n\n        else:\n            X[:, column] = value\n\n        return X"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute covariance matrix with transformed data.", "response": "def _get_covariance(self, X):\n        \"\"\"Compute covariance matrix with transformed data.\n\n        Args:\n            X: `numpy.ndarray` or `pandas.DataFrame`.\n\n        Returns:\n            np.ndarray\n\n        \"\"\"\n        result = pd.DataFrame(index=range(len(X)))\n        column_names = self.get_column_names(X)\n        for column_name in column_names:\n            column = self.get_column(X, column_name)\n            distrib = self.distribs[column_name]\n\n            # get original distrib's cdf of the column\n            cdf = distrib.cumulative_distribution(column)\n\n            if distrib.constant_value is not None:\n                # This is to avoid np.inf in the case the column is constant.\n                cdf = np.ones(column.shape) - EPSILON\n\n            # get inverse cdf using standard normal\n            result = self.set_column(result, column_name, stats.norm.ppf(cdf))\n\n        # remove any rows that have infinite values\n        result = result[(result != np.inf).all(axis=1)]\n        return pd.DataFrame(data=result).cov().values"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fit(self, X):\n        LOGGER.debug('Fitting Gaussian Copula')\n        column_names = self.get_column_names(X)\n        distribution_class = import_object(self.distribution)\n\n        for column_name in column_names:\n            self.distribs[column_name] = distribution_class()\n            column = self.get_column(X, column_name)\n            self.distribs[column_name].fit(column)\n\n        self.covariance = self._get_covariance(X)\n        self.fitted = True", "response": "Compute the distribution for each variable and then its covariance matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef probability_density(self, X):\n        self.check_fit()\n\n        # make cov positive semi-definite\n        covariance = self.covariance * np.identity(self.covariance.shape[0])\n        return stats.multivariate_normal.pdf(X, cov=covariance)", "response": "Compute the probability density function for given copula family."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cumulative_distribution(self, X):\n        self.check_fit()\n\n        # Wrapper for pdf to accept vector as args\n        def func(*args):\n            return self.probability_density(list(args))\n\n        # Lower bound for integral, to split significant part from tail\n        lower_bound = self.get_lower_bound()\n\n        ranges = [[lower_bound, val] for val in X]\n        return integrate.nquad(func, ranges)[0]", "response": "Computes the cumulative distribution function for the copula\n            X"}
